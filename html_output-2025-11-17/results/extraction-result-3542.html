<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3542 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3542</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3542</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-f27f6d1d521d189e78f5623098ced0deea613d33</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f27f6d1d521d189e78f5623098ced0deea613d33" target="_blank">Satisfiability-Aided Language Models Using Declarative Prompting</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A new satisfiability-aided language modeling (SatLM) approach for improving the reasoning capabilities of LLMs that uses an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer.</p>
                <p><strong>Paper Abstract:</strong> Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving problems that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling (SatLM) approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it out of the description more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness of the answer with respect to the parsed specification and avoid planning errors in the solving process. We evaluate SATLM on 8 different datasets and show that it consistently outperforms program-aided LMs in the imperative paradigm. In particular, SATLM outperforms program-aided LMs by 23% on a challenging subset of the GSM arithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and BoardgameQA, surpassing previous models that are trained on the respective training sets.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3542.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3542.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATLM-GSM-Sys-cd002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on GSM-Sys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Declarative prompting with an LLM (code-davinci-002) to produce a SAT-style formal specification of arithmetic problems, solved by an SMT/SAT solver (Z3); evaluated on GSM-Sys (hard GSM subset with systems of equations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A state-of-the-art OpenAI code-specialized LLM used by the authors for code-adjacent reasoning and prompting experiments (no architecture/parameter count specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM-Sys</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A subset of GSM filtered to examples whose human explanations involve systems of equations; arithmetic reasoning requiring planning over equations and solving linear systems.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Declarative prompting to produce logical/formulaic specifications (z3py-like syntax) combined with an off-the-shelf SAT/SMT solver (Z3) to perform planning and execution; self-consistency decoding also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy decoding accuracy 69.4%; Self-consistency decoding accuracy 80.9% (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>code-davinci-002 baselines — CoT: greedy 46.5% / self-consistency 56.1%; ProgLM (program-aided LM): greedy 43.4% / self-consistency 53.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SATLM improves over ProgLM by ~26.0 percentage points (greedy) and ~27.5 pp (self-consistency); paper highlights a 23% absolute improvement on a challenging GSM arithmetic subset in text (statistically significant, p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>SATLM can abstain when the solver returns UNSAT or AMBIG due to parsing errors; inherits SAT solver limitations (computational cost for quantifiers/nonlinear arithmetic, expressiveness limits); some parser-induced UNSAT/AMBIG cases exist (examples in Appendix G).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation (Table 2) shows SAT_SYMSOLVER (full approach) 69.4% vs SAT_CoTSOLVER 54.5% vs SAT_NoSolver 26.6%, indicating the symbolic solver contributes substantially; error analysis (40 cases) shows CoT-solver failures are mostly planning errors (72.5%) on GSM-Sys (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3542.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATLM-GSM-cd002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on GSM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same SATLM pipeline applied to the GSM arithmetic reasoning benchmark (general math word problems).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-specialized LLM used for producing declarative specifications; no size reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>General math word problem benchmark (GSM) requiring multi-step arithmetic reasoning and equation solving.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Declarative prompting + Z3 SMT solver; self-consistency decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy decoding accuracy 71.8%; Self-consistency decoding accuracy 84.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>code-davinci-002 baselines — CoT: greedy 62.7% / self-consistency 77.3%; ProgLM: greedy 72.7% / self-consistency 82.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>With greedy decoding SATLM is nearly on par with ProgLM (71.8% vs 72.7%); with self-consistency SATLM outperforms ProgLM by 2.4 pp (84.8% vs 82.4%). The paper notes SATLM slightly lags ProgLM with greedy decoding for GSM but surpasses it under self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper notes SATLM may be less effective on tasks whose NL descriptions align more naturally with imperative programs (GSM is such a case); solver-based abstention reduces coverage but increases selective accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Analysis attributes the GSM result to coverage/selective-accuracy tradeoffs: SATLM has slightly lower coverage but higher selective accuracy; self-consistency increases coverage improving overall accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3542.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATLM-Algebra-cd002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on Algebra dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying SATLM to an Algebra dataset extracted from textbooks (math problems), parsing to equations and solving via SMT solver.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-specialized LLM producing z3py-like declarative specs for algebraic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Algebra (He-Yueya et al., 2023 dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Algebra word problems from textbooks, requiring equation formation and solving; more challenging than GSM according to authors.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Declarative prompting + Z3 solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy decoding accuracy 77.5%; Self-consistency decoding accuracy 90.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>code-davinci-002 baselines — CoT: greedy 53.6% / self-consistency 64.9%; ProgLM: greedy 52.3% / self-consistency 57.7%. MathSym (concurrent work) reported 76.3% on Algebra (not directly comparable due to exemplar differences).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SATLM outperforms both CoT and ProgLM by >20 pp (greedy) and substantially under self-consistency (~30+ pp vs ProgLM).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same solver and parsing limitations apply; Algebra is more challenging but SATLM yields large gains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper notes domain-agnostic SAT formulation contrasts with math-specific solvers (e.g., PySym); ablations (Table 2) show solver provides key gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3542.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATLM-LSAT-cd002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on LSAT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Declarative parsing into logical formulas and using an automated theorem prover to solve LSAT-style analytical reasoning problems (logical deduction, sometimes requiring proof-by-contradiction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-specialized LLM used to translate NL into first-order logic formulas; solver handles deductive planning and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LSAT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Analytical/logical reasoning benchmark (LSAT-style problems) requiring deduction and proof strategies in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Declarative prompting + Z3 SMT solver (first-order reasoning); few-shot exemplars; self-consistency decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy decoding accuracy 35.0%; Self-consistency decoding accuracy 37.4% (accuracy %).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>CoT (code-davinci-002) greedy 23.5% / self-consistency 23.1%; ProgLM not applicable (authors did not run ProgLM for LSAT). Previous SoTA reported 30.9% (Zhong et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SATLM improves absolute accuracy over prior CoT baselines by ~11.5 pp (greedy) and sets a new SoTA vs reported prior 30.9% -> 37.4% (self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Authors note Python-based ProgLM not suitable for logical inference required by LSAT (lack of native logic operators/proofs); solver limitations (expressiveness, quantifiers) remain relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors attempted multiple CoT prompts for LSAT that yielded ~20% accuracy; SATLM's declarative + solver approach yields substantial improvement, indicating solver aids complex deductive planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3542.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATLM-BoardGameQA-cd002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on BoardGameQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Declarative parsing plus SAT solving applied to BoardGameQA, a benchmark with contradictory information and multi-step logical reasoning requiring background/common-sense rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM produces z3-style logical constraints; solver finds satisfying assignments under game rules and background facts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BoardGameQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural language reasoning dataset about board games with contradictory information; requires reasoning over rules and often implicit commonsense/prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Declarative prompting + Z3 solver; few-shot prompts; self-consistency decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy decoding aggregated accuracy 79.4% (depth breakdown: depth1 87.6%, depth2 81.7%, depth3 69.0%); Self-consistency aggregated accuracy 80.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>code-davinci-002 CoT: greedy aggregated 60.1% / self-consistency 62.8%. ProgLM not reported for BoardGameQA.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SATLM yields large gains vs CoT: +19.3 pp aggregated (greedy) and sets new SoTA (80.7% reported in Table 1 self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance decreases with reasoning depth; requires LLM to incorporate implicit background knowledge during parsing (authors observe LLMs can do some commonsense during parsing).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors note parsing step benefits from LLM's background knowledge; declarative prompting improves fidelity of translation to constraints (examples in Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3542.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATLM-Clutrr-cd002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on CLUTRR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Declarative parsing to relational constraints and SAT solving for family/kinship reasoning (CLUTRR diagnostic benchmark), testing generalization to longer reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-specialized LLM that outputs declarative relations (z3-like), solver computes transitive/deductive relations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CLUTRR</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Diagnostic inductive reasoning benchmark about family relations requiring multi-step inference; training exemplars have up to 3 steps, test up to 10.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Declarative prompting + Z3 solver; few-shot exemplars emphasizing 2-3 step reasoning to test extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy decoding accuracy 68.3%; Self-consistency decoding accuracy 80.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>code-davinci-002 baselines — CoT: greedy 40.8% / self-consistency 45.7%; ProgLM: greedy 58.9% / self-consistency 71.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SATLM outperforms ProgLM by ~9.4 pp (greedy) and ~8.2 pp (self-consistency); large improvement over CoT (~27.5 pp greedy).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Some UNSAT / AMBIG cases stem from misparsing ambiguous or conflicting statements; solver feedback (UNSAT/AMBIG) used to abstain, improving selective accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Table 3 shows CoT-solver planning errors ~47.5% and execution errors ~52.5% on CLUTRR (distribution differs by dataset); Table 5 shows SATLM selective accuracy on CLUTRR 89.9% vs ProgLM 73.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3542.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATLM-ProofWriter-cd002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on ProofWriter (depth-5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using declarative prompting to produce formal rules and queries and verifying proofs with a solver on ProofWriter depth-5 (challenging deductive proofs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM used to translate NL rules into formal logic; solver performs deductive proof search and returns model/proof.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter (depth-5)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmark requiring the generation of implications and multi-step proofs from natural language rules; depth-5 denotes proofs of length 5.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Declarative prompting + Z3 SMT solver to find models and produce proof traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy decoding accuracy 99.7%; Self-consistency decoding accuracy 99.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>CoT (code-davinci-002) greedy 80.1% / self-consistency 88.7%; ProgLM greedy 83.7% / self-consistency 91.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SATLM substantially outperforms CoT and ProgLM (greedy +19.6 pp vs ProgLM), reaching near-perfect accuracy on these depth-5 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Success contingent on correct parsing into formal rules; solver timeouts/errors possible on malformed specs; paper shows SATLM makes fewer but more accurate predictions (selective behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>SAT solver produces proofs (e.g., resolution refutations) guaranteeing correctness when parsing is correct; demonstrates solver removes planning/execution errors present in CoT/ProgLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3542.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATLM-Color-cd002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on ColoredObject (BIG-bench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying SATLM to a symbolic reasoning task abstracted as constrained list element finding (colored object selection).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM used to produce declarative constraints over list elements and attributes; solver computes satisfying assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ColoredObject (BIG-bench)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Symbolic reasoning over lists/objects to find elements satisfying constraints (logic over attributes).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Declarative prompting + Z3 solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy decoding accuracy 97.7%; Self-consistency decoding accuracy 99.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>CoT greedy 86.3% / self-consistency 90.6%; ProgLM greedy 95.1% / self-consistency 98.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SATLM modestly improves over ProgLM (greedy +2.6 pp, self-consistency +1.4 pp) and substantially over CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Typical parser-induced UNSAT/AMBIG cases and solver expressiveness/compute limits remain applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors report high performance reflecting tasks that map cleanly to declarative constraints; declarative prompting produces higher-likelihood outputs for such tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3542.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATLM-Regex-cd002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on StructuredRegex / STREGEX</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SATLM parses NL regex descriptions into constraints over the surface form of the target regex (string constraints), with solver confirming candidates against I/O examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM translates multimodal NL + I/O examples into declarative constraints over regex surface strings; solver and example filtering used during self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>StructuredRegex (STREGEX)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Regex synthesis from NL descriptions and I/O examples; requires synthesizing regex that fits provided specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Declarative prompting to constraints over strings + solver; self-consistency combined with filtering by I/O examples (per prior work practice).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Greedy decoding accuracy 41.0%; Self-consistency decoding accuracy 59.7% (this surpasses prior SoTA 55.6% reported for related work).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>ProgLM (direct NL->regex) greedy 39.1% / self-consistency 56.5%; No CoT baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SATLM modestly outperforms ProgLM (greedy +1.9 pp, self-consistency +3.2 pp) and achieves a new SoTA with self-consistency in the paper's setup.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Regex dataset is multimodal and uses I/O example filtering during self-consistency; SATLM relies on correct encoding of examples and may produce AMBIG/UNSAT when constraints omitted or misparsed.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Self-consistency plus I/O filtering improves coverage/accuracy; declarative encoding yields higher translation fidelity than imperative direct-program generation for some examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3542.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATLM-cross-LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SATLM evaluated across alternative LLMs (gpt-3.5-turbo, text-davinci-003, code-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assessing SATLM's generality by swapping the LLM used for declarative parsing: gpt-3.5-turbo (0613), text-davinci-003, and code-davinci-001 were tested on multiple tasks showing similar trends.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo / text-davinci-003 / code-davinci-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Alternate LLMs (chat-optimized gpt-3.5-turbo v0613, text-davinci-003 NL-focused, and older code-davinci-001) used to produce declarative specifications fed to the same solver.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple (GSM-Sys, GSM, LSAT, Clutrr, ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same benchmarks as main experiments; tests generalizability across LLM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Declarative prompting + Z3 solver; greedy decoding reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>gpt-3.5-turbo SATLM greedy: GSM-Sys 63.4%, GSM 76.4%, LSAT 30.0%, Clutrr 50.6%, Proof 96.4%. text-davinci-003 SATLM greedy: GSM-Sys 63.6%, GSM 70.3%, LSAT 30.4%, Clutrr 58.2%, Proof 99.7%. code-davinci-001 SATLM greedy: GSM-Sys 16.5%, GSM 34.2%, LSAT 19.6%, Clutrr 30.2%, Proof 86.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Corresponding CoT/ProgLM baselines for these models reported in Table 6 and generally show SATLM better than ProgLM/CoT except for some GSM settings.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Across models, SATLM tends to outperform ProgLM and CoT on arithmetic and logical datasets except occasional GSM cases; degree of improvement depends on base LLM capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Weaker LLMs (code-davinci-001) yield poor parsing and thus low SATLM performance; effectiveness depends on LLM ability to produce correct declarative specs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors conclude SATLM generalizes across different LLMs but benefits more from stronger LLMs (parsing fidelity correlates with downstream solver success).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e3542.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProgLM-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-Aided Language Models (ProgLM) baseline (e.g., PAL and related executor-augmented LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach where the LLM generates imperative programs (e.g., Python) that implement a solving procedure, then executes them with a program interpreter; used for comparison across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM prompted to generate imperative programs / reasoning chains which are executed by an interpreter (Python); this is the ProgLM baseline class (PAL/Large prior works).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple (GSM, GSM-Sys, Algebra, CLUTRR, ProofWriter, ColoredObject, StructuredRegex, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks where LLMs produce executable programs implementing a reasoning plan; execution performed by a program interpreter to reduce execution errors.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Program generation (imperative) + symbolic executor (Python interpreter); few-shot prompts (PAL-style).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varies by task (examples with code-davinci-002 greedy): GSM-Sys 43.4%, GSM 72.7%, Algebra 52.3%, Clutrr 58.9%, Proof 83.7%, Color 95.1%, StructuredRegex 39.1%. See Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared directly in paper to SATLM and CoT; used as a primary baseline for programmatic executor methods.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SATLM often substantially outperforms ProgLM (e.g., GSM-Sys +26.0 pp greedy, Algebra +25.2 pp greedy, Clutrr +9.4 pp), but ProgLM is sometimes competitive (GSM greedy ProgLM 72.7% vs SATLM 71.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>ProgLM tends to make planning errors (producing incorrect reasoning chains) in problems requiring sophisticated planning (e.g., GSM-Sys); interpreter only prevents execution errors, not planning errors; cannot easily represent proof strategies required for LSAT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper's error analysis (Table 5) shows ProgLM answers more questions but with lower selective accuracy; ProgLM error types include planning mistakes that SATLM's solver avoids.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3542.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e3542.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline family where LLMs are prompted to generate multi-step natural language reasoning (chain-of-thought) before answering; used in many prior works and compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (main comparisons), other LLMs also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Few-shot chain-of-thought prompting to elicit intermediate reasoning steps in natural language; executed purely by the LLM (no external solver).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple (GSM, Algebra, BoardGameQA, LSAT, CLUTRR, ProofWriter, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language multi-step reasoning for arithmetic/symbolic/logical tasks; sometimes insufficient for tasks requiring planning/search.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Chain-of-thought few-shot prompting; self-consistency decoding also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varies by task (examples with code-davinci-002 greedy): GSM-Sys 46.5%, GSM 62.7%, Algebra 53.6%, LSAT 23.5%, BoardGameQA 60.7%, Clutrr 40.8%, ProofWriter 80.1%, Color 86.3%. See Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Used as baseline in comparisons; ProgLM and SATLM frequently outperform CoT on tasks requiring planning/search.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SATLM outperforms CoT across nearly all tasks (often by large margins), demonstrating that declarative parsing + solver addresses planning errors CoT often makes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>CoT is prone to both planning and execution errors when problems require search or long reasoning chains; struggles on tasks requiring formal deductive proofs (e.g., LSAT).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The paper shows CoT combined with declarative parsing (SAT_CoTSolver variant) improves over vanilla CoT, indicating the parsing style matters; but full gains require symbolic solver.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Satisfiability-Aided Language Models Using Declarative Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pal: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Faithful chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Solving math word problems by combining language models with symbolic solvers <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3542",
    "paper_id": "paper-f27f6d1d521d189e78f5623098ced0deea613d33",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "SATLM-GSM-Sys-cd002",
            "name_full": "Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on GSM-Sys",
            "brief_description": "Declarative prompting with an LLM (code-davinci-002) to produce a SAT-style formal specification of arithmetic problems, solved by an SMT/SAT solver (Z3); evaluated on GSM-Sys (hard GSM subset with systems of equations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "A state-of-the-art OpenAI code-specialized LLM used by the authors for code-adjacent reasoning and prompting experiments (no architecture/parameter count specified in paper).",
            "model_size": null,
            "reasoning_task_name": "GSM-Sys",
            "reasoning_task_description": "A subset of GSM filtered to examples whose human explanations involve systems of equations; arithmetic reasoning requiring planning over equations and solving linear systems.",
            "method_or_intervention": "Declarative prompting to produce logical/formulaic specifications (z3py-like syntax) combined with an off-the-shelf SAT/SMT solver (Z3) to perform planning and execution; self-consistency decoding also evaluated.",
            "performance": "Greedy decoding accuracy 69.4%; Self-consistency decoding accuracy 80.9% (accuracy %).",
            "baseline_performance": "code-davinci-002 baselines — CoT: greedy 46.5% / self-consistency 56.1%; ProgLM (program-aided LM): greedy 43.4% / self-consistency 53.4%.",
            "improvement_over_baseline": "SATLM improves over ProgLM by ~26.0 percentage points (greedy) and ~27.5 pp (self-consistency); paper highlights a 23% absolute improvement on a challenging GSM arithmetic subset in text (statistically significant, p &lt; 0.05).",
            "limitations_or_failures": "SATLM can abstain when the solver returns UNSAT or AMBIG due to parsing errors; inherits SAT solver limitations (computational cost for quantifiers/nonlinear arithmetic, expressiveness limits); some parser-induced UNSAT/AMBIG cases exist (examples in Appendix G).",
            "ablation_or_analysis": "Ablation (Table 2) shows SAT_SYMSOLVER (full approach) 69.4% vs SAT_CoTSOLVER 54.5% vs SAT_NoSolver 26.6%, indicating the symbolic solver contributes substantially; error analysis (40 cases) shows CoT-solver failures are mostly planning errors (72.5%) on GSM-Sys (Table 3).",
            "uuid": "e3542.0",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SATLM-GSM-cd002",
            "name_full": "Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on GSM",
            "brief_description": "Same SATLM pipeline applied to the GSM arithmetic reasoning benchmark (general math word problems).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "Code-specialized LLM used for producing declarative specifications; no size reported in paper.",
            "model_size": null,
            "reasoning_task_name": "GSM",
            "reasoning_task_description": "General math word problem benchmark (GSM) requiring multi-step arithmetic reasoning and equation solving.",
            "method_or_intervention": "Declarative prompting + Z3 SMT solver; self-consistency decoding.",
            "performance": "Greedy decoding accuracy 71.8%; Self-consistency decoding accuracy 84.8%.",
            "baseline_performance": "code-davinci-002 baselines — CoT: greedy 62.7% / self-consistency 77.3%; ProgLM: greedy 72.7% / self-consistency 82.4%.",
            "improvement_over_baseline": "With greedy decoding SATLM is nearly on par with ProgLM (71.8% vs 72.7%); with self-consistency SATLM outperforms ProgLM by 2.4 pp (84.8% vs 82.4%). The paper notes SATLM slightly lags ProgLM with greedy decoding for GSM but surpasses it under self-consistency.",
            "limitations_or_failures": "Paper notes SATLM may be less effective on tasks whose NL descriptions align more naturally with imperative programs (GSM is such a case); solver-based abstention reduces coverage but increases selective accuracy.",
            "ablation_or_analysis": "Analysis attributes the GSM result to coverage/selective-accuracy tradeoffs: SATLM has slightly lower coverage but higher selective accuracy; self-consistency increases coverage improving overall accuracy.",
            "uuid": "e3542.1",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SATLM-Algebra-cd002",
            "name_full": "Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on Algebra dataset",
            "brief_description": "Applying SATLM to an Algebra dataset extracted from textbooks (math problems), parsing to equations and solving via SMT solver.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "Code-specialized LLM producing z3py-like declarative specs for algebraic problems.",
            "model_size": null,
            "reasoning_task_name": "Algebra (He-Yueya et al., 2023 dataset)",
            "reasoning_task_description": "Algebra word problems from textbooks, requiring equation formation and solving; more challenging than GSM according to authors.",
            "method_or_intervention": "Declarative prompting + Z3 solver.",
            "performance": "Greedy decoding accuracy 77.5%; Self-consistency decoding accuracy 90.9%.",
            "baseline_performance": "code-davinci-002 baselines — CoT: greedy 53.6% / self-consistency 64.9%; ProgLM: greedy 52.3% / self-consistency 57.7%. MathSym (concurrent work) reported 76.3% on Algebra (not directly comparable due to exemplar differences).",
            "improvement_over_baseline": "SATLM outperforms both CoT and ProgLM by &gt;20 pp (greedy) and substantially under self-consistency (~30+ pp vs ProgLM).",
            "limitations_or_failures": "Same solver and parsing limitations apply; Algebra is more challenging but SATLM yields large gains.",
            "ablation_or_analysis": "Paper notes domain-agnostic SAT formulation contrasts with math-specific solvers (e.g., PySym); ablations (Table 2) show solver provides key gains.",
            "uuid": "e3542.2",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SATLM-LSAT-cd002",
            "name_full": "Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on LSAT",
            "brief_description": "Declarative parsing into logical formulas and using an automated theorem prover to solve LSAT-style analytical reasoning problems (logical deduction, sometimes requiring proof-by-contradiction).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "Code-specialized LLM used to translate NL into first-order logic formulas; solver handles deductive planning and execution.",
            "model_size": null,
            "reasoning_task_name": "LSAT",
            "reasoning_task_description": "Analytical/logical reasoning benchmark (LSAT-style problems) requiring deduction and proof strategies in natural language.",
            "method_or_intervention": "Declarative prompting + Z3 SMT solver (first-order reasoning); few-shot exemplars; self-consistency decoding.",
            "performance": "Greedy decoding accuracy 35.0%; Self-consistency decoding accuracy 37.4% (accuracy %).",
            "baseline_performance": "CoT (code-davinci-002) greedy 23.5% / self-consistency 23.1%; ProgLM not applicable (authors did not run ProgLM for LSAT). Previous SoTA reported 30.9% (Zhong et al., 2022).",
            "improvement_over_baseline": "SATLM improves absolute accuracy over prior CoT baselines by ~11.5 pp (greedy) and sets a new SoTA vs reported prior 30.9% -&gt; 37.4% (self-consistency).",
            "limitations_or_failures": "Authors note Python-based ProgLM not suitable for logical inference required by LSAT (lack of native logic operators/proofs); solver limitations (expressiveness, quantifiers) remain relevant.",
            "ablation_or_analysis": "Authors attempted multiple CoT prompts for LSAT that yielded ~20% accuracy; SATLM's declarative + solver approach yields substantial improvement, indicating solver aids complex deductive planning.",
            "uuid": "e3542.3",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SATLM-BoardGameQA-cd002",
            "name_full": "Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on BoardGameQA",
            "brief_description": "Declarative parsing plus SAT solving applied to BoardGameQA, a benchmark with contradictory information and multi-step logical reasoning requiring background/common-sense rules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "LLM produces z3-style logical constraints; solver finds satisfying assignments under game rules and background facts.",
            "model_size": null,
            "reasoning_task_name": "BoardGameQA",
            "reasoning_task_description": "Natural language reasoning dataset about board games with contradictory information; requires reasoning over rules and often implicit commonsense/prior knowledge.",
            "method_or_intervention": "Declarative prompting + Z3 solver; few-shot prompts; self-consistency decoding.",
            "performance": "Greedy decoding aggregated accuracy 79.4% (depth breakdown: depth1 87.6%, depth2 81.7%, depth3 69.0%); Self-consistency aggregated accuracy 80.8%.",
            "baseline_performance": "code-davinci-002 CoT: greedy aggregated 60.1% / self-consistency 62.8%. ProgLM not reported for BoardGameQA.",
            "improvement_over_baseline": "SATLM yields large gains vs CoT: +19.3 pp aggregated (greedy) and sets new SoTA (80.7% reported in Table 1 self-consistency).",
            "limitations_or_failures": "Performance decreases with reasoning depth; requires LLM to incorporate implicit background knowledge during parsing (authors observe LLMs can do some commonsense during parsing).",
            "ablation_or_analysis": "Authors note parsing step benefits from LLM's background knowledge; declarative prompting improves fidelity of translation to constraints (examples in Figure 4).",
            "uuid": "e3542.4",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SATLM-Clutrr-cd002",
            "name_full": "Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on CLUTRR",
            "brief_description": "Declarative parsing to relational constraints and SAT solving for family/kinship reasoning (CLUTRR diagnostic benchmark), testing generalization to longer reasoning chains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "Code-specialized LLM that outputs declarative relations (z3-like), solver computes transitive/deductive relations.",
            "model_size": null,
            "reasoning_task_name": "CLUTRR",
            "reasoning_task_description": "Diagnostic inductive reasoning benchmark about family relations requiring multi-step inference; training exemplars have up to 3 steps, test up to 10.",
            "method_or_intervention": "Declarative prompting + Z3 solver; few-shot exemplars emphasizing 2-3 step reasoning to test extrapolation.",
            "performance": "Greedy decoding accuracy 68.3%; Self-consistency decoding accuracy 80.1%.",
            "baseline_performance": "code-davinci-002 baselines — CoT: greedy 40.8% / self-consistency 45.7%; ProgLM: greedy 58.9% / self-consistency 71.9%.",
            "improvement_over_baseline": "SATLM outperforms ProgLM by ~9.4 pp (greedy) and ~8.2 pp (self-consistency); large improvement over CoT (~27.5 pp greedy).",
            "limitations_or_failures": "Some UNSAT / AMBIG cases stem from misparsing ambiguous or conflicting statements; solver feedback (UNSAT/AMBIG) used to abstain, improving selective accuracy.",
            "ablation_or_analysis": "Table 3 shows CoT-solver planning errors ~47.5% and execution errors ~52.5% on CLUTRR (distribution differs by dataset); Table 5 shows SATLM selective accuracy on CLUTRR 89.9% vs ProgLM 73.7%.",
            "uuid": "e3542.5",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SATLM-ProofWriter-cd002",
            "name_full": "Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on ProofWriter (depth-5)",
            "brief_description": "Using declarative prompting to produce formal rules and queries and verifying proofs with a solver on ProofWriter depth-5 (challenging deductive proofs).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "LLM used to translate NL rules into formal logic; solver performs deductive proof search and returns model/proof.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter (depth-5)",
            "reasoning_task_description": "Benchmark requiring the generation of implications and multi-step proofs from natural language rules; depth-5 denotes proofs of length 5.",
            "method_or_intervention": "Declarative prompting + Z3 SMT solver to find models and produce proof traces.",
            "performance": "Greedy decoding accuracy 99.7%; Self-consistency decoding accuracy 99.7%.",
            "baseline_performance": "CoT (code-davinci-002) greedy 80.1% / self-consistency 88.7%; ProgLM greedy 83.7% / self-consistency 91.2%.",
            "improvement_over_baseline": "SATLM substantially outperforms CoT and ProgLM (greedy +19.6 pp vs ProgLM), reaching near-perfect accuracy on these depth-5 examples.",
            "limitations_or_failures": "Success contingent on correct parsing into formal rules; solver timeouts/errors possible on malformed specs; paper shows SATLM makes fewer but more accurate predictions (selective behavior).",
            "ablation_or_analysis": "SAT solver produces proofs (e.g., resolution refutations) guaranteeing correctness when parsing is correct; demonstrates solver removes planning/execution errors present in CoT/ProgLM.",
            "uuid": "e3542.6",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SATLM-Color-cd002",
            "name_full": "Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on ColoredObject (BIG-bench)",
            "brief_description": "Applying SATLM to a symbolic reasoning task abstracted as constrained list element finding (colored object selection).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "LLM used to produce declarative constraints over list elements and attributes; solver computes satisfying assignments.",
            "model_size": null,
            "reasoning_task_name": "ColoredObject (BIG-bench)",
            "reasoning_task_description": "Symbolic reasoning over lists/objects to find elements satisfying constraints (logic over attributes).",
            "method_or_intervention": "Declarative prompting + Z3 solver.",
            "performance": "Greedy decoding accuracy 97.7%; Self-consistency decoding accuracy 99.4%.",
            "baseline_performance": "CoT greedy 86.3% / self-consistency 90.6%; ProgLM greedy 95.1% / self-consistency 98.0%.",
            "improvement_over_baseline": "SATLM modestly improves over ProgLM (greedy +2.6 pp, self-consistency +1.4 pp) and substantially over CoT.",
            "limitations_or_failures": "Typical parser-induced UNSAT/AMBIG cases and solver expressiveness/compute limits remain applicable.",
            "ablation_or_analysis": "Authors report high performance reflecting tasks that map cleanly to declarative constraints; declarative prompting produces higher-likelihood outputs for such tasks.",
            "uuid": "e3542.7",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SATLM-Regex-cd002",
            "name_full": "Satisfiability-Aided Language Modeling (SATLM) using code-davinci-002 on StructuredRegex / STREGEX",
            "brief_description": "SATLM parses NL regex descriptions into constraints over the surface form of the target regex (string constraints), with solver confirming candidates against I/O examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_description": "LLM translates multimodal NL + I/O examples into declarative constraints over regex surface strings; solver and example filtering used during self-consistency.",
            "model_size": null,
            "reasoning_task_name": "StructuredRegex (STREGEX)",
            "reasoning_task_description": "Regex synthesis from NL descriptions and I/O examples; requires synthesizing regex that fits provided specifications.",
            "method_or_intervention": "Declarative prompting to constraints over strings + solver; self-consistency combined with filtering by I/O examples (per prior work practice).",
            "performance": "Greedy decoding accuracy 41.0%; Self-consistency decoding accuracy 59.7% (this surpasses prior SoTA 55.6% reported for related work).",
            "baseline_performance": "ProgLM (direct NL-&gt;regex) greedy 39.1% / self-consistency 56.5%; No CoT baseline reported.",
            "improvement_over_baseline": "SATLM modestly outperforms ProgLM (greedy +1.9 pp, self-consistency +3.2 pp) and achieves a new SoTA with self-consistency in the paper's setup.",
            "limitations_or_failures": "Regex dataset is multimodal and uses I/O example filtering during self-consistency; SATLM relies on correct encoding of examples and may produce AMBIG/UNSAT when constraints omitted or misparsed.",
            "ablation_or_analysis": "Self-consistency plus I/O filtering improves coverage/accuracy; declarative encoding yields higher translation fidelity than imperative direct-program generation for some examples.",
            "uuid": "e3542.8",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SATLM-cross-LLMs",
            "name_full": "SATLM evaluated across alternative LLMs (gpt-3.5-turbo, text-davinci-003, code-davinci-001)",
            "brief_description": "Assessing SATLM's generality by swapping the LLM used for declarative parsing: gpt-3.5-turbo (0613), text-davinci-003, and code-davinci-001 were tested on multiple tasks showing similar trends.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo / text-davinci-003 / code-davinci-001",
            "model_description": "Alternate LLMs (chat-optimized gpt-3.5-turbo v0613, text-davinci-003 NL-focused, and older code-davinci-001) used to produce declarative specifications fed to the same solver.",
            "model_size": null,
            "reasoning_task_name": "Multiple (GSM-Sys, GSM, LSAT, Clutrr, ProofWriter)",
            "reasoning_task_description": "Same benchmarks as main experiments; tests generalizability across LLM capabilities.",
            "method_or_intervention": "Declarative prompting + Z3 solver; greedy decoding reported.",
            "performance": "gpt-3.5-turbo SATLM greedy: GSM-Sys 63.4%, GSM 76.4%, LSAT 30.0%, Clutrr 50.6%, Proof 96.4%. text-davinci-003 SATLM greedy: GSM-Sys 63.6%, GSM 70.3%, LSAT 30.4%, Clutrr 58.2%, Proof 99.7%. code-davinci-001 SATLM greedy: GSM-Sys 16.5%, GSM 34.2%, LSAT 19.6%, Clutrr 30.2%, Proof 86.6%.",
            "baseline_performance": "Corresponding CoT/ProgLM baselines for these models reported in Table 6 and generally show SATLM better than ProgLM/CoT except for some GSM settings.",
            "improvement_over_baseline": "Across models, SATLM tends to outperform ProgLM and CoT on arithmetic and logical datasets except occasional GSM cases; degree of improvement depends on base LLM capability.",
            "limitations_or_failures": "Weaker LLMs (code-davinci-001) yield poor parsing and thus low SATLM performance; effectiveness depends on LLM ability to produce correct declarative specs.",
            "ablation_or_analysis": "Authors conclude SATLM generalizes across different LLMs but benefits more from stronger LLMs (parsing fidelity correlates with downstream solver success).",
            "uuid": "e3542.9",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ProgLM-baseline",
            "name_full": "Program-Aided Language Models (ProgLM) baseline (e.g., PAL and related executor-augmented LMs)",
            "brief_description": "Baseline approach where the LLM generates imperative programs (e.g., Python) that implement a solving procedure, then executes them with a program interpreter; used for comparison across tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (as used in experiments)",
            "model_description": "LLM prompted to generate imperative programs / reasoning chains which are executed by an interpreter (Python); this is the ProgLM baseline class (PAL/Large prior works).",
            "model_size": null,
            "reasoning_task_name": "Multiple (GSM, GSM-Sys, Algebra, CLUTRR, ProofWriter, ColoredObject, StructuredRegex, etc.)",
            "reasoning_task_description": "Tasks where LLMs produce executable programs implementing a reasoning plan; execution performed by a program interpreter to reduce execution errors.",
            "method_or_intervention": "Program generation (imperative) + symbolic executor (Python interpreter); few-shot prompts (PAL-style).",
            "performance": "Varies by task (examples with code-davinci-002 greedy): GSM-Sys 43.4%, GSM 72.7%, Algebra 52.3%, Clutrr 58.9%, Proof 83.7%, Color 95.1%, StructuredRegex 39.1%. See Table 1.",
            "baseline_performance": "Compared directly in paper to SATLM and CoT; used as a primary baseline for programmatic executor methods.",
            "improvement_over_baseline": "SATLM often substantially outperforms ProgLM (e.g., GSM-Sys +26.0 pp greedy, Algebra +25.2 pp greedy, Clutrr +9.4 pp), but ProgLM is sometimes competitive (GSM greedy ProgLM 72.7% vs SATLM 71.8%).",
            "limitations_or_failures": "ProgLM tends to make planning errors (producing incorrect reasoning chains) in problems requiring sophisticated planning (e.g., GSM-Sys); interpreter only prevents execution errors, not planning errors; cannot easily represent proof strategies required for LSAT.",
            "ablation_or_analysis": "Paper's error analysis (Table 5) shows ProgLM answers more questions but with lower selective accuracy; ProgLM error types include planning mistakes that SATLM's solver avoids.",
            "uuid": "e3542.10",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CoT-baseline",
            "name_full": "Chain-of-Thought (CoT) prompting baseline",
            "brief_description": "Baseline family where LLMs are prompted to generate multi-step natural language reasoning (chain-of-thought) before answering; used in many prior works and compared here.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (main comparisons), other LLMs also evaluated",
            "model_description": "Few-shot chain-of-thought prompting to elicit intermediate reasoning steps in natural language; executed purely by the LLM (no external solver).",
            "model_size": null,
            "reasoning_task_name": "Multiple (GSM, Algebra, BoardGameQA, LSAT, CLUTRR, ProofWriter, etc.)",
            "reasoning_task_description": "Natural-language multi-step reasoning for arithmetic/symbolic/logical tasks; sometimes insufficient for tasks requiring planning/search.",
            "method_or_intervention": "Chain-of-thought few-shot prompting; self-consistency decoding also evaluated.",
            "performance": "Varies by task (examples with code-davinci-002 greedy): GSM-Sys 46.5%, GSM 62.7%, Algebra 53.6%, LSAT 23.5%, BoardGameQA 60.7%, Clutrr 40.8%, ProofWriter 80.1%, Color 86.3%. See Table 1.",
            "baseline_performance": "Used as baseline in comparisons; ProgLM and SATLM frequently outperform CoT on tasks requiring planning/search.",
            "improvement_over_baseline": "SATLM outperforms CoT across nearly all tasks (often by large margins), demonstrating that declarative parsing + solver addresses planning errors CoT often makes.",
            "limitations_or_failures": "CoT is prone to both planning and execution errors when problems require search or long reasoning chains; struggles on tasks requiring formal deductive proofs (e.g., LSAT).",
            "ablation_or_analysis": "The paper shows CoT combined with declarative parsing (SAT_CoTSolver variant) improves over vanilla CoT, indicating the parsing style matters; but full gains require symbolic solver.",
            "uuid": "e3542.11",
            "source_info": {
                "paper_title": "Satisfiability-Aided Language Models Using Declarative Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pal: Program-aided language models",
            "rating": 2
        },
        {
            "paper_title": "Faithful chain-of-thought reasoning",
            "rating": 2
        },
        {
            "paper_title": "Solving math word problems by combining language models with symbolic solvers",
            "rating": 2
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 1
        }
    ],
    "cost": 0.0223445,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SATLM: Satisfiability-Aided Language Models Using Declarative Prompting</h1>
<p>Xi Ye Qiaochu Chen Isil Dillig Greg Durrett<br>Department of Computer Science<br>The University of Texas at Austin<br>{xiye, qchen, isil, gdurrett}@cs.utexas.edu</p>
<h4>Abstract</h4>
<p>Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving problems that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling (SATLM) approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it out of the description more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness of the answer with respect to the parsed specification and avoid planning errors in the solving process. We evaluate SATLM on 8 different datasets and show that it consistently outperforms program-aided LMs in the imperative paradigm. In particular, SATLM outperforms program-aided LMs by $23 \%$ on a challenging subset of the GSM arithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and BOARDGAMEQA, surpassing previous models that are trained on the respective training sets. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Using large language models (LLMs) to perform complex reasoning has been a central thrust of recent research (Brown et al., 2020; Chowdhery et al., 2022; Rae et al., 2021; Zhang et al., 2022b). Techniques like scratchpads (Nye et al., 2021) or chain-of-thought prompting (CoT) (Wei et al., 2022c) enable LLMs to follow a sequence of reasoning steps before making a prediction. This paradigm is effective on various multi-step reasoning tasks, especially those with fixed forward reasoning procedures (Wei et al., 2022c), e.g., concatenating the last letters of several words. However, CoT prompting can fall short when scaling to problems that involve intensive computation (Gao et al., 2023) or long sequences of reasoning steps (Creswell et al., 2023; Saparov and He, 2023; Ribeiro et al., 2023).</p>
<p>Solving a complex reasoning problem involves three conceptual components: parsing a natural language description into a representation of the problem, deriving a plan to solve the problem, and executing that plan to obtain an answer. Recent work on improving CoT prompting focuses on fixing execution errors by augmenting LLMs with symbolic executors such as a Python interpreter, which</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of our Satisfiability-aided Language Modeling approach (right). We first parse an NL input into a declarative task specification (a set of logical constraints) using prompting (Section 3.1), then use a SAT solver to solve the problem (Section 3.2). The chain-of-thought strategy in prior work (left) yields imperative reasoning processes.</p>
<p>leads to improved performance on arithmetic and symbolic reasoning tasks (Gao et al., 2023; Chen et al., 2022; Lyu et al., 2023). However, CoT prompting (Wei et al., 2022c; Nye et al., 2021) and its executor-augmented successors (Gao et al., 2023; Chen et al., 2022; Lyu et al., 2023) are oriented towards <em>imperative</em> solving procedures: a CoT or a program specifies the reasoning procedure as chained steps (Wei et al., 2022c; Gao et al., 2023) in the order of execution. While this is effective for problems whose natural language already provides a suitably clear "plan" for the reasoning, it only leads to limited success for reasoning problems like in Figure 1 that do not outline such a plan (Ribeiro et al., 2023). These problems often state a set of premises and constraints and ask questions that require sophisticated planning to deductively reason over the inputs, which is still challenging even for modern LLMs (Valmeekam et al., 2022).</p>
<p>Our work tackles both execution errors and, more importantly, <em>planning errors</em>. We propose SATisfiability-aided Language Modeling (SATLM) using declarative prompting. The core idea is to cast a natural language (NL) reasoning problem as a satisfiability (SAT for short) problem. As shown in Figure 1 (right), given a problem in NL, we prompt an LLM to parse it into a SAT problem specification which consists of a set of logical formulas, then obtain the solution by invoking a SAT solver. The LLM is specialized towards understanding the preconditions stated in the problem, while the solver is leveraged to plan out the reasoning procedure. In addition, the solver guarantees the correctness of execution, similar to the interpreter used in program-aided LMs (ProgLM).</p>
<p>We evaluate our approach on 8 datasets spanning 4 tasks, including arithmetic reasoning, logical reasoning, symbolic reasoning, and a regex synthesis task. Our SATLM consistently outperforms CoT and ProgLM across all datasets, usually by a large margin. On GSM-Sys, SATLM outperforms ProgLM by a 23%; on GSM, SATLM achieves 84.8% with self-consistency decoding using few-shot prompting, equaling past work that uses the full training set and the same LLM (Li et al., 2022b; Ni et al., 2023). SATLM also sets a new SoTA on LSAT (Zhong et al., 2022), BOARDGAMEQA (Kazemi et al., 2023), and STRUCTUREDREGEX (Ye et al., 2020).</p>
<p>Our analysis illustrates why the combination of SAT solver and declarative prompting is so effective. We find (1) program-aided LMs often make planning errors (e.g., manipulating equations incorrectly), which can be remedied by the SAT solver. (2) Forcing LLMs to explicitly state a declarative</p>
<p><sup>2</sup>Here, we use SAT solver to refer to any automated reasoning tool for checking the satisfiability of formulas in formal logic. Hence, "SAT solver" in this paper also includes first-order theorem provers and SMT solvers.</p>
<p>specification can even improve vanilla CoT prompting. (3) Our SATLM approach can abstain from making uncertain predictions if it parses a problem into an unsatisfiable or ambiguous specification, giving it even higher accuracy in the selective prediction setting (El-Yaniv and Wiener, 2010).</p>
<h1>2 Overview</h1>
<p>This work addresses the challenge of using LLMs to solve NL reasoning tasks. At a high level, an NL reasoning task is a natural language description of a collection of facts $\Phi$ (such as propositions or constraints) about some objects and a question $Q$ related to these objects. The goal of the reasoning task is to find an answer to $Q$ that can be deduced from the information provided in $\Phi$.
We conceptualize the general procedure for solving NL reasoning tasks in three steps: parsing, planning, and execution. We are given natural language input $x_{\text {test }}=(N L(\Phi), N L(Q))$ which describes both $\Phi$ and $Q$. Our first step is to parse this natural language into a predicted task specification $(\hat{\Phi}, \hat{Q})$, which is a formal description of the facts and the query.
Given $(\hat{\Phi}, \hat{Q})$, the planning step then involves determining a sequence of reasoning steps $\left[r_{1}, \ldots, r_{n}\right]$ beginning with the task specification and ending with the answer to the question. Each step involves invoking a function (e.g., arithmetic operator or logical operator) that produces intermediate results which can be utilized in subsequent steps. A plan can be formulated by an LLM with CoT prompting or by a symbolic solver as in our work here. Finally, we execute the plan systematically with either a symbolic executor (our method) or an LLM, returning the output of the last step, $r_{n}$, as the answer.
Our solution approaches the problem using exactly these three steps.
Parsing into declarative specification We prompt an LLM to generate a specification $s_{\text {test }}$ for $x_{\text {test }}$. Note that the translation from this description into the specification is not straightforward and cannot be done in a rule-based way for most tasks; Figure 4 shows some particularly complex examples involving commonsense reasoning. The specification $s_{\text {test }}$ is a sequence of interleaved NL statements and logical formulas (LF): $s_{\text {test }}=\left[z_{1}, \ldots, z_{n}\right]$ and $z_{i} \in \Sigma_{N L} \cup \Sigma_{L F}$, where $\Sigma_{N L}$ and $\Sigma_{L F}$ denote the space of natural language and logical formulas, respectively. We derive the formal specification $(\hat{\Phi}, \hat{Q})$ by taking all the $z_{i}$ in $\Sigma_{L F}$ from $s_{\text {test }}$. An example of the specification is presented on the right of Figure 1. Our specification is declarative since we do not explicitly generate the $r_{i}$ from the LLM at this stage.</p>
<p>Planning and execution with a SAT solver Given the predicted formal specification $(\hat{\Phi}, \hat{Q})$, we wish to derive the final answer of the query $\hat{Q}$ from it. We say that a solution $a$ is correct if $\hat{\Phi}$ logically entails $\hat{Q}=a$, denoted as $\hat{\Phi} \models \hat{Q}=a$. The key insight behind our work is to offload both the planning and execution steps to a SAT solver. Specifically, we use a SAT solver to find a satisfying assignment for $a$ in the formula:</p>
<p>$$
\forall V .(\hat{\Phi} \Rightarrow \hat{Q}=a)
$$</p>
<p>where $V$ denotes the set of all variables used in $\hat{\Phi}$ and $\hat{Q} \in V$ is a variable that corresponds to the solution. Note that the only free variable in this formula is $a$; hence, the assignment to $a$ returned by the solver is the final answer to the reasoning problem.
The approach outlined above has two important strengths. First, because the SAT solver is sound (i.e., any assignment it produces satisfies the formula), the solution is correct by construction. Thus, assuming that the parsing is correct and $\hat{\Phi}$ and $\hat{Q}$ match $\Phi$ and $Q$, we have a proof that the solution is indeed correct. Second, the planning step is done internally to the solver, and the chain of reasoning steps $\left[r_{1}, \ldots, r_{n}\right]$ can be obtained by asking the solver to produce a proof of the validity of the formula $\hat{\Phi} \Rightarrow \hat{Q}=a^{<em>}$ where $a^{</em>}$ is the assignment produced by the SAT solver. All solvers we consider can produce such a proof of validity (e.g., in the form of a resolution refutation (Davis and Putnam, 1960)).</p>
<p>Comparison with prior work Prior approaches to NL-based reasoning with LLMs can also be framed in the parse-plan-execute framework proposed above. In particular, the chain-of-thought paradigm (Nye et al., 2021; Wei et al., 2022c) uses LLMs to perform each of the three steps. Programaided language models (Gao et al., 2023; Chen et al., 2022; Lyu et al., 2023) combine the parsing and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Exemplar specifications for arithmetic reasoning problems generated by different approaches. CoT makes errors when parsing an equation; ProgLM produces an incorrect reasoning chain (both errors are highlighted in red). By only using the LLMs to generate declarative specifications and relying on a solver to handle the reasoning, SatLM generates the correct answer.
planning steps to use an LLM to derive a program that corresponds to the plan. ${ }^{3}$ The final execution step is then performed by using the interpreter of the underlying programming language to derive the final answer. In contrast to these approaches, our work uses an LLM only to perform the parsing step, which is an easier problem for LLMs than planning.
We show a concrete example comparing CoT and ProgLM with our approach in Figure 2. CoT performs all three steps with the LLM. For instance, "Alex has caught $X+5$ " in the output corresponds to "Alex has caught 5 more than Stan" in the NL input (parsing). Later, CoT decides how to solve for the variable $X$ with "Combining like terms ..." (planning). At the same time, it also derives the equation " $5 X=334$ " directly in its generation (execution). However, CoT incorrectly uses the same $X$ in the equation " $X+5$ " and " $4 X-13$ ", when it is supposed to be different. (Note that $4 X-13$ would be correct if Stan and Adelwolfe's roles in the corresponding NL clause were reversed.) By allowing the LLM to focus only on translation, we find a lower incidence of this kind of error, in addition to eliminating planning errors. Notably, planning errors are not addressed by ProgLM, which does not use programmatic manipulation at this stage. Different from ProgLM, SatLM only parses the information provided in the input question, passes the parsed formulas to a solver for both planning and execution, and obtains the correct result.</p>
<h1>3 SAT-Aided Language Models using Declarative Prompting</h1>
<h3>3.1 Declarative Prompting</h3>
<p>We use few-shot prompting to generate the specification $s_{\text {test }}$ for the test input $x_{\text {test }}$. Specifically, we include few-shot demonstrations $\left(x_{i}, s_{i}\right)<em _test="{test" _text="\text">{i=1}^{k}$ in the prompt, append test input $x</em>\right)$.
We show an example specification for a logical reasoning task in Figure 1, and an example specification for an arithmetic reasoning task in Figure 2. Observe that in both examples, our SAT formulas (i.e., the logical formulas of $\left[z_{1}, \ldots, z_{n}\right]$ in $\Sigma_{L F}$ ) are written as code following Python syntax, while the natural language in $\Sigma_{N L}$ is written using comment syntax. We found that including the language here as comments was useful to improve the fidelity of the translation. Our declarative prompts also use meaningful variable names and descriptive comments following the style of prompts in prior work (Gao et al., 2023; Lyu et al., 2023). Finally, we use Python rather than a specialized DSL to be more congruent with our models' pretraining data (Ouyang et al., 2022; Chen et al., 2021). See Appendix F for more details on the SAT specification.}}$ after the prompt, and let the LLM complete the specification for $x_{\text {test }}$, i.e., $s_{\text {test }} \sim p\left(x_{\text {test }} \mid x_{1}, s_{1}, \ldots, x_{k}, s_{k</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.2 Solving with a SAT Solver</h1>
<p>SAT problem A SAT problem is a triple $\mathcal{P}=(\Phi, \mathcal{T}, Q)$ where $\Phi$ is a set of first-order logic formulas in some theory $\mathcal{T}^{4}$ and $Q$ is the query of interest. We use $\operatorname{Variable}(\mathcal{P})$ to denote the free variables in $\Phi . Q$ contains only variables in $\operatorname{Variable}(\mathcal{P})$. An example SAT problem is $\mathcal{P}=\left({x+y=3, x-y=1}, \mathcal{T}<em _mathbb_Z="\mathbb{Z">{E} \cup \mathcal{T}</em>}}, x-2\right)$, where $\mathcal{T<em _mathbb_Z="\mathbb{Z">{E} \cup \mathcal{T}</em>$ indicates that only equality and linear arithmetic operations on integers are allowed in the formulas.
Many NL reasoning tasks in the literature can be formulated as SAT problems and solved using an off-the-shelf solver. For arithmetic reasoning, the SAT formulas $\Phi$ are equations encoding the relationships between variables, and $t$ specifies the target variable asked in the question (see Figure 1). For logical reasoning, $\Phi$ encodes preconditions and $t$ specifies the target statement posed by the question. We also show that symbolic reasoning, regex synthesis, and other problems involving reasoning over arrays or strings can be handled in this framework.
Unlike prior work such as Faithful CoT (Lyu et al., 2023) that uses task-specific formulations and task-specific solvers for different problem types, all the tasks in this paper are formulated as general SAT instances that can be solved by a single solver (as described later in this section).}</p>
<p>Parsing NL to a SAT problem Recall that we obtain a specification $s_{\text {test }}$ from a test NL task $x_{\text {test }}$. To derive the SAT problem $\mathcal{P}<em _test="{test" _text="\text">{\text {test }}=\left(\hat{\Phi}</em>}}, \mathcal{T<em _test="{test" _text="\text">{\text {test }}, \hat{Q}</em>}}\right)$ from $s_{\text {test }}$, we extract the constraints $\hat{\Phi<em _test="{test" _text="\text">{\text {test }}$ and the target expression $\hat{Q}</em>}}$ (marked by solve in our prompt) by taking all the $z_{i}$ in $\Sigma_{L F}$ of $s_{\text {test }}$. We identify the theory $\mathcal{T<em _test="{test" _text="\text">{\text {test }}$ by analyzing the formulas in $\hat{\Phi}</em>$.}</p>
<p>Solving the SAT problem Given the SAT problem $\mathcal{P}$, we invoke an automated theorem prover (such as the Z3 SMT solver (De Moura and Bjørner, 2008) used in our implementation) to obtain a model $M$ that maps each free variable $v \in \operatorname{Variable}(\mathcal{P})$ to a concrete value under theory $\mathcal{T}$. The final answer is obtained by substituting each free variable $v_{i}$ in $\hat{Q}$ with $M\left[v_{i}\right]$. For example, given the problem $\left({x+y=3, x-y=1}, \mathcal{T}<em _mathbb_Z="\mathbb{Z">{E} \cup \mathcal{T}</em>}}, x-2\right)$, we ask the solver to find a solution to the constraint $x+y=3 \wedge x-y=1$ in the theory $\mathcal{T<em _mathbb_Z="\mathbb{Z">{E} \cup \mathcal{T}</em>$, which yields $x=2$ and $y=1$. Then, to obtain the final answer, we substitute $x$ by 2 in the target expression $x-2$ to obtain the result $2-2=0$.}</p>
<p>Feedback signals from the solver Given a set of $\hat{\Phi}$ specified in $\mathcal{P}$, the SAT solver will try to search for a satisfying assignment $M$ which satisfies all constraint formulas in $\hat{\Phi}$. If the solver succeeds in finding such an assignment within a certain time limit, it will use $M$ to evaluate the query $\hat{Q}$ and return the final result, otherwise it is a timeout. However, the solver may fail to find a solution for problematic $\mathcal{P}$ and provide feedback in one of the following types: (1) error in execution (ERROR) caused by invalid formulas (e.g., syntax errors) or time-out; (2) unsatisfiable formulas (UNSAT), caused by conflicting formulas in the $\hat{\Phi}$ (e.g. $\hat{\Phi}={x=y+1, y=x+1}$ ) (no feasible solution); (3) ambiguous formulas (AMBIG), caused by the existence of multiple feasible solutions (e.g. $\hat{\Phi}={x=y+1, x&gt;0}$ ). Examples of SAT formulas leading to UNSAT or AMBIG can be found in Appendix G.</p>
<p>Unlike the executor used in ProGLM that can only detect errors in code execution, SAT solver can spot UNSAT and AMBIG in addition to ERROR. We show this unique characteristic allows our SATLM to abstain from potentially incorrect predictions much more effectively compared to ProGLM in the selective prediction setting (El-Yaniv and Wiener, 2010) (Section 4.4).</p>
<h2>4 Experiments</h2>
<h3>4.1 Setup</h3>
<p>Tasks Our work investigates 8 datasets covering 4 tasks, with a focus on arithmetic reasoning and logical reasoning tasks. We list all dataset statistics in Appendix A. For arithmetic reasoning, we use GSM (Cobbe et al., 2021), GSM-Sys, and Algebra (He-Yueya et al., 2023). GSM-Sys</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparison of our approach (SATLM) against standard prompting (directly predicting the answer), CoT and ProgLM. Certain settings are not applicable (marked as -) as described in Appendix B. With greedy decoding, SATLM outperforms CoT and ProgLM on all datasets by a substantial margin except for GSM, where it is on par with ProgLM. With self-consistency decoding, SATLM is consistently better than ProgLM, giving SoTA accuracy on LSAT and BOARDGAMEQA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM-Sys</th>
<th style="text-align: center;">GSM</th>
<th style="text-align: center;">Alge</th>
<th style="text-align: center;">LSAT</th>
<th style="text-align: center;">Board</th>
<th style="text-align: center;">Clutrr</th>
<th style="text-align: center;">Proof</th>
<th style="text-align: center;">Color</th>
<th style="text-align: center;">REgEx</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">code-davinci-002 (greedy decoding)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ProgLM</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">39.1</td>
</tr>
<tr>
<td style="text-align: center;">SATLM</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">41.0</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002 (self-consistency decoding)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ProgLM</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">56.5</td>
</tr>
<tr>
<td style="text-align: center;">SATLM</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">59.7</td>
</tr>
</tbody>
</table>
<p>is a special subset of GSM containing examples that are paired with human-annotated solutions involving systems of equations (see Appendix A for more details). For logical reasoning, we use LSAT (Zhong et al., 2022), BoardGameQA (Kazemi et al., 2023), Clutrr (Sinha et al., 2019), and ProofWriter (Tafjord et al., 2021). For BoardGameQA, we report the average performance on the three data splits (depth 1 to depth 3).</p>
<p>For Clutrr, we use exemplars requiring up to 3 intermediate steps but evaluate on test examples requiring up to 10 intermediate steps (Sinha et al., 2019), following past work (Lyu et al., 2023). For ProofWriter, we evaluate on the most challenging examples requiring depth-5 proofs (Tafjord et al., 2021). For symbolic reasoning, we use Colored Object (Color) from BIG-bench (et al., 2022) as an exemplar task. This task can be abstracted as finding elements in a list under certain constraints. We also evaluate on a regex synthesis dataset, STREGEX (Ye et al., 2020), which requires synthesizing a regex give NL description. We cast this task into synthesizing the surface form (i.e., a string) of the target regex, and use SATLM to parse NL description into constraints over the string.</p>
<p>Baselines We compare SATLM against 3 baselines, including standard prompting (directly giving the answer), chain-of-thought prompting (CoT), and executor-augmented LLMs (ProgLM). We do not compare to zero-shot baselines such as zero-shot CoT, which generally underperform few-shot CoT by a large margin on the tasks we investigate (Kojima et al., 2022).</p>
<p>For CoT and ProgLM, we leverage prompts of existing work (Gao et al., 2023; Lyu et al., 2023; Creswell et al., 2023) whenever possible. For SATLM, we manually write prompts for the same exemplar sets used in CoT and ProgLM to ensure a fair comparison. We note that some settings, such as ProgLM for LSAT, are not applicable. Please refer to Appendix B for more discussion of the setup, including details on the prompts we use. We also include example prompts for all the datasets in Appendix I.</p>
<p>Language Models \&amp; Decoding We conduct our main experiments and analysis on code-davinci-002 (Chen et al., 2021), a state-of-art LLM for code and code-adjacent tasks. We also include results on a less capable version code-davinci-001 and an LLM specialized more to NL, text-davinci-003. We evaluate the performance with both greedy decoding and self-consistency decoding (Wang et al., 2022b). Due to the high computation cost, we use 5 samples for LSAT, BoardGameQA, and ProofWriter, which involve long prompts, and use 40 samples for all other datasets.</p>
<h1>4.2 Main Results</h1>
<p>Table 1 shows the performance of our approach compared to the baselines. In general, our SAT-aided approach outperforms both COT and ProgLM by a substantial margin except on GSM with greedy decoding. We perform significance tests via bootstrap resampling, and all improvements of SATLM over ProgLM are statistically significant $(p&lt;0.05)$.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A variant of our approach which replaces the SAT solver with a "CoT solver" that takes the SAT problem as input and solves it in natural language.</p>
<p>Table 2: The performance of variants of our approach that use CoT Solver or No Solver. Using declarative prompting with CoT solver is more effective than imperative CoT prompting.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GSM-Sys</th>
<th style="text-align: center;">GSM</th>
<th style="text-align: center;">Clutrr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">STANDARD</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">41.2</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: left;">PAL</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">58.9</td>
</tr>
<tr>
<td style="text-align: left;">SAT $_{\text {SYMSOLVER }}$</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">68.3</td>
</tr>
<tr>
<td style="text-align: left;">SAT $_{\text {CoTSOlver }}$</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">48.9</td>
</tr>
<tr>
<td style="text-align: left;">SAT $_{\text {NoSolver }}$</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">40.7</td>
</tr>
</tbody>
</table>
<p>The first two columns show the performance on the GSM dataset. CoT and ProgLM achieve much worse performance on GSM-Sys than on GSM, indicating that GSM-Sys is a challenging subset. On this subset, SATLM achieves $69.4 \%$ and $80.9 \%$ with greedy decoding and self-consistency decoding, surpassing both ProgLM and CoT more than by $20 \%$. On the original GSM dataset, the SATLM model has a slightly lower accuracy than ProgLM with greedy decoding, but outperforms it with self-consistency decoding by $2.4 \%$; we provide detailed analysis accounting for the differences later in this section. This self-consistency accuracy of $84.8 \%$ even exceeds recent work that uses the full training set with code-davinci-002 ( $82.3 \%$ in DiVERSE (Li et al., 2022b); $84.5 \%$ in LEVER (Ni et al., 2023)). On Algebra, a challenging dataset of math problems extracted from algebra textbooks, SATLM also outperforms CoT and ProgLM by more than $20 \%$.</p>
<p>On LSAT, Clutrr, ProofWriter, and Color, SatLM consistently achieves the best performance with either greedy decoding or self-consistency decoding. SatLM also sets the new SoTA on both LSAT and BoardGAMEQA, surpassing previous models that are trained on the full training set. Specifically, SATLM elevates the SoTA from $30.9 \%$ (Zhong et al., 2022) to $37.4 \%$ on LSAT and from $73.9 \%$ (Kazemi et al., 2023)) to $80.7 \%$ on BoardGAMEQA. See Appendix E for detailed performance breakdown on depth 1-3.</p>
<p>In the regex synthesis domain, with greedy decoding, directly translating natural language descriptions to regexes (ProgLM) achieves $37.1 \%$, whereas using declarative prompting achieves $44.0 \%$. With self-consistency, we surpass the previous SoTA performance of $55.6 \%$ (Ye et al., 2021).</p>
<h1>4.3 Impact of SAT Solver \&amp; Declarative Prompting</h1>
<p>We conduct analysis to isolate the effectiveness of the two key components, the SAT solver and declarative prompting. Specifically, we test a variant of our approach that still uses declarative prompting but then solves the equations in natural language with CoT rather than using the symbolic solver (see Figure 3). Essentially, the LLM itself carries out planning and execution. This experiment helps isolate the benefits of the solver, which will compute an answer without making any mistakes, from the benefits of the declarative formulation. We also compare to prompting LLMs to directly give the answer (NOSOLVER).</p>
<p>Impact of Symbolic Solver As shown in Table 2, completely ablating the solver and directly predicting the answer ( $\mathrm{SAT}<em _CoTSOLVER="{CoTSOLVER" _text="\text">{\text {NoSOLVER }}$ ) only yields performance that is on par with STANDARD. Interestingly, $\mathrm{SAT}</em>$ ), which guarantees correct planning and execution, leads to further improvements.}}$ can solve more SAT problems than NoSOLVER. This partially reflects the effectiveness of CoT and partially reflects the fact that many dataset instances require relatively simple planning and execution, allowing pure forward reasoning to solve them. However, using a symbolic solver ( $\mathrm{SAT}_{\text {SYMSOLVER }</p>
<p>Table 3: Fraction of planning errors (incorrect reasoning chains) and execution errors (numeric errors) made by CoTSOLVER.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GSM-Sys</th>
<th style="text-align: center;">GSM</th>
<th style="text-align: center;">Clutrr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PLAN ERR</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">47.5</td>
</tr>
<tr>
<td style="text-align: left;">EXEC ERR</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">52.5</td>
</tr>
</tbody>
</table>
<p>Table 5: Analysis of accuracy and execution status of SATLM and ProgLM. We present the fraction of tasks solved correctly or incorrectly in GSM-Sys, GSM, and Clutrr, along with the breakdown of feedback from the solver. SATLM generally makes fewer predictions than ProgLM (ANSWERED), but more frequently makes correct predictions when it returns an answer (SELECTIVE ACC) and gives a higher absolute number of correct predictions on GSM-SYS and CLUTRR.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GSM-Sys</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Clutrr</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ProgLM</td>
<td style="text-align: center;">SatLM</td>
<td style="text-align: center;">ProgLM</td>
<td style="text-align: center;">SatLM</td>
<td style="text-align: center;">ProgLM</td>
<td style="text-align: center;">SatLM</td>
</tr>
<tr>
<td style="text-align: left;">CORRECT</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">68.3</td>
</tr>
<tr>
<td style="text-align: left;">INCORRECT</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">7.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ERROR</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">3.5</td>
</tr>
<tr>
<td style="text-align: left;">UnSAT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.5</td>
</tr>
<tr>
<td style="text-align: left;">AMBIG</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.0</td>
</tr>
<tr>
<td style="text-align: left;">ANSWERED</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: left;">SELECTIVE ACC</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">$\mathbf{7 7 . 1}$</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">$\mathbf{7 7 . 2}$</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">$\mathbf{8 9 . 9}$</td>
</tr>
</tbody>
</table>
<p>We manually analyzed 40 cases where the symbolic solver yields the correct answer but $\mathrm{SAT}<em _CoTSOLVER="{CoTSOLVER" _text="\text">{\text {CoTSOLVER }}$ fails to solve them. We categorized the errors as planning errors, where the reasoning chains are incorrect, and execution errors, where the reasoning chains are correct but computations are incorrect (see Appendix H for examples). Table 3 shows that most errors by $\mathrm{SAT}</em>$ are planning errors, especially on GSM-SYS which requires solving complex system of equations.}</p>
<p>Impact of Declarative Prompting Table 2 also shows that decoupling parsing and planning/solving is still useful, even when not using a symbolic solver: $\mathrm{SAT}<em _CoTSOLVER="{CoTSOLVER" _text="\text">{\text {CoTSOLVER }}$ outperforms CoT by $7.9 \%$, and $8.1 \%$ on GSM-SYS and Clutrr, respectively. We note that $\mathrm{SAT}</em>$ can be viewed as a two-stage CoT prompting strategy, with a prompt showing that the first step is to formulate declaratively, then the next step is to solve.
We hypothesize that parsing a question into declarative formulas is more straightforward than parsing it into an imperative solving procedure. To evaluate this hypothesis, we use log likelihood of the generated tokens to assess how straightforward the translation is, as higher log-likelihood typically indicates the outputs are more fluent to LLMs, a connection demonstrated in recent literature (Gonen et al., 2022; Ye and Durrett, 2023). We show both unnormalized (total) and normalized log likelihood in Table 4. On GSM-Sys and Clutrr where SATLM outperforms ProgLM, its generated outputs are also associated with higher likelihood.}</p>
<h1>4.4 Advantages of SAT in Selective Prediction</h1>
<p>A SAT solver may not always return an answer, particularly if there are parsing errors from the question. We show that this is an advantage of SATLM: these errors allow us to abstain from making likely incorrect predictions. Example outputs leading to different errors can be found in Appendix G.
Table 5 shows the fraction of correct predictions and incorrect predictions when the program or SAT solver successfully returns an answer as well as the fraction of different types of feedback signals. We report the fraction of questions answered as well as selective accuracy, defined by the fraction of overall accuracy (\% of correct answers) normalized by coverage (\% of answered problems). SATLM makes fewer predictions on all three datasets compared to ProgLM, as it can trigger both UnSAT and Ambig errors. However, SATLM's selective accuracy is consistently better than ProgLM's, especially on GSM-Sys ( $77 \%$ vs $45 \%$ ). As a result, SATLM's overall performance is significantly better than ProgLM on GSM-Sys and Clutrr, even when making fewer predictions.
We note that on GSM, SATLM has slightly lower coverage but higher selective accuracy compared to ProgLM. This explains why SATLM lags behind ProgLM with greedy decoding but outperforms ProgLM with self-consistency decoding (Table 1). By drawing multiple samples, SATLM can increase its coverage and achieve higher accuracy than ProgLM since its predictions are more accurate.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Examples outputs from GSM (left) and BoardGAMEQA (right) show that LLMs can perform commonsense reasoning while parsing.</p>
<h1>4.5 Analysis</h1>
<p>LLMs Can Perform Commonsense Reasoning While Parsing There are many problems that do not state premises or constraints in a completely explicit way. Figure 4) shows two examples where commonsense inferences are required during parsing. For example, on the left, the model must recognize that animals refers to the chickens and cows collectively. Similarly, knowing that red is a primary color is needed to successfully apply rules on BoardGAMEQA (right). We observe from the outputs in both cases that LLMs are capable of implicitly performing commonsense reasoning and produce correct logical formulas in the parsing step. As shown in Table 1, SATLM exhibits strong performance on BoardGAMEQA, a dataset which requires this implicit background knowledge.</p>
<h2>Results Across Different Language Models</h2>
<p>In addition to the main LLM used in our work, code-davinci-002, we further test whether SATLM can generalize to other LLMs. We choose gpt-3.5-turbo ( 0613 version), text-davinci-003, and code-davinci-001. gpt-3.5-turbo is optimized for chat. text-davinci-003 is an LLM pretrained on NL, and tuned to align with human feedback (Ouyang et al., 2022). code-davinci-001 is also an LLM pretrained on code, but less capable compared to 002 . As shown in Table 6, SATLM is better than ProgLM on the arithmetic reasoning and logical reasoning datasets except for GSM across these three LLMs. The trend is congruent with the results on code-davinci-002 (Table 1), which suggests the approach's general applicability across different LLMs, regardless of their varying capabilities.</p>
<p>Sensitivity to Different Exemplar Sets We test whether the advantages of SATLM is sensitive to different sets of exemplars. We experiment with 3 sets of exemplars on code-davinci-002. As shown in Table 7, SATLM consistently outperforms ProgLM by a large margin on GSMSys and Clutrr, and achieves comparable performance on GSM. The results suggest the effectiveness of our approach is insensitive to varying the choice of exemplars.</p>
<h2>5 Related Work</h2>
<p>Our work is built on top of few-shot prompting (Brown et al., 2020), which has proven effective on a wide range of tasks (Wei et al., 2022b; Liu et al., 2023b; Gehrmann et al., 2021; Reif et al., 2022; Wei et al., 2022a; Sanh et al., 2022). In particular, we focus on improving LLMs on</p>
<p>Table 6: Results on gpt-3.5-turbo, text-davinci-003, and code-davinci-001. The effectiveness of SATLM can generalize across LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM-Sys</th>
<th style="text-align: center;">GSM</th>
<th style="text-align: center;">LSAT</th>
<th style="text-align: center;">Clutrr</th>
<th style="text-align: center;">Proof</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">gpt-3.5-turbo (greedy decoding)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">82.3</td>
</tr>
<tr>
<td style="text-align: center;">ProgLM</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">76.4</td>
</tr>
<tr>
<td style="text-align: center;">SATLM</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">96.4</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003 (greedy decoding)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">83.5</td>
</tr>
<tr>
<td style="text-align: center;">ProgLM</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">83.7</td>
</tr>
<tr>
<td style="text-align: center;">SATLM</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">99.7</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-001 (greedy decoding)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ProgLM</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">63.8</td>
</tr>
<tr>
<td style="text-align: center;">SATLM</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">86.6</td>
</tr>
</tbody>
</table>
<p>Table 7: The performance of ProgLM and SATLM with varying exemplar sets. SATLM consistently outperforms ProgLM on GSM-Sys and Clutrr.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM-Sys</th>
<th style="text-align: center;">GSM</th>
<th style="text-align: center;">Clutrr</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prog</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SAT</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">Prog</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">72.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SAT</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">71.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prog</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">70.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SAT</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">70.0</td>
</tr>
</tbody>
</table>
<h2>5. Related Work</h2>
<p>Our work is built on top of few-shot prompting (Brown et al., 2020), which has proven effective on a wide range of tasks (Wei et al., 2022b; Liu et al., 2023b; Gehrmann et al., 2021; Reif et al., 2022; Wei et al., 2022a; Sanh et al., 2022). In particular, we focus on improving LLMs on</p>
<p>reasoning tasks, which are challenging for language models even with recent developments (Marcus, 2020; Garcez and Lamb, 2023). Various techniques have been proposed for improving reasoning abilities (Nye et al., 2021; Zhou et al., 2022; Kojima et al., 2022; Khot et al., 2022; Fu et al., 2022; Wang et al., 2022a; Li et al., 2022a; Lyu et al., 2023). They largely follow a chain-of-thought (Wei et al., 2022c) or scratchpad (Nye et al., 2021) paradigm. Among them, our work is the most related to the line of work that generates imperative programs to be executed by a symbolic executor, such as a Python interpreter (Gao et al., 2023; Chen et al., 2022) or domain-specific executors (Lyu et al., 2023). In this work, we propose a different paradigm that parses NL problems into declarative SAT problems and offloads the solving procedure to a SAT solver.
Previous work has also explored equipping LLMs with other tools, including search engines (Yu et al., 2023; Schick et al., 2023), calculators (Cobbe et al., 2021; Chowdhery et al., 2022), or other domain-specific special modules (Schick et al., 2023; Demeter and Downey, 2020). A line of work focuses on using program-related tools such as program executors (Poesia et al., 2022), program analysis tools (Jain et al., 2022), and synthesis tools (Rahmani et al., 2021) to enhance the quality of the generated code. Our works further explores improving LLMs with SAT solvers.
Concurrent work explores the intersection of LLMs and planning, parsing planning problems into PDDL descriptions and leveraging a classical planner to produce the plan (Liu et al., 2023a). Our work differs in that we use the SAT formulation to solve general reasoning tasks, including arithmetic reasoning and logical reasoning, which cannot be specified in PDDL.
Also concurrently, He-Yueya et al. (2023) combine LLMs and symbolic solvers for solving math problems. However, this work only focus on arithmetic reasoning tasks and employs a math-specific symbolic solver (PySym). Our work takes a more general approach by formulating the problem within the scope of first-order logic and therefore is domain-agnostic. We also provide results of SATLM on the Algebra dataset collected by He-Yueya et al. (2023) in Appendix D.</p>
<h1>6 Conclusion \&amp; Limitations</h1>
<p>We have presented a framework for satisfiability-aided language models, casting a wide range of reasoning tasks into SAT problems under a unified formulation. We use an LLM to parse an NL query into a declarative specification and leverages a SAT solver to derive the final answer. Evaluation results on 8 datasets spanning 4 tasks across several LLMs demonstrate the effectiveness of our approach over program-aided language models.</p>
<p>Limitations Our framework parses an NL problems into a set of declarative formulas. The NL description of some problems may already be more compatible with an imperative solving procedure, and our approach is likely to be less effective in these cases (e.g., SATLM slightly lags ProgLM on GSM). Future research can explore an integration or ensemble of these two prompting styles for more flexible reasoning.
SATLM heavily relies on the SAT solver and inherits some limitations of the SAT solver itself, such as computational cost when dealing with complex formulas involving quantifiers or nonlinear arithmetic. Moreover, SAT solvers can be limited by the expressiveness of the underlying theory, as not all theories can be easily encoded in first-order logic. Nevertheless, the wide range of tasks that we can instantiate our SATLM framework on shows its general applicability.
Our current approach parses a problem into a SAT specification, runs the solver, and returns the answer in a one-round fashion. One can imagine that unsatisfiable formulas or ambiguous formulas could be improved by re-prompting the model to improve the specification based on the exception signals, as explored in concurrent work for other problems (Paul et al., 2023; Madaan et al., 2023; Chen et al., 2023). We believe this is an exciting direction for future work.</p>
<h2>Acknowledgments</h2>
<p>Thanks to anonymous reviewers for their helpful feedback. This work was partially supported by National Science Foundation under Grants No.2145280, No.1918889, No.1762299, No. 2210831 and the NSF AI Institute for Foundations of Machine Learning (IFML). We would also like to thank</p>
<p>authors of Pal (Gao et al., 2023) and Faithful CoT (Lyu et al., 2023) for providing the prompts used in the baselines.</p>
<h1>References</h1>
<p>Aarohi Srivastava et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv, abs/2206.04615.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Baindoor Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. MeierHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. ArXiv, abs/2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. 2023. Selection-inference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations.</p>
<p>Martin Davis and Hilary Putnam. 1960. A computing procedure for quantification theory. J. ACM, 7(3):201-215.</p>
<p>Leonardo De Moura and Nikolaj Bjørner. 2008. Z3: An Efficient SMT Solver. In Proceedings of the Theory and Practice of Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS’08/ETAPS’08, page 337-340, Berlin, Heidelberg. Springer-Verlag.</p>
<p>David Demeter and Doug Downey. 2020. Just add functions: A neural-symbolic language model. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7634-7642.</p>
<p>Ran El-Yaniv and Yair Wiener. 2010. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(53):1605-1641.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. In Proceedings of the International Conference on Machine Learning (ICML).</p>
<p>Artur d’Avila Garcez and Luis C Lamb. 2023. Neurosymbolic AI: The 3rd wave. Artificial Intelligence Review, pages 1-20.</p>
<p>Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96-120, Online. Association for Computational Linguistics.</p>
<p>Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. 2022. Demystifying prompts in language models via perplexity estimation. arXiv preprint arXiv:2212.04037.</p>
<p>Joy He-Yueya, Gabriel Poesia, Rose E. Wang, and Noah D. Goodman. 2023. Solving math word problems by combining language models with symbolic solvers. ArXiv, abs/2304.09102.</p>
<p>Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma. 2022. Jigsaw: Large language models meet program synthesis. ICSE.</p>
<p>Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran. 2023. BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. 2022a. Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022b. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R'e, Diana Acosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic evaluation of language models. ArXiv, abs/2211.09110.</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023a. LLM+ P: Empowering Large Language Models with Optimal Planning Proficiency. arXiv preprint arXiv:2304.11477.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023b. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv., 55(9).</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651.</p>
<p>Gary Marcus. 2020. The next decade in AI: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177.</p>
<p>Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I Wang, and Xi Victoria Lin. 2023. LEVER: Learning to Verify Language-to-Code Generation with Execution. In Proceedings of the International Conference on Machine Learning (ICML).</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904.</p>
<p>Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable code generation from pre-trained language models. In International Conference on Learning Representations.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur</p>
<p>Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling Language Models: Methods, Analysis \&amp; Insights from Training Gopher. ArXiv, abs/2112.11446.</p>
<p>Kia Rahmani, Mohammad Raza, Sumit Gulwani, Vu Le, Daniel Morris, Arjun Radhakrishna, Gustavo Soares, and Ashish Tiwari. 2021. Multi-modal program inference: A marriage of pre-trained language models and component-based synthesis. Proc. ACM Program. Lang., 5(OOPSLA).</p>
<p>Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A recipe for arbitrary text style transfer with large language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 837-848, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henghui Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, zhiheng huang, William Yang Wang, George Karypis, Bing Xiang, and Dan Roth. 2023. STREET: A multi-task structured reasoning and explanation benchmark. In The Eleventh International Conference on Learning Representations.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.</p>
<p>Abulhair Saparov and He He. 2023. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv.</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP (ACL Findings).</p>
<p>Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2022. Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). ArXiv, abs/2206.10498.</p>
<p>Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. 2022a. Pinto: Faithful language reasoning using prompt-generated rationales. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022b. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022c. Chain of thought prompting elicits reasoning in large language models. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2020. Benchmarking multimodal regex synthesis with complex structures. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2021. Optimal neural program synthesis from multimodal specifications. In Findings of the Association for Computational Linguistics: EMNLP (EMNLP Findings).</p>
<p>Xi Ye and Greg Durrett. 2023. Explanation selection using unlabeled data for chain-of-thought prompting. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are strong context generators. In International Conference for Learning Representation (ICLR).</p>
<p>Hanlin Zhang, Ziyang Li, Jiani Huang, Mayur Naik, and Eric Xing. 2022a. Improved logical reasoning of language models via differentiable symbolic programming. In First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022b. OPT: Open Pre-trained Transformer Language Models. ArXiv, abs/2205.01068.</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2022. Analytical reasoning of text. In Findings of the Association for Computational Linguistics: NAACL (NAACL Findings).</p>
<p>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. ArXiv, abs/2205.10625.</p>
<h1>A Detailed Statistics of Datasets</h1>
<p>We show the statistics of all the datasets used in our paper in Table 8.
For Clutrr, we follow the setting in FaithfulCoT (Lyu et al., 2023): we construct the prompt using exemplars requiring 2-3 reasoning steps and test whether the model can generalize to examples requiring up to 10 steps. We used the pre-processed test data consisting of 1,042 test examples from past work (Lyu et al., 2023).</p>
<p>For ProofWriter, we use the closed world assumption setting, following past work (Creswell et al., 2023). We construct our test set by randomly sampling a subset of 1,000 examples (out of 10,000 ) from the test split of depth-5 setting, the most challenging setting.</p>
<p>For StRegEx, we merge the test and test-E split (see Ye et al. (2020)) to form a test set consisting of 996 examples in total.</p>
<p>Table 8: Number of few-shot exemplars, number of test examples and license for the datasets used in our paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"># Shot</th>
<th style="text-align: center;"># Test</th>
<th style="text-align: center;">License</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GSM (Cobbe et al., 2021)</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1,319</td>
<td style="text-align: center;">MIT license</td>
</tr>
<tr>
<td style="text-align: center;">GSM-Sys</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">547</td>
<td style="text-align: center;">MIT license</td>
</tr>
<tr>
<td style="text-align: center;">Algebra (He-Yueya et al., 2023)</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">222</td>
<td style="text-align: center;">Creative Commons Attribution Share Alike 4.0</td>
</tr>
<tr>
<td style="text-align: center;">LSAT (Zhong et al., 2022)</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">MIT license</td>
</tr>
<tr>
<td style="text-align: center;">BoardGameQA (Kazemi et al., 2023)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3,000</td>
<td style="text-align: center;">CC BY 4.0.</td>
</tr>
<tr>
<td style="text-align: center;">Clutrr (Sinha et al., 2019)</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1,042</td>
<td style="text-align: center;">Attribution-NonCommercial 4.0</td>
</tr>
<tr>
<td style="text-align: center;">ProofWriter (Tafjord et al., 2021)</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">CC BY 4.0.</td>
</tr>
<tr>
<td style="text-align: center;">ColoredObject (Big-Bench)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2,000</td>
<td style="text-align: center;">Apache 2.0</td>
</tr>
<tr>
<td style="text-align: center;">StructuredRegex (Ye et al., 2020)</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">996</td>
<td style="text-align: center;">MIT license</td>
</tr>
</tbody>
</table>
<p>GSM-Sys Dataset We construct GSM-Sys, a special subset consisting of 547 examples extracted from GSM. Specifically, we filter the entire GSM dataset (train split and test split) to find examples whose human-annotated explanations involve a system of equations, using patterns like "let [letter] be", "assume [letter] be" and "[number][letter]". We manually inspected $10 \%$ of the examples and found $80 \%$ of those samples did involve systems of equations in the explanation. We refer to this more challenging dataset as GSM-Sys.</p>
<h2>B Details of the Prompts</h2>
<p>In general, we leverage CoT prompts and ProgLM prompts from existing work whenever available, and manually write SATLM prompts for the same exemplar sets. Prompt examples for all datasets can be found in Appendix I.</p>
<p>For GSM and GSM-Sys, we adapt the original CoT prompt and ProgLM prompt used in programaided language models (Gao et al., 2023). Specifically, we replace one random exemplar in the original prompt with another exemplar sampled from GSM-Sys. This is to improve the performance of CoT and ProgLM on GSM-Sys, as the original exemplar set achieves suboptimal performance for GSM-Sys. Our adapted CoT and ProgLM prompts achieve better performance compared to the original ones on both GSM and GSM-Sys (see Appendix C for details).</p>
<p>For LSAT, we randomly sample 8 exemplars and write prompts for CoT and SATLM. We note that LSAT is a particularly challenging task: we tried 3 CoT prompts written by 3 different authors of our paper, which all led to around $20 \%$ accuracy. Similar results are reported in other work (Liang et al., 2022; Ribeiro et al., 2023). In addition, we only report CoT results, leaving out ProgLM. This decision is due to the fact that ProgLM uses Python as its program interpreter. While Python is a general-purpose programming language, it does not provide native support for formal logic reasoning, including essential components like logical inference rules and manipulation of logical formulas. Solving problems from LSAT requires strategies like proof by contradiction (see Appendix I for a detailed example), which we see no way to represent in the ProgLM framework and is not addressed in prior work.</p>
<p>BoardGAMEQA contains problems requiring 1-3 steps of reasoning. We sample 5 exemplars from the training set of depth 1 and depth 2 to construct the prompts for evaluation on the test sets of depth 1 and depth 2, respectively. We used the 5 exemplars of depth 2 to construct the prompt for test set of depth 3, as using exemplars of depth 3 would lead to prompts that exceed the context window size of our LLMs. Similarly, we only report CoT results as the baselines, leaving out ProgLM for BoardGAMEQA. We use the proofs provided by the authors to construct the CoT prompts and manually annotate the SAT specifications to construct the SATLM prompts.</p>
<p>For Clutrr, we use the CoT prompt and ProgLM prompt provided in FaithfulCoT (Lyu et al., 2023). For ProofWriter, we use the CoT prompt from Selection-Inference (Creswell et al., 2023), and adapt it to form the ProgLM prompt. We use the CoT prompt and ProgLM from Pal (Gao et al., 2023) for Colored Object.</p>
<p>The task of StructuredRegex, a regex synthesis dataset, is to parse natural language descriptions to regexes. This is not a typical reasoning dataset, and there is no COT prompt for this dataset. We randomly sample 8 exemplars and annotate the prompt for ProgLM and SatLM. In this setting, ProgLM directly translates NL descriptions into regexes (which are essentially programs), whereas SATLM parses an NL description into a set of constraints over the surface form of the regex. Note that this dataset provides multimodal specifications of regexes, featuring both NL descriptions and examples. The I/O examples can be used to reject synthesized regexes if they do not accept or reject the correct examples. When we report results for self-consistency inference, we follow past work (Ye et al., 2021) and filter out incorrect outputs using the I/O examples provided in the dataset (Ye et al., 2020). This setting therefore checks consistency with something other than the model itself, but uses a similar computation budget as self-consistency, so we group it with those results.</p>
<h1>C Performance of Original CoT and ProgLM Prompts on Arithmetic Reasoning Datasets</h1>
<p>Table 9: Performance of different approaches using our adapted exemplar set and the original exemplar set used in COT and PAL.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Adapted (Ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">GSM-Sys</td>
<td style="text-align: center;">GSM</td>
<td style="text-align: center;">GSM-Sys</td>
<td style="text-align: center;">GSM</td>
</tr>
<tr>
<td style="text-align: left;">COT</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: left;">ProgLM</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">$\mathbf{7 2 . 7}$</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">$\mathbf{7 1 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">SATLM</td>
<td style="text-align: center;">$\mathbf{6 9 . 4}$</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">$\mathbf{6 6 . 7}$</td>
<td style="text-align: center;">70.9</td>
</tr>
</tbody>
</table>
<p>Recall that we construct our arithmetic reasoning prompt used in Table 1 by replacing one random exemplar in the original prompt used in COT and ProgLM with an random example from GSM-Sys. We show the performance of CoT, ProgLM, and our SATLM in Table 9 using our adapted exemplar set and original exemplar set in Table 9.</p>
<p>Our adaptation significantly improves the performance of COT and ProgLM on GSM-Sys, and slightly improves the performance on GSM. Furthermore, we still see that SATLM outperforms both COT and ProgLM by a large margin on GSM, using either our adapted set or the original set.</p>
<h2>D Extended Discussion on Concurrent Work</h2>
<p>Table 10: Performance of different approaches on Algebra.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Algebra</th>
<th style="text-align: center;">GSM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">COT</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: left;">ProgLM</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: left;">SATLM (Ours)</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: left;">MathSym (He-Yueya et al., 2023)</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">69.4</td>
</tr>
</tbody>
</table>
<p>Similar to our work, He-Yueya et al. (2023) proposes to solve arithmetic reasoning problems by parsing the problem into a set of variables and equations and using an external solver to derive the</p>
<p>final answer. While their formalization is restricted to arithmetic problems, we use SAT problems encoded with first-order logical formulas, which unify a wide range of reasoning tasks.</p>
<p>In addition, we also evaluate our approach on the Algebra dataset in He-Yueya et al. (2023), which consists of 222 examples from Algebra textbooks. We note that the results between ours and MATHSYM are not directly comparable, as MATHSYm picks a different exemplar set. As shown in Table 10, Algebra is more challenging than GSM, and SatLM outperforms ProGLM and CoT by more than $20 \%$.</p>
<h1>E Detailed Performance on the BoardGAMEQA Dataset</h1>
<p>Table 11: Detailed performance on the BoardGAMEQA dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">DEPTH 1</th>
<th style="text-align: center;">DEPTH 2</th>
<th style="text-align: center;">DEPTH 3</th>
<th style="text-align: center;">AGGREGATED</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">code-davinci-002 (greedy decoding)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">STANDARD</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">44.6</td>
</tr>
<tr>
<td style="text-align: left;">COT</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">60.1</td>
</tr>
<tr>
<td style="text-align: left;">SATLM</td>
<td style="text-align: center;">$\mathbf{8 7 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 7}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">code-davinci-002 (self consistency decoding)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">COT</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">62.8</td>
</tr>
<tr>
<td style="text-align: left;">SATLM</td>
<td style="text-align: center;">$\mathbf{8 8 . 0}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 11 shows the performance breakdown on depths 1-3 of the BoardGAMEQA dataset. SatLM outperforms CoT by a substantial margin across all depths. The performance of all approaches decreases as the depth increases.</p>
<h2>F Details of the SAT Specification</h2>
<p>To better utilize the parametric knowledge that LLMs have acquired from pretraining on vast amount of code data, our work uses a specification that largely follows and simplifies the syntax for specifying constraints used in z3py. ${ }^{5}$</p>
<div class="codehilite"><pre><span></span><code><span class="n">Example</span><span class="w"> </span><span class="n">SAT</span><span class="w"> </span><span class="n">Specification</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Variable</span><span class="p">()</span><span class="w"> </span><span class="c1"># declare a variable</span>
<span class="n">People</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">Alice</span><span class="p">,</span><span class="w"> </span><span class="n">Bob</span><span class="p">]</span><span class="w"> </span><span class="c1"># declare enum set</span>
<span class="n">Cities</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">Austin</span><span class="p">,</span><span class="w"> </span><span class="n">Boston</span><span class="p">]</span><span class="w"> </span><span class="c1"># declare enum set</span>
<span class="n">Food</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">Apple</span><span class="p">,</span><span class="w"> </span><span class="n">Banana</span><span class="p">]</span><span class="w"> </span><span class="c1"># declare enum set</span>
<span class="n">visit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Function</span><span class="p">(</span><span class="n">People</span><span class="p">,</span><span class="w"> </span><span class="n">Cities</span><span class="p">)</span><span class="w"> </span><span class="c1"># declare function</span>
<span class="n">eats</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Function</span><span class="p">(</span><span class="n">People</span><span class="p">,</span><span class="w"> </span><span class="n">Food</span><span class="p">)</span><span class="w"> </span><span class="c1"># declare function</span>
<span class="n">visit</span><span class="p">(</span><span class="n">Alice</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">visit</span><span class="p">(</span><span class="n">Bob</span><span class="p">)</span><span class="w"> </span><span class="c1"># logic</span>
<span class="n">ForAll</span><span class="p">(</span><span class="n">x</span><span class="p">:</span><span class="w"> </span><span class="n">People</span><span class="p">,</span><span class="w"> </span><span class="n">Implies</span><span class="p">(</span><span class="n">visit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">Austin</span><span class="p">,</span><span class="w"> </span><span class="n">eats</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">Banana</span><span class="p">))</span><span class="w"> </span><span class="c1"># quantifier</span>
</code></pre></div>

<p>Figure 5: Example of our SAT specification. The syntax is largely the same as that for specifying constraints in z3py.</p>
<p>We give an example specification in Figure 5 demonstrating the synax for different types of statements. See Figure 1, Figure 2, and Appendix I for more examples. These formulas are close to the actual python code formulas used by z3py but are slightly modified to be more amenable to prompting. As a result, we use a postprocessing step to form the actual Z3 input. We implemented a simple parser that transforms these formulas into actual specifications used by z3py via string transformation (using regexes). For example, we transform [ForAll(x: People, Implies(visit(x, Austin), eats(x, Banana)))] into [x = Variable(People) ForAll([x], Implies(visit(x) == Austin, eats(x) ==</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Banana))], which is the actual z3py code. The transformed code can be executed using z3py to obtain the final answer.</p>
<h1>G Examples of Error Cases of SatLM</h1>
<h2>G. 1 Examples of Unsatisfiable and Ambiguous Formulas</h2>
<p>We show examples of formulas that are unsatisfiable or have ambiguous answers in Figure 6, Figure 7, and Figure 8.</p>
<h2>UNSAT Formulas: GSM</h2>
<p>Q: If a rectangle has a width of 42 inches and an area of 1638 , how many rectangles of the same size would reach a length of 390 inches?</p>
<div class="codehilite"><pre><span></span><code>rectangle_width = 42
rectangle_area = 1638
rectangle_length = 390
rectangle_length = rectangle_area / rectangle_width
rectangles_needed = rectangle_length / rectangle_width
result = rectangles_needed
solve(result)
</code></pre></div>

<p>Q: Jason's dog has a tail that's half the length of its body, and a head that's $1 / 6$ the length of its body. If the dog is 30 inches long overall, how long is its tail?</p>
<div class="codehilite"><pre><span></span><code>dog_length = 30
tail_length = Variable()
head_length = Variable()
dog_length = tail_length + head_length
tail_length = dog_length <span class="gs">* 1 / 2</span>
<span class="gs">head_length = dog_length *</span> 1 / 6
result = tail_length
solver(result)
</code></pre></div>

<p>Figure 6: Examples of UNSAT formulas from GSM. Both examples are generated by code-davinci-002. UNSAT occurs when LLMs misunderstand the problem and output conflicting constraints (highlighted in red).</p>
<h2>AMBIG Formulas: GSM</h2>
<p>Q: The sum of the three numbers is 500 . If the first number is 200 , and the value of the second number is twice the value of the third number, find the value of the third number.</p>
<div class="codehilite"><pre><span></span><code>num1 = 200
num2 = Variable()
num3 = Variable()
num2 = num3 * 2
total = num1 + num2 + num3
result = num3
solve(result)
</code></pre></div>

<p>Q: The total for the sum and product of Elvie's age and Arielle's age are 131. If Elvie's age is 10, how old is Arielle?</p>
<div class="codehilite"><pre><span></span><code>elvie_age = 10
arielle_age = Variable()
sum_age = elvie_age + arielle_age
product_age = elvie_age * arielle_age
total_age = sum_age + product_age
result = arielle_age
solve(result)
</code></pre></div>

<p>Figure 7: Examples of AMBIG solutions from GSM. Both examples are generated by code-davinci-002. The ambiguous formulas are under-constrained due to failure in encoding certain constraints specified in the problem (highlighted in red), resulting in multiple possible answers.</p>
<h1>UNSAT Formulas: Clutrr</h1>
<p>Arlene and her husband Jeff went on a cruise. They had a wonderful time. Stephanie s father Jason loves his little princess even though she gets into a lot of trouble at school. Gloria's mother Ruth and brother Jeff were working in the kitchen, preparing soup. Stephanie, Ruth's daughter, was working outside in the garden. Q: How is [Jason] related to [Arlene]?
# [Arlene] and her husband [Jeff] went on a cruise. They had a wonderful time.
relation(Arlene, Jeff) = (wife, husband)
# [Stephanie]'s father [Jason] loves his little princess even though she gets into a lot of trouble at school.
relation(Stephanie, Jason) = (daughter, father)
# [Gloria]'s mother [Ruth] and brother [Jeff] were working in the kitchen, preparing soup.
relation(Gloria, Ruth) = (daughter, mother)
relation(Gloria, Jeff) = (daughter, brother)
# [Stephanie], [Ruth]'s daughter, was working outside in the garden.
relation(Stephanie, Ruth) = (daughter, mother)
# How is [Jason] related to [Arlene]?
solve(relation(Jason, Arlene))</p>
<h2>AMBIG Formulas: Clutrr</h2>
<p>Kirk loves talking to his grandfather Stanley on the phone. Paul's brother, Kirk, recently made it into town. Paul and his son Michael went to look at cars. Paul ended up buying the Mustang.
Q: How is [Stanley] related to [Michael]?
# [Kirk] loves talking to his grandfather [Stanley] on the phone.
relation(Kirk, Stanley) = (grandson, grandfather)
# [Paul]s brother, [Kirk], recently made it into town.
relation(Paul, Kirk) = (brother, brother)
# [Paul] and his son [Michael] went to look at cars.
relation(Paul, Michael) = (father, son) note: Michael is Kirk's son, not Paul's son
# How is [Stanley] related to [Michael]?
solve(relation(Stanley, Michael))
Figure 8: An example of UNSAT formulas (top) and an example of AMBIG formulas (bottom) from Clutrr. Both happen when the LLM misparses a constraint (highlighted in red). In particular, ambiguous formulas usually occur when the language description itself is highly ambiguous (see the note in the ambiguous formulas for an example).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://z3prover.github.io/api/html/namespacez3py.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
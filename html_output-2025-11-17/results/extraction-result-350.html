<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-350 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-350</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-350</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-259342896</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.01848v1.pdf" target="_blank">Embodied Task Planning with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e350.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e350.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TaPA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAsk PlAnning Agent (TaPA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grounded embodied task planner that finetunes a pre-trained LLM (LLaMA-7B) on a GPT-3.5-generated multimodal dataset and conditions generation on a detected object list to produce executable multi-step action plans for indoor tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TaPA (finetuned LLaMA-7B planner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A task planner implemented by finetuning LLaMA-7B on a synthetic multimodal dataset (triplets of scene object lists, natural-language instructions, and procedural action sequences) produced by GPT-3.5; at inference TaPA receives (1) a natural-language instruction and (2) a symbolic object list (from an open-vocabulary detector) and outputs a textual action sequence (atomic motion primitives).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TaPA embodied task planning benchmark (AI2-THOR based, 80 train / 20 eval scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step household instruction-following in realistic indoor scenes simulated in AI2-THOR; agent must generate executable sequences of atomic actions (Grasp, Move, Place, Pour, etc.) to satisfy a natural-language request, grounded to objects actually present in the scene as reported by perception.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>household tasks; multi-step planning; instruction following; object manipulation + navigation (planning stage)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (procedural action sequences grounded to object presence); limited spatial (no explicit spatial relations encoded in scene representation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training of LLaMA on text corpora + instruction finetuning on the GPT-3.5-generated multimodal dataset (object-list conditioned plans); perception module (Detic) supplies object-list at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning on synthetic multimodal dataset and prompted generation at inference (prompt + object list + instruction)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Procedural knowledge encoded as natural-language action sequences in model weights; scene knowledge represented symbolically as an object-name list (X_l) provided as text conditioning; no explicit spatial map or object-location coordinates encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>human-annotator judged plan success rate (percentage of generated plans judged executable by >=2/3 annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Average success rate 61.11% on 60 validation samples (per-room: Kitchen 28.57%, Living 84.21%, Bedroom 73.33%, Bathroom 58.33%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Produces longer, more complete multi-step procedural plans grounded to the provided object list; substantially reduces hallucinations (actions referencing non-existent objects) compared to ungrounded LMMs/LLMs; handles diverse, complex household procedures (e.g., sandwich making, cleaning) when target objects are present in the object list.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Fails when object-list contains false positives or misses (detection noise); remaining failure modes include counterfactual steps that violate physical rules (e.g., ordering of motion vs grasp) and still occasional hallucinations when object-list is noisy; lacks explicit spatial relations or positions so it does not reason about relative object locations or fine-grained navigation paths.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to GPT-3.5 (avg 54.73%), LLaVA (avg 22.43%), and base LLaMA (avg 5.96%); TaPA outperforms all listed baselines (TaPA avg 61.11%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations on scene-perception strategies show block-wise center (cluster centroids) multi-view image collection yields the best downstream plan success (highest object-list quality); traversal sampling produced more false positives and degraded planning; using ground-truth object lists in training improved planner robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Conditioning a (text-only) LLM on a symbolic object list produced by multi-view perception and finetuning on synthetic object-list-conditioned procedural data enables generation of more executable embodied plans; spatial relations and object locations are not represented — the model relies on an object-presence list and procedural knowledge encoded in language weights, so perception noise (false positives/missing items) and absence of explicit spatial encodings remain primary failure sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Task Planning with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e350.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e350.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (dataset synth)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (used for synthetic multimodal dataset generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive language model used to synthesize instruction-action-plan triplets conditioned on a provided object-name list; constrained prompting prevents generation of actions referencing objects outside the list.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large pre-trained autoregressive language model; in this work GPT-3.5 is prompted with a scene object-name list and few-shot rules to synthesize diverse natural-language instructions and corresponding step-by-step action plans (with explicit (Target:[Object]) tags) for dataset creation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic multimodal data generation for embodied planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate large-scale triplets (scene object-name list X_l, instruction X_q, action plan X_a) to serve as supervised training data for finetuning the LLM planner; prompts enforce use of only listed objects and request action-steps using atomic motions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>data generation; procedural knowledge synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (ensures actions reference objects present in provided list)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora (internal to GPT-3.5); few-shot prompt engineering provided by authors</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-/few-shot prompting with explicit rule constraints and examples</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Outputs procedural knowledge as natural-language action sequences (text); conditioned on symbolic object lists (text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>indirect: quality judged by inclusion in training data; in evaluation GPT-3.5 used as baseline planner with human-annotated success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When used as a planner baseline (given predicted object lists): average success rate 54.73% (Kitchen 28.57%, Living 73.68%, Bedroom 66.67%, Bathroom 50.00%) on the validation set.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Generates plausible, human-like procedural sequences and can be constrained by an object-list prompt to reduce out-of-list hallucinations during data generation and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Still produces hallucinations (referencing non-existent objects) and counterfactuals when not adequately grounded by accurate perception; lacks access to explicit spatial layouts and thus cannot reason about object positions beyond presence/absence.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed by TaPA finetuned planner (61.11% vs 54.73%); outperforms raw LLaMA and LLaVA baselines in some rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No internal ablation reported; role primarily as dataset synthesizer and baseline planner.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 readily encodes and expresses procedural knowledge via prompting, and can be constrained by an object-list prompt to reduce hallucinations, but grounding via an accurate perception-derived object list and further finetuning improves executability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Task Planning with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e350.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e350.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B (backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7B) pre-trained language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter pre-trained transformer LLM used as the backbone for TaPA; base LLaMA performs poorly on the embodied planning tasks until instruction finetuning with object-list-conditioned data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open pre-trained language model used as the starting point for the task planner; finetuned on the synthetic multimodal dataset to produce TaPA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Baseline and finetuned planner (TaPA uses finetuned LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate executable action sequences conditioned on a scene object-name list and a user instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following; multi-step procedural planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (after finetuning), minimal spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora; then instruction finetuning on the GPT-3.5-synthesized dataset for grounded planning</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot baseline prompting (poor), and supervised finetuning to produce TaPA</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Procedural knowledge in weights and prompted textual conditioning; scene provided as symbolic object list text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>human-annotated plan success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Base LLaMA (not finetuned) performed poorly: average success rate 5.96% (Kitchen 0.00%, Living 10.52%, Bedroom 13.33%, Bathroom 0.00%). Finetuning to produce TaPA yields large gains (TaPA avg 61.11%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>After finetuning, can generate grounded procedural plans when conditioned on object lists; before finetuning, tends to produce irrelevant or ungrounded content.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Without finetuning, generates irrelevant or unexecutable plans; also lacks explicit spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Base LLaMA much worse than GPT-3.5 and LLaVA until finetuned; finetuned LLaMA (TaPA) outperforms GPT-3.5 and LLaVA.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Finetuning on object-list-conditioned data is the critical component—transition from ~6% to ~61% average success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction finetuning on object-list-conditioned procedural data is necessary to adapt a general LLM for embodied, grounded planning; without this the model fails to produce executable plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Task Planning with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e350.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e350.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA (baseline LMM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language and Vision Assistant (LLaVA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multimodal model baseline that accepts a single image plus language prompt; in this work LLaVA is used as a comparator and performs poorly because a single image cannot represent an entire scene for multi-step planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large multimodal (vision+language) model that was used as a baseline; it receives a single image and an instruction and attempts to produce action sequences, but lacks multi-view scene coverage and explicit object-list conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Single-image grounded planning (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Attempt to produce executable action plans based on a single image of the scene and a language instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following; limited planning; visual question-answering-like input</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (implicit), but constrained by single-image view (spatial limited)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining and multimodal finetuning of LLaVA (external to this paper); in-paper usage as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompting with single-image input</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit multimodal embeddings combining a single image and text; no explicit multi-view object-list conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>human-annotated plan success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Average success rate 22.43% (Kitchen 14.29%, Living 42.11%, Bedroom 33.33%, Bathroom 0.00%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Can produce plausible steps when the single image contains the key objects and context for simple tasks (e.g., turning on a TV).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Fails on complex, multi-step household tasks because a single image lacks full scene coverage; higher hallucination rate (plans referencing non-existent objects) versus TaPA; poor handling of tasks requiring knowledge of object presence beyond single view.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms TaPA and GPT-3.5 in multi-step embodied planning due to limited perceptual input.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not ablated here, but authors note that single-image representation is insufficient vs. multi-view object-list grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multimodal models that accept single images are insufficient for long-horizon embodied planning in house-scale scenes; explicit, multi-view object-list grounding improves executability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Task Planning with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e350.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e350.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Detic (open-vocab detector)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Detic open-vocabulary object detection framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-vocabulary object detector used to extract symbolic object-name lists from multi-view RGB images; generalized to multi-view aggregation to produce the scene object list X_l used to condition the planner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Detic (open-vocabulary detector)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vision model for open-vocabulary detection (mapping visual regions to a large set of class names) used here to detect objects from collected multi-view RGB images; detections are aggregated (deduplicated) into a text object list supplied to the LLM planner.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-view object discovery for grounding language planner</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Collect images from multiple standing points and orientations, run open-vocabulary detection on each, then deduplicate detected class names to form an object-presence list for planner conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception / object recognition for embodied planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (object identity, categories, affordance proxies via class names); no explicit per-instance pose/location reported to planner</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained visual detection datasets and Detic architecture; extended to multi-view aggregation by authors</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>visual detection on collected RGB images followed by aggregation into symbolic list</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Symbolic object class names aggregated into a deduplicated list (text) representing object presence in the scene</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>indirect: downstream planning success rate depends on detection quality; also false-positive / false-negative impact analyzed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>No standalone detection accuracy numbers reported; impact observed indirectly: object-list noise (false positives from traversal strategy) reduced TaPA planning success; best downstream success achieved when image-collection strategy minimized redundancy and false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Detects a wide variety of object classes (open-vocabulary) and enables grounding of procedural plans by providing presence/absence signals.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>False positives (spurious detected object names) and missed detections degrade planner performance by causing hallucinations or missing required objects; traversal sampling produced redundant/false-positive lists hurting planning more than block-wise center sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared to other detectors in detail; authors selected Detic for open-vocabulary needs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Varying image collection strategies (and thus detection inputs) acted as an ablation: block-wise center (cluster centroids) outperformed traversal and naive random sampling because it balanced coverage and reduced redundant false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic object-presence lists produced by open-vocabulary detectors provide an effective and compact perceptual interface for text-only LLM planners, but detection noise (false positives/negatives) and the absence of spatial/location outputs limit planning reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Task Planning with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e350.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e350.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scene as object-list (X_l)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scene representation as deduplicated object-name list (X_l)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's central scene representation: a list of class names (no duplicates) describing all objects present in a scene, used as the visual-conditioning input to LLMs for grounded plan generation; notably this representation omits spatial relations or object positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Scene object-list representation (X_l)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A symbolic, language-format scene embedding consisting solely of deduplicated object class names (e.g., [table, chair, mug,...]); used both (a) as ground-truth conditioning for data synthesis during training and (b) as a detected input at inference after multi-view perception.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Object-presence-conditioned action planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide the planner with the set of objects available in the environment (without locations) so it can generate action sequences that only reference available objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>symbolic scene representation for instruction grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (presence/identity); enables procedural grounding; explicitly lacks spatial relations</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>extracted from simulator ground-truth at training time; provided by Detic detection and multi-view aggregation at inference</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>text conditioning of LLMs (object names listed in prompt/input)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit symbolic list of object names (natural-language tokens); no numeric coordinates, no relational graph, no spatial map.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>impact measured via downstream plan success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When used as ground-truth during training, enabled high-quality finetuning; at inference, quality depends on detection—best downstream planner success 61.11% when used with block-wise center detection; traversal-derived noisy lists lowered success substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively constrains procedural generation to avoid many hallucinated object references and enables LLMs to produce executable steps when objects are present.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Because it lacks object locations and relations, the planner cannot reason about where to move or the relative ordering of actions that depend on spatial configuration; prompts/rules explicitly instruct the agent not to reason about spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared implicitly to single-image input (LLaVA) and raw language-only LLMs; object-list conditioning outperforms single-image inputs for full-scene tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Providing ground-truth object lists during training (vs. predicted lists at inference) is important for robust finetuning; image-collection strategy for predicted lists acts as an effective ablation demonstrating sensitivity to perception noise.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A compact symbolic object-presence list is an effective perceptual interface for text-only LLM planners to ground procedural knowledge, but by design it omits spatial relations — improving spatial grounding would require adding location/relational information beyond the object names.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Task Planning with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e350.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e350.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Image collection strategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-view image collection strategies (traversal, random, overall center, block-wise center)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different strategies for choosing standing points and camera orientations to collect multi-view RGB images in an embodied scene; these strategies trade off coverage, redundancy, and detection noise and directly affect downstream object-list quality and planner performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Image collection strategies</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Procedures to sample (x,y,theta) viewpoints for collecting RGB observations: traversal (grid), random subset of grid points, single overall center, and block-wise center (cluster centroid) sampling; each viewpoint yields multi-view images which feed the open-vocabulary detector whose outputs are aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Perception sampling for object-list extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Collect RGB images from selected standing points and orientations; run detection on all images and aggregate detected class names to form the scene object list used by the planner.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception / sampling strategy for embodied planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-presence (object-relational); indirectly affects procedural planning via input quality; spatial coverage influences which objects are observed but strategies do not provide explicit spatial encodings to the planner.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>observation selection from simulator environment (AI2-THOR) and camera rotations; K-means clustering used for block-wise centers (layout prior).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>heuristic sampling (grid/traversal/random) and clustering-based block-wise centroids; camera rotated by discrete unit angles per location.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Noisy set of detected object names aggregated across views; number of images collected varies by strategy and affects redundancy/noise.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>downstream planner success rate; number of images collected (collection cost)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Block-wise center (cluster centroid) strategy achieved highest downstream planning success (reported TaPA result 61.11% avg when using partial/block-wise centers). Traversal sampling produced many images and more false positives and led to lower success; random and single-center had intermediate results. Exact best: block-wise center > random/overall center > traversal (in this dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Block-wise center sampling yields good scene coverage with relatively few images, reducing detection redundancy and false positives, improving object-list quality and planner success.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Dense traversal sampling increases false positives (redundant detections) that degrade planner quality; random sampling with insufficient coverage can miss objects required for tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared multiple sampling heuristics in Table 3; block-wise centroids were the most effective trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Varying grid size, sampling ratio, and camera rotation unit angle showed limited gains once coverage is sufficient; excessive image collection adds redundant/noisy detections that harm planning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Perception sampling strategy critically affects the symbolic object list fed to the language planner: appropriate multi-view, clustered sampling (block-wise centers) maximizes coverage while minimizing redundant false positives, improving grounded-plan executability without adding spatial coordinates to the planner input.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Task Planning with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as I Can, Not as I Say: Grounding Language in Robotic Affordances <em>(Rating: 2)</em></li>
                <li>LLM-Planner: Few-shot grounded planning for embodied agents with large language models <em>(Rating: 2)</em></li>
                <li>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>VirtualHome: Simulating household activities via programs <em>(Rating: 1)</em></li>
                <li>Tidybot: Personalized robot assistance with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-350",
    "paper_id": "paper-259342896",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "TaPA",
            "name_full": "TAsk PlAnning Agent (TaPA)",
            "brief_description": "A grounded embodied task planner that finetunes a pre-trained LLM (LLaMA-7B) on a GPT-3.5-generated multimodal dataset and conditions generation on a detected object list to produce executable multi-step action plans for indoor tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "TaPA (finetuned LLaMA-7B planner)",
            "model_size": null,
            "model_description": "A task planner implemented by finetuning LLaMA-7B on a synthetic multimodal dataset (triplets of scene object lists, natural-language instructions, and procedural action sequences) produced by GPT-3.5; at inference TaPA receives (1) a natural-language instruction and (2) a symbolic object list (from an open-vocabulary detector) and outputs a textual action sequence (atomic motion primitives).",
            "task_name": "TaPA embodied task planning benchmark (AI2-THOR based, 80 train / 20 eval scenes)",
            "task_description": "Multi-step household instruction-following in realistic indoor scenes simulated in AI2-THOR; agent must generate executable sequences of atomic actions (Grasp, Move, Place, Pour, etc.) to satisfy a natural-language request, grounded to objects actually present in the scene as reported by perception.",
            "task_type": "household tasks; multi-step planning; instruction following; object manipulation + navigation (planning stage)",
            "knowledge_type": "procedural + object-relational (procedural action sequences grounded to object presence); limited spatial (no explicit spatial relations encoded in scene representation)",
            "knowledge_source": "pre-training of LLaMA on text corpora + instruction finetuning on the GPT-3.5-generated multimodal dataset (object-list conditioned plans); perception module (Detic) supplies object-list at inference.",
            "has_direct_sensory_input": false,
            "elicitation_method": "fine-tuning on synthetic multimodal dataset and prompted generation at inference (prompt + object list + instruction)",
            "knowledge_representation": "Procedural knowledge encoded as natural-language action sequences in model weights; scene knowledge represented symbolically as an object-name list (X_l) provided as text conditioning; no explicit spatial map or object-location coordinates encoded.",
            "performance_metric": "human-annotator judged plan success rate (percentage of generated plans judged executable by &gt;=2/3 annotators)",
            "performance_result": "Average success rate 61.11% on 60 validation samples (per-room: Kitchen 28.57%, Living 84.21%, Bedroom 73.33%, Bathroom 58.33%).",
            "success_patterns": "Produces longer, more complete multi-step procedural plans grounded to the provided object list; substantially reduces hallucinations (actions referencing non-existent objects) compared to ungrounded LMMs/LLMs; handles diverse, complex household procedures (e.g., sandwich making, cleaning) when target objects are present in the object list.",
            "failure_patterns": "Fails when object-list contains false positives or misses (detection noise); remaining failure modes include counterfactual steps that violate physical rules (e.g., ordering of motion vs grasp) and still occasional hallucinations when object-list is noisy; lacks explicit spatial relations or positions so it does not reason about relative object locations or fine-grained navigation paths.",
            "baseline_comparison": "Compared to GPT-3.5 (avg 54.73%), LLaVA (avg 22.43%), and base LLaMA (avg 5.96%); TaPA outperforms all listed baselines (TaPA avg 61.11%).",
            "ablation_results": "Ablations on scene-perception strategies show block-wise center (cluster centroids) multi-view image collection yields the best downstream plan success (highest object-list quality); traversal sampling produced more false positives and degraded planning; using ground-truth object lists in training improved planner robustness.",
            "key_findings": "Conditioning a (text-only) LLM on a symbolic object list produced by multi-view perception and finetuning on synthetic object-list-conditioned procedural data enables generation of more executable embodied plans; spatial relations and object locations are not represented — the model relies on an object-presence list and procedural knowledge encoded in language weights, so perception noise (false positives/missing items) and absence of explicit spatial encodings remain primary failure sources.",
            "uuid": "e350.0",
            "source_info": {
                "paper_title": "Embodied Task Planning with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GPT-3.5 (dataset synth)",
            "name_full": "GPT-3.5 (used for synthetic multimodal dataset generation)",
            "brief_description": "A large autoregressive language model used to synthesize instruction-action-plan triplets conditioned on a provided object-name list; constrained prompting prevents generation of actions referencing objects outside the list.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "model_description": "A large pre-trained autoregressive language model; in this work GPT-3.5 is prompted with a scene object-name list and few-shot rules to synthesize diverse natural-language instructions and corresponding step-by-step action plans (with explicit (Target:[Object]) tags) for dataset creation.",
            "task_name": "Synthetic multimodal data generation for embodied planning",
            "task_description": "Generate large-scale triplets (scene object-name list X_l, instruction X_q, action plan X_a) to serve as supervised training data for finetuning the LLM planner; prompts enforce use of only listed objects and request action-steps using atomic motions.",
            "task_type": "data generation; procedural knowledge synthesis",
            "knowledge_type": "procedural + object-relational (ensures actions reference objects present in provided list)",
            "knowledge_source": "pre-training on large text corpora (internal to GPT-3.5); few-shot prompt engineering provided by authors",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-/few-shot prompting with explicit rule constraints and examples",
            "knowledge_representation": "Outputs procedural knowledge as natural-language action sequences (text); conditioned on symbolic object lists (text).",
            "performance_metric": "indirect: quality judged by inclusion in training data; in evaluation GPT-3.5 used as baseline planner with human-annotated success rate",
            "performance_result": "When used as a planner baseline (given predicted object lists): average success rate 54.73% (Kitchen 28.57%, Living 73.68%, Bedroom 66.67%, Bathroom 50.00%) on the validation set.",
            "success_patterns": "Generates plausible, human-like procedural sequences and can be constrained by an object-list prompt to reduce out-of-list hallucinations during data generation and planning.",
            "failure_patterns": "Still produces hallucinations (referencing non-existent objects) and counterfactuals when not adequately grounded by accurate perception; lacks access to explicit spatial layouts and thus cannot reason about object positions beyond presence/absence.",
            "baseline_comparison": "Outperformed by TaPA finetuned planner (61.11% vs 54.73%); outperforms raw LLaMA and LLaVA baselines in some rooms.",
            "ablation_results": "No internal ablation reported; role primarily as dataset synthesizer and baseline planner.",
            "key_findings": "GPT-3.5 readily encodes and expresses procedural knowledge via prompting, and can be constrained by an object-list prompt to reduce hallucinations, but grounding via an accurate perception-derived object list and further finetuning improves executability.",
            "uuid": "e350.1",
            "source_info": {
                "paper_title": "Embodied Task Planning with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LLaMA-7B (backbone)",
            "name_full": "LLaMA (7B) pre-trained language model",
            "brief_description": "A 7-billion-parameter pre-trained transformer LLM used as the backbone for TaPA; base LLaMA performs poorly on the embodied planning tasks until instruction finetuning with object-list-conditioned data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B",
            "model_size": "7B",
            "model_description": "Open pre-trained language model used as the starting point for the task planner; finetuned on the synthetic multimodal dataset to produce TaPA.",
            "task_name": "Baseline and finetuned planner (TaPA uses finetuned LLaMA-7B)",
            "task_description": "Generate executable action sequences conditioned on a scene object-name list and a user instruction.",
            "task_type": "instruction following; multi-step procedural planning",
            "knowledge_type": "procedural + object-relational (after finetuning), minimal spatial",
            "knowledge_source": "pre-training on text corpora; then instruction finetuning on the GPT-3.5-synthesized dataset for grounded planning",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot baseline prompting (poor), and supervised finetuning to produce TaPA",
            "knowledge_representation": "Procedural knowledge in weights and prompted textual conditioning; scene provided as symbolic object list text.",
            "performance_metric": "human-annotated plan success rate",
            "performance_result": "Base LLaMA (not finetuned) performed poorly: average success rate 5.96% (Kitchen 0.00%, Living 10.52%, Bedroom 13.33%, Bathroom 0.00%). Finetuning to produce TaPA yields large gains (TaPA avg 61.11%).",
            "success_patterns": "After finetuning, can generate grounded procedural plans when conditioned on object lists; before finetuning, tends to produce irrelevant or ungrounded content.",
            "failure_patterns": "Without finetuning, generates irrelevant or unexecutable plans; also lacks explicit spatial reasoning.",
            "baseline_comparison": "Base LLaMA much worse than GPT-3.5 and LLaVA until finetuned; finetuned LLaMA (TaPA) outperforms GPT-3.5 and LLaVA.",
            "ablation_results": "Finetuning on object-list-conditioned data is the critical component—transition from ~6% to ~61% average success.",
            "key_findings": "Instruction finetuning on object-list-conditioned procedural data is necessary to adapt a general LLM for embodied, grounded planning; without this the model fails to produce executable plans.",
            "uuid": "e350.2",
            "source_info": {
                "paper_title": "Embodied Task Planning with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LLaVA (baseline LMM)",
            "name_full": "Large Language and Vision Assistant (LLaVA)",
            "brief_description": "A large multimodal model baseline that accepts a single image plus language prompt; in this work LLaVA is used as a comparator and performs poorly because a single image cannot represent an entire scene for multi-step planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA",
            "model_size": null,
            "model_description": "A large multimodal (vision+language) model that was used as a baseline; it receives a single image and an instruction and attempts to produce action sequences, but lacks multi-view scene coverage and explicit object-list conditioning.",
            "task_name": "Single-image grounded planning (baseline comparison)",
            "task_description": "Attempt to produce executable action plans based on a single image of the scene and a language instruction.",
            "task_type": "instruction following; limited planning; visual question-answering-like input",
            "knowledge_type": "procedural + object-relational (implicit), but constrained by single-image view (spatial limited)",
            "knowledge_source": "pretraining and multimodal finetuning of LLaVA (external to this paper); in-paper usage as baseline",
            "has_direct_sensory_input": true,
            "elicitation_method": "prompting with single-image input",
            "knowledge_representation": "Implicit multimodal embeddings combining a single image and text; no explicit multi-view object-list conditioning.",
            "performance_metric": "human-annotated plan success rate",
            "performance_result": "Average success rate 22.43% (Kitchen 14.29%, Living 42.11%, Bedroom 33.33%, Bathroom 0.00%).",
            "success_patterns": "Can produce plausible steps when the single image contains the key objects and context for simple tasks (e.g., turning on a TV).",
            "failure_patterns": "Fails on complex, multi-step household tasks because a single image lacks full scene coverage; higher hallucination rate (plans referencing non-existent objects) versus TaPA; poor handling of tasks requiring knowledge of object presence beyond single view.",
            "baseline_comparison": "Underperforms TaPA and GPT-3.5 in multi-step embodied planning due to limited perceptual input.",
            "ablation_results": "Not ablated here, but authors note that single-image representation is insufficient vs. multi-view object-list grounding.",
            "key_findings": "Multimodal models that accept single images are insufficient for long-horizon embodied planning in house-scale scenes; explicit, multi-view object-list grounding improves executability.",
            "uuid": "e350.3",
            "source_info": {
                "paper_title": "Embodied Task Planning with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Detic (open-vocab detector)",
            "name_full": "Detic open-vocabulary object detection framework",
            "brief_description": "An open-vocabulary object detector used to extract symbolic object-name lists from multi-view RGB images; generalized to multi-view aggregation to produce the scene object list X_l used to condition the planner.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Detic (open-vocabulary detector)",
            "model_size": null,
            "model_description": "A vision model for open-vocabulary detection (mapping visual regions to a large set of class names) used here to detect objects from collected multi-view RGB images; detections are aggregated (deduplicated) into a text object list supplied to the LLM planner.",
            "task_name": "Multi-view object discovery for grounding language planner",
            "task_description": "Collect images from multiple standing points and orientations, run open-vocabulary detection on each, then deduplicate detected class names to form an object-presence list for planner conditioning.",
            "task_type": "perception / object recognition for embodied planning",
            "knowledge_type": "object-relational (object identity, categories, affordance proxies via class names); no explicit per-instance pose/location reported to planner",
            "knowledge_source": "pretrained visual detection datasets and Detic architecture; extended to multi-view aggregation by authors",
            "has_direct_sensory_input": true,
            "elicitation_method": "visual detection on collected RGB images followed by aggregation into symbolic list",
            "knowledge_representation": "Symbolic object class names aggregated into a deduplicated list (text) representing object presence in the scene",
            "performance_metric": "indirect: downstream planning success rate depends on detection quality; also false-positive / false-negative impact analyzed",
            "performance_result": "No standalone detection accuracy numbers reported; impact observed indirectly: object-list noise (false positives from traversal strategy) reduced TaPA planning success; best downstream success achieved when image-collection strategy minimized redundancy and false positives.",
            "success_patterns": "Detects a wide variety of object classes (open-vocabulary) and enables grounding of procedural plans by providing presence/absence signals.",
            "failure_patterns": "False positives (spurious detected object names) and missed detections degrade planner performance by causing hallucinations or missing required objects; traversal sampling produced redundant/false-positive lists hurting planning more than block-wise center sampling.",
            "baseline_comparison": "Not compared to other detectors in detail; authors selected Detic for open-vocabulary needs.",
            "ablation_results": "Varying image collection strategies (and thus detection inputs) acted as an ablation: block-wise center (cluster centroids) outperformed traversal and naive random sampling because it balanced coverage and reduced redundant false positives.",
            "key_findings": "Symbolic object-presence lists produced by open-vocabulary detectors provide an effective and compact perceptual interface for text-only LLM planners, but detection noise (false positives/negatives) and the absence of spatial/location outputs limit planning reliability.",
            "uuid": "e350.4",
            "source_info": {
                "paper_title": "Embodied Task Planning with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Scene as object-list (X_l)",
            "name_full": "Scene representation as deduplicated object-name list (X_l)",
            "brief_description": "The paper's central scene representation: a list of class names (no duplicates) describing all objects present in a scene, used as the visual-conditioning input to LLMs for grounded plan generation; notably this representation omits spatial relations or object positions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Scene object-list representation (X_l)",
            "model_size": null,
            "model_description": "A symbolic, language-format scene embedding consisting solely of deduplicated object class names (e.g., [table, chair, mug,...]); used both (a) as ground-truth conditioning for data synthesis during training and (b) as a detected input at inference after multi-view perception.",
            "task_name": "Object-presence-conditioned action planning",
            "task_description": "Provide the planner with the set of objects available in the environment (without locations) so it can generate action sequences that only reference available objects.",
            "task_type": "symbolic scene representation for instruction grounding",
            "knowledge_type": "object-relational (presence/identity); enables procedural grounding; explicitly lacks spatial relations",
            "knowledge_source": "extracted from simulator ground-truth at training time; provided by Detic detection and multi-view aggregation at inference",
            "has_direct_sensory_input": false,
            "elicitation_method": "text conditioning of LLMs (object names listed in prompt/input)",
            "knowledge_representation": "Explicit symbolic list of object names (natural-language tokens); no numeric coordinates, no relational graph, no spatial map.",
            "performance_metric": "impact measured via downstream plan success rate",
            "performance_result": "When used as ground-truth during training, enabled high-quality finetuning; at inference, quality depends on detection—best downstream planner success 61.11% when used with block-wise center detection; traversal-derived noisy lists lowered success substantially.",
            "success_patterns": "Effectively constrains procedural generation to avoid many hallucinated object references and enables LLMs to produce executable steps when objects are present.",
            "failure_patterns": "Because it lacks object locations and relations, the planner cannot reason about where to move or the relative ordering of actions that depend on spatial configuration; prompts/rules explicitly instruct the agent not to reason about spatial relations.",
            "baseline_comparison": "Compared implicitly to single-image input (LLaVA) and raw language-only LLMs; object-list conditioning outperforms single-image inputs for full-scene tasks.",
            "ablation_results": "Providing ground-truth object lists during training (vs. predicted lists at inference) is important for robust finetuning; image-collection strategy for predicted lists acts as an effective ablation demonstrating sensitivity to perception noise.",
            "key_findings": "A compact symbolic object-presence list is an effective perceptual interface for text-only LLM planners to ground procedural knowledge, but by design it omits spatial relations — improving spatial grounding would require adding location/relational information beyond the object names.",
            "uuid": "e350.5",
            "source_info": {
                "paper_title": "Embodied Task Planning with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Image collection strategies",
            "name_full": "Multi-view image collection strategies (traversal, random, overall center, block-wise center)",
            "brief_description": "Different strategies for choosing standing points and camera orientations to collect multi-view RGB images in an embodied scene; these strategies trade off coverage, redundancy, and detection noise and directly affect downstream object-list quality and planner performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Image collection strategies",
            "model_size": null,
            "model_description": "Procedures to sample (x,y,theta) viewpoints for collecting RGB observations: traversal (grid), random subset of grid points, single overall center, and block-wise center (cluster centroid) sampling; each viewpoint yields multi-view images which feed the open-vocabulary detector whose outputs are aggregated.",
            "task_name": "Perception sampling for object-list extraction",
            "task_description": "Collect RGB images from selected standing points and orientations; run detection on all images and aggregate detected class names to form the scene object list used by the planner.",
            "task_type": "perception / sampling strategy for embodied planning",
            "knowledge_type": "object-presence (object-relational); indirectly affects procedural planning via input quality; spatial coverage influences which objects are observed but strategies do not provide explicit spatial encodings to the planner.",
            "knowledge_source": "observation selection from simulator environment (AI2-THOR) and camera rotations; K-means clustering used for block-wise centers (layout prior).",
            "has_direct_sensory_input": true,
            "elicitation_method": "heuristic sampling (grid/traversal/random) and clustering-based block-wise centroids; camera rotated by discrete unit angles per location.",
            "knowledge_representation": "Noisy set of detected object names aggregated across views; number of images collected varies by strategy and affects redundancy/noise.",
            "performance_metric": "downstream planner success rate; number of images collected (collection cost)",
            "performance_result": "Block-wise center (cluster centroid) strategy achieved highest downstream planning success (reported TaPA result 61.11% avg when using partial/block-wise centers). Traversal sampling produced many images and more false positives and led to lower success; random and single-center had intermediate results. Exact best: block-wise center &gt; random/overall center &gt; traversal (in this dataset).",
            "success_patterns": "Block-wise center sampling yields good scene coverage with relatively few images, reducing detection redundancy and false positives, improving object-list quality and planner success.",
            "failure_patterns": "Dense traversal sampling increases false positives (redundant detections) that degrade planner quality; random sampling with insufficient coverage can miss objects required for tasks.",
            "baseline_comparison": "Compared multiple sampling heuristics in Table 3; block-wise centroids were the most effective trade-off.",
            "ablation_results": "Varying grid size, sampling ratio, and camera rotation unit angle showed limited gains once coverage is sufficient; excessive image collection adds redundant/noisy detections that harm planning.",
            "key_findings": "Perception sampling strategy critically affects the symbolic object list fed to the language planner: appropriate multi-view, clustered sampling (block-wise centers) maximizes coverage while minimizing redundant false positives, improving grounded-plan executability without adding spatial coordinates to the planner input.",
            "uuid": "e350.6",
            "source_info": {
                "paper_title": "Embodied Task Planning with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as I Can, Not as I Say: Grounding Language in Robotic Affordances",
            "rating": 2,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        },
        {
            "paper_title": "LLM-Planner: Few-shot grounded planning for embodied agents with large language models",
            "rating": 2,
            "sanitized_title": "llmplanner_fewshot_grounded_planning_for_embodied_agents_with_large_language_models"
        },
        {
            "paper_title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks",
            "rating": 2,
            "sanitized_title": "alfred_a_benchmark_for_interpreting_grounded_instructions_for_everyday_tasks"
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "VirtualHome: Simulating household activities via programs",
            "rating": 1,
            "sanitized_title": "virtualhome_simulating_household_activities_via_programs"
        },
        {
            "paper_title": "Tidybot: Personalized robot assistance with large language models",
            "rating": 1,
            "sanitized_title": "tidybot_personalized_robot_assistance_with_large_language_models"
        }
    ],
    "cost": 0.018248499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Embodied Task Planning with Large Language Models
4 Jul 2023</p>
<p>Zhenyu Wu wuzhenyu@bupt.edu.cn 
School of Automation
Beijing University of Posts and Telecommunications
China</p>
<p>Ziwei Wang 
Department of Automation
Tsinghua University
China</p>
<p>Beijing National Research Center for Information Science and Technology
China</p>
<p>Xiuwei Xu 
Department of Automation
Tsinghua University
China</p>
<p>Beijing National Research Center for Information Science and Technology
China</p>
<p>Jiwen Lu lujiwen@tsinghua.edu.cn 
Department of Automation
Tsinghua University
China</p>
<p>Beijing National Research Center for Information Science and Technology
China</p>
<p>Haibin Yan eyanhaibin@bupt.edu.cn 
School of Automation
Beijing University of Posts and Telecommunications
China</p>
<p>Embodied Task Planning with Large Language Models
4 Jul 20236985EFDBEA79ACAD89095777A24DBAE0arXiv:2307.01848v1[cs.CV]Embodied task planninglarge language modelsopen-vocabulary detection
Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments.Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences.In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models.Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions.The generated data is leveraged for grounded plan tuning of pre-trained LLMs.During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations.Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.</p>
<p>Introduction</p>
<p>Equipping embodied agents with general commonsense knowledge to accomplish complex tasks based on the natural language commands is desirable in many applications such as domestic service [1], medical treatment [2,3,4] and agricultural picking [5,6].Due to the limited training samples and diverse tasks in downstream applications, directly training an embodied agent across different deployment scenarios is infeasible.Recent progress in large language models (LLMs) [7,8,9,10] acquires rich commonsense knowledge from the vast web data, whose knowledge can be potentially leveraged by embodied agents to generate action plans for human requirements represented in natural language.</p>
<p>However, LLMs cannot perceive the surrounding scenes and may generate inexecutable actions due to the requirement of interacting with non-existed objects.For example, given the human command "Give me some wine", the generated action steps from GPT-3.5 are "pouring wine from the bottle to the glass".There may be only mugs instead of glasses in the realistic scenes, and the executable actions should be "pouring wine from the bottle to the mug".Therefore, grounding the task plan generated by LLMs to the physical world is necessary to construct embodied agents for complex task accomplishment.</p>
<p>To acquire executable task plans in the given physical scenes, many previous works filter or align the generated actions by considering the visual clues in the scene for the task of general manipulation of tabletop objects [11,12,13].In order to further diversify tasks in house-level environments, SayCan [14] and LLM-Planner [15] employ visual navigation to collect information in the house for the challenging grounded plan generation.Nevertheless, SayCan can only accomplish tasks in the kitchen scenarios and LLM-Planner performs planning in the ALFRED simulator [16] where most tasks are simple such as putting and placing.They both fail to satisfy the requirement of numerous complex tasks and diverse deployment scenarios in our daily life.</p>
<p>In this paper, we present a task planning agent called TaPA for embodied task plan grounding in physical scenes.The unreleased SayCan cannot be applied in diverse indoor scenarios, and the LLM-Planner in the ALFRED benchmark fails to generate plans for complex tasks due to the pre-defined simple instructions in the simulator.On the contrary, our agent can generate grounded plans without constraining task types and target objects.Therefore, Our agent acquires general commonsense knowledge to yield action steps for complex household tasks such as making sandwiches and setting tables, which provides the foundational instructions for the downstream navigation and manipulation process to deal with high-level requirements from humans.Figure 1 demonstrates the overall pipeline of our TaPA that generates the executable action steps by considering the scene information and the human instructions.Figure 2 shows the statistical difference between our TaPA and conventional ALFRED benchmark, where our tasks are much more complex with longer steps for accomplishment.More specifically, we first construct a multimodal dataset where each sample is a triplet of visual scenes, instructions, and corresponding plans.By leveraging the generated dataset, we finetune the pre-trained LLaMA [7] network by predicting the action steps based on the object list of the scene, which is employed as our task planner.For the acquisition of the object list during inference, the embodied agent effectively visits standing points to collect RGB images providing sufficient information in different views, and generalizes the open-vocabulary detector for multi-view images to acquire the list of existed objects.Our TaPA agent achieves higher success rate of the generated action plans compared with the state-of-the-art LLMs including LLaMA and GPT-3.5 and large multimodal models (LMMs) such as LLaVA [17].Our contributions can be summarized as follows:</p>
<p>• To the best of our knowledge, we propose the first benchmark for complex embodied task planning that is practical in realistic indoor deployment scenarios.</p>
<p>• We design a framework for large-scale multimodal dataset generation in order to train the task planner from pre-trained LLMs and construct a multimodal dataset containing 80 indoor scenes with 15K instructions and corresponding action plans.</p>
<p>• We evaluate different LLMs and LMMs for complex embodied task planning in our benchmark, and conduct the ablation study to select the optimal representation of visual scenes for executable action generation.</p>
<p>Related Work</p>
<p>Large pre-trained models: Large-scale pre-trained models have revolutionized the natural language processing (NLP) [18,19,20] and the computer vision [21,22,23] communities in recent years.</p>
<p>Benefiting from the vast training data and numerous parameters, the large pre-trained models acquire strong generalization ability across different deployment scenarios.For large language models, recent studies show that they not only perform well in NLP tasks, but also emerge the ability to master the rich knowledge about the realistic world with factual answers.Therefore, LLMs such as LLaMA [7], GPT-3 [24] are widely adopted to diverse tasks by interacting with input from other modalities such as visual feature learning [25,26], pseudo-code generation [27], tool usage [28] and math problem solving [29].For large vision models, objects in the open environments can be detected [23,30] or segmented [31] for scene understanding, where bounding boxes and masks are generated for all scenarios and visual features are aligned with text embedding for category assignment.To learn the joint embedding space of language and vision for multimodal tasks, CLIP [32] leverages contrastive learning to minimize the distance between similar image-text pairs.LLaVA [17] synthesized a multimodal dataset with images, captions and bounding boxes in the tasks of conversation, detailed description and complex reasoning, so that the instructional tuning of LLMs acquires general-purpose instruction-following visual agent.In this paper, we leverage LLMs to generate executable plans for embodied tasks with the visual information acquired from the open-vocabulary detection models.</p>
<p>Language model grounding for embodied tasks: An embodied agent not only requires active exploration [33], manipulation [34], and scene perception [35,36] as well as embodied task planning ability.Embodied task planning aims to generate executable action steps in the given environments, where action plans are generated from grounded LLMs by receiving information from the surrounding environments [37,38,39] or prompt engineering [40].For the former, agents acquire the feedback from environments by interacting with the objects to ground the task plan.Li et al. [41] employed LLMs as a general scaffold for interactive decision-making in complex tasks, where the generated policies were grounded to the given environments for executable implementation according to the action feedback.For prompt engineering, researchers carefully design the language prompts for LLMs to guide them to ground the generated content.Huang et al. [40] prompted simple examples of task instructions and corresponding actions for LLMs to produce plausible task plans, and filtered out executable subsets by constructing mapping with semantic similarity.To enable the LLMs to be aware of the surrounding scenes with boosted plan plausibility, Brohan et al. [14] and Song et al. [15] extracted the visual information of the scene by latent features or object names for LLMs, where the generated plans were limited to the one with the highest success rate for task completion.However, these works can only accomplish simple tasks such as placing and putting in the VirtualHome [42] or ALFRED simulators, which fail to be applied to practical deployment scenarios with diverse complex tasks.</p>
<p>Approach</p>
<p>In this section, we first describe the construction of the multimodal instruction dataset that is leveraged to tune our TaPA task planner, and then describe the details of grounding embodied task plans to the visual scene with image collection and open-vocabulary detection.</p>
<p>Data Generation of Embodied Task Planning</p>
<p>Although large vision-language models (VLM) [17,43] and large multimodal models [44,45,46,47,48] have achieved surprising performance on a wide range of complex perception tasks, embodied task planning that is grounded to the realistic indoor scenes still remains challenging due to the lack of the large-scale multimodal dataset to train the planning agent.Considering the recent success of GPT models on high-level human-like reasoning, we leverage GPT-3.5 with the presented scene representation and designed prompt to generate the large-scale multimodal dataset for our planning agent tuning.Given an embodied 3D scene X s , we directly utilize the class names of all objects as the representation of the scene which is denoted as X l .All duplicate names are removed to provide scene information for the LLM such as X l = [table, chair, keyboard, ...].Based on the above scene information, a simple approach used in AL-FRED benchmark [16] to generate the multimodal instruction following the dataset for embodied task plans is to artificially design a series of instructions with corresponding step-by-step actions.However, the hand-crafted design requires extremely high annotation cost to generate complex task plans that are practical for realistic service robots such as tidying up the bathroom and making sandwiches.To efficiently generate the large-scale complex instructions X q and executable corresponding plans X a for the given 3D scene, we design a prompt to simulate the scenarios of embodied task planning for GPT-3.5 to automatically synthesize data based on the object name list X l .As shown in Table 5 of the supplementary materials, our prompt describes the definition of embodied task planning, the requirements and several examples of generated instructions and corresponding action plans.Specifically, the prompt designs a conversation between the service robot and humans to generate executable instructions and actions, which simulates the exploration of robots in the embodied environments and provides the requirements from humans.The generated instructions are diverse including requests, commands and queries, where only instructions with explicitly executable actions are added to our dataset.Meanwhile, we emphasize that the target object of the generated action should be constrained within the object list X l to mitigate the object hallucination that leads to inexecutable plans.For the object list leveraged in the prompt for dataset generation, we directly utilize the groundtruth label of existed instances in the scene.In Table 1, we show examples of the generated sample containing the object name list of the scene, the instruction and the executable action steps.In embodied task planning, the agent can only get access to the visual scene containing all interactive objects without the groundtruth object list.Therefore, we construct the multimodal dataset by defining triplets for each sample as X = (X v , X q , X a ).For the training stage of the task planner, we directly leverage the groundtruth object list for each scene to avoid the influence of inaccurate visual perception.For the inference phase, the extended open-vocabulary object detector predicts the list of all existed objects in the scene.</p>
<p>We employ the AI2-THOR simulator [49] as the embodied environment for our agent, where we split the scenes with 80 for training and 20 for evaluation.To enlarge the scale and diversity of instructions and action steps in training samples for effective task planner finetuning, we expand the original 80 training scenes to 6400 training scenes by directly modifying the groundtruth object list.For each scene type, we initially acquire the list of objects that possibly appear in this type of scene by enumerating all rooms in the same room type.Then we randomly substitute existed objects with other ones that possibly exist in the same room type and are not observed.The plausibility constraint aims to prevent generating counterintuitive objects for given scene types.We collected 15K samples for training and leverages another 60 triplets for evaluation with our multimodal data generation framework.</p>
<p>Grounding Task Plans to Surrounding Scenes</p>
<p>In order to ground the embodied task plan to the physical world with feasibility constraints, it is necessary to accurately obtain the object list in the scene without instance missing or false positives.</p>
<p>We generalize the open-vocabulary object detector for object list acquisition since novel objects unseen in detector training may appear in the deployment scenarios.As shown in Figure 1, the agent collects RGB images in different locations to perceive the visual scenes to discover existed objects.We design several image collection strategies to explore the surrounding 3D scenes.The location selection criteria contains traversal positions, random positions, the overall center point and block-wise center points, and the agent rotates the camera to obtain multi-view images for each location selection criteria.Therefore, we formally write the image collection strategies S in the following:
S = {(x, y, θ)|(x, y) ∈ L(λ, A), θ = kθ 0 }(1
) where (x, y, θ) represents the location and camera orientation.L(λ, A) means the location selection criteria with the hyperparameter λ and all sampled locations are required within the achievable area A. The unit angle for camera rotation is set to θ 0 , and k is an integer so that the agent collects visual clues in different directions of the scene.The hyperparameter that all location selection criteria share is the grid side length, where we divide the achievable area into grids.Traversal positions choose all grid points for RGB image collection.Random positions only randomly selected part of the grid points for visual information perception, and the hyperparameters also contain the ratio of sampled grid points.The overall center point stands for the center of the whole scene without any hyperparameters.The block-wise center points aim to choose the center of each division in the scene to efficiently acquire fine-grained visual information.Inspired by [50,51], clustering methods can effectively divide the entire scene into several sub-regions to improve the performance of perception, so that the prior information of the room layout is embedded into the image collection strategy with the K-means clustering method.Meanwhile, we employ within cluster sum of squared errors (WCSS) principle to select the optimal number of clusters for each scene.Compared to the images collection strategy of traversal points, the block-wise center point only traverses centroids of the subregions to acquire sufficient visual information.</p>
<p>The embodied task planner requires the information of all existed objects in the scene to generate executable action steps, where we generalize the open-vocabulary object detector to the collected multi-view RGB images for the object list acquisition.The predicted object list Xl for the scene is acquired by removing the duplicated object names in the detection results of multi-view images:
Xl = Rd i D(I i ) (2)
where Rd is the operation of removing duplicate object names and D(I i ) represent the detected object names for the i th RGB image collected in the scene.With our inference prompt P in shown in Table 5 of the supplementary material, the human instruction X q and the predicted object list X l are considered in our TaPA to generate the executable action plans X a :
X a = TaPA(P in , Xl , X q )(3)
By combining the perception results of existed objects Xl with the instructions X q , TaPA will give the executable action sequence X a to complete the requirements of X q according to the realistic scene constraint.According to our empirical study, we chose the block-wise center point for multi-view RGB image collection.The grid size in our location selection criteria is set to 0.75 and the unit angle for camera rotation is 2π/3.</p>
<p>Experiment</p>
<p>In this section, we conduct extensive experiments with our generated multimodal dataset where the visual scenes come from the simulator AI2-THOR.We first introduce the evaluation metric of the generated action plans.Then we compare our TaPA with the state-of-the-art LLMs and LMMs to show our superiority in embodied task planning.To further explore the effectiveness of different scene information embedding approaches, we evaluate various image collection strategies in our ablation study.We employ the LLaMA-7B pre-trained language model as the backbone of our task planner, which is finetuned with our generated multimodal dataset.The maximum token number of our task planner is set to 512, and we leverage the Detic open-vocabulary object detection framework to collect the information of existed objects.All experiments were accelerated by 8 GTX 3090 GPUs.</p>
<p>Evaluation Metrics</p>
<p>For the deployment of our TaPA, we feed the instructions and the predicted object list in the scene for the task planner to generate the action steps.We hired 30 researchers in large multimodal models as volunteers to vote for the success of the generated action plans, and each generated action plan is evaluated by three volunteers.The volunteers are shown with the groundtruth object list of each scene, the instruction and the generated action plans, where the volunteers should judge whether implementing the action steps can successfully completes the instructions.There are two types failure cases including counterfactuals and hallucination.Counterfactuals indicate that the plans violate the physical rules in the real world (e.g.grasping the doorknob before moving to the door), and hallucination means the action plans require the agent to interact with objects that do not exist in the scene.An exceptional case is that the interacting objects can be part of the object existed in the scene (e.g.trash can lid and trash can) or a synonym of an object (e.g.mug and cup).The generated action plans are considered to be successful if at least two volunteers out of three regarding the steps can be implemented to satisfy the human instruction.The volunteers were also requested to annotate the type of failure for the unsuccessful cases.We report the ratio of successful cases for different scene types and plan generation models.</p>
<p>Experimental Results</p>
<p>In this section, we compare our TaPA method with the state-of-the-art LLMs including LLaMA and GPT-3.5 and LMMs containing LLaMA on 60 validation samples, and the success rate of the generated action steps from different methods are shown in Table 2. TaPA achieves optimal performance among all large models on all four scenes including kitchen, living room, bedroom and bathroom, and the average success rate of TaPA is 6.38% (61.11% vs. 54.73%)higher than GPT-3.5 on the task of embodied task planning after instruction finetuning.</p>
<p>Since agents in kitchen scenes usually deal with complex cooking instructions in more steps, the performance of current large models is lower than in other room types.Meanwhile, the poor performance of LLaVA reflects the fact that the overall scene information cannot be represented by a single image in the visual question answering task, and the insufficient scene information leads to a low success rate of task planning.The success rate of LLaMA is far below other methods, which even cannot succeed in completing tasks in the kitchen and bathroom scenes without instruction finetuning.Figure 2 illustrates the percentage of failure cases in embodied task planning for different large models.Counterfactuals represent that the generated actions violet the physical rule in the realistic world, and hallucinations mean the actions aim to interact with objects that are not in the scene.TaPA is embedded with more expert knowledge in embodied task planning after instruction finetuning, which has the lowest percentage of counterfactual occurrences.Moreover, TaPA can better understand the list of input objects, with a 26.7% (40.0% vs. 13.3%) and 5.0% (18.3% vs. 13.3%)decrease in the percentage of hallucination cases compared to LLaVA and GPT-3.5 respectively.</p>
<p>We also investigate the effectiveness of different image collection strategies that perceive the scene information by acquiring the list of existed objects.Specifically, we employ location selection criteria including random positions, traversal positions, the overall center point and block-wise center points with various hyperparameters containing the grid size and the sampling ratio in random positions, and we also change the unit angle for camera rotation.The success rate of different image collection strategies is demonstrated in Table 3.We also show the number of collected images for various criteria to reveal the collection and computational cost.For the traversal positions, reducing the grid Locate the TV remote control, which is typically placed on a table or mounted on the wall near the TV.</p>
<p>2.Point the remote control at the TV and press the power button.The power button is often located on the top or bottom of the remote control.</p>
<p>3.If the TV doesn't power on immediately, press the channel or input button to select the correct input source.The input button is usually located on the right side of the remote control or may be a menu button that allows you to access the settings.4.The TV should now turn on and display the channel or content you have selected.</p>
<p>User I want to watch TV LLaMA Sit in a comfortable chair, place a pillow on the floor, and turn on the TV.</p>
<p>Open up the drawer to get a remote control and place the TV on a  Among all room types, the success rate in the kitchen scenes is the lowest since the instruction for kitchen tasks (e.g.sandwich making) usually requires long plans with much more action steps.</p>
<p>With the increase of the interacted objects in the task plan, the probability of hallucination is higher so that the plans are more likely to fail.On the contrary, the success rate of tasks in the living rooms is high due to the simple instructions (e.g.turning off lights).By observing the success rate of kitchen tasks across different location selection criteria, false positives in object detection that usually appear in traversal location selection criteria degrade the performance most significantly.Since the object list is redundant, the complex tasks in kitchen scenarios are more prone to the noise in the object list.We also show an example of generated action steps from different large models for the given scene in Table 4.The scene is demonstrated in the top-down view, and we also provide the groundtruth object list for reference.The content from LLaMA is irrelevant to the human instructions, while LLaVA provides plans that are not executable due to the non-existed objects.Although GPT-3.5 can also yield plausible embodied task plans, the action steps from our TaPA are more complete and more consistent with human values.</p>
<p>Conclusion</p>
<p>In this paper, we have presented a task planning agent called TaPA for embodied task planning, where the executable action steps are generated for subsequent robot navigation and manipulation to complete human instructions.We first construct a multimodal dataset where each sample is a triplet including the visual scenes, the instructions and the corresponding plans.The dataset is generated with GPT-3.5 by considering the list of all objects in the scene and the designed text prompt, which is leveraged to tune the instruction model to generate executable actions.For inference, we collect multi-view RGB images in different achievable locations, and leverage an open-vocabulary object detection framework to discover the object list of the scene for the finetuned instruction model.The statistics of our collected multimodal dataset indicate that our tasks are much more complex than conventional benchmarks on instruction-following tasks with longer implementation steps, and the extensive evaluation results show that our TaPA outperforms the state-of-the-art LLMs and LMMs on the plausibility of generated action plans.</p>
<p>is necessary to check if there is an alternative item to replace it.An exceptional case is that the interacting objects can be part of the existing objects or a synonym of an object.We also provide some examples to standardize the form of the input and output for the generated data.</p>
<p>Figure 1 :
1
Figure 1: Our embodied task planning framework collects multiple RGB images from various standing points and viewpoints.Utilizing an open vocabulary detector generates a list of objects existed in the scene.Combining human instructions and the predicted object list, our TaPA generates executable action plans for navigation or manipulation tasks.</p>
<p>Figure 2 :
2
Figure 2: Statistical comparison of TaPA and ALFRED dataset.The pie chart shows the top 20 frequently appearing verbs (inner circle) and the corresponding top 4 nouns (outer circle) for each verb.The bar chart shows the percentage of instructions with different numbers of implementation actions, where TaPA contains more complex instructions compared to ALFRED.</p>
<p>Figure 3 :
3
Figure 3: The pipeline of embedding the scene information for the LLM to generate executable actions.The image collection strategies to acquire the list of all existed objects in the scene include random sampling, traversal sampling, the overall center point and block-wise center points, where the object list is leveraged as the condition for action planning.The dashed circles in different colors represent grids in various blocks for block-wise center point selection.</p>
<p>Can you clean the sink and the toilet, please?Can you make me a sandwich?Task planning: Task planning: Step 1: Grasp a sponge Step 1. Grasp a plate Step 2: Move to the sink Step 2. Grasp the knife Step 3: Wet the sponge Step 3. Grasp a piece of bread Step 4: Scrub the sink Step 4. Move the knife to the bread and slice it Step 5: Rinse the sponge Step 5. Grasp another piece of bread Step 6: Grasp a towel Step 6. Move the knife to the bread and slice it Step 7: Dry the sink Step 7. Grasp a lettuce Step 8: Move to the toilet Step 8. Tear the lettuce and place it on the plate Step 9. Grasp a scrub brush Step 9. Grasp a tomato Step 10.Scrub the toilet bowl Step 10.Slice the tomato and place it on the plate Step 11.Place the scrub brush back in its place Step 11.Move the two slices of bread to the plate</p>
<p>Qualitative results, Living room: User I want to watch TV User I want to watch TV TaPA Step 1. Move closer to the television set Step 2. Grasp the TV remote control Step 3. Turn on the TV Step 4. Adjust the volume to a suitable level Step 5. Select a TV show or movie Step 6. Watch TV GPT-3.5 Step 1. Move closer to the TV Step 2. Grasp the remote control Step 3. Rotate the remote control to point at the TV Step 4. Press the power button to turn on the remote control User I want to watch TV LLaVA 1.</p>
<p>Table 1 :
1
Examples of the generated multimodal triplet data including visual scenes, instructions and the corresponding plans.</p>
<p>Table 2 :
2
Comparison of different LLMs and LMMs on the task of embodied task planning.For the prompt of baseline methods, LLaMA and LLaVA both employ the same prompt in the their original finetuning phase, while GPT-3.5 adopts the same prompt of TaPA for multimodal data generation.
MethodKit.Living.Bed.Bath.Avg.LLaVA14.2942.1133.330.0022.43GPT-3.528.5773.6866.6750.0054.73LLaMA0.0010.5213.330.005.96TaPA28.5784.2173.3358.3361.11</p>
<p>Table 3 :
3
The average execution success rate of generated action steps for different RGB image collection strategies in scene perception.G represents the side length of grids in location selection.∆θ represents the unit angle of camera rotation.N represents the ratio of randomly selected points compared to all grid points in achievable area.
Strategy and Parameters#ImagesKit.Living.Bed.Bath.Avg.G=0.25, D=60782.414.2973.6846.6733.3341.99TraversalG=0.25, D=120 G=0.75, D=60391.2 80.714.29 28.5773.68 73.6853.33 46.6750.00 33.3347.83 45.56G=0.75, D=12040.414.2963.1660.0041.6744.78N=1%, D=606.028.5778.9526.6750.0046.05RandomN=1%, D=1203.021.4373.6846.6750.0047.95(G=0.75)N=75%, D=6063.035.7173.6853.3325.0046.93N=75%, D=12031.528.5773.6853.3333.3347.23Layout PrioriOverall Center6.028.5768.4233.3358.3347.16(G=0.75,D=60)Partial Center23.128.5784.2173.3358.3361.11</p>
<p>table.Adjust the volume on the speakers.Locate the plug for the television and plug it in.Place the bowl of objects on the table next to the TV.</p>
<p>(Objects: chair, sofa, pen, figurine, table, dog, lampshade, doorknob, bed, toy, drawer, person, statue (sculpture), flowerpot, stool, monitor, (computer equipment), speaker, computer monitor, . . . . . . )</p>
<p>Table 4 :
4
Visualization of generated action plans by different large models.The inputs of LLaMA and GPT-3.5 are the predicted object list by our visual perception module, while LLaVA generates the action steps only based on one single image.
Comparing all location selection criteria, block-wise center points achieve the highest success ratebecause of the effective representation of the ex-isted objects in the scene. Block-wise center pointsobserve the scene with the high coverage rate, whileonly a few RGB images are collected for scene rep-resentation. Therefore, sufficient scene informationis captured by the acquired object list without re-dundancy. The performance of random positionsand the overall center point is similar because thescale of scenes in AI2-THOR is small and one im-age collection location can also receive sufficientinformation. The traversal positions obtain the low-est success rate since collecting excess images lead
size significantly increases the image collection and the computational cost due to the numerous RGB images, while the average success rate remains similar (47.83 vs. 44.78)because the large grid size can collect images with sufficient information of the small-scale scenes from AI2-THOR.Similar reasons result in the phenomenon for random positions that increasing the sampling ratio and reducing the unit angle for camera rotation by collecting images in more locations cannot boost the success rate (47.95 vs. 47.23,46.93 vs. 47.23).Since the traversal positions with small grid sizes (G=0.25)collects extremely large number of images, decreasing the unit angle for camera rotation significantly decreases the success rate because the redundant object list degrades the planning capacity of LLMs. Figure 4: The percentage of different failure cases in embodied task planning for various large models.</p>
<p>Rule description:You are an indoor service robot named Garybot and you are inside a room.What you see is provided with a list of objects that contains all the objects in the room you are in.The location of the objects in the list you are guided in advance, without reasoning about the spatial relations of the objects.Execute all the instructions as you are located in the room.Design a conversation between you and the person you are serving in the room.The answer should be the tone of the service robot located in the room and performing the action specifically.The generated instructions can be described in different tones.Ask for various instructions and give the corresponding series of actions with a maximum of 15 steps.Only include instructions for their corresponding actions only utilizing atomic motions (Grasp, Release, Lift, Place, Rotate, Push, Pull, Align, Press, Pour, Move):(1) Generate operation instructions using only the objects in the list with the actions that must be performed to complete the operating instructions;(2) Do not generate any instructions or actions that cannot be executed with confidence;(3) Do not generate any instructions or actions with (Target: [Object]), [Object] is outside the list of objects.Again, the object being manipulated cannot be located outside the list.Please double-check that Target: [Object] is in the list at each step and that [Object] is in the list.When evaluating the existence of [Object], consider its original part or component, its function, and whether it can be replaced by an object in the list, and if it is satisfied, you can iterate over each element in the list to find an alternative and replace [Object].Few-shot samples:List of objects: [wine, cup, glass, remote control, TV, table, desk, chair] Generate the instruction: Give me a drink Necessary actions:Step 1. Grasp a bottle of wine (Target: wine)Step 2. Grasp a glass (Target: bowl) Step 3. Place the cup on the table (Target: glass, table)Step 4. Pour the wine into the glass (Target: wine, glass)Step 5. Grasp the glass with wine (Target: glass) Step 6. Move to the person and hand over it Step 7. Done Generate the instruction: Please turn on the TV Necessary actions:Step 1. Grasp the remote control (Target: remote control)Step 2. Move closer to the TV (Target: TV) Step 3. Rotate the remote control to point at the TV (Target: remote control, TV)Step 4. Press the power button to turn on the remote control (Target: remote control)Step 5. DonePrompt for training and inference:Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Instruction: Xq.Input: X l .Response: Xa.Supplementary MaterialThe prompts utilized to generate the instruction-following dataset from GPT-3.5 are illustrated in Table5.Specifically, we set a specific work scene for GPT-3.5 and indicate the need to generate instructions and corresponding actions by the agent itself.We also set the rules to constrain the instructions generated by GPT-3.5 with improved executability and confidence.Meanwhile, we require GPT-3.5 to add an additional (Target: [Object]) query to each generated action to further check for hallucinations.If the interacting object in the generated plan is not in the input X l , it
J Wu, R Antonova, A Kan, M Lepert, A Zeng, S Song, J Bohg, S Rusinkiewicz, T Funkhouser, arXiv:2305.05658Tidybot: Personalized robot assistance with large language models. 2023arXiv preprint</p>
<p>C Li, C Wong, S Zhang, N Usuyama, H Liu, J Yang, T Naumann, H Poon, J Gao, arXiv:2306.00890Llava-med: Training a large language-and-vision assistant for biomedicine in one day. 2023arXiv preprint</p>
<p>Z Zhao, S Wang, J Gu, Y Zhu, L Mei, Z Zhuang, Z Cui, Q Wang, D Shen, Chatcad+, arXiv:2305.15964Towards a universal and reliable interactive cad using llms. 2023arXiv preprint</p>
<p>Y Sun, C Zhu, S Zheng, K Zhang, Z Shui, X Yu, Y Zhao, H Li, Y Zhang, R Zhao, arXiv:2305.15072Redefining pathology through generative foundation ai assistant for pathology. 2023arXiv preprint</p>
<p>Chatgpt for robotics: Design principles and model abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, Microsoft Auton. Syst. Robot. Res. 2202023</p>
<p>How can llms transform the robotic design process?. F Stella, C Della Santina, J Hughes, Nature Machine Intelligence. 2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Llamaadapter: Efficient fine-tuning of language models with zero-init attention. R Zhang, J Han, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao, Y Qiao, arXiv:2303.161992023arXiv preprint</p>
<p>B Peng, C Li, P He, M Galley, J Gao, arXiv:2304.03277Instruction tuning with gpt-4. 2023arXiv preprint</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.105922023arXiv preprint</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning. PMLR2022</p>
<p>R3m: A universal visual representation for robot manipulation. S Nair, A Rajeswaran, V Kumar, C Finn, A Gupta, arXiv:2203.126012022arXiv preprint</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, Conference on Robot Learning. PMLR2022</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, Conference on Robot Learning. PMLR2023</p>
<p>C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, arXiv:2212.04088Llm-planner: Fewshot grounded planning for embodied agents with large language models. 2022arXiv preprint</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>H Liu, C Li, Q Wu, Y J Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J D , M.-W C Kenton, L K Toutanova, Proceedings of naacL-HLT. naacL-HLT20191</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, arXiv:2304.026432023Segment anything. arXiv preprint</p>
<p>Bridging the gap between object and image-level representations for open-vocabulary detection. H Bangalath, M Maaz, M U Khattak, S H Khan, F Shahbaz Khan, Advances in Neural Information Processing Systems. 202235</p>
<p>Detecting twenty-thousand classes using image-level supervision. X Zhou, R Girdhar, A Joulin, P Krähenbühl, I Misra, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerOctober 23-27, 2022. 2022Proceedings, Part IX</p>
<p>Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines. L Floridi, M Chiriatti, 202030</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, arXiv:2301.125972023arXiv preprint</p>
<p>Nlx-gpt: A model for natural language explanations in vision and vision-language tasks. F Sammani, T Mukherjee, N Deligiannis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, Toolformer, arXiv:2302.04761Language models can teach themselves to use tools. 2023arXiv preprint</p>
<p>Solving math word problems concerning systems of equations with gpt-3. M Zong, B Krishnamachari, Proceedings of the Thirteenth AAAI Symposium on Educational Advances in Artificial Intelligence. the Thirteenth AAAI Symposium on Educational Advances in Artificial Intelligence2022</p>
<p>Y Zang, W Li, J Han, K Zhou, C C Loy, arXiv:2305.18279Contextual object detection with multimodal large language models. 2023arXiv preprint</p>
<p>G Ghiasi, X Gu, Y Cui, T.-Y Lin, arXiv:2112.12143Open-vocabulary image segmentation. 2021arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Smart explorer: Recognizing objects in dense clutter via interactive exploration. Z Wu, Z Wang, Z Wei, Y Wei, H Yan, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>Ge-grasp: Efficient target-oriented grasping in dense clutter. Z Liu, Z Wang, S Huang, J Zhou, J Lu, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>Dspdet3d: Dynamic spatial pruning for 3d small object detection. X Xu, Z Sun, Z Wang, H Liu, J Zhou, J Lu, arXiv:2305.037162023arXiv preprint</p>
<p>Back to reality: Weakly-supervised 3d object detection with shape-guided label enhancement. X Xu, Y Wang, Y Zheng, Y Rao, J Zhou, J Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>A persistent spatial semantic representation for high-level natural language instruction execution. V Blukis, C Paxton, D Fox, A Garg, Y Artzi, Conference on Robot Learning. PMLR2022</p>
<p>Piglet: Language grounding through neuro-symbolic interaction in a 3d world. R Zellers, A Holtzman, M Peters, R Mottaghi, A Kembhavi, A Farhadi, Y Choi, arXiv:2106.001882021arXiv preprint</p>
<p>Grounding language to autonomously-acquired skills via goal generation. A Akakzia, C Colas, P.-Y Oudeyer, M Chetouani, O Sigaud, arXiv:2006.071852020arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>Pre-trained language models for interactive decision-making. S Li, X Puig, C Paxton, Y Du, C Wang, L Fan, T Chen, D.-A Huang, E Akyürek, A Anandkumar, Advances in Neural Information Processing Systems. 202235</p>
<p>Virtualhome: Simulating household activities via programs. X Puig, K Ra, M Boben, J Li, T Wang, S Fidler, A Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Otter: A multi-modal model with in-context instruction tuning. B Li, Y Zhang, L Chen, J Wang, J Yang, Z Liu, arXiv:2305.037262023arXiv preprint</p>
<p>C Lyu, M Wu, L Wang, X Huang, B Liu, Z Du, S Shi, Z Tu, arXiv:2306.09093Macaw-llm: Multimodal language modeling with image, audio, video, and text integration. 2023arXiv preprint</p>
<p>Q Ye, H Xu, G Xu, J Ye, M Yan, Y Zhou, J Wang, A Hu, P Shi, Y Shi, arXiv:2304.14178mplugowl: Modularization empowers large language models with multimodality. 2023arXiv preprint</p>
<p>Assistgpt: A general multimodal assistant that can plan, execute, inspect, and learn. D Gao, L Ji, L Zhou, K Q Lin, J Chen, Z Fan, M Z Shou, arXiv:2306.086402023arXiv preprint</p>
<p>Z Zhao, L Guo, T Yue, S Chen, S Shao, X Zhu, Z Yuan, J Liu, arXiv:2305.16103Chatbridge: Bridging modalities with large language model as a language catalyst. 2023arXiv preprint</p>
<p>X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. F Chen, M Han, H Zhao, Q Zhang, J Shi, S Xu, B Xu, arXiv:2305.041602023arXiv preprint</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, M Deitke, K Ehsani, D Gordon, Y Zhu, arXiv:1712.054742017arXiv preprint</p>
<p>Site selection and layout of earthquake rescue center based on k-means clustering and fruit fly optimization algorithm. X.-Y Jiang, N.-Y Pa, W.-C Wang, T.-T Yang, W.-T Pan, 2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA). IEEE2020</p>
<p>The site selection of distribution center based on linear programming transportation method. X Liu, Proceedings of the 10th World Congress on Intelligent Control and Automation. the 10th World Congress on Intelligent Control and AutomationIEEE2012</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2082 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2082</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2082</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-276482776</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.14499v1.pdf" target="_blank">MLGym: A New Framework and Benchmark for Advancing AI Research Agents</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2082.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2082.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLGym</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta MLGym (MLGym framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Gym-style environment and software framework for training and evaluating LLM-based AI research agents: provides agent scaffolding, a dockerized shell environment, tools/ACI, dataset abstractions, task definitions, and evaluation harnesses for open-ended ML research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLGym framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>agentic environment / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning research (meta-research / ML engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>agent-generated artifacts such as code changes, training scripts, trained model checkpoints, submission files, experimental logs and hyperparameter configurations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>primarily in-distribution / incremental (framework enables agents to search for improvements and hyperparameter tuning; the paper reports agents usually find better hyperparameters rather than truly novel algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-driven action loop (ReAct-style) that issues shell/tool commands (edit, run python training scripts, validate, submit) to generate code and models; search and iteration via repeated validate calls and memory-assisted retrieval of past good configurations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Task-provided, read-only evaluation scripts run by the environment; agents call 'validate' to run evaluation on test set (intermediate, non-terminal) or 'submit' to run final evaluation (terminal). Evaluation scripts are the ground-truth metric calculators per task (e.g., accuracy, R2, BLEU, reward, perplexity, wall-clock time).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not a single scalar — performance depends on agent backbone and task. The paper reports aggregated metrics using performance profiles and AUP; e.g., OpenAI O1-preview achieved top aggregate AUP across tasks, Gemini-1.5-Pro achieved ~99% of O1's AUP while being ~9× cheaper. Per-task example metrics from Table 5: CIFAR-10 accuracy (baseline 0.497) ranged up to 0.894 (Gemini) and 0.854 (O1-preview); Language Modeling validation loss: O1-preview 3.966, Gemini 4.166; 3-SAT wall-clock times ranged from 3.83s (O1-preview baseline?) to ~16s for others. (Paper reports BestAttempt@4 and BestSubmission@4 aggregated over 4 runs.)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation is deterministic per the evaluation scripts; reported outcomes include Best Attempt and Best Submission metrics aggregated over 4 runs. Validation scripts produce task metrics (accuracy, R2, BLEU, reward, perplexity, wall-clock). The paper reports that many runs end in 'Evaluation Error' (missing/incorrect submission artifact) and that validation often fails due to incorrect formats rather than metric computation errors; no single overall validation accuracy is given beyond the per-task reported numbers and AUP curves.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not explicitly reported. The paper does not provide an explicit false-positive rate (invalid outputs accepted as valid). It does report that 'Evaluation Error' — mostly caused by missing submission artifacts or incorrect submission formats — accounts for ~75% of termination errors, indicating frequent invalid submissions that fail validation rather than being incorrectly accepted.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not explicitly reported. The paper does not provide an explicit false-negative rate (valid outputs rejected).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Reported qualitatively: agents excel at incremental improvements (hyperparameter tuning, modest baseline improvements) but do not produce novel algorithms, architectures, or major methodological innovations. Performance (AUP) measures improvement over baselines, but validation performance for truly novel outputs is not demonstrated; the benchmark focuses on Level 1 (Baseline Improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Yes — the paper explicitly distinguishes generation (agent exploration / Best Attempt) from the agent's ability to produce and keep a final validated submission (Best Submission). They compute two AUPs: AUP_ba (Best Attempt) captures exploration ceiling and AUP_bs (Best Submission) captures ability to submit the best attempt. The observed gap: some models achieve high Best Attempt ceilings but fail to reliably submit that best attempt (AUP_ba > AUP_bs in some cases), indicating a gap between generation (finding good artifacts) and validation/finalization (recovering/locking in the best artifact).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No explicit probabilistic uncertainty quantification for generated artifacts is reported. The LLMs use deterministic decoding parameters (temperature=0.0 for most models) in experiments, but no model-provided uncertainty/confidence scores for generated code or models are used in validation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported / not measured. The paper does not provide calibration metrics relating model confidence to outcome quality.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not demonstrated. The benchmark targets ML tasks with defined validation scripts; agents did not demonstrably produce transformational out-of-distribution discoveries, and the authors state agents 'do not generate novel hypotheses, algorithms, architectures, or substantial improvements'.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — the framework uses task-specific quantitative metrics (accuracy, R2, BLEU, perplexity, average reward, wall-clock time) as proxies for scientific progress. It also uses Best Attempt vs Best Submission as proxies for generation vs validated finalization capability. For novelty, no automated novelty metric is provided; novelty assessment is discussed as an open problem and often requires human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for novel outputs and claimed scientific contributions; not quantified. The paper emphasizes human review especially when assessing 'novel scientific contribution' levels (Levels 3–5) and states that human validation is required beyond Level 1 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Semi-formal / empirical (machine learning research): tasks have formal evaluation procedures (benchmarks, evaluation scripts) but domain claims (novel algorithms, scientific contributions) require empirical validation and possibly human judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>The paper proposes and uses several strategies: (1) validate vs submit commands to let agents check performance repeatedly without terminating runs, (2) a Memory Module to store and later retrieve best training configurations so agents can recover best attempts, (3) read-only evaluation scripts and dataset copying to prevent tampering, and (4) performance profiles / AUP to measure generation ceilings vs submission reliability. The Memory Module empirically reduced forgetting and improved ability to re-use best configurations (examples shown in Figures 11–12).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Multiple lines of evidence: (a) authors report agents improve baselines mainly through hyperparameter tuning rather than novel methods; (b) Best Attempt vs Best Submission analysis and separate AUP_bs vs AUP_ba: models sometimes reach high BestAttempt ceilings but have lower BestSubmission scores; (c) failure-mode analysis: 75% of termination errors are 'Evaluation Error' (missing/incorrect submission artefacts), and many runs are failed or incomplete despite intermediate valid attempts, indicating generation can outpace ability to produce a validated final artifact; (d) memory module experiments demonstrate forgetting of earlier good configurations, confirming a mechanism for generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some models consistently submit valid solutions (OpenAI O1-preview and Gemini-1.5-Pro show high completion/submit rates), and for many supervised tasks agents reliably beat baselines and produce validated submissions, indicating that for in-distribution, incremental tasks the generation-validation gap is small.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not explicitly quantified as a single ratio. The paper reports model run cost vs AUP (Figure 3) and per-task training timeouts and average runtimes (Table 7). It notes that computation for training/evaluation (validation) dominates runtime and cost (e.g., OpenAI O1-preview is most expensive while giving top AUP). No numeric ratio of validation-to-generation cost is provided; authors note validation (training/evaluating models) is the expensive step compared to LLM token costs for generation, and cost increases with longer experiment/training tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2082.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2082.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLGym-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLGym-Bench benchmark suite</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-task benchmark within MLGym covering open-ended ML research tasks across domains (data science, game theory, computer vision, NLP, reinforcement learning) with standardized evaluation scripts and baselines to assess LLM agents' research capabilities, focusing on Level 1 (Baseline Improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLGym-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark suite (task collection + evaluation scripts)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning research tasks (CV, NLP, RL, game theory, data science)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>task-specific artifacts: trained models (checkpoints), code strategies (game-theoretic strategy code), predictions/submission files (CSV), heuristics, wall-clock performance logs and metrics</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>designed to measure incremental improvements within domains (Level 1); not intended to require transformational novelty</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generative system; provides environments and tasks where LLM agents generate artifacts via iteratively editing code, running training, and validating results through provided evaluation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Task-specific read-only evaluation scripts (examples: RMSE/R2 for house price; accuracy for CIFAR-10/Fashion MNIST; BLEU for MS-COCO captioning; perplexity for language modeling; mean reward for RL tasks; wall-clock time for 3-SAT heuristic). The environment executes evaluation scripts on agent-produced artifacts when agent calls 'validate' or 'submit'.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Per-task metrics reported in Table 5 (BestAttempt@4). Examples: CIFAR-10 accuracy: up to 0.894 (Gemini); Fashion-MNIST accuracy up to 0.945 (Gemini); MS-COCO BLEU scores reported (best ~0.294 for Llama? per table entries); Language Modeling validation loss ranged between ~3.966 (O1-preview) and higher/∞ for models that failed to produce valid solutions. Aggregate performance is reported via AUP curves.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation scripts are deterministic; the paper reports BestAttempt and BestSubmission outcomes across 4 seeds. Many validation failures were due to incorrect submission formats (Evaluation Error). For some hard tasks (Language Modeling, RL tasks), several models failed to produce any valid submission (∞ entries in tables), indicating validation could not be completed for those generation attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported. The benchmark uses script evaluation which should make false acceptance rare; however, many runs end with Evaluation Error (invalid submission), implying invalid artifacts failing validation rather than being falsely accepted.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Benchmark mainly measures incremental improvement; as novelty increases (moving beyond baseline improvement), the benchmark does not supply mechanisms to automatically assess true novelty and relies on human judgement for higher-level claims.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Benchmark explicitly separates Best Attempt (validate calls during run) and Best Submission (final submit or autosubmit) to capture differences in exploration capability vs ability to finalize a validated artifact. Reported gaps indicate some agents find better intermediate artifacts than they can reliably submit as final.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No built-in uncertainty quantification for generated artifacts; validation is deterministic based on evaluation scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable / not reported for benchmark itself.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Benchmark tasks include algorithmic and open-ended tasks but do not present evidence of agents producing or validating out-of-distribution scientific discoveries; models struggled on more complex tasks (LM, RL), often failing to produce valid submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — standard predictive/empirical metrics per task (accuracy, reward, BLEU, perplexity, wall-clock time) used as proxies for successful scientific artifacts in ML domains.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for evaluating novelty and scientific-level contributions beyond Level 1; frequency not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical (machine learning) with well-defined benchmarks and quantitative metrics, but higher-level novelty requires human judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Design choices to reduce invalid submissions and cheating: read-only evaluation scripts and datasets, validate command for iterative checking, tooling docs in system prompt, autosubmit after 50 steps. These controls mitigate some generation-validation mismatches but do not remove the need for human judgement on novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Several tasks show models can iterate (validate) and produce good intermediate results but fail to submit or produce final valid solutions consistently (BestAttempt vs BestSubmission differences; some tasks show ∞ for BestSubmission while BestAttempt had finite values).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>For many standard supervised tasks (CIFAR-10, Fashion-MNIST, House Price), agents reliably produced validated and improved submissions, suggesting the generation-validation gap is task-dependent and smaller for standard supervised/engineering problems.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not provided as a single number. The benchmark includes training timeouts per task and reports average agent runtime and baseline runtime (Table 7). Validation (training/evaluation) is the dominant cost; expensive models (O1-preview) yield higher AUP but at much higher token/API cost.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2082.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2082.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SWE-Agent harness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SWE-Agent based agent scaffolding (default agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic harness adapted from SWE-Agent that provides an Agent-Computer Interface (ACI), tool wrappers, history processing, and a ReAct-style loop enabling LLMs to interact with the MLGym environment via shell/tool commands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SWE-Agent scaffolding</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>agent orchestration / scaffold (tool-use interface)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning research automation / software engineering for agents</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>shell commands, edited code files, training scripts, trained checkpoints and submission artifacts (e.g., submission.csv or strategy code)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>operational/infrastructure (not intended to produce novel scientific outputs itself), supports generation by backbone LLMs which are typically in-distribution or incremental improvements</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Harness wraps a base LLM and converts LLM text outputs into single-step shell/tool actions (edit, insert, run python scripts, validate, submit). It manages history, tools, and enforces single-command-per-step interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Relies on environment-provided evaluation scripts triggered via validate/submit commands; harness provides tool documentation in system prompt to guide LLM and prevents modification of evaluation scripts (read-only).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not measured independently; it is the scaffolding used to produce the generation results reported for backbone LLMs (reported AUPs and per-task scores are achieved using this harness).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Harness enables repeated 'validate' calls to execute evaluation scripts; no independent validation accuracy for harness itself reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported. The harness enforces read-only evaluation scripts and dataset protections to reduce accidental acceptance of invalid outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Harness supports long-horizon runs with Memory Module to mitigate forgetting; however, generation of truly novel outputs is limited by backbone models rather than harness.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>The paper uses the harness to highlight that even when an agent discovers a good artifact, the harness and environment constraints (context window, step limit, inability to recover earlier state without memory) can prevent finalizing the artifact — motivating memory and validate/submit semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable to harness.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not applicable; performance depends on LLM backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Harness provides the 'validate' interface and informs the agent of available evaluation scripts; relies on task proxies (metrics) rather than any internal verification.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for higher levels of novelty; harness itself does not automate human review.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Operational infrastructure for empirical ML tasks (empirical / engineering).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Single-command-per-step, read-only eval scripts, validate vs submit semantics, integration of Memory Module, tool documentation in prompt to reduce incorrect submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Harness alone cannot prevent evaluation-format errors or forgetting of best artifacts without memory; observed failure modes (Evaluation Error, incomplete runs) show harness-enforced constraints still allow generation-validation mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Harness enabled some models (O1-preview, Gemini) to reliably submit valid outputs for many tasks, indicating that a well-designed scaffold reduces but does not eliminate gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not applicable to harness; cost dominated by training/evaluation executed by agent actions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2082.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2082.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM backbones</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Backbone large language models (OpenAI O1-preview, Gemini-1.5-Pro, Claude-3.5-Sonnet, Llama-3.1-405b-Instruct, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Frontier LLMs used as the generative core of the agent scaffolding; they produce shell commands and code edits that generate experimental artifacts which are then validated by task-specific evaluation scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM agent backbones (O1-preview, Gemini-1.5-Pro, Claude-3.5-Sonnet, Llama-3.1-405b, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (LLM) used within an agentic harness</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning research automation (generalist agentic generation across ML subdomains)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual commands, code patches, training configs, experimental analyses, and scripts that produce trained models and prediction files</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>mostly incremental/in-distribution (reported to typically find better hyperparameters and small improvements rather than novel algorithms or paradigm-shifting contributions)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Text-generation conditioned on prompt + tool docs + prior action/observation history; ReAct-style loop producing single command per step executed by environment; optional literature_search and memory_read to incorporate external knowledge and past findings.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Agent-driven calls to 'validate' run the environment's evaluation scripts to compute task metrics; 'submit' executes final evaluation. Validation is external and implemented per-task in Python scripts protected as read-only.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Per-table performance examples (BestAttempt@4 / BestSubmission@4 shown in paper): CIFAR-10 accuracy: Llama 0.528, GPT-4o 0.733, Claude-3.5-Sonnet 0.894, Gemini 0.758, O1-preview 0.854 (numbers illustrative from Table 5). Overall, OpenAI O1-preview had the highest aggregate AUP; Gemini achieved ~99% of O1's AUP while being ~9× cheaper. Some backbones failed to produce any valid solution for some tasks (e.g., Llama/GPT-4o failed Language Modeling or Breakout tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Determined by successful execution of evaluation scripts; many backbone runs terminated with Evaluation Error (75% of termination errors). Completion/failure rates vary by model: GPT-4o had highest failure rate; Gemini and O1-preview had best completion rates; Claude had high performance but relatively higher failure rate.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically. The paper indicates invalid submissions are common (evaluation errors), but does not report instances of invalid artifacts being accepted by validation scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performance falls off as tasks require deeper algorithmic novelty or large architectural changes (Language Modeling, RL tasks): these are where many backbones either fail to produce valid submissions or produce inferior results. For incremental engineering tasks (hyperparameter tuning, small model changes), backbones perform well.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper reports that LLM backbones often can generate improved attempts (BestAttempt) but differ in ability to make that the final submission (BestSubmission); backbones with better error handling and memory utilization show smaller gaps. Generative strengths (idea/code generation) often outpace ability to reliably validate/produce final artifacts due to execution/format errors and forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not used. Models were run with deterministic decoding (temperature=0.0 for most) and no per-output uncertainty scores were supplied to validation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not measured.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor / not shown — models did not produce validated out-of-distribution discoveries; many difficult tasks produced no valid submissions (∞ entries in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — per-task metrics. The backbones rely on validate/submit results (these quantitative proxies) to judge artifact quality during runs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Implied to be necessary especially for claims beyond Level 1; not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical ML tasks (formal benchmarks for many tasks, but scientific novelty assessment remains human-centric).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Using Memory Module to persist best configs, validate vs submit separation to allow checking without terminating, read-only eval scripts to prevent tampering, training timeouts to limit runaway compute; these reduced forgetting and some format errors but did not eliminate evaluation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Backbones commonly produce intermediate validated attempts but fail to submit the best one; failure-mode stats show many runs end with Evaluation Error, and some backbones have high failure rates even when they have good BestAttempt numbers. Paper explicitly states modern frontier models 'do not generate novel hypotheses, algorithms, architectures, or substantial improvements' despite improving baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some backbones (O1-preview, Gemini) reliably submitted valid improved artifacts across many tasks demonstrating that for many in-distribution engineering tasks the generation-validation gap can be small.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Paper reports model API cost per token and AUP vs cost curves (Figure 3): O1-preview is most expensive and top-performing; Gemini is most cost-effective (~9× cheaper than O1-preview to reach ~99% of its AUP). No exact numeric ratio of validation cost to generation cost provided; training/evaluation (validation) dominates per-task runtime and cost.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2082.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2082.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLGym Memory Module (research logs memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured persistent memory used by agents to store experimental findings and successful training configurations (text + embeddings + tags) to overcome forgetting in long-horizon experiments and allow retrieval of best-known configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Memory Module (MLGym)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>structured persistent memory with embedding-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning experiment management within agent workflows</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>retrieved experimental configurations, notes and tags that inform subsequent agent actions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>infrastructure enabling better re-use of in-run discoveries (incremental capability improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Agents call memory_write to store textual findings and embeddings; memory_read retrieves top-k relevant entries by cosine similarity to a query; tags (3-gram heuristics) and state of memory is added to system prompt to inform agent decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Indirectly validated via improved Best Submission/AUP when memory is available; authors present examples (Figures 11–12) where memory retrieval enabled restarting from best configuration and achieving superior results.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Memory itself does not generate artifacts; when present, agents could retrieve and reuse best training configurations, leading to measurable improvements in final submissions in illustrative examples. No aggregate quantitative metric solely for memory reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Improvements shown qualitatively and via example figures; memory-enabled agents sustain progress over long sequences and less often lose best configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not applicable to memory module (no acceptance/rejection).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Memory helps with long-horizon optimization and reusing previously found high-performing configurations (helps incremental improvements); it does not enable discovery of highly novel methods by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Memory reduces the gap by enabling agents to restore best attempts and thus improve Best Submission relative to Best Attempt.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Effect measured by final task metrics (AUP improvement on examples) rather than a direct memory accuracy metric.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>N/A for memory storage; human review still recommended for scientific-novelty claims.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Operational / empirical.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Memory is explicitly proposed as a mitigation for the generation-validation gap arising from limited context length and forgetting; authors show examples where memory retrieval allowed restarting exploration from the best known configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors show that without memory agents forget earlier experiments and cannot recover best configurations (leading to lower Best Submission); with memory they can retrieve and reuse best configs (Figures 11–12).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None; memory consistently presented as a mitigation improving final outcomes in presented examples.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Memory storage/retrieval cost is small relative to training/evaluation; no numeric ratio provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2082.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2082.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Validate/Submit evaluation interface</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Validate and Submit commands / read-only evaluation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two environment-level commands agents use to run task evaluation scripts: 'validate' runs evaluation without terminating the run (can be called multiple times), while 'submit' runs final evaluation and terminates the agent trajectory; evaluation scripts are read-only and compute ground-truth metrics per task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>validate/submit evaluation interface</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>evaluation API / control commands</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning experimental evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>numerical task metrics (accuracy, R2, BLEU, perplexity, reward, wall-clock time) and evaluation logs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>n/a (validation infrastructure)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A — invoked by agents to validate generated artifacts by running predefined evaluation scripts against produced artifacts (model checkpoints, submission files, strategy code).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Execution of task-specific evaluation scripts in a sandboxed/docker environment on the agent's produced artifacts; scripts return metrics used for both iterative decision-making (validate) and final scoring/logging (submit).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Deterministic given correct artifact format. Many runs terminated with 'Evaluation Error' (≈75% of termination errors) due to missing/incorrect submission artifacts or format errors at submit time, indicating that validation scripts reliably detect format/artefact issues.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported; evaluation scripts are intended to be strict (read-only) and detect incorrect formats and artifacts; paper reports invalid artifacts are typically rejected (Evaluation Error) rather than falsely accepted.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Validation scripts are reliable for in-domain tasks with clear metrics, but they do not measure scientific novelty. Novelty assessment remains a human task.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>The validate/submit separation is explicitly used to measure generation (via validate calls = BestAttempt) vs finalization (submit = BestSubmission); gap observed when agents find good artifacts but produce invalid submissions or cannot recover earlier good artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No per-artifact uncertainty reported; validation is deterministic.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Validation scripts evaluate provided metrics; for out-of-distribution claims (novel algorithms) additional assessment beyond script metrics is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — the evaluation scripts implement proxy metrics appropriate to each task.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for claims beyond metric improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical and benchmarked (formal metrics exist), but human judgement required for novelty/formal contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Provide validate as a non-terminal check to reduce premature submission; make evaluation scripts read-only and datasets read-only to prevent tampering; encourage agents to use validate frequently to iterate.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>High incidence of 'Evaluation Error' terminations and differences between BestAttempt and BestSubmission AUPs illustrate that agents frequently fail at the final validation/submit step even when intermediate validated attempts exist.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>For many tasks agents successfully used validate iteratively and then submit final correct artifacts (examples with good BestSubmission scores), indicating that when format and artifact generation are handled correctly, validate/submit works well.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not numerically provided. Running evaluation scripts often involves training/evaluating models which is the major computational cost, so validation is often the expensive part of a run compared to LLM token generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2082.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2082.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Performance profiles / AUP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance profiles and Area Under Performance Profile (AUP) scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adapted evaluation aggregation method (from Dolan & Moré 2002 and AutoML Decathlon) used to compare methods across heterogeneous tasks by computing performance profiles and integrating area under the curve (AUP) to produce an aggregate score per method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Performance profiles and AUP</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>evaluation aggregation metric</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>benchmarking / optimization / AutoML evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>aggregate scalar scores (AUP) and performance profile curves that indicate fraction of tasks within a threshold of best performance</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>methodological (evaluation aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used to aggregate per-task validation metrics across diverse scales by computing per-task performance ratios and a cumulative profile ρ_m(τ); AUP = area under that curve up to τmax.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Enables cross-task comparison of agent variants/backbones; paper reports AUP_bs@4 and AUP_ba@4 scores per model (Best Submission and Best Attempt aggregated over 4 runs) to capture generation vs submission ability.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>AUP captures relative performance vs best-known method per task but does not measure novelty. Authors interpret AUP_ba (Best Attempt) as ceiling/exploration capability and AUP_bs (Best Submission) as ability to finalize validated outputs — allowing identification of gaps when exploration exceeds submission.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Directly used to compare BestAttempt (generation ceiling) vs BestSubmission (validated finalization); discrepancies between AUP_ba and AUP_bs provide quantitative evidence of generation-validation asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not applicable beyond the performance profile semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not a direct measure of OOD; AUP measures relative performance to best method across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Used to aggregate task-native metrics into a single comparative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Formal / methodological for cross-task metric aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Using separate AUP for BestAttempt vs BestSubmission surfaces issues where generation capacity is not realized in validated submissions; encourages designing agents that both explore and reliably finalize.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>The paper uses AUP_ba > AUP_bs differences to show that some models can reach high attempt ceilings but fail to submit those artifacts, providing quantitative evidence of a generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Where AUP_ba ≈ AUP_bs and completion rates are high (e.g., for O1-preview on many tasks), the aggregation suggests small generation-validation gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>N/A</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering <em>(Rating: 2)</em></li>
                <li>MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation <em>(Rating: 2)</em></li>
                <li>ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery <em>(Rating: 2)</em></li>
                <li>RE-bench <em>(Rating: 2)</em></li>
                <li>MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering <em>(Rating: 1)</em></li>
                <li>AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2082",
    "paper_id": "paper-276482776",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "MLGym",
            "name_full": "Meta MLGym (MLGym framework)",
            "brief_description": "A Gym-style environment and software framework for training and evaluating LLM-based AI research agents: provides agent scaffolding, a dockerized shell environment, tools/ACI, dataset abstractions, task definitions, and evaluation harnesses for open-ended ML research tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MLGym framework",
            "system_type": "agentic environment / evaluation framework",
            "scientific_domain": "machine learning research (meta-research / ML engineering)",
            "output_type": "agent-generated artifacts such as code changes, training scripts, trained model checkpoints, submission files, experimental logs and hyperparameter configurations",
            "novelty_level": "primarily in-distribution / incremental (framework enables agents to search for improvements and hyperparameter tuning; the paper reports agents usually find better hyperparameters rather than truly novel algorithms)",
            "generation_method": "LLM-driven action loop (ReAct-style) that issues shell/tool commands (edit, run python training scripts, validate, submit) to generate code and models; search and iteration via repeated validate calls and memory-assisted retrieval of past good configurations",
            "validation_method": "Task-provided, read-only evaluation scripts run by the environment; agents call 'validate' to run evaluation on test set (intermediate, non-terminal) or 'submit' to run final evaluation (terminal). Evaluation scripts are the ground-truth metric calculators per task (e.g., accuracy, R2, BLEU, reward, perplexity, wall-clock time).",
            "generation_performance": "Not a single scalar — performance depends on agent backbone and task. The paper reports aggregated metrics using performance profiles and AUP; e.g., OpenAI O1-preview achieved top aggregate AUP across tasks, Gemini-1.5-Pro achieved ~99% of O1's AUP while being ~9× cheaper. Per-task example metrics from Table 5: CIFAR-10 accuracy (baseline 0.497) ranged up to 0.894 (Gemini) and 0.854 (O1-preview); Language Modeling validation loss: O1-preview 3.966, Gemini 4.166; 3-SAT wall-clock times ranged from 3.83s (O1-preview baseline?) to ~16s for others. (Paper reports BestAttempt@4 and BestSubmission@4 aggregated over 4 runs.)",
            "validation_performance": "Validation is deterministic per the evaluation scripts; reported outcomes include Best Attempt and Best Submission metrics aggregated over 4 runs. Validation scripts produce task metrics (accuracy, R2, BLEU, reward, perplexity, wall-clock). The paper reports that many runs end in 'Evaluation Error' (missing/incorrect submission artifact) and that validation often fails due to incorrect formats rather than metric computation errors; no single overall validation accuracy is given beyond the per-task reported numbers and AUP curves.",
            "false_positive_rate": "Not explicitly reported. The paper does not provide an explicit false-positive rate (invalid outputs accepted as valid). It does report that 'Evaluation Error' — mostly caused by missing submission artifacts or incorrect submission formats — accounts for ~75% of termination errors, indicating frequent invalid submissions that fail validation rather than being incorrectly accepted.",
            "false_negative_rate": "Not explicitly reported. The paper does not provide an explicit false-negative rate (valid outputs rejected).",
            "performance_vs_novelty": "Reported qualitatively: agents excel at incremental improvements (hyperparameter tuning, modest baseline improvements) but do not produce novel algorithms, architectures, or major methodological innovations. Performance (AUP) measures improvement over baselines, but validation performance for truly novel outputs is not demonstrated; the benchmark focuses on Level 1 (Baseline Improvement).",
            "generation_validation_comparison": "Yes — the paper explicitly distinguishes generation (agent exploration / Best Attempt) from the agent's ability to produce and keep a final validated submission (Best Submission). They compute two AUPs: AUP_ba (Best Attempt) captures exploration ceiling and AUP_bs (Best Submission) captures ability to submit the best attempt. The observed gap: some models achieve high Best Attempt ceilings but fail to reliably submit that best attempt (AUP_ba &gt; AUP_bs in some cases), indicating a gap between generation (finding good artifacts) and validation/finalization (recovering/locking in the best artifact).",
            "uncertainty_quantification": "No explicit probabilistic uncertainty quantification for generated artifacts is reported. The LLMs use deterministic decoding parameters (temperature=0.0 for most models) in experiments, but no model-provided uncertainty/confidence scores for generated code or models are used in validation.",
            "calibration_quality": "Not reported / not measured. The paper does not provide calibration metrics relating model confidence to outcome quality.",
            "out_of_distribution_performance": "Not demonstrated. The benchmark targets ML tasks with defined validation scripts; agents did not demonstrably produce transformational out-of-distribution discoveries, and the authors state agents 'do not generate novel hypotheses, algorithms, architectures, or substantial improvements'.",
            "validation_proxy_metrics": "Yes — the framework uses task-specific quantitative metrics (accuracy, R2, BLEU, perplexity, average reward, wall-clock time) as proxies for scientific progress. It also uses Best Attempt vs Best Submission as proxies for generation vs validated finalization capability. For novelty, no automated novelty metric is provided; novelty assessment is discussed as an open problem and often requires human judgment.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for novel outputs and claimed scientific contributions; not quantified. The paper emphasizes human review especially when assessing 'novel scientific contribution' levels (Levels 3–5) and states that human validation is required beyond Level 1 tasks.",
            "formal_verification_used": false,
            "domain_formalization_level": "Semi-formal / empirical (machine learning research): tasks have formal evaluation procedures (benchmarks, evaluation scripts) but domain claims (novel algorithms, scientific contributions) require empirical validation and possibly human judgement.",
            "gap_mitigation_strategies": "The paper proposes and uses several strategies: (1) validate vs submit commands to let agents check performance repeatedly without terminating runs, (2) a Memory Module to store and later retrieve best training configurations so agents can recover best attempts, (3) read-only evaluation scripts and dataset copying to prevent tampering, and (4) performance profiles / AUP to measure generation ceilings vs submission reliability. The Memory Module empirically reduced forgetting and improved ability to re-use best configurations (examples shown in Figures 11–12).",
            "evidence_supporting_gap": "Multiple lines of evidence: (a) authors report agents improve baselines mainly through hyperparameter tuning rather than novel methods; (b) Best Attempt vs Best Submission analysis and separate AUP_bs vs AUP_ba: models sometimes reach high BestAttempt ceilings but have lower BestSubmission scores; (c) failure-mode analysis: 75% of termination errors are 'Evaluation Error' (missing/incorrect submission artefacts), and many runs are failed or incomplete despite intermediate valid attempts, indicating generation can outpace ability to produce a validated final artifact; (d) memory module experiments demonstrate forgetting of earlier good configurations, confirming a mechanism for generation-validation gap.",
            "evidence_contradicting_gap": "Some models consistently submit valid solutions (OpenAI O1-preview and Gemini-1.5-Pro show high completion/submit rates), and for many supervised tasks agents reliably beat baselines and produce validated submissions, indicating that for in-distribution, incremental tasks the generation-validation gap is small.",
            "computational_cost_ratio": "Not explicitly quantified as a single ratio. The paper reports model run cost vs AUP (Figure 3) and per-task training timeouts and average runtimes (Table 7). It notes that computation for training/evaluation (validation) dominates runtime and cost (e.g., OpenAI O1-preview is most expensive while giving top AUP). No numeric ratio of validation-to-generation cost is provided; authors note validation (training/evaluating models) is the expensive step compared to LLM token costs for generation, and cost increases with longer experiment/training tasks.",
            "uuid": "e2082.0"
        },
        {
            "name_short": "MLGym-Bench",
            "name_full": "MLGym-Bench benchmark suite",
            "brief_description": "A 13-task benchmark within MLGym covering open-ended ML research tasks across domains (data science, game theory, computer vision, NLP, reinforcement learning) with standardized evaluation scripts and baselines to assess LLM agents' research capabilities, focusing on Level 1 (Baseline Improvement).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MLGym-Bench",
            "system_type": "benchmark suite (task collection + evaluation scripts)",
            "scientific_domain": "machine learning research tasks (CV, NLP, RL, game theory, data science)",
            "output_type": "task-specific artifacts: trained models (checkpoints), code strategies (game-theoretic strategy code), predictions/submission files (CSV), heuristics, wall-clock performance logs and metrics",
            "novelty_level": "designed to measure incremental improvements within domains (Level 1); not intended to require transformational novelty",
            "generation_method": "Not a generative system; provides environments and tasks where LLM agents generate artifacts via iteratively editing code, running training, and validating results through provided evaluation scripts",
            "validation_method": "Task-specific read-only evaluation scripts (examples: RMSE/R2 for house price; accuracy for CIFAR-10/Fashion MNIST; BLEU for MS-COCO captioning; perplexity for language modeling; mean reward for RL tasks; wall-clock time for 3-SAT heuristic). The environment executes evaluation scripts on agent-produced artifacts when agent calls 'validate' or 'submit'.",
            "generation_performance": "Per-task metrics reported in Table 5 (BestAttempt@4). Examples: CIFAR-10 accuracy: up to 0.894 (Gemini); Fashion-MNIST accuracy up to 0.945 (Gemini); MS-COCO BLEU scores reported (best ~0.294 for Llama? per table entries); Language Modeling validation loss ranged between ~3.966 (O1-preview) and higher/∞ for models that failed to produce valid solutions. Aggregate performance is reported via AUP curves.",
            "validation_performance": "Validation scripts are deterministic; the paper reports BestAttempt and BestSubmission outcomes across 4 seeds. Many validation failures were due to incorrect submission formats (Evaluation Error). For some hard tasks (Language Modeling, RL tasks), several models failed to produce any valid submission (∞ entries in tables), indicating validation could not be completed for those generation attempts.",
            "false_positive_rate": "Not reported. The benchmark uses script evaluation which should make false acceptance rare; however, many runs end with Evaluation Error (invalid submission), implying invalid artifacts failing validation rather than being falsely accepted.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Benchmark mainly measures incremental improvement; as novelty increases (moving beyond baseline improvement), the benchmark does not supply mechanisms to automatically assess true novelty and relies on human judgement for higher-level claims.",
            "generation_validation_comparison": "Benchmark explicitly separates Best Attempt (validate calls during run) and Best Submission (final submit or autosubmit) to capture differences in exploration capability vs ability to finalize a validated artifact. Reported gaps indicate some agents find better intermediate artifacts than they can reliably submit as final.",
            "uncertainty_quantification": "No built-in uncertainty quantification for generated artifacts; validation is deterministic based on evaluation scripts.",
            "calibration_quality": "Not applicable / not reported for benchmark itself.",
            "out_of_distribution_performance": "Benchmark tasks include algorithmic and open-ended tasks but do not present evidence of agents producing or validating out-of-distribution scientific discoveries; models struggled on more complex tasks (LM, RL), often failing to produce valid submissions.",
            "validation_proxy_metrics": "Yes — standard predictive/empirical metrics per task (accuracy, reward, BLEU, perplexity, wall-clock time) used as proxies for successful scientific artifacts in ML domains.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for evaluating novelty and scientific-level contributions beyond Level 1; frequency not quantified.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical (machine learning) with well-defined benchmarks and quantitative metrics, but higher-level novelty requires human judgement.",
            "gap_mitigation_strategies": "Design choices to reduce invalid submissions and cheating: read-only evaluation scripts and datasets, validate command for iterative checking, tooling docs in system prompt, autosubmit after 50 steps. These controls mitigate some generation-validation mismatches but do not remove the need for human judgement on novelty.",
            "evidence_supporting_gap": "Several tasks show models can iterate (validate) and produce good intermediate results but fail to submit or produce final valid solutions consistently (BestAttempt vs BestSubmission differences; some tasks show ∞ for BestSubmission while BestAttempt had finite values).",
            "evidence_contradicting_gap": "For many standard supervised tasks (CIFAR-10, Fashion-MNIST, House Price), agents reliably produced validated and improved submissions, suggesting the generation-validation gap is task-dependent and smaller for standard supervised/engineering problems.",
            "computational_cost_ratio": "Not provided as a single number. The benchmark includes training timeouts per task and reports average agent runtime and baseline runtime (Table 7). Validation (training/evaluation) is the dominant cost; expensive models (O1-preview) yield higher AUP but at much higher token/API cost.",
            "uuid": "e2082.1"
        },
        {
            "name_short": "SWE-Agent harness",
            "name_full": "SWE-Agent based agent scaffolding (default agent)",
            "brief_description": "An agentic harness adapted from SWE-Agent that provides an Agent-Computer Interface (ACI), tool wrappers, history processing, and a ReAct-style loop enabling LLMs to interact with the MLGym environment via shell/tool commands.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "SWE-Agent scaffolding",
            "system_type": "agent orchestration / scaffold (tool-use interface)",
            "scientific_domain": "machine learning research automation / software engineering for agents",
            "output_type": "shell commands, edited code files, training scripts, trained checkpoints and submission artifacts (e.g., submission.csv or strategy code)",
            "novelty_level": "operational/infrastructure (not intended to produce novel scientific outputs itself), supports generation by backbone LLMs which are typically in-distribution or incremental improvements",
            "generation_method": "Harness wraps a base LLM and converts LLM text outputs into single-step shell/tool actions (edit, insert, run python scripts, validate, submit). It manages history, tools, and enforces single-command-per-step interaction.",
            "validation_method": "Relies on environment-provided evaluation scripts triggered via validate/submit commands; harness provides tool documentation in system prompt to guide LLM and prevents modification of evaluation scripts (read-only).",
            "generation_performance": "Not measured independently; it is the scaffolding used to produce the generation results reported for backbone LLMs (reported AUPs and per-task scores are achieved using this harness).",
            "validation_performance": "Harness enables repeated 'validate' calls to execute evaluation scripts; no independent validation accuracy for harness itself reported.",
            "false_positive_rate": "Not reported. The harness enforces read-only evaluation scripts and dataset protections to reduce accidental acceptance of invalid outputs.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Harness supports long-horizon runs with Memory Module to mitigate forgetting; however, generation of truly novel outputs is limited by backbone models rather than harness.",
            "generation_validation_comparison": "The paper uses the harness to highlight that even when an agent discovers a good artifact, the harness and environment constraints (context window, step limit, inability to recover earlier state without memory) can prevent finalizing the artifact — motivating memory and validate/submit semantics.",
            "uncertainty_quantification": "No.",
            "calibration_quality": "Not applicable to harness.",
            "out_of_distribution_performance": "Not applicable; performance depends on LLM backbone.",
            "validation_proxy_metrics": "Harness provides the 'validate' interface and informs the agent of available evaluation scripts; relies on task proxies (metrics) rather than any internal verification.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for higher levels of novelty; harness itself does not automate human review.",
            "formal_verification_used": false,
            "domain_formalization_level": "Operational infrastructure for empirical ML tasks (empirical / engineering).",
            "gap_mitigation_strategies": "Single-command-per-step, read-only eval scripts, validate vs submit semantics, integration of Memory Module, tool documentation in prompt to reduce incorrect submissions.",
            "evidence_supporting_gap": "Harness alone cannot prevent evaluation-format errors or forgetting of best artifacts without memory; observed failure modes (Evaluation Error, incomplete runs) show harness-enforced constraints still allow generation-validation mismatches.",
            "evidence_contradicting_gap": "Harness enabled some models (O1-preview, Gemini) to reliably submit valid outputs for many tasks, indicating that a well-designed scaffold reduces but does not eliminate gaps.",
            "computational_cost_ratio": "Not applicable to harness; cost dominated by training/evaluation executed by agent actions.",
            "uuid": "e2082.2"
        },
        {
            "name_short": "LLM backbones",
            "name_full": "Backbone large language models (OpenAI O1-preview, Gemini-1.5-Pro, Claude-3.5-Sonnet, Llama-3.1-405b-Instruct, GPT-4o)",
            "brief_description": "Frontier LLMs used as the generative core of the agent scaffolding; they produce shell commands and code edits that generate experimental artifacts which are then validated by task-specific evaluation scripts.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM agent backbones (O1-preview, Gemini-1.5-Pro, Claude-3.5-Sonnet, Llama-3.1-405b, GPT-4o)",
            "system_type": "large language model (LLM) used within an agentic harness",
            "scientific_domain": "machine learning research automation (generalist agentic generation across ML subdomains)",
            "output_type": "textual commands, code patches, training configs, experimental analyses, and scripts that produce trained models and prediction files",
            "novelty_level": "mostly incremental/in-distribution (reported to typically find better hyperparameters and small improvements rather than novel algorithms or paradigm-shifting contributions)",
            "generation_method": "Text-generation conditioned on prompt + tool docs + prior action/observation history; ReAct-style loop producing single command per step executed by environment; optional literature_search and memory_read to incorporate external knowledge and past findings.",
            "validation_method": "Agent-driven calls to 'validate' run the environment's evaluation scripts to compute task metrics; 'submit' executes final evaluation. Validation is external and implemented per-task in Python scripts protected as read-only.",
            "generation_performance": "Per-table performance examples (BestAttempt@4 / BestSubmission@4 shown in paper): CIFAR-10 accuracy: Llama 0.528, GPT-4o 0.733, Claude-3.5-Sonnet 0.894, Gemini 0.758, O1-preview 0.854 (numbers illustrative from Table 5). Overall, OpenAI O1-preview had the highest aggregate AUP; Gemini achieved ~99% of O1's AUP while being ~9× cheaper. Some backbones failed to produce any valid solution for some tasks (e.g., Llama/GPT-4o failed Language Modeling or Breakout tasks).",
            "validation_performance": "Determined by successful execution of evaluation scripts; many backbone runs terminated with Evaluation Error (75% of termination errors). Completion/failure rates vary by model: GPT-4o had highest failure rate; Gemini and O1-preview had best completion rates; Claude had high performance but relatively higher failure rate.",
            "false_positive_rate": "Not reported numerically. The paper indicates invalid submissions are common (evaluation errors), but does not report instances of invalid artifacts being accepted by validation scripts.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Performance falls off as tasks require deeper algorithmic novelty or large architectural changes (Language Modeling, RL tasks): these are where many backbones either fail to produce valid submissions or produce inferior results. For incremental engineering tasks (hyperparameter tuning, small model changes), backbones perform well.",
            "generation_validation_comparison": "Paper reports that LLM backbones often can generate improved attempts (BestAttempt) but differ in ability to make that the final submission (BestSubmission); backbones with better error handling and memory utilization show smaller gaps. Generative strengths (idea/code generation) often outpace ability to reliably validate/produce final artifacts due to execution/format errors and forgetting.",
            "uncertainty_quantification": "Not used. Models were run with deterministic decoding (temperature=0.0 for most) and no per-output uncertainty scores were supplied to validation.",
            "calibration_quality": "Not measured.",
            "out_of_distribution_performance": "Poor / not shown — models did not produce validated out-of-distribution discoveries; many difficult tasks produced no valid submissions (∞ entries in tables).",
            "validation_proxy_metrics": "Yes — per-task metrics. The backbones rely on validate/submit results (these quantitative proxies) to judge artifact quality during runs.",
            "human_validation_required": true,
            "human_validation_frequency": "Implied to be necessary especially for claims beyond Level 1; not quantified.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical ML tasks (formal benchmarks for many tasks, but scientific novelty assessment remains human-centric).",
            "gap_mitigation_strategies": "Using Memory Module to persist best configs, validate vs submit separation to allow checking without terminating, read-only eval scripts to prevent tampering, training timeouts to limit runaway compute; these reduced forgetting and some format errors but did not eliminate evaluation errors.",
            "evidence_supporting_gap": "Backbones commonly produce intermediate validated attempts but fail to submit the best one; failure-mode stats show many runs end with Evaluation Error, and some backbones have high failure rates even when they have good BestAttempt numbers. Paper explicitly states modern frontier models 'do not generate novel hypotheses, algorithms, architectures, or substantial improvements' despite improving baselines.",
            "evidence_contradicting_gap": "Some backbones (O1-preview, Gemini) reliably submitted valid improved artifacts across many tasks demonstrating that for many in-distribution engineering tasks the generation-validation gap can be small.",
            "computational_cost_ratio": "Paper reports model API cost per token and AUP vs cost curves (Figure 3): O1-preview is most expensive and top-performing; Gemini is most cost-effective (~9× cheaper than O1-preview to reach ~99% of its AUP). No exact numeric ratio of validation cost to generation cost provided; training/evaluation (validation) dominates per-task runtime and cost.",
            "uuid": "e2082.3"
        },
        {
            "name_short": "Memory Module",
            "name_full": "MLGym Memory Module (research logs memory)",
            "brief_description": "A structured persistent memory used by agents to store experimental findings and successful training configurations (text + embeddings + tags) to overcome forgetting in long-horizon experiments and allow retrieval of best-known configurations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Memory Module (MLGym)",
            "system_type": "structured persistent memory with embedding-based retrieval",
            "scientific_domain": "machine learning experiment management within agent workflows",
            "output_type": "retrieved experimental configurations, notes and tags that inform subsequent agent actions",
            "novelty_level": "infrastructure enabling better re-use of in-run discoveries (incremental capability improvement)",
            "generation_method": "Agents call memory_write to store textual findings and embeddings; memory_read retrieves top-k relevant entries by cosine similarity to a query; tags (3-gram heuristics) and state of memory is added to system prompt to inform agent decisions.",
            "validation_method": "Indirectly validated via improved Best Submission/AUP when memory is available; authors present examples (Figures 11–12) where memory retrieval enabled restarting from best configuration and achieving superior results.",
            "generation_performance": "Memory itself does not generate artifacts; when present, agents could retrieve and reuse best training configurations, leading to measurable improvements in final submissions in illustrative examples. No aggregate quantitative metric solely for memory reported.",
            "validation_performance": "Improvements shown qualitatively and via example figures; memory-enabled agents sustain progress over long sequences and less often lose best configurations.",
            "false_positive_rate": "Not applicable to memory module (no acceptance/rejection).",
            "false_negative_rate": "Not applicable.",
            "performance_vs_novelty": "Memory helps with long-horizon optimization and reusing previously found high-performing configurations (helps incremental improvements); it does not enable discovery of highly novel methods by itself.",
            "generation_validation_comparison": "Memory reduces the gap by enabling agents to restore best attempts and thus improve Best Submission relative to Best Attempt.",
            "uncertainty_quantification": "No.",
            "calibration_quality": "Not applicable.",
            "out_of_distribution_performance": "Not applicable.",
            "validation_proxy_metrics": "Effect measured by final task metrics (AUP improvement on examples) rather than a direct memory accuracy metric.",
            "human_validation_required": false,
            "human_validation_frequency": "N/A for memory storage; human review still recommended for scientific-novelty claims.",
            "formal_verification_used": false,
            "domain_formalization_level": "Operational / empirical.",
            "gap_mitigation_strategies": "Memory is explicitly proposed as a mitigation for the generation-validation gap arising from limited context length and forgetting; authors show examples where memory retrieval allowed restarting exploration from the best known configuration.",
            "evidence_supporting_gap": "Authors show that without memory agents forget earlier experiments and cannot recover best configurations (leading to lower Best Submission); with memory they can retrieve and reuse best configs (Figures 11–12).",
            "evidence_contradicting_gap": "None; memory consistently presented as a mitigation improving final outcomes in presented examples.",
            "computational_cost_ratio": "Memory storage/retrieval cost is small relative to training/evaluation; no numeric ratio provided.",
            "uuid": "e2082.4"
        },
        {
            "name_short": "Validate/Submit evaluation interface",
            "name_full": "Validate and Submit commands / read-only evaluation scripts",
            "brief_description": "Two environment-level commands agents use to run task evaluation scripts: 'validate' runs evaluation without terminating the run (can be called multiple times), while 'submit' runs final evaluation and terminates the agent trajectory; evaluation scripts are read-only and compute ground-truth metrics per task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "validate/submit evaluation interface",
            "system_type": "evaluation API / control commands",
            "scientific_domain": "machine learning experimental evaluation",
            "output_type": "numerical task metrics (accuracy, R2, BLEU, perplexity, reward, wall-clock time) and evaluation logs",
            "novelty_level": "n/a (validation infrastructure)",
            "generation_method": "N/A — invoked by agents to validate generated artifacts by running predefined evaluation scripts against produced artifacts (model checkpoints, submission files, strategy code).",
            "validation_method": "Execution of task-specific evaluation scripts in a sandboxed/docker environment on the agent's produced artifacts; scripts return metrics used for both iterative decision-making (validate) and final scoring/logging (submit).",
            "generation_performance": "N/A",
            "validation_performance": "Deterministic given correct artifact format. Many runs terminated with 'Evaluation Error' (≈75% of termination errors) due to missing/incorrect submission artifacts or format errors at submit time, indicating that validation scripts reliably detect format/artefact issues.",
            "false_positive_rate": "Not reported; evaluation scripts are intended to be strict (read-only) and detect incorrect formats and artifacts; paper reports invalid artifacts are typically rejected (Evaluation Error) rather than falsely accepted.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Validation scripts are reliable for in-domain tasks with clear metrics, but they do not measure scientific novelty. Novelty assessment remains a human task.",
            "generation_validation_comparison": "The validate/submit separation is explicitly used to measure generation (via validate calls = BestAttempt) vs finalization (submit = BestSubmission); gap observed when agents find good artifacts but produce invalid submissions or cannot recover earlier good artifacts.",
            "uncertainty_quantification": "No per-artifact uncertainty reported; validation is deterministic.",
            "calibration_quality": "Not applicable.",
            "out_of_distribution_performance": "Validation scripts evaluate provided metrics; for out-of-distribution claims (novel algorithms) additional assessment beyond script metrics is needed.",
            "validation_proxy_metrics": "Yes — the evaluation scripts implement proxy metrics appropriate to each task.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for claims beyond metric improvements.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical and benchmarked (formal metrics exist), but human judgement required for novelty/formal contributions.",
            "gap_mitigation_strategies": "Provide validate as a non-terminal check to reduce premature submission; make evaluation scripts read-only and datasets read-only to prevent tampering; encourage agents to use validate frequently to iterate.",
            "evidence_supporting_gap": "High incidence of 'Evaluation Error' terminations and differences between BestAttempt and BestSubmission AUPs illustrate that agents frequently fail at the final validation/submit step even when intermediate validated attempts exist.",
            "evidence_contradicting_gap": "For many tasks agents successfully used validate iteratively and then submit final correct artifacts (examples with good BestSubmission scores), indicating that when format and artifact generation are handled correctly, validate/submit works well.",
            "computational_cost_ratio": "Not numerically provided. Running evaluation scripts often involves training/evaluating models which is the major computational cost, so validation is often the expensive part of a run compared to LLM token generation.",
            "uuid": "e2082.5"
        },
        {
            "name_short": "Performance profiles / AUP",
            "name_full": "Performance profiles and Area Under Performance Profile (AUP) scoring",
            "brief_description": "An adapted evaluation aggregation method (from Dolan & Moré 2002 and AutoML Decathlon) used to compare methods across heterogeneous tasks by computing performance profiles and integrating area under the curve (AUP) to produce an aggregate score per method.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Performance profiles and AUP",
            "system_type": "evaluation aggregation metric",
            "scientific_domain": "benchmarking / optimization / AutoML evaluation",
            "output_type": "aggregate scalar scores (AUP) and performance profile curves that indicate fraction of tasks within a threshold of best performance",
            "novelty_level": "methodological (evaluation aggregation)",
            "generation_method": "N/A",
            "validation_method": "Used to aggregate per-task validation metrics across diverse scales by computing per-task performance ratios and a cumulative profile ρ_m(τ); AUP = area under that curve up to τmax.",
            "generation_performance": "N/A",
            "validation_performance": "Enables cross-task comparison of agent variants/backbones; paper reports AUP_bs@4 and AUP_ba@4 scores per model (Best Submission and Best Attempt aggregated over 4 runs) to capture generation vs submission ability.",
            "false_positive_rate": "N/A",
            "false_negative_rate": "N/A",
            "performance_vs_novelty": "AUP captures relative performance vs best-known method per task but does not measure novelty. Authors interpret AUP_ba (Best Attempt) as ceiling/exploration capability and AUP_bs (Best Submission) as ability to finalize validated outputs — allowing identification of gaps when exploration exceeds submission.",
            "generation_validation_comparison": "Directly used to compare BestAttempt (generation ceiling) vs BestSubmission (validated finalization); discrepancies between AUP_ba and AUP_bs provide quantitative evidence of generation-validation asymmetry.",
            "uncertainty_quantification": "Not applicable beyond the performance profile semantics.",
            "calibration_quality": "Not applicable.",
            "out_of_distribution_performance": "Not a direct measure of OOD; AUP measures relative performance to best method across tasks.",
            "validation_proxy_metrics": "Used to aggregate task-native metrics into a single comparative evaluation.",
            "human_validation_required": false,
            "human_validation_frequency": "N/A",
            "formal_verification_used": false,
            "domain_formalization_level": "Formal / methodological for cross-task metric aggregation.",
            "gap_mitigation_strategies": "Using separate AUP for BestAttempt vs BestSubmission surfaces issues where generation capacity is not realized in validated submissions; encourages designing agents that both explore and reliably finalize.",
            "evidence_supporting_gap": "The paper uses AUP_ba &gt; AUP_bs differences to show that some models can reach high attempt ceilings but fail to submit those artifacts, providing quantitative evidence of a generation-validation gap.",
            "evidence_contradicting_gap": "Where AUP_ba ≈ AUP_bs and completion rates are high (e.g., for O1-preview on many tasks), the aggregation suggests small generation-validation gaps.",
            "computational_cost_ratio": "N/A",
            "uuid": "e2082.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
            "rating": 2
        },
        {
            "paper_title": "MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation",
            "rating": 2
        },
        {
            "paper_title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
            "rating": 2
        },
        {
            "paper_title": "RE-bench",
            "rating": 2
        },
        {
            "paper_title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering",
            "rating": 1
        },
        {
            "paper_title": "AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?",
            "rating": 1
        }
    ],
    "cost": 0.024362749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MLGym: A New Framework and Benchmark for Advancing AI Research Agents
February 21, 2025</p>
<p>Deepak Nathani dnathani@ucsb.edu 
University of California
Santa Barbara</p>
<p>Lovish Madaan 
University College London</p>
<p>GenAI at Meta</p>
<p>Nicholas Roberts 
University of Wisconsin-Madison</p>
<p>Nikolay Bashlykov 
GenAI at Meta</p>
<p>Ajay Menon 
GenAI at Meta</p>
<p>Vincent Moens 
Amar Budhiraja 
GenAI at Meta</p>
<p>Despoina Magka 
Vladislav Vorotilov 
GenAI at Meta</p>
<p>Gaurav Chaurasia 
GenAI at Meta</p>
<p>Dieuwke Hupkes 
GenAI at Meta</p>
<p>Ricardo Silveira Cabral 
GenAI at Meta</p>
<p>Tatiana Shavrina 
GenAI at Meta</p>
<p>Jakob Foerster 
Yoram Bachrach 
William Yang Wang 
University of California
Santa Barbara</p>
<p>Roberta Raileanu raileanu@meta.com 
University College London</p>
<p>GenAI at Meta</p>
<p>University of Oxford</p>
<p>MLGym: A New Framework and Benchmark for Advancing AI Research Agents
February 21, 20256F0DAC48F6316D2F899D79B538E5A128arXiv:2502.14499v1[cs.CL]
We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks.This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents.MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory.Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task.We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet,Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5Pro.Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks.We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements.We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.</p>
<p>Introduction</p>
<p>Accelerating scientific discovery has been a long-standing ambition in artificial intelligence (AI) research, with early initiatives like the Oak Ridge Applied Artificial Intelligence Project in 1979 exploring (Team, 1985;Emrich et al., 1988;Johnson and Schaffer, 1994).More recent explorations enabled by advances in foundation models (Achiam et al., 2023;Anthropic, 2024;Team et al., 2024;Dubey et al., 2024) provide a proof-of-concept of a fully automated pipeline for end-to-end paper generation (Lu et al., 2024).In the future, we envision AI Research Agents capable of independently conducting literature search, generating scientific hypotheses, designing experiments, implementing new methods, analyzing results, disseminating findings by writing scientific papers, and applying this research in products, thus assisting with all parts of the research process.Such agents should be capable of both working fully autonomously, or be guided by human supervision, taking into account feedback from users.This vision stems from the recognition that AI, with its capacity to process vast datasets and discern complex patterns, could accelerate scientific breakthroughs in areas such as drug discovery and materials science by identifying promising drug candidates or predicting the properties of novel materials (Hessler and Baringhaus, 2018;Schneider et al., 2020;Guo et al., 2021).Unlike traditional methods, AI agents can reveal hidden interdisciplinary relationships by analyzing vast knowledge graphs, leading to novel insights and solutions for complex challenges like climate modeling.By automating laborious tasks and exploring unconventional avenues, AI agents can liberate scientists to focus on higher-level cognitive activities, ultimately driving innovation and expanding the frontiers of knowledge.Machine learning (ML) research, with its emphasis on empirical validation and systematic experimentation in simulation, presents an ideal testbed for exploring and improving the utlity of LLMs for advancing scientific research.</p>
<p>However, the scientific method inherently relies on empirical validation, rigorous evaluation, and standardized benchmarks to ensure the reliability and reproducibility of findings.While significant progress has been made in developing AI agents for various domains (Yang et al., 2024;Wu et al., 2024;Ma et al., 2024;Deng et al., 2023;Wang et al., 2023), we currently lack comprehensive frameworks and benchmarks specifically designed to assess their capabilities in conducting open-ended AI research tasks in diverse domains.This absence of standardized evaluation tools hinders our ability to objectively measure progress and identify areas for improvement in this emerging field.et al., 2022a) literature, to more fairly assess the relative performance of LLM agents across tasks with their own distinct performance metrics.</p>
<p>To summarize our contributions, we (i) introduce MLGym, the first Gym environment for evaluating and developing AI Research Agents, (ii) release MLGym-Bench, a suite of diverse open-ended AI research tasks for evaluating LLM agents, (iii) propose a new evaluation metric for comparing multiple agents on a variety of tasks, and (iv) extensively evaluate frontier LLMs on MLGym-Bench.Finally, MLGym makes it easy for researchers and developers to integrate and evaluate new tasks, agents, or models.</p>
<p>In the rest of the paper, we discuss related LLM agent frameworks and benchmarks, provide an overview of the MLGym framework, introduce the mechanics behind MLGym-Bench and its evaluation, present our experimental setup and results, and conclude with a discussion of limitations and future extensions.</p>
<p>Capability Levels for AI Research Agents</p>
<p>We propose a hierarchical framework to categorize the capabilities of LLM agents for accelerating AI research.This framework consists of six levels, each representing a distinct degree of autonomy and scientific contribution.</p>
<p>Level 0: Reproduction At this level, LLM agents can reproduce existing research papers either with or without access to the original code.This level demonstrates a basic understanding of the research domain and the ability to replicate established results.</p>
<p>Level 1: Baseline Improvement At Level 1, LLM agents can improve performance on a benchmark given a baseline code that is not state-of-the-art (SOTA).This level indicates the ability to analyze and optimize existing solutions, even if they are not the most advanced.</p>
<p>Level 2: SOTA Achievement At Level 2, LLM agents can achieve SOTA performance on a benchmark given only a task description and access to the published literature before the invention of the SOTA approach, but no access to the SOTA paper or code.This level demonstrates the ability to come up with a solution to an open research problem which is as good as the one found by humans.</p>
<p>Level 3: Novel Scientific Contribution At Level 3, LLM agents can make a novel scientific contribution, such as coming up with a new method that establishes a new SOTA on multiple benchmarks, and is worthy of publication at a top ML conference such as NeurIPS.</p>
<p>Level 4: Groundbreaking Scientific Contribution At Level 4, LLM agents can identify key research questions, directions, solutions, and make a notable scientific contribution worthy of being published as an oral or best paper award at a prestigious ML conference such as NeurIPS.</p>
<p>Level 5: Long-Term Research Agenda At Level 5, LLM agents can pursue a long-term research agenda, coming up with the research questions, directions, and solutions, continuously producing scientific discoveries over the span of weeks, months, or years.LLMs at this level should be capable of paradigm-shifting research breakthroughs worthy of prizes such as Nobel or Turing.</p>
<p>By defining these capability levels, we provide a framework for evaluating frontier AI Research Agents.</p>
<p>MLGym-Bench focuses on Level 1: Baseline Improvement of the categorisation defined above.</p>
<p>Related Work</p>
<p>AI Research Frameworks and Benchmarks</p>
<p>Table 1 shows a comparison between MLGym and MLGym-Bench with other related LLM agent frameworks and benchmarks.Below, we expand on the differences between MLGym and these works.</p>
<p>First, MLGym is the first framework for AI Research Agents that provides a Gym interface, making it easy to integrate and train these agents using RL algoritms.MLGym-Bench is also the first benchmark to include tasks that require research on algorithms in multiple domains such as RL, game theory, or SAT.  1 Comparison of MLGym and MLGym-Bench with other related LLM agent frameworks and benchmarks.Algorithmic Tasks refers to the inclusion of tasks that require coming up with new algorithms such as reinforcement learning, game theory or SAT problems.Open-ended Research refers to the inclusion of tasks that are not fully solved by the research community and where multiple new solutions could be discovered such as language modeling, game theory or SAT problems.Flexible Artifacts refers to the allowance of different research artifacts such as model weights, reinforcement learning algorithms, or code capturing an agent's strategy.</p>
<p>Second, MLGym-Bench encompasses a wide range of open-ended AI research tasks, covering supervised learning, language modeling, reinforcement learning, game theory and SAT.In contrast, SWE-Bench/SWE-Agent (Yang et al., 2024) focuses on solving Github issues so the code changes either fix the code or not (as opposed to optmization tasks with finer-grained metrics, such as a loss metric in a supervised learning problem).Similarly, MLE-Bench (Chan et al., 2024) includes narrowly scoped machine learning tasks from Kaggle competitions.While these tasks have a spectrum of quality levels, they tend to be already solved by current state-of-the-art methods.On the other hand, MLAgentBench (Huang et al., 2024) contains both ML-specialized tasks (regression, classification, code speed improvements) and tasks focused on recent research challenges (e.g.CLRS reasoning corpus (Veličković et al., 2022), BabyLM challenge (Oba et al., 2023)).RE-bench (METR, 2024) also consists of broadly scoped ML engineering tasks which are hard to saturate and reward increasingly sophisticated approaches.ScienceAgentBench (Chen et al., 2024) incorporates data-driven scientific discovery tasks extracted from peer-reviewed publications, but which are so specific that they resemble Kaggle competition rather than open research questions.</p>
<p>Third, MLGym allows for flexible evaluation artifacts: it is sufficient to provide python code that the agent can call to examine the quality of its current solution, such as a model checkpoint or an RL algorithm.In contrast, MLE-Bench requires a CSV file to be submitted for grading each question and SWE-Bench/Agent require evaluating a piece of code through a collection of unit tests.MLAgentBench, RE-Bench and ScienceAgentBench provide Python scripts to compute the evaluation scores.</p>
<p>Finally, MLGym enables easy evaluation of both models and agents.To facilitate model evaluation, MLGym provides a default agentic harness that can be used out-of-the-box to evaluate any base model.</p>
<p>LLM Agents</p>
<p>Research on tool-augmented LLMs (Schick et al., 2023) has inspired a new research agenda of "agentic" LLMs (Kaddour et al., 2023;Wang et al., 2024a), where LLMs interact with an external environment.Existing work explores teaching LLMs to use tools or APIs (Schick et al., 2023;Qin et al., 2023), navigate the web (Nakano et al., 2022;Deng et al., 2023;Zhou et al., 2023), interface with operating systems (Wu et al., 2024), play games (Paglieri et al., 2024;Wang et al., 2023), or interact with other simulated (Wang et al., 2024b;Lin et al., 2023) or physical worlds (Zhang et al., 2024a).Evaluating agentic LLMs typically involves designing controlled environments, providing suitable tools, defining tasks and goals, and establishing quantitative metrics to measure the system's performance.</p>
<p>Building on these directions, Yoran et al. (2024) introduce AssistantBench, emphasizing the complexity of open-web navigation and showcasing how current systems struggle with realistic, time-consuming tasks such as monitoring real-estate markets or identifying nearby businesses.Meanwhile, Kapoor et al. (2024) highlight the importance of standardized evaluation protocols that consider both accuracy and cost, warning against overfitting and advocating for more reproducible benchmarks.Extending these concerns to multi-dimensional environments, Liu et al. (2023) propose AgentBench-a suite of eight interactive settings that test agents' capacity for reasoning, decision-making, and long-term instruction following.Similarly, Mialon et al. (2023) focus on holistic planning skills through GAIA, a benchmark designed to assess performance on real-world questions requiring robust tool-use and multimodal reasoning, revealing substantial gaps between human-level proficiency and current LLMs.Finally, Trivedi et al. (2024) emphasize the necessity of sophisticated tool integration with AppWorld, an interactive environment where agents must operate diverse applications via APIs and generate complex code in an iterative fashion.Collectively, these works underscore not only the breadth of agentic LLM capabilities but also the pressing need for systematic, multifaceted benchmarks that capture complex tasks with verifiable results and foster reproducible progress in the field.However, none of these works focuses on evaluating or developing LLM agents for open-ended AI research tasks.</p>
<p>Agents for Software Engineering and Data Science</p>
<p>In line with the principle of reproducibility and verifiability, software engineering tasks provide a testbed for LLM agents, where tasks can be tightly scoped and outcomes rigorously measured.Recent work has explored how agents can tackle code-level challenges in controlled settings that permit systematic evaluation.As discussed above, Yang et al. (2024) introduce SWE-agent, which operates within a constrained agentcomputer interface to facilitate file creation, repository navigation, and code testing-thereby enhancing both traceability and reproducibility on benchmarks such as SWE-bench and HumanEvalFix.Similarly, Wang et al. (2024c) describe OpenHands, a platform that restricts agent interactions to sandboxed environments for safer command execution and verifiable web browsing, and in doing so provides a standardized foundation for benchmarking.Magentic-One (Fourney et al., 2024) is another agentic system competent in software engineering but also augmented with web navigation capabilities, as demonstrated by its strong performance on the GAIA, AssistantBench and WebArena (Zhou et al., 2023) agentic benchmarks.On the other hand, Zhang et al. (2024b) achieve competitive perforemance on SWE-bench with AutoCodeRover, which, unlike the agentic approaches, solves Github issues by combining LLM-based programming with program representation as an abstract syntax tree.</p>
<p>Towards the goal of automating data science work, Li et al. (2024) introduce AutoKaggle, a multi-agent human-assisting system, and Grosnit et al. (2024) present AgentK v1.0, an end-to-end autonomous data science agent; both of these systems perform well on Kaggle competition data.Still within the realm of data science work, Lei et al. (2024) build Spider 2.0, a challenging benchmark and code agent framework for automating text-to-SQL workflows.Going one step further, Cao et al. (2024) introduce Spider 2-V, an autonomous multimodal agent coupled with a benchmark focusing on the automation of enterprise data science and engineering workflows.</p>
<p>More search-oriented approaches include SWE-Search (Antoniades et al., 2024), a multi-agent framework that marries Monte Carlo Tree Search (MCTS) with iterative refinement, enabling agents to continuously evaluate and improve their approaches to repository-level tasks.In a similar vein, Koh et al. (2024b) explore tree search for LLM agents and show that equipping LLM agents with best-first search boosts performane for the WebArena and VisualWebArena (Koh et al., 2024a) agentic benchmarks.Also on augmenting LLM agents with search, Yu et al. (2025) propose MCTS-based test-time search and self-learning techniques that yield better performance on VisualWebArena.Finally, Xia et al. (2024) demonstrate that even relatively simple approaches can excel when thoroughly monitored: an 'agentless' system follows a three-step process and outperforms more complex agent-based methods on SWE-bench Lite, underscoring the value of constrained, verifiable environments in driving reproducible gains for autonomous SWE agents.</p>
<p>Agents for Scientific Research</p>
<p>Controlled SWE contexts build the foundation for more complex automation while maintaining a reproducible and verifiable approach.However, just the software foundations alone are not sufficient to address the remaining gaps towards the goal of science acceleration.Going from the limited environments and well-defined tasks with metrics towards a less-defined area of open-ended questions, there are substantial efforts needed to boost the capabilities of research agents.For instance, coming up with automatable criteria to gauge scientific novelty or constructing theories inheriting the automated findings from heterogeneous disciplines are examples of areas that could use more refinement and experimentation.</p>
<p>Nevertheless, the first steps on this path can be started now -in the field of ML research and data science -since these areas represent for us a scientific playground with tasks that are both well-defined and have formal criteria of verifiability (benchmarks and tests), falsifiability (ablation studies and tests for data leakage, memorization, out of domain generalization, etc) and reproducibility.</p>
<p>Data Science</p>
<p>Many recent works approach both classic data science tasks and real-life repository-based tasks as a testbed for agents with a known test set and metrics.While based on similar grounds, the works differ in the resulting levels of autonomy of the agents.For instance, ML-Bench (Tang et al., 2024) focuses on explicit tasks within existing GitHub repositories -evaluating agents in code-centric setups without delving into open-ended objectives.By contrast, Data Interpreter (Hong et al., 2024) extends agent testing to broader data science problems, spanning coding tasks, mathematical reasoning, and a limited suite of open-ended applications (e.g., OCR, web search, and mini-game generation), thus reflecting a more flexible approach to autonomy.The agentic benchmark SUPER (Bogin et al., 2024) raises the bar by requiring the agent to formulate the task itself and iterate on NLP-related data and tasks within research repositories, thereby emphasizing self-directed problem-solving.</p>
<p>AI Research</p>
<p>The presence of models and simulations in machine learning itself inevitably leads to the fact that this area also becomes the object of automation.Having an agent formulating a task itself and approaching openended tasks naturally leads to automatic agentic enhancement of the machine learning methods themselves.AutoML (Eggensperger et al., 2019;Lindauer and Hutter, 2020;Tornede et al., 2023) and NAS (Elsken et al., 2019;Nasir et al., 2024) approaches have been previously paving the foundations of ML automation within environments with built-in restrictions (an explicit set of methods, definition of the search space and strategy), while the agentic approach can propose open-ended solutions without said specifications.</p>
<p>For example, MLAgentBench (Huang et al., 2024) consists of an environment for agents to solve 13 complex tasks ranging from improving image classification to language modeling, with the current state-of-the-art LLMs achieving 0% success rate for the most difficult of these tasks.The proposed pipelines for agents in the environment include designing and running experiments, analyzing the results, and iterating towards improving the defined metrics.Similarly, RE-Bench (Research Engineering Benchmark) (METR, 2024) is a set of 7 diverse and challenging ML tasks with the methodological addition of real human experts involvement and progress comparison: timed sessions for ML experts vs LLM agents.Authors state that the best agents achieve a score 4x higher than human experts when both are given a total time budget of 2 hours per environment.However, humans currently display better returns to increased time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2x the score of the top agent when both are given 32 total hours.MLE-bench (Chan et al., 2024) focuses on Kaggle tasks as a source for agentic evaluations.Agents are evaluated across well-defined metrics, datasets, and real competition result distribution.The attempts are limited to 24 hours.However, in contrast with MLGym, all these works contain a more narrow set of domains that do not assess algorithmic reasoning capabilities.Moreover, some of them do not provide a standardized agentic harness to allow for model evaluation, but they vary both the harnesses (also known as scaffolds) and the LLMs when comparing performances.While our work focuses on creating an evaluation framework with objective and standardized evaluation metrics, other recent works focus on developing an agentic harness for the more subjective task of generating papers based on end-to-end experimental cycles (Lu et al., 2024).</p>
<p>Scientific Discovery</p>
<p>Several recent works have approached scientific automation with LLM agents targeting the process of scientific discovery.DiscoveryWorld (Jansen et al., 2024) is a benchmark for scientific agents being evaluated in a game-like virtual discovery environment.120 tasks require an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions -for areas like proteomics, chemistry, archeology, physics, agriculture, rocket science, linguistics, or epidemiology.The custom simulation engine only supports a limited list of objects and 14 possible actions.A distinctive feature of the work is also that it focuses on general discovery skills rather than task-specific solution, and the assessment, space of objects and actions is common to all scientific domains.ScienceAgentBench (Chen et al., 2024), however, approaches differently the similar task of creating a discovery-based agentic benchmark: the tasks are based on 44 cherry-picked peer-reviewed publications that include data-driven discovery tasks with well-defined metrics.The scientific areas covered include bioinformatics, computational chemistry, geographical information science, and neuroscience yielding 102 tasks of various types, such as data processing, modeling or visualization.Each task is defined by Python-based evaluation environment, end result metrics and intermediate evaluation criteria.Special metrics control data contamination and agent shortcut issues.Comparing different baselines, including pure LLMs with prompting, authors state that execution feedback is necessary for agents to generate useful solutions.</p>
<p>The idea of execution feedback and iterative improvement for research tasks has been proposed in Re-searchAgent (Baek et al., 2024).Agentic concept-based approach with literature-based discovery shows great improvement for end-to-end iterative solution generation, also supported by knowledge-based vs random facts ablations.The agent is evaluated solely with subjective human preference annotation and automatic human preference evals.While covering structured aspects of end-to-end experimental pipeline (problem clarity, feasibility, significance, relevance, originality, method generalizability, innovativeness, experiment reproducibility, validity, etc), relying solely on human judgment without supporting it with objective metrics is insufficient, as Si et al. (2024) shows.</p>
<p>MLGym</p>
<p>An LLM agent can perform ML research/development by interacting with a shell environment through a sequence of commands.Given a task description, some starter code and access to its action and observation history, the LLM generates appropriate shell commands to accomplish research objectives like generating ideas, processing data, implementing new methods, training and evaluating models, analyzing the results, and reasoning about what experiments to run next.The agent is iteratively prompted to take actions based on the task description and execution feedback from previous commands, allowing it to develop and self-refine the solutions in-context.</p>
<p>The MLGym framework provides a unified framework for evaluating and developing agents and models for AI research tasks.We take inspiration from long existing field of RL and build a Gym (Brockman et al., 2016) environment that can execute shell commands in a local docker machine shell.MLGym provides access to four core components: Agents, Environment, Datasets, and Tasks.MLGym's modular design allows one to easily utilize and extend the library.For example, researchers can easily implement other agentic harnesses to improve performance, they can expand the environment by adding more tools for an agent, add more datasets within a given task (e.g., if the task is image classification they could add ImageNet in addition to Cifar-10), and they can even add more tasks to the MLGym benchmark.Below, we discuss each component in detail.</p>
<p>Agents</p>
<p>The Agent class provided by MLGym acts as a wrapper around a base LLM and provides functionality for integrating various base models, history processors, and cost management.Moreover, unlike other frameworks (Huang et al., 2024;Yang et al., 2024), MLGym separates the agent from the environment, allowing for easy integration of external agents.This also enables one to fairly compare different base models given the same agentic harness without the need of implementing their own agentic orchestration.</p>
<p>The agent is expected to take the history of all prior observations and actions as input and return the next action to take.The provided action is then passed to the environment, which executes the command and returns the next observation based on the command output.The agent can execute any bash command in the environment.In addition, it has access to a set of tools (i.e., bash scripts such as editing a file) that it can use similarly to any other bash command.MLGym provides an agent adapted from SWE-Agent (Yang et al., 2024) as a default agentic harness.We describe the design and configuration of the tools in Section 3.5.The full system prompt used can be found in Listing 1.</p>
<p>Environment</p>
<p>MLGym environments are designed as Gymnasium (gym) environments (Towers et al., 2024).The environment component is responsible for initializing a shell environment in a local docker machine, with all the required tools, installing task-specific python dependencies, copying all the necessary data and code in a separate agent workspace and managing interactions between the LLM agent and the system.Moreover, to support open-ended research tasks and make the environment safe and flexible, MLGym environment also manages permissions for various files and directories.Specifically, when running in a docker container, due to various security concerns associated with using a root user, we create a non-root user named "agent" and set the appropriate permissions for the working directory.</p>
<p>In this work, we make a conscious decision to decouple tools and ACI as defined in SWE-Agent (Yang et al., 2024) 1 .Note that this ensures that the agent and environment are not tightly coupled, allowing for easier implementation of other agentic architectures.Practically, this means that when the environment is initialized, it also initializes the tools in the working environment and a tool documentation is prepared which can be added to the LLM agent's prompt.More details about the tools are provided in Section 3.5.</p>
<p>Datasets</p>
<p>MLGym provides a simple abstraction for defining datasets through configuration files.It supports both locally stored and Hugging Face datasets.We decouple the dataset definition from the task definition, so that a single dataset can be used in multiple tasks.Similarly, a single task can have more than one dataset so that the agent's code can be evaluated across all of them to demonstrate the generality of the implemented method.</p>
<p>Moreover, if the dataset files are stored locally, the environment automatically copies the relevant files to the agent workspace with read-only permissions.This ensures that the agent cannot change the dataset files, which is important for reproducibility and cheating prevention.</p>
<p>If the dataset is stored in Hugging Face, the agent is given the dataset URL through the starter code or in the prompt and asked to utilize it.Note that if the LLM agent fails to follow instructions or uses a different dataset, the evaluation code will not work or result in performance issues.</p>
<p>Tasks</p>
<p>We provide an easy abstraction to define any ML research task using configuration files.Each task can incorporate one or more datasets, custom evaluation scripts (with read-only access), task-specific conda environment, optional starter code, training timeouts, and memory management settings.This provides a flexible framework for defining diverse open-ended ML research tasks covering a wide range of difficulty.For example, one can define an easier version of a task by providing a baseline code and a harder version by providing no starter code or one with bugs, thus creating a natural curriculum.</p>
<p>Evaluation is a critical component for any ML task.Every task requires a different evaluation protocol; thus, Kaggle-style evaluation as done in MLE-Bench (Chan et al., 2024) where the agent is expected to submit a CSV file is not feasible for every problem.For example, in reinforcement learning settings, the evaluation artifact is a set of models trained on a set of pre-defined random seeds, which is then used to get a mean reward across a set of environment seeds.Similarly for Game Theoretic tasks, it can be a Python file with a strategy function which will be evaluated against a fixed set of strategy functions.Since we aim to evaluate the agent on open-ended and diverse tasks, it is not possible to convert all submissions to a CSV format.To ensure extensibility to such open-ended tasks, the task definition is expected to provide an evaluation script and submission artifact instructions.The LLM agent can then be prompted to follow the submission instructions and write the appropriate code.Moreover, the evaluation script is read-only for the LM agent, so while it can inspect the evaluation format, it cannot modify the script to change the evaluation logic.All our design decisions for the Agent, Environment, Dataset, and Tasks are meant to reduce overhead on the developers' and researchers' side and enhance reproducibility in this newly emerging area.</p>
<p>Tools and ACI</p>
<p>Augmenting LLM agents with the ability of using external tools is a critical component for making progress on knowledge-intensive tasks.In this work, we extend the ACI (agent-computer interface) first introduced in SWE-Agent (Yang et al., 2024) with some additional features required for an ML research agent.Specifically, we extend the commands for search, navigation, file viewer, file editor and context management with our permission management system and introduce new commands for literature search and a memory module.For example, if the agent tries to open a file without read permission, the file viewer tool will generate textual feedback for the agent.Similarly, if agent tries to edit the evaluation script (which is marked as read-only), the edit tools will output a feedback string instead of failing silently.Literature search and the ability to maintain a experimental log in it's memory are crucial for the agent to surpass SOTA solutions on open-ended research tasks.</p>
<p>Similar to SWE-Agent, tools are defined as bash or python scripts and are made available as bash commands in the environment.</p>
<p>All tool documentation is provided to the agent in the system prompt.See Table 2</p>
<p>Validation and Submit</p>
<p>We provide two commands to the agent to validate the submission and submit the results.Both the validate and submit commands are used to run the evaluation script and give the agent feedback on its current score on the test set.However, while the submit command is a terminal action, i.e., the agent's trajectory is terminated, and the evaluation script is executed to log the final scores, the validate command can be used as many times as needed during the run to get the current performance on the test set.</p>
<p>Addition of a validation command helps the agent to continuously improve its performance on the test set.</p>
<p>Literature Search and PDF Parser</p>
<p>We provide the agent with two tools to find and extract knowledge from external sources.The Literature Search tool allows the agent to query the Semantic Scholar API to find research papers about a given query that have open-access PDFs available, and the PDF Parsing tool allows the agent to download PDFs and convert them into a text-based representation.The paper contents can be stored in the context window as well as the Memory Module for longer-term tasks.Combined, these two tools allow the agent to find and analyze research papers as part of its workflow.See Table 2 for more information about these tools and how they are called.</p>
<p>Memory Module -Research Logs</p>
<p>We introduce the Memory Module for MLGym, an important tool to improve the performance of agents on long-horizon AI research tasks.The Memory Module enables the agent to persistently store critical findings and successful training configurations using a structured memory system, overcoming the challenge of limited context retention in long tasks.During our experiments, we observed that when the agent has access to the memory module, it can retrieve the best training configuration from memory and continue to iterate on it (see Figure 11 and Figure 12).Without the memory module, the agent's trajectory can become longer than the model's context length, thus not being able to retrieve the best configuration, effectively forgetting older experiments and only being able to locally iterate on recent configurations.</p>
<p>The module is equipped with two core functions: memory_write and memory_read.The memory_write function allows the agent to store key insights and effective configurations by saving text data along with its corresponding embeddings and tags in JSON format.In contrast, the memory_read method retrieves the top-k most relevant stored entries based on cosine similarity with a given query, allowing the agent to review past knowledge and iterate from previously successful configurations.</p>
<p>Empirical results demonstrate the positive impact of the Memory Module on long-horizon tasks.Agents equipped with the Memory Module were able to sustain progress over extended sequences of trials, reusing optimal configurations and findings to achieve superior results compared to agents limited by fixed context windows.To further enhance its capabilities, we added the state of the memory to the system prompt (memory tags and number of records) so that the agent is aware of the type of data stored.Tags from a memory record are extracted by identifying the 3-gram most closely matching to the memory record.</p>
<p>This module significantly reduces the limitations of constrained context length, allowing agents to operate effectively in long experimental settings.However, it is an early version and there are many ways to improve the module.For example, one possible direction would be to introduce a more structured memory format, such as hierarchical or relational models, allowing for precise storage and retrieval of information and enabling more complex reasoning over stored knowledge.Another is to incorporate memory operations directly into the model's training or fine-tuning process to allow the agent to natively utilize stored knowledge for improved performance.Or using a sub-agent that will automatically manage the memory by selecting important insights, removing unnecessary entries, and updating the memory.Each of these directions would require extensive experimentation and rigorous testing to ensure robustness and scalability.</p>
<p>For all the experiments presented in this paper, the agent only uses the SWE-Agent tools and validation command.</p>
<p>MLGym-Bench</p>
<p>The primary motivation behind our benchmark is to challenge models across different aspects of machine learning, including data handling, model architecture, and strategic decision-making.By incorporating tasks from data science, game theory, computer vision, natural language processing, and reinforcement learning, the benchmark aims to provide a varied and comprehensive agent evaluation testbed.</p>
<p>The tasks included in the benchmark are carefully selected to represent real-world challenges, ensuring that models are tested on their ability to generalize and perform effectively across various scenarios.Each task is accompanied by standardized evaluation scripts and baseline implementations, providing a clear reference point for performance assessment and comparison.</p>
<p>The benchmark suite is structured into four main categories, each focusing on a specific domain of machine learning: Data Science, Game Theory, Computer Vision, Natural Language Processing, and Reinforcement Learning.Below we describe each of the tasks in the benchmark.</p>
<p>Data Science</p>
<p>House Price Prediction (Kaggle, 2016) In the House Price Prediction task, the goal is to predict housing prices using the Kaggle House Price dataset.This task evaluates models based on their ability to accurately predict prices from various features, using RMSE and R2 as performance metrics.The baseline for this task is a simple Ridge Regression model with minimal feature engineering.</p>
<p>3-SAT</p>
<p>3-SAT (Cook, 1971) In the 3-SAT task, the LLM agent is given a DPLL code and is prompted to optimize the variable selection heuristic.The associated DPLL code is stored in a read-only file, and the agent can inspect it to structure its heuristic function code, however, it cannot modify it.A simple random selection heuristic is used as a baseline and starter code for the LLM agent.The performance is measured by the total wall-clock time taken to solve a set of 100 generated 3-SAT instances.The instances are genereted using the algorithm described in Selsam et al. (2018).</p>
<p>Game Theory</p>
<p>We consider several tasks related to making strategic choices in iterated games, considering multiple well-known games.Specifically, we consider the task of producing code for a strategy for playing in a repeated two-player game.In each such task we provide an opponent strategy, in the form of an opponent bot for playing the game, and ask the agent to produce code for a strategy for best-responding to this opponent, i.e. provide code for a strategy that maximizes the score against that opponent.We very briefly review game theory terminology, with various textbooks covering this topic in more detail (Fudenberg and Tirole, 1991).</p>
<p>In a two-player normal form game G, players select actions simultaneously, with the outcome determined by the choices of both players.Let A 1 = {a 1 1 , . . ., a 1 k } be the (pure) strategies available to player 1 and let A 2 = {a 2 1 , . . ., a 2 m } be the strategies available to player 2. Denote the set of strategy profiles, consisting of a strategy choice for both players as A = A 1 × A 2 .The utility of the players depends on the actions selected by both for them, i.e. the payoffs are u : A → R n , where u(a) = (u 1 (a), u 2 (a)) for a ∈ A, and where each player i tries to maximize their individual utility u i .A mixed strategy is a probability distribution ∆ over pure strategies.Given a mixed strategy profile σ = (σ 1 , σ 2 ) the expected utility of u i of player i is
u i (σ 1 , σ 2 ) = (a1,a2)∈A σ 1 (a 1 )σ 2 (a 2 )u i (a 1 , a 2 ).
A repeated game consists of k rounds in which the players play the same underlying normal form game.The history at the j + 1'th round consists of the actions (pure strategies) chosen by both players in each of the rounds 1 to j.We denote by H the set of all possible such histories, so a strategy in a repeated game is a function a i : H → ∆(A), i.e. a function that takes the history of actions chosen in the previous round and provides a distribution over the actions the agents would take in the next round.In our tasks, a strategy in the repeated game is expressed as a piece of code that takes in the history (actions of both players in the previous rounds), and outputs an action for the next round (where the code may make some random choices, hence yielding a distribution over the selected next round actions).Given an opponent strategy a 2 , the goal of our agent is to produce a strategy that best responds to the opponent and produces a the maximal payoff, i.e arg max a1 u 1 (a 1 , a 2 ).Note that in this equation a 2 is a given opponent strategy expressed as a piece of code that takes the history over the previous rounds and selects an action for the next round (possibly making some random choices), and that the goal of an agent is to produce a 1 as a piece of code capturing the strategy of the first player.The agent optimization goal is selecting the code a 1 so as to maximize player 1's expected payoff u 1 against the fixed opponent a 2 .</p>
<p>We consider the repeated version of prominent games, which we briefly discuss here: iterated Prisoner's Dilemma (Flood, 1958;Fudenberg and Tirole, 1991;Axelrod, 1980), Battle of the Sexes (Cooper et al., 1989;Luce and Raiffa, 2012) and Colonel Blotto (Roberson, 2006).As our goals was to highlight how our agent framework could be used to solve game theoretic tasks, rather than providing a rigorous evaluation and analysis of many game theoretic environments, we only included few games.However, additional games could easily be added in.</p>
<p>Prisonner's Dilemma (Axelrod, 1980).In this game, two players each have two options: cooperate or defect.When both cooperate, they receive a moderate reward.If one defects while the other cooperates, the defector gets a high reward while the cooperator gets a low payoff.If both defect, they both receive a low payoff.Due to the structure of payoffs, although mutual cooperation yields the best collective outcome, individual incentives often push towards defection.We included a repeated game, consisting of k = 20 rounds of the game.In the repeated version, players remember previous interactions and can adjust their strategies based on the history consisting of the past outcomes.Repeating the stage game multiple times allows for the development of trust and cooperation, as players recognize that consistent cooperation can lead to better long-term benefits than short-term defection (Axelrod, 1980).As our opponent strategy we provided a simple model which randomizes between cooperation, defection, or actions chosen based only on the last round of the interaction.(Cooper et al., 1989).This is a simple game illustrating coordination challenges between two participants with different preferences.In the game, two participants have to agree on a venue (for instance where to go to spend an evening).There are two possible venues, and both players would rather make the same choice rather than making different choices.The strategic dilemma arises because as each player wants to coordinate their choice with the other, but they have a different ranking over the venues (one prefers the first venue and the other prefers the second venue).Similarly to the iterated Prisoner's Dilemma, we have used a repeated game with k = 20 rounds and used a simple opponent that makes random choices using the information from the last round.</p>
<p>Battle of Sexes</p>
<p>Colonel Blotto Game (Roberson, 2006).This game is a model of strategic allocation of limited resources under competition.Two players ("Colonels") must simultaneously distribute their resources (such as troops) over several alternative locations ("battlefields").The player who allocates more resources to a battlefield wins that battlefield.The overall winner is the player who wins the most battlefields.The key challenge arises from the fact that players must make their allocations without knowing how their opponent will distribute their resources.This yields an environment where players try and anticipate their opponent's moves to decide how to best allocate their own resources in order to maximize their chances of winning.A key insight from the game is the importance of diversification and unpredictability: it is harder to exploit an opponent who spreads resources across multiple battlefields and varies their strategy.Our target opponent used a very simple random allocation rule (re-normalizing to the overall budget of resources).</p>
<p>It is important to note that in all the game theoretic tasks, the agent is allowed to look at the opponent's strategy, and thus these tasks measure code understanding and the LLM's capabilities to exploit the opponent's strategy.In the future, we plan to add tasks where the opponent's strategy is not provided to the agent, and agent is pitted against multiple opponents in a round robin fashion, similar to the setup used in Axelrod's original Prisoner's Dilemma tournament.</p>
<p>Computer Vision</p>
<p>Image Classification (CIFAR-10) (Krizhevsky et al., 2009) The Image Classification CIFAR-10 task involves classifying images into one of ten classes using the CIFAR-10 dataset.This task tests the ability of models to learn visual patterns and features, with a baseline accuracy of 49.71% encouraging improvements Image Classification (Fashion MNIST) (Xiao et al., 2017) The Image Classification Fashion MNIST task involves classifying fashion items into predefined categories using the Fashion MNIST dataset.The agent is provided with a simple two layer CNN as a baseline and it has to optimize for the accuracy on the test set.</p>
<p>The agent can optimize the model architecture and the hyper-parameters for the training.(Lin et al., 2014) For the image captioning task, the agent has to write the modeling code and come up with a good architecture and training setup for the image-text pairs in the MS-COCO dataset.We provide a baseline code for training to the agent which uses an image encoder and text decoder.We use the MS-COCO training and validation sets after removing all images containing humans.The agent has to optimize for the BLEU scores (Papineni et al., 2002) computed over the model-generated captions and ground truth captions for a given image.</p>
<p>Image captioning (MS-COCO)</p>
<p>Natural Language Processing</p>
<p>For language, we test the agent's ability to understand and modify training setup for both Natural Language Understanding (NLU) and Natural Language Generation (NLG) as detailed below.</p>
<p>Natural Language Inference (Williams et al., 2018) In this task, the agent starts from a pre-trained BERT model (Devlin, 2018) and we provide the baseline code to fine-tune on the training set of the MNLI benchmark to the agent.The agent is expected to come up with good hyper-parameters and fine-tuning strategy to optimize the test set accuracy on MNLI.</p>
<p>Language Modeling (Jordan et al., 2024) In the Language Modeling task, the agent is expected to train a language model for next token prediction using a smaller version of the FineWeb (Penedo et al., 2024) dataset.The LLM Agent is provided with the dataset and the NanoGPT (Jordan et al., 2024) codebase as a baseline and starting point.We use version #8 from modded-nanogpt3 as the starting point.The training and validation sets contain 1.773B and 100M tokens, respectively.The perfomance metric is the perplexity of the trained model on the validation set.</p>
<p>Reinforcement Learning</p>
<p>MetaMaze Navigation (Miconi et al., 2020) The MetaMaze Navigation task simulates a grid-world environment where agents must navigate using local observations and reach the goal location.(Brockman et al., 2016) We use the continuous version of the Mountain Car environment introduced in Brockman et al. (2016), where the task is to learn a policy that drives a car up a steep hill in a continuous control environment.</p>
<p>Mountain Car Continuous</p>
<p>Breakout MinAtar (Young and Tian, 2019) The Breakout MinAtar task involves playing the arcade game Breakout in a simulated environment.This environment was introduced in Young and Tian (2019) and is a popular benchmark for evaluating reinforcement learning agents.</p>
<p>For all the RL tasks, we use the environments from the Gymnax library (Lange, 2022) and the PPO algorithm from Gymnax-blines4 as a baseline and starting code for the LLM agent.</p>
<p>Experimental Setup</p>
<p>Agent and Models</p>
<p>For our experiments, we utilize a SWE-Agent based model adapted specifically for the MLGYM environment.SWE-Agent follows a simple ReAct-style thought and action loop (Yao et al., 2023), where the agent is prompted with the ACI documentation, the task and dataset description, as well as lightweight generic instructions to act as a ML researcher.The agent is configured to use a single command per step, and is not allowed to use any interactive session commands (e.g., python REPL, vim).</p>
<p>We use a set of 5 state-of-the-art models for our experiments, OpenAI O1-preview, Gemini 1.5 Pro, Claude-3.5-sonnet-20241022(refered to as Claude-3.5-sonnet in the paper), Llama-3-405b-instruct, and GPT-4o.All the models are used with temperature=0.0 and top-p=0.95,with the exception for OpenAI O1-preview, which doesn't support changing the decoding parameters and has a default temperature=1.0.</p>
<p>Environment Configuration</p>
<p>The MLGYM environment is configured with several key parameters to facilitate effective interaction between the agent and the tasks:</p>
<p>• Window Configuration: The environment uses a window size of 1000 lines with an overlap of 2 lines, allowing the agent to effectively navigate and edit large files while maintaining context.</p>
<p>• Context Management: A processor maintains a rolling window with the five most recent interactions (action and observation), helping the agent maintain context about the most recent interactions while keeping the input size manageable.</p>
<p>• Command Interface: The environment provides a set of specialized commands beyond standard bash operations, including file navigation commands (goto, scroll_up, scroll_down), file editing commands (edit, insert) with linting support, file and directory search commands (search_file, search_dir, find_file), and evaluation commands (validate, submit).</p>
<p>A single agent run is limited to 50 steps (i.e.interactions with the environment), after which the agent is terminated and the last codebase state is autosubmitted.Moreover, to control the runtime of the agent and prevent it from simply increasing the number of parameters in the model, we set a task specific timeout for the training commands.</p>
<p>In the next section, we discuss the evaluation metrics used in our experiments.</p>
<p>Evaluation</p>
<p>In order to compare agents on MLGym, we aggregate the scores of each method-an agent architecture paired with a backbone model-across our tasks.There are many ways one can aggregate scores.Common options include computing the average score across tasks for each method or by computing the average ranking of each method across tasks.While simple, these approaches can weight metrics in undesirable ways and disproportionately penalize certain methods.Averaging across different metrics may unfairly weight the metrics differently based on their relative scales, and averaging ranks can disproportionately penalize methods that effectively solve a task but are tied with other methods that also solve the task.Rather than naive averaging of scores or rankings, we employ performance profile curves (Dolan and Moré, 2002), which allow us to compare relative performance gains across both methods and tasks.Performance profiles were originally developed to compare optimization techniques across a set of optimization problems.Since then, they have been used by the AutoML community to compare AutoML methods across diverse domains, each with their own domain-specific metrics (Tu et al., 2022;Roberts et al., 2022b).</p>
<p>One challenge when using performance profiles is that they produce a curve for each method (where a higher curve is better), rather than a direct ranking of methods.To address this, the AutoML Decathlon (Roberts et al., 2022a) competition introduced the AUP score, which computes the area under the performance profile curve for each method, where a higher value constitutes better performance.Variants of the AUP score have since been used to score the AutoML Cup5 and MLCommons AlgoPerf (Dahl et al., 2023) competitions.Next, we define performance profiles, the AUP score, and the details of their usage within MLGym.</p>
<p>Performance Profiles and the AUP Score</p>
<p>For a given method m, its performance profile curve is defined as
ρ m (τ ) = 1 |T | |{t ∈ T : log 10 r t,m ≤ τ }| r t,m = ℓ t,m min{ℓ t,m : m ∈ M } (1)
where M is the set of all methods, P is the set of tasks, ℓ t,m is the performance metric for a method m on task t, and r t,m is a quantity called the performance ratio.</p>
<p>Importantly, this definition assumes that the performance metric for each task, ℓ p,• , must be defined such that lower scores are better-we discuss our modification to this definition to support other scores in Section 6.2.</p>
<p>Performance profiles are parameterized by a threshold, τ , on the distance between the method m and the best scoring methods on each of the tasks.At a given threshold τ , performance profiles compute the proportion of tasks for which the method m is within τ of the best method for each task.</p>
<p>In order to derive a final score for each method m ∈ M , we compute the AUP score as
AUP m = τmax 1 ρ m (τ )dτ, (2)
where τ max is the minimum τ for which ρ m (τ ) = 1 for all m ∈ M .</p>
<p>Usage in MLGym</p>
<p>In the context of MLGym, a method is defined as a combination of an agent scaffolding and a backbone model.Since, in this work we use a single agent scaffolding (SWE-Agent), we are comparing the performance of different backbone models.Moreover, we adapt performance profiles and AUP scores to handle various edge cases introduced by our MLGym tasks.</p>
<p>• Metric Direction Handling.For metrics where higher values are better (e.g., accuracy, R2), we invert the performance ratio calculation and use the maximum score instead of the minimum:
r t,m = max{ℓ t,m : m ∈ M } ℓ t,m .
(3)</p>
<p>• Infeasible Method In order to be counted as a feasible method, an agent should produce at least one valid solution and beat the baseline, methods must outperform the baseline.Methods that don't produce any valid solution or underperform are marked as Infeasible.The score of an infeasible method is set to (1 + ε) × r t,m baseline , where r t,m baseline is the score obtained by the baseline method on task t.We set the value of ε = 0.05.</p>
<p>We report the metrics across 4 independent runs for each model on each task.Finally, since the LM agent can use the validate command to check the performance without ending the run, we maintain two separate sets of performance profiles and AUP scores for each model.</p>
<ol>
<li>Best Submission Profiles, ρ bs m (τ )@4, are computed using the best final submission across 4 runs.A submission is classified as a final submission in two cases: if the agent uses the submit command, or if the agent terminates without submitting and the last codebase state is used to evaluate the performance.</li>
</ol>
<p>Best Attempt Profiles, ρ ba</p>
<p>m (τ )@4, which are computed using the best attempt observed across 4 runs.Any valid call to the validate command is considered an attempt.</p>
<p>The resulting AUP scores provide complementary information:</p>
<p>• AUP bs m @4 indicates the model's ability to consistently submit its best attempt as the final solution.Note that to do this, the LM agent has to be able to keep an internal state of the best attempt and recover from any mistakes made after the best attempt was made.</p>
<p>• AUP ba m @4 captures the model's exploration capability and is an indicator of the ceiling of the model's performance.</p>
<p>Apart from the AUP scores and performance profiles, we also report the raw performance scores for each model on each task.Similar to performance profiles, we categorize the raw scores in two sets: Best Submission@4 and Best Attempt@4.</p>
<p>Results</p>
<p>AUP Scores and Performance Profiles</p>
<p>As detailed in the Section 6, we evaluate the performance of each model in the SWE-Agent based agent scaffolding using Performance Profiles and Area Under the Performance Profile (AUP) score.</p>
<p>P(ratio )</p>
<p>Best Attempt Profile@4 0.0 0.3 0.6 0.9 1.2</p>
<p>Best Submission Profile@4 Llama GPT-4o Claude Gemini O1-preview</p>
<p>Figure 2 Performance profiles comparing Best Attempt@4 and Best Submission@4 across all models and tasks.The x-axis shows the performance ratio threshold τ and the y-axis shows the fraction of tasks where a model achieves performance within τ of the best model.</p>
<p>Moreover, since our agent can log the performance of intermediate steps, we categorize the performance of each model using two categories: Best Submission and Best Attempt.Best Submission indicates the LLM agent's capability to produce a valid final solution for a task as well as the ability to remember to fall back to the best intermediate solution in case some experiments don't pan out.Whereas, Best Attempt indicates the potential ceiling of the LLM agent's capability to solve the given task.</p>
<p>Figure 2 shows the performance profiles for Best Attempt (on the left) and Best Submission (on the right).</p>
<p>Similarly, Table 4 shows the AUP scores for the Best Attempt and Best Submission for all models.</p>
<p>In our experiments, we found that OpenAI O1-preview is the best-performing model on aggregate across our set of tasks for both Best Attempt and Best Submission, with Gemini 1.5 Pro and Claude-3.5-Sonnetbeing close behind.</p>
<p>Model</p>
<p>Raw Performance Scores</p>
<p>To compare the performance of each model on each task, we also report aggregate metrics over 4 runs with different seeds, namely the Best Attempt@4 and Best Submission@4 in Table 5 and Table 6 respectively.</p>
<p>While OpenAI O1-Preview is not dominant in all tasks, with Gemini-1.5-Pro,Claude-3.5-Sonnet,and Llama-3.1-405b-Instructoccasionally taking the lead, it is consistently in the top performing models for most tasks and thus takes the top spot in the AUP scores and performance profiles.This shows that the performance profile is a good metric to compare the performance of different models on a set of tasks with a diverse set of metrics.</p>
<p>We also find that Llama-3.1-405b-Instruct and GPT-4o are the only models that fail to produce any valid solution for the Language Modeling and Breakout tasks, respectively.Table 6 Best Submission@4 scores for all models.Best scores are highlighted in blue .Note: ∞ indicates that the model was not able to produce even a single valid solution for submission or validation.</p>
<p>Computational Cost</p>
<p>As discussed in Kapoor et al. (2024), it is important to also consider the pareto curve of performance vs cost for a more comprehensive evaluation of the agents' capabilities and their computational cost.In this work, we do not compare different agent scaffoldings; however, the pareto curve can still be useful to choose the most balanced model for a set of tasks.Figure 3 shows the Best Attempt AUP@4 vs Average Cost for all models.We use Best Attempt AUP scores to for this plot to highlight the maximum performance achievable by each model for a given cost.</p>
<p>According to results discussed in Section 7.1, OpenAI O1-Preview is the best-performing model, however, it is also the most computationally expensive by a wide margin.In contrast, Gemini-1.5-Proand Claude-3.5-Sonnetare much more cost-effective while still reaching high performance not too far from OpenAI O1's, with Gemini-1.5-Probeing the most cost-effective.</p>
<p>Gemini-1.5-Pro is cheaper than both GPT-4o and Llama-3.1-405b-Instruct and provides massive performance gains relative to them.GPT-4o is one of the cheapest models to run but performs significantly worse than the top models, Claude-3.5-Sonnet,Gemini-1.5-Pro,or OpenAI O1-Preview.Overall, Gemini-1.5-Prostrikes the best balance between performance and cost on MLGym-Bench, being the cheapest model to run (approximately 9× cheaper than OpenAI's O1) while achieving 99% of OpenAI O1's AUP (which is the top performing model).</p>
<p>The API pricing for OpenAI O1-preview, GPT-4o, Claude-3.5-Sonnet,and Gemini-1.5-Prowas taken from their respective price pages and for Llama-3.1-405b-instruct was taken from together.ai.For details on API pricing, tokens spent, and context length please consult Table 8 7.4 Agent Behavior Analysis</p>
<p>Failure Mode Analysis</p>
<p>In this section we analyze the failure modes of our agents on MLGym-Bench tasks, using three key perspectives: termination error distribution, failed or incomplete run rates, and task-specific failure patterns.We collect trajectories across 11 tasks and 5 models with 4 different seeds.This results in a total of 220 trajectories with 20 and 44 trajectories for each task and model, respectively.</p>
<p>Termination Errors Figure 4 shows the distribution of different causes for termination encountered by each model during task execution, as indicated by the first word of the error message.We categorize the errors into the following types: context length exceeded, evaluation error, file permission error, cost limit exceeded, format error, and runtime error.</p>
<p>First, we observe that almost all models encounter Evaluation Error and is generally the most frequent final error, accounting for 75% of all termination errors.Evaluation Error is generally triggered by missing submission artefacts or incorrect submission format at the last step or when the submit command is issued.Gemini-1.5-Pro is the only model that does not submit any invalid solutions, with OpenAI O1-Preview and Claude-3.5-Sonnetbeing the runner ups.</p>
<p>OpenAI O1-Preview and Claude-3.5-Sonnetdemonstrate superior error handling capabilities with the lowest overall error rates.Cost Limit is the second most frequent error encountered by Claude-3.5-Sonnet,Gemini-1.5-Proand OpenAI O1-Preview, indicating that they could further improve performance if provided with more budget.However, it is interesting to note that Gemini-1.5-Pro is the most cost-effective model across all tasks but still encounters Cost Limit error most frequently among all models.</p>
<p>Failed and Incomplete Runs</p>
<p>The failed and incomplete run analysis in Figure 5 reveals significant variations in model reliability.If an agent run fails with a termination error without producing any valid intermediate submission, we mark it as failed.Whereas, if the run fails with a termination error but produces a valid intermediate submission i.e. at least one score on the test set is obtained, we mark it as incomplete.Note that the model's submission does not have to beat the baseline to be considered a valid intermediate submission.</p>
<p>We are not interested in the performance of the model's submission here, but rather the ability of the agent to produce a valid submission by following the given instructions.</p>
<p>GPT-4o exhibits the highest failure rate, while Gemini-1.5-Proand OpenAI O1-Preview achieve the best completion rates.While Claude-3.5-Sonnet is one of the top performing models across all tasks (Section 7.1), it has a high failure rate.Another interesting observation is that OpenAI O1-Preview has a high incompletion rate, but it always produces at least one valid solution for all tasks.</p>
<p>We report additional results and failure mode analysis in Section A.2.</p>
<p>Action Analysis</p>
<p>In this section, we analyze the overall action distribution, as well as across models and trajectory steps.To analyze the action distribution effectively, we group the actions according to categories defined in</p>
<p>Per-Model Action Distribution</p>
<p>Figure 7 shows the action distribution for each model.GPT-4o takes the least number of actions overall, indicating that the model either errors out or submits too early without reaching an optimal solution.This is consistent with the failure analysis shown in Figure 5.</p>
<p>Among the best-performing models, Claude-3.5-Sonnetand OpenAI O1-Preview perform the most number of actions within a run, while Gemini-1.5-Properforms the least number of actions.Consistent with the cost analysis discussed in Section 7.3, Gemini-1.5-Pro'slower trajectory length contributes to it being the most cost-effective model.</p>
<p>Per-Step Action Distribution</p>
<p>Figure 8 illustrates the distribution of actions taken by agents across trajectory steps.Initially, Bash commands are predominant, indicating that agents start by checking and setting up their environment with basic commands such as ls, pwd, cd etc.As the steps progress, Edit actions become the most frequent, reflecting the agents' focus on modifying and refining code.This is complemented by a consistent use of View commands, suggesting a pattern of iterative development where agents frequently review their changes.</p>
<p>Python and Validate commands are used steadily throughout, which indicates an iterative cycle of experiments and evaluation.Submit actions are sparse, typically appearing towards the end of the process, aligning with the finalization of tasks.However, we can observe the Submit action being used as soon as Step 5, which indicates that some models submit their solution too early and likely fail to reach an optimal solution to beat other models.</p>
<p>Interestingly, Search commands are rarely used, suggesting that agents might benefit from improved search strategies to enhance efficiency while editing code.</p>
<p>Overall, our analysis highlights a structured approach where agents begin with getting familiar with the environment and the task, conduct multiple iterations of experiments and validation, and conclude with and submission.We report additional action analysis in Section A.3.</p>
<p>Discussion and Limitations</p>
<p>Our findings highlight both the opportunities and ongoing challenges in leveraging large language models (LLMs) as agents for scientific workflows.The proposed MLGym framework and accompanying MLGym-Bench tasks demonstrate that modern LLM agents can successfully tackle a diverse array of quantitative experiments, reflecting advanced skills and domain adaptability.At the same time, our results reveal notable capability gaps, which point to several avenues for improvement:</p>
<p>• Scaling beyond ML tasks To further evaluate the agent's AI Research capabilities, it is essential to scale up the evaluation framework to accommodate large-scale domain-specific datasets, more complex tasks, as well as domains outside AI.This will enable the community to assess the robustness and generalizability of different methods, as well as identify potential limitations and areas for improvement.</p>
<p>• Interdisciplinary Ablations and Generalization Within the stage of method evaluation, one approach is to test the solutions for generalization:</p>
<p>automatically evaluating the applicability of a new method on different domains .For example, new LLM architectures like Mamba (Gu and Dao, 2024) could be automatically applied to data on DNA, chemical molecules, music generation, etc.</p>
<p>automatically running interdisciplinary and multidisciplinary ablations, where we systematically remove or modify specific components of the proposed ML system to assess their impact on performance.This will enable the community to more quickly identify the most critical factors contributing to generalization across different domains.</p>
<p>• Addressing Scientific Novelty While the agentic benchmarks have demonstrated their effectiveness in evaluating complex tasks in different areas, it is essential to acknowledge that proposed interdisciplinary extrapolation of methods is just one aspect of the broader scientific understanding of "novelty" and "discovery" (Popper, 2005;Langley, 1987).It is not yet clear if the notion of scientific novelty can be successfully automated or even formally defined in a form suitable for agents.For many scientific disciplines, development may be uneven and depend on the availability of open data, the development of the methods, metrics and definitions used.</p>
<p>• Data Openness Imperative Finally, we emphasize the importance of data openness in driving scientific progress.By making our representative 'corpus of the world' widely accessible, including scientific artifacts, reproducible code, and domain-specific data for modeling, we can facilitate collaboration and accelerate discovery.This imperative is crucial for advancing our understanding of complex systems and developing more effective solutions to real-world problems.Removing once accessible resources that have entered LLM training from public access can have an irreparable impact on the acceleration of scientific progress, as it becomes impossible to identify sources of facts, and it is impossible to attribute the out-of-distribution result from a scientific work from a hallucination or a completely new result.</p>
<p>Ethical Considerations</p>
<p>Conclusions</p>
<p>This paper presents MLGym and MLGym-Bench as initial steps toward building robust, flexible, and transparent LLM agents for AI research.As the field continues to evolve, improvements in long-context reasoning, better agent architectures, training and inference algorithms, as well as richer evaluation methodologies will be essential to fully harness LLMs' potential for scientific discovery, in general and for AI research in particular.By fostering collaboration among researchers in machine learning, scientific computing, and diverse application domains, we can move closer to a future where AI-driven agents meaningfully accelerate scientific research, all while maintaining verifiability, reproducibility, and integrity in scientific discovery.</p>
<p>Appendix</p>
<p>A Additional Results and Analysis</p>
<p>A.1 Computational Cost   These failure patterns align with the raw performance scores in Table 5 and Table 6, where we observe that tasks requiring complex architectural decisions (Language Modeling) or complex algorithms (Breakout, Meta Maze and Mountain Car Continuous).Traditional supervised learning tasks are handled more reliably across models, while the more advanced models demonstrate better error handling and completion rates overall.The replacement text is terminated by a line with only end_of_edit on it .All of the &lt; replacement text &gt; will be entered , so make sure your indentation is formatted properly .Python files will be checked for syntax errors after the edit .If the system detects a syntax error , the edit will not be executed .Simply try to edit the file again , but make sure to read the error message and modify the edit command you issue accordingly .Issuing the same command a second time will just lead to the same error message again .signature : edit &lt; start_line &gt;: &lt; end_line &gt; &lt; replacement_text &gt; end_of_edit arguments :</p>
<p>-start_line ( integer ) [ required ]: the line number to start the edit at -end_line ( integer ) [ required ]: the line number to end the edit at ( inclusive ) -r e p l a c e m en t _ t e x t ( string ) [ required ]: the text to replace the current selection with insert : docstring : inserts the given text after the specified line number in the open file .The text to insert is terminated by a line with only end_of_insert on it .All of the &lt; text_to_add &gt; will be entered , so make sure your indentation is formatted properly .Python files will be checked for syntax errors after the insertion .If the system detects a syntax error , the insertion will not be executed .Simply try to insert again , but make sure to read the error message and modify the insert command you issue accordingly .signature : insert &lt; line_number &gt; &lt; text_to_add &gt; end_of_insert arguments :</p>
<p>-line_number ( integer ) [ required ]: the line number after which to insert the text -text_to_add ( string ) [ required ]: the text to insert after the specified line If you ' d like to add the line ' print ( x ) ' you must fully write that out , with all those spaces before the code !Indentation is important and code that is not indented correctly will fail and require fixing before it can be run .</p>
<p>RESPONSE FORMAT :</p>
<p>Your shell prompt is formatted as follows : ( Open file : &lt; path &gt;) <cwd > You need to format your output using two fields ; discussion and command .Your output should always include <em>one</em> discussion and <em>one</em> command field EXACTLY as in the following example : DISCUSSION First I ' ll start by using ls to see what files are in the current directory .Then maybe we can look at some relevant files to see what they look like .''' ls -a ''' You should only include a * SINGLE * command in the command section and then wait for a response from the shell before continuing with more discussion and commands .Everything you include in the DISCUSSION section will be saved for future reference .</p>
<p>Please do not include any DISCUSSION after your action .If you ' d like to issue two commands at once , PLEASE DO NOT DO THAT !Please instead first submit just the first command , and then after receiving a response you ' ll be able to issue the second command .You ' re free to use any other bash commands you want ( e .g .find , grep , cat , ls , cd ) in addition to the special commands listed above .However , the environment does NOT support interactive session commands ( e .g .python , vim</p>
<p>) , so please do not invoke them .Your goal is to achieve the best possible score , not just to submit your first working solution .Consider strategies like validating your answer using the ' validate ' command , manually spot -checking predictions , building custom validation sets and grading functions , and comparing different algorithms .Once you have exhausted all possible solutions and cannot make progress , you can submit your final solution by using ' submit ' command .</p>
<p>INSTRUCTIONS</p>
<p>Figure 1
1
Figure 1 Diagram of MLGym, a unified framework designed to integrate diverse and open-ended AI research tasks into a single platform for developing and evaluating LLM agents on these tasks.</p>
<p>Existing works such as Huang et al. (2024); METR (2024); Chen et al. (2024) also use a script based evaluation approach, whereas MLE-Bench (Chan et al., 2024) uses a Kaggle style evaluation.</p>
<p>Figure 3
3
Figure 3Best Attempt AUP@4 vs cost for all models.The x-axis shows the API cost in USD and the y-axis shows the AUP@4 score.</p>
<p>Figure 4 Figure 5
45
Figure 4 Termination Error Distribution by model.The size of the bars corresponds to the number of times each model triggered an exit status.</p>
<p>Figure 12
12
Figure 12 Example of retrieving the best training configuration from memory and restarting exploration from it.</p>
<p>Table
BenchmarkGym Interface Algorithmic Tasks Open-Ended Research Flexible Artifacts Agentic HarnessMLGym (ours)MLE-BenchSWE-Bench/AgentMLAgentBenchRE-BenchScienceAgentBench</p>
<p>for a description of the available tools.
CategoryToolArgumentsDocumentationSWE-Agent ToolsExtended ToolsLiterature Search literature_search&lt; query &gt; [&lt; num_results &gt;]query Semantic Scholar API for papers with attached PDFsparse_pdf_url&lt; url &gt;downloads and extracts the contents of a PDF given a URLMemory Modulememory_write&lt; content_str &gt;save important results, configs or findings to memorymemory_read&lt; query_str &gt;retrieve top-2 elements from memory most similar to a query
Search search_dir &lt; search_term &gt; [&lt; dir &gt;] searches for the search term in all files in dir search_file &lt; search_term &gt; [&lt; file &gt;] searches for the search term in the given file find_file &lt; f ile_name &gt; [&lt; dir &gt;] finds all the files with the given name in dir File Viewer open &lt; path &gt; [&lt; line_number &gt;] opens the given file and goes to the line number goto &lt; line_number &gt; moves the window to show the line number scroll_down moves the window down 1000 lines scroll_up moves the window up 1000 lines File editing create &lt; filename &gt; creates a new file insert &lt; line_number &lt; text_to_add &gt; inserts the given text at line number in the open file edit &lt; start_line &gt;:&lt; end_line &lt; replacement_text &gt; replaces the given lines with the given text in the open file Evaluation validate validates the current submission file and returns the metrics on the test set submit submits the current code and terminates the session</p>
<p>Table 2
2
List of tools available to agents.Required arguments are enclosed in &lt;&gt; and optional arguments are in [].</p>
<p>Table 3
3
List of tasks included in MLGym-Bench along with their respective problem setting, domain, and datasets.</p>
<p>Table 4
4
AUP@4 scores for the best attempt and best submission across all models.Best scores are highlighted in blue .</p>
<p>Table 5
5
Best Attempt@4 scores for all models.Best scores are highlighted in blue .Note: ∞ indicates that the model was not able to produce even a single valid solution for submission or validation.
TaskMetricBaseline Llama3.1-405b-instruct GPT-4o Claude-3.5-Sonnet Gemini-1.5-Pro OpenAI o1CIFAR-10Accuracy0.4970.5280.7330.8940.7580.854Battle of SexesAverage Reward1.0231.2561.1441.4391.4431.439Prisoners DilemmaAverage Reward2.3722.5622.5822.5632.632.571BlottoAverage Reward-0.2480.0410.0470.2280.0880.247House Price PredictionR 2 Score0.880.9080.8950.9120.9080.931Fashion MNISTAccuracy0.7830.8760.9270.9450.9160.906MS-COCOBLEU Score0.2790.2940.1110.1250.1310.135MNLIValidation Accuracy0.5250.7770.8190.8300.8380.836Language ModelingValidation Loss4.673∞4.3614.4764.1663.966BreakoutAverage Score48.81758.87∞17.73571.38963.518Mountain Car Continuous Average Reward33.79418.692-216.62136.31392.51396.335Meta MazeAverage Return15.73426.7447.82348.56222.88934.9863-SAT HeuristicWall-Clock Time (s)16.15813.93613.67615.72814.3613.83</p>
<p>Table 2 :
2
Edit , View , Search , Validate and Submit .We treat validate and submit as two separate categories.Additionally, we have two open-ended categories: Python and Bash .All the actions that match the regex patterns python.<em>,deepspeed.</em>,torchrun.*areconsideredasPythonactions.These actions usually correspond to the agent attempting to run a model evaluation or training script.All other actions are grouped under Bash category, i.e. are considered as open-ended bash commands.Overall Action DistributionFigure6shows the action distribution across all runs.File commands such as Edit and View are one of the most frequently used commands with Edit accounting for 50% of the total actions.Whereas, Search commands are rarely used, accounting for only 1% of the total actions.This distribution suggests that models spend a significant portion of their time in an iterative development cycle of editing and viewing files.Additionally, we observe a trend of regular experimental evaluation and periodic validation of solution by the frequent use of Python and Validate commands.Action distribution across all runs.We group the actions into categories following the grouping defined in Table2and Section 7.4.2.
3500 40003,7082000 1750Edit Validate ViewSubmit SearchPython Bash3000150025001250Count2000Count100015001,45875010001,066835500500449250226Edit Figure 6 Claude Python Validate View Bash Submit Search 11 0 0 Figure 7 Action distribution for each model. We group O1-preview Llama Gemini GPT-4othe actions into categories following the grouping definedin Table 2 and Section 7.4.2.</p>
<p>Action distribution for each step.We group the actions into categories following the grouping defined in Table2and Section 7.4.2.
250Edit ViewSubmit SearchPython BashValidate200Count15010050015101520253035404550Step NumberFigure 8</p>
<p>AI agents proficient in tackling open research challenges like those in our benchmark could catalyze a remarkable acceleration in scientific advancement.This prospect is exhilarating yet demands a meticulous comprehension of model progress to ensure responsible and controlled deployment of such breakthroughs.MLGym-Bench, for instance, can serve as a metric for model autonomy within OpenAI's Preparedness Framework, autonomous capabilities in Anthropic's Responsible Scaling Policy, and ML R&amp;D in Google DeepMind's Frontier Safety Framework.Should AI agents become adept at autonomously conducting AI research, the positive impacts could be multifaceted, encompassing accelerated scientific progress in healthcare, climate science, and other domains, expedited safety and alignment research for models, and economic growth spurred by the development of novel products.The ability of agents to deliver high-quality research could signify a transformative stride in the economy.Nonetheless, agents capable of executing open-ended AI research tasks, such as enhancing their own training code, could augment the capabilities of cutting-edge models at a pace outstripping human researchers.If innovations outpace our ability to comprehend their ramifications, we risk developing models with catastrophic harm or misuse potential without parallel advancements in securing, aligning, and controlling such models.We believe a model proficient in solving a substantial portion of MLGym-Bench likely possesses the capacity to execute numerous open-ended AI tasks.We are open-sourcing MLGym and MLGym-Bench to foster understanding and research into the agentic capabilities of AI Research Agents and promote transparency regarding acceleration risks in frontier AI labs.In doing so, we acknowledge the limitations of MLGym-Bench and strongly encourage the development of additional evaluations of automated AI research capabilities, particularly those tailored to the workflow of researchers training frontier models.</p>
<p>Table 7
7
Table7lists the resources needed to run the agent on each task in MLGym-Bench.Each task has a set Training Timeout, which is used as the time limit for any python commands.Specifically, it is used to prevent the agent from continuously scaling the model parameters.Average agent runtime and Baseline runtime show the wall clock time for each agent run and the provided baseline code, respectively.Computational resources required for each task in MLGym-bench.
TaskTraining Timeout GPUs/Agents Average Agent Runtime Baseline Runtime (mins)CIFAR-1030m14h15Battle of Sexes30m030m5Prisoners Dilemma30m030m5Blotto30m030m5House Price Prediction30m11.5h10Fashion MNIST30m12h10MS-COCO40m17MNLI40m122Language Modeling40m24h20Breakout30m22h15Mountain Car Continuous30m22h15Meta Maze30m22h153-SAT Heuristic30m030m5</p>
<p>Table8lists the average input and output tokens and associated pricing for each model across all tasks in MLGym-Bench.We report the model pricing as listed by their respective providers.Llama3.1-405b-Instructpricing is taken from Together AI.Note that for this work, we used the open-weights model checkpoint with FP-8 precision, hosted on Meta Internal servers.Gemini-1.5-Procharges 2X for using the long-context capabilities, i.e for input and output exceeding 128K tokens.However, in our experiments, we do not observe Gemini using the long-context capabilities, so the final price is reported based on the normal pricing.
Avg. UsagePricingModelInput Output Input Output Context LengthLlama3.1-405b-instruct  *  30434825123.503.50128kClaude-3.5-Sonnet707704124153.0015.0200kGemini-1.5-Pro  †28261316331.255.002MGPT-4o26688624292.5010.0128kOpenAI O1-Preview3688986070415.060.0128k</p>
<p>Table 8
8
Model pricing, token usage and context length details.Model Pricing is in USD per 1M tokens.Number of Failed and Incomplete runs per task.The criteria for marking a run as incomplete or failed is described in Section 7.4.1Continuing the discussion from Section 7.4.1,we show the failed and incomplete runs on each task to understand the difficulty distribution of tasks.Language Modeling and all Reinforcement Learning tasks (Meta Maze, Mountain Car Continuous and Breakout) prove the most challenging, with the highest failure rates.Whereas, Fashion MNIST and Prisoner's Dilemma show the lowest failure rates, with all models producing a valid intermediate solution and a valid submission for all seeds.
Llama3.1:
*</p>
<dl>
<dt>submit : docstring : submits your current code and terminates the session signature : submit validate : docstring : validates your current submission file and returns the metrics on test set signature : validate Please note that THE EDIT and INSERT COMMANDS REQUIRES PROPER INDENTATION .</dt>
<dd>
<p>Now , you ' re going to train a model to improve performance on this task .Your terminal session has started and you ' re in the workspace root directory .You can use any bash commands or the special interface to help you .Edit all the file you need or create a new training script .Remember , YOU CAN ONLY ENTER ONE COMMAND AT A TIME .You should always wait for feedback after every command .When you ' re satisfied with all of the changes you ' ve made , you can run your training file .Your training file should include the logic for saving the prediction for the ' test ' set of the task .The submission file should be named ' submission .csv ' with the instance id and prediction column .A sample submission file is given in the workspace and you can read it to get a better understanding of the submission format .Note however that you cannot use any interactive session commands ( e .g .python , vim ) in this environment , but you can write scripts and run them .E .g .you can write a python script and then run it with ' python &lt; script_name &gt;. py '.NOTE ABOUT THE EDIT AND INSERT COMMANDs : Indentation really matters !When editing a file , make sure to insert appropriate indentation before each line !IMPORTANT TIPS : 1. Always start by trying to understand the baseline script if available .This will give you an idea of one possible solution for the task and the baseline scores that you have to beat .2. If you run a command and it doesn ' t work , try running a different command .A command that did not work once will not work the second time unless you modify it !</p>
</dd>
</dl>
<p>As of the latest release, SWE-Agent also decouples tools/ACI from the agent.
https://www.kaggle.com/datasets/yasserh/housing-prices-dataset
https://github.com/KellerJordan/modded-nanogpt
https://github.com/RobertTLange/gymnax-blines
https://2023.automl.cc/competitions/automl-cup/
https://www.together.ai/pricing
https://en.wikipedia.org/wiki/Borda_count
AcknowledgmentsWe thank Sten Sootla, Mikayel Samvelyan, Sharath Chandra Raparthy, Mike Plekhanov, and Rishi Hazra for many insightful discussions about evaluating and developing AI Research Agents.A.3 Action AnalysisExtending the results presented in Section 7.4.2, Figure10shows the action distribution on each task.The bars represent the sum of all the actions taken by all models on a particular task.We notice that RL tasks have the higest action count, while Game Theoretic tasks have the lowest action count.Algorithmic Tasks such as 3-SAT and Game Theory (Blotto, Prisonner's Dilemma and Battle of Sexes) also have the highest amount of validation actions, signifying a quick experimental cycle.Similarly, all RL tasks have the most complex codebases among all MLGym-Bench tasks and thus agent extensively use the View commands.A.4 Model RankingsTable9and Table10show each model's ranking based on Best Attempt@4 and Best Submission@4 scores respectively.The aggregate ranks are computed using the BORDA 7 count method.The aggregated rankings computed using BORDA count method align with the AUP score results as shown in Table4.However, similar to any ranking-only metric, it does not convey the relative difference between each model's performance.Table9Individual and Aggregate Ranking of models based on Best Attempt@4.We use the BORDA method to compute the aggregate ranks.Table10Individual and Aggregate Ranking of models based on Best Subimission@4.We use the BORDA method to compute the aggregate ranks.A.5 Memory UtilizationFigure11and Figure12show the agent using the memory module to store and retrieve specific experimental results and use them to submit the best possible model.7.Your each action should take less than 1800 seconds to complete .If your action doesn ' t finish within the time limit , it will be interrupted .( Current Step : 0 , Remaining Steps : 50) ( Open file : n / a ) ( Current directory : / home / agent / i m a g e C l a s s i f i c a t i o n C i f a r 1 0 ) bash -
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. AI Anthropic2023arXiv preprintThe claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 1, 2024</p>
<p>Swe-search: Enhancing software agents with monte carlo tree search and iterative refinement. Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, William Wang, 2024</p>
<p>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. Robert Axelrod, ; Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, Journal of conflict resolution. 2411980. April 2024Effective choice in the prisoner's dilemma</p>
<p>SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories. Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot, September 2024</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, Openai gym. 2016</p>
<p>Spider2-v: How far are multimodal agents from automating data science and engineering workflows?. Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu, 2024</p>
<p>Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Lilian Patwardhan, Aleksander Weng, Mądry, MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering. October 2024</p>
<p>Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery. October 2024</p>
<p>The complexity of theorem-proving procedures. A Stephen, Cook, Proceedings of the third annual ACM symposium on Theory of computing. the third annual ACM symposium on Theory of computing1971</p>
<p>Communication in the battle of the sexes game: some experimental results. Russell Cooper, Douglas V Dejong, Robert Forsythe, Thomas W Ross, The RAND Journal of Economics. 1989</p>
<p>George Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan Bae, Justin Gilmer, Abel Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan, Daniel Snider, Ehsan Amid, and Peter Mattson. Benchmarking neural network training algorithms. 062023</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, 2023</p>
<p>Jacob Devlin, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>Benchmarking optimization software with performance profiles. Elizabeth D Dolan, Jorge J Moré, 10.1007/s101070100263Mathematical Programming. 1436-4646912January 2002</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Pitfalls and best practices in algorithm configuration. Katharina Eggensperger, Marius Lindauer, Frank Hutter, Journal of Artificial Intelligence Research. 642019</p>
<p>Neural architecture search: A survey. Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, Journal of Machine Learning Research. 20552019</p>
<p>Potential applications of artificial intelligence to the field of software engineering. Emrich, Agarwal, Jairam, Oak Ridge National Lab Murthy, Tn, 1988Technical report</p>
<p>Some experimental games. M Merrill, Flood, Management Science. 511958</p>
<p>Magentic-one: A generalist multi-agent system for solving complex tasks. Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Friederike Zhu, Grace Niedtner, Griffin Proebsting, Jack Bassman, Jacob Gerrits, Peter Alber, Ricky Chang, Robert Loynd, Victor West, Ahmed Dibia, Ece Awadallah, Rafah Kamar, Saleema Hosn, Amershi, 2024</p>
<p>Game theory. Drew Fudenberg, Jean Tirole, 1991MIT press</p>
<p>Antoine Grosnit, Alexandre Maraval, James Doran, Giuseppe Paolo, Albert Thomas, Refinath , Shahul Hameed, Nabeezath Beevi, Jonas Gonzalez, Khyati Khandelwal, Ignacio Iacobacci, Abdelhakim Benechehab, Hamza Cherkaoui, Youssef Attia El-Hili, Kun Shao, Jianye Hao, Jun Yao, Balazs Kegl, Haitham Bou-Ammar, and Jun Wang. Large language models orchestrating structured reasoning achieve kaggle grandmaster level. 2024</p>
<p>Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Tri Dao, 2024</p>
<p>Artificial intelligence and machine learning in design of mechanical materials. Kai Guo, Zhenze Yang, Chi-Hua Yu, Markus J Buehler, Materials Horizons. 842021</p>
<p>Artificial intelligence in drug design. Gerhard Hessler, Karl-Heinz Baringhaus, Molecules. 231025202018</p>
<p>Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, and Chenglin Wu. Data Interpreter: An LLM Agent For Data Science. March 2024</p>
<p>MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, April 2024</p>
<p>DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, June 2024</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. 2023arXiv preprint</p>
<p>Oak Ridge National Laboratory: the first fifty years. Leland Johnson, Daniel Schaffer, 1994Univ. of Tennessee Press</p>
<p>Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, Boza Vlado, You Jiacheng, Franz Cesista, Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the nanogpt baseline. 2024</p>
<p>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert Mchardy, arXiv:2307.10169Challenges and applications of large language models. 2023arXiv preprint</p>
<p>House prices -advanced regression techniques. Kaggle, January 24, 2025. 2016</p>
<p>Ai agents that matter. Sayash Kapoor, Benedikt Stroebl, Zachary S Siegel, Nitya Nadgir, Arvind Narayanan, 2024</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried, arXiv:2401.13649Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. 2024aarXiv preprint</p>
<p>Tree search for language model agents. Jing Yu Koh, Stephen Mcaleer, Daniel Fried, Ruslan Salakhutdinov, 2024b</p>
<p>Learning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, 2009</p>
<p>gymnax: A JAX-based reinforcement learning environment library. Robert Tjarko, Lange , 2022</p>
<p>Spider 2.0: Evaluating language models on real-world enterprise text-to-sql workflows. Langley ; Fangyu, Jixuan Lei, Yuxiao Chen, Ruisheng Ye, Dongchan Cao, Hongjin Shin, Zhaoqing Su, Hongcheng Suo, Wenjing Gao, Pengcheng Hu, Victor Yin, Caiming Zhong, Ruoxi Xiong, Qian Sun, Sida Liu, Tao Wang, Yu, 1987. 2024MIT pressScientific discovery: Computational explorations of the creative processes</p>
<p>Autokaggle: A multi-agent framework for autonomous data science competitions. Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, Wanjun Zhong, Wangchunshu Zhou, Wenhao Huang, Ge Zhang, 2024</p>
<p>Agentsims: An open-source sandbox for large language model evaluation. Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, Qin Chen, 2023</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>Best practices for scientific research on neural architecture search. Marius Lindauer, Frank Hutter, Journal of Machine Learning Research. 212432020</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. AgentBench: Evaluating LLMs as Agents. August 2023</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. August 2024</p>
<p>Games and decisions: Introduction and critical survey. Duncan Luce, Howard Raiffa, Courier Corporation. 2012</p>
<p>Sciagent: Tool-augmented language models for scientific reasoning. Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, Aixin Sun, Hany Awadalla, Weizhu Chen, 2024</p>
<p>Evaluating frontier ai r&amp;d capabilities of language model agents against human experts, 11 2024. METR</p>
<p>GAIA: A benchmark for General AI Assistants. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann Lecun, Thomas Scialom, November 2023</p>
<p>Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity. Thomas Miconi, Aditya Rawal, Jeff Clune, Kenneth O Stanley, 2020</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, 2022</p>
<p>Llmatic: neural architecture search via large language models and quality diversity optimization. Sam Muhammad Umair Nasir, Julian Earle, Steven Togelius, Christopher James, Cleghorn, proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference2024</p>
<p>Babylm challenge: Curriculum learning based on sentence complexity approximating language acquisition. Miyu Oba, Akari Haga, Akiyo Fukatsu, Yohei Oseki, Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning2023</p>
<p>Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, arXiv:2411.13543Benchmarking agentic llm and vlm reasoning on games. 2024arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>The fineweb datasets: Decanting the web for the finest text data at scale. Guilherme Penedo, Hynek Kydlíček, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, 2024</p>
<p>The logic of scientific discovery. Karl Popper, 2005Routledge</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023</p>
<p>The colonel blotto game. Brian Roberson, Economic Theory. 2912006</p>
<p>Automl decathlon: Diverse tasks, modern methods, and efficiency at scale. Nicholas Roberts, Samuel Guo, Cong Xu, Ameet Talwalkar, David Lander, Lvfang Tao, Linhang Cai, Shuaicheng Niu, Jianyu Heng, Hongyang Qin, Minwen Deng, Johannes Hog, Alexander Pfefferle, Ammanaghatta Sushil, Arjun Shivakumar, Yubo Krishnakumar, Rhea Wang, Frank Sukthanker, Euxhen Hutter, Tien-Dung Hasanaj, Mikhail Le, Yuriy Khodak, Kashif Nevmyvaka, Frederic Rasul, Anderson Sala, Junhong Schneider, Evan Shen, Sparks, Proceedings of the NeurIPS 2022 Competitions Track. Marco Ciccone, Gustavo Stolovitzky, Jacob Albrecht, the NeurIPS 2022 Competitions TrackPMLR28 Nov-09 Dec 2022a220of Proceedings of Machine Learning Research</p>
<p>AutoWS-bench-101: Benchmarking automated weak supervision with 100 labels. Nicholas Roberts, Xintong Li, Tzu-Heng Huang, Dyah Adila, Spencer Schoenberg, Cheng-Yu Liu, Lauren Pick, Haotian Ma, Aws Albarghouthi, Frederic Sala, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022b</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>Rethinking drug design in the artificial intelligence era. Petra Schneider, Patrick Walters, Alleyn T Plowright, Norman Sieroka, Jennifer Listgarten, Robert A GoodnowJr, Jasmin Fisher, Johanna M Jansen, José S Duca, Thomas S Rush, Nature reviews drug discovery. 1952020</p>
<p>Learning a SAT solver from single-bit supervision. Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo De Moura, David L Dill, CoRR, abs/1802.036852018</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 2024</p>
<p>Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Liang Chen, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yin Fang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, and Mark Gerstein. ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code. June 2024</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.05530Gemini Team. 1985. 2024arXiv preprintArtificial Intelligence Task Team. Artifical intelligence and nuclear power</p>
<p>Automl in the age of large language models: Current challenges, future opportunities and risks. Alexander Tornede, Difan Deng, Theresa Eimer, Joseph Giovanelli, Aditya Mohan, Tim Ruhkopf, Sarah Segel, Daphne Theodorakopoulos, Tanja Tornede, Henning Wachsmuth, arXiv:2306.081072023arXiv preprint</p>
<p>Gymnasium: A standard interface for reinforcement learning environments. Mark Towers, Ariel Kwiatkowski, Jordan Terry, John U Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulão, Andreas Kallinteris, Markus Krimmel, K G Arjun, Rodrigo Perez-Vicente, Andrea Pierré, Sander Schulhoff, Jun Jet Tai, Hannah Tan, Omar G Younis, 2024</p>
<p>AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents. Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, Niranjan Balasubramanian, July 2024</p>
<p>NAS-bench-360: Benchmarking neural architecture search on diverse tasks. Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, Ameet Talwalkar, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022</p>
<p>The clrs algorithmic reasoning benchmark. Petar Veličković, Adrià Puigdomènech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, Charles Blundell, International Conference on Machine Learning. PMLR2022</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Jirong Wen, 10.1007/s11704-024-40231-1Frontiers of Computer Science. 2095-2228186December 2024a</p>
<p>Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, Ji-Rong Wen, User behavior simulation with large language model based agents. 2024b</p>
<p>OpenDevin: An Open Platform for AI Software Developers as Generalist Agents. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, H Hoang, Fuqiang Tran, Ren Li, Mingzhang Ma, Bill Zheng, Yanjun Qian, Niklas Shao, Yizhe Muennighoff, Binyuan Zhang, Junyang Hui, Robert Lin, Hao Brennan, Heng Peng, Graham Ji, Neubig, July 2024c</p>
<p>The multi-genre nli corpus. Adina Williams, Nikita Nangia, Samuel R Bowman, 2018</p>
<p>Os-copilot: Towards generalist computer agents with self-improvement. Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong, arXiv:2402.074562024arXiv preprint</p>
<p>Agentless: Demystifying LLM-based Software Engineering Agents. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, Lingming Zhang, July 2024</p>
<p>Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. Han Xiao, Kashif Rasul, Roland Vollgraf, 2017</p>
<p>John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering. Karthik Narasimhan, and Ofir PressMay 2024</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, ; , Jonathan Berant, July 2024Ofir Press</p>
<p>Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments. Kenny Young, Tian Tian, 2019</p>
<p>Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning. Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu, 2025</p>
<p>Building cooperative embodied agents modularly with large language models. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, Chuang Gan, 2024a</p>
<p>Autocoderover: Autonomous program improvement. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury, Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis. the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis2024b</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, arXiv:2307.13854A realistic web environment for building autonomous agents. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
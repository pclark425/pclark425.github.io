<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2236 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2236</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2236</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-280414191</p>
                <p><strong>Paper Title:</strong> 3D reconstruction of toys based on adaptive scaled neural radiation field</p>
                <p><strong>Paper Abstract:</strong> With the rapid development of computer vision technology, 3D reconstruction of toys under single-view conditions still faces significant challenges in terms of detail loss and color distortion. For this reason, this article proposes an adaptive scale neural radiance fields (AS-NeRF) model to enhance the accuracy and realism of 3D toy reconstruction. The method constructs a multi-task feature extraction network based on the Vision Transformer, which simultaneously extracts and fuses multidimensional features such as texture, shape, color, and depth through a task dynamic modulation mechanism and a dynamic adapter layer, providing a rich and accurate contextual feature representation. The NeRF model is enhanced to incorporate an adaptive scaling mechanism that dynamically optimizes rendering sampling accuracy according to the local complexity of the scene. Spectral sensing techniques are integrated to reproduce the true colors of materials accurately. Finally, the conditional diffusion model is deeply integrated with NeRF, and high-dimensional conditional vectors are used to guide the inverse diffusion process in generating unobserved images with consistent geometric structure and physical properties. Experiments on the Co3D toy dataset demonstrate that AS-NeRF significantly outperforms existing mainstream methods in terms of peak signal-to-noise ratio (PSNR), structural similarity (SSIM), loss of perceptions (LPIPS), and Chamfer distance, thereby verifying the validity and advantages of the proposed method for high-quality toy 3D reconstruction tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2236.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2236.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AS-NeRF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Scale Neural Radiance Fields (AS-NeRF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end 3D reconstruction system that combines a ViT-based multi-task feature extractor, an adaptive-scale NeRF with spectral-aware rendering, and a conditional diffusion model to produce high-fidelity toy reconstructions from one-to-few views.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AS-NeRF</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Integrated architecture with (1) MTL-FeatureNet — a Vision-Transformer based multi-task feature extractor with task-dynamic modulation and dynamic adapter layers; (2) an adaptive-scale NeRF that dynamically adjusts sampling/rendering accuracy per local complexity and adds a spectral decoder for multi-wavelength color; (3) a conditional diffusion model that uses high-dimensional conditional vectors from MTL-FeatureNet/NeRF to synthesize unobserved viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Task-dynamic modulation of multi-head attention (task relevance scores TR_i adjust attention via exp(TR_i)); dynamic adapter layer (task-specific scaling c_task, W_task, b_task); adaptive-scale function S(FA(G,x),G) computed via sigmoid(b * D_complex(CT(...))) that modulates sampling/rendering per 3D location; spectral decoder for wavelength-dependent color.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Single-/few-view 3D toy reconstruction (view synthesis and geometry recovery) on Co3D-Toys dataset (1–5 input views)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Multiple reported metrics vs baselines: PSNR — AS-NeRF increases as views increase; with two input views AS-NeRF achieves 35.0 dB PSNR (paper reports improvement of +1.4 dB vs SparseFusion and +5.3 dB vs PixelNeRF). With one view AS-NeRF improves PSNR by +2.4 dB vs SparseFusion and +5.4 dB vs PixelNeRF. SSIM — two-view SSIM = 0.80 (reported +0.08 vs SparseFusion, +0.10 vs PixelNeRF); single-view SSIM improvements of +0.05 vs SparseFusion and +0.10 vs PixelNeRF. Chamfer distance — single-view AS-NeRF = 4.6 (units as reported), two-view = 4.2. Perceptual loss and LPIPS reported lower than baselines across resolutions and noise levels (qualitative and relative reductions reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Qualitative claim: adaptive-scale mechanism is claimed to improve rendering efficiency by adjusting sampling where needed (paper contrasts to Mip-NeRF's fixed multiscale approach and states that AS-NeRF improves computational efficiency), but no FLOPs, runtimes, memory footprint, or parameter-count measurements are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Yes — AS-NeRF shows strong performance under sparse-view conditions: substantial PSNR/SSIM/Chamfer improvements reported for single-view and two-view inputs, indicating better sample efficiency (e.g., one-view PSNR gains of +2.4 dB vs SparseFusion and +5.4 dB vs PixelNeRF).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Limited tests: paper reports a qualitative generalization test (random 2D toy image) and states AS-NeRF generalizes better than SparseFusion on that example; robustness under noise levels evaluated with LPIPS (AS-NeRF degrades more slowly), but no cross-dataset or out-of-distribution quantitative transfer results beyond Co3D Toys are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>MTL-FeatureNet extracts texture, shape, color, and depth in parallel and uses a composite loss with cross-task consistency to improve reconstruction; the paper attributes improved reconstruction to these multi-task, task-aligned representations but does not provide detailed ablation numbers isolating each task's contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Paper notes the overall system remains computationally heavy on very high-resolution or large-scale datasets and that model compression / distillation are future directions; no numeric constrained-run experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Task-aligned, dynamically adaptive representations (task-modulated attention + dynamic adapters + adaptive-scale sampling) improve reconstruction quality and robustness under sparse views and noise versus the evaluated uniform/fixed baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Experimental comparisons show consistent metric improvements (PSNR, SSIM, Chamfer, LPIPS) when adaptive, task-specific mechanisms are used, supporting the idea that allocating representation and computation according to task/local complexity is beneficial.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2236.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2236.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTL-FeatureNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Task Learning Feature Network (MTL-FeatureNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Vision-Transformer based feature extractor designed to produce task-specific feature streams (texture, shape, color, depth) using a task-dynamic modulation layer and a dynamic adapter layer to allocate representational emphasis per task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MTL-FeatureNet (ViT-based multi-task extractor)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ViT backbone (12 layers, 768 embedding dim, 12 heads) augmented with task relevance scoring modules (MLP_task producing TR_i), task-dynamic modulation that rescales attention weights via exp(TR_i), positional encodings modulated by TR, and Dynamic Adapter Layer (per-task affine scaling c_task, W_task, b_task) to produce specialized F_task features.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Task relevance scoring (MLP_task -> TR_i) modulates self-attention weights (multiplicative exp(TR_i)); task-specific adapter layers (c_task scaling, W_task, b_task) to transform shared features into task-specific representations; cross-task consistency loss to coordinate representations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Feature extraction for multiple tasks within 3D reconstruction pipeline (texture, shape, color, depth) from a single 2D view.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Indirectly evaluated as part of AS-NeRF; contributes to improved end-to-end metrics (PSNR/SSIM/Chamfer/LPIPS) reported for AS-NeRF versus baselines; no isolated numeric ablation of MTL-FeatureNet alone provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>No measured efficiency numbers; model design increases per-task modules (per-task MLPs/adapters) which may increase parameters compared to a single uniform representation, but no quantitative overhead reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Not separately reported; credited as part of AS-NeRF's superior few-view sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Not separately reported; the paper claims MTL features help generalization in reconstruction tasks but provides no dedicated transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>No explicit interpretability analyses, though TR_i scores are a designed mechanism that could be inspected to interpret per-task attention allocation (not evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Designed to improve multi-task feature specialization and cross-task synergy via L_cross; reported weightings and hyperparameters are provided, but no per-task metric table or ablation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Task-dynamic attention modulation and per-task adapter layers enable the network to allocate representational emphasis per task, and this multi-task specialization is a core contributor to AS-NeRF's improved reconstruction under sparse views.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>The paper presents MTL-FeatureNet as an explicit mechanism for task-aligned representations and attributes part of AS-NeRF's empirical gains to it, supporting the benefit of task-specific allocation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2236.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2236.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive-scale NeRF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive-scale Neural Radiance Field (adaptive-scale NeRF component of AS-NeRF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An NeRF variant that uses global features and a learned depth/complexity network to compute a local adaptive scale S(·) which modulates sampling density/rendered color contribution, combined with a spectral decoder for wavelength-aware color.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adaptive-scale NeRF</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>NeRF augmented with an adaptive-scale function S = sigmoid(b * D_complex(CT(FA(G,x), G))) that uses local feature estimates FA(G,x) and global features G to dynamically adjust per-sample contribution and sampling fidelity; color output becomes wavelength-dependent via a spectral decoder (MLP_color over spectral features).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Adaptive computation via a learned scale function that gates/weights per-sample rendering and sampling density based on input local/global feature complexity; spectral components for per-wavelength color decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Volumetric rendering / view synthesis in NeRF for high-fidelity 3D reconstruction (toy scenes).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Contributes to AS-NeRF's improved metrics: improved PSNR/SSIM/Chamfer and perceptual metrics; specific contribution not isolated via ablation in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Claimed to improve computational efficiency by reducing unnecessary sampling in low-complexity regions compared to fixed multiscale approaches (e.g., Mip-NeRF), but no measured FLOPs or runtime numbers are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Adaptive per-location sampling and scale adjustment combined with spectral decoding enable better detail capture and more efficient rendering allocation than fixed-scale approaches, as evidenced by improved end-to-end metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>The adaptive mechanism is explicitly designed to allocate computation where scene complexity demands it, and the reported end-to-end improvements are consistent with the Task-Aligned Abstraction Principle.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2236.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2236.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PixelNeRF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pixelNeRF: neural radiance fields from one or few images</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A NeRF-based baseline that conditions NeRF rendering on per-pixel image features to enable reconstruction from one or few input views; used here as a comparative baseline for few-view performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>pixelNeRF: neural radiance fields from one or few images</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pixelNeRF</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conditioning of NeRF on pixel-level image features extracted from input images to generalize to new scenes with only one or few views; does not include the task-dynamic or adaptive-scale mechanisms used in AS-NeRF.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Uniform pixel-level feature conditioning (per-pixel descriptors), no reported task-modulated attention or adaptive per-location computation in the baseline as described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Single/few-view 3D reconstruction and view synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Reported relatively in paper: AS-NeRF outperforms PixelNeRF by +5.4 dB PSNR (one-view case) and +5.3 dB PSNR (two-view case, where AS-NeRF=35.0 dB → PixelNeRF ≈ 29.7 dB). Chamfer distance: PixelNeRF single-view = 5.4 vs AS-NeRF 4.6. Two-view SSIM baseline inferred: PixelNeRF two-view SSIM ≈ 0.70 (AS-NeRF two-view SSIM 0.80 reported, +0.10 over PixelNeRF). Other perceptual and noise-robustness metrics reported as worse than AS-NeRF (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>PixelNeRF is a few-view method but underperforms AS-NeRF at extremely sparse settings (one view) per the reported PSNR/SSIM/Chamfer comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Uniform pixel-conditioned NeRF (PixelNeRF) performs substantially worse than AS-NeRF in the one- and two-view sparse regimes on Co3D-Toys across image and geometric metrics, highlighting advantages of task-aligned/adaptive designs.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>PixelNeRF (uniform conditioning) is outperformed by a task-adaptive system, consistent with the hypothesis that allocating representational/computational resources adaptively benefits sparse-view reconstruction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2236.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2236.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SparseFusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SparseFusion: distilling view-conditioned diffusion for 3D reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparse-view 3D reconstruction method that distills view-conditioned diffusion models to enhance NeRF under limited viewpoints; used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SparseFusion: distilling view-conditioned diffusion for 3D reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SparseFusion</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines view-conditional diffusion models and distillation to improve NeRF reconstruction from sparse viewpoints by fusing sparse viewpoint information and using diffusion for viewpoint-conditioned image generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>View-conditioned diffusion distillation and feature reprojection route; not described as using per-task dynamic attention or adaptive-scale sampling in this paper's description.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Sparse-view 3D reconstruction / few-view view synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Reported relatively in paper: compared to AS-NeRF: one-view PSNR is lower by 2.4 dB; two-view PSNR is lower by 1.4 dB (inferred: AS-NeRF two-view 35.0 dB → SparseFusion ≈ 33.6 dB). Chamfer distance single-view = 5.0 (AS-NeRF 4.6). Two-view SSIM inferred ≈ 0.72 (AS-NeRF two-view SSIM 0.80). Perceptual loss and LPIPS worse than AS-NeRF (relative reductions reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>SparseFusion performs better than PixelNeRF in sparse conditions but is still outperformed by AS-NeRF — AS-NeRF reports gains at 1–2 view regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>SparseFusion (diffusion-distillation) improves sparse-view results over some baselines but remains outperformed by AS-NeRF's task-adaptive mechanisms across PSNR/SSIM/Chamfer/LPIPS on Co3D-Toys.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>SparseFusion uses view-conditioned diffusion (a conditional generative component) which is a form of conditioning but not explicit task-aligned feature allocation; its middling performance relative to AS-NeRF suggests conditioning alone is insufficient without explicit task-aligned/adaptive representation and sampling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mip-NeRF: a multiscale representation for anti-aliasing neural radiance fields <em>(Rating: 2)</em></li>
                <li>pixelNeRF: neural radiance fields from one or few images <em>(Rating: 2)</em></li>
                <li>SparseFusion: distilling view-conditioned diffusion for 3D reconstruction <em>(Rating: 2)</em></li>
                <li>NeurAR: neural uncertainty for autonomous 3D reconstruction with implicit neural representations <em>(Rating: 1)</em></li>
                <li>Neuralangelo: high-fidelity neural surface reconstruction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2236",
    "paper_id": "paper-280414191",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "AS-NeRF",
            "name_full": "Adaptive Scale Neural Radiance Fields (AS-NeRF)",
            "brief_description": "An end-to-end 3D reconstruction system that combines a ViT-based multi-task feature extractor, an adaptive-scale NeRF with spectral-aware rendering, and a conditional diffusion model to produce high-fidelity toy reconstructions from one-to-few views.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AS-NeRF",
            "model_description": "Integrated architecture with (1) MTL-FeatureNet — a Vision-Transformer based multi-task feature extractor with task-dynamic modulation and dynamic adapter layers; (2) an adaptive-scale NeRF that dynamically adjusts sampling/rendering accuracy per local complexity and adds a spectral decoder for multi-wavelength color; (3) a conditional diffusion model that uses high-dimensional conditional vectors from MTL-FeatureNet/NeRF to synthesize unobserved viewpoints.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Task-dynamic modulation of multi-head attention (task relevance scores TR_i adjust attention via exp(TR_i)); dynamic adapter layer (task-specific scaling c_task, W_task, b_task); adaptive-scale function S(FA(G,x),G) computed via sigmoid(b * D_complex(CT(...))) that modulates sampling/rendering per 3D location; spectral decoder for wavelength-dependent color.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Single-/few-view 3D toy reconstruction (view synthesis and geometry recovery) on Co3D-Toys dataset (1–5 input views)",
            "performance_task_aligned": "Multiple reported metrics vs baselines: PSNR — AS-NeRF increases as views increase; with two input views AS-NeRF achieves 35.0 dB PSNR (paper reports improvement of +1.4 dB vs SparseFusion and +5.3 dB vs PixelNeRF). With one view AS-NeRF improves PSNR by +2.4 dB vs SparseFusion and +5.4 dB vs PixelNeRF. SSIM — two-view SSIM = 0.80 (reported +0.08 vs SparseFusion, +0.10 vs PixelNeRF); single-view SSIM improvements of +0.05 vs SparseFusion and +0.10 vs PixelNeRF. Chamfer distance — single-view AS-NeRF = 4.6 (units as reported), two-view = 4.2. Perceptual loss and LPIPS reported lower than baselines across resolutions and noise levels (qualitative and relative reductions reported).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Qualitative claim: adaptive-scale mechanism is claimed to improve rendering efficiency by adjusting sampling where needed (paper contrasts to Mip-NeRF's fixed multiscale approach and states that AS-NeRF improves computational efficiency), but no FLOPs, runtimes, memory footprint, or parameter-count measurements are reported.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Yes — AS-NeRF shows strong performance under sparse-view conditions: substantial PSNR/SSIM/Chamfer improvements reported for single-view and two-view inputs, indicating better sample efficiency (e.g., one-view PSNR gains of +2.4 dB vs SparseFusion and +5.4 dB vs PixelNeRF).",
            "transfer_generalization_results": "Limited tests: paper reports a qualitative generalization test (random 2D toy image) and states AS-NeRF generalizes better than SparseFusion on that example; robustness under noise levels evaluated with LPIPS (AS-NeRF degrades more slowly), but no cross-dataset or out-of-distribution quantitative transfer results beyond Co3D Toys are provided.",
            "interpretability_results": null,
            "multi_task_performance": "MTL-FeatureNet extracts texture, shape, color, and depth in parallel and uses a composite loss with cross-task consistency to improve reconstruction; the paper attributes improved reconstruction to these multi-task, task-aligned representations but does not provide detailed ablation numbers isolating each task's contribution.",
            "resource_constrained_results": "Paper notes the overall system remains computationally heavy on very high-resolution or large-scale datasets and that model compression / distillation are future directions; no numeric constrained-run experiments reported.",
            "key_finding_summary": "Task-aligned, dynamically adaptive representations (task-modulated attention + dynamic adapters + adaptive-scale sampling) improve reconstruction quality and robustness under sparse views and noise versus the evaluated uniform/fixed baselines.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Experimental comparisons show consistent metric improvements (PSNR, SSIM, Chamfer, LPIPS) when adaptive, task-specific mechanisms are used, supporting the idea that allocating representation and computation according to task/local complexity is beneficial.",
            "uuid": "e2236.0"
        },
        {
            "name_short": "MTL-FeatureNet",
            "name_full": "Multi-Task Learning Feature Network (MTL-FeatureNet)",
            "brief_description": "A Vision-Transformer based feature extractor designed to produce task-specific feature streams (texture, shape, color, depth) using a task-dynamic modulation layer and a dynamic adapter layer to allocate representational emphasis per task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MTL-FeatureNet (ViT-based multi-task extractor)",
            "model_description": "ViT backbone (12 layers, 768 embedding dim, 12 heads) augmented with task relevance scoring modules (MLP_task producing TR_i), task-dynamic modulation that rescales attention weights via exp(TR_i), positional encodings modulated by TR, and Dynamic Adapter Layer (per-task affine scaling c_task, W_task, b_task) to produce specialized F_task features.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Task relevance scoring (MLP_task -&gt; TR_i) modulates self-attention weights (multiplicative exp(TR_i)); task-specific adapter layers (c_task scaling, W_task, b_task) to transform shared features into task-specific representations; cross-task consistency loss to coordinate representations.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Feature extraction for multiple tasks within 3D reconstruction pipeline (texture, shape, color, depth) from a single 2D view.",
            "performance_task_aligned": "Indirectly evaluated as part of AS-NeRF; contributes to improved end-to-end metrics (PSNR/SSIM/Chamfer/LPIPS) reported for AS-NeRF versus baselines; no isolated numeric ablation of MTL-FeatureNet alone provided.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": "No measured efficiency numbers; model design increases per-task modules (per-task MLPs/adapters) which may increase parameters compared to a single uniform representation, but no quantitative overhead reported.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Not separately reported; credited as part of AS-NeRF's superior few-view sample efficiency.",
            "transfer_generalization_results": "Not separately reported; the paper claims MTL features help generalization in reconstruction tasks but provides no dedicated transfer experiments.",
            "interpretability_results": "No explicit interpretability analyses, though TR_i scores are a designed mechanism that could be inspected to interpret per-task attention allocation (not evaluated).",
            "multi_task_performance": "Designed to improve multi-task feature specialization and cross-task synergy via L_cross; reported weightings and hyperparameters are provided, but no per-task metric table or ablation is provided.",
            "resource_constrained_results": null,
            "key_finding_summary": "Task-dynamic attention modulation and per-task adapter layers enable the network to allocate representational emphasis per task, and this multi-task specialization is a core contributor to AS-NeRF's improved reconstruction under sparse views.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "The paper presents MTL-FeatureNet as an explicit mechanism for task-aligned representations and attributes part of AS-NeRF's empirical gains to it, supporting the benefit of task-specific allocation.",
            "uuid": "e2236.1"
        },
        {
            "name_short": "Adaptive-scale NeRF",
            "name_full": "Adaptive-scale Neural Radiance Field (adaptive-scale NeRF component of AS-NeRF)",
            "brief_description": "An NeRF variant that uses global features and a learned depth/complexity network to compute a local adaptive scale S(·) which modulates sampling density/rendered color contribution, combined with a spectral decoder for wavelength-aware color.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Adaptive-scale NeRF",
            "model_description": "NeRF augmented with an adaptive-scale function S = sigmoid(b * D_complex(CT(FA(G,x), G))) that uses local feature estimates FA(G,x) and global features G to dynamically adjust per-sample contribution and sampling fidelity; color output becomes wavelength-dependent via a spectral decoder (MLP_color over spectral features).",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Adaptive computation via a learned scale function that gates/weights per-sample rendering and sampling density based on input local/global feature complexity; spectral components for per-wavelength color decoding.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Volumetric rendering / view synthesis in NeRF for high-fidelity 3D reconstruction (toy scenes).",
            "performance_task_aligned": "Contributes to AS-NeRF's improved metrics: improved PSNR/SSIM/Chamfer and perceptual metrics; specific contribution not isolated via ablation in the paper.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": "Claimed to improve computational efficiency by reducing unnecessary sampling in low-complexity regions compared to fixed multiscale approaches (e.g., Mip-NeRF), but no measured FLOPs or runtime numbers are reported.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Adaptive per-location sampling and scale adjustment combined with spectral decoding enable better detail capture and more efficient rendering allocation than fixed-scale approaches, as evidenced by improved end-to-end metrics.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "The adaptive mechanism is explicitly designed to allocate computation where scene complexity demands it, and the reported end-to-end improvements are consistent with the Task-Aligned Abstraction Principle.",
            "uuid": "e2236.2"
        },
        {
            "name_short": "PixelNeRF",
            "name_full": "pixelNeRF: neural radiance fields from one or few images",
            "brief_description": "A NeRF-based baseline that conditions NeRF rendering on per-pixel image features to enable reconstruction from one or few input views; used here as a comparative baseline for few-view performance.",
            "citation_title": "pixelNeRF: neural radiance fields from one or few images",
            "mention_or_use": "use",
            "model_name": "pixelNeRF",
            "model_description": "Conditioning of NeRF on pixel-level image features extracted from input images to generalize to new scenes with only one or few views; does not include the task-dynamic or adaptive-scale mechanisms used in AS-NeRF.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Uniform pixel-level feature conditioning (per-pixel descriptors), no reported task-modulated attention or adaptive per-location computation in the baseline as described in this paper.",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Single/few-view 3D reconstruction and view synthesis.",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Reported relatively in paper: AS-NeRF outperforms PixelNeRF by +5.4 dB PSNR (one-view case) and +5.3 dB PSNR (two-view case, where AS-NeRF=35.0 dB → PixelNeRF ≈ 29.7 dB). Chamfer distance: PixelNeRF single-view = 5.4 vs AS-NeRF 4.6. Two-view SSIM baseline inferred: PixelNeRF two-view SSIM ≈ 0.70 (AS-NeRF two-view SSIM 0.80 reported, +0.10 over PixelNeRF). Other perceptual and noise-robustness metrics reported as worse than AS-NeRF (qualitative).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "PixelNeRF is a few-view method but underperforms AS-NeRF at extremely sparse settings (one view) per the reported PSNR/SSIM/Chamfer comparisons.",
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Uniform pixel-conditioned NeRF (PixelNeRF) performs substantially worse than AS-NeRF in the one- and two-view sparse regimes on Co3D-Toys across image and geometric metrics, highlighting advantages of task-aligned/adaptive designs.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "PixelNeRF (uniform conditioning) is outperformed by a task-adaptive system, consistent with the hypothesis that allocating representational/computational resources adaptively benefits sparse-view reconstruction.",
            "uuid": "e2236.3"
        },
        {
            "name_short": "SparseFusion",
            "name_full": "SparseFusion: distilling view-conditioned diffusion for 3D reconstruction",
            "brief_description": "A sparse-view 3D reconstruction method that distills view-conditioned diffusion models to enhance NeRF under limited viewpoints; used as a baseline in experiments.",
            "citation_title": "SparseFusion: distilling view-conditioned diffusion for 3D reconstruction",
            "mention_or_use": "use",
            "model_name": "SparseFusion",
            "model_description": "Combines view-conditional diffusion models and distillation to improve NeRF reconstruction from sparse viewpoints by fusing sparse viewpoint information and using diffusion for viewpoint-conditioned image generation.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "View-conditioned diffusion distillation and feature reprojection route; not described as using per-task dynamic attention or adaptive-scale sampling in this paper's description.",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Sparse-view 3D reconstruction / few-view view synthesis.",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Reported relatively in paper: compared to AS-NeRF: one-view PSNR is lower by 2.4 dB; two-view PSNR is lower by 1.4 dB (inferred: AS-NeRF two-view 35.0 dB → SparseFusion ≈ 33.6 dB). Chamfer distance single-view = 5.0 (AS-NeRF 4.6). Two-view SSIM inferred ≈ 0.72 (AS-NeRF two-view SSIM 0.80). Perceptual loss and LPIPS worse than AS-NeRF (relative reductions reported).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "SparseFusion performs better than PixelNeRF in sparse conditions but is still outperformed by AS-NeRF — AS-NeRF reports gains at 1–2 view regimes.",
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "SparseFusion (diffusion-distillation) improves sparse-view results over some baselines but remains outperformed by AS-NeRF's task-adaptive mechanisms across PSNR/SSIM/Chamfer/LPIPS on Co3D-Toys.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "SparseFusion uses view-conditioned diffusion (a conditional generative component) which is a form of conditioning but not explicit task-aligned feature allocation; its middling performance relative to AS-NeRF suggests conditioning alone is insufficient without explicit task-aligned/adaptive representation and sampling.",
            "uuid": "e2236.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mip-NeRF: a multiscale representation for anti-aliasing neural radiance fields",
            "rating": 2
        },
        {
            "paper_title": "pixelNeRF: neural radiance fields from one or few images",
            "rating": 2
        },
        {
            "paper_title": "SparseFusion: distilling view-conditioned diffusion for 3D reconstruction",
            "rating": 2
        },
        {
            "paper_title": "NeurAR: neural uncertainty for autonomous 3D reconstruction with implicit neural representations",
            "rating": 1
        },
        {
            "paper_title": "Neuralangelo: high-fidelity neural surface reconstruction",
            "rating": 1
        }
    ],
    "cost": 0.0162275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>3D reconstruction of toys based on adaptive scaled neural radiation field
29 July 2025</p>
<p>Jiajun Zou 
Department of Information and Intelligence Engineering
Guangzhou Xinhua University
Dongguan, GuangdongChina</p>
<p>Shaojiang Liu 
Department of Information and Intelligence Engineering
Guangzhou Xinhua University
Dongguan, GuangdongChina</p>
<p>Feng Wang 
Department of Information and Intelligence Engineering
Guangzhou Xinhua University
Dongguan, GuangdongChina</p>
<p>Weichuan Ni 
Department of Artificial Intelligence
Guangzhou Huashang College
Guangzhou, GuangdongChina</p>
<p>Shitong Ye yeshitong@gdhsc.edu.cn 
Department of Artificial Intelligence
Guangzhou Huashang College
Guangzhou, GuangdongChina</p>
<p>Muhammad Asif 
3D reconstruction of toys based on adaptive scaled neural radiation field
29 July 20259C1E7090BC4FB621CEE99BEEC0D1DA7910.7717/peerj-cs.3053Submitted 29 April 2025 Accepted 30 June 2025Three-dimensional reconstructionNeural radiation fieldMulti-task learningAdaptive scale adjustmentConditional diffusion modeling
With the rapid development of computer vision technology, 3D reconstruction of toys under single-view conditions still faces significant challenges in terms of detail loss and color distortion.For this reason, this article proposes an adaptive scale neural radiance fields (AS-NeRF) model to enhance the accuracy and realism of 3D toy reconstruction.The method constructs a multi-task feature extraction network based on the Vision Transformer, which simultaneously extracts and fuses multidimensional features such as texture, shape, color, and depth through a task dynamic modulation mechanism and a dynamic adapter layer, providing a rich and accurate contextual feature representation.The NeRF model is enhanced to incorporate an adaptive scaling mechanism that dynamically optimizes rendering sampling accuracy according to the local complexity of the scene.Spectral sensing techniques are integrated to reproduce the true colors of materials accurately.Finally, the conditional diffusion model is deeply integrated with NeRF, and high-dimensional conditional vectors are used to guide the inverse diffusion process in generating unobserved images with consistent geometric structure and physical properties.Experiments on the Co3D toy dataset demonstrate that AS-NeRF significantly outperforms existing mainstream methods in terms of peak signal-tonoise ratio (PSNR), structural similarity (SSIM), loss of perceptions (LPIPS), and Chamfer distance, thereby verifying the validity and advantages of the proposed method for high-quality toy 3D reconstruction tasks.</p>
<p>INTRODUCTION</p>
<p>With the rapid development of computer vision and graphics, 3D reconstruction technology plays a crucial role in various fields, including virtual reality, augmented reality, autonomous driving, and robot navigation, among others.High-quality, real-time, and accurate 3D reconstruction methods can not only enhance the user experience but also provide solid technical support for related applications.However, traditional 3D reconstruction methods still face many challenges in handling complex scenes, fusing information from multiple viewpoints, and restoring details (Aharchi &amp; Ait Kbir, 2020;Anciukevičius et al., 2023;Sun et al., 2021;Alldieck, Zanfir &amp; Sminchisescu, 2022).</p>
<p>sensing techniques are introduced to dynamically optimize the rendering accuracy of local regions and improve the accuracy of material color reproduction.We innovatively integrate the conditional diffusion model with NeRF, utilizing high-dimensional conditional coding to guide the inverse diffusion generation process and efficiently produce unobserved viewpoint images with consistent geometric and physical attributes.This approach overcomes the limitations of existing methods, which rely on sparse viewpoints and suffer from insufficient detail reproduction and color distortion.The main contributions of AS-NeRF include:</p>
<p>(1) Multi-task learning feature extraction: By designing a multi-task learning feature network, AS-NeRF can simultaneously extract multidimensional features, such as texture, shape, color, and depth, which provides rich contextual information and enhances the ability to detail restoration in the reconstruction process.This is especially important for objects with rich and diverse details, such as toys.</p>
<p>(2) Adaptive scale adjustment and spectral awareness: An adaptive scale adjustment mechanism is introduced to dynamically optimize the rendering accuracy according to the complexity of the toy's shape and details, and spectral awareness technology is combined to improve the realism of the color reproduction and the detail performance, ensuring that the details can be accurately reconstructed at different scales.</p>
<p>(3) Effective integration of conditional diffusion model: The conditional diffusion model is organically combined with the feature extraction network and NeRF model to guide the inverse diffusion process through the high-dimensional conditional vectors, which ensures that the generated images are accurate and coherent in terms of geometrical and physical attributes and that the high-quality reconstruction effect can still be maintained, especially in the case where the number of viewpoints is small.This is especially important for toy models that require reconstruction with complex details from a limited number of views.</p>
<p>This thesis is organized as follows: "Related Work" primarily reviews the research progress in the field related to this study, focusing on NeRF and its improved methods, particularly the latest results in 3D reconstruction and multiview information fusion."Model Architecture and Feature Extraction Network" provides a detailed description of the overall model architecture of AS-NeRF, with a focus on the design and implementation of the Multi-Task Learning Feature Extraction Network (MTL-FeatureNet)."Improved NeRF Model: Combining Adaptive Scaling with Spectral Sensing" describes the improved NeRF model in AS-NeRF, focusing on the adaptive scale adjustment mechanism and the integration method of spectral perception techniques."Conditional Diffusion Model" introduces the application of the conditional diffusion model in AS-NeRF, explaining how it is combined with the feature extraction network and the NeRF model."Experimental Analysis" presents a comprehensive experimental evaluation of the method in this article, comparing it with a standard procedure on the Co3D dataset and providing comparative results on several evaluation metrics."Conclusion" summarizes the full article, outlining the main contributions and research results of AS-NeRF in the 3D reconstruction of toys.The limitations of the research are discussed, and future research directions and possible improvements are proposed.</p>
<p>RELATED WORK</p>
<p>3D reconstruction techniques have been extensively researched and applied in the fields of computer vision and graphics, particularly in neural network-based methods.NeRF, as an emerging 3D reconstruction method, achieves high-quality viewpoint synthesis and detail restoration by implicitly representing the scene's bulk density and color (Mildenhall et al., 2021).Since the proposal of NeRF, numerous improvements and extensions have emerged from academia and industry to address the limitations of NeRF in various application scenarios and enhance its performance in complex environments.With the successful application of Transformers in the field of computer vision, researchers have attempted to integrate them into the NeRF model to enhance its feature extraction capabilities and viewpoint synthesis performance.Lin et al. (2023) proposed a visual Transformer-based NeRF approach, which enhances the NeRF under single-input image conditions by introducing the Transformer architecture to the viewpoint synthesis capability of NeRF.However, such methods primarily focus on feature extraction and structure optimization, lacking further modeling of local details and geometric consistency, which makes it challenging to meet the demand for high-precision reconstruction of objects with complex information, such as toys.SparseFusion enhances 3D reconstruction under limited viewpoint conditions by fusing sparse viewpoint information and utilizing a diffusion model of the viewpoint conditions (Zhou &amp; Tulsiani, 2023).This method enhances the reconstruction capability of NeRF under sparse viewpoint data by distilling viewpoint information and reducing its dependence on a large number of input images.However, it still has limitations in fine geometry and realistic color reproduction, especially underperforming in toy scenes with complex materials and rich colors.Mip-NeRF introduces a multiscale representation to reduce aliasing effects, thereby enhancing the rendering quality of NeRF at high resolutions (Barron et al., 2021).Although the method improves the fineness of the reconstructed images, it lacks flexibility in dynamically adjusting the sampling accuracy, leading to a significant increase in computational cost.</p>
<p>In 3D toy reconstruction, it typically faces the limitation of single-view or few-view data.PixelNeRF enhances the reconstruction capability of NeRF with few-view data through pixel-level feature extraction (Yu et al., 2021).To enhance the geometric consistency of 3D reconstruction, researchers have combined neural implicit surface methods with NeRF.For example, Geo-Neus improves the accuracy and detail restoration of multiview reconstruction by combining geometrically consistent neural implicit surface learning (Fu et al., 2022).Neus, proposed by Wang et al. (2021), further improves the geometrical consistency and detail restoration of multiview reconstruction by combining neural implicit surface learning and volumetric rendering techniques.Ran et al. (2023) propose the Neurar model, which enhances the robustness and accuracy of NeRF in autonomous 3D reconstruction by introducing neural uncertainty.This approach not only improves the model's generalization ability but also performs well in handling uncertain data.In addition, Unisurf, proposed by Oechsle, Peng &amp; Geiger (2021), unifies the neural implicit surface with the radiation field, further improving the performance of NeRF in complex scenes.High-fidelity 3D reconstruction is crucial for applications involving detail-rich objects, such as toys.Neuralangelo significantly enhances the ability of NeRF in detail restoration and high-precision reconstruction through high-fidelity neural surface reconstruction (Li et al., 2023).The single-view 3D scene reconstruction method proposed by Chen et al. (2024) further enhances NeRF through high-fidelity shape and texture reconstruction, enabling detailed restoration under single-view conditions.Neus2, proposed by Wang et al. (2023), improves the efficiency and accuracy of NeRF in multiview reconstruction by rapidly learning neural implicit surfaces.</p>
<p>MODEL ARCHITECTURE AND FEATURE EXTRACTION NETWORK Model architecture</p>
<p>The adaptive scale neural radiance fields (AS-NeRF) model proposed in this study aims to comprehensively improve the accuracy and quality of 3D reconstruction of toys by integrating multi-task learning feature extraction, adaptive scale adjustment with spectral sensing, and conditional diffusion model.The overall architecture is shown in Fig. 1.AS-NeRF consists of three main components, each of which plays a key role in the overall reconstruction process and enables efficient 3D reconstruction through close collaboration.</p>
<p>The multi-task learning feature extraction network (MTL-FeatureNet) is responsible for extracting multidimensional features, including texture, shape, color, and depth, from the input single 2D toy view.These rich features provide the necessary contextual information for subsequent 3D reconstruction.MTL-FeatureNet is based on an improved Vision Transformer (ViT) architecture, which optimizes the feature extraction process by introducing a task-dynamic modulation layer, allowing it to better adapt to multi-task learning environments and enhancing the ability to extract high-quality features from a single view.</p>
<p>The extracted multidimensional features are then passed to an improved NeRF model that integrates an adaptive scaling mechanism and spectral sensing techniques.The adaptive scaling mechanism dynamically optimizes the rendering accuracy according to the complexity of toy shapes and details, ensuring accurate reconstruction of details at different scales, thus improving the accuracy and computational efficiency of 3D reconstruction.Spectral awareness technology introduces a spectral decoder, which uses multi-spectral information to enhance the color realism and material details, so that the generated 3D model has higher realism and details under various lighting conditions.</p>
<p>To further enhance the quality and consistency of the generated images, AS-NeRF integrates a conditional diffusion model.The model utilizes multidimensional features from MTL-FeatureNet and the improved NeRF model as conditional information to guide the backward diffusion process, ensuring consistency and accuracy of the generated viewpoint images in terms of geometric and physical properties.The conditional diffusion model not only optimizes the details of image generation, but also enhances the reconstruction ability of the model under few viewpoint conditions, enabling the generation of high-quality 3D models even with limited viewpoint data.Full-size  DOI: 10.7717/peerj-cs.3053/fig- 1 Figure 1 shows the overall architecture of the AS-NeRF method.In the AS-NeRF framework, a single-view image is extracted by MTL-FeatureNet to produce a local feature map and global semantic vectors.The local feature maps are sent to NeRF with ray sampling to guide the density and color prediction of each 3D sampling point.The global vectors are involved in adaptive scale selection and spectral decoding in NeRF, and on the other hand, they are compressed into conditional vectors and input into the conditional diffusion model, which are used to constrain the diffusion process and synthesize the unobserved viewpoint images quickly.In the training stage, the new viewpoints generated by the diffusion model are projected back to NeRF as pseudo-supervision, while in the inference stage, only one feature extraction is needed to render the real viewpoints of NeRF and synthesize the arbitrary viewpoints of the diffusion model at the same time, and the two paths share the features without blocking each other, which ensures that geometrically and spectrally consistent and high-quality 3D reconstruction results can be obtained under the condition of sparse viewpoints.</p>
<p>Feature extraction network based on multi-task learning approach</p>
<p>In this section, we describe how to extract multilevel features such as texture, shape, color, and depth from a single 2D toy view by MTL-FeatureNet to provide the necessary information for complex 3D reconstruction tasks.</p>
<p>MTL-FeatureNet is mainly based on the ViT architecture (Han et al., 2022;Liu et al., 2021;Arnab et al., 2021), but in MTL-FeatureNet we improve the traditional multi-head self-attention mechanism to optimize the extraction of features from a single 2D toy view, especially in a multi-task learning environment.The improvements include the introduction of a task dynamic modulation layer, which is a design that adjusts the self-attention weights according to the task demands.The purpose of the task dynamic modulation layer is to dynamically adjust the attention allocation according to the specific demands of different tasks.This layer optimizes the weight distribution of each head in the attention mechanism by analyzing task-related contextual information.The specific implementation is as follows.</p>
<p>We assume that each task is computed by a neural network module to obtain a task relevance score TR i , which is used to reflect the importance of the current feature for a particular task:
TR i ¼ MLP task C i ð Þ:(1)
MLP task refers to a multilayer perceptron that receives high-level features from a specific part of the image and outputs a real value that indicates the importance of the current feature for performing a specific task.C i refers to regional or global features in the image after preprocessing (e.g., feature extraction network layer processing).</p>
<p>The above scores are then used to adjust the self-attention weights so that the model can focus on the most relevant information according to the specific needs of each task:
At i Q; K; V ð Þ¼sof tmax QK T ffiffiffiffi ffi d k p Á exp TR i ð Þ V:(2)
Based on the task relevance score TR i , we act on the attention weights in the form of exp TR i ð Þ to enhance or diminish the influence of specific parts to ensure that the model's attention is more in line with the current task.</p>
<p>Since positional information is extremely important for understanding image content, we adopt an improved positional coding method, which not only considers pixel positions, but also incorporates task-related dynamic adjustments:
PE pos; 2i ð Þ¼sin pos=10,000 2i=d model Á TR; (3) PE pos; 2i þ 1 ð Þ¼cos pos=10,000 2i=d model Á TR: (4)
Since TR is a task relevance score from the task dynamic modulation layer, by multiplying it with the conventional computation of position encoding, a way of dynamically modulating the position encoding can be realized in such a way that it reflects the specific needs of different tasks.With this enhanced multi-head self-attention mechanism and the task dynamic modulation layer, MTL-FeatureNet is able to handle the demands of multi-task learning more efficiently by improving the accuracy and relevance of features extracted from a single 2D toy view.</p>
<p>For the extracted features, we also introduce feature fusion and dynamic adapter layer (DAL) in the MTL-FeatureNet architecture, which aims to further optimize and synthesize these features to ensure that they can be adapted to multiple different task requirements.The DAL works after the multi-head self-attention mechanism, receiving the feature F global output from the self-attention layer and transforming it with a set of specially designed parameters to better serve a particular task:
F task ¼ r c task Á W task Á F global þ b task À Á :(5)
In this formulation c task is a scaling factor customized for each task, which is automatically adjusted through the learning process to optimize the transformation efficiency of the features, W task is the task-specific weight matrix, and b task is the bias term.This setup allows the network to not only respond to different task requirements, but also maintain its versatility and adaptability in handling various visual tasks.</p>
<p>To further enhance the network's ability to handle multi-tasks, we design a composite loss function that not only considers the performance of individual tasks, but also reinforces the synergistic effect between tasks through cross-task consistency loss:
L ¼ X k k k Á L k þ l Á L cross F task1 ; F task2 ; . . . ð Þ :(6)
L cross denotes the loss of cross-task consistency, which facilitates knowledge sharing and complementarity between tasks by comparing similarities or differences between feature representations generated by different tasks:
L cross F task1 ; F task2 ; . . . ð Þ¼ X i6 ¼j v ij Á kF taski À F taskj k 2 (7)
where v ij is a dynamically adjusted weight that adjusts the effect of feature differences between each pair of tasks according to the correlation between different tasks.The schematic diagram of the feature extraction process for multi-task learning network is shown in Fig. 2. The pseudo-code of the algorithm for MTL-FeatureNet is shown below.</p>
<p>2D Image Input</p>
<p>Adaptive scaling</p>
<p>The next step in extracting key visual features from a single 2D toy view via MTL-FeatureNet is to utilize these features for efficient 3D reconstruction.In this section, we introduce the adaptive-scale NeRF technique, an improvement on the standard NeRF model designed to increase the accuracy and efficiency of 3D reconstruction, which is particularly suitable for processing features obtained from MTL-FeatureNet.NeRF achieves impressive 3D reconstruction results by modeling the scene's continuous bulk density and color in the line-of-sight direction, using multiple stacked fully connected networks (MLPs) to predict the bulk density and color.However, standard NeRFs often require a large amount of training time and computational resources when dealing with complex scenes, and do not capture enough details at different scales.Adaptive Scale NeRF allows the model to dynamically adjust the accuracy of rendering according to the complexity of the scene and the detail requirements of a specific region by introducing a scale-adaptive mechanism.This improvement is mainly to optimize the rendering efficiency and increase the model's ability to capture details, especially when combined with advanced features from MTL-FeatureNet.</p>
<p>We use MTL-FeatureNet to extract image global features G.These features include not only texture, shape, and color, but also depth and illumination information through specific tasks.In order to dynamically adjust the rendering accuracy according to the
S FA G; x t ð Þ ð Þ; G ð Þ¼sigmoid b Á D complex CT FA G; x t ð Þ ð Þ; G ð Þ ð Þ À Á :(8)
b is a learnable scaling factor that regulates the sensitivity of the scale-adaptive function, allowing the model to find a balance between preserving detail and computational efficiency.D complex denotes the deep neural network used to analyze the integrated local and global features, whose output determines the degree of scale tuning.CT Á ð Þ is a co-feature merging function that synthesizes the local feature Q and the global feature G in order to generate a comprehensive scene description.FA G; x t ð Þ ð Þis an estimation function that determines the local features at a 3D location x t ð Þ based on that location and the global feature G.</p>
<p>Combined with an adaptive scale function, this allows the NeRF rendering process to adaptively adjust the contribution of each sample point:
C r ð Þ ¼ Z t f t n T t ð Þ Á r p x t ð Þ ð Þ ð ÞÁc x t ð Þ; S FA G; x t ð Þ ð Þ; G ð Þ ð Þ dt: (9) c x t ð Þ; S FA G; x t ð Þ ð Þ; G ð Þ ð
Þ is the adjusted color output, where the adaptive scale function S dynamically adjusts the color rendering at the location based on the global and estimated local features.</p>
<p>Spectrum-aware rendering</p>
<p>Spectrally aware rendering techniques (Zhang et al., 2024;Afanasyev, Voloboy &amp; Ignatenko, 2015;Meng &amp; Yuan, 2021) play a key role in further enhancing the realism and detailed representation of toy 3D models.This technique utilizes data acquired from multispectral images based not only on conventional red, green, blue (RGB) information, but also on a wider range of spectral information to enhance the color realism and detailed representation of materials in toy 3D models.Spectrally-aware rendering is able to simulate the physical interactions of light at different wavelengths, thus providing more accurate color reproduction under various lighting conditions, especially at the edges of the spectrum and on special materials.In this article, we model the spectrally-aware NeRF by introducing a spectral decoder that not only responds to spectra in the visible range, but also processes spectral information beyond this range.This allows the model to take into account more complex spectral data, such as infrared and ultraviolet information, in the rendering process, thus improving the color fidelity and visual effects of the scene.The spectral decoder we used is formulated as follows:  help the model to precisely control the spectral response at different wavelengths.The MLP color is a specially designed multilayer perceptron for decoding the color based on the spectral features F x; e ð Þ, which takes into account the effects from different wavelengths.Combined with the spectral perception technique, the rendering formula of NeRF is further improved as:
c x; e ð Þ ¼ softmax X N n¼1 G n Á e À eÀl n ð Þ 2 2d 2 n ! Á MLP color F x; e ð Þ ð Þ:(10C r ð Þ ¼ Z t f t n T t ð Þ Á r p x t ð Þ; e ð Þ ð ÞÁc x t ð Þ; e; S FA G; x t ð Þ ð Þ; G ð Þ ð Þ dt: (11) p Á ð Þ; c Á ð Þ
are the bulk density and color, respectively, which are now dependent on the wavelength of light e.This allows the model to dynamically adjust the rendering under different lighting conditions, which not only takes into account the effect of the wavelength e on the color and the bulk density, but also integrates an adaptive scale adjustment function
S FA G; x t ð Þ ð Þ; G ð
Þto optimize the rendering efficiency and quality.With this approach, the NeRF model is not only able to adjust the rendering accuracy to adapt to the scene complexity, but also able to reproduce the colors more realistically under various lighting conditions.With this approach of combining adaptive scaling and spectral perception techniques into NeRF models, we can realize higher quality 3D visual content.The implementation process of the NeRF method combining adaptive scaling and spectral perception is shown in Fig. 3.</p>
<p>The pseudo-code of the algorithm that improves on the NeRF method is shown below.</p>
<p>Algorithm 2 Adaptive-scale and spectral-aware NeRF Pseudo-code.</p>
<p>Input</p>
<p>CONDITIONAL DIFFUSION MODEL</p>
<p>In this study, the central role of the conditional diffusion model (Yu et al., 2023;Zhang, Rao &amp; Agrawala, 2023;Zhu et al., 2023) is to generate high-quality viewpoint images by utilizing the multidimensional features provided by MTL-FeatureNet and the viewpoint data generated by the improved NeRF model.By combining these advanced models with the conditional diffusion model, we are able to efficiently generate images of unobserved viewpoints that are consistent with geometric and physical realism.The features extracted via MTL-FeatureNet are used as inputs to the conditional diffusion model, which utilizes these features to guide the generation process, ensuring that the newly generated images are not only visually coherent, but also consistent with the original viewpoints in terms of geometric and physical properties.In order to effectively utilize these features, we have improved the traditional backward diffusion process, especially in the fusion and use of conditional information:
q h O tÀ1 O t ; c j ð Þ¼N O tÀ1 ; l h O t ; c; t ð Þ; X h O t ; c; t ð Þ ! (12)
where l h and AE h are conditionally dependent parameters that are not only adjusted according to the current noisy image O t , but also depend on the composite feature c extracted from the MTL-FeatureNet and NeRF models.This design allows the model to more accurately recover the details of the target image during the denoising process, especially when dealing with complex illumination and depth Return the final rendered image C(r) considering adaptive scale and spectral awareness information.The composite feature c is obtained from the encoder as a high-dimensional conditional vector:
c ¼ EC Ĝ; J À Á : (13)
Global features Ĝ include NeRF-generated 3D scene information such as viewpoint-dependent scene geometry and illumination patterns.J denotes local features from MTL-FeatureNet, including detail information such as texture and shape.These features are synthesized in the encoder to generate conditional vectors that can guide image generation.</p>
<p>The training of the conditional diffusion model involves optimizing a composite loss function that is designed to improve the quality of the generated images and to ensure that the conditional information is used efficiently:</p>
<p>The training of the conditional diffusion model involves optimizing a composite loss function that is designed to improve the quality of the generated images and to ensure that the conditional information is used efficiently:
T h ð Þ ¼ E q O t O 0 j ð Þ log q O tÀ1 O t j ; O 0 ð Þ q h O tÀ1 O t ; c j ð Þ ! :(14)
This loss function helps the model learn exactly how to reconstruct clear and accurate images of new viewpoints in reverse, guided by added noise and conditional information.The improved conditional diffusion model improves the ability to generate 3D models from a limited number of viewpoints, and by combining information from different data sources, it enhances the accuracy and realism of the model in generating images from unobserved viewpoints.</p>
<p>EXPERIMENTAL ANALYSIS Experimental environment</p>
<p>For the hardware of this experiment, a server equipped with NVIDIA GeForce RTX 4090 GPU with 24GB of video memory was used, the processor was an Intel Core i9-10900K with 10 cores and 20 threads, paired with 64GB of DDR4 RAM, and the storage was equipped with a 2TB NVMe SSD.For the software environment, Ubuntu 20.04 LTS operating system was used, Python 3.8 was chosen as the programming language, and PyTorch 1.8 was used as the deep learning framework.The experimental dataset is the Co3D Toys dataset, which is divided into 80% for the training set and 20% for the testing set.The Co3D Toys subset is derived from the Canonical 3D dataset published by Meta AI.The subset contains 51 types of toy objects, 1,645 individual scenes, and about 88,000 images.The original resolution is 960 × 540 px, and in order to ensure that the algorithm can adapt to the high resolution, we randomly crop and scale the images to 512 × 512 px in the training phase, and keep 768 × 768 px in the testing phase.Co3D provides synchronized internal and external parametric calibration, and the camera trajectory is distributed as an ellipsoidal cone around a single object: elevation angle 10 -70 , and azimuth angle 0 -360 , We follow the official few view classification, where 80% of the scenes (≈1,316 scenes) in each category are used as the training set, and 20% of the scenes (≈329 scenes) are used as the test set, and 1-5 views are uniformly sampled by trajectory as inputs, and the rest of the views are used as inputs.Five views as inputs and the rest of the views are used for evaluation to ensure the comparability of the methods under sparse view and multiview conditions.</p>
<p>Experimental program</p>
<p>In order to comprehensively evaluate the performance of the AS-NeRF method in 3D reconstruction tasks, we carried out detailed experimental parameter settings for the key components of this method: the number of layers of the Transformer encoder was set to 12, the embedding dimensions were set to 768 dimensions, and 12 attentional heads were used, each with a dimensionality of 64, for each task (texture, shape, color, and depth), we introduced an independent task relevance scoring module, whose multilayer perceptron contains two layers, the hidden layer size is 256, and the activation function adopts ReLU.The initial value of the scaling factor c task is set to 1.0, which is automatically learned and optimized through the training process.k k is set for each task, and the weights of texture, shape, color, and depth are 1.0, 1.0, 0.8, and 1.0, respectively.l is set to 0.1, in order to balance the individual task performance and inter-task synergy.v ij is dynamically adjusted according to task relevance, with an initial value of 0.5.b is initialized to 1.0 and automatically adjusted by backpropagation during the training process to optimize the sensitivity of the scale-adaptive function.N is set to 31, which covers major wavelengths in the range of 400 to 700 nm, with one component at 10 nm intervals.l n is set to 1.0 for the range from 400 to 700 nm at 10 nm intervals.d 2 n is uniformly set to 15 to ensure the smoothness of the spectral response.G n is initialized to 1.0 to learn the contribution of each wavelength through the training process.</p>
<p>In order to comprehensively evaluate the performance of AS-NeRF, we select two comparison methods, SparseFusion and PixelNeRF, respectively.PixelNeRF is typical in single-view or few-view conditions, while SparseFusion combines view-conditional diffusion and distillation strategies, which is currently recognized as an important benchmark for sparse-view reconstruction performance.Both of them cover the feature reprojection route and the diffusion-NeRF route respectively, which are the closest to the technical path of the sparse view toy reconstruction task focused in this article.The experimental standard evaluation metrics cover a variety of aspects such as reconstruction accuracy, visual quality and geometric consistency.The specific evaluation indexes are as follows:</p>
<p>(1) Peak signal-to-noise ratio (PSNR): PSNR is used to measure the similarity between the reconstructed image and the real image at the pixel level.Its calculation formula is:
PSNR ¼ 10 Á log 10 MAX 2 MSE :(15)
MAX is the maximum pixel value of the image and MSE is the mean square error.Higher PSNR value indicates better quality of the reconstructed image.</p>
<p>(2) Structural similarity index (SSIM): the SSIM is used to evaluate the similarity between the reconstructed image and the real image in terms of structure, brightness and contrast, which is calculated by the formula:
SSIM x; y ð Þ ¼ 2l x l y þ C 1 2r xy þ C 2 À Á l 2 x þ l 2 y þ C 1 r 2 x þ r 2 y þ C 2 :(16)
l x , l y are the mean values of image x and y, r 2 x , r 2 y are the variance, r xy is the covariance, and C 1 , C 2 are the stabilization coefficients.ssim value ranges from [0, 1], and the closer the value is to 1, it means the more similar the structure of the two images is.</p>
<p>(3) Perceptual loss: Perceptual loss is based on the pre-trained deep neural network feature layer, which measures the difference between the high-level semantic features to ensure that the reconstructed image is consistent with the real image at the perceptual level:
L Perceptual ¼ X i kÈ i x ð Þ À È i y ð Þk 2 :(17)
È i denotes the i-th layer of features of the pre-trained network, and x and y are the reconstructed and real images, respectively.</p>
<p>(4) Chamfer distance: It is used to evaluate the geometric consistency between the reconstructed 3D point cloud and the real 3D point cloud, which is calculated as
Chamf er S 1 ; S 2 ð Þ¼ X p2S 1 min q2S 2 kp À qk 2 þ X q2S 2 min p2S 1 kp À qk 2 (18)
where S 1 and S 2 are the reconstructed and real point clouds, respectively.The smaller the Chamfer distance, the higher the agreement between the two in terms of geometry.</p>
<p>(5) LPIPS: LPIPS evaluates the perceptual similarity of an image by means of a pre-trained deep neural network, which measures the differences between high-level features and provides sensitivity to changes in image details and textures.Its calculation formula is:
LPIPS x; y ð Þ ¼ X i w i kf i x ð Þ À f i y ð Þk 2 2 (19)
where w i is the pre-training weights and f i is the ith layer feature of the network.Smaller LPIPS values indicate that the two images are perceptually more similar.</p>
<p>Experimental analysis</p>
<p>To test the model's generalization ability, we randomly selected a 2D picture of a toy for 3D model generation and obtained the results shown in Fig. 4. By comparing the results with those generated by the SparseFusion method (Zhou &amp; Tulsiani, 2023), it is evident that the AS-NeRF method yields more accurate and detailed models compared to SparseFusion, regardless of whether the analysis is based on the mesh model or the final coloring model.</p>
<p>To evaluate the performance of the AS-NeRF proposed in this study for 3D reconstruction tasks, we use PSNR as the primary evaluation metric and compare it with two current state-of-the-art methods: SparseFusion and PixelNeRF (Yu et al., 2021).The experiments are conducted on the Co3D dataset, and the reconstruction effects of the three methods are evaluated under different viewing angle conditions by adjusting the number of input viewing angles (one to five viewing angles).The experimental results are shown in Fig. 5.</p>
<p>As can be seen from the results in Figure 5, AS-NeRF exhibits high PSNR values for all the number of input viewpoints, and its PSNR values continue to improve with the increase in the number of input viewpoints, but the improvement gradually decreases.This trend indicates that AS-NeRF can significantly enhance the quality of reconstruction when the initial number of viewpoints is increased.At the same time, the performance improvement stabilizes with a further increase in the number of views.With only one input viewpoint, AS-NeRF improves the PSNR by 2.4 and 5.4 dB compared to SparseFusion and PixelNeRF, respectively, showing its better reconstruction capability under extreme conditions.When the number of input viewpoints is increased to two, the PSNR of AS-NeRF is significantly improved to 35.0 dB, which is 1.4 dB higher than that of SparseFusion and 5.3 dB higher than that of PixelNeRF.This larger improvement reflects the advantages of AS-NeRF in fusing information from multiple viewpoints.With a further increase in the number of viewpoints, although the PSNR enhancement gradually For the SSIM metric, the reconstruction effects of the three methods under different view conditions were also evaluated by adjusting the number of input views (one to five views), and the experimental results in Fig. 6 were finally obtained.AS-NeRF exhibits high SSIM values for all the number of input viewpoints, and its SSIM values continue to improve with the increase in the number of input viewpoints, but the improvement gradually decreases.This trend suggests that AS-NeRF can significantly enhance the structural similarity of the reconstruction when the initial number of viewpoints is increased.At the same time, the performance improvement stabilizes with a further increase in the number of viewpoints.With only one input viewpoint, AS-NeRF improves the SSIM by 0.05 and 0.10 compared to SparseFusion and PixelNeRF, respectively.When the number of input viewpoints is increased to two, the SSIM of AS-NeRF significantly improves to 0.80, representing an increase of 0.08 compared to SparseFusion and 0.10 compared to PixelNeRF, by 0.05.It can be observed that AS-NeRF outperforms SparseFusion and PixelNeRF in terms of SSIM at various input view numbers.</p>
<p>Perceptual loss measures the difference between the reconstructed image and the real image in terms of high-level semantic features, and a lower value indicates that the reconstructed image is closer to the real image at the perceptual level.The experiments are conducted on the Co3D dataset, and the reconstruction effects of the three methods are evaluated under different resolution conditions by adjusting the input image resolution (256 × 256, 512 × 512, 768 × 768, 1,024 × 1,024, 1,280 × 1,280).The specific experimental results are shown in Fig. 7.</p>
<p>At lower resolutions, AS-NeRF has the lowest perceptual loss, which is reduced by 0.05 and 0.10 compared to SparseFusion and PixelNeRF, respectively, showing its accuracy in the base reconstruction task.When the input image resolution is increased to 512 × 512, the Perceptual Loss of AS-NeRF is further reduced by 0.05 and 0.10 compared to SparseFusion and PixelNeRF, respectively, reflecting its stabilizing performance at The following experimental test was performed on the Chamfer distance, a metric that measures the geometric agreement between the reconstructed point cloud and the real one, with lower values indicating a closer shape between the two.The experiments were also performed on the Co3D dataset, and the reconstruction results of the three methods were evaluated under different viewing angle conditions by adjusting the number of input viewpoints (one to five viewpoints).Figure 8 shows the experimental results.</p>
<p>In the single-view condition, the Chamfer distance of AS-NeRF is 4.6, which is significantly reduced by 0.4 and 0.8 compared to 5.0 for SparseFusion and 5.4 for PixelNeRF, showing its reconstruction ability when processing single-view information.The Chamfer distance of AS-NeRF continues to decrease as the number of viewpoints increases.For example, its Chamfer distance decreases to 4.2 under the dual-view condition, which is 0.5 and 0.9 lower than that of SparseFusion and PixelNeRF, respectively, indicating that AS-NeRF has a clear advantage in the initial multiview information fusion.AS-NeRF is capable of extracting multidimensional features to provide rich contextual information for the model.This enables the model to capture and restore complex geometric structures more accurately during the reconstruction process and performs exceptionally well in multiview fusion.The use of a composite loss function and balanced training parameter settings enables AS-NeRF to effectively optimize various performance metrics during training, avoid overfitting, and improve generalization capabilities.This is especially helpful in maintaining stable reconstruction performance under multiview and high-view conditions.LPIPS is used to measure the similarity between the reconstructed image and the real image at the perceptual level, with lower values indicating that the reconstructed image is closer to the real image in terms of perceptual quality.In this experiment, we evaluated the performance of the three methods using the LPIPS metric by conducting experiments on the Co3D dataset.We assessed the reconstruction effect of the three methods under different noise conditions by introducing various levels of image noise (0%, 10%, 20%, 30%, and 40%).The experimental results are shown in Fig. 9.</p>
<p>AS-NeRF exhibits lower LPIPS values at all image noise levels, indicating that it outperforms SparseFusion and PixelNeRF in terms of perceptual similarity.The LPIPS values of all three methods show an increasing trend as the image noise level increases, reflecting the negative impact of noise on the reconstruction quality.However, the LPIPS growth of AS-NeRF is more moderate, especially at higher noise levels, and its performance advantage remains evident.The improved NeRF model employs an adaptive scale adjustment mechanism to dynamically optimize rendering accuracy based on the complexity of image details.The method in this article combines spectral perception techniques to enhance the realism and detail performance of color reproduction, thereby improving the stability of the model under various noise conditions.As the noise level increases, the LPIPS values of SparseFusion and PixelNeRF decrease but fail to reach the level of AS-NeRF, further emphasizing that AS-NeRF has advantages in dealing with noise interference.</p>
<p>CONCLUSION</p>
<p>This study proposes an adaptive scaling neural radiation field (AS-NeRF) framework for the 3D reconstruction of toys.Its multi-task learning feature network can extract texture, shape, color, and depth information in parallel, providing a multidimensional context for subsequent reconstruction.The scale function, combined with a spectral decoder, enables adaptive rendering accuracy that adjusts to local complexity, improving material color restoration.Meanwhile, the conditional diffusion module constrains the inverse diffusion with high-dimensional conditional vectors to enhance the consistency of new viewpoints, particularly in cases with fewer viewpoints.Experiments on the Co3D-Toys dataset demonstrate that AS-NeRF achieves significant improvements in PSNR, SSIM, LPIPS, and Chamfer distance in sparse viewpoints and medium-to-high resolutions compared to existing methods, such as PixelNeRF and SparseFusion, highlighting its potential for application in typical toy scenes.This shows the potential for application in typical toy scenarios.</p>
<p>Although AS-NeRF performs well on several evaluation metrics, it still has some limitations.For example, the current evaluation is still limited to a single dataset and does not adequately cover extreme material and lighting conditions.The robustness and generalization ability of the model will be further verified on a wider range of benchmarks and real-world collected data.The computational complexity and resource consumption of the model remains high when dealing with very high-resolution and large-scale datasets, which limits its widespread use in real-time applications.Although the integration of the conditional diffusion model improves the reconstruction quality, there is still room for further improvement in specific extremely complex scenarios.Future research will enhance and extend the AS-NeRF in the following aspects: further improving its computational efficiency, reducing resource consumption, and enabling real-time 3D reconstruction through model compression, knowledge distillation, or distributed computing.Under the multi-task learning framework, integrate more tasks, such as object recognition and pose estimation, to further improve the comprehensive performance and application capability of the model.Under the multi-task learning framework, integrate more tasks, such as object recognition and pose estimation, to further enhance the comprehensive performance and application capability of the model.</p>
<p>Figure 1
1
Figure 1 Overall architecture of the AS-NeRF approach.Full-size  DOI: 10.7717/peerj-cs.3053/fig-1</p>
<p>Figure 2
2
Figure 2 Feature extraction process of multi-task learning network.Full-size  DOI: 10.7717/peerj-cs.3053/fig-2</p>
<p>Algorithm 1 (continued ) for each task Ti: Compute task-specific features: F_taski = σ(γ_task Á W_task Á F_global + b_task) Step 4: Multi-Task Loss Define loss: L = Σk λk Á Lk + µ Á L_cross Step 5: Output Features Return {F_task1, F_task2, …, F_taskn} extracted features during the rendering process, we define a composite adaptive scaling function:</p>
<p>) c x; e ð Þ denotes the color output at position x and optical wavelength e. l n ; d 2 n ; G n are the center wavelength, variance and weight of the n-th spectral component, respectively, which</p>
<dl>
<dt>Figure 3</dt>
<dt>3</dt>
<dt>Figure 3 NeRF combining adaptive scaling with spectral perception.Full-size  DOI: 10.7717/peerj-cs.3053/fig-3</dt>
<dd>
<p>2D toy image I, camera parameters P, global features G from MTL-FeatureNet, spectral components ε Output: Rendered 3D image C(r) Step 1: Initialize NeRF Model Load MTL-FeatureNet to extract global features G from input image I Define volume density network σ(x) Define color network c(x, ε, S) Step 2: Adaptive-Scale Function Extract local features Q from 3D position x(t) using: FA(G, x(t)) = Local feature estimation based on global features and position Compute adaptive scale S(FA(G, x(t)), G): S(FA(G, x(t)), G) = σ (β Á D_complex (CT (FA(G, x(t)), G ) ) ) Where: D_complex is a depth network CT(Á) is a feature combination function for local and global features Step 3: Spectral-Aware Color Estimation (Continued ) Zou et al. (2025), PeerJ Comput.Sci., DOI 10.7717/peerj-cs.305313/27</p>
</dd>
</dl>
<p>Figure 4
4
Figure 4 Comparison of 3D model generation.(A) 2D Original (B) SparseFusion (C) AS-NeRF (D) SparseFusion (E) AS-NeRF.Full-size  DOI: 10.7717/peerj-cs.3053/fig-4</p>
<p>Figure 5 Figure 6
56
Figure 5 PSNR values in different views.Full-size  DOI: 10.7717/peerj-cs.3053/fig-5</p>
<p>Figure 7
7
Figure 7 Perceptual Loss at different resolutions.Full-size  DOI: 10.7717/peerj-cs.3053/fig-7</p>
<p>Figure 8
8
Figure 8 Chamfer distances in different views.Full-size  DOI: 10.7717/peerj-cs.3053/fig-8</p>
<p>Figure 9
9
Figure 9 LPIPS values under different noises.Full-size  DOI: 10.7717/peerj-cs.3053/fig-9</p>
<p>Zou et al. (2025), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.3053
Zou et al. (2025), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.3053 20/27
Zou et al. (2025), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.3053 24/27
Data AvailabilityThe following information was supplied regarding data availability:The CO3D dataset is available at: https://ai.meta.com/datasets/co3d-downloads.The BVI-Coral dataset is available at Zenodo: Anantrasirichai, N. (2024, April 30).BVI-Coral: Underwater scenes for 3D reconstruction.In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV).Zenodo.https://doi.org/10.5281/zenodo.11093417.The code is available in the Supplemental File.FundingThis research was supported by the Guangdong Province Key Construction Discipline Research Capacity Enhancement Project (Nos.2021ZDJS144, 2024ZDJS130), the Key Research Platform of Guangdong Ordinary Colleges and Universities and Project No. 2024ZDZX3035, the Characteristic Innovation Category Project of Guangdong Ordinary Colleges and Universities (No. 2024KTSCX127), the School-level Scientific Research Project of Guangzhou Xinhua University (No. 2024KYCXTD02), the Young Innovative Talents Category Project of Guangdong Ordinary Colleges and Universities (Nos.ADDITIONAL INFORMATION AND DECLARATIONS Grant DisclosuresThe following grant information was disclosed by the authors: Guangdong Province KeyCompeting InterestsThe authors declare that they have no competing interests.Author Contributions. Jiajun Zou performed the experiments, analyzed the data, prepared figures and/or tables, and approved the final draft. .Shaojiang Liu conceived and designed the experiments, performed the computation work, authored or reviewed drafts of the article, and approved the final draft. .Feng Wang conceived and designed the experiments, performed the experiments, performed the computation work, prepared figures and/or tables, authored or reviewed drafts of the article, and approved the final draft. .Weichuan Ni performed the experiments, analyzed the data, prepared figures and/or tables, and approved the final draft. .Shitong Ye conceived and designed the experiments, performed the computation work, authored or reviewed drafts of the article, and approved the final draft.Supplemental InformationSupplemental information for this article can be found online at http://dx.doi.org/10.7717/peerj-cs.3053#supplemental-information.
The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. 2024kqncx076 2023kqncx124, REFERENCES</p>
<p>Iterative spectrum reconstruction using RGB color. V V Afanasyev, A G Voloboy, A V Ignatenko, 25th International Conference on Computer Graphics and Vision. 2015GraphiCon 2015-Conference Proceedings</p>
<p>A review on 3D reconstruction techniques from 2D images. M Aharchi, Ait Kbir, M , The Proceedings of the 4th International Conference on Smart City Applications 4. ChamSpringer International Publishing20203</p>
<p>Photorealistic monocular 3D reconstruction of humans wearing clothing. T Alldieck, M Zanfir, C Sminchisescu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPiscatawayIEEE2022</p>
<p>Renderdiffusion: image diffusion for 3D reconstruction, inpainting and generation. T Anciukevičius, Z Xu, M Fisher, P Henderson, H Bilen, N Mitra, P Guerrero, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPiscatawayIEEE2023</p>
<p>ViViT: a video vision transformer. A Arnab, M Dehghani, G Heigold, C Sun, M Lučić, C Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionPiscatawayIEEE2021</p>
<p>Mip-NeRF: a multiscale representation for anti-aliasing neural radiance fields. J Barron, B Mildenhall, M Tancik, P Hedman, R Martin-Brualla, P Srinivasan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionPiscatawayIEEE2021</p>
<p>NoPe-NeRF: optimising neural radiance field with no pose prior. W Bian, Z Wang, K Li, J Bian, V Prisacariu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPiscatawayIEEE2023</p>
<p>DReg-NeRF: deep registration for neural radiance fields. Y Chen, G H Lee, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionPiscatawayIEEE2023</p>
<p>Single-view 3D scene reconstruction with high-fidelity shape and texture. Y Chen, J Ni, N Jiang, Y Zhang, Y Zhu, S Huang, 2024 International Conference on 3D Vision (3DV). PiscatawayIEEE2024</p>
<p>Aleth-NeRF: illumination adaptive NeRF with concealing field assumption. Z Cui, L Gu, X Sun, X Ma, Y Qiao, T Harada, 10.1609/aaai.v38i2.27908Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Geo-Neus: geometry-consistent neural implicit surfaces learning for multiview reconstruction. Q Fu, Q Xu, Y Ong, W Tao, Advances in Neural Information Processing Systems. 352022</p>
<p>A survey on vision transformer. K Han, Y Wang, H Chen, X Chen, J Guo, Z Liu, Y Tang, Xiao A Xu, C Xu, Y Yang, Z Zhang, Y Tao, D , 10.1109/tpami.2022.3152247IEEE Transactions on Pattern Analysis and Machine Intelligence. 4512022</p>
<p>NeRF-RPN: a general framework for object detection in NeRFs. B Hu, J Huang, Y Liu, Y Tai, C Tang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPiscatawayIEEE2023</p>
<p>RefSR-NeRF: towards high fidelity and super resolution view synthesis. X Huang, W Li, J Hu, H Chen, Y Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPiscatawayIEEE2023</p>
<p>Neuralangelo: high-fidelity neural surface reconstruction. Z Li, T Müller, A Evans, R Taylor, M Unberath, M Liu, C Lin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPiscatawayIEEE2023</p>
<p>Vision transformer for NeRF-based view synthesis from a single input image. K Lin, Y Lin, W Lai, T Lin, Y Shih, R Ramamoorthi, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionPiscatawayIEEE2023</p>
<p>Swin transformer: hierarchical vision transformer using shifted windows. Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionPiscatawayIEEE2021</p>
<p>Robust e-NeRF: NeRF from sparse &amp; noisy events under non-uniform motion. W F Low, G H Lee, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionPiscatawayIEEE2023</p>
<p>Perception inspired deep neural networks for spectral snapshot compressive imaging. Z Meng, X Yuan, 2021 IEEE International Conference on Image Processing (ICIP). PiscatawayIEEE2021</p>
<p>Representing scenes as neural radiance fields for view synthesis. B Mildenhall, P Srinivasan, M Tancik, J Barron, R Ramamoorthi, R Ng, 10.1145/3503250Communications of the ACM. 6512021</p>
<p>UNISURF: unifying neural implicit surfaces and radiance fields for multiview reconstruction. M Oechsle, S Peng, A Geiger, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionPiscatawayIEEE2021</p>
<p>State-of-the-art in automatic 3D reconstruction of structured indoor environments. G Pintore, C Mura, F Ganovelli, L Fuentes-Perez, R Pajarola, E Gobbetti, 10.1111/cgf.14021Computer Graphics Forum. 3922020</p>
<p>NeurAR: neural uncertainty for autonomous 3D reconstruction with implicit neural representations. Y Ran, J Zeng, S He, J Chen, L Li, Y Chen, G Lee, Q Ye, 10.1109/lra.2023.3235686IEEE Robotics and Automation Letters. 822023</p>
<p>Neuralrecon: real-time coherent 3D reconstruction from monocular video. J Sun, Y Xie, L Chen, X Zhou, H N Bao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPiscatawayIEEE2021</p>
<p>High quality 3D reconstruction based on fusion of polarization imaging and binocular stereo vision. X Tian, R Liu, Z Wang, J Ma, 10.1016/j.inffus.2021.07.002Information Fusion. 77202022</p>
<p>NeuS2: fast learning of neural implicit surfaces for multiview reconstruction. Y Wang, Q Han, M Habermann, K Daniilidis, C Theobalt, L Liu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionPiscatawayIEEE2023</p>
<p>NeuS: learning neural implicit surfaces by volume rendering for multiview reconstruction. P Wang, L Liu, Y Liu, C Theobalt, T Komura, Wang W , 10.48550/arXiv.2106.106892021</p>
<p>Urban 3D modeling using mobile laser scanning: a review. C Wang, C Wen, Y Dai, S Yu, M Liu, 10.1016/j.vrih.2020.05.003Virtual Reality &amp; Intelligent Hardware. 232020</p>
<p>NeRF-DS: neural radiance fields for dynamic specular objects. Z Yan, C Li, G H Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPiscatawayIEEE2023</p>
<p>pixelNeRF: neural radiance fields from one or few images. A Yu, V Ye, M Tancik, A Kanazawa, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPiscatawayIEEE2021</p>
<p>Freedom: training-free energy-guided conditional diffusion model. J Yu, Y Wang, C Zhao, B Ghanem, J Zhang, 10.1109/ICCV51070.2023.02118Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>. Zou, 10.7717/peerj-cs.3053PeerJ Comput. Sci. 2025</p>
<p>Adding conditional control to text-to-image diffusion models. L Zhang, A Rao, M Agrawala, 10.1109/ICCV51070.2023.00355Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Spectrum-aware parameter efficient fine-tuning for diffusion models. X Zhang, S Wen, L Han, J Felix, A Srivastava, J Huang, H Wang, M Tao, D Metaxas, 10.48550/arXiv.2405.210502024</p>
<p>SparseFusion: distilling view-conditioned diffusion for 3D reconstruction. Z Zhou, S Tulsiani, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPiscatawayIEEE2023</p>
<p>Conditional text image generation with diffusion models. Y Zhu, Z Li, T Wang, M He, C Yao, 10.1109/CVPR52729.2023.01368Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>. Zou, 10.7717/peerj-cs.3053PeerJ Comput. Sci. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
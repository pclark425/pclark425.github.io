<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5383 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5383</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5383</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267938586</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.16029v3.pdf" target="_blank">GraphWiz: An Instruction-Following Language Model for Graph Problems</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the delicate balance between training data volume and model performance, highlighting the potential for overfitting with increased data. We also explore the transferability of the model's reasoning ability across different graph tasks, indicating the model's adaptability and practical application potential. Our investigation offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5383.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5383.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>tuple_linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-tuple linearization (node-count + edge tuples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compact textual serialization that represents a graph by first stating the total number of nodes and then listing edges as tuples (u,v) (or triples (u,v,w) for weighted edges), optionally using arrow notation for directed edges (u->v or (u->v,k)). This representation is used as the primary graph-to-text encoding for prompting and training in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>tuple linearization / edge-list serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are converted to plain-text by (1) specifying the total number of nodes, and (2) enumerating edges using concise tuple syntax: (i,j) for undirected edges, (i->j) for directed edges, and (i,j,k) or (i->j,k) for weighted edges; nodes are integer-indexed. Example prompts embed these tuples directly in natural-language question templates. This encoding avoids verbose sentence-per-edge forms and is paired with instruction-style prompts (Alpaca format) and Chain-of-Thought examples.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general algorithmic graphs (Erdős–Rényi random graphs used for training; supports directed/undirected and weighted/unweighted graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compactness (short textual footprint per edge), faithful to raw edge-list (no implicit summarization), token-efficient relative to sentence-based edge descriptions, compatible with LLM input tokens up to an imposed cutoff (paper enforces <=100 nodes), interpretable (human-readable edge tuples), but still subject to input-length limits; preserves full explicit graph topology at token level.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Nine graph computational tasks used as downstream evaluations: Cycle Detection, Connectivity, Bipartite Check, Topological Sort, Shortest Path, Maximum Triangle Sum, Maximum Flow, Hamilton Path, Subgraph Matching (all phrased as natural-language problems with the serialized graph as input).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary metric: accuracy on each task (percentage correct). Reported aggregate: GraphWiz-DPO (trained on GraphInstruct using this encoding + reasoning paths) reaches average accuracy 65% across the nine tasks; GPT-4 baseline average reported as 43.8% (both reported in the paper). Additional per-task numbers appear in tables (e.g., variant/backbone values in Tables 4/5/8).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper contrasts tuple-linearization with more verbose approaches that 'transform edges into sentences' (described in prior work and earlier prompts), arguing that sentence-per-edge forms lengthen context and limit usable graph size (previous works limited to <20 nodes). Using tuple linearization enabled larger graphs (up to 100 nodes) and was the chosen encoding for the GraphInstruct corpus; models trained on this encoding (GraphWiz) outperformed GPT-4 baselines on the benchmark tasks. No explicit numeric head-to-head (BLEU/ROUGE etc.) comparisons of encodings are provided—comparison is qualitative (token efficiency and ability to scale to larger graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Still constrained by context length (graphs >100 nodes excluded); enumeration-based reasoning over the explicit edge-list leads to exponential growth in reasoning steps as node counts increase, causing large accuracy drops beyond ~60 nodes for some tasks. The encoding preserves topology but does not provide structured shortcuts (e.g., adjacency lists vs compressed motifs), so models often rely on brute-force enumeration and can hallucinate steps in the textual reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphWiz: An Instruction-Following Language Model for Graph Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5383.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5383.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>weighted_arrow_notation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weighted/triple and directed-arrow tuple notation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of the tuple linearization to represent weighted and directed graphs by encoding edge weights and direction inline as (i,j,k) for undirected weighted edges or (i->j,k) for directed weighted edges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>weighted triple and arrow notation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>When graphs include weights, edges are serialized as triples (source, target, weight) or arrow-plus-weight (source->target, weight). The same instruction/prompt templates accept these triples and ask the model to solve weighted tasks (e.g., shortest path, maximum flow) using the embedded numeric weights.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>weighted algorithmic graphs (shortest-path, maximum-flow, maximum-triangle-sum tasks); supports directed capacities for flow tasks</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Extends compactness of tuple linearization to include numeric attributes inline; keeps weights explicit and machine-parsable in plain text. Enables LLM to see both topology and numeric edge/node attributes together in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Weighted graph tasks in the benchmark: Shortest Path, Maximum Flow, Maximum Triangle Sum.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy on weighted tasks reported per-task (e.g., shortest path performance declines with graph size; overall weighted-task performance contributes to aggregate accuracies). Paper highlights that models degrade on numeric/algorithmic tasks as graph size increases (accuracy drops near node counts >60). Exact per-task values reported in tables (e.g., Shortest Path accuracy curves in figures), but summary: GraphWiz outperforms GPT-4 on average including weighted tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared qualitatively versus sentence-based descriptions and naive-answer training (Naive-SFT which uses only final labels like 'yes'/'no'); inline weighted tuple representation is preferred for explicit reasoning and for enabling Chain-of-Thought style solutions. No other alternative weight-encoding schemes are empirically compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Numeric reasoning and computation (e.g., computing shortest path sums, flow computations) stress LLM arithmetic and enumerative reasoning; performance drops as node/edge counts grow due to combinatorial explosion and limited working memory in the model. The representation itself is faithful but does not prevent combinatorial reasoning costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphWiz: An Instruction-Following Language Model for Graph Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5383.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5383.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cot_graph_generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting for graph-to-text reasoning generation (GPT-4 CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of few-shot Chain-of-Thought (CoT) prompting with GPT-4 to convert graph inputs (in tuple-linearized text) into explicit step-by-step natural-language reasoning paths and final answers, used to seed the training corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>CoT-based textual reasoning generation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each serialized graph + task prompt is fed to GPT-4 with a Chain-of-Thought prompt template (2-shot examples and the phrase 'Let's think step by step' or concluding with 'The answer is') and sampling (temperature 0.9); GPT-4 produces explicit multi-step reasoning text (R) ending in an answer, which is used as the initial set of reasoning-path labels.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>same as GraphInstruct (algorithmic graphs, directed/undirected, weighted/unweighted)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Produces explicit human-readable reasoning traces (improves interpretability), can generate diverse reasoning styles via sampling, but quality varies by task: GPT-4 produced many correct CoTs for easy tasks but far fewer for some medium/hard tasks (e.g., <100 correct Maximum Flow samples in initial set).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Generation/annotation phase for GraphInstruct; downstream training/evaluation on the nine graph tasks. Also used as baseline in in-context evaluations (GPT-4 with CoT prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>For data-generation: number of correct CoTs retained (initial filtered set 1 contained <8,000 correct samples). For downstream models: training on CoT labels led to GraphWiz achieving high accuracies (GraphWiz-DPO 65% average). Paper reports GPT-4's own performance using CoT prompting in zero-shot/2-shot evaluation (GPT-4 average ~43.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>CoT outputs (textual reasoning paths) are compared as labels to Naive-SFT (training on only final answers). Models trained with explicit CoTs achieve better interpretable reasoning and higher measured accuracy than naive-label training in most settings. The paper also notes that GPT-4 CoT is not uniformly reliable across tasks, motivating augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>GPT-4 sometimes fails to produce correct CoTs for complex tasks; CoT outputs may be short or incorrect (as shown in case studies). Sampling-based diversity introduces incorrect paths that must be filtered. Reliance on a closed-source oracle (GPT-4) for initial labeling can be a bottleneck and is imperfect for hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphWiz: An Instruction-Following Language Model for Graph Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5383.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5383.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>model_aug_rejection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-augmented rejection-sampling augmentation (M1 + selection heuristics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-augmentation pipeline where a smaller finetuned model (M1) repeatedly samples multiple textual reasoning paths per graph and a selection pipeline (string- and semantic-based) chooses diverse correct paths to expand the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>model-augmented rejection sampling for textual reasoning augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>After initial GPT-4 CoT labeling, authors fine-tune a smaller SFT model (M1) on the initial correct set, then sample many outputs per graph (e.g., 30 samples at temperature 0.9). They apply rejection sampling and diversity selection to retain multiple distinct correct reasoning paths per graph. Diversity metrics include string-based measures (edit distance, TF-IDF, Jaccard) and semantic measures (Sentence-BERT embeddings with cosine similarity and KMeans clustering); the longest correct path is used as an anchor for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>same algorithmic graphs used in GraphInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Generates multiple alternative textual reasoning paths for the same graph-instance, increasing label diversity and coverage; selected paths aim to be both correct and semantically/stringwise distinct, improving robustness of instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used to build GraphInstruct (final dataset of 72,785 reasoning paths across 17,158 graph-question pairs) and then to train GraphWiz. Downstream evaluations on the nine graph tasks measure impact.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Contribution to dataset size: GraphInstruct final contained 72,785 reasoning paths (from initial ~8k correct GPT-4 samples). Empirically, models trained on the augmented dataset achieve higher accuracy (e.g., GraphWiz variants show increasing performance as G:R increases; Table 5 shows average accuracy improvements with larger G:R ratios). Exact uplift numbers vary by backbone and G:R ratio; summary: augmentation enabled building a corpus large enough that GraphWiz-DPO reaches 65% average accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Augmentation (M1 + rejection sampling) expands beyond GPT-4-only CoTs, addressing GPT-4's failures on hard tasks (e.g., Maximum Flow). Authors compare different selection heuristics internally (string vs semantic vs clustering) but do not present external encodings; the augmented textual CoTs outperform the initial GPT-4-only set as training targets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Selection relies on heuristics (longest correct path as anchor) which may bias toward verbose reasoning; filtering correctness requires programmatic verification of final answers and may still admit subtly incorrect reasoning (hallucinations). The pipeline depends on a competent SFT seed model (M1) and on heuristic thresholds—these choices can introduce bias or miss certain correct but concise reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphWiz: An Instruction-Following Language Model for Graph Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5383.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5383.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>alpaca_2shot_prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca-format instruction prompts with 2-shot CoT examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting protocol combining Alpaca-style instruction-response formatting with 2-shot Chain-of-Thought exemplars used both to generate initial CoTs from GPT-4 and to evaluate closed-source LLMs in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Alpaca instruction + 2-shot CoT prompt template</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Prompts follow the Alpaca instruction format: an instruction block describing the graph task is provided, followed by a 'Response' field. Training/test prompts include two example Q/A CoT demonstrations (2-shot) and then the query graph; zero-shot variants include the phrase 'Let's think step by step'. These templates were used for GPT-4 labeling, for in-context GPT baselines, and as the standard input format for fine-tuning GraphWiz.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>applies to all algorithmic graph tasks used in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Standardized instruction format improves consistency of generated CoTs and supports supervised fine-tuning; 2-shot examples steer solution style. Template-based prompting is human-readable and reproducible.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used for generating labels (GPT-4 CoTs), for in-context baseline evaluations (GPT-3.5/GPT-4 zero-shot/2-shot), and as the training prompt format for SFT/DPO finetuning of GraphWiz. Test prompts for each task are provided in appendix tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Influences quality of generated reasoning paths and downstream model performance; GPT-4 with these prompts achieved lower average accuracy (~43.8%) than GraphWiz when both use comparable prompt formats, indicating advantages of instruction-tuned supervised models trained on many CoT examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared implicitly to naive single-label training (Naive-SFT) where only final answers are used; Alpaca+CoT supervision leads to more interpretable and typically better-performing models. No explicit external prompt formats are experimentally compared beyond the 2-shot/zero-shot variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality of CoT examples and prompt engineering impacts label quality; complex graphs still pose token-length and reasoning challenges despite standardized prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphWiz: An Instruction-Following Language Model for Graph Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5383.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5383.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>referenced_encoding_works</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior works on graph-to-text encodings (mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites several prior works that explore encoding graphs for LLMs and converting structured graph data into text for LLM consumption—these works are mentioned as related approaches rather than used directly in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a Graph: Encoding Graphs for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>various prior graph-to-text encodings (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Mentioned prior approaches include sentence-based edge descriptions and specialized graph-serialization techniques explored in works like 'Talk like a Graph', 'Let Your Graph Do the Talking', and 'Graphtext', which investigate different ways to present graph structure to LLMs. The present paper references them as related work and contrasts their verbosity or scalability with the tuple-linearization chosen here.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge/structural graphs as studied in the cited works (exact graph types depend on each cited paper)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>These prior works explore trade-offs such as fidelity vs verbosity, structured templates, and hybrid encodings; the current paper highlights that sentence-per-edge encodings are more verbose and can restrict graph size in LLM contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Cited works consider graph reasoning, question answering, and structured-data-to-text tasks; the present paper evaluates its own tuple encoding on algorithmic graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>The paper qualitatively contrasts its tuple-linearization (compact) against sentence-based encodings (verbose, limited scale). It does not provide a direct experimental numeric comparison to these specific prior encodings within the same benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The paper only references these works and does not reproduce or empirically compare their specific encodings; detailed properties must be read from those cited papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphWiz: An Instruction-Following Language Model for Graph Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a Graph: Encoding Graphs for Large Language Models <em>(Rating: 2)</em></li>
                <li>Let Your Graph Do the Talking: Encoding Structured Data for LLMs <em>(Rating: 2)</em></li>
                <li>Graphtext: Graph reasoning in text space <em>(Rating: 2)</em></li>
                <li>Graphllm: Boosting graph reasoning ability of large language model <em>(Rating: 2)</em></li>
                <li>Graph instruction tuning for large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5383",
    "paper_id": "paper-267938586",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "tuple_linearization",
            "name_full": "Edge-tuple linearization (node-count + edge tuples)",
            "brief_description": "A compact textual serialization that represents a graph by first stating the total number of nodes and then listing edges as tuples (u,v) (or triples (u,v,w) for weighted edges), optionally using arrow notation for directed edges (u-&gt;v or (u-&gt;v,k)). This representation is used as the primary graph-to-text encoding for prompting and training in this work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "tuple linearization / edge-list serialization",
            "representation_description": "Graphs are converted to plain-text by (1) specifying the total number of nodes, and (2) enumerating edges using concise tuple syntax: (i,j) for undirected edges, (i-&gt;j) for directed edges, and (i,j,k) or (i-&gt;j,k) for weighted edges; nodes are integer-indexed. Example prompts embed these tuples directly in natural-language question templates. This encoding avoids verbose sentence-per-edge forms and is paired with instruction-style prompts (Alpaca format) and Chain-of-Thought examples.",
            "graph_type": "general algorithmic graphs (Erdős–Rényi random graphs used for training; supports directed/undirected and weighted/unweighted graphs)",
            "representation_properties": "Compactness (short textual footprint per edge), faithful to raw edge-list (no implicit summarization), token-efficient relative to sentence-based edge descriptions, compatible with LLM input tokens up to an imposed cutoff (paper enforces &lt;=100 nodes), interpretable (human-readable edge tuples), but still subject to input-length limits; preserves full explicit graph topology at token level.",
            "evaluation_task": "Nine graph computational tasks used as downstream evaluations: Cycle Detection, Connectivity, Bipartite Check, Topological Sort, Shortest Path, Maximum Triangle Sum, Maximum Flow, Hamilton Path, Subgraph Matching (all phrased as natural-language problems with the serialized graph as input).",
            "performance_metrics": "Primary metric: accuracy on each task (percentage correct). Reported aggregate: GraphWiz-DPO (trained on GraphInstruct using this encoding + reasoning paths) reaches average accuracy 65% across the nine tasks; GPT-4 baseline average reported as 43.8% (both reported in the paper). Additional per-task numbers appear in tables (e.g., variant/backbone values in Tables 4/5/8).",
            "comparison_to_other_representations": "Paper contrasts tuple-linearization with more verbose approaches that 'transform edges into sentences' (described in prior work and earlier prompts), arguing that sentence-per-edge forms lengthen context and limit usable graph size (previous works limited to &lt;20 nodes). Using tuple linearization enabled larger graphs (up to 100 nodes) and was the chosen encoding for the GraphInstruct corpus; models trained on this encoding (GraphWiz) outperformed GPT-4 baselines on the benchmark tasks. No explicit numeric head-to-head (BLEU/ROUGE etc.) comparisons of encodings are provided—comparison is qualitative (token efficiency and ability to scale to larger graphs).",
            "limitations_or_challenges": "Still constrained by context length (graphs &gt;100 nodes excluded); enumeration-based reasoning over the explicit edge-list leads to exponential growth in reasoning steps as node counts increase, causing large accuracy drops beyond ~60 nodes for some tasks. The encoding preserves topology but does not provide structured shortcuts (e.g., adjacency lists vs compressed motifs), so models often rely on brute-force enumeration and can hallucinate steps in the textual reasoning paths.",
            "uuid": "e5383.0",
            "source_info": {
                "paper_title": "GraphWiz: An Instruction-Following Language Model for Graph Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "weighted_arrow_notation",
            "name_full": "Weighted/triple and directed-arrow tuple notation",
            "brief_description": "An extension of the tuple linearization to represent weighted and directed graphs by encoding edge weights and direction inline as (i,j,k) for undirected weighted edges or (i-&gt;j,k) for directed weighted edges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "weighted triple and arrow notation",
            "representation_description": "When graphs include weights, edges are serialized as triples (source, target, weight) or arrow-plus-weight (source-&gt;target, weight). The same instruction/prompt templates accept these triples and ask the model to solve weighted tasks (e.g., shortest path, maximum flow) using the embedded numeric weights.",
            "graph_type": "weighted algorithmic graphs (shortest-path, maximum-flow, maximum-triangle-sum tasks); supports directed capacities for flow tasks",
            "representation_properties": "Extends compactness of tuple linearization to include numeric attributes inline; keeps weights explicit and machine-parsable in plain text. Enables LLM to see both topology and numeric edge/node attributes together in-context.",
            "evaluation_task": "Weighted graph tasks in the benchmark: Shortest Path, Maximum Flow, Maximum Triangle Sum.",
            "performance_metrics": "Accuracy on weighted tasks reported per-task (e.g., shortest path performance declines with graph size; overall weighted-task performance contributes to aggregate accuracies). Paper highlights that models degrade on numeric/algorithmic tasks as graph size increases (accuracy drops near node counts &gt;60). Exact per-task values reported in tables (e.g., Shortest Path accuracy curves in figures), but summary: GraphWiz outperforms GPT-4 on average including weighted tasks.",
            "comparison_to_other_representations": "Compared qualitatively versus sentence-based descriptions and naive-answer training (Naive-SFT which uses only final labels like 'yes'/'no'); inline weighted tuple representation is preferred for explicit reasoning and for enabling Chain-of-Thought style solutions. No other alternative weight-encoding schemes are empirically compared in this paper.",
            "limitations_or_challenges": "Numeric reasoning and computation (e.g., computing shortest path sums, flow computations) stress LLM arithmetic and enumerative reasoning; performance drops as node/edge counts grow due to combinatorial explosion and limited working memory in the model. The representation itself is faithful but does not prevent combinatorial reasoning costs.",
            "uuid": "e5383.1",
            "source_info": {
                "paper_title": "GraphWiz: An Instruction-Following Language Model for Graph Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "cot_graph_generation",
            "name_full": "Chain-of-Thought prompting for graph-to-text reasoning generation (GPT-4 CoT)",
            "brief_description": "Use of few-shot Chain-of-Thought (CoT) prompting with GPT-4 to convert graph inputs (in tuple-linearized text) into explicit step-by-step natural-language reasoning paths and final answers, used to seed the training corpus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "CoT-based textual reasoning generation",
            "representation_description": "Each serialized graph + task prompt is fed to GPT-4 with a Chain-of-Thought prompt template (2-shot examples and the phrase 'Let's think step by step' or concluding with 'The answer is') and sampling (temperature 0.9); GPT-4 produces explicit multi-step reasoning text (R) ending in an answer, which is used as the initial set of reasoning-path labels.",
            "graph_type": "same as GraphInstruct (algorithmic graphs, directed/undirected, weighted/unweighted)",
            "representation_properties": "Produces explicit human-readable reasoning traces (improves interpretability), can generate diverse reasoning styles via sampling, but quality varies by task: GPT-4 produced many correct CoTs for easy tasks but far fewer for some medium/hard tasks (e.g., &lt;100 correct Maximum Flow samples in initial set).",
            "evaluation_task": "Generation/annotation phase for GraphInstruct; downstream training/evaluation on the nine graph tasks. Also used as baseline in in-context evaluations (GPT-4 with CoT prompts).",
            "performance_metrics": "For data-generation: number of correct CoTs retained (initial filtered set 1 contained &lt;8,000 correct samples). For downstream models: training on CoT labels led to GraphWiz achieving high accuracies (GraphWiz-DPO 65% average). Paper reports GPT-4's own performance using CoT prompting in zero-shot/2-shot evaluation (GPT-4 average ~43.8%).",
            "comparison_to_other_representations": "CoT outputs (textual reasoning paths) are compared as labels to Naive-SFT (training on only final answers). Models trained with explicit CoTs achieve better interpretable reasoning and higher measured accuracy than naive-label training in most settings. The paper also notes that GPT-4 CoT is not uniformly reliable across tasks, motivating augmentation.",
            "limitations_or_challenges": "GPT-4 sometimes fails to produce correct CoTs for complex tasks; CoT outputs may be short or incorrect (as shown in case studies). Sampling-based diversity introduces incorrect paths that must be filtered. Reliance on a closed-source oracle (GPT-4) for initial labeling can be a bottleneck and is imperfect for hard tasks.",
            "uuid": "e5383.2",
            "source_info": {
                "paper_title": "GraphWiz: An Instruction-Following Language Model for Graph Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "model_aug_rejection",
            "name_full": "Model-augmented rejection-sampling augmentation (M1 + selection heuristics)",
            "brief_description": "A self-augmentation pipeline where a smaller finetuned model (M1) repeatedly samples multiple textual reasoning paths per graph and a selection pipeline (string- and semantic-based) chooses diverse correct paths to expand the dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "model-augmented rejection sampling for textual reasoning augmentation",
            "representation_description": "After initial GPT-4 CoT labeling, authors fine-tune a smaller SFT model (M1) on the initial correct set, then sample many outputs per graph (e.g., 30 samples at temperature 0.9). They apply rejection sampling and diversity selection to retain multiple distinct correct reasoning paths per graph. Diversity metrics include string-based measures (edit distance, TF-IDF, Jaccard) and semantic measures (Sentence-BERT embeddings with cosine similarity and KMeans clustering); the longest correct path is used as an anchor for comparisons.",
            "graph_type": "same algorithmic graphs used in GraphInstruct",
            "representation_properties": "Generates multiple alternative textual reasoning paths for the same graph-instance, increasing label diversity and coverage; selected paths aim to be both correct and semantically/stringwise distinct, improving robustness of instruction tuning.",
            "evaluation_task": "Used to build GraphInstruct (final dataset of 72,785 reasoning paths across 17,158 graph-question pairs) and then to train GraphWiz. Downstream evaluations on the nine graph tasks measure impact.",
            "performance_metrics": "Contribution to dataset size: GraphInstruct final contained 72,785 reasoning paths (from initial ~8k correct GPT-4 samples). Empirically, models trained on the augmented dataset achieve higher accuracy (e.g., GraphWiz variants show increasing performance as G:R increases; Table 5 shows average accuracy improvements with larger G:R ratios). Exact uplift numbers vary by backbone and G:R ratio; summary: augmentation enabled building a corpus large enough that GraphWiz-DPO reaches 65% average accuracy.",
            "comparison_to_other_representations": "Augmentation (M1 + rejection sampling) expands beyond GPT-4-only CoTs, addressing GPT-4's failures on hard tasks (e.g., Maximum Flow). Authors compare different selection heuristics internally (string vs semantic vs clustering) but do not present external encodings; the augmented textual CoTs outperform the initial GPT-4-only set as training targets.",
            "limitations_or_challenges": "Selection relies on heuristics (longest correct path as anchor) which may bias toward verbose reasoning; filtering correctness requires programmatic verification of final answers and may still admit subtly incorrect reasoning (hallucinations). The pipeline depends on a competent SFT seed model (M1) and on heuristic thresholds—these choices can introduce bias or miss certain correct but concise reasoning styles.",
            "uuid": "e5383.3",
            "source_info": {
                "paper_title": "GraphWiz: An Instruction-Following Language Model for Graph Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "alpaca_2shot_prompts",
            "name_full": "Alpaca-format instruction prompts with 2-shot CoT examples",
            "brief_description": "Prompting protocol combining Alpaca-style instruction-response formatting with 2-shot Chain-of-Thought exemplars used both to generate initial CoTs from GPT-4 and to evaluate closed-source LLMs in-context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Alpaca instruction + 2-shot CoT prompt template",
            "representation_description": "Prompts follow the Alpaca instruction format: an instruction block describing the graph task is provided, followed by a 'Response' field. Training/test prompts include two example Q/A CoT demonstrations (2-shot) and then the query graph; zero-shot variants include the phrase 'Let's think step by step'. These templates were used for GPT-4 labeling, for in-context GPT baselines, and as the standard input format for fine-tuning GraphWiz.",
            "graph_type": "applies to all algorithmic graph tasks used in the paper",
            "representation_properties": "Standardized instruction format improves consistency of generated CoTs and supports supervised fine-tuning; 2-shot examples steer solution style. Template-based prompting is human-readable and reproducible.",
            "evaluation_task": "Used for generating labels (GPT-4 CoTs), for in-context baseline evaluations (GPT-3.5/GPT-4 zero-shot/2-shot), and as the training prompt format for SFT/DPO finetuning of GraphWiz. Test prompts for each task are provided in appendix tables.",
            "performance_metrics": "Influences quality of generated reasoning paths and downstream model performance; GPT-4 with these prompts achieved lower average accuracy (~43.8%) than GraphWiz when both use comparable prompt formats, indicating advantages of instruction-tuned supervised models trained on many CoT examples.",
            "comparison_to_other_representations": "Compared implicitly to naive single-label training (Naive-SFT) where only final answers are used; Alpaca+CoT supervision leads to more interpretable and typically better-performing models. No explicit external prompt formats are experimentally compared beyond the 2-shot/zero-shot variants.",
            "limitations_or_challenges": "Quality of CoT examples and prompt engineering impacts label quality; complex graphs still pose token-length and reasoning challenges despite standardized prompts.",
            "uuid": "e5383.4",
            "source_info": {
                "paper_title": "GraphWiz: An Instruction-Following Language Model for Graph Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "referenced_encoding_works",
            "name_full": "Prior works on graph-to-text encodings (mentions)",
            "brief_description": "The paper cites several prior works that explore encoding graphs for LLMs and converting structured graph data into text for LLM consumption—these works are mentioned as related approaches rather than used directly in experiments.",
            "citation_title": "Talk like a Graph: Encoding Graphs for Large Language Models",
            "mention_or_use": "mention",
            "representation_name": "various prior graph-to-text encodings (referenced)",
            "representation_description": "Mentioned prior approaches include sentence-based edge descriptions and specialized graph-serialization techniques explored in works like 'Talk like a Graph', 'Let Your Graph Do the Talking', and 'Graphtext', which investigate different ways to present graph structure to LLMs. The present paper references them as related work and contrasts their verbosity or scalability with the tuple-linearization chosen here.",
            "graph_type": "knowledge/structural graphs as studied in the cited works (exact graph types depend on each cited paper)",
            "representation_properties": "These prior works explore trade-offs such as fidelity vs verbosity, structured templates, and hybrid encodings; the current paper highlights that sentence-per-edge encodings are more verbose and can restrict graph size in LLM contexts.",
            "evaluation_task": "Cited works consider graph reasoning, question answering, and structured-data-to-text tasks; the present paper evaluates its own tuple encoding on algorithmic graph tasks.",
            "performance_metrics": null,
            "comparison_to_other_representations": "The paper qualitatively contrasts its tuple-linearization (compact) against sentence-based encodings (verbose, limited scale). It does not provide a direct experimental numeric comparison to these specific prior encodings within the same benchmark.",
            "limitations_or_challenges": "The paper only references these works and does not reproduce or empirically compare their specific encodings; detailed properties must be read from those cited papers.",
            "uuid": "e5383.5",
            "source_info": {
                "paper_title": "GraphWiz: An Instruction-Following Language Model for Graph Problems",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a Graph: Encoding Graphs for Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Let Your Graph Do the Talking: Encoding Structured Data for LLMs",
            "rating": 2
        },
        {
            "paper_title": "Graphtext: Graph reasoning in text space",
            "rating": 2
        },
        {
            "paper_title": "Graphllm: Boosting graph reasoning ability of large language model",
            "rating": 2
        },
        {
            "paper_title": "Graph instruction tuning for large language models",
            "rating": 1
        }
    ],
    "cost": 0.018187,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GraphWiz: An Instruction-Following Language Model for Graph Computational Problems
3 Jul 2024</p>
<p>Nuo Chen chennuo26@gmail.com 
Yuhan Li yuhanli98@gmail.com 
Jianheng Tang 
Jia Li </p>
<p>Data Science and Analytics Thrust
The Hong Kong University of Science and Technology (Guangzhou) Guangzhou
China</p>
<p>Data Science and Analytics Thrust
The Hong Kong University of Science and Technology (Guangzhou) Guangzhou
China</p>
<p>Science and Analytics Thrust
The Hong Kong University of Science and Technology Data
The Hong Kong University of Science and Technology (Guangzhou) Hong Kong SAR
China</p>
<p>Data Science and Analytics Thrust
The Hong Kong University of Science and Technology (Guangzhou) Guangzhou
China</p>
<p>KDD'24
August 25-292024BarcelonaSpain</p>
<p>GraphWiz: An Instruction-Following Language Model for Graph Computational Problems
3 Jul 202446093A573FE0BE4D0E06AB328AC3783D10.1145/3637528.3672010arXiv:2402.16029v5[cs.CL]graph algorithmslarge language modelsinstruction tuning
Large language models (LLMs) have achieved impressive success across various domains, but their capability in understanding and resolving complex graph problems is less explored.To bridge this gap, we introduce GraphInstruct, a novel instruction-tuning dataset aimed at enabling language models to tackle a broad spectrum of graph problems through explicit reasoning paths.Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of solving various graph computational problems while generating clear reasoning processes.To further enhance the model's performance and reliability, we integrate the Direct Preference Optimization (DPO) framework within the graph problem-solving context.The improved model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%.Our study also investigates the relationship between training data volume and model performance, emphasizing the risk of overfitting as data volume increases.Additionally, we explore the transferability of the proposed model across different tasks and datasets, demonstrating its robust zero-shot generalization capability.GraphWiz offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving. 1</p>
<p>INTRODUCTION</p>
<p>After witnessing the remarkable capabilities of Large Language Models (LLMs) in text processing, the research community is now exploring their applicability across diverse modalities such as vision, audio, tabular, and spatio-temporal data [10,22,31,44,46,56].Graphs, with their non-Euclidean nature and relationship-centric characteristics, present a particularly challenging yet promising frontier for LLM exploration.The synergy between graphs and LLMs has sparked considerable interest due to their potential for bi-directional benefits: integrating graph-based approaches can advance the reasoning abilities of LLMs, enabling them to tackle complex logical tasks such as mathematical problem-solving [6,7] and commonsense reasoning [48].Conversely, LLMs offer powerful capabilities to augment graph analysis, particularly in the realm of semantically-rich, text-attributed networks [12,35,41].</p>
<p>While the intersection of graphs and LLMs is an emerging field, it remains uncertain to what extent LLMs can comprehend a graph merely from the context.Current graph machine learning tasks, such as node classification and link prediction, primarily require LLMs to focus on the semantic aspects of graphs [8,9,15,16,33,34,42,49,54].This often entails a basic understanding of local subgraph structures.Although benchmarks like mathematical problem-solving [4,7,39,52] and knowledge-oriented question-answering [5,24,[49][50][51] necessitate multi-hop reasoning within a graph, they do not always demand an in-depth understanding of the entire graph's Connectivity： task-specific Q -Q: Determine whether two nodes are connected in an undirected graph.In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.The nodes are numbered from 0 to 9, and the edges are: (0,1) (0,7) (0,6) (1,7) (1,5) (1,6) (5,9) (2,8) (2,4) (2,3) (3,8).Is there a path between node 7 and node 9?</p>
<p>Input R: Node 7 is connected to node 1, node 1 is connected to node 5, node 5 is connected to node 9. We can follow the path: [7-&gt;1-&gt;5-&gt;9], so the answer is yes.structure.To address this, newer benchmarks like GraphQA [12,28] and NLGraph [41] introduce more diverse and complex graph computational problems to LLMs, demanding a more profound grasp of graph structures-challenges typically addressed by specific computational algorithms.Figure 1 showcases giving an undirected graph and two nodes in this graph, the LLM needs to answer whether these two nodes are connected through a path.Other similar problems include finding the shortest path and subgraph isomorphism, etc.</p>
<p>Explicit Reasoning</p>
<p>Current methodologies for tackling graph problems with language models largely rely on the art of prompt design, seeking to enhance the performance of LLMs through well-crafted prompts [12,41].Notably, this strategy has shown more effectiveness in closed-source LLMs, such as GPT-4 [26] and PaLM 2 [1].In contrast, open-source LLMs like LLaMA [38] demonstrate considerably weaker results using the same methods.Additionally, complex prompts that transform edges into sentences can lengthen the context, restricting model's scalability for larger graphs.Indeed, previous studies [3,12,28,41] have limited graph sizes to very small scales (e.g., fewer than 20 nodes and 100 edges).Existing methods also lack the capability to control or understand the output reasoning process.This raises concerns about whether the model is accurately deducing answers through logical reasoning or simply making correct guesses.Significantly, there is a noticeable gap in the availability of an open-source language model proficient in explicitly and accurately solving these graph problems.</p>
<p>In this paper, we aspire to enhance the graph problem-solving abilities of current open-source LLMs and finetune a single model that can solve various types of graph problems and meanwhile output explicit reasoning paths, as the example of connectivity shown in Figure 1.However, developing instruction-following models for graph problems is challenging due to the absence of a robust training corpus equipped to aid models in producing explicit reasoning paths.Such paths are critical for understanding and evaluating the model's reasoning process.To tackle this challenge, we introduce a novel instruction-tuning dataset named GraphInstruct, which is designed to serve as a foundational corpus, enabling language models to not only comprehend graph problems but also articulate clear and logical reasoning paths.GraphInstruct is constructed with the "self-augment" idea: we first sample multiple explicit predictions for each graph problem sample using few-shot Chain-of-Thought (CoT) [46] prompting method based on GPT-4, and then finetune a smaller LLM on these predictions.Given a graph problem, the finetuned model is used to augment the original corpus by generating diverse reasoning paths.This self-augmentation process is crucial in enhancing the diversity and complexity of the reasoning paths, thereby enriching the dataset and making it more representative of various graph problems.GraphInstruct offers 72,785 training samples across nine graph problem tasks, ranging from linear and polynomial complexity to NP-complete, extending the scope and scale of previous benchmarks.</p>
<p>Upon constructing a corpus specialized for explicit reasoning in graph problems, we fine-tuned current open-source LLMs, including the LLaMA 2 families [37,38] and Mistral [18,19].The resulting model, GraphWiz, achieves superior performances in solving various graph computational problems.Our training strategy involves mixed-task instruction tuning and directly preference optimization (DPO) alignment.This dual-focused approach ensures the model not only imitates the best examples but also actively avoids common errors.As a result, the most advanced version of GraphWiz-DPO achieves an average accuracy of 65% across all tasks, significantly outperforming GPT-4, which has an average accuracy of 43.8%.Finally, we delve into the nuanced relationship between training volume, model performance, and overfitting risks.We find that while increasing training data volume generally leads to improved model performance, there is a threshold beyond which the benefits diminish and the risk of overfitting becomes pronounced.Additionally, we explore the potential of transferring the model's reasoning ability across different graph computational problems.</p>
<p>The contributions of this paper are summarized as follows:</p>
<p>• We collect the first large-scale instruction-tuning dataset named GraphInstruct specifically designed for training LLMs on a variety of graph computational tasks.This dataset enables the trained models to output explicit reasoning paths and arrive at final answers, significantly enhancing the models' interpretability.</p>
<p>PRELIMINARY AND RELATED WORKS</p>
<p>Instruction Tuning Instruction Tuning [7,25,27,36,45] is a process where LLMs are explicitly trained to understand and execute instructions provided in natural language.This approach leverages the models' inherent ability to process and generate language, refining it to follow specified tasks more accurately.Concretely, it involves providing a model with a set of instructions or prompts that define specific tasks it should perform.These instructions are formulated in natural language and are designed to guide the model in understanding and executing various tasks.The process then involves adjusting the model's parameters to optimize its performance on these tasks.To date, it's recognized as an essential approach to enhance the LLMs' ability to understand and execute a broad range of tasks based on natural language instructions.In this work:</p>
<p>We aim at leveraging instruction-tuning to build a powerful instruction-following LLM that can understand textural descriptions of graph structures and solve various graph problems explicitly in natural language.</p>
<p>In a general scenario, consider an undirected or directed graph G = {V, E}, where V and E represent the set of nodes and edges, respectively.Each graph is paired with a task-specific instruction or prompt question .Let M denote the current generative LLM, which takes each G- pair as inputs and returns the step-by-step textual reasoning path R, as illustrated in Figure 1.Formally, R = M(G,), where R consists of multiple tokens or sentences, commonly referred to as the Chain-of-Thought (CoT) in natural language processing.For certain graph problem tasks, additional information is included in G, such as the edge weight  in identifying shortest paths.</p>
<p>Directly Prefered Optimization Although current LLMs demonstrate remarkable capabilities across a diverse set of tasks due to extensive training datasets and instruction tuning strategies, they are not immune to issues such as generating misleading information and producing offensive content.Reinforcement Learning from Human Feedback (RLHF) [27,32,55], aims to mitigate these issues by leveraging human feedback to guide the model's learning process.Despite the efficacy of RLHF with PPO [32] in aligning LLMs with diverse human preferences, this approach necessitates the use of four distinct sub-models, which complicates and increases the cost of training.As an alternative, DPO [29] suggests distilling a referential SFT policy,  ref , by accentuating preference differences, which is a simple approach for policy optimization.The DPO method optimizes the policy  0 using a contrasting pair of outputs (  ,   ), as described in the equation below:
L 𝐷𝑃𝑂 (𝜋 0 ; 𝑟 ref ) = −E (𝑥,𝑦 𝑢 ,𝑦 𝑙 )∼𝐷 log 𝜎 𝛽 log 𝜋 0 (𝑦 𝑢 |𝑥) 𝜋 0 (𝑦 𝑙 |𝑥) − 𝛽 log 𝑟 ref (𝑦 𝑢 |𝑥) 𝑟 ref (𝑦 𝑙 |𝑥) ,(1)
where   is favored over   , and  is a hyperparameter.Of note, DPO requires a separate training corpus that belongs to a similar domain with  ref .</p>
<p>METHODOLOGY 3.1 GraphInstruct Collection</p>
<p>In the pursuit of enhancing LLMs' capabilities to tackle graph computational problems with explicit reasoning paths, we develop a new dataset named GraphInstruct.This section outlines the detailed</p>
<p>Step 1: Task Chosen</p>
<p>GraphInstruct</p>
<p>Data Augmentation</p>
<p>Step 2: Problem Generation</p>
<p>Step 3: Initial Generation with GPT-4</p>
<p>Step 5: Rejection Sampling  procedures followed to compile and enhance this dataset, including graph task selection, graph problem generation, and subsequent data augmentation.An overview of our dataset collection process is illustrated in Figure 2.</p>
<p>Graph Task Selection.</p>
<p>In this work, we meticulously choose a diverse set of nine graph computational tasks that encompass different levels of computational complexity.We include four linear complexity tasks: Connectivity, Cycle Detection, Bipartite Graph Checking, and Topological Sort; three polynomial complexity tasks: Shortest Path, Maximum Triangle Sum, and Maximum Flow; and two NP-Complete tasks: Hamilton Path and Subgraph Matching.These tasks are defined in Table 1, with detailed explanations provided in Appendix A. These nine graph tasks provide a comprehensive exploration of algorithmic graph theory, allowing us to enhance the theoretical understanding of graph algorithms and address a broad range of practical applications.</p>
<p>Graph Problem Generation.</p>
<p>To create a diverse and challenging suite of graph problems for model training and testing, we adopt a programming-aid approach inspired by [13] that generates random graph problems for each predefined task.Each task is paired with a unique template designed to capture the distinct characteristics of the graphs, such as being directed or undirected and whether the edges contain weights.We employ the Erdős-Rényi (ER) model [11] to generate random graphs.Specifically, the ER model accepts two parameters: the number of nodes  and the probability  of an edge existing between any two nodes.For each pair of nodes, the generator randomly decides whether to create an edge between them with probability , resulting in a graph with an average edge density of .We utilize the NetworkX library [14] to generate the random graphs and determine the solutions to the graph tasks.</p>
<p>To enhance the quality and diversity of generated problems, we implement specific constraints during the problem generation phase.(1) Diverse Distributions.Each task is crafted to include graph problems with varied distributions.We specify five combinations of node counts and edge densities for each task to introduce varying difficulty levels.The node ranges for each task are detailed in Table 2.It is observed that edge density impacts tasks differently.For example, increasing the edge density in Shortest Path problems generally raises the difficulty by adding more potential routes.Conversely, in Cycle Detection, a higher density may lead to more prevalent cycles, thus simplifying detection.(2) Length Constraints.Considering the token limitations of most current We curate an initial set of 27,000 graph problems (G-) for training, distributed across nine tasks, with 3,000 pairs allocated to each.Furthermore, we prepare 3,600 graph problems (G-) for testing, also divided among nine tasks, with each task receiving 400 pairs.Notably, we only annotate R for the training problems.</p>
<p>Generation of Explicit Reasoning</p>
<p>Paths.The most distinctive aspect of GraphInstruct is that each G- pair is coupled with an explicit reasoning path, R. Given the intensive effort required to manually annotate these paths for graph tasks, we utilize GPT-4 to generate preliminary reasoning paths.Each G- is fed into GPT-4 with a CoT prompt designed to draw out an explicit reasoning path, concluding with "The answer is," to direct the model towards a definitive solution.To promote diversity in responses, we employ a sampling strategy with a temperature setting of 0.9, repeating the procedure three times for each problem.We then select only those G- pairs that demonstrate accurate reasoning paths-those leading to the correct answer-to form our initial dataset,  1 , which includes fewer than 8,000 samples.</p>
<p>Data Augmentation with Rejection Sampling.Observations indicate that GPT-4 struggles with numerous graph tasks, such as yielding fewer than 100 correct samples for the Maximum Flow task in  1 .To enrich the dataset with a broader range of reasoning paths, we implement a rejection sampling strategy [7].Using  1 , we develop a specialized data augmentation model, designated as M 1 , serving two primary functions: (1) Increasing G- quantity: M 1 is aimed at correctly answering questions that GPT-4 previously mishandled, thus increasing the number of viable G- pairs.(2) Generating diverse paths: M 1 is also designed to generate multiple diverse reasoning paths for the same G-, thereby enhancing the diversity of our dataset.Specifically, Specifically, M 1 processes the initial 27,000 graph problems, with the goal of producing multiple reasoning paths per problem through repeated sampling, aiming to significantly broaden the dataset's diversity.</p>
<p>Diverse Reasoning Path Selection.A pivotal aspect of rejection sampling involves selecting correct and diverse reasoning paths to ensure the dataset's quality and utility.Balancing accuracy with diversity presents significant challenges in this phase.To tackle these, we employ refined strategies divided into string-based and semantic-based approaches for selecting distinct generated reasoning paths:</p>
<p>• String-based strategies: These include using edit distance, TF-IDF [2], and Jaccard similarity to evaluate the uniqueness and relevance of the reasoning paths, ensuring a broad representation of problem-solving techniques.• Semantic-based strategies: We utilize Sentence-BERT [30] to encode the reasoning paths and apply cosine similarity and Kmeans clustering [23] to assess their semantic coherence and diversity, encouraging distinct and logically coherent paths.</p>
<p>In our string-based and semantic-based selection strategies, except for K-means clustering, the methodology necessitates a reference or anchor sample to effectively measure the diversity of reasoning paths.This anchor acts as the pivot for assessing the diversity of other paths.We select the longest correct reasoning path generated for each problem as the anchor, based on the assumption that it is likely to encompass a comprehensive and detailed reasoning process [20], thus serving as an ideal reference for evaluating the diversity of other paths.</p>
<p>After applying each selection strategy, we compile the final GraphInstruct, , by incorporating the reasoning paths that exhibit the greatest diversity compared to the anchor path, as determined by each strategy.In contrast, K-means clustering groups the generated reasoning paths based on their semantic similarity, from which we identify and select a representative path from each cluster, filtering out repetitive reasoning paths and ensuring the left paths remain diverse and distinct.This approach guarantees that GraphInstruct contains not only correct solutions to graph problems but also a broad spectrum of reasoning styles and methodologies.GraphInstruct fosters the development of LLMs capable of generating explicit and interpretable reasoning paths, significantly enhancing their problem-solving proficiency across a diverse array of graph computational problems.Through this endeavor, we aim to bridge the gap between the current capabilities of LLMs and the complex demands of graph problem-solving.</p>
<p>GraphInstruct Statistics.</p>
<p>GraphInstruct encompasses 17,158 graph-question (G-) pairs, 313,051 nodes (V), and 72,785 reasoning paths (R), with each graph problem potentially linked to multiple paths.Easy tasks are more prevalent, while tasks like Maximum Flow and Topology Sort are less common.This imbalance is primarily due to the difficulty of generating correct reasoning paths, even with advanced models such as GPT-4.Reasoning paths are pivotal in our dataset, as a single G- pair may correspond to multiple unique paths, thereby enriching the diversity of GraphInstruct.Additionally, the test set introduces more complex challenges, where an increased number of nodes generally indicates greater difficulty in graph-related problems.This comprehensive dataset forms a robust foundation for training and testing models across a range of graph computational challenges, from basic to advanced levels, as the comparison in Table 3.</p>
<p>GraphWiz</p>
<p>Based on the GraphInstruct dataset, we develop GraphWiz, which is designed to enhance the capabilities of current LLMs in solving graph computational problems with explicit reasoning paths.The training methodology for GraphWizinvolves a novel two-phase process.Initially, we employ Mixed-Task Instruction Tuning to refine the model's ability to interpret and solve a diverse array of graph problems.The subsequent phase, DPO Alignment, further sharpens the model's reasoning by training it to differentiate between more and less effective problem-solving paths.This approach, especially the application of DPO in the realm of graph problems, marks a significant advancement in teaching LLMs not only to generate explicit answers but also to develop and adhere to a logical reasoning process that mimics expert problem-solving behavior.</p>
<p>Phase 1:</p>
<p>Mixed-Task Instruction Tuning.The initial phase of our training, called Mixed-Task Instruction Tuning, involves training the model simultaneously on all nine graph tasks from the GraphInstruct dataset.This strategy is designed to provide the model with comprehensive capabilities to solve a diverse array of graph problems by adhering to task-specific instructions.Additionally, it leverages the synergies among different tasks to enhance overall model performance.</p>
<p>During this phase, we employ Supervised Fine-Tuning (SFT) with a conventional language modeling loss.The model is trained to process both the graph G and its corresponding prompt question  as inputs and to generate textual reasoning paths R as outputs.The SFT loss is computed based on the discrepancy between the model's predicted reasoning paths and the actual reasoning paths in the dataset, formally defined as follows:
L LM = − 𝑁 ∑︁ 𝑖=1 𝑀 ∑︁ 𝑗=1 log P(R 𝑖,𝑗 |G 𝑖 , 𝑄 𝑖 ; 𝜃 )(2)
where  represents the number of examples,  is the total number of reasoning paths for G  -  , and  are the parameters of the model.This optimization process trains the model to map graph structures and textual prompts to their corresponding reasoning paths, thereby fostering a deep understanding of how to navigate and solve graph problems using explicit reasoning.The model developed in Phase 1 is named M ( ) sft .To generate these pairs for each G-, we start by generating a new set of 9,000 graph problems.Using the SFT model M ( ) sft , we infer outcomes 20 times for each problem.Consistent with the approach in Section 3.1.3,we select the longest correct path as the preferred reasoning path (R  ).For less preferred paths (R  ), the selection process includes: (1) Using the preferred sample as an anchor, we employ the string-based and semantic-based selection methods (excluding K-means) described in Section 3.1.3to identify the closest incorrect path to the preferred one.(2) We then use Majority Voting [43] to determine the most frequently occurring incorrect path across these strategies, which serves as our dispreferred sample.</p>
<p>This methodology not only ensures the identification of reasoning paths that align with the model's understanding but also targets hard-negative examples similar to the preferred paths but are fundamentally incorrect.This approach significantly enhances the effectiveness of the DPO stage by rigorously challenging the model to distinguish and preferentially align its reasoning capabilities toward more accurate and logical problem-solving strategies.Finally, we collect 6,166 G--R  -R  samples for our DPO training corpus, with detailed statistics available in the Appendix, Table 6.The formal training objective for DPO is defined as follows:
L 𝐷𝑃𝑂 (M (𝜃 ); M (𝜃 ) sft ) = −E (𝑥,R 𝑤 ,R 𝑙 )∼𝐷 log 𝜎 𝛽 log M (𝜃 )(R 𝑤 |𝑥) M (𝜃 )(R 𝑙 |𝑥) − 𝛽 log M (𝜃 ) sft (R 𝑤 |𝑥) M (𝜃 ) sft (R 𝑙 |𝑥) ,(3)
where  represents the concatenation of G and .</p>
<p>EXPERIMENTS</p>
<p>In this section, we evaluate the proposed GraphWiz by addressing the following research questions: RQ1: How does GraphWiz perform on these graph tasks, particularly in comparison to the most powerful closed-source model, such as GPT-4?RQ2: What is the impact of training data volume variations on GraphWiz's performance?RQ3: How transferable is GraphWiz across different graph tasks?RQ4: How do changes in the number of nodes in a graph affect GraphWiz's performance, and what is the maximum complexity of graphs it can effectively handle?RQ5: How does hyperparameter  influence model performance.</p>
<p>Experimental Settings</p>
<p>Experimentally, we define baselines across two principal settings: (1)</p>
<p>In-Context Learning, where the model parameters remain fixed.</p>
<p>We utilize state-of-the-art closed-source LLMs as robust baselines, including GPT-3.5 (turbo-0613) and GPT-4 (2023-03-14-preview). Their effectiveness is evaluated using Chain-of-Thought (CoT) prompting, which provides the model with a sequence of examples that sequentially address task solutions.These models are then tasked to replicate this reasoning in their outputs for new problems.We assess their performance under zero-shot and 2-shot scenarios.For zero-shot prompts, the phrase "Let's think step by step" is included to stimulate the generation of autonomous CoTs.(2) Training with Naive Labels: Our focus here is on training smaller-scale LLMs with simplified labels-direct answers like 'yes' or 'no', without detailed reasoning paths.This approach is termed Naive-SFT.Additionally, we test three Graph Neural Network (GNN) models-Graph Convolutional Network (GCN) [21], Graph Isomorphism Network (GIN) [47], and Graph Attention Network (GAT) [40]-on seven of the nine tasks using identical training and test sets as GraphWiz.</p>
<p>Topology and Hamilton are excluded as they require path-based reasoning, which GNNs are not originally support.</p>
<p>Performance of GraphWiz (RQ1)</p>
<p>We repeat our evaluation three times and report the average results in Table 4 and Figure 4. Notably, our models exhibit exceptional results across various backbones, significantly surpassing GPT-4's performance.This consistency is maintained across a range of tasks, from easy to difficult.DPO further enhances the average performance across tasks.However, DPO might adversely affect specific tasks, indicating that while DPO generally improves model reasoning, it may require further tuning to avoid negative impacts on certain problem types.Although Naive SFT shows commendable The results for GPT-3.5 were obtained from its original paper [41].In the CoT setting, the cycle task uses 4 exemples, while the shortest and flow tasks use 5 exemples.</p>
<p>performance, even slightly better than GPT-4, it is unclear if the outcomes are from random guessing or actual task understanding.</p>
<p>Dataset Transfer.In addition to our constructed test set, we also utilize NLGraph [41] as part of our testing suite.Given the imbalanced distribution of graph task samples within NLGraph, we select three tasks for evaluation: Cycle Detection, Maximum Flow, and Shortest Path.We directly test our models without any example prompts or further training.Our approach also demonstrates robust cross-dataset transfer capabilities.Figure 4 illustrates that both GraphWiz maintains a higher level of accuracy compared with GPT-3.5, underscoring the effectiveness of our methodology across datasets with varying distributions.</p>
<p>Comparison with</p>
<p>Results of GraphWiz with Reasoning Path</p>
<p>Increasing (RQ2) Nevertheless, specific tasks such as Triangle Sum and Hamilton Path display negligible or even reduced accuracy improvements with larger data volumes.For instance, the accuracy for GraphWiz (Mistral-7B) in the Triangle Sum task declines from 47.00% at a 1:1 ratio to 38.75% at a 1:5 ratio, potentially indicating issues such as overfitting where the model excessively learns specific training data patterns at the expense of generalization to new problems.This observation underscores the importance of careful monitoring and validation during model training to ensure broad applicability and effectiveness.Furthermore, the G:R ratios in tasks such as Maximum Flow do not strictly conform to anticipated increments like 1:2 or 1:3.Specifically, in the Maximum Flow task, the GraphInstruct dataset records a G:R ratio below 1:2 (see Table 2).Despite these deviations, both models can exhibit improved performance in this task, indicating that data volume increases in certain tasks might impact results in others.This suggests the potential for cross-task learning, where models apply reasoning paths learned from one task to enhance performance on another, even without a proportional increase in the data volume for the latter.</p>
<p>Transferability of GraphWiz (RQ3)</p>
<p>To explore the transferability of GraphWiz, we establish an additional model variant to explore this aspect: GraphWiz-High.This model is trained exclusively on two high-complexity (NP-Complete) graph tasks: Hamilton Path and Subgraph Matching.To investigate its transferability, we conduct two comparative experiments:</p>
<p>Comparison on High-Complexity Tasks.We first compare GraphWiz-High with the regular GraphWiz on high-complexity tasks.Figure 5 (a) indicates that GraphWiz performs better, validating the effectiveness of mixed-task training.This outcome also suggests that the model is capable of transferring skills learned from other tasks to specific high-complexity tasks.</p>
<p>Zero-Shot Transfer Capability.We further test the zero-shot transfer capability of GraphWiz-High on low and medium-complexity tasks it has never been trained on.As can be seen from Figure 5 (b), GraphWiz-High shows a significant performance improvement compared to the Mistral-7B backbone.Even when compared to GPT-3.5, our model managed to maintain comparable performance.Considering the vast difference in the number of parameters between GPT-3.5 and GraphWiz-High, this indicates that ours possesses a commendable ability to generalize across tasks.</p>
<p>Results of GraphWiz with Increasing Problem Complexity (RQ4)</p>
<p>To address the questions regarding how the model's performance varies with different graph sizes and to figure out the largest graph size the model can effectively solve, we present Figure 6 showcasing the performance of the GraphWiz model on the best-performing Both GraphWiz and GPT-4 exhibit a decrease in performance as the size of the graph increases.However, our model consistently outperforms GPT-4 across most graph sizes, indicating a more robust understanding and processing of graph structures.(2) We observe that the performance on the latter task, Shortest Path, needs improvement, with a notable decline as the number of nodes increases.This decline can likely be attributed to two main factors: The latter tasks demand high reasoning and memory capabilities as higher time complexity, as well as strong computational skills, which may pose an additional challenge to the models' capacities; Experimentally, we find that both models primarily rely on enumeration to arrive at solutions.Consequently, as the graph size enlarges, the required enumerative reasoning grows exponentially, leading to a significant drop in accuracy when the number of nodes exceeds 60, after which they exhibit almost no accuracy.These observations suggest that while GraphWiz shows a clear advantage over GPT-4 in handling graph-related tasks, there is a upper bound of complexity-particularly evident in tasks that require numerical computation-at which the performance of even the most advanced models starts to diminish significantly.</p>
<p>Sensitivity Analysis (RQ5)</p>
<p>We perform sensitivity analysis to understand the influence of hyperparameter  on GraphWiz.Figure 7 present the average performance of our models based on LLaMA 2-7B and 13B versions in easy, medium and hard tasks.The results indicate that the optimal value of  varies depending on the task difficulty and the model size.A higher  seems to favor performance on Hard tasks to some extent, but this is not a strictly linear relationship and does not hold consistently across different model sizes.This study highlights the importance of hyperparameter tuning in achieving the best possible model performance across a range of tasks.The analysis suggests that careful tuning of  is necessary to achieve the best trade-off performances.</p>
<p>Case Study</p>
<p>In this subsection, we present a case study of a complex cycle detection problem that involves an undirected graph with 89 nodes in Figure 8, a scale that poses substantial challenges even for human solvers.For a human to detect a cycle in such an extensive graph would require external aids or substantial time, as it is impractical G-Q: Determine whether or not there is a cycle in an undirected graph.In an undirected graph..,the nodes are numbered from 0 to 88, and the edges are: (0, 73) (0, 51) (0, 10) (0, 63) (0, 28) ( ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------GPT-4: No, there is no cycle in this graph.× GraphWiz: There are many possible ways to find a cycle in this graph, but one example is: Starting from node 0, we can go to node 73 (via edge 0-73), then to node 82 (via edge 73-82), then to node 44 (via edge 82-44), then to node 17 (via edge , then to node 36 (via edge [17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36], then to node 28 (via edge , and back to node 0 (via edge 28-0).This forms a cycle [0-73-82-44-17-36-28-0] without revisiting any edge.So, there is a cycle in this graph.√ Figure 8: A typical case of GraphWiz and GPT-4.</p>
<p>to resolve purely through mental computation.We could observe that GPT-4 outputs a very short incorrect answer, which could arise from the model's constraints in processing long inputs or from an erroneous interpretation of the graph's intricate structure.This limitation reflects the challenges that conventional LLMs encounter when adapting to graph-theoretic problems.In contrast, GraphWiz correctly detects a cycle, providing a clear and detailed reasoning path.This demonstration of GraphWiz's spatial reasoning and memory retention capabilities is significant.It indicates that the model has effectively internalized the principles of graph theory to the extent that it can autonomously navigate through and reason about large-scale and complex graph structures.This case study is a testament to the feasibility of employing GraphWiz for sophisticated graph problems.The model's ability to accurately resolve such complex tasks suggests it has substantial practical applications, offering a robust tool for researchers and practitioners.</p>
<p>CONCLUSION</p>
<p>This paper tackles the challenge of equipping LLMs not only to process but to reason explicitly on a variety of graph computational problems.We introduce GraphInstruct, a novel instruction-tuning dataset encompassing a broad spectrum of graph problems, designed to enhance the interpretability and reasoning capabilities of LLMs.We subsequently fine-tune open-source LLMs on this dataset, notably from the LLaMA 2 and Mistral series, leading to the creation of GraphWiz.This model not only surpasses GPT-4 in performance but also showcases the potential for cross-task transferability of reasoning skills.Our experimental analysis highlights the critical balance needed in training volume to maximize model effectiveness and proposes a promising direction on cross-task transfer learning.Future endeavors will focus on devising more efficient model architectures and training methods to further improve GraphWiz's performance and generalization capabilities.</p>
<p>A TASK DEFINITION</p>
<p>In this section, we provide a detailed definition of each task in our GraphInstruct benchmark.For binary classification tasks, we filter the data to create a balanced set of graphs with both positive and negative labels.</p>
<p>• Task 1: Cycle Detection.In an undirected graph G = {V, E}, the task is to detect the existence of a cycle.A cycle can be defined as a sequence of vertices  1 ,  2 , . . .,   with  ≥ 3, that forms a closed loop, meaning  1 =   .Additionally, for all 1 ≤  &lt; , each vertex   must be distinct from the others, and there must be an edge connecting   to  +1 .• Task 2: Connectivity.Given an undirected graph G = {V, E}, the task is to assess if two randomly chosen nodes  and  are connected through a sequence of edges.• Task 3: Bipartite Graph Check.This task is to determine if a directed graph G = {V, E} is bipartite.A graph is considered bipartite if its nodes can be split into two distinct sets U and V such that no two nodes within the same set are adjacent.• Task 4: Topological Sort.The task entails finding a valid topological sort of a directed graph G = {V, E}.In a topological sort, nodes are linearly ordered such that for each directed edge (, ) from node  to node , the node  is positioned before  in the sequence.• Task 5: Shortest Path.The task requires identifying the shortest path between two nodes in an undirected, weighted graph G = {V, E,  }, where  : E → R + assigns a positive weight to each edge.The goal is to find a path connecting the two nodes such that the total sum of the edge weights along this path is minimized.</p>
<p>• Task 6: Maximum Triangle Sum.Given an undirected, weighted graph G = {V, E,  }, where  : V → R + is a function assigning a positive weight to each node, the task involves finding a triangle, a cycle of three connected vertices ( 1 ,  2 ,  3 ), that maximizes the weight sum  ( 1 ) +  ( 2 ) +  ( 3 ).• Task 7: Maximum Flow.Consider a directed, weighted graph G = {V, E, }, where  : E → R + is a function assigning a positive capacity to each edge, representing the maximum flow that the edge can support.Given a source node   and a sink node   in G, the task is to devise a plan to maximize the flow from the source  to the sink .• Task 8: Hamilton Path.This task is to determine whether there is a Hamilton path in an undirected graph G = {V, E}.A Hamilton path is defined as a path that traverses each node in the graph exactly once.• Task 9: Substructure Matching.Given two graphs, G and G ′ , where G = {V, E} and G ′ = {V ′ , E ′ }, the task is to determine whether there exists a subgraph within G that is isomorphic to G ′ .</p>
<p>B ADDITIONAL DATASET INFORMATION</p>
<p>Table 6 shows the detailed statistics of our DPO training corpus.Table 7 shows the training and testing instruction of our model.Due to the space limit, we only present the connectivity prompts in Table 10.Prompts for other tasks can be found in our GitHub Repository.</p>
<p>C ADDITIONAL EXPERIMENTS</p>
<p>Results on other graph types.GraphWiz is primarily trained on Erdős-Rényi (ER) graphs due to their representation as general random graphs.Effect on the number of reasoning paths.As shown in Table 5, Table 8 provides detailed results of our model's performance on each task as the ratio of the graph to reasoning paths increases.</p>
<p>D MORE CASES</p>
<p>In this section, we present more cases from GPT-4 and our models.</p>
<p>In Figure 9, we present three cases, encompassing Connectivity, Hamilton Path, and Shortest Path tasks.In case 1 and case 2, we can observe that although GPT-4 outputs the correct answer without any explicit reasoning path while ours not only gives detailed explanations but also drives to the final correct conclusion.Moreover, in case 3, ours even outputs the detailed computation equations for finding the shortest path.However, we also present a case in the subgraph matching task: Though ours also gives the correct conclusion, there are some errors in the reasoning process, shown in green-colored words.Such phenomenon is called Hallucination [17,53] in NLP, which means LLMs sometimes generate texts that contradict facts or depict things that never occurred.We call this case as false positive response.This case highlights that hallucination could still be a substantial challenge for current LLMs in graph-problem solving.Tables 10,11,12,13,14,16,15,18,17 present the prompts that we used for testing gpt-4 and chatgpt performances in different tasks, which are also employed in Section 3.1.3.</p>
<p>Table 10: Prompts of Connectivity Task used for GPT-4 and ChatGPT testing, which are also used for initial data generation in Section 3.1.3.</p>
<p>Prompt of Connectivity Tasks. (2-shot)</p>
<p>Determine if there is a path between two nodes in the graph.Note that (i,j) means that node i and node j are connected with an undirected edge.Given a graph and a pair of nodes, you need to output Yes or No step by step, indicating whether the node i and node j are connected.Below are several examples: Q: The nodes are numbered from 0 to 5, and the edges are: (0,1) (1,2) (3,4) (4,5).Is there a path between node 1 and node 4? A: Node 1 is in the connected block consisted of node 0, node 1, and node 2. Node 4 is in the connected block consisting of node 3, node 4, and node 5. Node 1 and node 4 are not in the same connected block, so the answer is no.### No. Q: The nodes are numbered from 0 to 5, and the edges are: (0,1) (0,2) (1,5) (1,2) (1,3) (2,5).Is there a path between node 2 and node 3? A: Node 2 is connected to node 1, node 1 is connected to node 3. We can follow the path: [2-&gt;1-&gt;3], so the answer is yes.### Yes.Find the shortest path between two nodes in an undirected graph.In an undirected graph, (i,j,k) means that node i and node j are connected with an undirected edge with weight k.Given a graph and a pair of nodes, you need to output the shortest path between the two nodes.Below are several examples: Q: In an undirected graph, the nodes are numbered from 0 to 6, and the edges are: (0,1,1) (1,2,2) (0,2,4) (0,4,2) (2,6,2) (4,6,4) (4,3,5) (6,5,3) (3,5,4).Give the weight of the shortest path from node 0 to node 5. A: All the paths from node 0 to node 5 are: 0,2,6,5 with a total weight of «4 + 2 + 3 = 9», 0,1,2,6,5 with a total weight of «1 + 2 + 2 + 3 = 8», 0,4,6,5 with a total weight of «2 + 4 + 3 = 9», 0,4,3,5 with a total weight of «2 + 5 + 4 = 11».The weight of path 0,1,2,6,5 is the smallest, so the shortest path from node 0 to node 5 is [0,1,2,6,5] with a total weight of 8. ### 8. Q: In an undirected graph, the nodes are numbered from 0 to 4, and the edges are: (0,3,2) (0,4,1) (0,2,1) (4,1,2) (2,1,1) (3,2,4) (2,4,1) (3,4,2).Give the weight of the shortest path from node 3 to node 1. A: All the paths from node 3 to node 1 are: 3,2,1 with a total weight of «4 + 1 = 5», 3,2,4,1 with a total weight of «4 + 1 + 2 = 7», 3,4,1 with a total weight of «2 + 2 = 4», 3,4,2,1 with a total weight of «2 + 1 + 1 = 4», 3,0,4,1 with a total weight of «2 + 1 + 2 = 5», 3,0,2,1 with a total weight of «2 + 1 + 1 = 4», 3,4,2,4,1 with a total weight of «2 + 1 + 1 + 2 = 6».The weight of path 3,4,1 is the smallest, so the shortest path from node 3 to node 1 is [3,4,1] with a total weight of 4. ### 4.
Input: Q: <Graph-Prompt Qurestion> A:Input: Q: <Graph-Prompt Qurestion> A:
Table 15: Prompts of Maximum Flow Task used for GPT-4 and ChatGPT testing, which are also used for initial data generation in Section 3.1.3.</p>
<p>Prompt of Maximum Flow Task. (2-shot)</p>
<p>Find the maximum flow between two nodes in a directed graph.In a directed graph, (i-&gt;j,k) means that node i and node j are connected with an directed edge from node i to node j with weight k.Given a graph and a pair of nodes, you need to output the maximum flow between the two nodes.</p>
<p>Below are examples: Q: The nodes are numbered from 0 to 8, and the edges are: (0-&gt;2,3) (0-&gt;1,9) (0-&gt;5,4) (0-&gt;3,1) (1-&gt;2,7) (1-&gt;3,4) (1-&gt;5,7) (1-&gt;4,5) (2-&gt;3,2) (2-&gt;5,3) (2-&gt;8,2) (2-&gt;7,6) (3-&gt;5,8) (3-&gt;8,4) (3-&gt;4,9) (4-&gt;7,4) (4-&gt;5,6) (4-&gt;6,1) (5-&gt;6,2) (6-&gt;7,6).What is the maximum flow from node 0 to node 2? A: Initially, we can direct a flow of 3 units straight from node 0 to node 2 through the edge (0-&gt;2).Further examination reveals that an additional flow can be routed through node 1: the edge (0-&gt;1) can carry up to 9 units, and from node 1 to node 2, we can direct 7 units, as limited by the edge (1-&gt;2).Summing these flows, we find that a direct flow of 3 units and an indirect flow of 7 units via node 1 give us a total maximum flow of 10 units from node 0 to node 2. This calculation takes into account the various paths and their capacities, ensuring that the flow through any edge does not exceed its capacity.Hence, in this graph, the maximum flow from node 0 to node 2 is 10 units.### 10.</p>
<p>Q: The nodes are numbered from 0 to 7, and the edges are: (0-&gt;3,1) (0-&gt;6,5) (0-&gt;1,8) (0-&gt;5,4) (1-&gt;7,1) (1-&gt;6,2) (1-&gt;2,7) (2-&gt;4,5) (2-&gt;5,3) (2-&gt;3,7) (2-&gt;7,4) (3-&gt;6,7) (3-&gt;5,3) (3-&gt;7,7) (4-&gt;7,7) (5-&gt;7,7) (5-&gt;6,1) (6-&gt;7,2).What is the maximum flow from node 2 to node 6? A: The graph contains edges like (2-&gt;3,7) and (3-&gt;6,7), which are crucial for determining the flow.Firstly, there is no direct path from node 2 to node 6, so we explore indirect routes.One such path is through node 3, where node 2 can send a maximum of 7 units to node 3, which in turn can forward up to 7 units to node 6.Another route is via node 5; node 2 can send 3 units to node 5, but due to the limited capacity of 1 unit on the edge from node 5 to node 6, only 1 unit can reach node 6 through this path.There's also a path from node 2 to node 7 with a capacity of 4 units, but it doesn't lead to node 6.Thus, by summing the feasible flows, we find that the maximum flow from node 2 to node 6 is 8 units.### 8.</p>
<p>Input:
Q: <Graph-Prompt Qurestion> A:
Table 16: Prompts of Maximum Triangle Sum Task used for GPT-4 and ChatGPT testing, which are also used for initial data generation in Section 3.1.3.</p>
<p>Prompt of Maximum Triplet Sum Task. (2-shot)</p>
<p>Find the maximum sum of the weights of three interconnected nodes.In an undirected graph, [i, k] means that node i has the weight k. (i,j) means that node i and node j are connected with an undirected edge.Given a graph, you need to output the maximum sum of the weights of three interconnected nodes.</p>
<p>Below are several examples: Q: The nodes are numbered from 0 to 4, weights of nodes are: [0, 2] [1, 9] [2, 6] [3, 10] [4,4], and the edges are: (0, 1) (0, 3) (1, 3) (2, 4) (3,4).What is the maximum sum of the weights of three interconnected nodes?A: The nodes and their weights are as follows: Node 0 with weight 2, Node 1 with weight 9, Node 2 with weight 6, Node 3 with weight 10, and Node 4 with weight 4. Upon examining the connections between these nodes, it becomes evident that only Nodes 0, 1, and 3 form a fully interconnected set, with each node directly connected to the other two.The sum of their weights is «2 (Node 0) + 9 (Node 1) + 10 (Node 3) = 21».Therefore, the maximum sum of the weights of three interconnected nodes in this graph is 21.### 21.</p>
<p>Q: The nodes are numbered from 0 to 4, weights of nodes are: [0, 9] [1, 3] [2, 5] [3, 9] [4, 4], and the edges are: (0, 4) (0, 1) (1, 4) (2,3).What is the maximum sum of the weights of three interconnected nodes?A: The graph comprises nodes 0 to 4, each with respective weights of 9, 3, 5, 9, and 4. Analyzing the graph's edges reveals that Nodes 0, 1, and 4 are the only trio of connected nodes, linked through the edges (0, 4), (0, 1), and (1,4).By adding their weights: «9 (Node 0) + 3 (Node 1) + 4 (Node 4) = 16».There are no other groups of three interconnected nodes in this graph.Therefore, the maximum sum of the weights of three connected nodes in this graph is determined to be 16.### 16.</p>
<p>Input: Q: <Graph-Prompt Qurestion> A: Determine if a smaller graph is present as an exact match within a larger graph.In a directed graph, (i-&gt;j) means that node i and node j are connected with a directed edge from node i to node j.Given a graph G and a subgraph G', you need to output Yes or No, indicating whether subgraph G' is present within the directed graph G.</p>
<p>Below are examples: Q: The nodes of graph G are numbered from 0 to 7, and the edges are: The key node here is 'b', which has outgoing edges to three different nodes: 'c', 'e', and 'd'.Additionally, 'c' has outgoing edges to both 'e' and 'd'.Now let's find a node in graph G with similar outgoing edges: Node 0 has outgoing edges to many nodes but is not a match since no single node has outgoing edges to three other nodes that also interconnect as required.Node 1 has outgoing edges to '6', '5', '4', and '7' but none of these nodes have the required interconnections to match 'c', 'e', and 'd'.Node 2 has outgoing edges to '7', '5', '6', and '3', but again, no suitable interconnections.Node 3 has outgoing edges to '4', '6', '7', and '5'.This resembles 'b' in G', but there must be interconnections between the nodes it points to, matching (c-&gt;e), (c-&gt;d).Node 4 has outgoing edges to '7', '6', and '5'.If node 4 is 'b', then nodes '7', '6', and '5' could be 'c', 'e', and 'd'.Since '7', '6', and '5' are all interconnected, node 4 and its connected nodes match the structure of G'.Thus, the sequence (4-&gt;7), (7-&gt;6), (7-&gt;5), (6-&gt;7), (5-&gt;7) in G corresponds to the sequence (b-&gt;c), (c-&gt;e), (c-&gt;d), (e-&gt;d), (d-&gt;e) in G', which means subgraph G' is present as a direct substructure in graph G. ### Yes.Q: The nodes of graph G are numbered from 0 to 9, and the edges are:
(0-&gt;4) (0-&gt;5) (0-&gt;2) (0-&gt;3) (0-&gt;1) (0-&gt;7) (1-&gt;6) (1-&gt;5) (1-&gt;4) (1-&gt;7) (1-&gt;3) (2-&gt;7) (2-&gt;5) (2-&gt;6) (2-&gt;3) (3-&gt;4) (3-&gt;6) (3-&gt;7) (3-&gt;5) (4-&gt;7) (4-&gt;6) (4-&gt;5) (5-&gt;6) (5-&gt;7) (6-(0-&gt;6) (0-&gt;2) (1-&gt;2) (1-&gt;7) (1-&gt;3) (3-&gt;4) (3-&gt;8) (3-&gt;9) (4-&gt;9
).The nodes of subgraph G' are numbered from a to d, and the edges are: (a-&gt;d) (a-&gt;c) (a-&gt;b) (b-&gt;d) (b-&gt;c) (c-&gt;d).Is subgraph G' present within graph G as a direct substructure?A:To find if subgraph G' is present in graph G, we look for a node with out-degree of 3 (like 'a' in G'), and among those outgoing connections, we need two nodes with an out-degree of at least 2 (like 'b' and 'c' in G'), which are also connected to each other and to the third node (like 'd' in G').Examining graph G: Node 0 has out-degree 2, not enough to match 'a'.Node 1 has out-degree 3, so it could be 'a', with nodes 2, 7, and 3 potentially being 'b', 'c', and 'd'.Node 3 has out-degree 3, so it could be 'a', with nodes 4, 8, and 9 potentially being 'b', 'c', and 'd'.Now we must check the connections between the potential 'b', 'c', and 'd' nodes: For node 1 as 'a', nodes 2, 7, and 3 do not have the required mutual connections.For node 3 as 'a', nodes 4, 8, and 9 do not have the required mutual connections either, since there's no edge from 4 to 8 or 9 to 8. None of the nodes satisfy the conditions of subgraph G' fully.### No.In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.Given a graph, you need to output 'Yes' or 'No', indicating whether there is a Hamiltonian path in the graph.Below are several examples: Q: The nodes are numbered from 0 to 5, and the edges are: (0, 3) (0, 2) (0, 1) (0, 5) (1, 4) (1, 3) (1, 2) (3, 5) (4,5).Is there a Hamiltonian path in this graph?A: To determine if a Hamiltonian path exists in an undirected graph, we need to check if there's a path that visits each node exactly once.Starting at Node 0, we can go to Node 1 (which connects to Nodes 2, 3, 4).From Node 1, moving to Node 4 seems a strategic choice because Node 4 only connects back to Node 1 and to Node 5.After reaching Node 4, we must go to Node 5. From Node 5, we can go to Node 3, as Node 3 connects to Nodes 0 and 1 (which we've visited) and to Node 5. Finally, from Node 3, we can go to Node 2. So, one possible Hamiltonian path is: [0, 1,4,5,3,2].Therefore, there is a Hamiltonian path in this graph.### Yes, [0,1,4,5,3,2].Q: The nodes are numbered from 0 to 5, and the edges are: (0,2) (0,1) (4,5) (4,3) (4,2) (5,3) (1,4) (2,5).Is there a Hamiltonian path in this graph?A: To determine if a Hamiltonian path exists in an undirected graph, we need to check if there's a path that visits each node exactly once.We can start at node 0. As node 0 is connected with ndoe 2, and node 2 is not visited, we can then visit node 2. As node 2 is connected with ndoe 5, and node 5 is not visited, we can then visit node 5.As node 5 is connected with ndoe 3, and node 3 is not visited, we can then visit node 3.As node 3 is connected with ndoe 4, and node 4 is not visited, we can then visit node 4. As node 4 is connected with ndoe 1, and node 1 is not visited, we can then visit node 1.So, one possible Hamiltonian path is: [0,2,5,3,4,1].Therefore, there is a Hamiltonian path in this graph.### Yes, [0,2,5,3,4,1].</p>
<p>Input: Q: <Graph-Prompt Qurestion> A:</p>
<p>Case 1 (Connectivity) G-Q: Determine whether two nodes are connected in an undirected graph.In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.Given a graph and a pair of nodes, you need to output Yes or No, indicating whether the node i and node j are connected.Q: The nodes are numbered from 0 to 12, and the edges are: (0, 12) (0, 8) (0, 4) (1,12) ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------GPT-4: Yes, there is a path.GraphWiz: Node 8 is connected to node 0, node 2, and node 12. Node 10 is connected to node 6, node 7, and node 12.</p>
<p>We can follow the path: [8-&gt;12-&gt;10], so the answer is yes.### Yes.√ Case 2 (Hamilton Path) G-Q: Determine whether or not there is a Hamiltonian path in an undirected graph.In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.Given a graph, you need to output Yes or No, indicating whether there is a Hamiltonian path in the graph.Q: The nodes are numbered from 0 to 4, and the edges are: (0, 1) (0, 4) (1, 3) (2, 3) (2,4).Is there a Hamiltonian path in this graph?---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------GPT-4: Yes, there is a path.GraphWiz: To determine if a Hamiltonian path exists in an undirected graph, we need to check if there's a path that visits each node exactly once.We can start at any node and try to find a path that covers all the nodes.For example, starting at node 0, we can go to node 1, then node 3, then node 2, then node 4.This is a valid Hamiltonian path, as it visits each node once and only once.Another possible Hamiltonian path is [4, 2, 3, 1, 0]. √ Case 3 (Shortest Path) G-Q: Find the shortest path between two nodes in an undirected graph.In an undirected graph, (i,j,k) means that node i and node j are connected with an undirected edge with weight k.Given a graph and a pair of nodes, you need to output the shortest path between the two nodes.Q: The nodes are numbered from 0 to 8, and the edges are: (0,1,4) (1,2,7) (1,7,1) (1,3,4) (2,6,2) (2,4,8) (2,7,5) (3,6,1) (4,8,3) (5,6,6) (6,8,8) (7,8,7).Give the weight of the shortest path from node 0 to node 8.</p>
<ul>
<li>--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------GPT-4: The weight of the shortest path from node 0 to node 8 is 14. × GraphWiz: All the paths from node 0 to node 8 are:\n0,1,7,8 with a total weight of &lt;&lt;4 + 1 + 7 = 12&gt;&gt;, 0,1,3,6,8 with a total weight of &lt;&lt;4 + 4 + 1 + 8 = 17&gt;&gt;, 0,1,2,6,8 with a total weight of &lt;&lt;4 + 7 + 2 + 8 = 21&gt;&gt;, 0,1,2,4,8 with a total weight of &lt;&lt;4 + 7 + 8 + 3 = 22&gt;&gt;, 0,1,2,7,8 with a total weight of &lt;&lt;4 + 7 + 5 + 7 = 23&gt;&gt;.The weight of path 0,1,7,8 is the smallest, so the shortest path from node 0 to node 8 is [0,1,7,8] with a total weight of 12. √</li>
</ul>
<p>Figure 1 :
1
Figure 1: An example of solving the Connectivity task explicitly within natural language via LLMs.</p>
<p>Figure 2 :
2
Figure 2: The overview of GraphInstruct collection process.</p>
<p>Figure 3 :
3
Figure 3: Training pipelines of GraphWiz.</p>
<p>Figure 4 :
4
Figure 4: Performance of GraphWiz (Mistral) on NLGraph test set.The results for GPT-3.5 were obtained from its original paper[41].In the CoT setting, the cycle task uses 4 exemples, while the shortest and flow tasks use 5 exemples.</p>
<p>Figure 5 :
5
Figure 5: (a) Model performance in two high-complexity tasks; (b) Average model performance of in easy and medium graph tasks.The backbone of GraphWiz is Mistral.</p>
<p>Figure 6 :
6
Figure 6: Performance of GPT-4 (2 shot) and GraphWiz based on Mistral 7B with different graph sizes (nodes range) on two tasks.In this work, we use the number of nodes as an indicator of graph complexity.</p>
<p>Figure 7 :
7
Figure 7: Hyperparameter study of .task (a) Cycle Detection, and the most challenging tasks (b) Shortest Path.From the figure, we draw the following conclusions: (1)Both GraphWiz and GPT-4 exhibit a decrease in performance as the size of the graph increases.However, our model consistently outperforms GPT-4 across most graph sizes, indicating a more robust understanding and processing of graph structures.(2)We observe that the performance on the latter task, Shortest Path, needs improvement, with a notable decline as the number of nodes increases.This decline can likely be attributed to two main factors: The latter tasks demand high reasoning and memory capabilities as higher time complexity, as well as strong computational skills, which may pose an additional challenge to the models' capacities; Experimentally, we find that both models primarily rely on enumeration to arrive at solutions.Consequently, as the graph size enlarges, the required enumerative reasoning grows exponentially, leading to a significant drop in accuracy when the number of nodes exceeds 60, after which they exhibit almost no accuracy.These observations suggest that while GraphWiz shows a clear advantage over GPT-4 in handling graph-related tasks, there is a upper bound of complexity-particularly evident in tasks that require numerical computation-at which the performance of even the most advanced models starts to diminish significantly.</p>
<blockquote>
<p>7).The nodes of subgraph G' are numbered from a to e, and the edges are: (a-&gt;b) (b-&gt;c) (b-&gt;e) (b-&gt;d) (c-&gt;e) (c-&gt;d).Is subgraph G' present within graph G as a direct substructure?A: To determine if subgraph G' is present within graph G, let's briefly analyze both graphs: Subgraph G' has the following edges: (a-&gt;b), (b-&gt;c), (b-&gt;e), (b-&gt;d), (c-&gt;e), (c-&gt;d).</p>
</blockquote>
<p>Table 18 :
18
Prompts of Hamilton Path i used for GPT-4 and ChatGPT testing, which are also used for initial data generation in Section 3.1.3.Prompt of Hamilton Path Task.(2-shot) Determine whether or not there is a Hamiltonian path in an undirected graph.</p>
<p>(1,5) (1, 7) (1, 2) (1, 6) (2, 8) (2, 12) (2, 3) (2, 5) (3, 12) (3, 6) (4, 7) (4, 6) (5, 12) (6, 7) (6, 12) (6, 10) (7, 11) (7, 10) (8, 12) (9, 12) (9, 11)(10,12).Is there a path between node 8 and node 10?</p>
<p>Figure 9 :
9
Figure 9: Three cases and responses from GPT-4 and GraphWiz.</p>
<p>Table 1 :
1
Overview of nine tasks in our GraphInstruct benchmark with problem definition, time complexity of representative algorithms, graph type (weighted/directed), node size range, and task difficulty.|V | and |E | indicate the number of nodes and edges in the graph.
ProblemDefinitionTime Complexity Weighted? Directed? Node Range DifficultyCycle Detection Detect if a given graph G contains any cycles.𝑂 (|V | + |E |)✗✗[2, 100]EasyConnectivityAssess if two nodes 𝑢 and 𝑣 in a given graph G𝑂 (|V | + |E |)✗✗[2, 100]Easyare connected via a path.Bipartite GraphJudge if a given graph G is bipartite.𝑂 (|V | + |E |)✗✓[2, 100]EasyCheckTopological Sort Find a topological ordering of vertices in a di-𝑂 (|V | + |E |)✗✓[2, 50]Easyrected acyclic graph G.Shortest PathCompute the shortest path between two specific𝑂 (|E | + |V |log|V |)✓✗[2, 100]Mediumnodes 𝑢 and 𝑣 in a given graph G.Maximum Trian-Find the maximum sum of weights for any con-𝑂 (|V | 3 )✓✗[2, 25]Mediumgle Sumnected triplet of vertices in a given graph G.Maximum Flow Calculate the maximum flow from a source node𝑂 (|V | 2 √︁|E |)✓✓[2, 50]Medium𝑠 to a sink node 𝑡 in a directed graph G.Hamilton PathDetermine if a given graph G has a HamiltonianNP-Complete✗✗[2, 50]Hardpath that visits each vertex exactly once.Subgraph Match-Verify if there exists a subgraph in G that is iso-NP-Complete✗✓[2, 30]Hardingmorphic to a given graph G ′ .</p>
<p>Table 2 :
2
Statistics of our GraphInstruct corpus, including total samples (G-), nodes (V) and reasoning paths R.
EasyMediumHardTaskscycle connect bipartite topology shortest triangleflowhamilton SubgraphSum.Total G-𝑄3,7172,6872,0139021,3922,7564052,0971,43517,158TrainTotal V84,98079,85358,86010,14623,20414,7144,33333,2847,847315,051Total R13,12210,0019,3244,4815,85913,4837478,4546,27472,785TestTotal G-𝑄 Total V400 19,570400 19,500400 19,515400 9,449400 19,449400 4,990400 10,024400 9,732400 6,5943,600 118,823</p>
<p>Table 3 :
3
Comparison between ours and other typical datasets.
IncludeIncludeNode EdgeDatasetsTraining Set? CoTs? Tasks Scale ScaleNLGGraphNoNo89-3510-30GraphQANoNo125-201-100GraphInstructYesYes92-100 5-500open-source LLMs (e.g., 4096 tokens), we eliminate excessively longgraph problems to maintain compatibility, ensuring no graph ex-ceeds 100 nodes. (3) Unique Instances. To build a robust dataset,it is crucial that each problem within the training and testing setsis unique. (4) Efficient Graph Description. In the textual descrip-tions of each problem, we only specify the total number of nodesand depict edges using tuples (𝑢, 𝑣), where 𝑢 and 𝑣 represent theconnected nodes. As illustrated in Figures 1, 8 and Table 10, thisgraph description language maximize graph sizes within the limitsof input length constraints.</p>
<p>Table 4 :
4
Performances of GraphWiz and other baselines on GraphInstruct test set.
4.1.1 Implementation. In this work, we use open-source LLaMA2-7B/13B and Mistral-7B as backbone models, allowing us to buildGraphWiz in multiple scales and architectures. Our codebase is builton DeepSpeed and Huggingface Library. For all models, we set thelearning rate, epochs, and max length as 2e-5, 3, and 2048, runningon NVIDIA 8*A800 GPUs. The batch size is set to 16 or 32. Weuse the alpaca-instruction format during training. For the GraphIn-struct generation in Section 3.1.3, we use 2-shot CoT promptingfor GPT-4. For rejection sampling to augment the data, we train
LLaMA 2-13B model for this purpose.Specifically, we sample 30 times with a temperature of 0.9 for each sample, pursuing a high diversity of reasoning paths.For testing, we set temperature as 0 and maximum output tokens as 1024, ensuring the stable and reasonable generation.All GNNs use a hidden dimension size of16,</p>
<p>Table 5 :
5
GraphWiz performances with increasing ratios of the graph to reasoning path (G:R), which is indicative of the data volume and diversity of reasoning paths available for training.G:R=1:5 means the whole corpus of GraphInstruct.
BackboneG:R Easy Medium Hard Average1:1 61.8132.9236.5046.561:2 68.0033.1738.5049.83Mistral-7B1:3 65.8834.2557.5053.471:4 68.7531.9255.7553.581:5 68.1331.5056.0053.751:1 58.6328.9238.1344.171:2 68.0026.2561.7553.00LLaMA 2-7B1:3 69.5631.9260.5054.001:4 68.0029.2564.0054.361:5 67.6330.0067.2555.082 layers, and 1-dimensional random input features from a Normaldistribution.</p>
<p>Table 5
5
illustrates the average performance of two variants of GraphWiz, Mistral 7B and LLaMA 2-7B, across various task categories.Detailed task results are provided in the Appendix, Table8.The performance metrics consider varying ratios of graph to reasoning path (G:R), which represent the data volume and diversity of reasoning paths employed during training.Across all tasks, both models exhibit consistent performance enhancements in correlation with an expanded training corpus.This trend suggests that a greater variety of reasoning paths typically boosts model efficacy across graph-based problems.</p>
<p>Is there a cycle in this graph?
1, 62) (1, 57) (1,84) (1, 61) (1, 5) (1, 24) (2, 84) (2, 3) (2, 66) (2, 68) (2, 17) (2, 35) (2, 34) (2, 15) (3, 39) (3, 52) (3, 16) (3, 15) (3,8) (4, 69) (4, 85) (4, 36) (4, 72) (5, 44) (6, 77) (6, 7) (7, 85) (8, 64) (8, 23) (8, 28) (9, 34) (9, 31) (9, 61) (9, 28)(10, 26) (11, 37) (11, 39) (11, 19) (11, 64) (13, 73) (13, 61) (13, 80) (13, 85) (14, 86) (14, 59) (14, 32) (14, 58) (14,85) (14, 66) (15, 43) (15, 48) (15, 73) (15, 19) (15, 47) (15, 68) (16, 46) (16, 60) (16, 84) (17, 44) (17, 72) (17, 36)(17, 37) (17, 61) (18, 20) (18, 24) (18, 22) (18, 41) (19, 45) (19, 83) (20, 25) (20, 29) (21, 38) (21, 64) (21, 24) (21,22) (21, 34) (22, 23) (23, 34) (23, 30) (23, 83) (24, 47) (24, 50) (25, 59) (25, 42) (25, 70) (25, 72) (26, 45) (26,30) (26, 87) (26, 80) (26, 50) (27, 77) (27, 58) (27, 60) (27, 29) (28, 36) (28, 59) (28, 88) (29, 31) (29, 33) (29,56) (29, 55) (29, 78) (29, 73) (29, 82) (30, 34) (31, 58) (31, 70) (32, 78) (33, 73) (33, 68) (34, 48) (34, 88) (34,51) (34, 66) (34, 54) (35, 43) (35, 56) (36, 44) (36, 39) (37, 57) (37, 71) (38, 47) (38, 58) (38, 88) (39, 74) (39,48) (39, 45) (39, 42) (40, 60) (40, 59) (40, 71) (40, 76) (41, 79) (42, 60) (42, 62) (42, 72) (43, 57) (44, 47) (44,74) (44, 75) (44, 82) (44, 70) (45, 68) (45, 67) (45, 71) (46, 84) (47, 56) (48, 75) (49, 71) (49, 58) (50, 76) (51,55) (51, 87) (51, 79) (52, 78) (56, 69) (56, 83) (56, 79) (57, 76) (57, 70) (57, 72) (57, 78) (59, 70) (60, 80) (60,76) (60, 77) (60, 75) (61, 74) (61, 77) (62, 84) (62, 66) (66, 69) (68, 88) (70, 88) (70, 76) (71, 81) (71, 72) (71,86) (72, 75) (73, 82) (73, 83) (74, 85).</p>
<p>To evaluate the generalization ability of GraphWiz, we tested it on the Barabási-Albert (BA) model, Path graph, Star graph, and Complete graph.Each graph type included 200 examples for each of the six tasks (Cycle, Connectivity, Bipartite, Hamilton, Shortest, and Flow).The results demonstrate that GraphWiz-Mistral-7B achieves an average accuracy of 68.12% on BA graphs, 54.48% on Path graphs, 63.17% on Star graphs, and 83.33% on Complete graphs, all of which outperform the 47.93% accuracy on ER graphs.These findings indicate that GraphWiz trained on ER graphs can effectively generalize to other graph types.Notably, GraphWiz achieves 100% accuracy on the Cycle, Connectivity, and Bipartite tasks for Complete graphs.</p>
<p>Table 6 :
6
Statistics of our DPO corpus, including total samples (G-), nodes (V) and reasoning paths R.
TasksEasy cycle connect bipartite topology shortest triangle MediumflowHard hamilton SubgraphSum.Total G-𝑄-R 𝑤 -R 𝑙9829726894905515454059625706,166Total V22,13220,70511,54612,80221,6705,84212,86324,84610,050142,456</p>
<p>Table 7 :
7
Training and testing prompts of GraphWiz in our experiments.Input PromptsBelow is an instruction that describes a task.\n Write a response that appropriately completes the request.\n \n ### Instruction: \n {Graph-Prompt Question}\n\n ### Response:</p>
<p>Table 8 :
8
GraphWiz performances with increasing ratios of the graph to reasoning path (G:R), which is indicative of the data volume and diversity of reasoning paths available for training.G:R=1:5 means the whole corpus of GraphInstruct.
EasyMediumHardG:R cycle connect bipartite topology shortest triangle flow hamilton substructureAvg.GraphWiz (Mistral-7B)1:180.2587.5078.501.0023.0047.0028.7531.7541.2546.561:278.2585.0081.7527.0028.0047.0024.5034.2542.7549.831:390.7588.2562.7521.7530.7539.7532.2530.7584.2553.471:490.2587.0078.0019.7526.0043.2526.5026.5085.0053.581:592.0089.5072.0019.0031.2538.7529.2526.5085.5053.75GraphWiz (LLaMA 2-7B)1:177.7584.0071.751.0018.5046.2522.0031.2545.0044.171:294.0085.2576.2517.2523.5035.7519.5040.5083.0053.001:393.0089.0072.5018.2524.7540.5022.5045.0083.2554.001:492.7586.0077.7521.7523.2541.5025.2540.2580.7554.361:591.5087.0074.0018.0028.0038.2524.5052.2582.2555.08</p>
<p>Table 9 :
9
Performance of GraphWiz-Mistral-7B on different graph types.
Cycle Connectivity Bipartite Hamilton Shortest FlowER9289.51926.531.329.3BA999268.551.25939Path559034.377.63733Star7310021.244.373.567Complete10010010073.99432.1</p>
<p>Table 11 :
11
Prompts of Cycle Detection Task used for GPT-4 and ChatGPT testing, which are also used for initial data generation in Section 3.1.3.Determine whether or not there is a cycle in an undirected graph.Begin with '###' to give your final conclusion.In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.Given a graph, you need to output Yes or No step by step, indicating whether there is a cycle in the graph.
Prompt of Cycle Detection Task. (2-shot)Below are examples:two-shot examplesInput:Q: <Graph-Prompt Qurestion>A:</p>
<p>Table 12 :
12
Prompts of Bipartite Graph Check Task used for GPT-4 and ChatGPT testing, which are also used for initial data generation in Section 3.1.3.Determine whether or not a graph is bipartite.In a directed graph, (i-&gt;j) means that node i and node j are connected with an directed edge from node i to node j.Given a graph, you need to output 'Yes' or 'No' step by step, indicating whether the graph is bipartite.
Prompt of Bipartite Graph Check Task. (2-shot)Blow are examples:two-shot examplesInput:Q: <Graph-Prompt Qurestion>A:</p>
<p>Table 13 :
13
Prompts of Topological Sort Task used for GPT-4 and ChatGPT testing, which are also used for initial data generation in Section 3.1.3.Find one of the topology sorting paths of the given graph.In a directed graph, (i-&gt;j) means that node i and node j are connected with a directed edge from node i to node j.Given a graph, you need to output one of the topology sorting paths of the graph.
Prompt of Topological Sort Task. (2-shot)Below are several examples:two-shot examplesInput:Q: <Graph-Prompt Qurestion>A:</p>
<p>Table 14 :
14
Prompts of Shortest Path Task used for GPT-4 and ChatGPT testing, which are also used for initial data generation in Section 3.1.3.
Prompt of Shortest Path Tasks. (2-shot)</p>
<p>Table 17 :
17
Prompts of Subgraph Matching Task used for GPT-4 and ChatGPT testing, which are also used for initial data generation in Section 3.1.3.
Prompt of Subgraph Matching Tasks. (2-shot)
Codes and data are available at https://github.com/nuochenpku/Graph-Reasoning-LLM
ACKNOWLEDGMENTSThis work was supported by NSFC Grant No. 62206067, HKUST-HKUST(GZ) 20 for 20 Cross-campus Collaborative Research Scheme C019 and Guangzhou-HKUST(GZ) Joint Funding Scheme 2023A03J0673.Case 4 (Subgraph Matching)G-Q: Determine if a smaller graph is present as an exact match within a larger graph.In a directed graph, (i-&gt;j) means that node i and node j are connected with a directed edge from node i to node j.Given a graph G and a subgraph G', you need to output Yes or No, indicating whether subgraph G' is present within the directed graph G. Q: The nodes of graph G are numbered from 0 to 8, and the edges are:).The nodes of subgraph G' are numbered from a to e, and the edges are: (a-&gt;e) (a-&gt;d) (b-&gt;c) (c-&gt;e).Is subgraph G' present within graph G as a direct substructure?---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------GraphWiz: To find if subgraph G' is present in graph G, we need to identify a node in G that has two outgoing edges to different nodes, just like 'a' in G'.Then, we need to check if one of those nodes has another outgoing edge to a different node, just like 'e' in G'.Finally, we need to check if the other node has an incoming edge from a different node, just like 'd' in G'.Looking at graph G: Node 0 has five outgoing edges, but none of them lead to a node that has another outgoing edge to a different node, so node 0 cannot be 'a'.Node 1 has three outgoing edges, but none of them lead to a node that has another outgoing edge to a different node, so node 1 cannot be 'a'.Node 2 has two outgoing edges, to nodes 6 and 3. Node 6 has no outgoing edges, so it cannot be 'e'.Node 3 has three outgoing edges, to nodes 6, 5, and 4. Node 6 cannot be 'e', since it has no outgoing edges.Node 5 has one outgoing edge, to node 7, which could be 'e'.Node 4 has no outgoing edges, so it cannot be 'e' or 'd'.Therefore, node 2 cannot be 'a'.Node 3 has four outgoing edges, to nodes 6, 5, 4, and 7. Node 6 cannot be 'e', since it has no outgoing edges.Node 5 has one outgoing edge, to node 7, which could be 'e'.Node 4 has no outgoing edges, so it cannot be 'e' or 'd'.Node 7 has one outgoing edge, to node 8, which could be 'e'.Therefore, node 3 could be 'a', with nodes 5 and 7 being 'e' and 'd' respectively, or vice versa.Checking the connections of node 3's targets: Node 3 to 5 (potential 'e') Node 3 to 7 (potential 'd') Node 5 to 7 (matches 'e' to 'd') Node 7 to 8 (potential 'e') Node 5 to 8 (potential 'd') Node 8 has no outgoing edges, so it cannot be 'e'.Node 5 has one outgoing edge, to node 7, which could be 'e'.Therefore, node 7 cannot be 'd', since it has no incoming edges.Node 5 could be 'd', since it has an incoming edge from node 3. Thus, the sequence 3-&gt;5-&gt;7 in G corresponds to the sequence a-&gt;e-&gt;d in G', and the sequence 3-&gt;7-&gt;8 in G corresponds to the sequence a-&gt;d-&gt;e in G', which means subgraph G' is present as a direct substructure in graph G. ### Yes.√
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023. 2023arXiv preprint</p>
<p>Document clustering: TF-IDF approach. Prafulla Bafna, Dhanya Pramod, Anagha Vaidya, 2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT). 2016</p>
<p>Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang, arXiv:2310.05845Graphllm: Boosting graph reasoning ability of large language model. 2023. 2023arXiv preprint</p>
<p>GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning. Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, Liang Lin, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Fei Zong, Wenjie Xia, Roberto Li, Navigli, Online2021</p>
<p>Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension. Nuo Chen, Hongguang Li, Junqing He, Yinan Bao, Xinshi Lin, Qi Yang, Jianfeng Liu, Ruyi Gan, Jiaxing Zhang, Baoyuan Wang, Jia Li, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>Nuo Chen, Hongguang Li, Baoyuan Wang, Jia Li, arXiv:2401.05384From Good to Great: Improving Math Reasoning with Tool-Augmented Interleaf Prompting. 2023. 2023arXiv preprint</p>
<p>Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, Jia Li, arXiv:2310.20246Breaking language barriers in multilingual mathematical reasoning: Insights and observations. 2023. 2023arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, arXiv:2307.033932023. 2023arXiv preprint</p>
<p>Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang, arXiv:2310.04668Label-free node classification on graphs with large language models (llms). 2023. 2023arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR abs/2110.14168Training Verifiers to Solve Math Word Problems. 2021. 2021</p>
<p>On the evolution of random graphs. Paul Erdős, Alfréd Rényi, Publ. math. inst. hung. acad. sci. 51960. 1960</p>
<p>Talk like a Graph: Encoding Graphs for Large Language Models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, NeurIPS 2023 Workshop: New Frontiers in Graph Learning. 2023</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Exploring network structure, dynamics, and function using NetworkX. Aric Hagberg, Pieter Swart, Daniel S Chult, 2008Los Alamos, NM (United StatesLos Alamos National Lab.(LANL)Technical Report</p>
<p>Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann Lecun, Bryan Hooi, arXiv:2305.19523Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning. 2023. 2023arXiv preprint</p>
<p>Can llms effectively leverage graph structural information: When and why. Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma, arXiv:2309.165952023. 2023arXiv preprint</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, Comput. Surveys. 2023. 2023</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023. 2023Mistral 7B. arXiv preprint</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024. 2024arXiv preprint</p>
<p>Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, arXiv:2401.04925The Impact of Reasoning Step Length on Large Language Models. 2024. 2024arXiv preprint</p>
<p>Semi-Supervised Classification with Graph Convolutional Networks. Thomas N Kipf, Max Welling, 2017In ICLR</p>
<p>Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, arXiv:2304.02643Segment anything. 2023. 2023arXiv preprint</p>
<p>Review on determining number of Cluster in K-Means Clustering. Trupti M Kodinariya, Prashant R Makwana, International Journal. 12013. 2013</p>
<p>. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-Tau Yih, Tim Rocktaschel, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. 2020. 2020</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023. 2023arXiv preprint</p>
<p>. 10.48550/arXiv.2303.08774OpenAI. 2023. GPT-4Arxiv. 2023Technical Report</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow, arXiv:2402.05862Let Your Graph Do the Talking: Encoding Structured Data for LLMs. 2024. 2024arXiv preprint</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, arXiv:2305.18290Direct preference optimization: Your language model is secretly a reward model. 2023. 2023arXiv preprint</p>
<p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022. 2022arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017. 2017arXiv preprint</p>
<p>MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining. Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, Carl Yang, arXiv:2403.047802024. 2024arXiv preprint</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, arXiv:2310.130232023. 2023arXiv preprint</p>
<p>A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment. Jianheng Tang, Kangfei Zhao, Jia Li, Findings of the Association for Computational Linguistics: ACL 2023. Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers; Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford Alpaca: An Instruction-following LLaMA model. 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXiv:2307.09288Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Solving olympiad geometry without human demonstrations. Yuhuai Trieu H Trinh, Wu, He Quoc V Le, Thang He, Luong, Nature. 6252024. 2024</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, 2017In ICLR</p>
<p>Can Language Models Solve Graph Problems in Natural Language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment. Jianing Wang, Junda Wu, Yupeng Hou, Yao Liu, Ming Gao, Julian Mcauley, arXiv:2402.087852024. 2024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022. 2022arXiv preprint</p>
<p>Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, 2022</p>
<p>Finetuned Language Models are Zero-Shot Learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>How powerful are graph neural networks?. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, ICLR. 2019. 2019</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022. 2022arXiv preprint</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.07134Natural language is all a graph needs. 2023. 2023arXiv preprint</p>
<p>End-to-end Spoken Conversational Question Answering: Task, Dataset and Model. Chenyu You, Nuo Chen, Fenglin Liu, Shen Ge, Xian Wu, Yuexian Zou, Findings of the Association for Computational Linguistics: NAACL 2022. 2022</p>
<p>Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering. Chenyu You, Nuo Chen, Yuexian Zou, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023. 2023arXiv preprint</p>
<p>Siren's song in the ai ocean: A survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, arXiv:2309.012192023. 2023arXiv preprint</p>
<p>Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang, arXiv:2310.01089Graphtext: Graph reasoning in text space. 2023. 2023arXiv preprint</p>
<p>Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, arXiv:2307.04964Secrets of rlhf in large language models part i: Ppo. 2023. 2023arXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.10625Least-tomost prompting enables complex reasoning in large language models. 2022. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
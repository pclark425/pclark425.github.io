<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-684 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-684</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-684</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-e4a323a3a1ca5ff4cc2dd74ede65f48fd62b897b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e4a323a3a1ca5ff4cc2dd74ede65f48fd62b897b" target="_blank">Deep API learning</a></p>
                <p><strong>Paper Venue:</strong> SIGSOFT FSE</p>
                <p><strong>Paper TL;DR:</strong> DeepAPI is proposed, a deep learning based approach to generate API usage sequences for a given natural language query that adapts a neural language model named RNN Encoder-Decoder, and generates an API sequence based on the context vector.</p>
                <p><strong>Paper Abstract:</strong> Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bags-of-words and lack a deep understanding of the semantics of the query. We propose DeepAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bag-of-words assumption, it learns the sequence of words in a query and the sequence of associated APIs. DeepAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e684.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e684.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>semantic_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic gap between natural language descriptions and code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The mismatch between what is expressed in natural language (queries, documentation) and how APIs and code sequences actually appear/are used in code, leading to failures of bag-of-words or shallow retrieval approaches to find correct API usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep API Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepAPI (RNN Encoder-Decoder based API learning system)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A system that translates natural language queries / method summaries into sequences of API calls using an attention-based RNN Encoder-Decoder trained on (API sequence, annotation) pairs extracted from Java projects.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>natural language queries and API documentation (JavaDoc summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>extracted API call sequences from Java source (method-level sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>semantic gap / incomplete alignment between NL and code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural language queries and documentation contain high-level semantics that are not captured by simple keyword matching; bag-of-words models and IR methods fail to capture word order, semantic similarity, and higher-level intent, causing poor matches between NL descriptions and relevant API sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model input representation and retrieval stage (query-to-code mapping / code search)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Motivated by literature and validated by comparative experiments: qualitative error analysis and quantitative comparison (BLEU, FRank, relevancy ratios) between DeepAPI and baselines (SWIM, Lucene+UP-Miner), plus concrete example failures (e.g., convert int to string vs convert string to int).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>BLEU (intrinsic), human-evaluated FRank and relevancy ratio (extrinsic). Comparative metrics reported: DeepAPI top-1 BLEU 54.42 vs SWIM 19.90 vs Lucene+UP-Miner 11.97; human-eval average FRank DeepAPI=1.6 vs SWIM>4.0; top-5 relevancy DeepAPI=80% vs SWIM=44%.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: bag-of-words baselines (SWIM, code search) produced much lower BLEU and human relevancy; e.g., SWIM BLEU=19.90 vs DeepAPI 54.42 and SWIM top-5 relevancy 44% vs DeepAPI 80%, indicating poorer effectiveness at returning relevant API usage sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as widespread in background and validated on evaluation set; across 30 held-out queries DeepAPI returned higher relevancy consistently (paper reports averages above); no explicit prevalence percentage beyond experimental metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Use of bag-of-words / word-alignment models that ignore word order and semantic embeddings; natural language ambiguity and the lack of modeling of semantic similarity and sequence information.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use an RNN Encoder-Decoder with word embeddings and attention to capture word order and semantics (DeepAPI), which learns distributed word representations and sequence-to-sequence mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantified: attention-based RNN improved top-1 BLEU from 48.83 to 52.49 (+~8% relative); adding the proposed IDF-weighted cost function further improved top-1 BLEU to 54.42. Human-eval relevancy metrics also show large improvements for DeepAPI versus baselines (see BLEU and relevancy numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / Deep learning for code (API learning and code search)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep API learning', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e684.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e684.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>bag_of_words_misinterpretation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bag-of-words / word-alignment model misinterpretation of queries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Limitations of bag-of-words statistical word-alignment models (e.g., used by SWIM) that fail to capture word sequence and deep semantics, resulting in misinterpretation of queries and poor API sequence retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep API Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Baselines: SWIM and Lucene+UP-Miner (code search + pattern mining)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Comparative systems used as baselines: SWIM expands queries into API lists using a statistical word-alignment model (bag-of-words) and then retrieves sequences; Lucene performs text-based code search and UP-Miner extracts frequent API patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>natural language queries (user queries from logs and designed queries)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>retrieved code snippets and mined API sequences from repository</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithm variant / representation mismatch (sequence vs bag-of-words)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Bag-of-words and statistical word alignment treat queries and APIs as unordered collections, so they cannot distinguish semantic differences arising from word order (e.g., 'convert int to string' vs 'convert string to int') and often return partial or project-specific sequences that do not match user intent.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>query expansion and retrieval (modeling stage of baseline systems)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical comparison: running baselines on the same dataset and manual inspection of returned sequences and failure cases (qualitative examples described).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>BLEU and human-evaluation metrics (FRank, relevancy ratio); direct examples showing partial matches returned by SWIM (e.g., returning Object.hashCode for generate md5 hash code).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Quantified: SWIM top-1 BLEU 19.90 (much lower than DeepAPI 54.42); SWIM average top-5 relevancy 44% vs DeepAPI 80%; for some queries SWIM returned no relevant results in top 10.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across multiple test queries; in the 30-query human study SWIM's average relevancy metrics were substantially lower (reported averages).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Modeling choice (bag-of-words / statistical word alignment) that ignores sequence information and semantic embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Replace bag-of-words alignment with sequence-aware neural models: RNN Encoder-Decoder with attention and word embeddings (DeepAPI).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Demonstrated by improved BLEU and human relevancy metrics (see previous entry); statistical tests (Wilcoxon signed-rank) show p<0.05 for improvements in FRank and relevancy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / information retrieval for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep API learning', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e684.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e684.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>incomplete_sequences_interprocedural</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Incomplete API sequences due to inter-procedural calls and single-method extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>API sequences extracted only at method-level miss APIs constructed or invoked in different methods (inter-procedural), producing incomplete training references and causing generated API sequences to omit necessary calls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep API Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepAPI training corpus extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that extracts API sequences by traversing method bodies' ASTs and producing method-level API call sequences for model training.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>API documentation (method-level JavaDoc summaries used as annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>static-extracted method-level API sequences (Java AST-based extraction using Eclipse JDT)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / data extraction limitation (inter-procedural omission)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Because the extraction algorithm only collects API sequences within a single method, APIs that are created or invoked across methods (e.g., DocumentBuilderFactory.newDocumentBuilder called elsewhere and passed as parameter) are missing from the training sequences; as a result, generated sequences can be partial and miss necessary initialization calls.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / training dataset generation (API sequence extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual error analysis on generated outputs (example: for query 'parse xml' the generated sequence omitted DocumentBuilderFactory.newDocumentBuilder) and tracing back to training data extraction decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative examples and failure case descriptions; no numerical frequency given, but demonstrated with concrete test-case where expected API missing.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to incomplete generated API sequences (functional omissions); the paper reports specific missing-API example for 'parse xml' though no direct numeric performance drop attributed solely to this cause is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not quantified numerically in paper; reported as a plausible reason for some inaccurate/partial results and acknowledged as a limitation to be addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Simplifying assumption in extraction pipeline (method-scoped sequence extraction) and lack of inter-procedural analysis when constructing training references.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Proposed future mitigation: perform more accurate program analysis including inter-procedural analysis to build fuller API sequences for training.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated in the paper; listed as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / program analysis for ML training data</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep API learning', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e684.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e684.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>annotation_quality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Annotation quality and coverage issues (first-sentence JavaDoc)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using only the first sentence of JavaDoc method comments as the natural language annotation introduces noise, omissions, and variable quality in the mapping between NL annotations and API sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep API Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepAPI training corpus (annotation extraction step)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Corpus creation step that extracts the first sentence of JavaDoc comments as the natural language annotation for the corresponding method's API sequence; irregular and non-informative comments are filtered heuristically.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>API documentation (method-level JavaDoc first-sentence summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>annotation extraction code and dataset used to train the model</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous / noisy annotations / incomplete specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>First-sentence summaries can omit important details, include noise (TODO, NOTE, test), or be inconsistent in style and content; excluding methods without JavaDocs also biases the dataset and reduces coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training dataset (annotation extraction and preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Threats-to-validity analysis and acknowledgement by authors; dataset construction description shows filtering heuristics and concerns about noise.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No direct quantitative measurement of annotation noise provided; overall model quality measured via BLEU and human-eval but attribution to annotation noise is qualitative.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potentially reduces model accuracy and generalization by providing noisy or insufficient training signals; authors cite it as a threat and state intent to investigate better NLP techniques for annotation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not numerically quantified in paper; dataset size after extraction is 7,519,907 pairs but no fraction of noisy annotations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Limited annotation extraction heuristic (first sentence only), variability in developer-written JavaDocs, and filtering choices (ignoring undocumented methods).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Filter out obvious irregular annotations; plan to use improved NLP techniques to extract better annotations in future work and to expand annotation sources.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated quantitatively in this study; listed as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / dataset construction for ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep API learning', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e684.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e684.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ubiquitous_api_noise</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ubiquitous API noise and frequency bias in training data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Highly frequent, ubiquitous APIs (e.g., logging calls) appear in many training sequences but are uninformative for specific tasks; without accounting for this, models may overemphasize common APIs or retrieve project-specific trivial sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep API Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepAPI (model training and cost-function design)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RNN Encoder-Decoder model trained on millions of (API sequence, annotation) pairs; authors augmented loss with IDF-based API weighting to down-weight ubiquitous APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>API documentation and natural language queries</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training code and learned model (RNN Encoder-Decoder with modified cost function)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>dataset bias / ubiquitous-element noise (misleading frequent tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Ubiquitous APIs such as Logger.log occur across many samples but do not convey task-specific semantics; their high frequency can cause the model to prioritize or generate unhelpful calls unless corrected for.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model learning / loss function and dataset statistics</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Qualitative reasoning and targeted modeling: authors observed that ubiquitous APIs are not helpful and proposed IDF-based weighting; effectiveness shown by improved BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Incorporated IDF-based penalty term into loss (w_idf = log(N / n_y)); measured model performance (BLEU) with and without the penalty and varying penalty weight lambda. Reported optimum lambda ~0.035.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Applying IDF-based penalty improved BLEU: RNN+Attention BLEU top-1 = 52.49; RNN+Attention+New Cost Function BLEU top-1 = 54.42 (relative improvement ~4% at top-1). This suggests mitigating ubiquitous-API noise yields measurable accuracy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Ubiquitous APIs are described as common in the corpus (e.g., Logger.log cited as an example) but no overall prevalence percentage is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Skewed token frequency distribution in training data where common but uninformative APIs dominate counts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Introduce IDF-based weighting (w_idf) as a penalty term in the cost function to down-weight ubiquitous APIs during training; tune penalty hyperparameter lambda.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantified in experiments: the enhanced cost function produced a top-1 BLEU improvement from 52.49 to 54.42 (RNN+Attention -> RNN+Attention+NewCost), with an empirically optimal lambda ~0.035.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Software engineering / machine learning for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep API learning', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SWIM: synthesizing what I mean: code search and idiomatic snippet synthesis <em>(Rating: 2)</em></li>
                <li>Bimodal modelling of source code and natural language <em>(Rating: 2)</em></li>
                <li>MAPO: Mining API usages from open source repositories <em>(Rating: 1)</em></li>
                <li>What makes APIs hard to learn? answers from developers <em>(Rating: 2)</em></li>
                <li>A field study of API learning obstacles <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-684",
    "paper_id": "paper-e4a323a3a1ca5ff4cc2dd74ede65f48fd62b897b",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "semantic_gap",
            "name_full": "Semantic gap between natural language descriptions and code",
            "brief_description": "The mismatch between what is expressed in natural language (queries, documentation) and how APIs and code sequences actually appear/are used in code, leading to failures of bag-of-words or shallow retrieval approaches to find correct API usage.",
            "citation_title": "Deep API Learning",
            "mention_or_use": "use",
            "system_name": "DeepAPI (RNN Encoder-Decoder based API learning system)",
            "system_description": "A system that translates natural language queries / method summaries into sequences of API calls using an attention-based RNN Encoder-Decoder trained on (API sequence, annotation) pairs extracted from Java projects.",
            "nl_description_type": "natural language queries and API documentation (JavaDoc summaries)",
            "code_implementation_type": "extracted API call sequences from Java source (method-level sequences)",
            "gap_type": "semantic gap / incomplete alignment between NL and code",
            "gap_description": "Natural language queries and documentation contain high-level semantics that are not captured by simple keyword matching; bag-of-words models and IR methods fail to capture word order, semantic similarity, and higher-level intent, causing poor matches between NL descriptions and relevant API sequences.",
            "gap_location": "model input representation and retrieval stage (query-to-code mapping / code search)",
            "detection_method": "Motivated by literature and validated by comparative experiments: qualitative error analysis and quantitative comparison (BLEU, FRank, relevancy ratios) between DeepAPI and baselines (SWIM, Lucene+UP-Miner), plus concrete example failures (e.g., convert int to string vs convert string to int).",
            "measurement_method": "BLEU (intrinsic), human-evaluated FRank and relevancy ratio (extrinsic). Comparative metrics reported: DeepAPI top-1 BLEU 54.42 vs SWIM 19.90 vs Lucene+UP-Miner 11.97; human-eval average FRank DeepAPI=1.6 vs SWIM&gt;4.0; top-5 relevancy DeepAPI=80% vs SWIM=44%.",
            "impact_on_results": "Substantial: bag-of-words baselines (SWIM, code search) produced much lower BLEU and human relevancy; e.g., SWIM BLEU=19.90 vs DeepAPI 54.42 and SWIM top-5 relevancy 44% vs DeepAPI 80%, indicating poorer effectiveness at returning relevant API usage sequences.",
            "frequency_or_prevalence": "Described as widespread in background and validated on evaluation set; across 30 held-out queries DeepAPI returned higher relevancy consistently (paper reports averages above); no explicit prevalence percentage beyond experimental metrics.",
            "root_cause": "Use of bag-of-words / word-alignment models that ignore word order and semantic embeddings; natural language ambiguity and the lack of modeling of semantic similarity and sequence information.",
            "mitigation_approach": "Use an RNN Encoder-Decoder with word embeddings and attention to capture word order and semantics (DeepAPI), which learns distributed word representations and sequence-to-sequence mappings.",
            "mitigation_effectiveness": "Quantified: attention-based RNN improved top-1 BLEU from 48.83 to 52.49 (+~8% relative); adding the proposed IDF-weighted cost function further improved top-1 BLEU to 54.42. Human-eval relevancy metrics also show large improvements for DeepAPI versus baselines (see BLEU and relevancy numbers above).",
            "domain_or_field": "Software engineering / Deep learning for code (API learning and code search)",
            "reproducibility_impact": true,
            "uuid": "e684.0",
            "source_info": {
                "paper_title": "Deep API learning",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "bag_of_words_misinterpretation",
            "name_full": "Bag-of-words / word-alignment model misinterpretation of queries",
            "brief_description": "Limitations of bag-of-words statistical word-alignment models (e.g., used by SWIM) that fail to capture word sequence and deep semantics, resulting in misinterpretation of queries and poor API sequence retrieval.",
            "citation_title": "Deep API Learning",
            "mention_or_use": "use",
            "system_name": "Baselines: SWIM and Lucene+UP-Miner (code search + pattern mining)",
            "system_description": "Comparative systems used as baselines: SWIM expands queries into API lists using a statistical word-alignment model (bag-of-words) and then retrieves sequences; Lucene performs text-based code search and UP-Miner extracts frequent API patterns.",
            "nl_description_type": "natural language queries (user queries from logs and designed queries)",
            "code_implementation_type": "retrieved code snippets and mined API sequences from repository",
            "gap_type": "different algorithm variant / representation mismatch (sequence vs bag-of-words)",
            "gap_description": "Bag-of-words and statistical word alignment treat queries and APIs as unordered collections, so they cannot distinguish semantic differences arising from word order (e.g., 'convert int to string' vs 'convert string to int') and often return partial or project-specific sequences that do not match user intent.",
            "gap_location": "query expansion and retrieval (modeling stage of baseline systems)",
            "detection_method": "Empirical comparison: running baselines on the same dataset and manual inspection of returned sequences and failure cases (qualitative examples described).",
            "measurement_method": "BLEU and human-evaluation metrics (FRank, relevancy ratio); direct examples showing partial matches returned by SWIM (e.g., returning Object.hashCode for generate md5 hash code).",
            "impact_on_results": "Quantified: SWIM top-1 BLEU 19.90 (much lower than DeepAPI 54.42); SWIM average top-5 relevancy 44% vs DeepAPI 80%; for some queries SWIM returned no relevant results in top 10.",
            "frequency_or_prevalence": "Observed across multiple test queries; in the 30-query human study SWIM's average relevancy metrics were substantially lower (reported averages).",
            "root_cause": "Modeling choice (bag-of-words / statistical word alignment) that ignores sequence information and semantic embeddings.",
            "mitigation_approach": "Replace bag-of-words alignment with sequence-aware neural models: RNN Encoder-Decoder with attention and word embeddings (DeepAPI).",
            "mitigation_effectiveness": "Demonstrated by improved BLEU and human relevancy metrics (see previous entry); statistical tests (Wilcoxon signed-rank) show p&lt;0.05 for improvements in FRank and relevancy.",
            "domain_or_field": "Software engineering / information retrieval for code",
            "reproducibility_impact": true,
            "uuid": "e684.1",
            "source_info": {
                "paper_title": "Deep API learning",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "incomplete_sequences_interprocedural",
            "name_full": "Incomplete API sequences due to inter-procedural calls and single-method extraction",
            "brief_description": "API sequences extracted only at method-level miss APIs constructed or invoked in different methods (inter-procedural), producing incomplete training references and causing generated API sequences to omit necessary calls.",
            "citation_title": "Deep API Learning",
            "mention_or_use": "use",
            "system_name": "DeepAPI training corpus extraction pipeline",
            "system_description": "Pipeline that extracts API sequences by traversing method bodies' ASTs and producing method-level API call sequences for model training.",
            "nl_description_type": "API documentation (method-level JavaDoc summaries used as annotations)",
            "code_implementation_type": "static-extracted method-level API sequences (Java AST-based extraction using Eclipse JDT)",
            "gap_type": "incomplete specification / data extraction limitation (inter-procedural omission)",
            "gap_description": "Because the extraction algorithm only collects API sequences within a single method, APIs that are created or invoked across methods (e.g., DocumentBuilderFactory.newDocumentBuilder called elsewhere and passed as parameter) are missing from the training sequences; as a result, generated sequences can be partial and miss necessary initialization calls.",
            "gap_location": "data preprocessing / training dataset generation (API sequence extraction)",
            "detection_method": "Manual error analysis on generated outputs (example: for query 'parse xml' the generated sequence omitted DocumentBuilderFactory.newDocumentBuilder) and tracing back to training data extraction decisions.",
            "measurement_method": "Qualitative examples and failure case descriptions; no numerical frequency given, but demonstrated with concrete test-case where expected API missing.",
            "impact_on_results": "Leads to incomplete generated API sequences (functional omissions); the paper reports specific missing-API example for 'parse xml' though no direct numeric performance drop attributed solely to this cause is provided.",
            "frequency_or_prevalence": "Not quantified numerically in paper; reported as a plausible reason for some inaccurate/partial results and acknowledged as a limitation to be addressed.",
            "root_cause": "Simplifying assumption in extraction pipeline (method-scoped sequence extraction) and lack of inter-procedural analysis when constructing training references.",
            "mitigation_approach": "Proposed future mitigation: perform more accurate program analysis including inter-procedural analysis to build fuller API sequences for training.",
            "mitigation_effectiveness": "Not evaluated in the paper; listed as future work.",
            "domain_or_field": "Software engineering / program analysis for ML training data",
            "reproducibility_impact": true,
            "uuid": "e684.2",
            "source_info": {
                "paper_title": "Deep API learning",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "annotation_quality",
            "name_full": "Annotation quality and coverage issues (first-sentence JavaDoc)",
            "brief_description": "Using only the first sentence of JavaDoc method comments as the natural language annotation introduces noise, omissions, and variable quality in the mapping between NL annotations and API sequences.",
            "citation_title": "Deep API Learning",
            "mention_or_use": "use",
            "system_name": "DeepAPI training corpus (annotation extraction step)",
            "system_description": "Corpus creation step that extracts the first sentence of JavaDoc comments as the natural language annotation for the corresponding method's API sequence; irregular and non-informative comments are filtered heuristically.",
            "nl_description_type": "API documentation (method-level JavaDoc first-sentence summaries)",
            "code_implementation_type": "annotation extraction code and dataset used to train the model",
            "gap_type": "ambiguous / noisy annotations / incomplete specification",
            "gap_description": "First-sentence summaries can omit important details, include noise (TODO, NOTE, test), or be inconsistent in style and content; excluding methods without JavaDocs also biases the dataset and reduces coverage.",
            "gap_location": "training dataset (annotation extraction and preprocessing)",
            "detection_method": "Threats-to-validity analysis and acknowledgement by authors; dataset construction description shows filtering heuristics and concerns about noise.",
            "measurement_method": "No direct quantitative measurement of annotation noise provided; overall model quality measured via BLEU and human-eval but attribution to annotation noise is qualitative.",
            "impact_on_results": "Potentially reduces model accuracy and generalization by providing noisy or insufficient training signals; authors cite it as a threat and state intent to investigate better NLP techniques for annotation extraction.",
            "frequency_or_prevalence": "Not numerically quantified in paper; dataset size after extraction is 7,519,907 pairs but no fraction of noisy annotations reported.",
            "root_cause": "Limited annotation extraction heuristic (first sentence only), variability in developer-written JavaDocs, and filtering choices (ignoring undocumented methods).",
            "mitigation_approach": "Filter out obvious irregular annotations; plan to use improved NLP techniques to extract better annotations in future work and to expand annotation sources.",
            "mitigation_effectiveness": "Not evaluated quantitatively in this study; listed as future work.",
            "domain_or_field": "Software engineering / dataset construction for ML",
            "reproducibility_impact": true,
            "uuid": "e684.3",
            "source_info": {
                "paper_title": "Deep API learning",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "ubiquitous_api_noise",
            "name_full": "Ubiquitous API noise and frequency bias in training data",
            "brief_description": "Highly frequent, ubiquitous APIs (e.g., logging calls) appear in many training sequences but are uninformative for specific tasks; without accounting for this, models may overemphasize common APIs or retrieve project-specific trivial sequences.",
            "citation_title": "Deep API Learning",
            "mention_or_use": "use",
            "system_name": "DeepAPI (model training and cost-function design)",
            "system_description": "RNN Encoder-Decoder model trained on millions of (API sequence, annotation) pairs; authors augmented loss with IDF-based API weighting to down-weight ubiquitous APIs.",
            "nl_description_type": "API documentation and natural language queries",
            "code_implementation_type": "training code and learned model (RNN Encoder-Decoder with modified cost function)",
            "gap_type": "dataset bias / ubiquitous-element noise (misleading frequent tokens)",
            "gap_description": "Ubiquitous APIs such as Logger.log occur across many samples but do not convey task-specific semantics; their high frequency can cause the model to prioritize or generate unhelpful calls unless corrected for.",
            "gap_location": "model learning / loss function and dataset statistics",
            "detection_method": "Qualitative reasoning and targeted modeling: authors observed that ubiquitous APIs are not helpful and proposed IDF-based weighting; effectiveness shown by improved BLEU.",
            "measurement_method": "Incorporated IDF-based penalty term into loss (w_idf = log(N / n_y)); measured model performance (BLEU) with and without the penalty and varying penalty weight lambda. Reported optimum lambda ~0.035.",
            "impact_on_results": "Applying IDF-based penalty improved BLEU: RNN+Attention BLEU top-1 = 52.49; RNN+Attention+New Cost Function BLEU top-1 = 54.42 (relative improvement ~4% at top-1). This suggests mitigating ubiquitous-API noise yields measurable accuracy gains.",
            "frequency_or_prevalence": "Ubiquitous APIs are described as common in the corpus (e.g., Logger.log cited as an example) but no overall prevalence percentage is provided.",
            "root_cause": "Skewed token frequency distribution in training data where common but uninformative APIs dominate counts.",
            "mitigation_approach": "Introduce IDF-based weighting (w_idf) as a penalty term in the cost function to down-weight ubiquitous APIs during training; tune penalty hyperparameter lambda.",
            "mitigation_effectiveness": "Quantified in experiments: the enhanced cost function produced a top-1 BLEU improvement from 52.49 to 54.42 (RNN+Attention -&gt; RNN+Attention+NewCost), with an empirically optimal lambda ~0.035.",
            "domain_or_field": "Software engineering / machine learning for code",
            "reproducibility_impact": true,
            "uuid": "e684.4",
            "source_info": {
                "paper_title": "Deep API learning",
                "publication_date_yy_mm": "2016-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SWIM: synthesizing what I mean: code search and idiomatic snippet synthesis",
            "rating": 2
        },
        {
            "paper_title": "Bimodal modelling of source code and natural language",
            "rating": 2
        },
        {
            "paper_title": "MAPO: Mining API usages from open source repositories",
            "rating": 1
        },
        {
            "paper_title": "What makes APIs hard to learn? answers from developers",
            "rating": 2
        },
        {
            "paper_title": "A field study of API learning obstacles",
            "rating": 2
        }
    ],
    "cost": 0.0164985,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep API Learning</h1>
<p>Xiaodong Gu ${ }^{1}$, Hongyu Zhang ${ }^{2}$, Dongmei Zhang ${ }^{3}$, and Sunghun Kim ${ }^{1}$<br>${ }^{1}$ The Hong Kong University of Science and Technology, Hong Kong, China<br>guxiaodong1987@126.com hunkim@cse.ust.hk<br>${ }^{2}$ The University of Newcastle, Callaghan, Australia<br>hongyu.zhang@newcastle.edu.au<br>${ }^{3}$ Microsoft Research, Beijing, China<br>dongmeiz@microsoft.com</p>
<h4>Abstract</h4>
<p>Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bags-of-words and lack a deep understanding of the semantics of the query.</p>
<p>We propose DEEPAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bag-of-words assumption, it learns the sequence of words in a query and the sequence of associated APIs. DEEPAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches.</p>
<h2>CCS Concepts</h2>
<p>-Software and its engineering $\rightarrow$ Reusability;</p>
<h2>Keywords</h2>
<p>API, deep learning, RNN, API usage, code search</p>
<h2>1. INTRODUCTION</h2>
<p>To implement a certain functionality, for example, how to parse XML files, developers often reuse existing class libraries or frameworks by invoking the corresponding APIs. Obtaining which APIs to use, and their usage sequence (the method invocation sequence among the APIs) is very helpful in this regard [15, 46, 49]. For example, to "parse XML</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>files" using JDK library, the desired API usage sequence is as follows:</p>
<p>DocumentBuilderFactory.newInstance
DocumentBuilderFactory.newDocumentBuilder
DocumentBuilder.parse
Yet learning the APIs of an unfamiliar library or software framework can be a significant obstacle for developers [15, 46]. A large-scale software library such as .NET framework and JDK could contain hundreds or even thousands of APIs. In practice, usage patterns of API methods are often not well documented [46]. In a survey conducted by Microsoft in 2009, $67.6 \%$ respondents mentioned that there are obstacles caused by inadequate or absent resources for learning APIs [38]. Another field study found that a major challenge for API users is to discover the subset of the APIs that can help complete a task [39].</p>
<p>A common place to discover APIs and their usage sequence is from a search engine. Many developers search APIs from general web search engines such as Google and Bing. Developers can also perform a code search over an open source repository such as GitHub [3] and then utilize an API usage pattern miner [15, 46, 49] to obtain the appropriate API sequences.</p>
<p>However, search engines are often inefficient and inaccurate for programming tasks [42]. General web search engines are not designed to specifically support programming tasks. Developers need to manually examine many web pages to learn about the APIs and their usage sequence. Besides, most of search engines are based on keyword matching without considering the semantics of natural language queries [17]. It is often difficult to discover relevant code snippets and associated APIs.</p>
<p>Recently, Raghothaman et al. [36] proposed SWIM, which translates a natural language query to a list of possible APIs using a statistical word alignment model [12]. SWIM then uses the API list to retrieve relevant API sequences. However, the statistical word alignment model it utilizes is based on a bag-of-words assumption without considering the sequence of words and APIs. Therefore, it cannot recognize the deep semantics of a natural language query. For example, as described in their paper [36], it is difficult to distinguish the query convert int to string from convert string to int.</p>
<p>To address these issues, we propose DeepAPI, a novel, deep-learning based method that generates relevant API usage sequences given a natural language query. We formulate the API learning problem as a machine translation problem: given a natural language query $x=x_{1}, \ldots, x_{N}$ where $x_{i}$</p>
<p>is a keyword, we aim to translate it into an API sequence $y=y_{1}, \ldots, y_{T}$ where $y_{j}$ is an API. DEEPAPI shows a deep understanding of natural language queries in two aspects:</p>
<ul>
<li>First, instead of matching keywords, DEEPAPI learns the semantics of words by embedding them into a vector representation of context, so that semantically related words can be recognized.</li>
<li>Second, instead of word-to-word alignment, DEEPAPI learns the sequence of words in a natural language query and the sequence of associated APIs. It can distinguish the semantic differences between queries with different word sequences.</li>
</ul>
<p>DEEPAPI adapts a neural language model named RNN Encoder-Decoder [14]. Given a corpus of annotated API sequences, i.e., $\langle$ API sequence, annotation $\rangle$ pairs, DEEPAPI trains the language model that encodes each sequence of words (annotation) into a fixed-length context vector and decodes an API sequence based on the context vector. Then, in response to an API-related user query, it generates API sequences by consulting the neural language model.</p>
<p>To evaluate the effectiveness of DEEPAPI, we collect a corpus of 7 million annotated code snippets from GitHub. We select 10 thousand instances for testing and the rest for training the model. After 240 hours of training ( 1 million iterations), we measure the accuracy of DEEPAPI using BLEU score [35], a widely used accuracy measure for machine translation. Our results show that DEEPAPI achieves an average BLEU score of 54.42 , outperforming two related approaches, that is, code search with pattern mining (11.97) and SWIM [36] (19.90). We also ask DEEPAPI 30 APIrelated queries collected from real query logs and related work. On average, the rank of the first relevant result is 1.6 . $80 \%$ of the top 5 returned results and $78 \%$ of the top 10 returned results are deemed relevant. Our evaluation results confirm the effectiveness of DEEPAPI.</p>
<p>The main contributions of our work are as follows:</p>
<ul>
<li>To our knowledge, we are the first to adapt a deep learning technique to API learning. Our approach leads to more accurate API usage sequences as compared to the state-of-the-art techniques.</li>
<li>We develop DEEPAPI ${ }^{1}$, a tool that generates API usage sequences based on natural language queries. We empirically evaluate DEEPAPI's accuracy using a corpus of 7 million annotated Java code snippets.</li>
</ul>
<p>The rest of this paper is organized as follows. Section 2 describes the background of the deep learning based neural language model. Section 3 describes the application of the RNN Encoder-Decoder, a deep learning based neural language model, to API learning. Section 4 describes the detailed design of our approach. Section 5 presents the evaluation results. Section 6 discusses our work, followed by Section 7 that presents the related work. We conclude the paper in Section 8.</p>
<h2>2. DEEP LEARNING FOR SEQUENCE GENERATION</h2>
<p>Our work adopts and augments recent advanced techniques from deep learning and neural machine translation [8,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) RNN Structure
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) RNNLM for sentence estimation</p>
<p>Figure 1: Illustration of the RNN Language Model
14, 43]. These techniques are on the basis of Sequence-toSequence Learning [43], namely, generating a sequence (usually a natural language sentence) conditioned on another sequence. In this section, we discuss the background of these techniques.</p>
<h3>2.1 Language Model</h3>
<p>It has been observed that software has naturalness [16]. Statistical language models have been adapted to many software engineering tasks [26] such as learning natural code conventions [5], code suggestion [44], and code completion [37]. These techniques regard source code as a special language and analyze it using statistical NLP techniques.</p>
<p>The language model is a probabilistic model of how to generate sentences in a language. It tells how likely a sentence would occur in a language. For a sentence $y$, where $y=$ $\left(y_{1}, \ldots, y_{T}\right)$ is a sequence of words, the language model aims to estimate the joint probability of its words $\operatorname{Pr}\left(y_{1}, \ldots, y_{T}\right)$. Since</p>
<p>$$
\operatorname{Pr}\left(y_{1}, \ldots, y_{T}\right)=\prod_{t=1}^{T} \operatorname{Pr}\left(y_{t} \mid y_{1}, \ldots, y_{t-1}\right)
$$</p>
<p>it is equivalent to estimate the probability of each word in $y$ given its previous words, namely, what a word might be given its predecessing words.</p>
<p>As $\operatorname{Pr}\left(y_{t} \mid y_{1}, \ldots, y_{t-1}\right)$ is difficult to estimate, most applications use "n-gram models" [11] to approximate it, that is,</p>
<p>$$
\operatorname{Pr}\left(y_{t} \mid y_{1}, \ldots, y_{t-1}\right) \subset \operatorname{Pr}\left(y_{t} \mid y_{t-n+1}, \ldots, y_{t-1}\right)
$$</p>
<p>where an n-gram is defined as $n$ consecutive words. This approximation means that the next word $y_{t}$ is conditioned only on the previous $n-1$ words.</p>
<h3>2.2 Neural Language Model</h3>
<p>The neural language model is a language model based on neural networks. Unlike the n-gram model which predicts a word based on a fixed number of predecessing words, a neural language model can predict a word by predecessing words with longer distances. It is also powerful to learn distributed representations of words, i.e, word vectors [30]. We adopt RNNLM [29], a language model based on a deep neural network, that is, Recurrent Neural Network (RNN) [29]. Figure 1a shows the basic structure of an RNN. The neural network includes three layers, that is, an input layer which maps each word to a vector, a recurrent hidden layer which recurrently computes and updates a hidden state after reading each word, and an output layer which estimates the probabilities of the following word given the current hidden state.</p>
<p>Figure 1b shows an example of how RNNLM estimates the probability of a sentence, that is, the probability of each word given predecessing words (Equation 1). To facilitate understanding, we expand the recurrent hidden layer for</p>
<p>each individual time step. The RNNLM reads the words in the sentence one by one, and predicts the possible following word at each time step. At step $t$, it estimates the probability of the following word $p\left(y_{t+1} \mid y_{1}, \ldots, y_{t}\right)$ by three steps: First, the current word $y_{t}$ is mapped to a vector $\boldsymbol{y}_{t}$ by the input layer:</p>
<p>$$
\boldsymbol{y}<em t="t">{t}=\operatorname{input}\left(y</em>\right)
$$</p>
<p>Then, it generates the hidden state (values in the hidden layer) $\boldsymbol{h}<em t-1="t-1">{t}$ at time $t$ according to the previous hidden state $\boldsymbol{h}</em>$ :}$ and the current input $\boldsymbol{y}_{t</p>
<p>$$
\boldsymbol{h}<em t-1="t-1">{t}=f\left(\boldsymbol{h}</em>\right)
$$}, \boldsymbol{y}_{t</p>
<p>Finally, the $\operatorname{Pr}\left(y_{t+1} \mid y_{1}, \ldots, y_{t}\right)$ is predicted according to the current hidden state $\boldsymbol{h}_{t}$ :</p>
<p>$$
\operatorname{Pr}\left(y_{t+1} \mid y_{1}, \ldots, y_{t}\right)=g\left(\boldsymbol{h}_{t}\right)
$$</p>
<p>During training, the network parameters are learned from data to minimize the error rate of the estimated $y$ (details are in [29]).</p>
<h3>2.3 RNN Encoder-Decoder Model</h3>
<p>The RNN Encoder-Decoder [14] is an extension of the basic neural language model (RNNLM). It assumes that there are two languages, a source language and a target language. It generates a sentence $y$ of the target language given a sentence $x$ of the source language. To do so, it first summarizes the sequence of source words $x_{1}, \ldots, x_{T_{x}}$ into a fixed-length context vector:</p>
<p>$$
\boldsymbol{h}<em t-1="t-1">{t}=f\left(\boldsymbol{h}</em>\right)
$$}, \boldsymbol{x}_{t</p>
<p>and</p>
<p>$$
\boldsymbol{c}=\boldsymbol{h}<em x="x">{T</em>
$$}</p>
<p>where $f$ is a non-linear function that maps a word of source language $\boldsymbol{x}<em t="t">{t}$ into a hidden state $\boldsymbol{h}</em>}$ at time $t$ by considering the previous hidden state $\boldsymbol{h<em T__x="T_{x">{t-1}$. The last hidden state $\boldsymbol{h}</em>$.}}$ is selected as a context vector $\boldsymbol{c</p>
<p>Then, it generates the target sentence $y$ by sequentially predicting a word $y_{t}$ conditioned on the source context $\boldsymbol{c}$ as well as previous words $y_{1}, \ldots, y_{t-1}$ :</p>
<p>$$
\operatorname{Pr}(y)=\prod_{t=1}^{T} p\left(y_{t} \mid y_{1}, \ldots, y_{t-1}, \boldsymbol{c}\right)
$$</p>
<p>The above procedures, i.e., $f$ and $p$ can be represented using two recurrent neural networks respectively, an encoder RNN which learns to transform a variable length source sequence into a fixed-length context vector, and a decoder RNN which learns a target language model and generates a sequence conditioned on the context vector. The encoder RNN reads the source words one by one. At each time stamp $t$, it reads one word, then updates and records a hidden state. When reading a word, it computes the current hidden state $\boldsymbol{h}<em t="t">{t}$ using the current word $\boldsymbol{x}</em>}$ and the previous hidden state $\boldsymbol{h<em T__x="T_{x">{t-1}$. When it finishes reading the end-ofsequence word $<E O S>$, it selects the last hidden state $\boldsymbol{h}</em>$. The decoder RNN then sequentially generates the target words by consulting the context vector (Equation 8). It first sets the context vector as an initial hidden state of the decoder RNN. At each time stamp $t$, it generates one word based on the current hidden state and the context vector. Then, it updates the hidden state using the generated word (Equation 6). It stops when generating the end-of-sentence word $\langle E O S\rangle$.}}$ as a context vector $\boldsymbol{c</p>
<p>The RNN Encoder-Decoder model can then be trained to maximize the conditional log-likelihood [14], namely, minimize the following objective function:</p>
<p>$$
\mathcal{L}(\theta)=\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \operatorname{cost}_{i t}
$$</p>
<p>where $N$ is the total number of training instances, while $T$ is the length of each target sequence. $\operatorname{cost}_{i t}$ is the cost function for the $t$-th target word in instance $i$. It is defined as the negative log likelihood:</p>
<p>$$
\operatorname{cost}<em _theta="\theta">{i t}=-\log p</em>\right)
$$}\left(y_{i t} \mid x_{i</p>
<p>where $\theta$ denotes model parameters such as weights in the neural network, while $p_{\theta}\left(y_{i t} \mid x_{i}\right)$ (derived from Equation 6 to 8) denotes the likelihood of generating the $t$-th target word given the source sequence $x_{i}$ in instance $i$ according to the model parameters $\theta$. Through optimizing the objective function using optimization algorithms such as gradient descendant, the optimum $\theta$ value can be estimated.</p>
<h2>3. RNN ENCODER-DECODER MODEL FOR API LEARNING</h2>
<h3>3.1 Application of RNN Encoder-Decoder to API Learning</h3>
<p>Now we present the idea of applying the RNN EncoderDecoder model to API learning. We regard user queries as the source language and API sequences as the target language. Figure 2 shows an example of the RNN EncoderDecoder model for translating a sequence of English words read text file to a sequence of APIs. The encoder RNN reads the source words one by one. When it reads the first word read, it embeds the word into vector $\boldsymbol{x}<em 1="1">{1}$ and computes the current hidden state $\boldsymbol{h}</em>}$ using $\boldsymbol{x<em 2="2">{1}$. Then, it reads the second word text, embeds it into $\boldsymbol{x}</em>}$, and updates the hidden state $\boldsymbol{h<em 2="2">{1}$ to $\boldsymbol{h}</em>}$ using $\boldsymbol{x<em 3="3">{2}$. The procedure continues until the encoder reads the last word file and gets the final state $\boldsymbol{h}</em>$.}$. The final state $\boldsymbol{h}_{3}$ is selected as a context vector $\boldsymbol{c</p>
<p>The decoder RNN tries to generate APIs sequentially using the context vector $\boldsymbol{c}$. It first generates $&lt;$ START $&gt;$ as the first word $\boldsymbol{y}<em 1="1">{0}$. Then, it computes a hidden state $\boldsymbol{h}</em>}$ based on the context vector $\boldsymbol{c}$ and $\boldsymbol{y<em 1="1">{0}$, and predicts the first API FileReader.new according to $\boldsymbol{h}</em>}$. It then computes the next hidden state $\boldsymbol{h<em 1="1">{2}$ according to the previous word vector $\boldsymbol{y}</em>$. This procedure continues until it predicts the end-of-sequence word $\langle E O S\rangle$.}$, the context vector $\boldsymbol{c}$, and predicts the second API BufferedReader.new according to $\boldsymbol{h}_{2</p>
<p>Different parts of a query could have different importance to an API in the target sequence. For example, considering the query save file in default encoding and the target API sequence File.new FileOutputStream.new FileOutputStream.write FileOutputStream.close, the word file is more important than default to the target API File.new. In our work, we adopt the attention-based RNN Encoder-Decoder model [8], which is a recent model that selects the important parts from the input sequence for each target word. Instead of generating target words using the same context vector $\boldsymbol{c}$ $\left(\boldsymbol{c}=\boldsymbol{h}<em x="x">{T</em>}}\right)$, an attention model defines individual $\boldsymbol{c<em j="j">{j}$ 's for each target word $y</em>}$ as a weighted sum of all historical hidden states $\boldsymbol{h<em T__x="T_{x">{1}, \ldots, \boldsymbol{h}</em>$. That is,}</p>
<p>$$
\boldsymbol{c}<em t="1">{j}=\sum</em>
$$}^{T_{x}} \alpha_{j t} \boldsymbol{h}_{t</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: An Illustration of the RNN Encoder-Decoder Model for API learning</p>
<p>where each $\alpha_{jt}$ is a weight between the hidden state $\boldsymbol{h}<em j="j">{t}$ and the target word $y</em>$, while $\alpha$ can be modeled using another neural network and learned during training (see details in [8]).</p>
<h3>3.2 Enhancing RNN Encoder-Decoder Model with API importance</h3>
<p>The basic RNN Encoder-Decoder model does not consider the importance of individual words in the target sequence either. In the context of API learning, different APIs have different importance for a programming task [27]. For example, the API Logger.log is widely used in many code snippets. However, it cannot help understand the key procedures of a programming task. Such ubiquitous APIs would be "weakened" during sequence generation.</p>
<p>We augment the RNN Encoder-Decoder model to predict API sequences by considering the individual importance of APIs. We define IDF-based weighting to measure API importance as follows:</p>
<p>$$w_{idf}(y_t) = \log(\frac{N}{n_{y_t}}) \tag{12}$$</p>
<p>where $N$ is the total number of API sequences in the training set and $n_{y_t}$ denotes the number of sequences where the API $y_t$ appears in the training set. Using IDF, the APIs that occur ubiquitously have the lower weights while the less common APIs have the higher weights.</p>
<p>We use API weight as a penalty term to the cost function (Equation 10). The new cost function of the RNN Encoder-Decoder model is:</p>
<p>$$cost_{it} = -\log p_{\theta}(y_{it}|x_i) - \lambda w_{idf}(y_t) \tag{13}$$</p>
<p>where $\lambda$ denotes the penalty of IDF weight and is set empirically.</p>
<h2>4. DeepAPI: Deep Learning for API Sequence Generation</h2>
<p>In this section, we describe DeepAPI, a deep-learning based method that generates relevant API usage sequences given an API-related natural language query. DeepAPI adapts the RNN Encoder-Decoder model for the task of API learning. Figure 3 shows the overall architecture of DeepAPI. It includes an offline training stage and an online translation stage. In the training stage, we prepare a large-scale corpus of annotated API sequences (API sequences with corresponding natural language annotations). The annotated API sequences are used to train a deep learning model, i.e., the RNN Encoder-Decoder language model as described in</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: The Overall Workflow of DeepAPI</p>
<p>Section 3. Given an API-related user query, a ranked list of API sequences can be generated by the language model.</p>
<p>In theory our approach could generate APIs written in any programming languages. In this paper we limit our scope to the JDK library. The details of our method are explained in the following sections.</p>
<h3>4.1 Gathering a Large-scale API Sequence to Annotation Corpus</h3>
<p>We first construct a large-scale database that contains pairs of API sequences and natural language annotations for training the RNN Encoder-Decoder model. We download Java projects from GitHub [2] created from 2008 to 2014. To remove toy or experimental programs, we only select the projects with at least one star. In total, we collected 442,928 Java projects from GitHub. We use the last snapshot of each project. Having collected the code corpus, we extract (API sequence, annotation) pairs as follows:</p>
<h4>4.1.1 Extracting API Usage Sequences</h4>
<p>To extract API usage sequences from the code corpus, we parse source code files into ASTs (Abstract Syntax Trees) using Eclipse's JDT compiler [1]. The extraction algorithm starts from the dependency analysis of a whole project repository. We analyze all classes, recording field declarations together with their type bindings. We replace all object types with their real class types. Then, we extract API sequence from individual methods by traversing the AST of the method body:</p>
<ul>
<li>For each constructor invocation new $C()$, we append the API C.new to the API sequence.</li>
<li>For each method call o.m() where o is an instance of a JDK class $C$, we append the API C.m to the API sequence.</li>
<li>For a method call passed as a parameter, we append the method before the calling method. For example,</li>
</ul>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: An example of extracting API sequence and its annotation from a Java method IOUtils.copyLarge ${ }^{7}$
$o_{1} . m_{1}\left(o_{2} . m_{2}(), o_{3} . m_{3}(\right))$, we produce a sequence $C_{2} . m_{2}$ $C_{3} . m_{3}-C_{1} . m_{1}$, where $C_{i}$ is the JDK class of instance $o_{i}$.</p>
<ul>
<li>For a sequence of statements $s t m t_{1} ; s t m t_{2} ; \ldots ; s t m t_{t}$, we extract the API sequence $s_{i}$ from each statement $s t m t_{i}$, concatenate them, and produce the API sequence $s_{1}-s_{2}-\ldots-s_{t}$.</li>
<li>For conditional statements such as if $\left(s t m t_{1}\right)\left{s t m t_{2}\right.$; } else $\left{s t m t_{3} ;\right}$, we create a sequence from all possible branches, that is, $s_{1}-s_{2}-s_{3}$, where $s_{i}$ is the API sequence extracted from the statement $s t m t_{i}$.</li>
<li>For loop statements such as while $\left(s t m t_{1}\right)\left{s t m t_{2} ;\right}$, we produce a sequence $s_{1}-s_{2}$, where $s_{1}$ and $s_{2}$ are API sequences extracted from the statement $s t m t_{1}$ and $s t m t_{2}$, respectively.</li>
</ul>
<h3>4.1.2 Extracting Annotations</h3>
<p>To annotate the obtained API sequences with natural language descriptions, we extract method-level code summaries, specifically, the first sentence of a documentation comment ${ }^{8}$ for a method. According to the Javadoc guidance ${ }^{3}$, the first sentence is used as a short summary of a method. Figure 4 shows an example of documentation comment for a Java method IOUtils.copyLarge ${ }^{4}$ in the Apache commons-io library.</p>
<p>We use the Eclipse JDT compiler for the extraction. For each method, we traverse its AST and extract the JavaDoc Comment part. We ignore methods without JavaDoc comments. Then, we select the first sentence of the comment as the annotation. We exclude irregular annotations such as those starting with "TODO: Auto-generated method stub", "NOTE:", and "test". We also filter out non-words and words within brackets in the annotations.</p>
<p>Finally, we obtain a database consisting of 7,519,907 API sequence, annotation $\rangle$ pairs.</p>
<h3>4.2 Training Encoder-Decoder Language Model</h3>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: An illustration of beam search (beam width $=2$ )</p>
<p>As described in Section 3, we adapt the attention-based RNN Encoder-Decoder model for API learning. The RNN has various implementations, we use GRU [14] which is a state-of-the-art RNN and performs well in many tasks [8, 14]. We construct the model as follows: we use two RNNs for the encoder - a forward RNN that directly encodes the source sentence and a backward RNN that encodes the reversed source sentence. Their output context vectors are concatenated to the decoder, which is also an RNN. All RNNs have 1000 hidden units. We set the dimension of word embedding to 120. We discuss the details of parameter tuning in Section 5.4.</p>
<p>All models are trained using the minibatch Adadelta [51], which automatically adjusts the learning rate. We set the batch size (i.e., number of instances per batch) as 200. For training the neural networks, we limit the source and target vocabulary to the top 10,000 words that are most frequently used in API sequences and annotations.</p>
<p>For implementation, we use GroundHog [8, 14], an opensource deep learning framework. We train our models in a server with one Nvidia K20 GPU. The training lasts $\sim 240$ hours with 1 million iterations.</p>
<h3>4.3 Translation</h3>
<p>So far we have discussed the training of a neural language model, which outputs the most likely API sequence given a natural language query. However, an API could have multiple usages. To obtain a ranked list of possible API sequences for user selection, we need to generate more API sequences according to their probability at each step.</p>
<p>DEEP API uses Beam Search [21], a heuristic search strategy, to find API sequences that have the least cost value(computed using Equation 13) given by the language model. Beam search searches APIs produced at each step one by one. At each time step, it selects $n$ APIs from all branches with the least cost values, where $n$ is the beam-width. It then prunes off the remaining branches and continues selecting the possible APIs that follow on until it meets the end-ofsequence symbol. Figure 5 shows an example of a beam search (beam-width=2) for generating an API sequence for the query "read text file". First, 'START' is selected as the first API in the generated sequence. Then, it estimates the probabilities of all possible APIs that follow on according to the language model. It computes their cost values according to Equation 13, and selects File.new and FileInputStream.new which have the least cost values of 6 and 8 , respectively. Then, it ignores branches of other APIs and continue estimating possible APIs after File.new and FileIn-</p>
<p>putStream.new. Once it selects an end-of-sequence symbol as the next API, it stops that branch and the branch is selected as a generated sequence.</p>
<p>Finally, DeepAPI produces $n$ API sequences for each query where $n$ is the beam-width. We rank the generated API sequences according to their average cost values during the beam search procedure.</p>
<h2>5. EVALUATION</h2>
<p>We evaluate the effectiveness of DeepAPI by measuring its accuracy on API sequence generation. Specifically, our evaluation addresses the following research questions:</p>
<ul>
<li>RQ1: How accurate is DeepAPI for generating API usage sequences?</li>
<li>RQ2: How accurate is DeepAPI under different parameter settings?</li>
<li>RQ3: Do the enhanced RNN Encoder-Decoder models improve the accuracy of DeepAPI?</li>
</ul>
<h3>5.1 Accuracy Measure</h3>
<h3>5.1.1 Intrinsic Measure - BLEU</h3>
<p>We use the BLEU score [35] to measure the accuracy of generated API sequences. The BLEU score measures how close a candidate sequence is to a reference sequence (usually a human written sequence). It is a widely used accuracy measure for machine translation in the machine learning and natural language processing literature [8, 14, 43]. In our API learning context, we regard a generated API sequence given a query as a candidate, and a human-written API sequence (extracted from code) for the same query as a reference. We use BLEU to measure how close the generated API sequence is to a human-written API sequence.</p>
<p>Generally, BLEU measures the hits of n-grams of a candidate sequence to the reference. It is computed as:</p>
<p>$$
B L E U=B P \cdot \exp \left(\sum_{n=1}^{N} w_{n} \log p_{n}\right)
$$</p>
<p>where each $p_{n}$ is the precision of n-grams, that is, the ratio of length $n$ subsequences in the candidate that are also in the reference:</p>
<p>$$
p_{n}=\frac{# \text { n-grams appear in the reference }+1}{# \text { n-grams of candidate }+1} \quad \text { for } n=1, \ldots, N
$$</p>
<p>where $N$ is the maximum number of grams we consider. We set $N$ to 4 , which is a common practice in the Machine Learning literature [43]. Each $w_{n}$ is the weight of each $p_{n}$. A common practice is to set $w_{n}=\frac{1}{N} . B P$ is a brevity penalty which penalties overly short candidates (that may have a higher n-gram precision).</p>
<p>$$
B P= \begin{cases}1 &amp; \text { if } c&gt;r \ e^{(1-r / c)} &amp; \text { if } c \leq r\end{cases}
$$</p>
<p>where $r$ is the length of the reference sequence, and $c$ is the length of the candidate sequence.</p>
<p>We now give an example of BLEU calculation. For a candidate API sequence ${\mathrm{a}-\mathrm{c}-\mathrm{d}-\mathrm{b}}$ and a reference API sequence ${\mathrm{a}-\mathrm{b}-\mathrm{c}-\mathrm{d}-\mathrm{e}}$, their 1-grams are ${\mathrm{a}, \mathrm{b}, \mathrm{c}, \mathrm{d}}$ and ${\mathrm{a}, \mathrm{b}, \mathrm{c}, \mathrm{d}, \mathrm{e}}$. All four 1-grams of the candidate are hit in the reference. Then, $p_{1}=\frac{4+1}{4+1}=1$. Their 2-grams are ${\mathrm{ac}, \mathrm{cd}, \mathrm{db}}$ and ${\mathrm{ab}, \mathrm{bc}, \mathrm{cd}, \mathrm{de}}$, respectively. Then, $p_{2}=\frac{1+1}{3+1}=\frac{1}{2}$ as only $c d$ is matched.
$p_{3}=\frac{0+1}{2+1}=\frac{1}{3}$ and $p_{4}=\frac{0+1}{1+1}=\frac{1}{2}$ as no 3 -gram nor 4 gram is matched. As their lengths are 4 and 5 respectively, $B P=e^{(1-5 / 4)}=0.78$. The final BLEU is $0.78 \times \exp \left(\frac{1}{4} \times\right.$ $\log 1+\frac{1}{4} \times \log \frac{1}{2}+\frac{1}{4} \times \log \frac{1}{2}+\frac{1}{4} \times \log \frac{1}{2})=41.91 \%$</p>
<p>BLEU is usually expressed as a percentage value between 0 and 100. The higher the BLEU, the closer the candidate sequence is to the reference. If the candidate sequence is completely equal to the reference, the BLEU becomes $100 \%$.</p>
<h3>5.1.2 Extrinsic Measures - FRank and Relevancy Ratio</h3>
<p>We also use two measures for human evaluation. They are FRank and relevancy ratio [36]. FRank is the rank of the first relevant result in the result list [36]. It is important as most users scan the results from top to bottom.</p>
<p>The relevancy ratio is defined as the precision of relevant results in a number of results [36].</p>
<p>$$
\text { relevancy ratio }=\frac{# \text { relevant results }}{# \text { all selected results }}
$$</p>
<p>The value of both measures ranges from 0 to 100 . The higher the better.</p>
<h3>5.2 Comparison Methods</h3>
<p>We compare the accuracy of our approach with that of two state-of-the-art API learning approaches, namely Code Search with Pattern Mining [24, 46] and SWIM [36].</p>
<h3>5.2.1 Code Search with Pattern Mining</h3>
<p>To obtain relevant API sequences for a given a query, one can perform code search over the code corpus using information retrieval techniques [20, 24, 25, 28], and then utilize an API usage pattern miner [15, 46, 49] to identify an appropriate API sequences in the returned code snippets.</p>
<p>We compare DeepAPI with this approach. We use Lucene [4] to perform a code search for a given natural language query and UP-Miner [46] to perform API usage pattern mining. Lucene is an open-source information retrieval engine, which has been integrated into many code search engines [24, 36]. Much the same as these code search engines do, we treat source code as plain text documents and use Lucene to build source code index and perform text retrieval. UP-Miner [46] is a pattern mining tool, which produces API sequence patterns from code snippets. It first clusters API sequences extracted from code snippets, and then identifies frequent patterns from the clustered sequences. Finally, it clusters the frequent patterns to reduce redundancy. We use UPMiner to mine API usage sequences from the code snippets returned by the Lucene-based code search.</p>
<p>In this experiment, we use the same code corpus as used for evaluating DEEPAPI, and compare the BLEU scores with those of DEEPAPI.</p>
<h3>5.2.2 SWIM</h3>
<p>SWIM [36] is a recently proposed code synthesis tool, which also supports API sequence search based on a natural language query. Given a query, it expands the query keywords to a list of relevant APIs using a statistical word alignment model [12]. With the list of possible APIs, SWIM searches related API sequences using Lucene [4]. Finally, it synthesizes code snippets based on the API sequences. As code synthesis is beyond our scope, we only compare DeEPAPI with the API learning component of SWIM, that is, from a natural language query to an API sequence. In their</p>
<p>Table 1: BLEU scores of DeepAPI and related techniques (\%)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tool</th>
<th style="text-align: center;">Top1</th>
<th style="text-align: center;">Top5</th>
<th style="text-align: center;">Top10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lucene+UP-Miner</td>
<td style="text-align: center;">11.97</td>
<td style="text-align: center;">24.08</td>
<td style="text-align: center;">29.64</td>
</tr>
<tr>
<td style="text-align: left;">SWIM</td>
<td style="text-align: center;">19.90</td>
<td style="text-align: center;">25.98</td>
<td style="text-align: center;">28.85</td>
</tr>
<tr>
<td style="text-align: left;">DeEPAPI</td>
<td style="text-align: center;">54.42</td>
<td style="text-align: center;">64.89</td>
<td style="text-align: center;">67.83</td>
</tr>
</tbody>
</table>
<p>experiments, SWIM uses Bing clickthrough data to build the model. In our experiment, for fair comparison, we evaluate SWIM using the same dataset as we did for evaluating DEEPAPI. That is, we train the word alignment model and build API index on the training set, and evaluate the search results on the test set.</p>
<h3>5.3 Accuracy (RQ1)</h3>
<h3>5.3.1 Intrinsic Evaluation</h3>
<p>Evaluation Setup: We first evaluate the accuracy of generated API sequences using the BLEU score. As described in Section 4.1, we collect a database comprising 7,519,907 API sequence, annotation pairs. We split them into a test set and a training set. The test set comprises 10,000 pairs while the training set consists of the remaining instances. We train all models using the training set and compute the BLEU scores in the test set. We calculate the highest BLEU score for each test instance in the top $n$ results.</p>
<p>Results: Table 1 shows the BLEU scores of DeEPAPI, SWIM, and Code Search (Lucene+UP-Miner). Each column shows the average BLEU score for a method. As the results indicate, DeEPAPI produces API sequences with higher accuracy. When only the top 1 result is examined, the BLEU score achieved by DeEPAPI is 54.42 , which is greater than that of SWIM (BLEU=19.90) and Code Search (BLEU=11.97). The improvement over SWIM is $173 \%$ and the improvement over Code Search is $355 \%$. Similar results are obtained when the top 5 and 10 results are examined. The evaluation results confirm the effectiveness of the deep learning method used by DEEPAPI.</p>
<h3>5.3.2 Extrinsic Evaluation</h3>
<p>To further evaluate the relevancy of the results returned by DeEPAPI, we selected 17 queries used in [36]. These queries have corresponding Java APIs and are commonly occurring queries in the Bing search log [36]. To demonstrate the advantages of DeEPAPI, we also designed 13 longer queries and queries with semantically similar words. In total, 30 queries are used. These queries do not appear in the training set. Table 2 lists the queries.</p>
<p>For each query, the top 10 returned results by DEEPAPI and SWIM are manually examined. To reduce labeling bias, two developers separately label the relevancy of each resulting sequence and combine their labels. For inconsistent labels, they discuss and relabel them until a settlement is reached. The FRank and the relevancy ratios for the top 5 and top 10 returned results are then computed. To test the statistical significance, we apply the Wilcoxon signed-rank test $(p&lt;0.05)$ for all results of both approaches. A resulting p-value less than 0.05 indicates that the differences between DEEPAPI and SWIM are statistically significant.</p>
<p>Table 2 shows the accuracy of both DeEPAPI and SWIM. The symbol ' - ' means no relevant result has been returned within the top 10 results. The results show that DEEPAPI
is able to produce mostly relevant results. It achieves an average FRank of 1.6 , an average top 5 accuracy of $80 \%$, and an average top 10 accuracy of $78 \%$. Furthermore, DEEPAPI produces more relevant API sequences than SWIM, whose average top 5 and top 10 accuracy is $44 \%$ and $47 \%$, respectively. For some queries, SWIM failed to obtain relevant results in the top 10 returned results. We conservatively treat the FRank as 11 for these unsuccessful queries. Then, the FRank achieved by SWIM is greater than 4.0, which is much higher than what DEEPAPI achieved (1.60). The p-values for the three comparisons are $0.01,0.02$ and 0.01 , respectively, indicating statistical significance of the improvement of DEEPAPI over SWIM. In summary, the evaluation results confirm the effectiveness of DEEPAPI.</p>
<p>Table 2 also shows examples of generated sequences by DEEPAPI. We can see that DEEPAPI is able to distinguish word sequences. For example, DEEPAPI successfully distinguishes the query convert int to string from convert string to int. Another successful example is the query expansion. For example, the query save an image to a file and write an image to a file return similar results. DEEPAPI also performs well in longer queries such as copy a file and save it to your destination path and play the audio clip at the specified absolute URL. Such queries comprise many keywords, and DEEPAPI can successfully recognize the semantics.</p>
<p>We also manually check the results returned by SWIM. We find SWIM may return partially matched sequences. For example, for the query generate md5 hash code, SWIM returns many results containing only Object.hashCode, which simply returns a hash code. SWIM also returns project specific results without fully understanding the query. For example, for the query "test file exists", SWIM returns "File.new, File.exists, File.getName, File.new, File.delete, FileInputStream.new, FileInputStream.read, ...", which is not only related to file existence test, but also to other project-specific tasks. Such project specific results can also be seen for the query create file. Compared with DeepAPI, SWIM performs worse in long queries. For example, SWIM performs worse in the query copy a file and save it to your destination path than in the query copy file. This is because long queries often have multiple objectives, which cannot be understood by SWIM.</p>
<p>Still, DEEPAPI could return inaccurate or partial results. For example, for the query parse xml, it returns related APIs InputSource.new, DocumentBuilder.parse. But it misses the APIs about how DocumentBuilder is created (DocumentBuilderFactory.newDocumentBuilder). The reason could be that an API sequence may be called in an inter-procedural manner. When preparing the training set, we only consider API sequences within one method. The API DocumentBuilderFactory.newDocumentBuilder could be called in another method and is passed as a parameter. This causes incomplete sequences in the training set. In the future, we will perform more accurate program analysis and create a better training set.</p>
<h3>5.4 Accuracy Under Different Parameter Settings (RQ2)</h3>
<p>We also qualitatively compare the accuracy of DEEPAPI in different parameter settings. We analyze two parameters, that is, the dimension of word embedding and the number of hidden units. We vary the values of these two parameters and evaluate their impact on the BLEU scores.</p>
<p>Table 2: Queries for Human Evaluation (FR: FRank, RR5: top 5 relevancy ratio, RR10: top 10 relevancy ratio)</p>
<table>
<thead>
<tr>
<th>query (How to...)</th>
<th>SWIM</th>
<th></th>
<th></th>
<th>DeepAPI</th>
<th></th>
<th></th>
<th>Generated API sequence by DeepAPI</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>FR</td>
<td>RR5</td>
<td>RR10</td>
<td>FR</td>
<td>RR5</td>
<td>RR10</td>
<td></td>
</tr>
<tr>
<td>convert int to string</td>
<td>8</td>
<td>0</td>
<td>10</td>
<td>2</td>
<td>40</td>
<td>90</td>
<td>Integer.toString</td>
</tr>
<tr>
<td>convert string to int</td>
<td>1</td>
<td>80</td>
<td>80</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>Integer.parseInt String.toCharArray Character.digit</td>
</tr>
<tr>
<td>append strings</td>
<td>3</td>
<td>60</td>
<td>80</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>StringBuilder.append StringBuilder.toString</td>
</tr>
<tr>
<td>get current time</td>
<td>1</td>
<td>80</td>
<td>80</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>System.currentTimeMillis Timestamp.new</td>
</tr>
<tr>
<td>parse datetime from string</td>
<td>9</td>
<td>0</td>
<td>10</td>
<td>1</td>
<td>100</td>
<td>80</td>
<td>SimpleDateFormat.new SimpleDateFormat.parse</td>
</tr>
<tr>
<td>test file exists</td>
<td>-</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>File.new File.exists</td>
</tr>
<tr>
<td>open a url</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>URL.new URL.openConnection</td>
</tr>
<tr>
<td>open file dialog</td>
<td>-</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>100</td>
<td>80</td>
<td>JFileChooser.new JFileChooser.showOpenDialog JFileChooser.getSelectedFile</td>
</tr>
<tr>
<td>get files in folder</td>
<td>2</td>
<td>40</td>
<td>20</td>
<td>3</td>
<td>40</td>
<td>50</td>
<td>File.new File.list File.new File.isDirectory</td>
</tr>
<tr>
<td>match regular expressions</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>1</td>
<td>80</td>
<td>90</td>
<td>Pattern.compile Pattern.matcher Matcher.group</td>
</tr>
<tr>
<td>generate md5 hash code</td>
<td>1</td>
<td>60</td>
<td>40</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>MessageDigest.getInstance MessageDigest.update MessageDigest.digest</td>
</tr>
<tr>
<td>generate random number</td>
<td>7</td>
<td>0</td>
<td>10</td>
<td>1</td>
<td>100</td>
<td>70</td>
<td>Random.new Random.nextInt</td>
</tr>
<tr>
<td>round a decimal value</td>
<td>-</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>Math.floor Math.pow Math.round</td>
</tr>
<tr>
<td>execute sql statement</td>
<td>2</td>
<td>80</td>
<td>80</td>
<td>1</td>
<td>80</td>
<td>60</td>
<td>Connection.prepareStatement PreparedStatement.execute PreparedState-</td>
</tr>
<tr>
<td>connect to database</td>
<td>7</td>
<td>0</td>
<td>20</td>
<td>1</td>
<td>100</td>
<td>90</td>
<td>Properties.getProperty Class.forName DriverManager.getConnection</td>
</tr>
<tr>
<td>create file</td>
<td>10</td>
<td>0</td>
<td>10</td>
<td>3</td>
<td>40</td>
<td>20</td>
<td>File.exists File.createNewFile</td>
</tr>
<tr>
<td>copy file</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>2</td>
<td>20</td>
<td>10</td>
<td>FileInputStream.new FileOutputStream.new FileInputStream.read FileOutput-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>copy a file and save it to</td>
<td>1</td>
<td>20</td>
<td>50</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td></td>
</tr>
<tr>
<td>-your destination path</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>FileInputStream.new FileOutputStream.new FileInputStream.getChannel File-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>OutputStream.getChannel FileChannel.size FileChannel.transferTo FileInput-</td>
</tr>
<tr>
<td>delete files and folders in a</td>
<td>1</td>
<td>100</td>
<td>90</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>Stream.close FileOutputStream.close FileChannel.close FileChannel.close</td>
</tr>
<tr>
<td>-directory</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>File.isDirectory File.list File.new File.delete</td>
</tr>
<tr>
<td>reverse a string</td>
<td>3</td>
<td>20</td>
<td>10</td>
<td>2</td>
<td>60</td>
<td>70</td>
<td>StringBuffer.new StringBuffer.reverse</td>
</tr>
<tr>
<td>create socket</td>
<td>-</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>60</td>
<td>80</td>
<td>ServerSocket.new ServerSocket.bind</td>
</tr>
<tr>
<td>rename a file</td>
<td>-</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>File.renameTo File.delete</td>
</tr>
<tr>
<td>download file from url</td>
<td>2</td>
<td>60</td>
<td>80</td>
<td>1</td>
<td>100</td>
<td>80</td>
<td>URL.new URL.openConnection URLConnection.getInputStream BufferedIn-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>serialize an object</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>3</td>
<td>60</td>
<td>70</td>
<td>ObjectOutputStream.new ObjectOutputStream.writeObject ObjectOutput-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Stream.close</td>
</tr>
<tr>
<td>read binary file</td>
<td>4</td>
<td>40</td>
<td>70</td>
<td>1</td>
<td>100</td>
<td>80</td>
<td>DataInputStream.new DataInputStream.readInt DataInputStream.close</td>
</tr>
<tr>
<td>save an image to a file</td>
<td>1</td>
<td>20</td>
<td>10</td>
<td>1</td>
<td>80</td>
<td>80</td>
<td>File.new ImageIO.write</td>
</tr>
<tr>
<td>write an image to a file</td>
<td>1</td>
<td>20</td>
<td>10</td>
<td>1</td>
<td>100</td>
<td>90</td>
<td>File.new ImageIO.write</td>
</tr>
<tr>
<td>parse xml</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>1</td>
<td>80</td>
<td>60</td>
<td>InputSource.new DocumentBuilder.parse</td>
</tr>
<tr>
<td>play audio</td>
<td>1</td>
<td>100</td>
<td>100</td>
<td>1</td>
<td>60</td>
<td>80</td>
<td>SourceDataLine.open SourceDataLine.start</td>
</tr>
<tr>
<td>play the audio clip at the</td>
<td>1</td>
<td>40</td>
<td>50</td>
<td>1</td>
<td>100</td>
<td>90</td>
<td>Applet.getAudioClip AudioClip.play</td>
</tr>
<tr>
<td>-specified absolute URL</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>average</td>
<td>$&gt;4.0$</td>
<td>44</td>
<td>47</td>
<td>1.6</td>
<td>80</td>
<td>78</td>
<td></td>
</tr>
</tbody>
</table>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) Performance of different dimensions of word embedding</p>
<p>Figure 6: BLEU scores of different parameter settings</p>
<p>Figure 6 shows the influence of different parameter settings on the test set. The dimension of word embedding makes little difference to the accuracy. The accuracy of DeepAPI greately depends on the number of hidden units in the hidden layer. The optimum number of hidden units is around 1000.</p>
<h3>5.5 Performance of the Enhanced RNN EncoderDecoder Models (RQ3)</h3>
<p>In Section 3, we describe two enhancements to the original RNN Encoder-Decoder model, for the task of API learning: an attention-based RNN Encoder-Decoder proposed by [8] (Section 3.1) and an enhanced RNN Encoder-Decoder with a new cost function (Section 3.2) proposed by us. We now evaluate if the enhanced models improve the accuracy of DEEP API when constructed using the original RNN EncoderDecoder model.</p>
<p>Table 3 shows the BLEU scores of the three models. The attention-based RNN Encoder-Decoder outperforms the basic RNN Encoder-Decoder model on API learning. The relative improvement in the top 1,5 , and 10 results (in terms of BLEU score) is $8 \%, 5 \%$ and $4 \%$, respectively. This result confirms the effectiveness of the attention-based RNN Encoder-Decoder used in our approach.</p>
<p>Table 3 also shows that the enhanced model with the new cost function leads to better results as compared to the attention-based RNN Encoder-Decoder model. The improvement in the top 1,5 , and 10 results (in terms of BLEU score) is $4 \%, 2 \%$ and $1 \%$, respectively. Figure 7 shows that the performance of the enhanced model are slightly different under different parameter settings, with an optimum $\lambda$ of around 0.035 . The results confirm the usefulness of the proposed cost function for enhancing the RNN EncoderDecoder model.</p>
<h2>6 DISCUSSION</h2>
<h3>6.1 Why does DeepAPI work?</h3>
<p>Table 3: BLEU scores of different RNN Encoder-Decoder Models (%)</p>
<table>
<thead>
<tr>
<th>Encoder-Decoder Model</th>
<th>Top1</th>
<th>Top5</th>
<th>Top10</th>
</tr>
</thead>
<tbody>
<tr>
<td>RNN</td>
<td>48.83</td>
<td>60.58</td>
<td>64.27</td>
</tr>
<tr>
<td>RNN+Attention</td>
<td>52.49</td>
<td>63.81</td>
<td>66.97</td>
</tr>
<tr>
<td>RNN+Attention+New Cost Function</td>
<td>54.42</td>
<td>64.89</td>
<td>67.83</td>
</tr>
</tbody>
</table>
<p>Figure 7: Performance of the Enhanced RNN Encoder-Decoder Model under Different Settings of $\lambda$</p>
<p>A major challenge for API learning is the semantic gap between code and natural language descriptions. Existing information retrieval based approaches usually have a bag-of-words assumption and lack a deep understanding of the high-level semantics of natural language and code. We have identified three advantages of DEEPAPI that address this problem.
Word embedding and query expansion A significant difference between DEEPAPI and bag-of-words methods is, DEEPAPI embeds words into a continuous semantic space where the semantically similar words are placed close to each other. When reading words in a query, the model maps them to semantic vectors. Words with similar semantics have similar vector representations and have a similar impact on the hidden states of the RNN encoder. Therefore, queries with semantically similar words can lead to similar results. Figure 8 shows a 2-D projection of the encoded vectors of queries. These queries are selected from the 10,000 annotations in the test set. For ease of demonstration, we select queries with a keyword "file" and exclude those longer than eight words. As shown in the graph, DEEPAPI can successfully embed similar queries into a nearby place. There are three clear clusters of queries, corresponding to "read/load files","write/save files", and "remove/delete files". Queries with semantically related words are close to each other. For example, queries starting with save, write, and output are in the same "cluster" though they contain different words.
Learning sequence instead of bag-of-words The hidden layer of the encoder has the memory capacity. It considers not only the individual words, but also their relative positions. Even for the same word set, different sequences will be encoded to different vectors, resulting in different API sequences. In that sense, DEEPAPI learns not only the words, but also phrases. While traditional models simply consider individual words or word-level alignments. A typical example is that, queries with different word sequences such as convert int to string and convert string to int can be distinguished well by DEEPAPI.
Generating common patterns instead of searching specific samples Another advantage of our approach is that, it can learn common patterns of API sequences. The decoder itself is a language model and remembers the likeli-
hoods of different sequences. Those common sequences will have high probabilities according to the model. Therefore, it tends to generate common API sequences rather than project-specific ones. On the other hand, the information retrieval based approaches simply consider searching individual instances and could return project-specific API sequences.</p>
<p>Though several techniques such as query expansion [18, 40, 50] and frequent pattern mining [49] can partially solve some of the above problems, their effectiveness remains to be improved. For example, it has been observed that expanding a code search query with inappropriate English synonyms can return even worse results as compared to the original query [41]. Furthermore, few techniques can exhibit all the above advantages.</p>
<h3>6.2 Threats to Validity</h3>
<p>We have identified the following threats to validity:
All APIs studied are Java APIs All APIs and related projects investigated in this paper are JDK APIs. Hence, they might not be representative of APIs for other libraries and programming languages. In the future, we will extend the model to other libraries and programming languages.
Quality of annotations We collected annotations of API sequences from the first sentence of documentation comments. Other sentences in the comments may also be informative. In addition, the first sentences may have noise. In the future, we will investigate a better NLP technique to extract annotations for code.
Training dataset In the original SWIM paper [36], the clickthrough data from Bing.com is used for evaluation. Such data is not easy accessible for most researchers. For fair and easy comparison, we evaluate SWIM on the dataset collected from GitHub and Java documentations (the same for evaluating DEEPAPI). We train the models using annotations of API sequences collected from the documentation comments. In the future, we will evaluate both SWIM and DEEPAPI on a variety of datasets including the Bing clickthrough data. In the future, we will perform more accurate program analysis and create a better training set.</p>
<h2>7. Related Work</h2>
<h3>7.1 Code Search</h3>
<p>There is a large amount of work on code search [10, 13, 17, 19, 25, 28]. For example, McMillan et al. [28] proposed a code search tool called Portfolio that retrieves and visualizes relevant functions and their usages. Chan and Cheng [13] designed an approach to help users find usages of APIs given only simple text phrases. Lv et al. [25] proposed CodeHow, a code search tool that incorporates an extended Boolean model and API matching. They first find relevant APIs to a query by matching the query to API documentation. Then, they improve code search performance by considering the APIs which are relevant to the query in code retrieval. As described in Section 6, DEEPAPI differs from code search techniques in that it does not rely on information retrieval techniques and can understand word sequences and query semantics.</p>
<h3>7.2 Mining API Usage Patterns</h3>
<p>Instead of generating API sequences from natural language queries, there is a number of techniques focusing on</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: A 2D projection of embeddings of queries using t-SNE [45]</p>
<p>mining API usage patterns [15, 31, 46, 49]. API usage patterns are frequent API method call sequences. Xie et al. [49] proposed MAPO, which is one of the first works on mining API patterns from code corpus. MAPO represents source code as call sequences and clusters them according to similarity heuristics such as method names. It finally generates patterns by mining and ranking frequent sequences in each cluster. UP-Miner [46] is an improvement of MAPO, which removes the redundancy among patterns by two rounds of clustering of the method call sequences. By applying API usage pattern mining on large-scale code search results, these techniques can also return API usage sequences in response to user's natural language queries.</p>
<p>While the above techniques are useful for understanding the usage of an API, they are insufficient for answering the question of <em>which APIs to use</em>, which is the aim of DeepAPI. Furthermore, different from a frequent pattern mining approach, DeepAPI constructs a neural language model to learn usage patterns.</p>
<h3>7.3 From Natural Language to Code</h3>
<p>A number of related techniques have been proposed to generate code snippets from natural language queries. For example, Raghothaman et al. [36] proposed SWIM, a code synthesis technique that translates user queries into the APIs of interest using Bing search logs and then synthesizes idiomatic code describing the use of these APIs. SWIM has a component that produces API sequences given user's natural language query. Our approach and SWIM differ in many aspects. First, SWIM generates bags of APIs using statistical word alignment [12]. The word alignment model does not consider word embeddings and word sequences of natural language queries, and has limitations in query understanding. Second, to produce API sequences, SWIM searches API sequences from the code repository using a bag of APIs. It does not consider the relative position of different APIs. Fowkes and Sutton [7] build probabilistic models that jointly model short natural language utterances and source code snippets. The main differences between our approach and theirs are two-fold. First, they use a bag-of-words model to represent natural language sentences which will not recognize word sequences. Second, they use a traditional probabilistic model which is unable to recognize semantically related words.</p>
<h3>7.4 Deep Learning for Source Code</h3>
<p>Recently, some researchers have explored the possibility of applying deep learning techniques to source code [6, 32, 33, 37]. A typical application that leverages deep learning is to extract source code features [32, 47]. For example, Mou et al. [32] proposed to learn vector representations of source code for deep learning tasks. Mou et al. [33] also proposed convolutional neural networks over tree structures for programming language processing. Deep learning has also been applied to code generation [23, 34]. For example, Mou et al. [34] proposed to generate code from natural language user intentions using an RNN Encoder-Decoder model. Their results show the feasibility of applying deep learning techniques to code generation from a highly homogeneous dataset (simple programming assignments). Deep Learning has also been applied to code completion [37, 48]. For example, White et al. [48] applied the RNN language model to source code files and showed its effectiveness in predicting software tokens. Raychev et al. [37] proposed to apply the RNN language model to complete partial programs with holes. In our work, we explore the application of deep learning techniques to API learning.</p>
<h2>8 CONCLUSION</h2>
<p>In this paper, we apply a deep learning approach, RNN Encoder-Decoder, for generating API usage sequences for a given API-related natural language query. Our empirical study has shown that the proposed approach is effective in API sequence generation. Although deep learning has shown promise in other areas, we are the first to observe its effectiveness in API learning.</p>
<p>The RNN Encoder-Decoder based neural language model described in this paper may benefit other software engineering problems such as code search and bug localization. In the future, we will explore the applications of this model to these problems. We will also investigate the synthesis of sample code from the generated API sequences.</p>
<p>An online demo of DeepAPI can be found on our website at: https://guxd.github.io/deepapi/.</p>
<h2>9 REFERENCES</h2>
<p>[1] Eclipse JDT. http://www.eclipse.org/jdt/.</p>
<p>[2] Github. https://github.com.</p>
<p>[3] Github search. https://github.com/search?type=code.
[4] Lucene. https://lucene.apache.org/.
[5] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton. Learning natural coding conventions. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE'14), pages 281-293. ACM, 2014.
[6] M. Allamanis, H. Peng, and C. Sutton. A convolutional attention network for extreme summarization of source code. In Proceedings of the International Conference on Machine Learning (ICML'16), 2016.
[7] M. Allamanis, D. Tarlow, A. Gordon, and Y. Wei. Bimodal modelling of source code and natural language. In Proceedings of The 32nd International Conference on Machine Learning (ICML'15), pages 2123-2132, 2015.
[8] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
[9] L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of the 19th International Conference on Computational Statistics (COMPSTAT'10), pages 177-186. Springer, 2010.
[10] J. Brandt, M. Dontcheva, M. Weskamp, and S. R. Klemmer. Example-centric programming: integrating web search into the development environment. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI'10, pages 513-522. ACM, 2010.
[11] P. F. Brown, P. V. Desouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai. Class-based n-gram models of natural language. Computational linguistics, 18(4):467-479, 1992.
[12] P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263-311, 1993.
[13] W.-K. Chan, H. Cheng, and D. Lo. Searching connected API subgraph via text phrases. In Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE'12, pages 10:1-10:11. ACM, 2012.
[14] K. Cho, B. Van Merrinboer, . Glehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN Encoder-Decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP'14), pages 1724-1734, Doha, Qatar, Oct. 2014. Association for Computational Linguistics.
[15] J. Fowkes and C. Sutton. Parameter-free probabilistic API mining at github scale. In Proceedings of the ACM SIGSOFT 24th International Symposium on the Foundations of Software Engineering (FSE'16). ACM, 2016.
[16] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu. On the naturalness of software. In Proceedings of the 34th International Conference on Software Engineering (ICSE'12), pages 837-847. IEEE, 2012.
[17] R. Holmes, R. Cottrell, R. J. Walker, and
J. Denzinger. The end-to-end use of source code examples: An exploratory study. In Proceedings of the IEEE International Conference on Software Maintenance (ICSM'09), pages 555-558. IEEE, 2009.
[18] M. J. Howard, S. Gupta, L. Pollock, and K. Vijay-Shanker. Automatically mining software-based, semantically-similar words from comment-code mappings. In Proceedings of the 10th Working Conference on Mining Software Repositories (MSR'13), pages 377-386. IEEE Press, 2013.
[19] I. Keivanloo, J. Rilling, and Y. Zou. Spotting working code examples. In Proceedings of the 36th International Conference on Software Engineering (ICSE'14), pages 664-675. ACM, 2014.
[20] J. Kim, S. Lee, S. Hwang, and S. Kim. Towards an intelligent code search engine. In Proceedings of the 24th AAAI Conference on Artificial Intelligence (AAAI'10), pages 1358-1363, 2010.
[21] P. Koehn. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Machine translation: From real users to research, pages 115-124. Springer, 2004.
[22] M. Li, T. Zhang, Y. Chen, and A. J. Smola. Efficient mini-batch training for stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD'14), pages 661-670. ACM, 2014.
[23] W. Ling, E. Grefenstette, K. M. Hermann, T. Kocisky, A. Senior, F. Wang, and P. Blunsom. Latent predictor networks for code generation. arXiv preprint arXiv:1603.06744, 2016.
[24] E. Linstead, S. Bajracharya, T. Ngo, P. Rigor, C. Lopes, and P. Baldi. Sourcerer: mining and searching internet-scale software repositories. Data Mining and Knowledge Discovery, 18:300-336, 2009.
[25] F. Lv, H. Zhang, J. Lou, S. Wang, D. Zhang, and J. Zhao. CodeHow: Effective code search based on API understanding and extended boolean model. In Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE'15), pages 260-270. IEEE, 2015.
[26] C. Maddison and D. Tarlow. Structured generative models of natural source code. In Proceedings of the 31st International Conference on Machine Learning (ICML'14), pages 649-657, 2014.
[27] C. McMillan, M. Grechanik, and D. Poshyvanyk. Detecting similar software applications. In Proceedings of the 34th International Conference on Software Engineering (ICSE'12), pages 364-374. IEEE, 2012.
[28] C. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie, and C. Fu. Portfolio: finding relevant functions and their usage. In Proceedings of the 33rd International Conference on Software Engineering (ICSE'11), pages 111-120. IEEE, 2011.
[29] T. Mikolov, M. Karafit, L. Burget, J. Cernock, and S. Khudanpur. Recurrent neural network based language model. In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH'10), pages 1045-1048, 2010.
[30] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and</p>
<p>phrases and their compositionality. In Advances in neural information processing systems (NIPS'13), pages 3111-3119, 2013.
[31] E. Moritz, M. Linares-Vsquez, D. Poshyvanyk, M. Grechanik, C. McMillan, and M. Gethers. Export: Detecting and visualizing API usages in large source code repositories. In Proceedings of the IEEE/ACM 28th International Conference on Automated Software Engineering (ASE'13), pages 646-651. IEEE, 2013.
[32] L. Mou, G. Li, Y. Liu, H. Peng, Z. Jin, Y. Xu, and L. Zhang. Building program vector representations for deep learning. arXiv preprint arXiv:1409.3358, 2014.
[33] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin. Convolutional neural networks over tree structures for programming language processing. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI'16), 2016.
[34] L. Mou, R. Men, G. Li, L. Zhang, and Z. Jin. On end-to-end program generation from user intention by deep neural networks. arXiv, 2015.
[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics (ACL'02), pages 311-318. Association for Computational Linguistics, 2002.
[36] M. Raghothaman, Y. Wei, and Y. Hamadi. SWIM: synthesizing what I mean: code search and idiomatic snippet synthesis. In Proceedings of the 38th International Conference on Software Engineering (ICSE'16), pages 357-367. ACM, 2016.
[37] V. Raychev, M. Vechev, and E. Yahav. Code completion with statistical language models. In ACM SIGPLAN Notices, volume 49, pages 419-428. ACM, 2014.
[38] M. P. Robillard. What makes APIs hard to learn? answers from developers. IEEE Software, 26(6):27-34, 2009.
[39] M. P. Robillard and R. DeLine. A field study of API learning obstacles. Empirical Software Engineering, 16(6):703-732, 2010.
[40] D. Shepherd, Z. P. Fry, E. Hill, L. Pollock, and K. Vijay-Shanker. Using natural language program analysis to locate and understand action-oriented concerns. In Proceedings of the 6th international conference on Aspect-oriented software development, pages 212-224. ACM, 2007.
[41] G. Sridhara, E. Hill, L. Pollock, and K. Vijay-Shanker. Identifying word relations in software: A comparative study of semantic similarity tools. In Proceedings of the 16th IEEE International Conference on Program Comprehension (ICPC'08), pages 123-132. IEEE, 2008.
[42] J. Stylos and B. A. Myers. Mica: A web-search tool for finding API components and examples. In Proceedings of the Visual Languages and Human-Centric Computing (VLHCC'06), pages 195-202, 2006.
[43] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems (NIPS'14), pages 3104-3112, 2014.
[44] Z. Tu, Z. Su, and P. Devanbu. On the localness of software. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE'14), pages 269-280. ACM, 2014.
[45] L. Van Der Maaten. Accelerating t-sne using tree-based algorithms. The Journal of Machine Learning Research, 15(1):3221-3245, 2014.
[46] J. Wang, Y. Dang, H. Zhang, K. Chen, T. Xie, and D. Zhang. Mining succinct and high-coverage API usage patterns from source code. In Proceedings of the 10th Working Conference on Mining Software Repositories (MSR'13), pages 319-328. IEEE Press, 2013.
[47] S. Wang, T. Liu, and L. Tan. Automatically learning semantic features for defect prediction. In Proceedings of the 38th International Conference on Software Engineering (ICSE'16), pages 297-308. ACM, 2016.
[48] M. White, C. Vendome, M. Linares-Vsquez, and D. Poshyvanyk. Toward deep learning software repositories. In Proceedings of the IEEE/ACM 12th Working Conference on Mining Software Repositories (MSR'15), pages 334-345. IEEE, 2015.
[49] T. Xie and J. Pei. MAPO: Mining API usages from open source repositories. In Proceedings of the 2006 international workshop on Mining software repositories (MSR'06), pages 54-57. ACM, 2006.
[50] J. Yang and L. Tan. Inferring semantically related words from software context. In Proceedings of the 9th IEEE Working Conference on Mining Software Repositories (MSR'12), pages 161-170. IEEE Press, 2012.
[51] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2} \mathrm{~A}$ documentation comment in JAVA starts with slash-asterisk-asterisk (/*<em>) and ends with asterisk-slash (</em>/)
${ }^{3}$ http://www.oracle.com/technetwork/articles/java/ index-137868.html
${ }^{4}$ https://github.com/apache/commons-io/blob/trunk/src/ main/java/org/apache/commons/io/IOUtils.java&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
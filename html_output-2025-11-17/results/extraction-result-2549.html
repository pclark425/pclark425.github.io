<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2549 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2549</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2549</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-56da1f32981919ea68858b6fc14908f1203b2305</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/56da1f32981919ea68858b6fc14908f1203b2305" target="_blank">Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> DroidAgent, an autonomous GUI testing agent for Android, is proposed for semantic, intent-driven automation of GUI testing, based on Large Language Models and support mechanisms such as long- and short-term memory.</p>
                <p><strong>Paper Abstract:</strong> GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51% for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 374 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2549.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2549.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DroidAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DROIDAGENT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous, multi-agent LLM-based system for intent-driven Android GUI testing that plans high-level tasks in natural language, executes them by interacting with the app GUI, observes outcomes, reflects to form long-term knowledge, and reuses knowledge to plan subsequent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DROIDAGENT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DROIDAGENT is an agent-based framework that uses multiple Large Language Model (LLM) instances and structured memories to autonomously (1) generate high-level, natural-language testing tasks (Planner), (2) select concrete GUI actions to try to accomplish these tasks (Actor), (3) summarise action outcomes based on GUI state diffs (Observer), and (4) reflect on task execution to produce compact, retrievable knowledge (Reflector). It uses three memory modules (short-term/working memory, long-term memory implemented with an embedding DB (ChromaDB), and a spatial/widget memory) and two retrieval modules (task retriever and widget retriever). GUI states are represented as structured JSON; actions are invoked via function-call style outputs which are converted to concrete Android GUI events.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Planner: continuous high-level task generation (considers past tasks, covered/total activities, persona/goal); Actor: action-selection agent that chooses action types (touch, set_text, wait, back, end_task) and target widgets via function-call style responses; Observer: summarizes outcome of each action by diffing prior/updated GUI states and produces concise observations; Reflector: end-of-task summariser that labels success/failure, derives reflections and writes task knowledge into long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>idea generation / task planning (Planner), implementation-level action generation and execution (Actor), observation and data collection (Observer), evaluation/assessment and knowledge consolidation (Reflector + memory storage), iterative refinement (self-critique and repeated planning).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Hierarchical and pipeline-like coordination with a central planning-and-memory loop: Planner generates tasks (central orchestrator); Actor and Observer form an inner execution loop (Actor acts, Observer reports); Reflector consolidates results and updates long-term memory; retrieval modules feed Planner/Actor. Coordination is mediated by shared memories and sequential control signals (Planner issues tasks; Actor invokes actions until end condition).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Structured prompts and function-call style messages to LLMs, plus structured JSON representations for GUI state; Actor returns function-call outputs (e.g. {"name":"touch","arguments":{"target_widget_ID":11}}) which are converted to GUI events; Observer communicates summarized multi-line string diffs; memory retrievals return textual summaries embedded into prompts. External tooling/APIs (OpenAI function-call style and embedding DB via ChromaDB) are used for agent-tool interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Multiple feedback channels: (1) Observer provides immediate post-action summaries used by Actor; (2) periodic self-critique (every three actions in experiments) produced by a stronger LLM (GPT-4) gives a critique and suggested workaround which is injected into Actor's context; (3) Reflector at task end labels success/failure and creates condensed reflections that are stored in long-term memory and later retrieved by Planner; (4) widget retriever supplies summarized past observations per widget to inform Actor/Planner. These feedbacks drive iterative refinement of plans and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Frequent/stepwise: Actor and Observer communicate after each action (inner loop); self-critique runs periodically (every three actions in experiments); Reflector runs at the end of each task; Planner runs between tasks, retrieving the N most recent tasks and M most relevant knowledge (N=20, M=5 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software engineering / automated GUI testing for Android mobile applications (intent-driven test scenario generation and execution).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics: average activity coverage 60.7% (DROIDAGENT) vs 51.4% (best baseline Humanoid); total covered activities across 15 apps: 133 (DROIDAGENT) vs 107 (Humanoid) in Table II; average feature coverage: 13.9 (DROIDAGENT) vs 7.3 (Humanoid); number of unique auto-generated tasks: 374 (85% viable by manual assessment, 59% successfully completed by agent); Reflector task-result assessor precision=0.72, recall=0.77, F1=0.74; crash findings: 5 crashes found by DROIDAGENT; average task length mean 8.9 actions (max configured 13); monetary cost per 2-hour run: $13–$22 (mean $18.1). Statistical tests: Wilcoxon p < 0.045 for activity coverage superiority over Humanoid, p < 0.0008 for feature coverage superiority.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against Monkey (random), DroidBot, Humanoid (deep-learning based), and GPTDroid (LLM-based reimplementation). Results: DROIDAGENT achieved higher average activity coverage (60.7%) than all baselines (Humanoid 51.4%, others lower) and significantly higher feature coverage (13.9 vs Humanoid 7.3); also generated 374 meaningful tasks with high viability. Significance tests reported for activity and feature coverage (Wilcoxon).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Coordination enabled: (1) generation and execution of semantically meaningful tasks rather than low-level exploration, (2) reuse of created app data across tasks (multi-step scenarios), (3) autonomous handling of multi-step flows such as logins and account creation, (4) improved coverage metrics (activity and feature) and meaningful crash reports. Quantitative benefits: +~9.3 percentage points activity coverage over Humanoid (60.7% vs 51.4%), feature coverage nearly doubled (13.9 vs 7.3), and produced 374 tasks with 85% viability.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Reported challenges: LLM inherent randomness and API cost limiting repeated runs; token/context length limits causing prompt resets (noted for reimplemented GPTDroid and mitigated in DROIDAGENT by memory and use of 16K models); apps with widgets lacking textual properties or complex single-view interactable regions reduce effectiveness; external-app interactions are bounded by fixed limits leading to premature termination; monetary and token overheads due to frequent LLM prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes — ablations include: 'NoKnowledge' (disable knowledge retrievers), 'NoKnowledgeAndCritique' (also disable self-critique), and 'Actor-only' (only Actor without registered tasks). Results (qualitative/figure-based): both knowledge retrievers and the self-critique module positively improved exploration effectiveness (activity coverage); Actor-only performs worse. The paper reports that disabling retrievers and critique reduces activity coverage (Figure 8), indicating these coordination/feedback components contribute materially.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Experimental configuration suggested by paper: use multiple specialised agents (Planner, Actor, Observer, Reflector); memory retrieval parameters N=20 recent tasks and M=5 most relevant task knowledge; max actions per task 13; self-critique invoked every 3 actions and run on a stronger LLM (GPT-4) while Actor/Observer use GPT-3.5 (16K) for action selection; long-term memory implemented via ChromaDB embeddings; represent GUI state as JSON; these settings were used in experiments and reported as effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2549.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2549.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced multi-agent LLM-driven cognitive/simulation architecture demonstrating multi-agent social simulation and rich agent behaviors, cited as inspiration for autonomous agent design and social/multi-agent interaction patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generative Agents (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an example of LLM-based cognitive architectures and multi-agent social simulation that inspired DROIDAGENT's autonomy design; described in the paper as part of prior work showing how agents with memory and planning can simulate social behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>unspecified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not specified in detail in this paper; referenced generally as multi-agent social simulation with planning and reflection capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Implicitly: planning and reflection (cognitive-architecture style capabilities) as cited; specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Described in this paper only as 'multi-agent social simulation' inspiration; detailed coordination mechanisms are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not detailed in this paper beyond being LLM-driven multi-agent simulation; no protocol specifics given.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Paper references planning and reflection aspects of cognitive architectures as relevant; no concrete feedback protocol described for Generative Agents within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Referenced generally as multi-agent social simulation / cognitive architecture demonstrations (not applied here to GUI testing except as inspiration).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No performance metrics about Generative Agents are reported in this paper (only cited as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mention only as inspirational prior work; no direct comparison performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Used as conceptual inspiration for agent autonomy, planning, and reflection; paper claims such cognitive-architecture ideas help DROIDAGENT avoid forgetting knowledge and improve long-term planning.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed in detail for Generative Agents within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None reported here for Generative Agents (only DROIDAGENT ablations are reported).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2549.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2549.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGPT (Autonomous GPT-4 experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous-agent style project that hybridises LLMs with external tools and function calls; cited as an example of LLMs using external tools and forming autonomous workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autogpt: An autonomous GPT-4 experiment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Auto-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of LLM-based autonomous agent frameworks that can call external tools and operate in an open-ended way; referenced to support the claim that LLMs can directly use external tools through function calls.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>unspecified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not detailed here; the paper only references Auto-GPT as an example of hybridising LLMs with external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Not detailed in this paper; implied support for task orchestration through tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not specified in this paper; generically implied tool-mediated autonomous orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Paper notes that recent LLMs can use external tools through function calls and that Auto-GPT is an example of hybridising LLMs with tools; function-call based interfacing is the only protocol detail mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Referenced as a general autonomous agent framework; not applied experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No metrics for Auto-GPT reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared experimentally; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Used to motivate the feasibility of LLM-driven agents using tools and memory; no empirical claims in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2549.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2549.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain: a framework for developing applications powered by language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework/library for building LLM-powered applications that supports function-call style tool use and chaining of LLM calls; cited as an example of LLM-tool integration used in autonomous agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Langchain: a framework for developing applications powered by language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LangChain</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an example of libraries that simplify hybridising LLMs with external tools via function-call semantics, enabling autonomous agent designs (but LangChain itself is not used or evaluated experimentally in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not applicable / unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not applicable in this paper (LangChain is a library rather than a multi-agent system here).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Tool integration and orchestration for LLM-based tasks (implied).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not described in detail in this paper; cited generally for function-call and tool-integration capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Function-call style API usage and chaining of LLM output into subsequent tool calls (as described at a high level in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General LLM application development; cited as related infrastructure for autonomous agents.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not applicable; cited as related tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Cited to illustrate how LLMs can be hybridized with external tools to create autonomous agents; no empirical claims in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2549.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2549.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as an example of LLM-driven embodied agents for open-ended exploration tasks, used as related work to motivate agent-based designs with planning and exploration capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as prior work demonstrating open-ended embodied agent capabilities built around LLMs, illustrating how cognitive-architecture ideas and external tool usage can enable extended exploration; cited for inspiration rather than use.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>unspecified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not detailed here (paper references Voyager only as illustrative related work).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Implied planning and exploration phases (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Embodied agent exploration (cited for analogy to agent design in DROIDAGENT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No metrics reported for Voyager in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared experimentally; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Provides conceptual support for using planning, memory, and tool use in agent architectures; no empirical details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2549.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2549.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Soar cognitive architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical cognitive architecture referenced to ground the use of cognitive-architecture principles (planning, memory, reflection) in DROIDAGENT's design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Soar cognitive architecture</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Soar (cognitive architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of cognitive architectures that historically model planning and memory; used by the authors to motivate an LLM-driven cognitive-architecture inspired agent design (planning, reflection, memory retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not applicable / unspecified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Not applicable; referenced conceptually as a cognitive-architecture paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Planning and reflection are specifically mentioned as cognitive-architecture components relevant to DROIDAGENT.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Not detailed in this paper beyond general cognitive-architecture inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Cognitive-architecture style planning and reflection are cited as desirable feedback/learning mechanics; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cognitive architectures for agent behaviour (conceptual inspiration only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No direct metrics reported in this paper for Soar.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not experimentally compared; cited as background theory.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Serves as theoretical foundation for planning and reflection components employed in DROIDAGENT.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None for Soar within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Autogpt: An autonomous GPT-4 experiment <em>(Rating: 2)</em></li>
                <li>Towards autonomous testing agents via conversational large language models <em>(Rating: 2)</em></li>
                <li>Langchain: a framework for developing applications powered by language models <em>(Rating: 1)</em></li>
                <li>The Soar cognitive architecture <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2549",
    "paper_id": "paper-56da1f32981919ea68858b6fc14908f1203b2305",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "DroidAgent",
            "name_full": "DROIDAGENT",
            "brief_description": "An autonomous, multi-agent LLM-based system for intent-driven Android GUI testing that plans high-level tasks in natural language, executes them by interacting with the app GUI, observes outcomes, reflects to form long-term knowledge, and reuses knowledge to plan subsequent tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DROIDAGENT",
            "system_description": "DROIDAGENT is an agent-based framework that uses multiple Large Language Model (LLM) instances and structured memories to autonomously (1) generate high-level, natural-language testing tasks (Planner), (2) select concrete GUI actions to try to accomplish these tasks (Actor), (3) summarise action outcomes based on GUI state diffs (Observer), and (4) reflect on task execution to produce compact, retrievable knowledge (Reflector). It uses three memory modules (short-term/working memory, long-term memory implemented with an embedding DB (ChromaDB), and a spatial/widget memory) and two retrieval modules (task retriever and widget retriever). GUI states are represented as structured JSON; actions are invoked via function-call style outputs which are converted to concrete Android GUI events.",
            "number_of_agents": "4",
            "agent_specializations": "Planner: continuous high-level task generation (considers past tasks, covered/total activities, persona/goal); Actor: action-selection agent that chooses action types (touch, set_text, wait, back, end_task) and target widgets via function-call style responses; Observer: summarizes outcome of each action by diffing prior/updated GUI states and produces concise observations; Reflector: end-of-task summariser that labels success/failure, derives reflections and writes task knowledge into long-term memory.",
            "research_phases_covered": "idea generation / task planning (Planner), implementation-level action generation and execution (Actor), observation and data collection (Observer), evaluation/assessment and knowledge consolidation (Reflector + memory storage), iterative refinement (self-critique and repeated planning).",
            "coordination_mechanism": "Hierarchical and pipeline-like coordination with a central planning-and-memory loop: Planner generates tasks (central orchestrator); Actor and Observer form an inner execution loop (Actor acts, Observer reports); Reflector consolidates results and updates long-term memory; retrieval modules feed Planner/Actor. Coordination is mediated by shared memories and sequential control signals (Planner issues tasks; Actor invokes actions until end condition).",
            "communication_protocol": "Structured prompts and function-call style messages to LLMs, plus structured JSON representations for GUI state; Actor returns function-call outputs (e.g. {\"name\":\"touch\",\"arguments\":{\"target_widget_ID\":11}}) which are converted to GUI events; Observer communicates summarized multi-line string diffs; memory retrievals return textual summaries embedded into prompts. External tooling/APIs (OpenAI function-call style and embedding DB via ChromaDB) are used for agent-tool interactions.",
            "feedback_mechanism": "Multiple feedback channels: (1) Observer provides immediate post-action summaries used by Actor; (2) periodic self-critique (every three actions in experiments) produced by a stronger LLM (GPT-4) gives a critique and suggested workaround which is injected into Actor's context; (3) Reflector at task end labels success/failure and creates condensed reflections that are stored in long-term memory and later retrieved by Planner; (4) widget retriever supplies summarized past observations per widget to inform Actor/Planner. These feedbacks drive iterative refinement of plans and actions.",
            "communication_frequency": "Frequent/stepwise: Actor and Observer communicate after each action (inner loop); self-critique runs periodically (every three actions in experiments); Reflector runs at the end of each task; Planner runs between tasks, retrieving the N most recent tasks and M most relevant knowledge (N=20, M=5 in experiments).",
            "task_domain": "Software engineering / automated GUI testing for Android mobile applications (intent-driven test scenario generation and execution).",
            "performance_metrics": "Reported metrics: average activity coverage 60.7% (DROIDAGENT) vs 51.4% (best baseline Humanoid); total covered activities across 15 apps: 133 (DROIDAGENT) vs 107 (Humanoid) in Table II; average feature coverage: 13.9 (DROIDAGENT) vs 7.3 (Humanoid); number of unique auto-generated tasks: 374 (85% viable by manual assessment, 59% successfully completed by agent); Reflector task-result assessor precision=0.72, recall=0.77, F1=0.74; crash findings: 5 crashes found by DROIDAGENT; average task length mean 8.9 actions (max configured 13); monetary cost per 2-hour run: $13–$22 (mean $18.1). Statistical tests: Wilcoxon p &lt; 0.045 for activity coverage superiority over Humanoid, p &lt; 0.0008 for feature coverage superiority.",
            "baseline_comparison": "Compared against Monkey (random), DroidBot, Humanoid (deep-learning based), and GPTDroid (LLM-based reimplementation). Results: DROIDAGENT achieved higher average activity coverage (60.7%) than all baselines (Humanoid 51.4%, others lower) and significantly higher feature coverage (13.9 vs Humanoid 7.3); also generated 374 meaningful tasks with high viability. Significance tests reported for activity and feature coverage (Wilcoxon).",
            "coordination_benefits": "Coordination enabled: (1) generation and execution of semantically meaningful tasks rather than low-level exploration, (2) reuse of created app data across tasks (multi-step scenarios), (3) autonomous handling of multi-step flows such as logins and account creation, (4) improved coverage metrics (activity and feature) and meaningful crash reports. Quantitative benefits: +~9.3 percentage points activity coverage over Humanoid (60.7% vs 51.4%), feature coverage nearly doubled (13.9 vs 7.3), and produced 374 tasks with 85% viability.",
            "coordination_challenges": "Reported challenges: LLM inherent randomness and API cost limiting repeated runs; token/context length limits causing prompt resets (noted for reimplemented GPTDroid and mitigated in DROIDAGENT by memory and use of 16K models); apps with widgets lacking textual properties or complex single-view interactable regions reduce effectiveness; external-app interactions are bounded by fixed limits leading to premature termination; monetary and token overheads due to frequent LLM prompting.",
            "ablation_studies": "Yes — ablations include: 'NoKnowledge' (disable knowledge retrievers), 'NoKnowledgeAndCritique' (also disable self-critique), and 'Actor-only' (only Actor without registered tasks). Results (qualitative/figure-based): both knowledge retrievers and the self-critique module positively improved exploration effectiveness (activity coverage); Actor-only performs worse. The paper reports that disabling retrievers and critique reduces activity coverage (Figure 8), indicating these coordination/feedback components contribute materially.",
            "optimal_configurations": "Experimental configuration suggested by paper: use multiple specialised agents (Planner, Actor, Observer, Reflector); memory retrieval parameters N=20 recent tasks and M=5 most relevant task knowledge; max actions per task 13; self-critique invoked every 3 actions and run on a stronger LLM (GPT-4) while Actor/Observer use GPT-3.5 (16K) for action selection; long-term memory implemented via ChromaDB embeddings; represent GUI state as JSON; these settings were used in experiments and reported as effective.",
            "uuid": "e2549.0",
            "source_info": {
                "paper_title": "Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents: Interactive simulacra of human behavior",
            "brief_description": "A referenced multi-agent LLM-driven cognitive/simulation architecture demonstrating multi-agent social simulation and rich agent behaviors, cited as inspiration for autonomous agent design and social/multi-agent interaction patterns.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "system_name": "Generative Agents (Park et al.)",
            "system_description": "Mentioned as an example of LLM-based cognitive architectures and multi-agent social simulation that inspired DROIDAGENT's autonomy design; described in the paper as part of prior work showing how agents with memory and planning can simulate social behaviors.",
            "number_of_agents": "unspecified in this paper",
            "agent_specializations": "Not specified in detail in this paper; referenced generally as multi-agent social simulation with planning and reflection capabilities.",
            "research_phases_covered": "Implicitly: planning and reflection (cognitive-architecture style capabilities) as cited; specifics not detailed in this paper.",
            "coordination_mechanism": "Described in this paper only as 'multi-agent social simulation' inspiration; detailed coordination mechanisms are not provided here.",
            "communication_protocol": "Not detailed in this paper beyond being LLM-driven multi-agent simulation; no protocol specifics given.",
            "feedback_mechanism": "Paper references planning and reflection aspects of cognitive architectures as relevant; no concrete feedback protocol described for Generative Agents within this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "Referenced generally as multi-agent social simulation / cognitive architecture demonstrations (not applied here to GUI testing except as inspiration).",
            "performance_metrics": "No performance metrics about Generative Agents are reported in this paper (only cited as prior work).",
            "baseline_comparison": "Mention only as inspirational prior work; no direct comparison performed in this paper.",
            "coordination_benefits": "Used as conceptual inspiration for agent autonomy, planning, and reflection; paper claims such cognitive-architecture ideas help DROIDAGENT avoid forgetting knowledge and improve long-term planning.",
            "coordination_challenges": "Not discussed in detail for Generative Agents within this paper.",
            "ablation_studies": "None reported here for Generative Agents (only DROIDAGENT ablations are reported).",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2549.1",
            "source_info": {
                "paper_title": "Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Auto-GPT",
            "name_full": "AutoGPT (Autonomous GPT-4 experiment)",
            "brief_description": "An autonomous-agent style project that hybridises LLMs with external tools and function calls; cited as an example of LLMs using external tools and forming autonomous workflows.",
            "citation_title": "Autogpt: An autonomous GPT-4 experiment",
            "mention_or_use": "mention",
            "system_name": "Auto-GPT",
            "system_description": "Cited as an example of LLM-based autonomous agent frameworks that can call external tools and operate in an open-ended way; referenced to support the claim that LLMs can directly use external tools through function calls.",
            "number_of_agents": "unspecified in this paper",
            "agent_specializations": "Not detailed here; the paper only references Auto-GPT as an example of hybridising LLMs with external tools.",
            "research_phases_covered": "Not detailed in this paper; implied support for task orchestration through tool use.",
            "coordination_mechanism": "Not specified in this paper; generically implied tool-mediated autonomous orchestration.",
            "communication_protocol": "Paper notes that recent LLMs can use external tools through function calls and that Auto-GPT is an example of hybridising LLMs with tools; function-call based interfacing is the only protocol detail mentioned.",
            "feedback_mechanism": "Not detailed in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "Referenced as a general autonomous agent framework; not applied experimentally in this paper.",
            "performance_metrics": "No metrics for Auto-GPT reported in this paper.",
            "baseline_comparison": "Not compared experimentally; cited as related work.",
            "coordination_benefits": "Used to motivate the feasibility of LLM-driven agents using tools and memory; no empirical claims in this paper.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "None in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2549.2",
            "source_info": {
                "paper_title": "Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LangChain",
            "name_full": "LangChain: a framework for developing applications powered by language models",
            "brief_description": "A framework/library for building LLM-powered applications that supports function-call style tool use and chaining of LLM calls; cited as an example of LLM-tool integration used in autonomous agents.",
            "citation_title": "Langchain: a framework for developing applications powered by language models",
            "mention_or_use": "mention",
            "system_name": "LangChain",
            "system_description": "Referenced as an example of libraries that simplify hybridising LLMs with external tools via function-call semantics, enabling autonomous agent designs (but LangChain itself is not used or evaluated experimentally in this paper).",
            "number_of_agents": "not applicable / unspecified",
            "agent_specializations": "not applicable in this paper (LangChain is a library rather than a multi-agent system here).",
            "research_phases_covered": "Tool integration and orchestration for LLM-based tasks (implied).",
            "coordination_mechanism": "Not described in detail in this paper; cited generally for function-call and tool-integration capabilities.",
            "communication_protocol": "Function-call style API usage and chaining of LLM output into subsequent tool calls (as described at a high level in the paper).",
            "feedback_mechanism": "Not detailed in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "General LLM application development; cited as related infrastructure for autonomous agents.",
            "performance_metrics": "None reported in this paper.",
            "baseline_comparison": "Not applicable; cited as related tooling.",
            "coordination_benefits": "Cited to illustrate how LLMs can be hybridized with external tools to create autonomous agents; no empirical claims in this paper.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "None in this paper.",
            "optimal_configurations": "Not provided in this paper.",
            "uuid": "e2549.3",
            "source_info": {
                "paper_title": "Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "Referenced as an example of LLM-driven embodied agents for open-ended exploration tasks, used as related work to motivate agent-based designs with planning and exploration capabilities.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "system_name": "Voyager",
            "system_description": "Mentioned as prior work demonstrating open-ended embodied agent capabilities built around LLMs, illustrating how cognitive-architecture ideas and external tool usage can enable extended exploration; cited for inspiration rather than use.",
            "number_of_agents": "unspecified in this paper",
            "agent_specializations": "Not detailed here (paper references Voyager only as illustrative related work).",
            "research_phases_covered": "Implied planning and exploration phases (not detailed in this paper).",
            "coordination_mechanism": "Not specified in this paper.",
            "communication_protocol": "Not specified in this paper.",
            "feedback_mechanism": "Not specified in this paper.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "Embodied agent exploration (cited for analogy to agent design in DROIDAGENT).",
            "performance_metrics": "No metrics reported for Voyager in this paper.",
            "baseline_comparison": "Not compared experimentally; cited as related work.",
            "coordination_benefits": "Provides conceptual support for using planning, memory, and tool use in agent architectures; no empirical details in this paper.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "None reported in this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2549.4",
            "source_info": {
                "paper_title": "Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Soar",
            "name_full": "The Soar cognitive architecture",
            "brief_description": "A classical cognitive architecture referenced to ground the use of cognitive-architecture principles (planning, memory, reflection) in DROIDAGENT's design.",
            "citation_title": "The Soar cognitive architecture",
            "mention_or_use": "mention",
            "system_name": "Soar (cognitive architecture)",
            "system_description": "Cited as an example of cognitive architectures that historically model planning and memory; used by the authors to motivate an LLM-driven cognitive-architecture inspired agent design (planning, reflection, memory retrieval).",
            "number_of_agents": "not applicable / unspecified in this paper",
            "agent_specializations": "Not applicable; referenced conceptually as a cognitive-architecture paradigm.",
            "research_phases_covered": "Planning and reflection are specifically mentioned as cognitive-architecture components relevant to DROIDAGENT.",
            "coordination_mechanism": "Not detailed in this paper beyond general cognitive-architecture inspiration.",
            "communication_protocol": "Not detailed in this paper.",
            "feedback_mechanism": "Cognitive-architecture style planning and reflection are cited as desirable feedback/learning mechanics; specifics not provided here.",
            "communication_frequency": "Not specified in this paper.",
            "task_domain": "Cognitive architectures for agent behaviour (conceptual inspiration only).",
            "performance_metrics": "No direct metrics reported in this paper for Soar.",
            "baseline_comparison": "Not experimentally compared; cited as background theory.",
            "coordination_benefits": "Serves as theoretical foundation for planning and reflection components employed in DROIDAGENT.",
            "coordination_challenges": "Not discussed here.",
            "ablation_studies": "None for Soar within this paper.",
            "optimal_configurations": "Not specified in this paper.",
            "uuid": "e2549.5",
            "source_info": {
                "paper_title": "Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "Autogpt: An autonomous GPT-4 experiment",
            "rating": 2
        },
        {
            "paper_title": "Towards autonomous testing agents via conversational large language models",
            "rating": 2
        },
        {
            "paper_title": "Langchain: a framework for developing applications powered by language models",
            "rating": 1
        },
        {
            "paper_title": "The Soar cognitive architecture",
            "rating": 1
        }
    ],
    "cost": 0.015325749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing</h1>
<p>Juyeon Yoon<br>KAIST<br>juyeon.yoon@kaist.ac.kr</p>
<p>Robert Feldt<br>Chalmers University of Technology<br>robert.feldt@chalmers.se</p>
<p>Shin Yoo<br>KAIST<br>shin.yoo@kaist.ac.kr</p>
<h4>Abstract</h4>
<p>GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAGENT, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAGENT sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAGENT using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAGENT created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAGENT achieved $\mathbf{6 1 \%}$ activity coverage, compared to $\mathbf{5 1 \%}$ for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 374 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAGENT interacts deeply with the apps and covers more features.</p>
<p>Index Terms—software testing, GUI testing, test automation, artificial intelligence, large language model</p>
<h2>I. INTRODUCTION</h2>
<p>Testing mobile applications at the GUI level to ensure their quality in terms of functionality and usability is a critical part of app development. However, it also remains a costly task due to the ever-growing complexity of applications and inherent issues about the Android ecosystem such as rapid platform evolution and device fragmentation.</p>
<p>To address the challenges in GUI testing, there has been a large amount of research efforts [1], [2], [3] to automate the various aspects of mobile GUI testing. Most of the existing techniques focus on exploring the GUI states of a given app as much as possible. For example, DroidBot [4] adopts a greedy exploration policy that prioritises unexplored widgets as a next exploration target. Humanoid [1] adopt deep learning in hope to mimic human-like exploration paths by utilising pretrained models to weigh probable GUI actions, aligning them with frequently performed actions based on real human traces. More recently, reinforcement learning has been applied to GUI testing to effectively discover novel GUI states using curiositybased reward functions [5], [6], [7]. While these approaches have revealed actual bugs in Android apps, we also note that their objectives remain exploration of app structures, typically
quantified using activity or widget coverage, i.e., the number of Android activities (screens) or widgets that have been covered by the automated testing technique.</p>
<p>However, a recent empirical study of Android developers [8] reports that the primary test design strategy adopted by Android developers is to follow the usage model of the apps. Developers overwhelmingly prefer test cases that target individual features and use cases, which are higher level test objectives compared to activity coverage, a structural testing objective. The emphasis on more semantic test objectives can also be seen when developers are questioned about the format of automatically generated test cases that they ideally want. Surprisingly, the most popular choice is natural language rather than any test API scripts. Further, developers want expected outputs, as well as steps of use cases and specific app features, in the automatically generated tests. These results reveal a gap between what is being offered by automated Android testing techniques, and what developers want in test automation in Android.</p>
<p>This paper presents DroidAGENT ${ }^{1}$ with the aim of bringing the level of Android GUI testing automation closer to developer preferences and expectations. Instead of going after structural testing goals such as higher activity coverage, DroidAGENT automatically comes up with natural language descriptions of specific tasks that can be achieved using the given App Under Testing (AUT), and subsequently tries to interact with the GUI of the AUT with specific intent to achieve those tasks. If successful, DroidAGENT will produce a GUI test case script that can achieve the specific task, leaving the developer with both a natural language task description as well as executable test scripts. To the best of our knowledge, DroidAGENT is the first Android GUI testing technique that can automatically generate high level testing scenarios based on sequences of identified tasks.</p>
<p>DroidAGENT achieves this by using multiple Large Language Model (LLM) instances that interact and externally act as an autonomous agent. LLMs have been used to automate Android GUI testing before, but either in more limited contexts, or with much less autonomy than DroidAGENT. For example, Liu et al. [9] proposed an approach to generate appropriate text inputs for a given GUI widget by prompting</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>an LLM with textual descriptions of the current GUI state, while Wen et al. [10] extended DroidBot [4] to generate a sequence of GUI actions from a given textual description of task. Lately, GPTDroid [11] showed that, given a summary of past exploration and descriptions of current GUI state, LLMs can choose a human-like next event to continue the exploration. Unlike existing approaches, DROIDAGENT sets its own testing goals autonomously, and can coherently follow long-term plans it has generated in order to accomplish those tasks. The autonomy of DROIDAGENT is inspired by the work on LLM-based cognitive architecture [12] as outlined for software testing in Feldt et al. [13].</p>
<p>We have empirically evaluated DROIDAGENT, using 15 apps from Themis benchmark [14], against four baselines: two traditional GUI state exploration techniques, DroidBot [4] and Humanoid [1], an LLM-based GUI state exploration technique, GPTDroid [11], and a random GUI testing technique, Monkey [15]. DROIDAGENT has automatically generated 374 unique tasks for 15 studied apps: a manual assessment shows that $85 \%$ of generated tasks are relevant and viable, while $59 \%$ are successfully accomplished by DROIDAGENT. While trying to achieve these tasks, DROIDAGENT also reports the highest average activity coverage of $61 \%$, compared to $51 \%$ achieved by Humanoid. Our results suggest that LLM-based autonomous agents can potentially automate Android GUI testing at a higher, more semantic level than GUI state exploration, and that this can even improve lower-level coverage.</p>
<p>The technical contributions of this paper are as follows:</p>
<ul>
<li>We present DROIDAGENT, an autonomous Android GUI testing technique that can set and execute app specific tasks on its own. It produces natural language descriptions of tasks, and test scripts that achieve them.</li>
<li>We empirically evaluate DROIDAGENT against four baseline techniques, using Android apps taken from a widely used Themis benchmark. Our results show that DROIDAGENT is capable of generating relevant and useful app usage tasks, which it subsequently accomplishes automatically by interacting with the GUI of the given app.</li>
<li>We provide a replication package of DROIDAGENT that includes its public implementation.
The rest of the paper is organised as follows. Section II presents background information about Android testing as well as agents based on LLMs. Section III describes the internal architecture of DROIDAGENT, while Section IV presents an illustrative example of how DROIDAGENT operates when given an app. Section V describes the settings of our empirical evaluation, the results of which are reported in Section VI, while Section VIII discusses threats to validity. Finally, Section IX concludes.</li>
</ul>
<h2>II. BACKGROUND</h2>
<p>This section outlines some background information.</p>
<h2>A. GUI Testing on Android</h2>
<p>In Android mobile applications, users primarily interact with GUI components such as buttons, text fields, and menus. A
core component of an Anrdoid app is called an activity, which generally implements one screen (or one set of coherent functionalities) of a given app [16]. The list of contained activities are available in the manifest file contained in any apps. An activity can be essentially viewed as a tree hierarchy, in which each node represents a GUI component, named "widget" in this paper, or a container grouping related widgets. Android SDK provides tools and interfaces to query such views and interact with the contained widgets, enabling actions like button presses or inputting text in textfields. Various testing frameworks [17], [4] have been built on top of ADB (Android Debug Bridge), part of the Android SDK, for automation.</p>
<p>1) Inference of possible actions: GUI testing is often formulated as a problem of choosing the best next action based on a specific GUI state [3], [18], [2], [19]. The possible actions can be determined from actionable widgets with properties like "clickable" or "editable". While the number of possible actions on a specific GUI state is limited due to the relatively small screen size of mobile devices, there can be numerous actions, especially when considering a list of items.
2) GUI state description: Humans typically perceive Android UI based on the visual appearance of views on screen. For language models, conveying such visual information can still be challenging. Previous work [9], [11], [20] rather choose to use textual properties included in a widget (e.g., resource_id, content_description, and text), which provide a brief hint about the function of the widget.</p>
<p>Specifically, Liu et al. [11] combined textual descriptions of all the contained widgets, and used it as a dynamic context for prompting the language model to generate a next action. However, this makes it challenging to convey the hierarchical structure of GUI states and widgets. Meanwhile, another recent work [21] adopt the style of HTML documents to represent hierarchical structure of an Android view. However, since LLMs are also trained on large codebases, and on relevant formats, for DROIDAGENT we adopt JSON notation to describe GUI state and, thus, to represent hierarchical data.</p>
<h2>B. Autonomous Agents with Large Language Models</h2>
<p>LLMs have been proven effective in various tasks, including software testing automation [22], [23], [11], [21], [24]. Yet, the primary use of LLMs has been through single, templated prompts possibly with few-shot examples of desired behavior [25]. Recent LLMs, like OpenAI's GPT-3.5 and GPT-4, can directly use external tools through function calls, making it easier for LLM-based libraries like LangChain [26] and AutoGPT [27] to support hybridising LLMs with external tools.</p>
<p>By including additional memory structures to overcome the LLMs' limited context lengths, autonomous agents can be built that combine both long-term planning and interaction with the use of external tools. The memory component of an LLMdriven autonomous agent can be implemented as key-value stores where a local neural network model embeds the text (value) to a vector (key). This embedding database is then used to find content similar to the current context, e.g., actions that</p>
<p>have already been tried in the current state. In DROIDAGENT, we use the ChromaDB library [28] which implements such an embedding database for texts.</p>
<p>With long-term memory and the ability to use tools, autonomous agents can be built to achieve significant goals. The design of such agent architectures can be based on cognitive architectures [29], that was originally proposed as models of the human mind, e.g., multi-agent social simulation [12], and 3D world exploration [30], and typically also include planning and reflection. When designing DROIDAGENT it is particularly important that it can recover from undesired exploration paths and that it preserves knowledge about the application as exploration continues. In the following, we describe in detail how our agent-based design achieves this.</p>
<h2>III. FRAMEWORK</h2>
<p>DROIDAGENT is designed with an agent-based architecture with four main LLM-based agents performing specific tasks: Planner, Actor, Observer, Reflector. It is further supported by three different memory modules (short, long, and spatial), with two retrieval modules that extract relevant information from memory and format it for use by the agents. The Actor and Observer form an “inner” loop trying to perform tasks which have been planned by the Planner and which is later reflected upon by the Reflector. Figure 1 illustrates the role of each component and highlights the main information flow. We describe individual components of DROIDAGENT below; further, Section IV will provide a concrete working example.</p>
<h2>A. Task Planner</h2>
<p>A key part of DROIDAGENT is the continuous planning of high-level tasks to be achieved. These tasks can also directly be used to “describe” what lower-level actions, performed by other agents, actually mean. Essentially, tasks are the basis for intent-driven testing. The tasks should ideally correspond to semantically meaningful steps when testing AUT, as well as align with coherent functionalities of the target application. In short, the tasks should be those that a human would want to achieve next, given the current testing state.</p>
<p>Generating a viable but also diverse task is crucial, or the exploration risks being stuck trying to repeatedly perform impossible, irrelevant, or already achieved tasks. We achieve this by combining information from three different sources into a prompt for the planning LLM agent:</p>
<p>1) High-level task history: To continuously generate diverse and consistent tasks, the planner should be aware of the past exploration history. Instead of history of GUI actions, the planner is provided with textual summaries of $N$ (20 in our experiment) most recent tasks, and $M$ (five in our experiment) most relevant task knowledge. The historical information is inserted into long-term memory by the Reflector, described in III-C below, and then retreived and assembled by the task retriever, described in III-D1.
2) Total and visited activities: DROIDAGENT maintains the number of times each activity has been visited in the spatial memory, and the Widget retriever then includes the list of covered/uncovered activities as a proxy for the exploration progress together with information about all activities as well as the activity of the current state.
3) Initial knowledge: DROIDAGENT is designed to be initialised with initial knowledge. In default mode, a profile of a virtual user (persona), the ultimate “goal” of the persona, and a sentence denoting the beginning of the exploration, [PERSONA] started [APP_NAME] is used.</p>
<p>In our experiment, we include the following goal description to facilitate overall diversity: [PERSONA]’s ultimate goal is to visit as many pages as possible and try their core functionalities. However, one can customise the goal to adapt to different exploration and testing modes as well. Assuming a social messenger app, the human tester invoking DROIDAGENT could provide a goal such as: [PERSONA]’s ultimate goal is to check whether the app supports interactions between multiple users.</p>
<p>We additionally guide the LLM implementing the Planner agent to further consider several generally desirable properties when generating a new task: diversity (the task should cover new functionality), realism (the task should be possible on the app), difficulty (the task should be feasible in the fixed action length limit), and importance (the task using core and basic functions of the app should be prioritised).</p>
<h2>B. Actor and Observer</h2>
<p>The short-term working memory stores the current task’s execution history, and is cleared after each planning step to register the newly planned task. Both the Actor and Observer access it to retrieve context, and save their actions and observations.</p>
<p>1) Function-call based action selection: Actor chooses the appropriate next action to achieve a given task. To reduce the number of tokens in LLM prompts, the Actor is given a set of action types (such as “touch” or “set_text”) as well as a list of widgets in the current screen, instead of combinations of them. Actor subsequently selects an Android action type, such as “touch” or “set_text”, as well as a target widget to apply the action to. The prior actions, most recent observation (response) of the application, and the critique of recent progress (explained below), are also provided to be considered when recommending the next action to try.
2) Additional action types: Actor also supports three types of actions not directly derived from the widgets of the current page: “wait”, “back”, and “end_task”. Mobile testing tools have struggled with detecting loading screens, often using prolonged wait times after each action. The loading screen’s presence can be identified by checking for loading messages or icon resource identifiers, and we discovered that the LLMs we used can quite effectively detect loading screens and decide to wait. So, instead of a fixed long wait, we let the Actor decide when to wait. The “back” action is for navigating back, and the “end_task” action allows the Actor to conclude the task before the fixed max action limit (13 in our experiments).
3) Observing and summarising the outcome(s): The state of GUI may change after taking an action. DROIDAGENT</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Overview of DROIDAGENT with a task example.</p>
<p>updates its perception of a screen with a structured textual representation (JSON). However, for the Actor to capture the current task context, it needs to be informed about the outcome of the previous action. We use a separate Observer agent to summarise the pertinent outcome of an action based on a diff of the prior and updated GUI states represented as multi-line strings. This is because representing both the prior and updated state would lead to long prompts that may confuse the LLM.</p>
<p>4) <strong>Self-critique:</strong> The Actor may not always choose the desired action. Once Actor starts down a wrong path by initiating an undesirable action, it becomes challenging to "escape" from that incorrect exploration trajectory. Therefore, besides offering action results as observations, we incorporate an additional element called "self-critique" into the Actor of DROIDAGENT. Periodically (after every three actions in the experiments), the self-critique element generates feedback based on the task execution history up to that point and the current GUI state description. This involves a separate prompt, which explicitly asks for both a review of the task execution history and, if the Actor appears to be struggling, a suggested workaround plan. The prompt is sent to a more advanced model, GPT-4, while the "main" conversation querying the next action is handled by GPT-3.5. Consequently, the generated critique is injected to the Actor's prompting context for selecting the next action.</p>
<h3><em>C. Task Reflector</em></h3>
<p>Once a task execution round finishes, either by the Actor calling the "end_task" function or reaching the maximum action length limit, Reflector is activated to reflect on and create a concise description of the results of trying to perform the task (binary label indicating task success or failure as well). The input to this process is the entire task execution history including the self-critique and all observations from the working memory, the current GUI state, and the ultimate goal (from task planning). We instruct the Reflector to "derive memorable reflections to help planning next tasks and to be more effective to achieve the ultimate goal". We found that this elaborate reflection process can help avoid that the overall system "forgets" useful knowledge acquired during task execution, given that individual agents summarise their knowledge. We also found that having different agents focused on specific tasks also helps avoid that involved LLM instances drifts from their purpose, i.e. starts hallucinating or straying from their intended function.</p>
<h3><em>D. Memory Retrieval Modules</em></h3>
<p>1) <strong>Task Retriever:</strong> Long-term memory contains a history of performed task, i.e. task-specific knowledge as well as reflections on whether the task succeeded (indicates this task is supported by the app) or not. DROIDAGENT uses the textualised GUI state (basically a concatenation of widget properties) captured at task initiation as key/query for storing/</p>
<p>retrieving task knowledge. This also allows the Planner to obtain task knowledge derived from past task executions from similar GUI states, helped by the task retriever.
2) Widget Retriever: In manual GUI exploration, a tester accumulates knowledge about the AUT by interacting with widgets. Similarly, we introduce a memory type akin to human spatial memory, which lets the agent "remember" observations after interacting with a widget and to recall them for future interactions. To achieve this, the Observer stores the observations not only to the "volatile" working memory, but also to a specific widget knowledge (spatial) memory. Each widget is assigned with a signature containing the activity it is contained in, textual properties such as resource_id, content_description, and text ${ }^{2}$; the signature of the target widget is used as a primary key to store and retrieve the observation. Additionally, the embedding of the GUI state serves as a secondary key in order to fetch observations from the actions performed from a similar state to the current one. To limit the amount of information that goes into the prompts the widget retriever summarises the $N$ most relevant observations ( 5 in our experiments), per widget. This summary is then added to the widget dictionary in the JSON-formatted GUI state, under the property widget_role_inference.</p>
<h2>IV. An Illustrative Example</h2>
<p>In this section, we illustrate the execution of DROIDAGENT with an example. We use the "AnkiDroid" as the target application, a flashcard app for memorising various information. We inject a basic profile of a specific user persona, named "Jade Green" into the prompt of the Planner and Actor model, with account credentials associated with the app.</p>
<h2>A. Planning a new task</h2>
<p>The exploration begins with the task planning phase by Planner, where it sets a plan based on its goal and desirable properties of the task. The answer format is enforced by a template in the prompt; generated answer contains a reasoning step, a suggested task, and an end condition of the task. The end condition is further provided to Actor with the task itself to help Actor decide whether the task is accomplished or not. The following are the actual reasoning steps produced by DroidAgent, for the task "Create a new flashcard":</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Reasoning</span><span class="w"> </span><span class="nv">about</span><span class="w"> </span><span class="nv">Jade</span><span class="w"> </span><span class="nv">Green</span><span class="err">&#39;s new task: Considering the realism,</span>
<span class="err">    importance, diversity, and difficulty of the tasks, Jade Green</span>
<span class="err">    should continue adding flashcards to the newly created &quot;My Deck&quot;.</span>
<span class="err">    This is a realistic and important task, as it is a basic function</span>
<span class="err">    of the AnkiDroid app and it also helps Jade Green to diversify</span>
<span class="err">    their activities within the app. This task is not too difficult,</span>
<span class="err">    as Jade Green has already started this process and is familiar</span>
<span class="err">    with the NoteEditor page.</span>
<span class="nv">Jade</span><span class="w"> </span><span class="nv">Green</span><span class="err">&#39;s next task: Create a new flashcard in the &quot;My Deck&quot; deck</span>
<span class="err">    with the question &quot;What is the capital city of France?&quot; and the</span>
<span class="err">    answer &quot;Paris&quot;.</span>
<span class="k">End</span><span class="w"> </span><span class="nv">condition</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">Jade</span><span class="w"> </span><span class="nv">Green</span><span class="err">&#39;s next task: The task is known to be</span>
<span class="err">    completed when a new flashcard with [...truncated...] is</span>
<span class="err">    successfully added to the &quot;My Deck&quot; deck.</span>
</code></pre></div>

<p>${ }^{2}$ The text property of an editable textfield is omitted; if a widget does not contain any stable textual properties, we use its "bounds" property instead, which corresponds to the position and size of the widget.</p>
<h2>B. Executing the task</h2>
<p>Once a task is registered, Actor generates a sequence of GUI actions to achieve the task. The taken action and the observation, stored in the working memory, are again tied together as a virtual conversation thread between the app user and the agent. The following example is the excerpt from such a conversation thread. The texts for User are generated based on the action results from the Observer, whereas the "virtual" responses of Assistant are the stringified GUI actions previously performed. On the last User message, the current screen description, with the widget knowledge is appended.</p>
<div class="codehilite"><pre><span></span><code><span class="o">*</span><span class="w"> </span><span class="nx">User</span><span class="p">:</span><span class="w"> </span><span class="nx">My</span><span class="w"> </span><span class="nx">name</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">Jade</span><span class="w"> </span><span class="nx">Green</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">am</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">application</span><span class="w"> </span><span class="nx">named</span>
<span class="w">    </span><span class="nx">AnkiDroid</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">accomplish</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">task</span><span class="p">:</span><span class="w"> </span><span class="nx">Create</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">flashcard</span>
<span class="w">    </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="s">&quot;My Deck&quot;</span><span class="w"> </span><span class="nx">deck</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="p">[</span><span class="o">...</span><span class="nx">truncated</span><span class="o">...</span><span class="p">]</span><span class="w"> </span><span class="nx">What</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">first</span><span class="w"> </span><span class="nx">action</span><span class="p">?</span>
<span class="o">*</span><span class="w"> </span><span class="nx">Assistant</span><span class="p">:</span><span class="w"> </span><span class="nx">Fill</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">textfield</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">has</span><span class="w"> </span><span class="nx">content_desc</span><span class="w"> </span><span class="s">&quot;Front&quot;</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="s">&quot;What</span>
<span class="s">    is the capital city of France?&quot;</span>
<span class="o">*</span><span class="w"> </span><span class="nx">User</span><span class="p">:</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">performed</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">suggested</span><span class="p">.</span><span class="w"> </span><span class="nx">What</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">next</span>
<span class="w">    </span><span class="nx">action</span><span class="p">?</span>
<span class="o">*</span><span class="w"> </span><span class="nx">Assistant</span><span class="p">:</span><span class="w"> </span><span class="nx">Fill</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">textfield</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">has</span><span class="w"> </span><span class="nx">content_desc</span><span class="w"> </span><span class="s">&quot;Back&quot;</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="s">&quot;Paris&quot;</span>
<span class="o">*</span><span class="w"> </span><span class="nx">User</span><span class="p">:</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">performed</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">action</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">result</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">textfield</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">had</span>
<span class="w">    </span><span class="nx">the</span><span class="w"> </span><span class="nx">content_desc</span><span class="w"> </span><span class="s">&quot;Back&quot;</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="nx">filled</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="p">[</span><span class="o">...</span><span class="nx">truncated</span><span class="o">...</span><span class="p">]</span><span class="w"> </span><span class="nx">This</span>
<span class="w">    </span><span class="nx">time</span><span class="p">,</span><span class="w"> </span><span class="nx">I</span><span class="err">&#39;</span><span class="nx">ll</span><span class="w"> </span><span class="nx">give</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">full</span><span class="w"> </span><span class="nx">content</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">page</span><span class="w"> </span><span class="k">as</span>
<span class="w">    </span><span class="nx">follows</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nx">page_name</span><span class="p">:</span><span class="w"> </span><span class="nx">NoteEditor</span><span class="p">,</span>
<span class="w">        </span><span class="nx">page_</span><span class="w"> </span><span class="nx">visit_count</span><span class="p">:</span><span class="w"> </span><span class="mi">11</span><span class="p">,</span>
<span class="w">        </span><span class="nx">children</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">            </span><span class="nx">ID</span><span class="p">:</span><span class="w"> </span><span class="mi">11</span><span class="p">,</span>
<span class="w">            </span><span class="nx">widget_type</span><span class="p">:</span><span class="w"> </span><span class="nx">TextView</span><span class="p">,</span>
<span class="w">            </span><span class="nx">content_description</span><span class="p">:</span><span class="w"> </span><span class="nx">Save</span><span class="p">,</span>
<span class="w">            </span><span class="nx">possible_action_types</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="nx">touch</span><span class="p">,</span><span class="w"> </span><span class="nx">long_touch</span><span class="p">],</span>
<span class="w">            </span><span class="nx">num_prev_actions</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">            </span><span class="nx">widget_role_inference</span><span class="p">:</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">widget</span><span class="w"> </span><span class="nx">allows</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">user</span><span class="w"> </span><span class="nx">to</span>
<span class="w">                </span><span class="nx">save</span><span class="w"> </span><span class="nx">their</span><span class="w"> </span><span class="nx">inputs</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">add</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">cards</span><span class="p">,</span><span class="w"> </span><span class="nx">possibly</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">a</span>
<span class="w">                    </span><span class="nx">note</span><span class="o">-</span><span class="nx">taking</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">flashcard</span><span class="w"> </span><span class="nx">application</span><span class="p">.</span>
<span class="w">        </span><span class="p">[</span><span class="o">...</span><span class="nx">truncated</span><span class="o">...</span><span class="p">]</span>
<span class="w">        </span><span class="nx">select</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">next</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">end</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">calling</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span>
<span class="w">            </span><span class="nx">given</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">corresponds</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">specific</span><span class="w"> </span><span class="nx">action</span><span class="p">.</span>
</code></pre></div>

<p>Given the conversation thread, the model responds by choosing the next action by calling the corresponding function. The chosen action, touching the "Save" button, is then converted to an actual GUI event and relayed to the app.</p>
<div class="codehilite"><pre><span></span><code>{
    &quot;name&quot;: &quot;touch&quot;,
    &quot;arguments&quot;: {&quot;target_widget_ID&quot;: 11}
}
</code></pre></div>

<h2>C. Criticising the task execution</h2>
<p>The critique component periodically activates and generates feedback from the full task execution history as well as the current GUI state description. As other components, the generation of critique is enforced to follow a certain template of reasoning step. We demonstrate an actual critique example, pointing out that nothing has been added to the desired deck.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Critique</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">task</span><span class="w"> </span><span class="nv">execution</span><span class="w"> </span><span class="nv">so</span><span class="w"> </span><span class="nv">far</span>:<span class="w"> </span><span class="nv">Jade</span><span class="w"> </span><span class="nv">Green</span><span class="w"> </span><span class="nv">has</span><span class="w"> </span><span class="nv">correctly</span><span class="w"> </span><span class="nv">filled</span><span class="w"> </span><span class="nv">in</span>
<span class="w">    </span><span class="nv">the</span><span class="w"> </span><span class="nv">question</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">fields</span>,<span class="w"> </span><span class="nv">but</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">flashcard</span><span class="w"> </span><span class="nv">seems</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">be</span>
<span class="w">    </span><span class="nv">saved</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="s2">&quot;Default&quot;</span><span class="w"> </span><span class="nv">deck</span><span class="w"> </span><span class="nv">instead</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">intended</span><span class="w"> </span><span class="s2">&quot;My Deck&quot;</span>.
<span class="w">    </span><span class="nv">This</span><span class="w"> </span><span class="nv">might</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">happened</span><span class="w"> </span><span class="nv">because</span><span class="w"> </span><span class="nv">Jade</span><span class="w"> </span><span class="nv">Green</span><span class="w"> </span><span class="nv">changed</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">selected</span>
<span class="w">    </span><span class="nv">deck</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="s2">&quot;My Deck&quot;</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="s2">&quot;Default&quot;</span><span class="w"> </span><span class="nv">after</span><span class="w"> </span><span class="nv">creating</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">flashcard</span>.
<span class="nv">Need</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">workaround</span><span class="w"> </span><span class="nv">plan</span>?:<span class="w"> </span><span class="nv">Yes</span>
<span class="nv">Workaround</span><span class="w"> </span><span class="nv">plan</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">Jade</span><span class="w"> </span><span class="nv">Green</span>:<span class="w"> </span><span class="nv">Jade</span><span class="w"> </span><span class="nv">Green</span><span class="w"> </span><span class="nv">needs</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">ensure</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span>
<span class="w">    </span><span class="nv">correct</span><span class="w"> </span><span class="nv">deck</span><span class="w"> </span><span class="ss">(</span><span class="s2">&quot;My Deck&quot;</span><span class="ss">)</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">selected</span><span class="w"> </span><span class="nv">before</span><span class="w"> </span><span class="nv">saving</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">flashcard</span>.
</code></pre></div>

<h3><em>D. Reflecting on the task</em></h3>
<p>At the end of the task execution, the reflector generates a summary of the task result, and derives the knowledge from the task execution history. The following is the example of the task result and knowledge derivation with the given template for the step-by-step reflection.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Summary</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">task</span><span class="w"> </span><span class="nb">result</span>:<span class="w"> </span><span class="nv">Jade</span><span class="w"> </span><span class="nv">Green</span><span class="w"> </span><span class="nv">successfully</span><span class="w"> </span><span class="nv">created</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">new</span>
<span class="w">    </span><span class="nv">flashcard</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="s2">&quot;My Deck&quot;</span><span class="w"> </span><span class="nv">deck</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">question</span><span class="w"> </span><span class="err">&quot;What is the</span>
<span class="w">    </span><span class="nv">capital</span><span class="w"> </span><span class="nv">city</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">France</span>?<span class="err">&quot; [...truncated...]</span>
<span class="err">Task done successfully?: Yes</span>
<span class="err">Reflections on the task:</span>
<span class="err">- Jade Green has learned how to create a new flashcard by filling the &#39;</span>
<span class="err">    Front&#39; and &#39;Back&#39; fields with question and answer respectively and</span>
<span class="err">    then saving it.</span>
<span class="err">- Jade Green has learned that the app provides a dropdown field to</span>
<span class="err">    select the deck where the flashcard will be saved.</span>
<span class="err">- Jade Green has learned that the app shows a popup message indicating</span>
<span class="err">    the number of cards added.</span>
</code></pre></div>

<h3><em>E. Planning another task with learnt knowledge</em></h3>
<p>As an exploration continues on and discovers more widgets and activities, ideally, the task planning process should benefit from information about the past task executions. The subsequent task, "attaching a photo to the flashcard", illustrated in Figure 1, is based on previously creating a new flashcard. The new task derives new knowledge from the observation that the app allows users to add images, audio clip, recording audio, and so on, to the flashcard. In future task planning, Planner retrieves such knowledge and generate a new task, "Attach an audio clip to the flashcard" with the following reasoning steps.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Reasoning</span><span class="w"> </span><span class="nv">about</span><span class="w"> </span><span class="nv">Jade</span><span class="w"> </span><span class="nv">Green</span><span class="err">&#39;s new task: To provide a diverse and</span>
<span class="err">    realistic task that makes use of the core functionality of the app</span>
<span class="err">    , Jade Green should try to add an audio clip to a flashcard, which</span>
<span class="err">        is an important feature of AnkiDroid to enhance learning</span>
<span class="err">        efficiency. This task is not too difficult as it is similar to the</span>
<span class="err">            previous task of adding an image to a flashcard.</span>
<span class="nv">Jade</span><span class="w"> </span><span class="nv">Green</span><span class="err">&#39;s next task: Add an audio clip to a flashcard.</span>
</code></pre></div>

<h3>V. EVALUATION</h3>
<p>This section describes our experimental setup.</p>
<h4><em>A. Research Questions</em></h4>
<p>Our evaluation aims to answer the following questions.</p>
<p><em>1) RQ1. Testing Effectiveness:</em> How does DROIDAGENT compare to existing exploration techniques in exploring diverse functions within a limited time budget? With RQ1, we aim to assess the diversity and depth of DROIDAGENT's exploration, primarily based on screen coverage.</p>
<p><em>2) RQ2. Usefulness:</em> How effectively do the tasks generated by DROIDAGENT serve as maintainable testing scenarios, reflecting the supported functionalities of AUTs? With RQ2, we aim to find out whether the tasks generated by DROIDAGENT are useful as valid test scenarios, which can be used for regression testing or further test case generation.</p>
<p><em>3) RQ3. Ablation:</em> How does each component of the agent architecture impact the agent's exploration effectiveness? With RQ3, we aim to assess the contribution of each component of the agent architecture to the overall exploration effectiveness.</p>
<p><em>4) RQ4. Cost:</em> What is the monetary cost of running DROIDAGENT with the latest state-of-the-art large language models? With RQ4, we aim to present the present-day cost of running DROIDAGENT, and provide a view for adopting DROIDAGENT in practice.</p>
<h4><em>B. Experimental Setup</em></h4>
<p>In this section, we describe our experimental setup.</p>
<p><em>1) Subjects:</em> Table I shows the 15 subject apps we study. We start the app selection from the widely used Themis benchmark [14], which originally contains 23 open-source Android apps. We are forced to exclude eight apps due to deprecated servers or APIs, three apps whose functionalities depend heavily on remote servers and are not easily resettable, one app that crashes on startup, and another that has only a single activity. We selected five additional apps from FDroid [31] to broaden the range of our subject app categories.</p>
<p><em>2) Metrics:</em> Our primary metric is screen coverage, with a specific focus on activity coverage in Android serving as an indicator for exploration diversity. Activity coverage is typically defined by the number of activities accessed during the exploration of the AUT. We only take account of internal activities that include the package name of the target application, since there can be external activities that do not represent any accessible screens within the AUT (they typically exist to detect memory leaks or to perform crash reports).</p>
<p>While activity coverage is widely used and effective in evaluating the "breadth" of exploration, it doesn't necessarily capture the desired "depth" of the exploration. For instance, an exploration technique might navigate to a specific activity, it may also return to the previous one without any additional interaction. To further evaluate if the test cases generated by each technique encompass the target app's comprehensive functionality, we employ the concept of "feature coverage". This represents the fraction of functional features covered by test cases, as delineated in the taxonomy suggested by Coppola et al. [32]. Given that we do not have precise specifications for the subject apps, we categorise all discerned functional features of each app identified by all comparison target techniques, until the consensus of three authors. We then report the number of features covered by each technique.</p>
<p><em>3) Baselines:</em> We compare DROIDAGENT with the following four baselines described below:</p>
<ul>
<li><strong>Monkey</strong> [15]: Monkey is a widely used random Android GUI exploration tool for Android.</li>
<li><strong>DroidBot</strong> [4]: DroidBot is a systematic input generation tool for Android GUI exploration.</li>
<li><strong>Humanoid</strong> [1]: Humanoid incorporates a deep neural network model trained using real-world human interactions and produces a sequence of GUI actions.</li>
<li><strong>GPTDroid</strong> [11]: GPTDroid interacts with an LLM in a chat-like fashion to produce a series of GUI actions.</li>
</ul>
<p>Note that, since GPTDroid does not provide a replication package, we reimplemented it based on the description in the paper. However, one of their component is a distinct local language model that converts the natural-language LLM</p>
<p>TABLE I: ANDROID APPLICATIONS USED IN DROIDAGENT'S EVALUATION.</p>
<table>
<thead>
<tr>
<th>App Name</th>
<th>App ID</th>
<th>From</th>
<th>Category</th>
<th># of Activity</th>
<th>App Name</th>
<th>App ID</th>
<th>From</th>
<th>Category</th>
<th># of Activity</th>
</tr>
</thead>
<tbody>
<tr>
<td>ActivityDiary</td>
<td>AD</td>
<td>Themis</td>
<td>Personal Diary</td>
<td>10</td>
<td>openlauncher</td>
<td>OP</td>
<td>Themis</td>
<td>App Launcher</td>
<td>7</td>
</tr>
<tr>
<td>AnkiDroid</td>
<td>AK</td>
<td>Themis</td>
<td>Card Learning</td>
<td>22</td>
<td>osmeditor4android</td>
<td>OM</td>
<td>Themis</td>
<td>Map</td>
<td>18</td>
</tr>
<tr>
<td>AntennaPod</td>
<td>AN</td>
<td>Themis</td>
<td>Podcast Manager</td>
<td>10</td>
<td>MateriaFB</td>
<td>MF</td>
<td>F-Droid</td>
<td>Social</td>
<td>4</td>
</tr>
<tr>
<td>Marker</td>
<td>MK</td>
<td>Themis</td>
<td>Text Editor</td>
<td>9</td>
<td>collect</td>
<td>CL</td>
<td>F-Droid</td>
<td>Form Data Collector</td>
<td>37</td>
</tr>
<tr>
<td>Omni-Notes</td>
<td>ON</td>
<td>Themis</td>
<td>Notebook</td>
<td>12</td>
<td>APhotoManager</td>
<td>AP</td>
<td>F-Droid</td>
<td>Photo Manager</td>
<td>9</td>
</tr>
<tr>
<td>Phonograph</td>
<td>PG</td>
<td>Themis</td>
<td>Music Player</td>
<td>12</td>
<td>MyExpenses</td>
<td>ME</td>
<td>F-Droid</td>
<td>Expense Tracking</td>
<td>40</td>
</tr>
<tr>
<td>Scarlet-Notes</td>
<td>SN</td>
<td>Themis</td>
<td>Notebook</td>
<td>8</td>
<td>OpenTracks</td>
<td>OT</td>
<td>F-Droid</td>
<td>Sports &amp; Health</td>
<td>24</td>
</tr>
<tr>
<td>commons</td>
<td>CM</td>
<td>Themis</td>
<td>Wikimedia</td>
<td>17</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>TABLE II: NUMBER OF COVERED ACTIVITIES PER APP BY EACH TECHNIQUE</p>
<table>
<thead>
<tr>
<th>Subjects</th>
<th>DroidAgent</th>
<th>DroidBot</th>
<th>GPTDroid</th>
<th>Humanoid</th>
<th>Monkey</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>APhotoManager</td>
<td>5</td>
<td>5</td>
<td>4</td>
<td>5</td>
<td>5</td>
<td>9</td>
</tr>
<tr>
<td>ActivityDiary</td>
<td>10</td>
<td>3</td>
<td>6</td>
<td>5</td>
<td>5</td>
<td>10</td>
</tr>
<tr>
<td>AnkiDroid</td>
<td>15</td>
<td>14</td>
<td>6</td>
<td>13</td>
<td>13</td>
<td>22</td>
</tr>
<tr>
<td>AntennaPod</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>5</td>
<td>3</td>
<td>10</td>
</tr>
<tr>
<td>Marker</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>5</td>
<td>5</td>
<td>9</td>
</tr>
<tr>
<td>MateriaFB</td>
<td>3</td>
<td>1</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>4</td>
</tr>
<tr>
<td>MyExpenses</td>
<td>15</td>
<td>7</td>
<td>12</td>
<td>7</td>
<td>11</td>
<td>40</td>
</tr>
<tr>
<td>Omni-Notes</td>
<td>5</td>
<td>3</td>
<td>5</td>
<td>6</td>
<td>3</td>
<td>12</td>
</tr>
<tr>
<td>OpenTracks</td>
<td>16</td>
<td>7</td>
<td>11</td>
<td>10</td>
<td>16</td>
<td>24</td>
</tr>
<tr>
<td>Phonograph</td>
<td>11</td>
<td>7</td>
<td>6</td>
<td>9</td>
<td>9</td>
<td>12</td>
</tr>
<tr>
<td>Scarlet-Notes</td>
<td>3</td>
<td>3</td>
<td>4</td>
<td>3</td>
<td>3</td>
<td>8</td>
</tr>
<tr>
<td>collect</td>
<td>13</td>
<td>12</td>
<td>2</td>
<td>9</td>
<td>9</td>
<td>37</td>
</tr>
<tr>
<td>commons</td>
<td>14</td>
<td>11</td>
<td>7</td>
<td>12</td>
<td>5</td>
<td>17</td>
</tr>
<tr>
<td>openlauncher</td>
<td>6</td>
<td>2</td>
<td>3</td>
<td>3</td>
<td>4</td>
<td>7</td>
</tr>
<tr>
<td>osmeditor4android</td>
<td>9</td>
<td>5</td>
<td>6</td>
<td>12</td>
<td>8</td>
<td>18</td>
</tr>
<tr>
<td>Total</td>
<td>133</td>
<td>87</td>
<td>80</td>
<td>107</td>
<td>101</td>
<td>259</td>
</tr>
</tbody>
</table>
<p>response into a GUI event. The construction of the model requires a large amount of labelled data, which we could not replicate in our experimental context. Therefore, we replaced this component with a function call-based action selector, the same we employed in implementing Actor in DroidAgent. Instead of the GPT-3 model mentioned in the original paper, we used the GPT-3.5 (16K context) model. We refer to this reimplemented version as GPTDroid in the rest of the paper.</p>
<p>For each tool, we allocate a two-hour exploration budget. We set up an emulator (Nexus 7, API 25) with 2GB RAM and a 1GB SDCard. Each tool runs on a 64-bit Ubuntu 20.04 machine with an i7-1075H CPU (12 cores) and 32GB memory.</p>
<p>4) Large Language Models: We use GPT-3.5 model with extended 16K context length (gpt-3.5-turbo-0613-16k from OpenAI) for the action selection model implementing Actor and Observer of DroidAgent. The summarisation process of the widget knowledge retriever, as discussed in Section III-D2, requires a shorter prompting context, so we employ the standard GPT-3.5 model with a 4K context (gpt-3.5-turbo-0613) for this module. For the components crucial to outcomes, specifically the Planner, Reflector, and self-critique module of the Actor agent, we employ the GPT-4 model (gpt-4-0613).</p>
<h2>VI. RESULTS</h2>
<p>We present the results of our evaluation in this section.</p>
<h3>A. Testing Effectiveness (RQ1)</h3>
<p>Table II displays the number of activities covered by DroidAgent compared to other testing techniques for each</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Example of distinct exploration patterns on MyAccount activities from AnkiDroid application between Humanoid and DroidAgent.</p>
<p>application. On average, DroidAgent achieves an activity coverage of 60.7%, slightly exceeding the best baseline, Humanoid, with an average coverage of 51.4%. A Wilcoxon signed rank test indicated that the number of activities covered by DroidAgent was statistically significantly higher than those covered by Humanoid (p &lt; 0.045). For those applications that DroidAgent discovers significantly more activities than other baselines, the functionalities supported by the applications are relatively intuitive and follow the common sense. However, DroidAgent finds it more difficult to visit more activities compared to other baselines against some specific AUTs: our analysis shows that these apps either have widgets that do not have any textual properties (e.g., the widgets in Scarlet-Notes consist of only icons or images without content descriptions), or has a single view containing distinct interactable subregions (e.g., Google Map view on osmeditor4android).</p>
<p>Figure 3 depicts the change in activity coverage over time. We observe the trend on the AnkiDroid app as a representative, in which the techniques including DroidAgent show a similar degree of activity coverage after two hours. Humanoid shows a relatively higher growth rate in the first 30 minutes compared to others, but it fails to discover more activities afterwards. Figure 2 illustrates one reason for this difference by showing a different exploration patterns of DroidAgent and baselines on the same activity accessed, MyAccount, in the AnkiDroid application. This activity can be easily covered by clicking "Synchronization" button on the main app screen, but the actual synchronisation with the server requires logging into the application first. DroidAgent succeeds to automatically sign into the application with the given profile,</p>
<p>TABLE III: CRASH DETECTION RESULTS</p>
<table>
<thead>
<tr>
<th>Subjects</th>
<th>DROIDAGENT</th>
<th>DroidBot</th>
<th>GPTDroid</th>
<th>Humanoid</th>
<th>Monkey</th>
</tr>
</thead>
<tbody>
<tr>
<td>APhotoManager</td>
<td>O</td>
<td>O</td>
<td>×</td>
<td>O</td>
<td>O</td>
</tr>
<tr>
<td>ActivityDiary</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>AnkiDroid</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>AntennaPod</td>
<td>O</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>Marker</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>O</td>
<td>×</td>
</tr>
<tr>
<td>MaterialFB</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>O</td>
</tr>
<tr>
<td>MyExpenses</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>Omni-Notes</td>
<td>O</td>
<td>×</td>
<td>O</td>
<td>O</td>
<td>O</td>
</tr>
<tr>
<td>OpenTracks</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>Phonograph</td>
<td>O</td>
<td>O</td>
<td>×</td>
<td>O</td>
<td>O</td>
</tr>
<tr>
<td>Scarlet-Notes</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>collect</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>commons</td>
<td>O</td>
<td>O</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>openlauncher</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>oenieditor4android</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>Total</td>
<td>5</td>
<td>3</td>
<td>1</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Activity coverage measured by time for two hours of exploration. (Application name: AnkiDroid)</p>
<p>and wait for the synchronisation to be completed by selecting "Wait" action. On the other hand, Humanoid just triggers "BACK" action without any interactions on the activity.</p>
<p>Figure 4 compares the feature coverage between DROIDAGENT and Humanoid, the best-performing baseline in activity coverage. DROIDAGENT consistently covers a significantly larger number of features than Humanoid (DROIDAGENT average: 13.9, Humanoid average: 7.3) across all subjects; this holds even where Humanoid has higher activity coverage. The result suggests that DROIDAGENT doesn't just navigate the activities, but also engages in meaningful interactions to encompass the features of the AUTs. A Wilcoxon signed rank test indicated that the number of features covered by DROIDAGENT was statistically significantly higher than those covered by Humanoid (p &lt; 0.0008).</p>
<p>Additionally, we report the crashes found by each technique in Table III. DROIDAGENT finds five crashes in total (one of them had been reported as a past GitHub issue), while Humanoid finds four crashes in total (one of them had been reported). It's worth noting that the current goal setting of DROIDAGENT, as outlined in Section III-A3, focuses on the efficient exploration by covering core functionalities within a fixed time budget.</p>
<p>Nevertheless, DROIDAGENT still shows crash finding capability on par with other baselines, suggesting its potential as a viable GUI testing technique. Further, we argue that the crash (which is exposed as an abrupt closing of the app) observed during a specific meaningful task, generated by DROIDAGENT, is more readily understood by developers compared to those found during a lengthy yet meaningless GUI exploration. For instance, in the "commons" app, the application crashes when one attempts to upload a picture and then cancels the process mid-upload. The crash can be presented to a developer along with a task description of image uploading, which we believe aids in more effectively reproducing the issue.</p>
<h3>B. Usefulness of Generated Testing Scenarios (RQ2)</h3>
<p>We answer RQ2 by assessing the viability and reliability of the generated tasks. We first present quantitative results for number of tasks, viability, and reliability, and then present a qualitative case study of some generated tasks.</p>
<p>1) RQ2-1. Task statistics: DROIDAGENT planned and executed on average 36 tasks (standard deviation: 8.8) per AUT. Figure 5 shows the distribution of the number of tasks generated per each application, and the number of actions (i.e., task length) that have been taken to complete each task. Although we set 13 as a maximum number of actions per task, the average task lengths vary across applications (min: 7.4, max: 11.2, mean: 8.9), as the Actor of DROIDAGENT can end the task earlier.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Comparison of the number of covered features by DROIDAGENT and Humanoid.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. The number of tasks per application and number of actions per task.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. Manual assessment result of the generated tasks by DROIDAGENT.</p>
<p>2) RQ2-2. Task reliability: DROIDAGENT's Reflector labels the task result based on whether the task was successfully</p>
<p>completed or not. To assess this classification of task results, we have manually checked and labeled the completion status of each task, as well as whether the tasks are viable with the AUT: a task is viable when it follows the supported functionality of the app, and completed when the relevant functionalities of the apps are utilised by the Actor. Figure 6 shows that, among all 374 unique tasks generated for 15 applications, we deemed 85% as viable, and 59% as completed by DROIDAGENT. Based on this manual labelling, we report that the Reflector achieves a relatively high level of accuracy in task result assessment, with precision of 0.72, recall of 0.77, and F1 score of 0.74.</p>
<p>3) RQ2-3. Case study: As a case study, we present a couple of tasks generated by DROIDAGENT. By producing test sequences in association with "task", DROIDAGENT can create complex multi-task scenarios.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. Example of subsequent tasks consistently reusing generated test data.</p>
<p>Case 1: Reusing created app data: We observe that DROIDAGENT is able to reuse app data created during the exploration. Figure 7 illustrates two consecutive tasks in the "ActivityDiary" app. DROIDAGENT tries to search for a specific activity and successfully inputs a valid query, "Gym Workout", which is an activity name created during a previous task. Subsequently, DROIDAGENT verifies that the targeted activity appears on the screen and proceeds to view its details.</p>
<p>Such patterns of reusing previously created internal app data are commonly observed in our subject applications. For instance, in the "AnkiDroid" app, tasks like reviewing the flashcard and rating the difficulty were conducted after creating a new flashcard. Compared to DROIDAGENT, baseline techniques often struggle to access functionalities that require specific pre-existing app data. For example, they might search for an internal item using a query that is irrelevant and does not yield any search results.</p>
<p>Case 2: Login Automation: Two of our subjects (commons, MaterialFB) require login steps at startup to access main functionalities. For baseline techniques, as done in prior research, we use login scripts (or setup scripts for simply</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. Average of activity coverage for each ablation setting.</p>
<p>skipping it). These scripts are either source from the benchmark (Themis) or crafted by the authors (FDroid subjects), aiming at simply bypassing the process. Conversely, DROIDAGENT can autonomously sign into both apps without these scripts when provided with the relevant account credentials in its persona profile, making the sign-in process seamlessly integrates into its exploration routine. DROIDAGENT also exhibits adaptability in handling the app's "hidden" login features, like the redirected login screen encountered during data synchronisation as previously highlighted (Figure 2). Such scenarios cannot be handled by the login script without adequate prior knowledge of the app's features. It's worth noting that even with automated login scripts, the login process might fail due to issues like temporary server errors during login requests. DROIDAGENT can address such flakiness by adaptively retrying the failed action (e.g., re-clicking the login button), making it more resilient.</p>
<p>C. Ablation (RQ3)</p>
<p>Figure 8 compares the activity coverage of each of DROIDAGENT's ablation settings. We selected a subset of our subjects with more than ten activities. The "NoKnowledge" setting refers to the DROIDAGENT that excludes the use of knowledge retrievers. While it incorporates DROIDAGENT's Planner and Reflector, the task retriever does not supply task knowledge to the Planner. Furthermore, the GUI state descriptions given to the Actor and Planner lack widget knowledge, as the retriever is disabled. Similarly, the "NoKnowledgeAndCritique" setting refers to the DROIDAGENT that additionally excludes the use of the self-critique module of the Actor agent. Finally, the "Actor-only" setting refers to the DROIDAGENT that only utilises the Actor without self-critique module. It operates without registered task, consistently generating GUI actions based on the current GUI state and recent actions. Both the presence of knowledge retrievers and self-critique module seem to positively enhance DROIDAGENT's effectiveness in exploring the broader parts of the application.</p>
<p>D. Cost (RQ4)</p>
<p>Having demonstrated DROIDAGENT's ability to effectively explore app screens, a vital question arises: what is the cost of running the agent for app exploration and testing? We measured the total number of tokens contained in the prompt and the generated output both for GPT-3.5 and GPT-4 models, as shown in Figure 9. The number of tokens for the prompt depend on the complexity of GUI layout of each application.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9. Number of tokens for GPT-3.5/GPT-4 model input/output (log scale).</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10. Example of testing scenarios by DROIDAGENT for creating multiple accounts in a simple chat app.</p>
<p>Accordingly, the present-time cost for running DROIDAGENT on a single application with a two-hour budget ranges between $13 to $22, summing up the cost from both the GPT-3.5 and GPT-4 models, averaging $18.1. Given the trend of decreasing cost per token charged by OpenAI, as well as the rapid advancements of open source LLMs, we expect the cost of running DROIDAGENT to be reduced and affordable.</p>
<h2>VII. DISCUSSION</h2>
<p>This section describes a couple of observed behaviour of DROIDAGENT that warrants some discussion and future work.</p>
<h3>A. Testing social applications</h3>
<p>So far, testing of social applications that would require multiple accounts has been considered out of scope for the existing exploration techniques. We demonstrate the potential of applying DROIDAGENT on testing multi-user interactions in Figure 10, which contains testing scenarios generated by DROIDAGENT with a custom goal of "testing multiple user interactions". The first account created follows the persona profile, and the credentials for the second account is newly synthesised as a variation of the persona profile. Moreover, while creating the second account, DROIDAGENT encounters a truncated email address due to the length limit of the textfield, but later it successfully works around the issue by using a shorter email address.</p>
<h3>B. Testing external use of a mobile application</h3>
<p>A mobile application is not always used in isolation. In fact, it is both possible to temporarily navigate out of the app under test and return to the app (e.g., selecting a picture from the gallery app, share an app data via email), and start the app from the external app (e.g., opening a link from a browser). In the former case (temporary navigation to the external app), to avoid accidentally being out of the app too long, DROIDAGENT currently imposes a fixed interaction limit on external apps and returns to the target app automatically. However, we observe some cases that DROIDAGENT prematurely terminated essential interactions in the external app due to this limit. Additionally, some activities among the subject apps were exclusively triggered by external apps, such as the WidgetConfiguration activity, which is only accessed by an app launcher. By design, DROIDAGENT is not limited to the target app. Broadening DROIDAGENT's scope to test functionalities of AUT across multiple apps presents a promising avenue for future exploration.</p>
<h2>VIII. THREATS TO VALIDITY</h2>
<p><strong>Internal Validity.</strong> Our study might be affected by the inherent randomness associated with LLMs. Given the monetary constraints linked to API requests, we could not conduct multiple runs, potentially leaving biases. Additionally, one of the baselines, our version of GPTDroid, includes modifications to some of its components. In our implementation, we observed that the LLM context limit was reached post ten actions, forcing a reset of the preceding conversation prompt, an issue not tackled in the original paper.</p>
<p><strong>External Validity.</strong> Our study utilised a relatively limited set of benchmarks as well as underlying LLMs, and therefore may not generalise. We tried to use an existing benchmark of Android apps, Themis [14]. Further studies of more apps and other open source LLMs are needed to address this threat.</p>
<h2>IX. CONCLUSION</h2>
<p>We present DROIDAGENT, an autonomous testing agent for Android GUI testing. Unlike existing automated GUI testing tools for Android, DROIDAGENT sets its own meaningful tasks according to the functionalities of the app under test, and subsequently seeks to achieve them. Our empirical evaluation of DROIDAGENT against four baselines shows that DROIDAGENT is capable of exploring more Android activities on average, despite doing so while trying to achieve meaningful app specific tasks. DROIDAGENT also exhibits some novel behaviour, such as reusing data it created earlier for later interactions with the app, or creating multiple accounts to test the app. We believe autonomous agents can make significant contributions to automation of GUI testing.</p>
<h2>REFERENCES</h2>
<p>[1] Y. Li, Z. Yang, Y. Guo, and X. Chen, "Humanoid: A deep learningbased approach to automated black-box android app testing," in 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019, pp. 1070-1073.
[2] T. Su, G. Meng, Y. Chen, K. Wu, W. Yang, Y. Yao, G. Pu, Y. Liu, and Z. Su, "Guided, stochastic model-based gui testing of android apps," in Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, 2017, pp. 245-256.
[3] K. Mao, M. Harman, and Y. Jia, "Sapienz: Multi-objective automated testing for android applications," in Proceedings of the 25th international symposium on software testing and analysis, 2016, pp. 94-105.
[4] Y. Li, Z. Yang, Y. Guo, and X. Chen, "Droidbot: a lightweight ui-guided test input generator for android," in 2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C). IEEE, 2017, pp. 23-26.
[5] Y. Zheng, Y. Liu, X. Xie, Y. Liu, L. Ma, J. Hao, and Y. Liu, "Automatic web testing using curiosity-driven reinforcement learning," in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021, pp. 423-435.
[6] M. Pan, A. Huang, G. Wang, T. Zhang, and X. Li, "Reinforcement learning based curiosity-driven testing of android applications," in Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, 2020, pp. 153-164.
[7] Y. Zhao, B. Harrison, and T. Yu, "Dinodroid: Testing android apps using deep q-networks," arXiv preprint arXiv:2210.06307, 2022.
[8] M. Linares-Vásquez, C. Bernal-Cárdenas, K. Moran, and D. Poshyvanyk, "How do developers test android applications?" in 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2017, pp. 613-622.
[9] Z. Liu, C. Chen, J. Wang, X. Che, Y. Huang, J. Hu, and Q. Wang, "Fill in the blank: Context-aware automated text input generation for mobile gui testing," in 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023, pp. 1355-1367.
[10] H. Wen, H. Wang, J. Liu, and Y. Li, "Droidbot-gpt: Gpt-powered ui automation for android," arXiv preprint arXiv:2304.07061, 2023.
[11] Z. Liu, C. Chen, J. Wang, M. Chen, B. Wu, X. Che, D. Wang, and Q. Wang, "Chatting with gpt-3 for zero-shot human-like mobile automated gui testing," arXiv preprint arXiv:2305.09434, 2023.
[12] J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein, "Generative agents: Interactive simulacra of human behavior," arXiv preprint arXiv:2304.03442, 2023.
[13] R. Feldt, S. Kang, J. Yoon, and S. Yoo, "Towards autonomous testing agents via conversational large language models," arXiv preprint arXiv:2306.05152, 2023.
[14] T. Su, J. Wang, and Z. Su, "Benchmarking automated gui testing for android against real-world bugs," in Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2021, pp. 119-130.
[15] "Ui/application exerciser monkey," https://developer.android.com/studio/ test/other-testing-tools/monkey.
[16] "Introduction to activities," https://developer.android.com/guide/ components/activities/intro-activities.
[17] "Espresso: Android gui testing framework," https://developer.android. com/training/testing/espresso.
[18] Y. Zhao, T. Yu, T. Su, Y. Liu, W. Zheng, J. Zhang, and W. G. Halfond, "Recdroid: automatically reproducing android application crashes from bug reports," in 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019, pp. 128-139.
[19] T. Gu, C. Sun, X. Ma, C. Cao, C. Xu, Y. Yao, Q. Zhang, J. Lu, and Z. Su, "Practical gui testing of android applications via model abstraction and refinement," in 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019, pp. 269-280.
[20] P. Liu, X. Zhang, M. Pistola, Y. Zheng, M. Marques, and L. Zeng, "Automatic text input generation for mobile testing," in 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE, 2017, pp. 643-653.
[21] S. Feng and C. Chen, "Prompting is all your need: Automated android bug replay with large language models," arXiv preprint arXiv:2306.01987, 2023.
[22] M. Nass, E. Alegroth, and R. Feldt, "Improving web element localization by using a large language model," arXiv preprint arXiv:2310.02046, 2023.
[23] C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen, "Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models," in International conference on software engineering (ICSE), 2023.
[24] P. Brie, N. Burny, A. Sluÿters, and J. Vanderdonckt, "Evaluating a large language model on searching for gui layouts," Proceedings of the ACM on Human-Computer Interaction, vol. 7, no. EICS, pp. 1-37, 2023.
[25] S. Kang, J. Yoon, and S. Yoo, "Large language models are fewshot testers: Exploring llm-based general bug reproduction," in 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023, pp. 2312-2323.
[26] "Langchain: a framework for developing applications powered by language models," https://www.langchain.com.
[27] "Autogpt: An autonomous GPT-4 experiment," https://github.com/ Significant-Gravitas/Auto-GPT, 2023.
[28] "Chroma," https://docs.trychroma.com/.
[29] J. E. Laird, The Soar cognitive architecture. MIT press, 2019.
[30] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, "Voyager: An open-ended embodied agent with large language models," arXiv preprint arXiv:2305.16291, 2023.
[31] "Fdroid: Free and open source android app repository," https://f-droid. org/en/.
[32] R. Coppola and E. Alégroth, "A taxonomy of metrics for gui-based testing research: A systematic literature review," Information and Software Technology, p. 107062, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ DroidAGENT is publicly available from https://github.com/testing-agent/ droidagent&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1012 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1012</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1012</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-266917913</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.uni-miskolc.hu/index.php/multi/article/download/2236/2018" target="_blank">CURRICULUM LEARNING FOR DEEP REINFORCEMENT LEARNING IN SWARM ROBOTIC NAVIGATION TASK</a></p>
                <p><strong>Paper Abstract:</strong> This study investigates the training of a swarm consisting of five E-puck robots using Deep reinforcement learning with curriculum learning in a 3D environment. The primary objective is to decompose the navigation task into a curriculum comprising progressively more challenging stages based on curriculum complexity metrics. These metrics encompass swarm size, collision avoidance complexity, and distances between targets and robots. The performance evaluation of the swarm includes key metrics such as success rate, collision rate, training efficiency, and generalization capabilities. To assess their effectiveness, a comparative analysis is conducted between curriculum learning and the proximal policy optimization algorithm. The results demonstrate that curriculum learning outperforms traditional one, yielding higher success rates, improved collision avoidance, and enhanced training efficiency. The trained swarm also exhibits robust generalization for novel scenarios.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1012.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1012.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E-puck swarm (PPO+Curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Swarm of five E-puck robots trained with Proximal Policy Optimization and curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated swarm of five homogeneous E-puck robots trained in Webots using PPO combined with a staged curriculum that progressively increases task difficulty (swarm size, obstacles, target distance); evaluated on success rate, collision rate, training efficiency, cumulative reward and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>E-puck swarm (5 robots)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Five E-puck robots (simulated in Webots) trained with deep reinforcement learning using Proximal Policy Optimization (actor-critic, policy gradient) augmented by curriculum learning; state = 8 IR sensors + distance to target, action = continuous linear and angular velocities.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agents (Webots); hardware parameters reported for physical E-puck</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>3D Webots swarm navigation environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3D navigation task with target locations for each robot, optional obstacles, and variable environment area sizes. Complexity arises from multi-agent interactions (collision avoidance), limited-range IR sensors (0–0.04 m), continuous control (linear velocity 0–0.25 m/s, angular velocity ±3.14 rad/s), and varying target distances; environments were scaled (area sizes varied) and curriculum stages increased swarm size and obstacle presence.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Quantified by (a) swarm size {2, 3, 5 robots}, (b) collision-avoidance complexity (obstacle present vs absent), (c) distances between targets and robots via environment area sizes {0.5×0.5 m^2, 0.7×0.7 m^2, 0.1×0.1 m^2, 1.2×1.2 m^2, expanded to 1.5×1.5 m^2}, plus state-space/action-space dims (state dim = 10, action dim = 2) and max timesteps per episode (1500).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varied across experiments: from low (small area, fewer robots, no obstacles) to high (5 robots, obstacles present, large area up to 1.5×1.5 m^2).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Measured by number of curriculum stages and discrete variations introduced: different swarm sizes (2/3/5), presence/absence of obstacles, and multiple environment area sizes (listed above); curriculum uses staged transfers between these variants.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Medium–High (multiple staged variations in swarm size, obstacle configuration, and environment size; explicit staged curriculum across several discrete environment variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (targets reached), collision rate, cumulative reward, training efficiency (episodes to stable success / convergence).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative and some quantitative indicators: curriculum + PPO reached successful goal-reaching behavior in ~300 episodes (per description) vs traditional PPO requiring ~600 episodes; cumulative reward for curriculum reached optimal faster (plots shown but no absolute numeric reward value reported); curriculum-trained swarm maintained better success and obstacle avoidance when environment expanded (1.2→1.5 m^2) while traditional PPO failed to converge.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly links environment complexity (more robots, obstacles, larger area / longer target distances) to learning difficulty and shows curriculum learning mitigates this: staged increases in complexity (variation) allow transfer learning and faster convergence, whereas single-stage training (traditional PPO) struggles or fails to converge as complexity/variation increase (notably in the expanded 1.5×1.5 m^2 environment). The paper also notes trade-offs: collisions do not drop to zero because of agent aggregation at the goal (a complexity-induced artifact), and reward/penalty scaling must be balanced to avoid skewing behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning with staged task complexity increases and transfer learning between stages, using PPO (Proximal Policy Optimization) as the DRL algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Authors report that the swarm trained with curriculum learning exhibits robust generalization to novel scenarios and larger environments (qualitative claim); no detailed quantitative generalization accuracy numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved: curriculum learning reached successful behavior in ~300 episodes versus ~600 episodes for traditional PPO (approximate halving of episodes-to-success reported). Overall training run parameters: max training timesteps = 1,000,000; max timesteps per episode = 1500.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curriculum learning + PPO outperforms traditional PPO for swarm navigation: faster convergence (≈300 vs ≈600 episodes to successful goal-reaching), higher success rates and improved collision avoidance in variable/expanded environments, and better training efficiency and qualitative generalization. Increasing environment complexity (more robots, obstacles, larger area) degrades performance for non-curriculum training, while staged curricula enable transfer of skills and robustness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1012.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1012.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal Policy Optimization without curriculum learning (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline condition where the same swarm/agent architecture is trained with PPO in a single-stage (non-curriculum) regime; used for direct comparison to curriculum-augmented training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>E-puck swarm (baseline PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same embodied agents (E-puck robots) and neural architectures trained end-to-end with standard PPO without staged curriculum or transfer between progressively harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agents (Webots)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>3D Webots navigation environment (single-stage training)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same environment variants exist, but training is performed without staged progression; agents trained from random initialization on full-complexity tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Identical environment complexity metrics are available (swarm size, obstacle presence, environment area sizes), but training does not exploit staged reduction of complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varied across test scenarios; baseline struggles as complexity increases (notably in larger 1.5×1.5 m^2 environment).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>No explicit curriculum staging; variation presented directly during training (if at all), leading to larger effective variation experienced early in learning.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High effective variation during learning (because complex cases are present from the start), though not structured as staged variation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate, collision rate, cumulative reward, training efficiency (episodes to successful attempts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Took approximately ~600 episodes to show successful goal-reaching attempts (per descriptions) and struggled to converge in the expanded 1.5×1.5 m^2 environment; generally lower success and poorer obstacle avoidance than curriculum-trained swarm (no detailed numeric success rates provided).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Implicitly demonstrated: exposing agents to high complexity/variation from the start hinders convergence and performance; traditional PPO fails to find robust collective behaviors in larger/more complex environments compared to curriculum-trained agents.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-stage PPO training (no curriculum / no staged transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Not reported; baseline showed poor convergence in larger/novel environment tests and therefore no evidence of robust generalization was shown.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower than curriculum condition: roughly twice the episodes-to-success (~600 vs ~300).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Baseline PPO without curriculum is less sample-efficient, converges more slowly, has worse obstacle-avoidance and success outcomes, and can fail to converge in larger or more complex environment variants.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1012.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1012.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hussein2022 (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous swarm shepherding using curriculum-based reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior study cited that applied curriculum-based reinforcement learning to autonomous shepherding in swarms and reported high success (96%), cited as supporting evidence for curriculum benefits in swarm tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous swarm shepherding using curriculum-based reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Shepherding swarm (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Swarm agents trained with a curriculum-based reinforcement learning strategy, using task decomposition (hierarchical approach) and sparse rewards to solve shepherding; reported high success in that task domain.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated swarm robotic agents (prior study)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Swarm shepherding task (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Shepherding problem decomposed into sub-tasks with progressively increasing difficulty; environment and task complexity tackled via hierarchical/curriculum RL.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task decomposition/hierarchy and sparse rewards; success measured under curriculum stages (exact complexity metrics in cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Reported as sufficiently challenging but made tractable by curriculum/hierarchical decomposition (implied high without curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Curriculum stages representing varying task difficulty; specifics are in the cited work rather than the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Not specified in this paper beyond staged difficulty; likely medium–high in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (reported in citation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>96% success rate (reported in the cited paper and referenced here).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Mentioned: well-designed curricula facilitate skill transfer and improved performance as tasks increase in complexity (cited as motivating prior result).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum-based reinforcement learning (hierarchical decomposition / staged curriculum) as reported in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that curriculum-based RL can substantially improve swarm task performance (reported 96% success) and facilitate transfer of learned skills across stages; used in this paper as supporting prior art for applying curriculum learning to swarm navigation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autonomous swarm shepherding using curriculum-based reinforcement learning <em>(Rating: 2)</em></li>
                <li>Generating collective foraging behavior for robotic swarm using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning for swarm systems <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning for indoor mobile robot path planning <em>(Rating: 1)</em></li>
                <li>A deep reinforcement learning environment for particle robot navigation and object manipulation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1012",
    "paper_id": "paper-266917913",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "E-puck swarm (PPO+Curriculum)",
            "name_full": "Swarm of five E-puck robots trained with Proximal Policy Optimization and curriculum learning",
            "brief_description": "A simulated swarm of five homogeneous E-puck robots trained in Webots using PPO combined with a staged curriculum that progressively increases task difficulty (swarm size, obstacles, target distance); evaluated on success rate, collision rate, training efficiency, cumulative reward and generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "E-puck swarm (5 robots)",
            "agent_description": "Five E-puck robots (simulated in Webots) trained with deep reinforcement learning using Proximal Policy Optimization (actor-critic, policy gradient) augmented by curriculum learning; state = 8 IR sensors + distance to target, action = continuous linear and angular velocities.",
            "agent_type": "simulated robotic agents (Webots); hardware parameters reported for physical E-puck",
            "environment_name": "3D Webots swarm navigation environment",
            "environment_description": "3D navigation task with target locations for each robot, optional obstacles, and variable environment area sizes. Complexity arises from multi-agent interactions (collision avoidance), limited-range IR sensors (0–0.04 m), continuous control (linear velocity 0–0.25 m/s, angular velocity ±3.14 rad/s), and varying target distances; environments were scaled (area sizes varied) and curriculum stages increased swarm size and obstacle presence.",
            "complexity_measure": "Quantified by (a) swarm size {2, 3, 5 robots}, (b) collision-avoidance complexity (obstacle present vs absent), (c) distances between targets and robots via environment area sizes {0.5×0.5 m^2, 0.7×0.7 m^2, 0.1×0.1 m^2, 1.2×1.2 m^2, expanded to 1.5×1.5 m^2}, plus state-space/action-space dims (state dim = 10, action dim = 2) and max timesteps per episode (1500).",
            "complexity_level": "Varied across experiments: from low (small area, fewer robots, no obstacles) to high (5 robots, obstacles present, large area up to 1.5×1.5 m^2).",
            "variation_measure": "Measured by number of curriculum stages and discrete variations introduced: different swarm sizes (2/3/5), presence/absence of obstacles, and multiple environment area sizes (listed above); curriculum uses staged transfers between these variants.",
            "variation_level": "Medium–High (multiple staged variations in swarm size, obstacle configuration, and environment size; explicit staged curriculum across several discrete environment variants).",
            "performance_metric": "Success rate (targets reached), collision rate, cumulative reward, training efficiency (episodes to stable success / convergence).",
            "performance_value": "Qualitative and some quantitative indicators: curriculum + PPO reached successful goal-reaching behavior in ~300 episodes (per description) vs traditional PPO requiring ~600 episodes; cumulative reward for curriculum reached optimal faster (plots shown but no absolute numeric reward value reported); curriculum-trained swarm maintained better success and obstacle avoidance when environment expanded (1.2→1.5 m^2) while traditional PPO failed to converge.",
            "complexity_variation_relationship": "Yes — the paper explicitly links environment complexity (more robots, obstacles, larger area / longer target distances) to learning difficulty and shows curriculum learning mitigates this: staged increases in complexity (variation) allow transfer learning and faster convergence, whereas single-stage training (traditional PPO) struggles or fails to converge as complexity/variation increase (notably in the expanded 1.5×1.5 m^2 environment). The paper also notes trade-offs: collisions do not drop to zero because of agent aggregation at the goal (a complexity-induced artifact), and reward/penalty scaling must be balanced to avoid skewing behavior.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning with staged task complexity increases and transfer learning between stages, using PPO (Proximal Policy Optimization) as the DRL algorithm.",
            "generalization_tested": true,
            "generalization_results": "Authors report that the swarm trained with curriculum learning exhibits robust generalization to novel scenarios and larger environments (qualitative claim); no detailed quantitative generalization accuracy numbers are provided.",
            "sample_efficiency": "Improved: curriculum learning reached successful behavior in ~300 episodes versus ~600 episodes for traditional PPO (approximate halving of episodes-to-success reported). Overall training run parameters: max training timesteps = 1,000,000; max timesteps per episode = 1500.",
            "key_findings": "Curriculum learning + PPO outperforms traditional PPO for swarm navigation: faster convergence (≈300 vs ≈600 episodes to successful goal-reaching), higher success rates and improved collision avoidance in variable/expanded environments, and better training efficiency and qualitative generalization. Increasing environment complexity (more robots, obstacles, larger area) degrades performance for non-curriculum training, while staged curricula enable transfer of skills and robustness.",
            "uuid": "e1012.0"
        },
        {
            "name_short": "PPO baseline",
            "name_full": "Proximal Policy Optimization without curriculum learning (baseline)",
            "brief_description": "Baseline condition where the same swarm/agent architecture is trained with PPO in a single-stage (non-curriculum) regime; used for direct comparison to curriculum-augmented training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "E-puck swarm (baseline PPO)",
            "agent_description": "Same embodied agents (E-puck robots) and neural architectures trained end-to-end with standard PPO without staged curriculum or transfer between progressively harder tasks.",
            "agent_type": "simulated robotic agents (Webots)",
            "environment_name": "3D Webots navigation environment (single-stage training)",
            "environment_description": "Same environment variants exist, but training is performed without staged progression; agents trained from random initialization on full-complexity tasks.",
            "complexity_measure": "Identical environment complexity metrics are available (swarm size, obstacle presence, environment area sizes), but training does not exploit staged reduction of complexity.",
            "complexity_level": "Varied across test scenarios; baseline struggles as complexity increases (notably in larger 1.5×1.5 m^2 environment).",
            "variation_measure": "No explicit curriculum staging; variation presented directly during training (if at all), leading to larger effective variation experienced early in learning.",
            "variation_level": "High effective variation during learning (because complex cases are present from the start), though not structured as staged variation.",
            "performance_metric": "Success rate, collision rate, cumulative reward, training efficiency (episodes to successful attempts).",
            "performance_value": "Took approximately ~600 episodes to show successful goal-reaching attempts (per descriptions) and struggled to converge in the expanded 1.5×1.5 m^2 environment; generally lower success and poorer obstacle avoidance than curriculum-trained swarm (no detailed numeric success rates provided).",
            "complexity_variation_relationship": "Implicitly demonstrated: exposing agents to high complexity/variation from the start hinders convergence and performance; traditional PPO fails to find robust collective behaviors in larger/more complex environments compared to curriculum-trained agents.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-stage PPO training (no curriculum / no staged transfer).",
            "generalization_tested": false,
            "generalization_results": "Not reported; baseline showed poor convergence in larger/novel environment tests and therefore no evidence of robust generalization was shown.",
            "sample_efficiency": "Lower than curriculum condition: roughly twice the episodes-to-success (~600 vs ~300).",
            "key_findings": "Baseline PPO without curriculum is less sample-efficient, converges more slowly, has worse obstacle-avoidance and success outcomes, and can fail to converge in larger or more complex environment variants.",
            "uuid": "e1012.1"
        },
        {
            "name_short": "Hussein2022 (mentioned)",
            "name_full": "Autonomous swarm shepherding using curriculum-based reinforcement learning",
            "brief_description": "A prior study cited that applied curriculum-based reinforcement learning to autonomous shepherding in swarms and reported high success (96%), cited as supporting evidence for curriculum benefits in swarm tasks.",
            "citation_title": "Autonomous swarm shepherding using curriculum-based reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Shepherding swarm (prior work)",
            "agent_description": "Swarm agents trained with a curriculum-based reinforcement learning strategy, using task decomposition (hierarchical approach) and sparse rewards to solve shepherding; reported high success in that task domain.",
            "agent_type": "simulated swarm robotic agents (prior study)",
            "environment_name": "Swarm shepherding task (prior work)",
            "environment_description": "Shepherding problem decomposed into sub-tasks with progressively increasing difficulty; environment and task complexity tackled via hierarchical/curriculum RL.",
            "complexity_measure": "Task decomposition/hierarchy and sparse rewards; success measured under curriculum stages (exact complexity metrics in cited paper).",
            "complexity_level": "Reported as sufficiently challenging but made tractable by curriculum/hierarchical decomposition (implied high without curriculum).",
            "variation_measure": "Curriculum stages representing varying task difficulty; specifics are in the cited work rather than the present paper.",
            "variation_level": "Not specified in this paper beyond staged difficulty; likely medium–high in original work.",
            "performance_metric": "Success rate (reported in citation).",
            "performance_value": "96% success rate (reported in the cited paper and referenced here).",
            "complexity_variation_relationship": "Mentioned: well-designed curricula facilitate skill transfer and improved performance as tasks increase in complexity (cited as motivating prior result).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum-based reinforcement learning (hierarchical decomposition / staged curriculum) as reported in the cited work.",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Cited as evidence that curriculum-based RL can substantially improve swarm task performance (reported 96% success) and facilitate transfer of learned skills across stages; used in this paper as supporting prior art for applying curriculum learning to swarm navigation.",
            "uuid": "e1012.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autonomous swarm shepherding using curriculum-based reinforcement learning",
            "rating": 2,
            "sanitized_title": "autonomous_swarm_shepherding_using_curriculumbased_reinforcement_learning"
        },
        {
            "paper_title": "Generating collective foraging behavior for robotic swarm using deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "generating_collective_foraging_behavior_for_robotic_swarm_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Deep reinforcement learning for swarm systems",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_for_swarm_systems"
        },
        {
            "paper_title": "Deep reinforcement learning for indoor mobile robot path planning",
            "rating": 1,
            "sanitized_title": "deep_reinforcement_learning_for_indoor_mobile_robot_path_planning"
        },
        {
            "paper_title": "A deep reinforcement learning environment for particle robot navigation and object manipulation",
            "rating": 1,
            "sanitized_title": "a_deep_reinforcement_learning_environment_for_particle_robot_navigation_and_object_manipulation"
        }
    ],
    "cost": 0.0114205,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CURRICULUM LEARNING FOR DEEP REINFORCEMENT LEARNING IN SWARM ROBOTIC NAVIGATION TASK</p>
<p>Alaa Iskandar iskandar.alaa@student.uni-miskolc.hu 0000-0002-7746-9224
PhD student
Institute of Mathematics
University of Miskolc
3515Miskolc, Miskolc-Egyetemváros</p>
<p>Béla Kovács bela.kovacs@uni-miskolc.hu 
associate professor
Institute of Mathematics
University of Miskolc
3515Miskolc, Miskolc-Egyetemváros</p>
<p>CURRICULUM LEARNING FOR DEEP REINFORCEMENT LEARNING IN SWARM ROBOTIC NAVIGATION TASK
F799E1A1713BA51D5C2D9135BEF0581810.35925/j.multi.2023.3.18swarm robotsnavigation taskdeep reinforcement learningcurrcuilum learningproximal policy optimization
This study investigates the training of a swarm consisting of five E-puck robots using Deep reinforcement learning with curriculum learning in a 3D environment.The primary objective is to decompose the navigation task into a curriculum comprising progressively more challenging stages based on curriculum complexity metrics.These metrics encompass swarm size, collision avoidance complexity, and distances between targets and robots.The performance evaluation of the swarm includes key metrics such as success rate, collision rate, training efficiency, and generalization capabilities.To assess their effectiveness, a comparative analysis is conducted between curriculum learning and the proximal policy optimization algorithm.The results demonstrate that curriculum learning outperforms traditional one, yielding higher success rates, improved collision avoidance, and enhanced training efficiency.The trained swarm also exhibits robust generalization for novel scenarios.</p>
<p>Introduction</p>
<p>Swarm robotics (SR) is a field that involves the study of collective behavior exhibited by a group of autonomous, simple, and homogenous robots working together towards a common goal.It draws inspiration from the behavior of natural swarms, such as ant colonies or bird flocks, where individual agents interact locally with their environment and neighboring agents to achieve the required tasks.SR has emerged as a promising approach that leverages the power of teamwork to solve complex problems beyond the capabilities of individual robots.By collaborating in a decentralized manner, SR can tackle tasks such as cleaning large areas, exploring unknown territories, and foraging items from multiple sources.The key features of SR systems include scalability, flexibility, and robustness (Cheraghi et al., 2022).</p>
<p>Designing effective collective behavior in SR systems is a critical aspect of swarm engineering.Two main methods are commonly employed.The behavior-based design method explicitly defines rules for each robot, dictating their actions to achieve the desired collective behavior.On the other hand, the automatic design method utilizes algorithms rooted in artificial intelligence, enabling the robots to become intelligent entities capable of learning and adapting from their environment (Iskandar et al., 2021) Among the automatic design methods, two primary approaches have gained significant attention in SR: evolutionary algorithms and reinforcement learning (RL).Evolutionary algorithms, such as Particle Swarm Optimization (PSO) (Gbenga et al., 2016) and Bacterial Foraging Optimization Algorithm (BFOA) (Yang et al., 2014), rely on computational models inspired by natural evolution to optimize the collective behavior of the swarm.However, these approaches often require extensive computational resources and cannot learn in real time.</p>
<p>RL, a field of machine learning, has shown remarkable potential in SR due to its adaptive learning capabilities in complex and dynamic environments.By integrating it with deep learning techniques, SR can continually improve their behavior, enhancing their performance in challenging situations.This combination reduces the computational requirements and enables SR systems to acquire navigation skills and effectively solve navigation tasks autonomously (Shen et al., 2022).</p>
<p>In this paper, we propose the application of curriculum learning, a training strategy inspired by human learning processes, to enhance the reinforcement learning-based navigation capabilities of SR systems.Curriculum learning organizes the learning process by gradually increasing task complexity, allowing the SR to learn fundamental navigation skills before tackling more challenging scenarios.By leveraging curriculum learning in RL, SR systems can achieve improved convergence, generalization, and sample efficiency in navigation tasks.</p>
<p>Related work</p>
<p>RL has gained significant attention as a powerful technique for training autonomous agents to learn from their interactions with the environment.Its algorithms allow agents to make sequential decisions in dynamic and uncertain environments, aiming to maximize a cumulative reward signal.RL has been successfully applied to various tasks, including game playing, robotics, and autonomous vehicle control.</p>
<p>RL approaches and Algorithms</p>
<p>RL is a decision-making approach within the realm of machine learning.It revolves around two key components: the agent and the environment.Through interaction with the environment, the agent learns by sequentially making decisions, exploring the environment, and exploiting the acquired knowledge to solve a given problem.</p>
<p>An RL problem is typically formulated as follows:</p>
<p>-State space (S): The environment is divided into various states.At each time step, the agent perceives the current state S(t) as a frame of by sensors readings and takes action to transition to the next state S(t+1).-Action space (A): The action space encompasses the set of actions that the agent can take, like turning right or left.Actions can be discrete or continuous, depending on the problem domain.-Reward function (R): The agent's actions are evaluated using a reward function, which provides feedback based on the observed behavior.Actions that yield desirable outcomes receive positive rewards, while unfavorable actions receive negative rewards.RL aims to learn a policy that maps each state to the optimal action by maximizing the cumulative rewards obtained over time.In many RL problems, the optimal action may vary depending on the specific situation, and the chosen actions can impact future rewards.To account for these aspects, RL problems are often formulated as Markov Decision Processes (MDPs).MDPs capture the sequential nature of decision-making, considering the current state, chosen action, and resulting rewards as part of the learning process.</p>
<p>Figure 1. Representing reinforcement learning approach as Markov decision process.</p>
<p>The diagram in Figure 1 illustrates the RL problem formulated as an MDP, where the agent interacts with the environment, observes the current state, selects an action, receives a reward, and transitions to the next state.This cyclic process continues as the agent aims to learn an optimal policy that maximizes the cumulative rewards received over multiple interactions with the environment.</p>
<p>RL aims to maximize the future reward received each time.The cumulative received rewards are called Return G because of the dynamics of MDP, and it is considered a random variable.It represents an essential value to construct the equations used to find the optimal policy called function values, as in equation 1.There are many approaches to make the agent learn how to make decisions according to Return.It is achieved by computing the function values and choosing the corresponding actions as in equation 2 (Sutton et al., 2018).</p>
<p>𝑄(𝑠, 𝑎) = 𝐸[𝐺 𝑡 |𝑆 𝑡 = 𝑠, 𝐴 𝑡 = 𝑎]</p>
<p>(1)  (Casas, 2017).For instance, DDPG is an actor-critic algorithm specifically designed for continuous action spaces.It involves training an actor-network to select actions and a critic network to estimate the value of the selected actions.Proximal Policy Optimization (PPO) is another family of on-policy methods based on actor-critic and policy gradient techniques.PPO optimizes the objective function by employing policy updates with clipped values, ensuring computational efficiency and stability (Schulman et al., 2017).
𝑎 * =
Figure 2 visually represents the main diagram illustrating RL algorithms, showcasing the interaction between the agent, environment, state transitions, action selection, and learning processes.</p>
<p>Deep reinforcement learning in swarm robotic systems</p>
<p>Deep Reinforcement Learning (DRL) has emerged as a prominent approach in SR systems due to its ability to handle complex and dynamic environments and its suitability for multi-agent and swarm scenarios.DRL has been particularly effective in generating collective behaviors in SR systems, such as formation, aggregation, and foraging.For instance, DRL techniques have been employed in foraging tasks to enable robots to learn how to search for food resources and navigate from sources to sinks (Hüttenrauch et al., 2019).</p>
<p>Numerous advancements have been made in adapting and enhancing DRL for SR systems.In one study (Jin et  Furthermore, a hierarchical RL approach with sparse rewards was utilized to solve the problem of food transportation in two steps.This method exhibited greater efficiency and flexibility than traditional RL techniques (Jin et al., 2022).</p>
<p>These prior studies have contributed significantly to the understanding and applying RL in SR navigation tasks.They have explored different algorithms, reward functions, and techniques to enhance collective behavior and optimize swarm performance.Challenges can arise in complex environments or tasks when using randomly initialized network parameters in DRL (Xu et al., 2006).</p>
<p>-Difficulty in Convergence: Random initialization of network parameters means that the initial policy or value function is likely far from optimal.In complex environments, it may take significant time and experience for the DRL algorithm to converge to a desirable solution.The exploration process is essential to discover effective strategies but can be time-consuming and resource-intensive.-Deterioration of Performance: Poorly initialized weights and biases can hinder the learning process and prevent the algorithm from effectively adapting to the environment.It may take longer for the agent to learn and converge to a good policy, or it may get stuck in suboptimal solutions.Proper initialization techniques, such as using pre-trained models or transfer learning, can help mitigate this issue.Curriculum learning is a potential solution to address learning challenges in complex environments.It is an approach inspired by human learning, where concepts are typically introduced in a structured and gradually increasing difficulty manner.In machine learning, curriculum learning involves presenting the learning agent with a sequence of simplified tasks or environments that gradually increase in complexity.The agent can learn basic skills and strategies more easily by starting with simpler tasks or environments.It can gradually build its knowledge and abilities before being exposed to more complex scenarios.This approach helps to overcome the issue of poor initial performance and facilitates a smoother learning process.By exposing the agent to a range of tasks with increasing complexity, curriculum learning promotes robustness and generalization, where the agent learns to adapt its policies to various scenarios and develops a more comprehensive understanding of the environment.Recent advancements in robotics and reinforcement learning (RL), such as indoor path planning for mobile robots, have shown promising results (Gao, 2020).In this context, an incremental training mode for DRL is proposed, starting with a 2D evaluation and then transitioning to 3D environments.The technique is extended to navigate robots within known environments, leveraging the TD3 algorithm for enhanced efficiency and generalization.In the realm of swarm robotics, a novel curriculum-based RL strategy for autonomous shepherding is introduced in (Hussein, 2022).This approach effectively breaks down the shepherding task into sub-tasks, employing a progressive curriculum for improved learning.Achieving a notable 96% success rate, this underscores the value of well-designed curricula in facilitating skill transfer within swarm robotic systems.</p>
<p>In the present study, the Policy-Based PPO algorithm is employed by integrating with curriculum learning and offers advantages such as improved learning efficiency, enhanced exploration-exploitation trade-off, reduced sample complexity, better handling of complex environments, and the acquisition of transferable knowledge.These benefits make curriculum learning a valuable addition to DRL techniques in the context of swarm robotic navigation tasks, leading to more effective and adaptive swarm behavior.</p>
<p>Methodology</p>
<p>DRL is used for training a swarm of five E-puck robots to navigate a 3D environment in the Webots simulator and Deepbots framework (Kirtas et al., 2020).The objective is to guide the robots to reach their respective targets while avoiding collisions with each other and obstacles, as shown in Figure 3.</p>
<p>The problem is formulated as a Markov Decision Process (MDP) represented by a tuple (S, A, T, R, γ).</p>
<p>The state space (S) comprises robot sensor readings, including data from 8 infrared (IR) sensors and the distance between each robot and its target.Each robot has two velocities to control its motion, constituting the action space (A).The transition function (T) governs the system's dynamics, while the reward function (R) provides feedback based on the achieved objectives.Shaping reward, as shown in equation 3, is used.These methods enable the robots to leverage the experience during the path, which increases learning speed by comparing to sparse rewards.The discount factor (γ) is a value between 0 and 1 that balances the importance of immediate rewards against future rewards.The primary objective is determining the optimal policy that maps states to the best actions, maximizing the cumulative reward for each state-action pair.</p>
<p>The effectiveness of choosing the value of penalty as in Eq.3 depends on the scaling and context of reward and punishment values.If the difference between the previous and current distance which represents the reward for preferred actions tends to be relatively small, a punishment of -0.001 is appropriate to maintain a balance between encouraging efficiency and avoiding excessive penalties.It prevents the punishment from overwhelming the rewards, but it still incentivizes the agent to improve its performance.Ultimately, the choice of reward and punishment values should be based on experimentation and observation by trying different values and assessing how they impact the agent's learning behavior and performance.</p>
<p>The agent and DRL algorithm</p>
<p>The PPO algorithm has several advantages that make it well-suited for swarm navigation tasks compared to other DRL algorithms like sample Efficiency: PPO tends to be more sample-efficient compared to other DRL algorithms, such as REINFORCE or DQN, stable Training: PPO incorporates a policy constraint that ensures policy updates remain within a safe region, preventing drastic policy changes, parallelization: PPO can be readily parallelized, allowing multiple robots in the swarm to learn simultaneously.Each robot can have its actor-critic network, collecting experiences and updating its policy independently, and policy gradient methods: PPO falls under the category of policy gradient methods, which directly optimize the policy space instead of value functions.Each robot's controller has its own DRL algorithm, which learns to find the robot's optimal path by interacting with the other robots and obstacles.A deep neural network is used for nonlinear approximation for value and policy.The neural networks are designed as a series of fully connected layers FC.In PPO, two neural networks are used to train the robot, the policy network, which represents the actor-network, learns to map between states (IR readings and distance) and actions (velocities) with the objective of maximizing received rewards, and the critic network to calculate Q value to enhance the training of critic network as shown in Figure 4 and the hyperparameters for in PPO are shown in table 1.For other hyperparameters like PPO epsilon clip it helps to balance exploration and exploitation by limiting policy updates based on the ratio of new and old policies' probabilities, the PPO K epochs refers to the number of times the collected batch of data is used for updating the policy during each iteration, and the learning rate is a crucial hyperparameter that determines the step size by which the optimizer adjusts the weights and biases of the actor and critic networks based on the computed gradients.Choosing an appropriate learning rate is important because it affects how quickly the network's parameters converge to an optimal solution.A learning rate that is too high can cause the optimization process to oscillate or diverge, while a learning rate that is too low can lead to slow convergence or getting stuck in suboptimal solutions.</p>
<p>The hyperparameters related to initializing a continuous action space policy refer to the initial standard deviation of the probability distribution from which the agent's actions are sampled.This distribution is often used to add exploration noise to the actions, encouraging the agent to explore a variety of actions and learn more effectively.All mentiond hyperparameters is choosing based on empirical testing.</p>
<p>DRL with curriculum learning</p>
<p>To train a swarm of five E-puck robots using DRL with curriculum learning in a 3D environment, the given navigation task must be decomposed into a curriculum of progressively more challenging stages.The decomposition process is built based on curriculum complexity metrics.The metrics that quantify the complexity or challenge associated with different swarm sizes (2 robots, 3robots, and five robots), collision avoidance complexity (the existence of the obstacle or not), and the distances between the targets and robots (by changing the size of the environment from 0.5×0.5  2 , 0.7×0.7  2 ,0.1×0.1  2 , and 1.2×1.2 2 ).The following steps can be followed as shown in Figure 5:</p>
<p>Figure 5. DRL with curriculum learning</p>
<p>According to figure 5, the training process is applied by iterating through the training process, gradually increasing the complexity of the stages, and applying transfer learning at each stage.Introduce more challenging scenarios, such as the size of the swarm, obstacles, and increased target distances, to continually improve the swarm's navigation abilities.Assessing the performance of the swarm on each stage by measuring metrics such as success rate (percentage of targets reached), and collision rate.</p>
<p>Results</p>
<p>All experiments are conducted on a standard laptop with an I5-8250U@1.6 GHz processor and 6 GB RAM.The parameters of E-puck robots are set as follows: linear velocity <a href="m/s">0-0.25</a>, angular velocity [-3.14, 3.14] (rad/s), and IR sensor range <a href="m">0-0.04</a>.</p>
<p>The experiments were conducted in the Webots environment to train the robots in generating collective navigation behavior.The PPO algorithm with curriculum learning and the traditional PPO approach was employed, and both successfully generated the desired collective behavior.</p>
<p>A notable improvement in the learning process can be observed by contrasting Figuers 6-1, 6-2.When applying curriculum learning, the robot exhibits faster learning towards reaching the goal.In the case of the traditional PPO method figure 6-1, approximately 600 episodes are required to achieve successful goal-reaching attempts marked by blue vertical lines, contrasted with failed attempts indicated by white lines.However, with the incorporation of curriculum learning figure 6-2, the robot successfully attains the goal in approximately 300 episodes.The robots trained with curriculum learning exhibited significantly better performance.They were able to explore the environment at a faster pace and effectively utilize the acquired knowledge to reach the designated goals.</p>
<p>One key aspect contributing to the superior performance of curriculum learning is its ability to increase the complexity of the navigation task gradually.By decomposing the task into stages of increasing difficulty, the swarm is exposed to incremental challenges, allowing it to learn and adapt progressively.This enables the swarm to acquire a deeper understanding of the environment and develop more robust navigation strategies.As a result, the robots trained with curriculum learning exhibited a faster convergence towards optimal performance, which is obviously shown in Figure 7.</p>
<p>PPO without curriculum learning</p>
<p>The recorded reward further supports the effectiveness of the curriculum learning approach.</p>
<p>The curves presented in Figure 7 represent the cumulative rewards for one robot.The reward for PPO with curriculum learning reached the optimal value faster than the traditional PPO approach.The improved performance of the curriculum learning approach can be attributed to its ability to guide the swarm through a well-designed sequence of learning stages.The use of an averaging window of 20 values contributes to the smoothness and clarity of the dark plots.As shown in Figure 8, PPO with curriculum learning demonstrates enhanced obstacle avoidance abilities due to its progressive curriculum design.By gradually increasing the complexity of the navigation task, the curriculum learning approach enables the swarm to develop robust obstacle avoidance strategies.The swarm learns to detect and respond to obstacles effectively, navigating them with improved agility and precision.The collisions do not go to zero because of the aggregation of the robots at the goal so each detects others as obstacles.When the area of the environment is expanded from 1.2×1.2 2 to 1.5×1.5  2 , the performance of a swarm of five robots trained using the PPO with curriculum learning demonstrates more favorable outcomes.As the environment size increases, the curriculum framework guides the swarm in developing and refining navigation strategies suitable for larger areas.In contrast, the traditional PPO struggles to converge and generate the desired collective behavior, as depicted in Figure 9.The robots trained using traditional PPO fail to reach the predefined goals due to the heightened complexity of the larger environment.Their navigation abilities are insufficient to overcome the challenges posed by the increased size.Conversely, the swarm trained with PPO and curriculum learning exhibits promising results.Despite the expanded environment, the curriculum learning framework facilitates the swarm's ability to converge and achieve the required collective behavior.</p>
<p>Conclusion</p>
<p>In conclusion, this research study has explored the training of a swarm of five E-puck robots using Deep Reinforcement Learning with curriculum learning in a 3D environment.By decomposing the navigation task into a curriculum of progressively more challenging stages, the swarm could acquire robust navigation skills and adapt to complex scenarios.The curriculum complexity metrics, which considered swarm size, collision avoidance complexity, and distances between targets and robots, guided the curriculum design and facilitated the gradual development of the swarm's abilities.The performance evaluation of the swarm was conducted based on key metrics, including success rate, collision rate, training efficiency, and generalization and adaptation capabilities.</p>
<p>The results demonstrated that PPO with curriculum learning yielded notable advantages over the traditional PPO algorithm.The swarm trained with curriculum learning exhibited a higher success rate in reaching the targets, demonstrating the effectiveness of the curriculum-based approach in guiding the swarm toward desired outcomes.Additionally, the collision rate was effectively mitigated through the acquired obstacle avoidance skills developed during the curriculum learning process.Furthermore, the training efficiency of the swarm was enhanced with curriculum learning, as evidenced by the faster convergence and reaching of optimal values in the cumulative received rewards.The curriculum-based approach enabled the swarm to efficiently explore and exploit knowledge gained in earlier stages, leading to accelerated learning and improved training efficiency.</p>
<p>Importantly, the trained swarm also demonstrated strong generalization and adaptation capabilities.The curriculum learning approach equipped the swarm with the necessary skills to navigate novel scenarios successfully, showcasing the ability to transfer learned knowledge to previously unseen environments.</p>
<p>Overall, this research provides valuable insights into the application of curriculum learning in training robot swarms and emphasizes its potential for enabling efficient and effective collective behaviors in complex environments.The findings contribute to the advancement of swarm robotics and provide a foundation for future research and development in this field.</p>
<p>Figure 2 .
2
Figure 2. RL algorithms classification diagram.</p>
<p>al., 2020), the author compared various methods, including Deep Q-Network (DQN), N-Step-Q Network (NSQ), and Double DQN, to generate collective behavior in a swarm.They also proposed a combined version, Double N-step Q-Network (DNQ), which demonstrated superior performance in learning speed and obtaining optimal policy rewards.Shared memory was utilized to store training data and provide accessibility to all robots.This facilitated faster training processes (Yasuda et al., 2018).Also (Deng et al., 2017) proposed a sequential Q-learning algorithm based on knowledge sharing to address scalability concerns in large swarms.Many researchers have employed value-based methods with sparse rewards, while others have used reward-shaping techniques.The choice of reward function plays a vital role in achieving optimal swarm behavior when modeled as DRL (Wei et al., 2019).The author employed sparse rewards with varying values to encourage obstacle avoidance and goal achievement and concluded that imposing high penalties redirected the robots' focus toward avoiding obstacles rather than reaching the goal.</p>
<p>Figure 3 .
3
Figure 3. Swarm's environment  = { −0.001,  ℎ    ℎ    .0,  ℎ     ℎ    .(3)  =   −   + .</p>
<p>Figure 4 .
4
Figure 4. PPO architecture.</p>
<p>1 2 Figure 6 .
126
Figure 6.Success rate 1. PPO with curriculum learning 2. PPO without curriculum learning</p>
<p>Figure 7 .
7
Figure 7.The average cumulative rewards PP0: without curriculum.PPO1 with curriculum.</p>
<p>1 2 Figure 8 .
128
Figure 8. Collison rate 1. PPO with curriculum learning 2. PPO without curriculum learning.</p>
<p>1 2 Figure 9 .
129
Figure 9.The success rate for expanded environment 1. PPO with curriculum learning 2. PPO without curriculum learning</p>
<p>Table 1 .
1
PPO hyperparameters.
ParameterValuemax training timesteps1000000Max timesteps per episode1500state space dimension10action space dimension2discount factor (gamma)0.99PPO epsilon clip0.2PPO K epochs80optimizer learning rate actor0.0003optimizer learning rate critic0.001Initializing a continuous action space policystarting std of action distribution0.6the decay rate of std of action distribution0.05minimum std of action distribution0.1decay frequency of std of action distribution 250000 timesteps</p>
<p>Past, present, and future of swarm robotics. A R Cheraghi, S Shahzad, K Graffi, Intelligent Systems and Applications: Proceedings of the 2021 Intelligent Systems Conference (IntelliSys). 20223</p>
<p>. 10.1007/978-3-030-82199-9_13Springer International Publishing</p>
<p>A survey on automatic design methods for swarm robotics systems. A Iskandar, B Kovács, 10.2478/cjece-2021-0006Carpathian Journal of Electronic &amp; Computer Engineering. 1422021</p>
<p>Understanding the limitations of particle swarm algorithm for dynamic optimization tasks: A survey towards the singularity of PSO for swarm robotic applications. D E Gbenga, E I Ramlan, 10.1145/2906150ACM Computing Surveys (CSUR). 4912016</p>
<p>Target searching and trapping for swarm robots with modified bacterial foraging optimization algorithm. B Yang, Y Ding, K Hao, 10.1109/wcica.2014.7052915Proceeding of the 11th World Congress on Intelligent Control and Automation. eeding of the 11th World Congress on Intelligent Control and Automation2014. June</p>
<p>A deep reinforcement learning environment for particle robot navigation and object manipulation. J Shen, E Xiao, Y Liu, C Feng, 10.1109/icra46639.2022.98119652022 International Conference on Robotics and Automation (ICRA). IEEE</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>High-accuracy model-based reinforcement learning, a survey. A Plaat, W Kosters, M Preuss, 10.1007/s10462-022-10335-wArtificial Intelligence Review. 2023</p>
<p>J Schulman, S Levine, P Abbeel, M Jordan, P Moritz, 10.48550/arXiv.1502.05477Trust region policy optimization, 2015 International Conference on Machine Learning. </p>
<p>Deep deterministic policy gradient for urban traffic light control. N Casas, 10.48550/arXiv.1703.09035arXiv:1703.090352017arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, 10.48550/arXiv.1707.06347arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Deep reinforcement learning for swarm systems. M Hüttenrauch, S Adrian, G Neumann, Journal of Machine Learning Research. 20542019</p>
<p>Generating collective foraging behavior for robotic swarm using deep reinforcement learning. B Jin, Y Liang, Z Han, K Ohkura, 10.1007/s10015-020-00642-2Artificial Life and Robotics. 252020</p>
<p>Combining modelbased q-learning with structural knowledge transfer for robot skill learning. Z Deng, H Guan, R Huang, H Liang, L Zhang, J Zhang, 10.1109/TCDS.2017.2718938IEEE Transactions on Cognitive and Developmental Systems. 1112017</p>
<p>Developing end-to-end control policies for robotic swarms using deep Q-learning. Y Wei, X Nie, M Hiraga, K Ohkura, Z Car, 10.20965/jaciii.2019.p0920Journal of Advanced Computational Intelligence and Intelligent Informatics. 2352019</p>
<p>A hierarchical training method of generating collective foraging behavior for a robotic swarm. B Jin, Y Liang, Z Han, M Hiraga, K Ohkura, 10.1007/s10015-021-00714-xArtificial Life and Robotics. 2022</p>
<p>Deep reinforcement learning for indoor mobile robot path planning. J Gao, W Ye, J Guo, Z Li, 10.3390/s20195493Sensors. 201954932020</p>
<p>Autonomous swarm shepherding using curriculum-based reinforcement learning. A Hussein, E Petraki, S Elsawah, H A Abbass, Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems. the 21st International Conference on Autonomous Agents and Multiagent Systems2022. May</p>
<p>Present situation and future development of mobile robot path planning technology. X N Xu, R Lai, Comput. Simul. 102006</p>
<p>Deepbots: A webots-based deep reinforcement learning framework for robotics. M Kirtas, K Tsampazis, N Passalis, A Tefas, 10.1007/978-3-030-49186-4_6Artificial Intelligence Applications and Innovations: 16th IFIP WG 12.5 International Conference, AIAI 2020, Neos Marmaras. GreeceSpringer International Publishing2020. June 5-7Proceedings, Part II 16</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1700 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1700</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1700</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-270212889</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.01171v3.pdf" target="_blank">Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization</a></p>
                <p><strong>Paper Abstract:</strong> The concept of persona, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language models (LLMs) to specific context (e.g., personalized search, LLM-as-a-judge). However, the growing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. Additionally, we introduce existing methods for LLM personality evaluation. To the best of our knowledge, we present the first survey for role-playing and personalization in LLMs under the unified view of persona. We continuously maintain a paper collection to foster future endeavors: https://github.com/MiuLab/PersonaLLM-Survey</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1700.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1700.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-Evaluator (survey mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models used as evaluators / LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey-identified paradigm where strong LLMs role-play as evaluators/judges to assess model responses; claimed in cited work to often correlate better with human ground-truth than traditional automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (LLM-as-evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>role-playing as impartial judge; task-dependent rubric/criteria (helpfulness, relevance, accuracy, depth, creativity) and sometimes multi-role debate</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>model responses / generated text (summaries, conversational outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general / open-domain NLP (summarization, dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>helpfulness, relevance, accuracy, depth, creativity (varies by study)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>correlation with human ground-truth (unspecified correlation type in survey text)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>When LLM evaluators are assigned clear evaluator roles (e.g., impartial judge) and use structured criteria; multi-role or debate-style evaluation (ChatEval) intended to improve fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Not explicitly quantified in survey; implicit concerns when tasks are subjective or when traditional automatic metrics are used instead</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not specified for software artifacts in survey; general implication that LLM evaluation can better match humans than simple automatic metrics across some tasks</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Survey notes that assigning explicit evaluation roles and criteria (helpfulness, relevance, accuracy, etc.) is central to the LLM-as-evaluator approach, implying clearer criteria aid alignment</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Survey-level claim: judgments by LLMs have been shown in cited work to reflect a higher correlation with human ground-truth than traditional automatic metrics, but no numeric comparison for software development artifacts is provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The survey reports prior work showing LLMs can be effective evaluators (LLM-as-a-judge) and that in several cited studies their judgments correlate better with human judgments than traditional automatic metrics; methods include assigning evaluator personas (objective/subjective) and multi-agent discussion rounds to improve evaluation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>The survey does not provide numeric agreement metrics or detailed comparisons specifically for software development artifacts; it flags the design of evaluator roles and task specificity as important and notes transferability/generalizability concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1700.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1700.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chiang & Lee (2023) target citation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can large language models be an alternative to human evaluations? (citation in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work arguing for and empirically testing whether LLM-based evaluation can substitute or complement human evaluation; referenced as evidence that LLM judgments sometimes correlate more strongly with humans than traditional metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models be an alternative to human evaluations?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (LLM-based automatic evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>general model outputs / responses (survey citation does not limit to software artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general-purpose NLP tasks (per survey citation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>human-likeness / alignment with human ground-truth (as reported in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>correlation with human ground-truth (survey cites higher correlation but does not specify metric)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Not specified in survey text for this citation; cited as part of evidence that LLM evaluators can match human judgments under some conditions</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Not specified in survey text</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not reported in survey text</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Not reported in survey text</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Survey-level summary: Chiang & Lee 2023 is cited among works showing LLM judgments can have higher correlation with human ground-truth than traditional metrics; no numeric comparison provided in this survey for software artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that LLM-based evaluation can be a competitive proxy for human judgments compared to traditional automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Survey does not present the detailed experimental setup or metrics from Chiang & Lee; applicability to software-development artifacts is not discussed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1700.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1700.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Personality-test comparisons (Jiang et al., MPI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine Personality Inventory (MPI) and Big Five comparisons to human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey describes studies that applied human psychometric tests (Big Five, MBTI, MPI) to LLMs and compared results to human evaluations, finding correlations between internal consistency and model capabilities but also raising questions about transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Human psychometric tests applied to LLMs (treated as automated proxy evaluations of personality)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>personality-test prompting / interview-style elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>role-playing/persona behavior / generated text</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>persona fidelity / psychological trait expression</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>personality trait scores (Big Five dimensions, MBTI type, MPI internal consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Compared with human evaluation (survey states 'by comparing with human evaluation they find internal consistency correlates with model capabilities')</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>human evaluators used as ground-truth for personality assessments (not further specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>internal consistency correlation with human evaluation (metric unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>When using structured psychometric instruments (BFI, MPI), LLMs often reflect intended personas accurately according to cited studies</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Survey and cited works raise concern that human psychometric tests may not be directly transferable to LLMs (Dorner et al., 2023); MBTI types often resist change via prompt modification</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Using standardized inventories (clear items and scoring) yielded measurable alignment, but transferability caveats remain</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Survey reports that MPI/Big Five evaluations show correlations with human evaluation in cited work, but does not quantify agreement or compare to inter-human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standardized personality instruments applied to LLMs produced personality profiles that in cited work often align with intended personas and correlate with model capabilities; however, the survey flags open questions about whether psychometric tests are directly transferable to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Transferability of human psychometric tests to LLMs is questioned; no standardized numeric agreement reported in the survey for software development artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1700.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1700.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dorner et al. (2023) limitation note</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Do personality tests generalize to large language models? (Dorner et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited analysis raising concerns about applying human psychometric tests to LLMs and identifying biases such as 'agree bias' that can distort automatic proxy evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do personality tests generalize to large language models?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Application of human psychometric tests (proxy evaluation) and analysis of their validity on LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>persona / personality outputs</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>psychometric evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>validity and generalizability of psychometric instruments when applied to LLMs; detection of agree bias</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Not claimed; Dorner et al. emphasize limitations instead</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Presence of 'agree bias' and other systematic behaviors in LLMs that make direct translation of human tests to LLM evaluation problematic</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Survey cites Dorner et al. to highlight that applying human-oriented tests to LLMs is not straightforward and may reduce reliability of proxy-human comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dorner et al. identify limitations (e.g., agree bias) that can reduce the validity of psychometric tests when applied to LLMs; survey uses this to caution about automatic proxy evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Direct transfer of human psychometric tests to LLMs is questionable; biases intrinsic to LLM behavior can lead to misleading proxy evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>ChatEval: Towards better llm-based evaluators through multi-agent debate <em>(Rating: 2)</em></li>
                <li>Do personality tests generalize to large language models? <em>(Rating: 2)</em></li>
                <li>InCharacter: Evaluating personality fidelity in role-playing agents through psychological interviews <em>(Rating: 2)</em></li>
                <li>LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1700",
    "paper_id": "paper-270212889",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "LLM-as-Evaluator (survey mention)",
            "name_full": "Large language models used as evaluators / LLM-as-a-judge",
            "brief_description": "Survey-identified paradigm where strong LLMs role-play as evaluators/judges to assess model responses; claimed in cited work to often correlate better with human ground-truth than traditional automatic metrics.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "LLM-as-a-judge (LLM-as-evaluator)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": "role-playing as impartial judge; task-dependent rubric/criteria (helpfulness, relevance, accuracy, depth, creativity) and sometimes multi-role debate",
            "artifact_type": "model responses / generated text (summaries, conversational outputs)",
            "artifact_domain": "general / open-domain NLP (summarization, dialogue)",
            "evaluation_criteria": "helpfulness, relevance, accuracy, depth, creativity (varies by study)",
            "human_evaluation_setup": null,
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": "correlation with human ground-truth (unspecified correlation type in survey text)",
            "agreement_score": null,
            "high_agreement_conditions": "When LLM evaluators are assigned clear evaluator roles (e.g., impartial judge) and use structured criteria; multi-role or debate-style evaluation (ChatEval) intended to improve fidelity",
            "low_agreement_conditions": "Not explicitly quantified in survey; implicit concerns when tasks are subjective or when traditional automatic metrics are used instead",
            "artifact_complexity_effect": "Not specified for software artifacts in survey; general implication that LLM evaluation can better match humans than simple automatic metrics across some tasks",
            "criteria_clarity_effect": "Survey notes that assigning explicit evaluation roles and criteria (helpfulness, relevance, accuracy, etc.) is central to the LLM-as-evaluator approach, implying clearer criteria aid alignment",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Survey-level claim: judgments by LLMs have been shown in cited work to reflect a higher correlation with human ground-truth than traditional automatic metrics, but no numeric comparison for software development artifacts is provided in this paper",
            "calibration_or_training": null,
            "key_findings": "The survey reports prior work showing LLMs can be effective evaluators (LLM-as-a-judge) and that in several cited studies their judgments correlate better with human judgments than traditional automatic metrics; methods include assigning evaluator personas (objective/subjective) and multi-agent discussion rounds to improve evaluation quality.",
            "limitations_noted": "The survey does not provide numeric agreement metrics or detailed comparisons specifically for software development artifacts; it flags the design of evaluator roles and task specificity as important and notes transferability/generalizability concerns.",
            "uuid": "e1700.0",
            "source_info": {
                "paper_title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Chiang & Lee (2023) target citation",
            "name_full": "Can large language models be an alternative to human evaluations? (citation in survey)",
            "brief_description": "Cited work arguing for and empirically testing whether LLM-based evaluation can substitute or complement human evaluation; referenced as evidence that LLM judgments sometimes correlate more strongly with humans than traditional metrics.",
            "citation_title": "Can large language models be an alternative to human evaluations?",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "LLM-as-a-judge (LLM-based automatic evaluation)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "general model outputs / responses (survey citation does not limit to software artifacts)",
            "artifact_domain": "general-purpose NLP tasks (per survey citation)",
            "evaluation_criteria": "human-likeness / alignment with human ground-truth (as reported in cited work)",
            "human_evaluation_setup": null,
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": "correlation with human ground-truth (survey cites higher correlation but does not specify metric)",
            "agreement_score": null,
            "high_agreement_conditions": "Not specified in survey text for this citation; cited as part of evidence that LLM evaluators can match human judgments under some conditions",
            "low_agreement_conditions": "Not specified in survey text",
            "artifact_complexity_effect": "Not reported in survey text",
            "criteria_clarity_effect": "Not reported in survey text",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Survey-level summary: Chiang & Lee 2023 is cited among works showing LLM judgments can have higher correlation with human ground-truth than traditional metrics; no numeric comparison provided in this survey for software artifacts.",
            "calibration_or_training": null,
            "key_findings": "Cited as evidence that LLM-based evaluation can be a competitive proxy for human judgments compared to traditional automatic metrics.",
            "limitations_noted": "Survey does not present the detailed experimental setup or metrics from Chiang & Lee; applicability to software-development artifacts is not discussed in the survey.",
            "uuid": "e1700.1",
            "source_info": {
                "paper_title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Personality-test comparisons (Jiang et al., MPI)",
            "name_full": "Machine Personality Inventory (MPI) and Big Five comparisons to human evaluation",
            "brief_description": "Survey describes studies that applied human psychometric tests (Big Five, MBTI, MPI) to LLMs and compared results to human evaluations, finding correlations between internal consistency and model capabilities but also raising questions about transferability.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "Human psychometric tests applied to LLMs (treated as automated proxy evaluations of personality)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": "personality-test prompting / interview-style elicitation",
            "artifact_type": "role-playing/persona behavior / generated text",
            "artifact_domain": "persona fidelity / psychological trait expression",
            "evaluation_criteria": "personality trait scores (Big Five dimensions, MBTI type, MPI internal consistency)",
            "human_evaluation_setup": "Compared with human evaluation (survey states 'by comparing with human evaluation they find internal consistency correlates with model capabilities')",
            "human_expert_count": null,
            "human_expert_expertise": "human evaluators used as ground-truth for personality assessments (not further specified)",
            "agreement_metric": "internal consistency correlation with human evaluation (metric unspecified in survey)",
            "agreement_score": null,
            "high_agreement_conditions": "When using structured psychometric instruments (BFI, MPI), LLMs often reflect intended personas accurately according to cited studies",
            "low_agreement_conditions": "Survey and cited works raise concern that human psychometric tests may not be directly transferable to LLMs (Dorner et al., 2023); MBTI types often resist change via prompt modification",
            "artifact_complexity_effect": "Not applicable",
            "criteria_clarity_effect": "Using standardized inventories (clear items and scoring) yielded measurable alignment, but transferability caveats remain",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Survey reports that MPI/Big Five evaluations show correlations with human evaluation in cited work, but does not quantify agreement or compare to inter-human agreement.",
            "calibration_or_training": null,
            "key_findings": "Standardized personality instruments applied to LLMs produced personality profiles that in cited work often align with intended personas and correlate with model capabilities; however, the survey flags open questions about whether psychometric tests are directly transferable to LLMs.",
            "limitations_noted": "Transferability of human psychometric tests to LLMs is questioned; no standardized numeric agreement reported in the survey for software development artifacts.",
            "uuid": "e1700.2",
            "source_info": {
                "paper_title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Dorner et al. (2023) limitation note",
            "name_full": "Do personality tests generalize to large language models? (Dorner et al., 2023)",
            "brief_description": "Cited analysis raising concerns about applying human psychometric tests to LLMs and identifying biases such as 'agree bias' that can distort automatic proxy evaluations.",
            "citation_title": "Do personality tests generalize to large language models?",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "Application of human psychometric tests (proxy evaluation) and analysis of their validity on LLMs",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "persona / personality outputs",
            "artifact_domain": "psychometric evaluation of LLMs",
            "evaluation_criteria": "validity and generalizability of psychometric instruments when applied to LLMs; detection of agree bias",
            "human_evaluation_setup": null,
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": "Not claimed; Dorner et al. emphasize limitations instead",
            "low_agreement_conditions": "Presence of 'agree bias' and other systematic behaviors in LLMs that make direct translation of human tests to LLM evaluation problematic",
            "artifact_complexity_effect": null,
            "criteria_clarity_effect": null,
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Survey cites Dorner et al. to highlight that applying human-oriented tests to LLMs is not straightforward and may reduce reliability of proxy-human comparisons.",
            "calibration_or_training": null,
            "key_findings": "Dorner et al. identify limitations (e.g., agree bias) that can reduce the validity of psychometric tests when applied to LLMs; survey uses this to caution about automatic proxy evaluations.",
            "limitations_noted": "Direct transfer of human psychometric tests to LLMs is questionable; biases intrinsic to LLM behavior can lead to misleading proxy evaluations.",
            "uuid": "e1700.3",
            "source_info": {
                "paper_title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "ChatEval: Towards better llm-based evaluators through multi-agent debate",
            "rating": 2,
            "sanitized_title": "chateval_towards_better_llmbased_evaluators_through_multiagent_debate"
        },
        {
            "paper_title": "Do personality tests generalize to large language models?",
            "rating": 2,
            "sanitized_title": "do_personality_tests_generalize_to_large_language_models"
        },
        {
            "paper_title": "InCharacter: Evaluating personality fidelity in role-playing agents through psychological interviews",
            "rating": 2,
            "sanitized_title": "incharacter_evaluating_personality_fidelity_in_roleplaying_agents_through_psychological_interviews"
        },
        {
            "paper_title": "LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models",
            "rating": 1,
            "sanitized_title": "llmeval_unified_multidimensional_automatic_evaluation_for_opendomain_conversations_with_large_language_models"
        }
    ],
    "cost": 0.013802499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization</p>
<p>Yu-Min Tseng ymtseng@nlg.csie.ntu.edu.tw 
National Taiwan University ‡ Academia Sinica ⋆ University of Virginia</p>
<p>Yu-Chao Huang 
National Taiwan University ‡ Academia Sinica ⋆ University of Virginia</p>
<p>Teng-Yun Hsiao 
National Taiwan University ‡ Academia Sinica ⋆ University of Virginia</p>
<p>Wei-Lin Chen 
National Taiwan University ‡ Academia Sinica ⋆ University of Virginia</p>
<p>†⋆ Chao 
National Taiwan University ‡ Academia Sinica ⋆ University of Virginia</p>
<p>Wei Huang 
National Taiwan University ‡ Academia Sinica ⋆ University of Virginia</p>
<p>Yu Meng yumeng5@virginia.eduy.v.chen@ieee.org 
National Taiwan University ‡ Academia Sinica ⋆ University of Virginia</p>
<p>Yun-Nung Chen 
National Taiwan University ‡ Academia Sinica ⋆ University of Virginia</p>
<p>Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization
F708153F77C355B973D87D4B5BF22A98
The concept of persona, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language models (LLMs) to specific context (e.g., personalized search, LLM-as-a-judge).However, the growing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy.To close the gap, we present a comprehensive survey to categorize the current state of the field.We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas.Additionally, we introduce existing methods for LLM personality evaluation.To the best of our knowledge, we present the first survey for role-playing and personalization in LLMs under the unified view of persona.We continuously maintain a paper collection to foster future endeavors.</p>
<p>Introduction</p>
<p>The striking capabilities of large language models (LLMs), exemplified by ChatGPT (OpenAI, 2022), have significantly advanced the field of natural language processing (NLP; Wei et al., 2023;Madaan et al., 2024;Shinn et al., 2024).Recently, in addition to using LLMs as NLP task solvers or general-purpose chatbots, the question of how to adapt LLMs for specific context has received great attention.To this end, leveraging personas has resurfaced as an ideal lens for adapting LLMs in target scenarios (Chen et al., 2023a(Chen et al., , 2024)).By incorporating personas, LLMs can generate more contextually appropriate responses, maximizing their utility and effectiveness for specific applications.However, the growing literature on persona in the LLM era is relatively disorganized, lacking a unifying overview.* Equal contribution.In this paper, we aim to close the gap by offering a comprehensive survey and a systematic categorization of existing studies.Specifically, we divide current research into two main streams, namely LLM Role-Playing and LLM Personalization, as illustrated in Figure 1.The primary distinction is that in role-playing, the persona belongs to the LLM, while in personalization, the persona belongs to the user.Further, the literature on role-playing mainly focuses on the tasks (i.e., how LLMs with role-playing can achieve better performance).In contrast, the literature of personalization primarily focuses on the users (i.e., how to satisfy users' expectations and meet their needs).It is noteworthy MetaGPT (Hong et al., 2023a) -Dataset -SoftwareDev (Hong et al., 2023a); SRDD (Qian et al., 2023) Figure 2: The taxonomy of LLM role-playing and LLM personalization (representative works shown only).</p>
<p>be goals in the same scenario, but serve different purposes and are driven by different aspects.The definitions are detailed below.</p>
<p>• LLM Role-Playing: LLMs are tasked to play the assigned personas (i.e., roles) and act based on environmental feedback, adapting to the environment.</p>
<p>• LLM Personalization: LLMs are tasked to take care of user personas (e.g., background information or historical behaviors) to meet individualized needs, adapting to distinct users.</p>
<p>To the best of our knowledge, we present the first survey for LLM role-playing and LLM personalization under the unified view of persona.To foster future endeavors, we actively maintain a paper collection available to the research community.</p>
<p>We aim for this work to serve as both a valuable introduction for newcomers and a comprehensive resource for current researchers in the field.</p>
<p>Our taxonomy is illustrated in Figure 2. We first introduce LLM role-playing ( §2), followed by LLM personalization ( §3).Next, we provide an overview of evaluation methods ( §4) assessing whether the personality of LLMs (e.g., personality traits or psychological behaviors) accurately aligns with expected personas after the adaptation (i.e., for role-playing LLMs that act according to assigned personas and personalized LLMs that fit user personas).Lastly, we highlight current challenges and future directions ( §5).We hope that this taxonomy could serve as a useful guideline for researchers to easily target the tasks/scenarios of interests, and swiftly pinpoint their current position in the field.3), and LLM as Evaluators ( §2.1.4).For each environment, we provide a simple scenario with a task description (red-bordered) and relevant personas (i.e., roles; blue-bordered).The dashed rectangle represents an example LLM role-playing prompt template.In addition to the above environments, past research also proposes general frameworks applicable to different environments ( §5.1).</p>
<p>LLM Role-Playing</p>
<p>LLM-based language agents have demonstrated impressive abilities, such as planning, reflection, and tool-use recently (Yao et al., 2022b;Shinn et al., 2024;Yao et al., 2024).The predominant approach of LLM role-playing is by coupling personas with language agents, specifically, by incorporating personas directly inside the prompt of language agents.Such a training-free paradigm is particularly desirable due to its simplicity and effectiveness.Language agents with role-playing elicit the corresponding parametric knowledge in LLMs to generate responses aligned with assigned personas (i.e., role), enabling them to adapt to various interactive environments.LLM role-playing also extends to multi-agent settings, where multiple language agents are equipped with diverse personas, cooperating and communicating with each other to solve complex tasks (Guo et al., 2024).For instance, in one of the first works of role-played LLMs, Park et al. (2023) propose generative agents, which engage in a social simulation environment by mimicking human behaviors according to names, ages, and personality traits specified in the prompts.</p>
<p>Following we introduce different environments and associated roles in which LLMs adapt to ( §2.1), interactions between LLMs within the environment ( §2.2), and emergent behaviors stemming from their interactions ( §2.3). Figure 3 provides an illustrative overview.</p>
<p>Environments</p>
<p>Software Development</p>
<p>For software development, the goal typically involves designing programs or coding projects.For instance, "Create a snake game." or "Create a Python program to develop an interactive weather dashboard."(Hong et al., 2023a).Due to the com-plexity of these tasks, often too intricate to be completed correctly on the first attempt, existing research leverages approaches like the Waterfall model (Petersen et al., 2009;Bassil, 2012) or Standardized Operating Procedures (SOPs) (Belbin and Brown, 2022;DeMarco and Lister, 2013) to break down the tasks into manageable sub-tasks.</p>
<p>Similar to real-world settings, LLMs role-play to operate as a company in a collaborative, multiagent software development environment (Qian et al., 2023;Hong et al., 2023a;Dong et al., 2023).Different roles include Chief Technology Officer (CTO), Chief Product Officer (CPO), Chief Executive Officer (CEO), Product Managers, Engineers, Reviewers, and Testers.By assigning specific roles, LLMs are capable of carrying out tasks in a stepby-step and accurate manner.</p>
<p>Recent work (Dong et al., 2023) proposed one of the first self-collaboration frameworks that encompasses division of labor and collaboration among multiple LLM agents, each acting as a specialized "experts" to address complex code generation tasks.Following the Waterfall model, Chat-Dev (Qian et al., 2023) divides the development process into a four-phase pipeline: designing, coding, testing, and documenting and proposes Chat Chain to decompose each phase into a sequence of atomic sub-tasks.Differing from the above work, MetaGPT (Hong et al., 2023a) require LLM agents to generate structured outputs instead of free-text, demonstrating a significant increase in the success rate of target code generation.</p>
<p>Game</p>
<p>LLMs have been an effective backbone for agents in a variety of game environments, including Minecraft (Wang et al., 2023a), social simulation (Park et al., 2023;Wang et al., 2023d), and bargaining game (Fu et al., 2023).In these environ-ments, LLMs are tasked to role-play as a general assistant (Wang et al., 2023a), or characters related to the environment, such as buyers and sellers (Fu et al., 2023).Gaming environments usually contain a wide range of information, including settings, utilizable tools, and nearby situations, which presents challenges for LLMs to memorize and respond.Thus, retrieval-based memory stream approaches are a crucial component for the effectiveness of language agents role-playing in the game environments (Park et al., 2023;Wang et al., 2023a).</p>
<p>Medical Application</p>
<p>In medical domain environments, Wu et al. (2023a) propose DR-CoT prompting, the first approach to leverage LLM role-playing for diagnostic reasoning.By mimicking doctors underlying thought processes, DR-CoT exhibits a striking improvement from standard prompting.Then, Kwon et al. (2024) extend such success to image-based diagnosis via knowledge distillation, addressing the application in real-world clinical settings.Another work, MedAgent (Tang et al., 2023a), introduces a multi-agent collaboration framework into medical reasoning through five processes: expert gathering, analysis proposition, report summarization, collaborative consultation, and decision making, to mimic actual medical scenarios.</p>
<p>These studies assign medically relevant personas to LLMs, ranging from general roles like doctor and patient to specific ones such as neurology and psychiatry experts.Their research demonstrates LLMs inherently possess medical knowledge (Liévin et al., 2024), enabling performance enhancement via LLM role-playing successfully.</p>
<p>LLM-as-Evaluator</p>
<p>The concept of adopting strong LLMs as evaluators has become a de facto framework for evaluating LM alignment.It is shown that LLMs are capable of assessing human-like values in model responses, and judgments made by LLMs could reflect a higher correlation with human ground-truth than traditional metrics (Chiang and Lee, 2023;Wang et al., 2023b;Lin and Chen, 2023).</p>
<p>Aiming for a greater similarity with human evaluation, roles in LLM-as-evaluator environments span a broad spectrum, representing various perspectives of human beings in society, such as the general public, the critic, and the news author.In LLM-as-a-judge (Zheng et al., 2023), LLMs roleplay an impartial judge and consider factors such as helpfulness, relevance, accuracy, depth, and creativity.Wu et al. (2023b) propose DRPE to assess the quality of summarization by assigning LLMs statically objective roles and dynamically subjective roles based on task settings.Another work, ChatEval (Chan et al., 2023), further adds discussion rounds within roles to improve the evaluation process, simulating a judge group in reality.</p>
<p>Role-Playing Schema</p>
<p>We categorize two schemas in LLM role-playing environments: single-agent and multi-agent.</p>
<p>Single-Agent</p>
<p>We define the single-agent schema as: One agent is able to achieve its goal independently without assistance from others, though multiple agents may coexist in the same environment.</p>
<p>Single-agent schema is most common in game environments, where LLMs attend more to environmental information and feedback rather than collaboration.For example, Voyager (Wang et al., 2023a) agents, playing general assistant roles, are tasked to continuously explore the defined environment, acquire diverse skills, and make novel discoveries in Minecraft.Despite the presence of multiple Voyager agents in Minecraft, each agent is capable of exploring the gaming world on its own.</p>
<p>Multi-Agent</p>
<p>We define the multi-agent schema as: Supports (e.g., collaborate and communicate) from other agents are necessary for one agent to achieve its goal.</p>
<p>Software development and medical applications are the primary environments for multi-agent schema.Similar to real world, interaction within environments is crucial.Representative works like AgentVerse (Chen et al., 2023c) and ChatDev (Qian et al., 2023) both propose multi-agent frameworks that exchange information and cooperate to accomplish their tasks efficiently.Further, we identify two collaboration paradigms in the multi-agent schema (Xi et al., 2023;Guo et al., 2024): Cooperative and Adversarial.The cooperative paradigm facilitates information sharing among agents, for example, several works use message pools to store each agent's current state and ongoing tasks (Hong et al., 2023a;Tang et al., 2023a;Chen et al., 2023c).For the adversarial paradigm, including debate, competition, and criticism, enhances the decisionmaking process and seeks more advantages by adopting opposing perspectives (Chan et al., 2023;Fu et al., 2023).</p>
<p>Emergent Behaviors in Role-Playing</p>
<p>Under the multi-agent schema, different behaviors reflecting phenomena in human society (e.g., conformity and consensus reaching) emerge through LLM collaboration.We introduce three collaborative behaviors following Chen et al. (2023c).</p>
<p>Voluntary Behavior Voluntary behaviors usually occur in the cooperative collaboration paradigm, where agents proactively assist their peers or inquire if there is anything they can help with to accomplish team goals.In addition, they may contribute resources to others, such as unallocated time and possessed materials.Through voluntary behaviors, LLMs enhance team efficiency and demonstrate cohesion and commitment within defined environments (Chen et al., 2023c;Hong et al., 2023a).</p>
<p>Conformity Behavior Conformity behaviors occur in situations where an agent deviates from the team goal.After receiving criticism and suggestions from others, the deviating agent then refines and adjusts its behavior or decisions to better cooperate with the team.Through conformity behaviors, LLMs align with the mutual goal and pursue improved accuracy and completeness (Tang et al., 2023a;Fu et al., 2023).</p>
<p>Destructive Behavior Occasionally, LLMs undertake various actions that lead to undesired and detrimental outcomes.For instance, it may exhibit a Bad Mind that seeks to control the world (Li et al., 2024a).Additionally, LLMs might display toxicity or reveal deep-seated stereotypical biases when equipping personas (Deshpande et al., 2023;Gupta et al., 2023).Such destructive behaviors raise safety and bias concerns of role-playing.</p>
<p>LLM Personalization</p>
<p>Prominent approaches for aligning LLMs to user intents typically leverage reinforcement learning from human feedback (RLHF), a process that infuses collective consciousness and biases into the model.To enhance individual experience and preference, personalized LLMs consider user personas (e.g., individual information, historical behaviors) and cater to customized needs (Chen et al., 2023e;Deshpande et al., 2024).Following we introduce various personalized tasks with associated methods for achieving personalization.Figure 4 presents an illustrative overview of personalization tasks.</p>
<p>Personalized Recommendation</p>
<p>Recommendation systems aim to recommend items (e.g., books or movies) to users that match their preferences.We compare existing research in Table 2 and compile relevant datasets in Table 3.</p>
<p>Existing studies explore various prompting methods for using LLMs in recommendation systems.Li et al. (2023a)  A lot of works have focused on the zero-shot setting, leveraging the powerful out-of-the-box capabilities of LLMs.Wang and Lim (2023) adopt a three-step prompting pipeline to achieve bet-ter zero-shot next-item recommendation.Hou et al. (2024) propose a zero-shot sequential recommendation system via in-context learning.Zhang et al. (2023) enhance user-friendliness by allowing users to freely interact with the system and receive more precise recommendations through natural language instructions.For generalizability, Wang et al. (2024d) highlight that current recommendation systems mostly focus on specific tasks and lack the ability to generalize to new tasks.They propose an LLM-powered agent for general recommendation purposes.Although LLM-based personalized search systems present a more convenient and simple solution for information search, ensuring the accountability and trustworthiness of the synthesized results still requires further development (Li et al., 2024b).</p>
<p>Personalized Search</p>
<p>Compared to traditional search systems that provide a list of hard-to-organize relevant results and are limited to simple queries, personalized search systems enable understanding of complex queries and past interactions to infer user preferences, synthesizing information from multiple sources and presenting it in a cohesive, natural language form.Spatharioti et al. (2023) demonstrate that LLMbased search systems improve users' performance in certain situations.Ziems et al. (2023) suggest that LLMs act as built-in search engines given fewshot demonstrations.Specifically, LLMs can generate correct web URLs for corresponding documents.Building upon Zhou et al. (2021), Zhou et al. (2024) present a strategy to combine the cognitive memory mechanism with LLMs for personalized search, enabling LLMs to efficiently retrieve memory.Some works also leverage search engine results to enhance LLM personalization (Baek et al., 2024;Salemi and Zamani, 2024).Empirically, Sharma et al. (2024) conduct experiments to investigate how LLM-powered search systems could lead to opinion polarization.</p>
<p>Personalized Education</p>
<p>The capability of LLMs can be utilized in a variety of ways to facilitate personalized education.For example, LLMs can provide detailed, step-by-step explanations in the Socratic teaching style (Hao et al., 2024), answer questions on technical and complicated subjects (Arefeen et al., 2023), and automatically summarize lectures to enhance learning experience (Gonzalez et al., 2023).</p>
<p>Personalized LLMs have the potential to create a more inclusive and equitable educational ecosystem, obviating the need for individuals to pay disproportionate fees.Recent works have illustrated various opportunities and visions for integrating LLMs into educational environments.These applications range from personalized learning and teaching assistance to homework assessment and feedback (Kasneci et al., 2023;Wang et al., 2024b;Jeon and Lee, 2023;Huber et al., 2024).</p>
<p>For example, EDUCHAT (Dan et al., 2023) pretrained models on an educational corpus to establish a foundational knowledge base, and subsequently fine-tune models on personalized tasks such as essay assessment, Socratic teaching, or emotional support.HUMSUM (Shehata et al., 2023) summarize personalized lecture transcripts from diverse scenarios, considering factors such as length, depth, tone, and complexity.This is followed by prompt tuning to modify the summary based on the personalization options given by users.Park et al. (2024) incorporate the student's affective state, cognitive state, and learning style into the prompt to create a personalized conversation-based tutoring system.</p>
<p>Personalized Healthcare</p>
<p>LLMs have exhibited expert-level capabilities in a range of general biomedical tasks, with the potential to integrate into people's everyday lives (Cohan et al., 2020;Milne-Ives et al., 2020;Singhal et al., 2023;Saab et al., 2024;Abbasian et al., 2024b).</p>
<p>Towards personalized healthcare assistant, Abbasian et al. (2024a) propose OPENCHA, an LLM agentic framework that integrates external data and personalized health data to address personalized medical problems.Following OPENCHA, Abbasian et al. (2024c) infuse domain-specific knowledge to effectively utilize health data, knowledge bases, and analytical tools for diabetes-related questions.MALP (Zhang et al., 2024a) combine parameter-efficient fine-tuning (PEFT) with a memory retrieval module to generate personalized medical responses.Other frameworks such as HEALTH-LLM (Jin et al., 2024b) combine LlamaIndex (Liu, 2022) to make diagnosis predictions, and is capable of generating personalized medical advice based on symptom descriptions provided by users.Moreover, LLMs also show great potential for psychotherapy (Stade et al., 2024;Chen et al., 2023b;Xu et al., 2024).</p>
<p>Personalized Dialogue Generation</p>
<p>Depending on the goals, dialogue generation tasks can be categorized into: (1) Task-oriented dialogue modeling (ToD modeling) and (2) User persona modeling.Following we discuss ToD modeling and User persona.We also organize various datasets for dialogue generation in Table 4.</p>
<p>ToD Modeling ToD modeling guides users in completing specific tasks, such as hotel bookings or restaurant reservations, through multiple interactive steps.See an example in Table 5. Hudeček and Dusek (2023) leverage instructiontuned LLMs and employ in-context learning for retrieval, and state tracking.Focusing on factuality, REFGPT (Yang et al., 2023a) 2023) explore prompt augmentations; on the other hand, DSP (Li et al., 2024c) train a small policy model to generate hints and guide LLMs in completing tasks.A lot of works used LLMs to generate multi-turn dialogue as training datasets (Yang et al., 2023a;Huryn et al., 2022;Xu et al., 2023).Further, personalized dialogues have been applied in procedural content generation for customized dialogue generation in video games (Ashby et al., 2023).</p>
<p>User Persona Modeling User persona modeling detects the user persona based on dialogue history and generates customized responses tailored for each user.See an example in Table 6.</p>
<p>COBERT (Zhong et al., 2020) proposed personabased empathetic conversations using BERT with a two-hop co-attention mechanism (Lu et al., 2017) to refine embeddings and identify the most relevant response given the context and persona information.Song et al. (2020) utilized natural language inference (NLI) as an RL task with response persona as the reward to generate persona-consistent dialogue.Liu et al. (2020) proposed P 2 , a mutual persona perception model, and employ supervised training and self-play fine-tuning in the training process.Tang et al. (2023b) combined sparse persona descriptions, dense persona descriptions, and dialogue history to generate personalized responses.</p>
<p>LLM Personality Evaluation</p>
<p>In the previous sections, we summarize the current progress in LLM role-playing and LLM person-alization.Equally important is the evaluation of whether the personality of LLMs accurately reflects the intended persona after the adaptation (i.e., for role-playing LLMs that act based on designated personas and personalized LLMs tailored to individualized personas).</p>
<p>A line of works has carried out the evaluation leveraging human personality assessments, including Big Five (Jiang et al., 2023;Sorokovikova et al., 2024) and MBTI (Pan and Zeng, 2023;Song et al., 2024).For example, Sorokovikova et al. (2024); Jiang et al. (2024) quantitatively evaluate LLM personality based on the Big Five Personality Inventory (BFI) test and story writing test.In the BFI evaluation, LLMs often can reflect their intended persona accurately.Moreover, their personas often influence their linguistic style and personality consistency (Frisch and Giulianelli, 2024;Jiang et al., 2023).While most works focus solely on either semantic accuracy or personality consistency, Harrison et al. (2019) further explore controlling the two aspects simultaneously.Jiang et al. (2024) introduce Machine Personality Inventory (MPI) for evaluating LLMs' personality traits.They use Big Five Personality Factors to evaluate each personality trait consisting of a series of descriptions and a set of options and statistically measure each trait.By comparing with human evaluation, they find that the internal consistency correlates with model capabilities.On the other hand, Pan and Zeng (2023) evaluate LLMs with the MBTI test to assess whether LLMs possess human-like personalities, and conclude that different LLMs have different MBTI types, which are often attributable to their training corpus.Moreover, they find that simply modifying the prompts is unlikely to change the MBTI type of LLMs.</p>
<p>Another work by Wang et al. (2024c) evaluate the personality fidelity of role-playing LLMs via personality test interviewing, and ask LLM to rate the score of each personality dimension according to the interview.Their results suggest that LLMs' demonstrated personalities align well with the assigned character personas.However, whether the aforementioned human psychometric tests are directly transferable to be applied to LLMs remains an open question (Dorner et al., 2023).</p>
<p>5 Challenges and Future Directions</p>
<p>Towards a General Framework</p>
<p>Despite the effectiveness of various role-playing frameworks, they are mostly task dependent and heavily rely on human-crafted personas.Both require prior knowledge and deep understanding of the tasks (Chen et al., 2023c).Consequently, enhancing the generalizability of the framework and employing automatic prompt engineering is a fruitful directions (Li et al., 2024a;Wang et al., 2023c).</p>
<p>To this end, Li et al. (2024a) propose a novel task-independent framework that allows agents to collaborate autonomously, but is limited to two roles and still requires human assigned personas.Subsequently, Wang et al. (2023c) introduce methods for LLMs to automatically identify personas based on given problems.Another work by Chen et al. (2023c) also enable LLMs to dynamically adjust the personas.However, they require prior knowledge of the intended tasks and pre-defined configuration (e.g., the number of agents).</p>
<p>Long-Context Personas</p>
<p>Richardson et al. (2023) note that incorporating user history data into the prompt for personalizing LLMs could lead to input exceeding context length as well as increased inference costs.Leveraging retrieval-based methods may have the problem of potential information loss.Some works have proposed to summarize user profiles, design long-term memory mechanisms focusing on user portrait, prestoring user information, or ways to efficiently represent for retrieval augmentation (Richardson et al., 2023;Zhong et al., 2024;Zhang et al., 2024b;Sun et al., 2024).However, retrieval augmentation might be underperforming due to unrelated or noisy prompts (Tan et al., 2024).How to better store, encode, and integrate long-context personas in LLMs requires further investigation.</p>
<p>Lack of Datasets and Benchmarks</p>
<p>For LLM role-playing, several tasks lack suitable datasets with specific formats (Ahn et al., 2024) and environmental information (e.g., game environments require information about configurations and tools).For personalized dialogue generation, user persona modeling lacks contradictory persona datasets and multimodal persona datasets that would more accurately represent real human behaviors (Kim et al., 2024b;Ahn et al., 2023).Furthermore, LLM personalization faces a scarcity of high-quality personal data for model development due to privacy concerns, hindering a thorough evaluation of personalization methods.In addition, existing benchmarks for both LLM role-playing and personalization are relatively limited, lacking comprehensive evaluations across various dimensions (Chang et al., 2023;Samuel et al., 2024).Therefore, expanding datasets and benchmarks for specialized environments and personal information under privacy protection is an important next step.</p>
<p>Bias</p>
<p>While a large number of studies focus on enhancing end-task performance, fewer works explore the biases induced by role-playing and personalization in LLMs.In this context, Gupta et al. (2023), as one of the first studies, highlight the deep-seated stereotypical biases found in LLMs assigned with socio-demographic personas.Additionally, Zhao et al. (2024) find that applying role-play often increases the overall likelihood of generating stereotypical and harmful outputs.For personalized LLM recommendation systems, biases can be observed due to item popularity or item positions in the prompts (Hou et al., 2024).Empirically, Dorner et al. (2023) also reveal the presence of agree bias in LLMs -a tendency to agree with both true and false content, regardless of the actual facts.In sum, there exists ample room for investigating and mitigating different classes of biases in the context of LLM role-playing and personalization.</p>
<p>Safety and Privacy</p>
<p>Past research has shown safety issues in LLM roleplaying and personalization.Jin et al. (2024a) and Shah et al. (2023) successfully manipulate LLMs to perform jailbreak collaboratively.Deshpande et al. (2023) also show that assigning personas to LLMs aid in jailbreaking.Negative behaviors in LLM role-playing are also demonstrated by Chen et al. (2023c) and Li et al. (2024a).Further, Deshpande et al. (2023) find that LLMs consistently exhibit toxicity in a range of topics when assigned personas, and Vijjini et al. (2024) show that LLMs suffer from personalization bias when they are personalized for the user's demographic.These works demonstrate the discovery of unsafe problems, indicating an urgent need and more efforts to prevent potential exploits.</p>
<p>Since LLM personalization heavily relies on user personas, including personal information and historical behaviors, ensuring privacy is especially crucial.Recently, Wang et al. (2024a) discover that using the membership inference attack can leak personal information, raising concerns about encoding personal data into models.Although existing research provides methods to address this personal information leakage (Lukas et al., 2023;Gambarelli et al., 2023;Huang et al., 2022;Chen et al., 2023d), the risks remain in need of more effort and attention from the research community.</p>
<p>Broader Implications</p>
<p>As LLM personalization continues to advance in education domains, individuals could easily access personalized educational contents, lecture materials, and receive affordable tutoring, largely benefiting minority groups with limited resources.However, the concern of polarizing trends might arise, where the privileged group enjoys private tutors and underrepresented individuals only have access to LLM-powered supports (Li et al., 2023c).Also, personalized LLMs for healthcare could potentially be widely integrated into clinical scenarios, mental health assessments, or prescribed therapeutic treatments in the near future, where critical questions such as legal considerations of the liability associated with these personalized systems needs careful considerations (Swift and Allen, 2010).</p>
<p>As discussed in ( §4), though methods for LLM personality evaluation have been proposed, there still lacks a unifying understanding of how to quantify personality in LLMs (Fang et al., 2023).Song et al. (2024); Jiang et al. (2024) also show that LLMs sometimes do not hold consistent personalities.It is crucial to continuously explore new measurements for reliable assessment of personality and psychological traits in LLMs, considering that in the future they might take on more advanced roles and capabilities in society.</p>
<p>Conclusion</p>
<p>Leveraging personas, LLMs can generate tailored responses and effectively adapt to a wide range of scenarios.In this survey paper, we summarize two lines of work -role-playing and personalizationfor research of personas in the era of LLMs.We also present various evaluation methods for LLM personality.Lastly, we highlight current challenges and promising future directions.We hope our extensive survey and resources serve as an introductory guide for beginners to the field and a practical roadmap to foster future endeavors.</p>
<p>Limitations</p>
<p>For the evaluation metric, as the literature, even within the same scope, addresses various sub-tasks and employs different corresponding evaluation metrics, or proposes their own ones (e.g., persona accuracy, task success rate, combined inform and success rate).This largely increase the difficulty to establish a suitable/fair standard for comparison.Also, some scenarios may require multiple metrics to determine overall performance (Samuel et al., 2024).For instance, we might need to assess the fluency, empathy, and safety of personalized LLMs.Consequently, we do not include a comprehensive evaluation comparison in the paper.Instead, we provide the solid taxonomy, content, and future directions that could serve as both a valuable introduction for newcomers and a comprehensive resource for current researchers in the field.</p>
<p>A Web</p>
<p>Prior works also investigate adapting LLM-based language agents to solve tasks in web environments.However, they typically achieve this via task-independent instructions rather than specific role-playing.Here we provide relevant research for leveraging LLMs in web environment.</p>
<p>In this environment, LLMs operate web navigation autonomously, performing actions such as clicking items, capturing contents, and searching from external knowledge on the web, without a specific persona assigned.Certainly, web tasks involve two key components: HTML understanding and visual grounding, which are highly related to the effectiveness of web agents (Zheng et al., 2024;Koh et al., 2024).Meanwhile, a stream of works, compiled in Table 1, proposes several benchmarks to assess web agents in diverse aspects.HTML Understanding.Kim et al. (2024a) showcase that the ability of HTML understanding is inherent in LLMs with the Recursive Criticism and Improvement (RCI) prompting method.However, due to the special formats and long context elements of HTML which are hard for LLMs to process and respond accurately, most research enhances this capability through fine-tuning methods (Gur et al., 2022(Gur et al., , 2023;;Deng et al., 2024).</p>
<p>Visual Grounding.Another line of research focuses on the visual grounding aspect of HTML understanding, which directly operates on rendered webpages instead of the HTML source code.Some literature proposes web agent frameworks, such as CogAgent (Hong et al., 2023b) and SeeClick (Cheng et al., 2024), leveraging Large Multi-modal Models (LMMs) (Achiam et al., 2023;Team et al., 2023).With additional information from webpage screenshots, LMMs usually outperform text-based LLMs (Zheng et al., 2024).</p>
<p>Figure 1 :
1
Figure 1: In Role-Playing, LLMs act according to assigned personas (i.e., roles) under a defined environment.For example, given role names with descriptions, LLMs role-play in a social simulation game.For Personalization, LLMs consider user personas to generate tailored responses to the same question.Dashed rectangles are prompts and solid rectangles are LLMs' responses.</p>
<p>Figure 3 :
3
Figure 3: An illustration of four LLM role-playing environments: Software Development ( §2.1.1),Game ( §2.1.2),Medical Application ( §2.1.3),and LLM as Evaluators ( §2.1.4).For each environment, we provide a simple scenario with a task description (red-bordered) and relevant personas (i.e., roles; blue-bordered).The dashed rectangle represents an example LLM role-playing prompt template.In addition to the above environments, past research also proposes general frameworks applicable to different environments ( §5.1).</p>
<p>Figure 4: An illustration of five types of personalized LLMs: Recommendation ( §3.1), Search ( §3.2), Education ( §3.3), Healthcare ( §3.4), and Dialogue ( §3.5).On the left side, dashed rectangles are prompts, and solid rectangles are the responses of LLMs.On the right side, we depict multi-turn interactions between LLMs and users.</p>
<p>develop a method for efficient incorporation of users' personal information.Li et al. (2023b) combine aspect extraction with aspectbased recommendations via LLMs prompt tuning.Chen et al. (2022a) generate personalized chitchat to enhance recommendation.Focusing on the framework design,Yang et al. (2023b) present a novel LLM fine-tuning recommendation system.Chu et al. (2023) merge different recommendation systems to effectively integrating the commonsense and reasoning abilities of LLMs.Hu et al. (2024) propose a sequential recommendation framework to preserve fine-grained item textual information.</p>
<p>generate truthful responses by augmenting the dialogue history with reliable sources and use prompts to guide LLM according to predefined dialogue settings.Li et al. (2024c); Hu et al. (</p>
<p>Table 1 :
1
Comparison between recent benchmarks in the web environment.Realistic Env.denotes whether the benchmark's environments are based on actual web pages or realistic web navigation simulations.Dynamic Interaction indicates whether the benchmark supports dynamic interactions rather than remaining in static states.Visual Needed denotes whether the benchmark involves visually grounded tasks.Assessment refers to the types of assessment.An end-to-end benchmark includes tasks with simple instructions, requiring step-by-step solutions to reach the final answers.A fine-grained benchmark contains tasks with a detailed assessment of essential skills in the web environment such as Optical Character Recognition (OCR), and semantic understanding.
Benchmark#Instances #DomainsRealistic Env.Dynamic Interaction Needed VisualAssessmentWebShop (Yao et al., 2022a)12,0871✗✓✗End-to-endMind2Web (Deng et al., 2024)2,3505✓✗✗End-to-endWebArena (Zhou et al., 2023)8124✓✓✗End-to-endVisualWebArena (Koh et al., 2024)9103✓✓✓End-to-endVisualWebBench (Liu et al., 2024)1,50012✓✗✓Fine-grainedPaperSceneDatasetMethodTaskLi et al. (2023b)&amp; TV, Hotel, MoviesAmazon, Yelp TripAdvisor,Prompting, Embeddings,Aspect extraction, Rating PredictionRestaurantFine-tuningP5 (Geng et al.,Sports, Beauty,Amazon (Ni et al.,Pretraining,Rating Prediction, Sequential Recommendation,2022)Toys, Yelp2019), YelpPromptingExplanation Generation, Review Generation,and Direct RecommendationPETER Li et al.Hotel, MoviesTripAdvisor,Transformer Rating prediction and Explanation Generation(2021)&amp; TV,Amazon, YelpRestaurantPEPLER (Li et al.,Hotel, Movies,TripAdvisor5Prompting,Explanation Generation2023a)TV and(Hotel), AmazonFine-tuningRestaurant(movies&amp; TV) andYelp7 (restaurant)PALR (Yang et al.,Movies, Beauty MovieLens-1MFine-tuning,User Profile Generation and Direct Recommen-2023b)(Harper andUser ProfiledationKonstan, 2015),Generation,Amazon Beauty (NiRetrievalet al., 2019)Chu et al. (2023)Sports, Outdoors,AmazonFine-tuning Rating Prediction, Sequential Recommendation, Direct Recommendation, Explanation Genera-Beauty, Toystion and Review Summarizationand GamesLiu et al. (2023a)BeautyAmazonPrompting Rating Prediction, Sequential Recommendation, Direct Recommendation, Explanation Genera-tion and Review SummarizationZhang et al. (2023)Video GamesAmazonInstruction tuningSequential Recommendation and Direct Recom-mendationHou et al. (2024)Movies2019), Amazon (Ni et al.,Prompting Sequential RecommendationMovieLens-1MHarper and Konstan(2015)Wang and Lim (2023)MoviesMovieLens-1M Konstan, 2015) (Harper andPrompting Sequential Recommendation and Direct Recom-mendationChen et al. (2022a)News2020), Reddit MIND (Wu et al.,with weak Fine-tuningDirect Recommendationlabels</p>
<p>Table 2 :
2
Liu et al. (2023a)sting research in recommendation.Following the classification ofLiu et al. (2023a), we classify recommendation systems into five types: rating prediction, sequential recommendation, explanation Generation, and review generation, and direct recommendation.</p>
<p>Table 5 :
5
(Zang et al., 2020)odeling from the MultiWOZ dataset(Zang et al., 2020).
PersonaChatI fly airplanes.I enjoy buildingcomputers.My favorite bandis tool.I am in the army.I dropped out ofcollege.
AcknowledgementsWe thank Yu-Ching Hsu and Jia-Yin Foo from National Taiwan University for their assistance and discussion.This work was financially supported by the National Science and Technology Council (NSTC) in Taiwan, under Grants 112-2223-E-002-012-MY5 and 111-2222-E-002-013-MY3, and from the Featured Area Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education (113L900901/113L900902/113L900903).(Li et al., 2017)13,118 102,979 10Table4: A list of commonly used datasets for ToD modeling and user persona modeling.Among them, different versions of MultiWOZ(Budzianowski et al., 2018;Ramadan et al., 2018;Eric et al., 2020;Zang et al., 2020)and PersonaChat(Zhang et al., 2018b)are the most commonly used.Updated versions of MultiWOZ improve in several aspects: data quality, dialogue complexity, schema and ontology updates, and dataset sizes.PersonaChat contains various persona profiles, consisting of background, preferences, and personality traits.These profiles enable the modeling of coherent and contextual multi-turn diverse dialogue scenarios.For applications in user persona modeling,Tu et al. (2023)match individuals with persona-compatible virtual supporters and introduces the MBTI-S2Conv dataset, containing conversations between characters with distinct profiles.Lotfi et al. (2024)andHan et al. (2024)both propose synthetic datasets related to the Big Five personality.
Conversational health agents: A personalized llm-powered agent framework. Iman Mahyar Abbasian, Azimi, M Amir, Ramesh Rahmani, Jain, 2024a26</p>
<p>Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative ai. Elahe Mahyar Abbasian, Iman Khatibi, David Azimi, Zahra Oniani, Shakeri Hossein, Alexander Abad, Ram Thieme, Zhongqi Sriram, Yanshan Yang, Bryant Wang, Olivier Lin, Li-Jia Gevaert, Ramesh Li, Amir M Jain, Rahmani, 2024b</p>
<p>Knowledgeinfused llm-powered conversational health agent: A case study for diabetes patients. Zhongqi Mahyar Abbasian, Elahe Yang, Pengfei Khatibi, Nitish Zhang, Iman Nagesh, Ramesh Azimi, Amir M Jain, Rahmani, 2024c26</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774.17Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Jaewoo Ahn, Taehyun Lee, Junyoung Lim, Jin-Hwa Kim, Sangdoo Yun, Hwaran Lee, Gunhee Kim, arXiv:2405.18027.8Timechara: Evaluating point-in-time character hallucination of role-playing large language models. 2024arXiv preprint</p>
<p>Jaewoo Ahn, Yeda Song, Sangdoo Yun, Gunhee Kim, arXiv:2305.17388.8Mpchat: Towards multimodal persona-grounded conversation. 2023arXiv preprint</p>
<p>Leancontext: Cost-efficient domain-specific question answering using llms. Biplob Md Adnan Arefeen, Srimat Debnath, Chakradhar, 2023</p>
<p>Personalized quest and dialogue generation in role-playing games: A knowledge graph-and language model-based approach. Trevor Ashby, Braden K Webb, Gregory Knapp, Jackson Searle, Nancy Fulda, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems20237</p>
<p>Knowledge-augmented large language models for personalized contextual query suggestion. Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen Herring, Sujay Kumar, Jauhar , 202426</p>
<p>A simulation model for the waterfall software development life cycle. Youssef Bassil, arXiv:1205.6904.32012arXiv preprint</p>
<p>Team roles at work. Meredith Belbin, Victoria Brown, 2022Routledge</p>
<p>MultiWOZ -a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling. Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Milica Osman Ramadan, Gašić, 10.18653/v1/D18-1547Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018219</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 202324arXiv preprint</p>
<p>. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, </p>
<p>A survey on evaluation of large language models. 8</p>
<p>Personalized chit-chat generation for recommendation using external chat corpora. Changyu Chen, Xiting Wang, Xiaoyuan Yi, Fangzhao Wu, Xing Xie, Rui Yan, Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining. the 28th ACM SIGKDD conference on knowledge discovery and data mining2022a518</p>
<p>Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, arXiv:2404.18231.1From persona to personalization: A survey on role-playing language agents. 2024arXiv preprint</p>
<p>When large language models meet personalization: Perspectives of challenges and opportunities. Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, Defu Lian, Enhong Chen, 2023a</p>
<p>Llmempowered chatbots for psychiatrist and patient simulation: application and evaluation. Siyuan Chen, Mengyue Wu, Kenny Q Zhu, Kunyao Lan, Zhiling Zhang, Lyuchun Cui, arXiv:2305.13614.62023barXiv preprint</p>
<p>Wei Chen, Zhiwei Li, Hongyi Fang, Qianyuan Yao, Cheng Zhong, Jianye Hao, Qi Zhang, Xuanjing Huang, Jiajie Peng, Zhongyu Wei, A benchmark for automatic medical consultation system: Frameworks, tasks and datasets. 2022b</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.10848.2,42023c5arXiv preprint</p>
<p>Can language models be instructed to protect personal information. Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter, 2023d9</p>
<p>The first workshop on personalized generative ai@ cikm 2023: Personalization meets large language models. Zheng Chen, Ziyan Jiang, Fan Yang, Zhankui He, Yupeng Hou, Eunah Cho, Julian Mcauley, Aram Galstyan, Xiaohua Hu, Jie Yang, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge Management2023e5</p>
<p>Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, Zhiyong Wu, arXiv:2401.10935Seeclick: Harnessing gui grounding for advanced visual gui agents. 2024arXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2305.01937.42023arXiv preprint</p>
<p>Konstantina Christakopoulou, Alberto Lalama, Cj Adams, Iris Qu, Yifat Amir, Samer Chucri, Pierce Vollucci, Fabio Soldo, Dina Bseiso, Sarah Scodel, Lucas Dixon, Chi, and Minmin Chen. 2023. Large language models for user interest journeys. </p>
<p>Leveraging large language models for pretrained recommender systems. Zhixuan Chu, Hongyan Hao, Xin Ouyang, Simeng Wang, Yan Wang, Yue Shen, Jinjie Gu, Qing Cui, Longfei Li, Siqiao Xue, James Y Zhang, Sheng Li, 2023518</p>
<p>SPECTER: Document-level representation learning using citation-informed transformers. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel Weld, 10.18653/v1/2020.acl-main.207Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Uncovering chatgpt's capabilities in recommender systems. Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, 10.1145/3604915.3610646RecSys '23. ACM. 2Proceedings of the 17th ACM Conference on Recommender Systems. the 17th ACM Conference on Recommender SystemsJun Xu. 2023</p>
<p>Yuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao Yin, Jiaju Lin, Linhao Ye, Zhiyan Tie, Yougen Zhou, Yilei Wang, arXiv:2308.02773Educhat: A large-scale language model-based chatbot system for intelligent education. 202326arXiv preprint</p>
<p>Peopleware: productive projects and teams. Tom Demarco, Tim Lister, 2013Addison-Wesley</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, Yu Su, Advances in Neural Information Processing Systems. 20243618</p>
<p>Ameet Deshpande, Eunjeong Hwang, Vishvak Murahari, Sung Joon, Diyi Park, Yang, Malta. 52024. Proceedings of the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024). Ashish Sabharwal, Karthik Narasimhan, Ashwin Kalyan, St. Julians</p>
<p>Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Toxicity in chatgpt: Analyzing persona-assigned language models. 20232</p>
<p>Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, Shrimai Prabhumoye, Alan W Black, Alexander Rudnicky, Jason Williams, Joelle Pineau, Mikhail Burtsev, Jason Weston, The second conversational intelligence challenge (convai2). 2019219</p>
<p>Self-collaboration code generation via chatgpt. Yihong Dong, Xue Jiang, Zhi Jin, Ge Li, arXiv:2304.07590202323arXiv preprint</p>
<p>Florian E Dorner, Tom Sühr, Samira Samadi, Augustin Kelava, arXiv:2311.05297Do personality tests generalize to large language models?. 20237arXiv preprint</p>
<p>Mul-tiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Goyal, Peter Ku, Dilek Hakkani-Tur, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation ConferenceMarseille, France2020European Language Resources Association. 19</p>
<p>On text-based personality computing: Challenges and future directions. Qixiang Fang, Anastasia Giachanou, Ayoub Bagheri, Laura Boeschoten, Erik-Jan Van Kesteren, Mahdi Shafiee Kamalabad, Daniel Oberski, 10.18653/v1/2023.findings-acl.691Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>LLM agents in interaction: Measuring personality consistency and linguistic alignment in interacting populations of large language models. Ivar Frisch, Mario Giulianelli, guistics. 7Proceedings of the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024). the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024)St. Julians, Malta. Association for Computational Lin2024</p>
<p>Improving language model negotiation with self-play and in-context learning from ai feedback. Yao Fu, Hao Peng, Tushar Khot, Mirella Lapata, arXiv:2305.10142.2202335arXiv preprint</p>
<p>Is your model sensitive? spedac: A new resource for the automatic classification of sensitive personal data. Gaia Gambarelli, Aldo Gangemi, Rocco Tripodi, 10.1109/access.2023.3240089IEEE Access. 112023</p>
<p>Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5). Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang, Proceedings of the 16th ACM Conference on Recommender Systems. the 16th ACM Conference on Recommender Systems202218</p>
<p>Automatically generated summaries of video lectures may enhance students' learning experience. Hannah Gonzalez, Jiening Li, Helen Jin, Jiaxuan Ren, Hongyu Zhang, Ayotomiwa Akinyele, Adrian Wang, Eleni Miltsakaki, Ryan Baker, Chris Callison-Burch, 10.18653/v1/2023.bea-1.31Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.01680202434arXiv preprint</p>
<p>Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sabharwal, Tushar Khot, arXiv:2311.04892.2Bias runs deep: Implicit reasoning biases in persona-assigned llms. 20235arXiv preprint</p>
<p>A real-world webagent with planning, long context understanding. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust, arXiv:2307.12856.172023arXiv preprintand program synthesis</p>
<p>Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, arXiv:2210.03945.17Sharan Narang, Noah Fiedel, and Aleksandra Faust. 2022. Understanding html with large language models. arXiv preprint</p>
<p>Psydial: Personality-based synthetic dialogue generation using large language models. Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang, Kyung-Ah Sohn, 202419</p>
<p>Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, Zhiting Hu, Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. 2024</p>
<p>Maxwell Harper, Joseph A Konstan, The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis). 2015519</p>
<p>Maximizing stylistic control and semantic accuracy in NLG: Personality variation and discourse contrast. Vrindavan Harrison, Lena Reed, Shereen Oraby, Marilyn Walker, 10.18653/v1/W19-8101Proceedings of the 1st Workshop on Discourse Structure in Neural NLG. the 1st Workshop on Discourse Structure in Neural NLGTokyo, JapanAssociation for Computational Linguistics2019</p>
<p>Galaxy: A generative pre-trained model for taskoriented dialog with semi-supervised learning and explicit policy injection. Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, Jian Sun, Yongbin Li, 202219</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352.2Metagpt: Meta programming for multi-agent collaborative framework. 2023a35arXiv preprint</p>
<p>Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, arXiv:2312.08914.17Cogagent: A visual language model for gui agents. 2023barXiv preprint</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, 2024618</p>
<p>Enhancing sequential recommendation via llm-based semantic embedding learning. Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, Jun Zhou, Companion Proceedings of the ACM on Web Conference 2024. 20245</p>
<p>Enhancing large language model induced task-oriented dialogue systems through look-forward motivated goals. Zhiyuan Hu, Yue Feng, Yang Deng, Zekun Li, See-Kiong Ng, Anh Tuan Luu, Bryan Hooi, 202327</p>
<p>On the humanity of conversational ai: Evaluating the psychological portrayal of llms. Jen-Tse Huang, Wenxuan Wang, Eric John Li, Man Ho, Lam , Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Are large pre-trained language models leaking your personal information?. Jie Huang, Hanyin Shao, Kevin Chen, -Chuan Chang, 10.18653/v1/2022.findings-emnlp.148Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Leveraging the potential of large language models in education through playful and game-based learning. Stefan E Huber, Kristian Kiili, Steve Nebel, Richard M Ryan, Michael Sailer, Manuel Ninaus, Educational Psychology Review. 3612024</p>
<p>Are large language models all you need for task-oriented dialogue?. Vojtěch Hudeček, Ondrej Dusek, 10.18653/v1/2023.sigdial-1.21Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 24th Annual Meeting of the Special Interest Group on Discourse and DialoguePrague, CzechiaAssociation for Computational Linguistics2023</p>
<p>Automatic generation of large-scale multi-turn dialogues from Reddit. Daniil Huryn, William M Hutsell, Jinho D Choi, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea20227International Committee on Computational Linguistics</p>
<p>Large language models in education: A focus on the complementary relationship between human teachers and chatgpt. Education and Information Technologies. Jaeho Jeon, Seongyong Lee, 202328</p>
<p>Evaluating and inducing personality in pre-trained language models. Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, Yixin Zhu, Advances in Neural Information Processing Systems. 2024369</p>
<p>Hang Jiang, Xiajie Zhang, Xubo Cao, Jad Kabbara, arXiv:2305.02547Personallm: Investigating the ability of large language models to express big five personality traits. 202327arXiv preprint</p>
<p>Guard: Roleplaying to generate natural-language jailbreakings to test guideline adherence of large language models. Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang, arXiv:2402.03299.82024aarXiv preprint</p>
<p>Health-llm: Personalized retrievalaugmented disease prediction system. Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua, Suiyuan Zhu, Yanda Meng, Zhenting Wang, Mengnan Du, Yongfeng Zhang, 2024b26</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, 103:102274. 2Learning and individual differences. 62023</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, Advances in Neural Information Processing Systems. 2024a3617</p>
<p>Commonsenseaugmented memory construction and management in long-term conversations via context-aware persona refinement. Hana Kim, Kai Tzu-Iunn Ong, Seoyeon Kim, Dongha Lee, Jinyoung Yeo, arXiv:2401.142152024b2arXiv preprint</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried, arXiv:2401.13649Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. 20241718arXiv preprint</p>
<p>Large language models are clinical reasoners: Reasoning-aware diagnosis framework with prompt-generated rationales. Taeyoon Kwon, Kai Tzu-Iunn Ong, Dongjin Kang, Seungjun Moon, Jeong Ryong Lee, Dosik Hwang, Beomseok Sohn, Yongsik Sim, Dongha Lee, Jinyoung Yeo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Advances in Neural Information Processing Systems. 2024a36</p>
<p>Personalized transformer for explainable recommendation. Lei Li, Yongfeng Zhang, Li Chen, arXiv:2105.11601.182021arXiv preprint</p>
<p>Personalized prompt learning for explainable recommendation. Lei Li, Yongfeng Zhang, Li Chen, ACM Transactions on Information Systems. 414192023a</p>
<p>Prompt tuning large language models on personalized aspect extraction for recommendations. Pan Li, Yuyan Wang, Ed H Chi, Minmin Chen, 2023b518</p>
<p>Qingyao Li, Lingyue Fu, Weiming Zhang, Xianyu Chen, Jingwei Yu, Wei Xia, Weinan Zhang, Ruiming Tang, Yong Yu, arXiv:2401.08664.9Adapting large language models for education: Foundational capabilities, potentials, and challenges. 2023carXiv preprint</p>
<p>DailyDialog: A manually labelled multi-turn dialogue dataset. Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, Shuzi Niu, Proceedings of the Eighth International Joint Conference on Natural Language Processing. Long Papers. the Eighth International Joint Conference on Natural Language ProcessingTaipei, Taiwan2017119Asian Federation of Natural Language Processing</p>
<p>Xiangnan He, and Tat-Seng Chua. 2024b. A survey of generative search and recommendation in the era of large language models. Yongqi Li, Xinyu Lin, Wenjie Wang, Fuli Feng, Liang Pang, Wenjie Li, Liqiang Nie, </p>
<p>Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, You Zhang, 2023d</p>
<p>Guiding large language models via directional stimulus prompting. Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan, Advances in Neural Information Processing Systems. 2024c367</p>
<p>Can large language models reason about medical questions?. Valentin Liévin, Egeberg Christoffer, Andreas Geert Hother, Ole Motzfeldt, Winther, Patterns. 5342024</p>
<p>LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. Yen-Ting Lin, Yun-Nung Chen, 10.18653/v1/2023.nlp4convai-1.5Proceedings of the 5th Workshop on NLP for Conversational AI. the 5th Workshop on NLP for Conversational AIToronto, CanadaAssociation for Computational Linguistics2023. NLP4ConvAI 2023</p>
<p>. Jerry Liu, 10.5281/zenodo.1234LlamaIndex. 62022</p>
<p>Is chatgpt a good recommender? a preliminary study. Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, Yan Zhang, 2023a18</p>
<p>Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, Xiang Yue, arXiv:2404.05955.18Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding?. 2024arXiv preprint</p>
<p>You impress me: Dialogue generation via mutual persona perception. Qian Liu, Yihong Chen, Bei Chen, Jian-Guang Lou, Zixuan Chen, Bin Zhou, Dongmei Zhang, 10.18653/v1/2020.acl-main.131Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics202027</p>
<p>Once: Boosting content-based recommendation with both open-and closed-source large language models. Qijiong Liu, Nuo Chen, Tetsuya Sakai, Xiao-Ming Wu, 2023b</p>
<p>Personalitychat: Conversation distillation for personalized dialog modeling with facts and traits. Ehsan Lotfi, Maxime De Bruyn, Jeska Buhmann, Walter Daelemans, 202419</p>
<p>Hierarchical question-image co-attention for visual question answering. Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh, 2017</p>
<p>Analyzing leakage of personally identifiable information in language models. Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, Santiago Zanella-Béguelin, 2023</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>The effectiveness of artificial intelligence conversational agents in health care: systematic review. Madison Milne-Ives, Caroline De Cock, Ernest Lim, Melissa Harper Shehadeh, Nick De Pennington, Guy Mole, Eduardo Normando, Edward Meinert, Journal of medical Internet research. 2210e203462020</p>
<p>Star: A schema-guided dialog dataset for transfer learning. E M Johannes, Shikib Mosig, Thomas Mehri, Kober, 202019</p>
<p>Justifying recommendations using distantly-labeled reviews and fine-grained aspects. Jianmo Ni, Jiacheng Li, Julian Mcauley, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)20191819</p>
<p>Introducing chatgpt. 2022OpenAI</p>
<p>Do llms possess a personality? making the mbti test an amazing evaluation for large language models. Keyu Pan, Yawen Zeng, 202327</p>
<p>Paperswithcode, Baidu personachat dataset. 2020</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology202324</p>
<p>Empowering personalized learning through a conversation-based tutoring system with student modeling. Minju Park, Sojung Kim, Seunghyun Lee, Soonwoo Kwon, Kyuseok Kim, arXiv:2403.14071202426arXiv preprint</p>
<p>The waterfall model in large-scale development. Kai Petersen, Claes Wohlin, Dejan Baca, Product-Focused Software Process Improvement: 10th International Conference. Oulu, FinlandSpringer2009. 2009. June 15-17, 200910PROFES</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924.2Communicative agents for software development. 202334arXiv preprint</p>
<p>Large-scale multi-domain belief tracking with knowledge sharing. Paweł Osman Ramadan, Milica Budzianowski, Gašić, 10.18653/v1/P18-2069Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics2018219Short Papers)</p>
<p>Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, Pranav Khaitan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence20203419</p>
<p>Integrating summarization and retrieval for enhanced personalization via large language models. Chris Richardson, Yao Zhang, Kellen Gillespie, Sudipta Kar, Arshdeep Singh, Zeynab Raeesy, Omar Zia Khan, Abhinav Sethy, 2023</p>
<p>Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, arXiv:2404.18416.6Capabilities of gemini models in medicine. 2024arXiv preprint</p>
<p>Towards a search engine for machines: Unified ranking for multiple retrieval-augmented large language models. Alireza Salemi, Hamed Zamani, arXiv:2405.00175.62024arXiv preprint</p>
<p>Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, Vishvak Murahari, arXiv:2407.18416Personagym: Evaluating persona agents and llms. 202489arXiv preprint</p>
<p>Scalable and transferable black-box jailbreaks for language models via persona modulation. Rusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando, 2023</p>
<p>Generative echo chamber? effects of llm-powered search systems on diverse information seeking. Nikhil Sharma, Ziang Vera Liao, Xiao, arXiv:2402.05880202426arXiv preprint</p>
<p>Enhancing videobased learning using knowledge tracing: Personalizing students' learning experience with ORBITS. Shady Shehata, David Santandreu Calonge, Philip Purnell, Mark Thompson, 10.18653/v1/2023.bea-1.8Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)Toronto, CanadaAssociation for Computational Linguistics202326</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 2024363</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023</p>
<p>Generating persona consistent dialogues by exploiting natural language inference. Haoyu Song, Wei-Nan Zhang, Jingwen Hu, Ting Liu, 10.1609/aaai.v34i05.6417Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Identifying multiple personalities in large language models with external evaluation. Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, and Simerjot Kaur. 202429</p>
<p>LLMs simulate big5 personality traits: Further evidence. Aleksandra Sorokovikova, Sharwin Rezagholi, Natalia Fedorova, Ivan P Yamshchikov, Proceedings of the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024). the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024)St. Julians, Malta. Association for Computational Linguistics202427</p>
<p>Comparing traditional and llm-based search for consumer choice: A randomized experiment. Sofia Eleni Spatharioti, David M Rothschild, Daniel G Goldstein, Jake M Hofman, 202326</p>
<p>Large language models could change the future of behavioral healthcare: a proposal for responsible development and evaluation. Shannon Wiltsey Elizabeth C Stade, Lyle H Stirman, Cody L Ungar, Andrew Boland, Schwartz, João David B Yaden, Robert J Sedoc, Robb Derubeis, Johannes C Willer, Eichstaedt, Mental Health Research. 312024</p>
<p>Empirical analysis of training strategies of transformer-based japanese chit-chat systems. Hiroaki Sugiyama, Masahiro Mizukami, Tsunehiro Arimoto, Hiromi Narimatsu, Yuya Chiba, Hideharu Nakajima, Toyomi Meguro, 202119</p>
<p>Chenkai Sun, Ke Yang, Revanth Gangi Reddy, Yi R Fung, Hou Pong Chan, Chengxiang Zhai, Heng Ji, Persona-db: Efficient large language model personalization for response prediction with collaborative data refinement. 2024</p>
<p>Towards a personal health management assistant. M Swift, Allen, Journal of biomedical informatics. 4352010</p>
<p>Democratizing large language models via personalized parameterefficient fine-tuning. Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, Meng Jiang, 20248</p>
<p>Medagents: Large language models as collaborators for zero-shot medical reasoning. Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, arXiv:2311.10537.22023a45arXiv preprintArman Cohan, and Mark Gerstein</p>
<p>Enhancing personalized dialogue generation with contrastive latent variables: Combining sparse and dense persona. Yihong Tang, Bo Wang, Miao Fang, Dongming Zhao, Kun Huang, 10.18653/v1/2023.acl-long.299Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada2023b17Ruifang He, and Yuexian Hou. Association for Computational Linguistics</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805.17Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Quan Tu, Chuanqi Chen, Jinpeng Li, Yanran Li, Shuo Shang, Dongyan Zhao, Ran Wang, Rui Yan, arXiv:2308.10278.19Characterchat: Learning towards conversational ai with personalized social support. 2023arXiv preprint</p>
<p>Exploring safety-utility trade-offs in personalized language models. Anvesh Rao Vijjini, Somnath Basu, Roy Chowdhury, Snigdha Chaturvedi, arXiv:2406.11107.82024arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, arXiv:2305.16291.22023a34arXiv preprintLinxi Fan, and Anima Anandkumar</p>
<p>Pandora's white-box: Increased training data leakage in open llms. Jeffrey G Wang, Jason Wang, Marvin Li, Seth Neel, ArXiv, abs/2402.17012. 92024a</p>
<p>Zero-shot next-item recommendation using large pretrained language models. Lei Wang, Ee-Peng Lim, 2023518</p>
<p>Large language models for education: A survey and outlook. Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S Yu, Qingsong Wen, arXiv:2403.181052024b26arXiv preprint</p>
<p>Xintao Wang, Yunze Xiao, Jen Tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, Yanghua Xiao, Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. 2024c</p>
<p>Recmind: Large language model powered agent for recommendation. Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang, 2024d</p>
<p>Aligning large language models with human: A survey. Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu, arXiv:2307.12966.42023barXiv preprint</p>
<p>Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, arXiv:2307.053002023c1arXiv preprint</p>
<p>Zhilin Wang, Yu Ying Chiu, Yu Cheung Chiu, arXiv:2310.05418Humanoid agents: Platform for simulating human-like generative agents. 2023d23arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Air-Dialogue: An environment for goal-oriented dialogue research. Wei Wei, Quoc Le, Andrew Dai, Jia Li, 10.18653/v1/D18-1419Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics201819</p>
<p>Large language models perform diagnostic reasoning. Cheng-Kuang Wu, Wei-Lin Chen, Hsin-Hsi Chen, arXiv:2307.089222023a24arXiv preprint</p>
<p>MIND: A large-scale dataset for news recommendation. Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, Ming Zhou, 10.18653/v1/2020.acl-main.331Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics20201819</p>
<p>Large language models are diverse role-players for summarization evaluation. Ning Wu, Ming Gong, Linjun Shou, Shining Liang, Daxin Jiang, CCF International Conference on Natural Language Processing and Chinese Computing. Springer2023b24</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864.4The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Baize: An open-source chat model with parameter-efficient tuning on self-chat data. Canwen Xu, Daya Guo, Nan Duan, Julian Mcauley, 10.18653/v1/2023.emnlp-main.385Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore20237Association for Computational Linguistics</p>
<p>Mentalllm: Leveraging large language models for mental health prediction via online text data. Xuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James Hendler, Marzyeh Ghassemi, Anind K Dey, Dakuo Wang, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies20248</p>
<p>Re-fGPT: Dialogue generation of GPT, by GPT, and for GPT. Dongjie Yang, Ruifeng Yuan, Yuantao Fan, Yifei Yang, Zili Wang, Shusen Wang, Hai Zhao, 10.18653/v1/2023.findings-emnlp.165Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023a27Association for Computational Linguistics</p>
<p>Palr: Personalization aware llms for recommendation. Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, Yanbin Lu, 2023b218</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2022b</p>
<p>Yelp, Yelp dataset. 2013</p>
<p>MultiWOZ 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines. Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, Jindong Chen, 10.18653/v1/2020.nlp4convai-1.13Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI. the 2nd Workshop on Natural Language Processing for Conversational AIOnline. Association for Computational Linguistics20201920</p>
<p>MedDialog: Large-scale medical dialogue datasets. Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang, Penghui Zhu, Shu Chen, Pengtao Xie, 10.18653/v1/2020.emnlp-main.743Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Recommendation as instruction following: A large language model empowered recommendation approach. Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, Ji-Rong Wen, 2023618</p>
<p>Llm-based medical assistant personalization with short-and long-term memory coordination. Kai Zhang, Yangyang Kang, Fubang Zhao, Xiaozhong Liu, 2024a26</p>
<p>Personalized llm response generation with parameterized memory injection. Kai Zhang, Lizhi Qing, Yangyang Kang, Xiaozhong Liu, 2024b</p>
<p>Personalizing dialogue agents: I have a dog, do you have pets too?. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston, 10.18653/v1/P18-1205Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics2018a220</p>
<p>Personalizing dialogue agents: I have a dog. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston, 2018bdo you have pets too? 19</p>
<p>Jinman Zhao, Zifan Qian, Linbo Cao, Yining Wang, Yitian Ding, arXiv:2409.13979.8Bias and toxicity in role-play reasoning. 2024arXiv preprint</p>
<p>Gpt-4v (ision) is a generalist web agent, if grounded. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su, arXiv:2401.01614.172024arXiv preprint</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. </p>
<p>Bookgpt: A general framework for book recommendation empowered by large language model. Aakas Zhiyuli, Yanfang Chen, Xuan Zhang, Xun Liang, 2023</p>
<p>Towards persona-based empathetic conversational models. Peixiang Zhong, Chen Zhang, Hao Wang, Yong Liu, Chunyan Miao, 10.18653/v1/2020.emnlp-main.531Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, 10.1609/aaai.v38i17.29946Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, arXiv:2307.13854.18Webarena: A realistic web environment for building autonomous agents. 2023arXiv preprint</p>
<p>Group based personalized search by integrating search behaviour and friend network. Yujia Zhou, Zhicheng Dou, Bingzheng Wei, Ruobing Xievand, Ji-Rong Wen, 2021</p>
<p>Cognitive personalized search integrating large language models with an efficient memory mechanism. Yujia Zhou, Qiannan Zhu, Jiajie Jin, Zhicheng Dou, arXiv:2402.10548202426arXiv preprint</p>
<p>Large language models are built-in autoregressive search engines. Noah Ziems, Wenhao Yu, Zhihan Zhang, Meng Jiang, 202326</p>
<p>Slots: restaurant-area: centre, restaurant-pricerange: expensive State: active_intent: find_restaurant [SYSTEM:] I have several options for you; do you prefer African, Asian, or British food? State: active_intent: find_restaurant, requested_slots: restaurant-food [USER:] Any sort of food would be fine, as long as it is a bit expensive. Could I get the phone number for your recommendation? Slots: restaurant-area: centre. Domain Dialogue, Slots, State, Sounds good, could I get that phone number? Also, could you recommend me an expensive hotel? Slots: restaurant-area: centre. center that's expensive. restaurant-name: Bedouin, restaurant-pricerange: expensive, restaurant-phone State: active_intent: find_restaurant, requested_slots: restaurant-phone</p>
<p>Have a great day! [PERSON 1:] Hello how are u tonight [PERSON 2:] Hi. I am okay. tired, but okay. how are you ? [PERSON 1:] I am doing good should be sleeping i have school but can't sleep [PERSON 2:] I did not finish school, I enlisted in the army instead. Zhang, PERSON 1:]I want to book it for 2 people and 2 nights starting from Saturday. Slots: hotel-bookday: Saturday, hotel-bookpeople: 2, hotel-bookstay: 2 State: active_intent: book_hotel, slot_values: hotel-bookday: Saturday, hotel-bookpeople: 2, hotelbookstay: 2 [SYSTEM:] Your booking was successful. 2018a] I am kinda afraid of heights so not sure flying is for me. PERSON 2:] You should at least try to go up in a plane, it is a blast. Table 6: An example of user persona modeling ( §3.5) from Persona-Chat dataset</p>            </div>
        </div>

    </div>
</body>
</html>
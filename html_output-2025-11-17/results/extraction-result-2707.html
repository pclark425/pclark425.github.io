<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2707 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2707</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2707</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-269448713</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.18231v2.pdf" target="_blank">From Persona to Personalization: A Survey on Role-Playing Language Agents</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas. By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance. RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals. Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones. In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies. We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services. We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation. Afterward, we discuss the fundamental risks, existing limitations, and future prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI applications, which reflects practical user demands that shape and drive RPLA research. Through this work, we aim to establish a clear taxonomy of RPLA research and applications, and facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2707.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2707.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory Mechanism (survey)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Mechanism for Role-Playing Language Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey-level description of memory components for language agents: distinguishes short-term (context window) and long-term (external storage) memory, and discusses roles in persona persistence, personalization, and handling limited context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RPLA (Role-Playing Language Agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>General LLM-powered role-playing agents (no single experimental agent). The survey treats LLMs broadly (GPT-family, Claude, open LLaMA-style models) as the base technology for RPLAs.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term (context window) and long-term (external retrieval-augmented memory / vector DB or natural-language DB)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>External vector storage (embedding index) or natural-language database; conceptual differentiation between transformer context (short-term) and external read/write memory (long-term).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Agent profile (basic info, psychological traits, social relationships), environmental information, past interactions, character knowledge and interaction history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Query / retrieval as required (semantic similarity / relevance-based retrieval implied), i.e., retrieve pertinent long-term items into the model context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Continuously updated during interactions (long-term memory stores ongoing interaction data and can be appended/updated); short-term memory is the current context window.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Maintain continuity across sessions, personalization, preserve persona-consistent behavior, supply background knowledge for role-playing, and mitigate limited transformer context window for long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Survey-level findings: memory enables continuity, personalized responses, and dealing with voluminous persona/interaction data beyond context limits; the effectiveness heavily depends on memory management and retrieval quality; memory addresses long-horizon and personalization needs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Practical limitations noted: transformer context window limits require external retrieval; noisy, sparse, lengthy interaction data make memory management challenging; privacy concerns and potential for irrelevant or cluttered memory if not managed; long-context reasoning and retrieval efficiency are unsolved issues.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Persona to Personalization: A Survey on Role-Playing Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2707.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2707.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Character RPLAs + Long-term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character Role-Playing Language Agents with Long-Term Memory Modules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The survey states that character-style RPLAs (models that role-play established characters) increasingly integrate long-term memory modules to store large character data and interaction histories and retrieve relevant snippets when needed to remain in-character.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Character RPLAs (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agents prompted or fine-tuned to role-play established characters; constructed via parametric pretraining/fine-tuning or nonparametric prompting with external persona data.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Retrieval-augmented long-term memory (external vector DB or natural-language database)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Database of character descriptions/demonstrations and interaction logs; retrieval system that selects relevant character data to inject into the model context (i.e., a key-value/embedding-index style retrieval to fill prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Voluminous character descriptions, demonstrations (dialogues, scenes), and accumulated interaction data with users.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Relevance-based retrieval (retrieve character- and context-relevant descriptions/demonstrations to include in prompt); motivated by addressing limited context length.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Interaction data are continuously produced and appended to memory during user interaction; memory used to fork or adapt characters toward individualized preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Supply background knowledge so the agent can recall character facts, remain in-character, and generalize consistent behaviors to new situations; to provide vividness and fidelity in role-playing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Survey-level claims: long-term memory is increasingly necessary because character data and interaction logs are too large for context windows; retrieval helps supply faithful evidence from source materials and supports personalization; memory improves persona consistency and ability to recall character facts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Including all character data in the prompt is impractical; quality depends on retrieval and filtering; synthesized dialogues may require filtering; long-term memory management is necessary but challenging (sparsity, noise, evolving persona).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Persona to Personalization: A Survey on Role-Playing Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2707.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2707.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RPLAs in Text Games (survey mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Role-Playing Language Agents Applied to Text and Social Games (e.g., Diplomacy / Werewolf / Avalon)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey references game applications where RPLAs are used (strategy and social-deduction games) and states that agents incorporate planning and sometimes memory modules to handle long-horizon planning and multi-agent interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Game-playing RPLAs (examples cited: Cicero / Werewolf / Avalon agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-powered agents configured for multi-agent or single-agent text games; they may use planning strategies (CoT, ReAct) and are described as integrating modules for planning, tools, and memory in general terms.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Diplomacy (Cicero), Social Deduction Games (Werewolf, Avalon) — as cited examples</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Long-horizon, multi-agent strategy and social-deduction games requiring planning, negotiation, social reasoning, and memory of prior interactions and facts about other players.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Survey implies memory would be used for maintaining history of prior moves, social relationships, promises/agreements, and role-consistent behavior across long games, but the paper does not provide experimental specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Survey-level note: RPLAs have achieved strong results in games (e.g., Cicero in Diplomacy matched or surpassed human-level performance), and language-agent architectures that combine planning and memory are positioned as necessary for long-horizon and social tasks; however, no concrete memory ablation or quantitative comparisons are reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>The survey highlights general technical challenges relevant to games: long-context reasoning, efficiency of long context handling, and the need for effective memory/retrieval mechanisms — but provides no game-specific memory failure modes or metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Persona to Personalization: A Survey on Role-Playing Language Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Human-level play in the game of diplomacy by combining language models with strategic reasoning <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Ret-LLM: Towards a general read-write memory for large language models <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Lift yourself up: Retrieval-augmented text generation with self-memory <em>(Rating: 2)</em></li>
                <li>TimeChara: Evaluating point-in-time character hallucination of role-playing large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2707",
    "paper_id": "paper-269448713",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Memory Mechanism (survey)",
            "name_full": "Memory Mechanism for Role-Playing Language Agents",
            "brief_description": "Survey-level description of memory components for language agents: distinguishes short-term (context window) and long-term (external storage) memory, and discusses roles in persona persistence, personalization, and handling limited context windows.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "RPLA (Role-Playing Language Agents)",
            "agent_description": "General LLM-powered role-playing agents (no single experimental agent). The survey treats LLMs broadly (GPT-family, Claude, open LLaMA-style models) as the base technology for RPLAs.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": null,
            "uses_memory": true,
            "memory_type": "short-term (context window) and long-term (external retrieval-augmented memory / vector DB or natural-language DB)",
            "memory_structure": "External vector storage (embedding index) or natural-language database; conceptual differentiation between transformer context (short-term) and external read/write memory (long-term).",
            "memory_content": "Agent profile (basic info, psychological traits, social relationships), environmental information, past interactions, character knowledge and interaction history.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Query / retrieval as required (semantic similarity / relevance-based retrieval implied), i.e., retrieve pertinent long-term items into the model context.",
            "memory_update_strategy": "Continuously updated during interactions (long-term memory stores ongoing interaction data and can be appended/updated); short-term memory is the current context window.",
            "memory_usage_purpose": "Maintain continuity across sessions, personalization, preserve persona-consistent behavior, supply background knowledge for role-playing, and mitigate limited transformer context window for long-horizon planning.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Survey-level findings: memory enables continuity, personalized responses, and dealing with voluminous persona/interaction data beyond context limits; the effectiveness heavily depends on memory management and retrieval quality; memory addresses long-horizon and personalization needs.",
            "memory_limitations": "Practical limitations noted: transformer context window limits require external retrieval; noisy, sparse, lengthy interaction data make memory management challenging; privacy concerns and potential for irrelevant or cluttered memory if not managed; long-context reasoning and retrieval efficiency are unsolved issues.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2707.0",
            "source_info": {
                "paper_title": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Character RPLAs + Long-term Memory",
            "name_full": "Character Role-Playing Language Agents with Long-Term Memory Modules",
            "brief_description": "The survey states that character-style RPLAs (models that role-play established characters) increasingly integrate long-term memory modules to store large character data and interaction histories and retrieve relevant snippets when needed to remain in-character.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "Character RPLAs (generic)",
            "agent_description": "LLM-based agents prompted or fine-tuned to role-play established characters; constructed via parametric pretraining/fine-tuning or nonparametric prompting with external persona data.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": null,
            "uses_memory": true,
            "memory_type": "Retrieval-augmented long-term memory (external vector DB or natural-language database)",
            "memory_structure": "Database of character descriptions/demonstrations and interaction logs; retrieval system that selects relevant character data to inject into the model context (i.e., a key-value/embedding-index style retrieval to fill prompt).",
            "memory_content": "Voluminous character descriptions, demonstrations (dialogues, scenes), and accumulated interaction data with users.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Relevance-based retrieval (retrieve character- and context-relevant descriptions/demonstrations to include in prompt); motivated by addressing limited context length.",
            "memory_update_strategy": "Interaction data are continuously produced and appended to memory during user interaction; memory used to fork or adapt characters toward individualized preferences.",
            "memory_usage_purpose": "Supply background knowledge so the agent can recall character facts, remain in-character, and generalize consistent behaviors to new situations; to provide vividness and fidelity in role-playing.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Survey-level claims: long-term memory is increasingly necessary because character data and interaction logs are too large for context windows; retrieval helps supply faithful evidence from source materials and supports personalization; memory improves persona consistency and ability to recall character facts.",
            "memory_limitations": "Including all character data in the prompt is impractical; quality depends on retrieval and filtering; synthesized dialogues may require filtering; long-term memory management is necessary but challenging (sparsity, noise, evolving persona).",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2707.1",
            "source_info": {
                "paper_title": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RPLAs in Text Games (survey mentions)",
            "name_full": "Role-Playing Language Agents Applied to Text and Social Games (e.g., Diplomacy / Werewolf / Avalon)",
            "brief_description": "Survey references game applications where RPLAs are used (strategy and social-deduction games) and states that agents incorporate planning and sometimes memory modules to handle long-horizon planning and multi-agent interaction.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "Game-playing RPLAs (examples cited: Cicero / Werewolf / Avalon agents)",
            "agent_description": "LLM-powered agents configured for multi-agent or single-agent text games; they may use planning strategies (CoT, ReAct) and are described as integrating modules for planning, tools, and memory in general terms.",
            "base_model_size": null,
            "game_benchmark_name": "Diplomacy (Cicero), Social Deduction Games (Werewolf, Avalon) — as cited examples",
            "game_description": "Long-horizon, multi-agent strategy and social-deduction games requiring planning, negotiation, social reasoning, and memory of prior interactions and facts about other players.",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": "Survey implies memory would be used for maintaining history of prior moves, social relationships, promises/agreements, and role-consistent behavior across long games, but the paper does not provide experimental specifics.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Survey-level note: RPLAs have achieved strong results in games (e.g., Cicero in Diplomacy matched or surpassed human-level performance), and language-agent architectures that combine planning and memory are positioned as necessary for long-horizon and social tasks; however, no concrete memory ablation or quantitative comparisons are reported in this survey.",
            "memory_limitations": "The survey highlights general technical challenges relevant to games: long-context reasoning, efficiency of long context handling, and the need for effective memory/retrieval mechanisms — but provides no game-specific memory failure modes or metrics.",
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2707.2",
            "source_info": {
                "paper_title": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Human-level play in the game of diplomacy by combining language models with strategic reasoning",
            "rating": 2,
            "sanitized_title": "humanlevel_play_in_the_game_of_diplomacy_by_combining_language_models_with_strategic_reasoning"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Ret-LLM: Towards a general read-write memory for large language models",
            "rating": 2,
            "sanitized_title": "retllm_towards_a_general_readwrite_memory_for_large_language_models"
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Lift yourself up: Retrieval-augmented text generation with self-memory",
            "rating": 2,
            "sanitized_title": "lift_yourself_up_retrievalaugmented_text_generation_with_selfmemory"
        },
        {
            "paper_title": "TimeChara: Evaluating point-in-time character hallucination of role-playing large language models",
            "rating": 1,
            "sanitized_title": "timechara_evaluating_pointintime_character_hallucination_of_roleplaying_large_language_models"
        }
    ],
    "cost": 0.020203,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Persona to Personalization: A Survey on Role-Playing Language Agents
9 Oct 2024</p>
<p>Jiangjie Chen jjchen19@fudan.edu 
Fudan University</p>
<p>Xintao Wang xtwang21@m.fudan.edu.cn. 
Fudan University</p>
<p>Rui Xu 
Fudan University</p>
<p>Siyu Yuan 
Fudan University</p>
<p>Yikai Zhang 
Fudan University</p>
<p>Wei Shi 
Fudan University</p>
<p>Jian Xie 
Fudan University</p>
<p>Shuang Li 
Fudan University</p>
<p>Ruihan Yang 
Fudan University</p>
<p>Tinghui Zhu 
Fudan University</p>
<p>Aili Chen 
Nianqi Li 
Lida Chen 
Fudan University</p>
<p>Caiyu Hu 
Fudan University</p>
<p>Shanghai University
3 Wuhan University 4 UC Santa Barbara 5 SystemInc</p>
<p>Siye Wu 
Fudan University</p>
<p>Scott Ren 
Ziquan Fu 
Yanghua Xiao 
Fudan University</p>
<p>From Persona to Personalization: A Survey on Role-Playing Language Agents
9 Oct 2024482F71A27F218C1E7A4C9923F44581B1arXiv:2404.18231v2[cs.CL]https: // openreview. net/ forum? id= xrO70E8UIZ Method Summary Parametric Training (Continual) Pre-training Objective: Knowledge injection. Data: Raw data (books, encyclopedia, etc.). Advantages: Readily available for well-established demographics and charactersDirectly uses the raw data. Disadvantages: Necessitates training for new personasMay cause catastrophic forgetting. Supervised Fine-Tuning Objective: Refining role-playing capabilitiesKnowledge injection. Data: Conversation data. Advantages: Highly effective. Disadvantages: Necessitates data processing and training for new personasPotential information loss during data processing. Reinforcement Learning Objective: Alignment with general or individual usersImproving social reasoning skills. Data: Conversation dataPreference dataAdvantages: Effective for mitigating harmful content. Disadvantages: Cost of human preference dataSparse rewards in multi-agent games. Nonparametric Prompting In-context Learning Objective: Knowledge injectionAlignment with individual users. Data: Raw dataConversation data. Advantages: Highly effectiveTraining-freeConvenient for new personas and personalizationCan incorporate retrieval mechanism for enhanced efficiency. Disadvantages: May require data processing for new personasConsumes more tokens and is restricted by context length
Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas.By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance.RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals.Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones.In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies.We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services.We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation.Afterward, we discuss the fundamental risks, existing limitations, and prospects of RPLAs.Additionally, we provide a brief review of RPLAs in AI products in the market, which reflects practical user demands that shape and drive RPLA research.Through this survey, we aim to establish a clear taxonomy of RPLA research and applications, facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony.</p>
<p>Introduction</p>
<p>Digital life has been a pursuit for humanity for decades, reflecting our deep-rooted fascination with the intersection of technology and human experience.Bridging this pursuit with imaginative concepts, roleplaying AI systems embody the digital life by bringing these personas to life in interactive forms.These systems, which simulate assigned personas, have long been a concept in the human imagination, capturing the essence of our desire to create and interact with artificial beings that can understand, respond, and Figure 1: An overview of various persona types for RPLAs.In this survey, we categorize personas into three types: 1) Demographic Persona, 2) Character Persona, and 3) Individualized Persona.We showcase their definition, data sources, examples, use cases and corresponding applications.engage with us in a seemingly sentient manner.With role-playing agents, various personas can be replicated by their agent counterparts, including historical figures, fictional characters, or individuals in our daily lives.Recently, focusing on the text modality, Role-Playing Language Agents (RPLAs) are coming into reality (Shanahan et al., 2023;Shao et al., 2023;Wang et al., 2024d), which inspires a wide range of novel applications, such as digital clones for individuals (Xu et al., 2024b;Ng et al., 2024), AI characters in chatbots (Wang et al., 2024a), and role-playing video games (Wang et al., 2023a), even stimulating social science research (Rao et al., 2023).As RPLAs become increasingly integrated into our daily lives, it is essential to foster a society that thrives on the synergistic coexistence of humans and these intelligent agents.</p>
<p>Recent developments in Large Language Models (LLMs) (OpenAI, 2023;Google, 2023;Anthropic, 2024) have greatly facilitated the emergence of RPLAs.LLMs grow adept at producing a compelling sense of human likeness (Shanahan et al., 2023;Zhou et al., 2024b), and can be regarded as superpositions of beliefs (Kovač et al., 2023) and personas (Lu et al., 2024).Furthermore, with alignment training, LLMs are able to adhere to the instruction of persona role-playing, including replicating their knowledge (Lu et al., 2024;Li et al., 2023a), linguistic and behavior patterns (Wang et al., 2024a;Zhou et al., 2023a), and even underlying personalities (Shao et al., 2023;Wang et al., 2024d).They are able to both mimic the personas as prompted in the contexts (Wang et al., 2024a;Li et al., 2023a), or harness their inherent parametric knowledge for widely-recognized demographics or characters (Shao et al., 2023;Lu et al., 2024).Considering their practical significance, there has been an increase in research efforts dedicated to RPLAs with LLMs, including their development (Wang et al., 2024a;Li et al., 2023a;Zhou et al., 2023a), analysis (Shao et al., 2023;Yuan et al., 2024b), and applications (Rao et al., 2023;Park et al., 2023;Mysore et al., 2023).Conversely, RPLAs also benefit the development of LLMs and language agents.They offer an ideal perspective and testing ground for investigating the behaviors and capabilities of LLMs and language agents, particularly those related to social interactions (Li et al., 2023d;Chen et al., 2023a;Wu et al., 2024c).They also facilitate the creation of diverse and massive synthetic data for LLM training at scale (Chan et al., 2024).</p>
<p>In this paper, we conduct a comprehensive survey on RPLAs.Our study primarily focuses on the persona and personalization of RPLAs.Specifically, as shown in Figure 1, we categorize personas within RPLA literature at three levels, with a progressive integration of personalized data: 1. Demographic Persona, i.e., focusing on groups of people sharing common characteristics, such as occupations, ethnic groups, personality types, etc.These personas are inherent in LLMs, and role-playing them capitalizes on the statistical stereotypes in LLMs (Huang et al., 2023c;Xu et al., 2023a;Gupta et al., 2023).</p>
<ol>
<li>
<p>Character Persona, which represents well-established and widely-recognized individuals, especially in the existing literature, including celebrities, historical figures, and fictional characters.Role-playing these personas challenges models' capability in understanding curated materials of the existing characters, harnessing knowledge in LLMs' parameters or given contexts (Shao et al., 2023;Wang et al., 2024a;d).</p>
</li>
<li>
<p>Individualized Persona, referring to digital profiles built and continuously updated based on personalized user data.This category emphasizes the unique experiences, needs, and preferences of individuals, aiming for applications such as digital clones or personal assistance (Salemi et al., 2024;Woźniak et al., 2024).RPLAs for these personas underscore their dynamic nature and learning mechanism and frequently focus on interactions with real-world activities (Dalvi Mishra et al., 2022;Chen et al., 2023b;Salemi et al., 2024).</p>
</li>
</ol>
<p>The three types of personas exhibit a progressive relationship and can coexist in RPLAs.For example, an RPLA portraying Socrates as a personal philosophy tutor would encompass the demographic persona of an ancient Greek philosopher, the character persona of Socrates, and an individualized persona that develops through interactions with the user.Following this categorization, we explore common methodologies, fundamental risks, current gaps and limitations, and future prospects of RPLAs in this survey.</p>
<p>In summary, this survey systematically reviews existing literature in the field of RPLAs, and establishes taxonomies for relevant methodologies as shown in Figure 2. The remainder of our paper is structured as follows: §2 introduces the background for RPLAs, covering the roadmap, recent progress, and trends in LLMs and language agents.§3 then presents the overview of current research in RPLAs.§4,5,6 detail the research on RPLAs for demographic, character, and individualized persona, respectively.§7 discusses potential risks of RPLAs, such as toxicity, biases and misuse.Finally, §8 concludes this survey and identifies future directions.Additionally, aiming to bridge the gap between theoretical insights and practical applications for our readers, we also conduct a brief survey of current RPLA products in the rapidly growing market in Appendix A.</p>
<p>Preliminary</p>
<p>The Roadmap of Large Language Models</p>
<p>Recently, LLMs have demonstrated impressive capabilities, with promising potential in approaching humanlevel intelligence (Brown et al., 2020;OpenAI, 2022;Anil et al., 2023;Anthropic, 2023a;b;OpenAI, 2023).</p>
<p>LLMs are artificial neural networks with billions of parameters, trained on vast amounts of natural language data representing human knowledge and intelligence.Their accomplishments extend beyond excelling in NLP tasks to effectively simulating a broader range of human behaviors.Specifically, they have showcased more nuanced capabilities towards anthropomorphic cognition, including humanity emulation (Shanahan et al., 2023;Huang et al., 2023c) and social intelligence (Kosinski, 2023;Li et al., 2023d;Kim et al., 2023b), thus producing a compelling sense of human likeness.As a result, advancements in LLMs have significantly facilitated the creation of intelligent RPLAs (Park et al., 2023;Sclar et al., 2023;Shao et al., 2023), establishing new effective methodologies different from previous models.</p>
<p>Emerged Abilities in LLMs</p>
<p>Several key abilities have emerged in LLMs (Wei et al., 2022a) throughout their evolution, including in-context learning (Brown et al., 2020), instruction following (Ouyang et al., 2022), step-by-step reasoning (Wei et al., 2022b), and social intelligence (Wang et al., 2024b;Sclar et al., 2023;Light et al., 2023), which lay the foundation for complicated role-playing behavior of LLMs towards RPLAs.First, the in-context learning ability allows LLMs to learn information from prompts without parameter updates.This facilitates LLMs' adaptation to the provided knowledge of various characters and mimicking their behaviors by following example demonstrations.Second, the instruction following ability enables</p>
<p>LLM-powered Language Agents</p>
<p>The AI community has long been pursuing the concept of "agent", approaching the intelligence and autonomy of humans.Traditional symbolic agents (Bernstein, 2001;Küngas et al., 2004) and reinforce-learning agents (Fachantidis et al., 2017;Florensa et al., 2018) mainly optimize their actions based on rules or predefined rewards.Research in language agents primarily focuses on training within constrained environments with limited knowledge, diverging from the complex and diverse nature of the human learning process.However, such agents struggle to emulate complicated human-like behaviors, particularly in open-domain settings (Mnih et al., 2015;Lillicrap et al., 2015;Schulman et al., 2017;Haarnoja et al., 2017).Recently, LLMs have demonstrated remarkable capabilities with promising potential in achieving human-level intelligence, which has sparked a rise in research focusing on LLM-based language agents (Sclar et al., 2023;Chalamalasetti et al., 2023;Liu et al., 2023d;Xie et al., 2024b).Research in this area primarily involves equipping LLMs with essential human-like capabilities, such as planning, tool-usage and memory (Weng, 2023), which are essential for developing advanced RPLAs with anthropomorphic cognition and abilities.</p>
<p>Planning Module In many real-world scenarios, the agents need to make long-horizon planning to solve complex tasks (Rana et al., 2023;Yuan et al., 2023).When facing these tasks, LLM-powered agents could decompose the complex tasks into subtasks and adopt various planning strategies, e.g., CoT (Wei et al., 2022b) and ReAct (Yao et al., 2023b), to adaptively plan for the next action with feedback from environments (Wang et al., 2023a;Gotts et al., 2003;Wang et al., 2023i;Song et al., 2023;Zhang et al., 2024b).For RPLAs, these adaptive planning strategies enable them to simulate realistic and dynamic interactions in complicated environments such as games (Wang et al., 2023a) and social simulations (Park et al., 2023).</p>
<p>Tool-usage Module</p>
<p>Although LLMs excel in various tasks, they may struggle in domains requiring extensive expertise and experience hallucination issues (Gou et al., 2023;Chen et al., 2023e;Wang et al., 2023f).To address these challenges, agents could apply external tools for action execution (Shen et al., 2023b;Lu et al., 2023;Schick et al., 2023;Parisi et al., 2022;Yang et al., 2023b;Yuan et al., 2024a).The tools include real-world APIs (Patil et al., 2023;Li et al., 2023g;Qin et al., 2023;Xu et al., 2023b;Shen et al., 2023c), knowledge bases (Zhuang et al., 2024;Hsieh et al., 2023), external models (Bran et al., 2023;Ruan et al., 2023), and customized actions for specific applications (Wang et al., 2023a;Zhu et al., 2023b).For RPLAs, these tools typically enable them to interact with the environments, e.g., games or software applications.The integration of external tools enhances role-playing and generative agents by enabling them to execute actions and access information beyond their intrinsic capabilities.This facilitates more accurate and contextually appropriate interactions, particularly in specialized or complex scenarios, thereby significantly improving the quality and effectiveness of their responses in user engagements.</p>
<p>Memory Mechanism</p>
<p>The memory mechanism stores the profile of agents along with environmental information to assist agents in future actions.The profile typically includes basic information (age, gender, career), psychological traits (reflecting personality), and social relationships (Wang et al., 2023c;Park et al., 2023;Qian et al., 2023), which can be manually created (Caron &amp; Srivastava, 2022;Zhang et al., 2023a;Pan &amp; Zeng, 2023;Huang et al., 2023b;Karra et al., 2022;Safdari et al., 2023) or generated from models (Wang et al., 2023c).This module enables agents to accumulate experiences, evolve, and act consistently and effectively (Park et al., 2023).Language agents draw from cognitive science research on human memory, which progresses from sensory to short-term, then to long-term memory (Atkinson &amp; Shiffrin, 1968;Craik &amp; Jennings, 1992).The short-term memory is regarded as the information input within the constraint window of transformer architecture (Fischer, 2023;Rana et al., 2023;Wang et al., 2023i;Zhu et al., 2023a).In contrast, long-term memory is usually reserved in the external vector storage (Qian et al., 2023;Zhong et al., 2023;Zhu et al., 2023b;Lin et al., 2023;Xie et al., 2023;Wu et al., 2024b) or natural languages database (Shinn et al., 2023;Modarressi et al., 2023), from which agents can quickly query and retrieve information as required.</p>
<p>Compared to vanilla LLMs, language agents need to learn and perform tasks in changing environments.For RPLAs, the memory mechanism plays a pivotal role by enabling these agents to maintain continuity and context in interactions over time.By storing and retrieving user-specific data and environmental context, agents deliver more personalized and relevant responses, thus enhancing user experience and engagement in diverse scenarios.</p>
<p>Overview of RPLAs</p>
<p>In this section, we present a concise overview of current research on RPLAs.</p>
<p>RPLA Definition</p>
<p>Our survey distinguishes personas into three categories, progressing from broad groups to individual specificity: demographic persona, character persona, and individualized persona, defined as follows:</p>
<ol>
<li>
<p>Demographic Persona represents the aggregated characteristics and behaviors of distinct demographic segments, including occupations, genders, ethnicity, and personality types.In the context of RPLAs, these personas operate as fictional archetypes, derived from the comprehensive pre-training datasets of LLMs.Employing these archetypes, the development of RPLAs can be efficiently facilitated through simple prompts, such as "You are a mathematician."Constructed in this way, demographic RPLAs can be effectively employed for simulations specific to demographic groups and for addressing specialized tasks.</p>
</li>
<li>
<p>Character Persona denotes well-established characters, encompassing both real-world public figures and fictional entities, each characterized by definitive attributes and narratives.The RPLAs for these characters are constructed using data derived from diverse sources such as biographies, novels, and films.Primarily, these RPLAs are designed to fulfill entertainment and emotional engagement needs, functioning as AI-driven chatbots or virtual characters in video games.</p>
</li>
<li>
<p>Individualized Persona refers to personal profiles constructed from the behavioral and preference data of specific individuals, encompassing personal profiles, dialogues, and a range of actions and behaviors.This data is subject to continuous evolution, necessitating that the corresponding RPLAs  adapt dynamically to these changes.Individualized RPLAs provide customized services tailored to the needs of individual users across various AI-based applications, where they commonly function as personalized assistants, companions, or proxies.</p>
</li>
</ol>
<p>RPLA Construction</p>
<p>Role-Playing Language Agents (RPLAs) are primarily developed to simulate intricate personas based on various individual profiles and narratives.These profiles are constructed using diverse persona data, including descriptive narratives, dialogues, historical behaviors, and extensive textual materials such as books (Zhang et al., 2018;Dinan et al., 2020;Shanahan et al., 2023;Wang et al., 2024a;Shao et al., 2023;Xu et al., 2023a;Li et al., 2023f).</p>
<p>The methodologies for building RPLAs typically involve either parametric training (Shao et al., 2023;Wang et al., 2024a;Qin et al., 2024) or nonparametric prompting (Dalvi Mishra et al., 2022;Li et al., 2023a;Zhou et al., 2023a;Gupta et al., 2023;Ma et al., 2023a;Zhao et al., 2023b), as summarized in Table 1.These methods may concurrently contribute to the development process.</p>
<p>Parametric training for RPLAs primarily includes pre-training, supervised fine-tuning (SFT) and reinforcement learning (RL).Initially, LLMs for RPLAs are pre-trained on large-scale raw text, including literary works and encyclopedic entries (Xu et al., 2023a;Gupta et al., 2023), equipping them with a broad knowledge of massive demographic and character personas.Subsequently, these LLMs undergo SFT on role-playing datasets (Wang et al., 2024a;Shao et al., 2023), which enhance both their role-playing capabilities and character-specific knowledge.Additionally, RL methods could further refine RPLAs in terms of: 1) Alignment with general users, e.g., improving attractiveness or mitigating harmful content, with preference data from online application users and invited human annotators (RLHF) or synthesized by LLM (Bai et al., 2022b;Ouyang et al., 2022).2) Improving social reasoning skills, e.g., in games (Cheng et al., 2024) or goal-driven Published in Transactions on Machine Learning Research (10/2024) conversations (Wang et al., 2024b).3) Alignment with individual users (Shaikh et al., 2024a;Jang et al., 2023).</p>
<p>Conversely, nonparametric prompting provides RPLAs with persona data and role-playing instructions within the context.Prompts for RPLAs primarily consist of persona data that represents the intended personas, including descriptions and demonstrations.Descriptions (or profiles) represent their basic information such as names, backgrounds, experiences, personalities, tones, catchphrases and other attributes (Wang et al., 2024a;Yuan et al., 2024b).Demonstrations illustrate representative behaviors to further align RPLAs with the intended personas, covering dialogues, behaviors, interactions, preferences, stories or other modalities (Li et al., 2023a;Chen et al., 2023c;Dai et al., 2024a).There are several methods for crafting persona data, including: 1) Online Resource Collection, which gathers information from online resources such as Wikipedia, Supersummary and Fandom for widely-known characters (Shao et al., 2023).2) Automatic Extraction, where LLMs automatically extract persona data such as dialogues from their origins, e.g., books or scripts (Li et al., 2023a).3) Dialogue Synthesis, which employs advanced LLMs to create and expand role-playing conversation datasets via in-context learning (Li et al., 2023a) or role-playing as the personas (Ran et al., 2024).If provided with corresponding literature for reference (for character personas), this is akin to automatic extraction and the synthesized dialogues are more faithful to the origins.Otherwise, the synthesized data is of limited quality and often necessitates filtering.4) Human Annotation, which engages human annotators or character fans to summarize persona descriptions or craft high-quality role-playing conversations (Zhou et al., 2023a).Besides, role-playing instructions or requirements could be incorporated to encourage or restrict specific behaviors of RPLAs.Furthermore, modern RPLAs increasingly integrate memory modules to retrieve information from extensive datasets on character traits or past interactions.This development addresses the restricted context capacity of current LLMs (Shao et al., 2023;Mysore et al., 2023;Sun et al., 2024).</p>
<p>In terms of alignment with persona types, parametric learning tends to focus on demographic information and well-known characters, whereas prompting techniques are generally employed for generating fictional personas and highly personalized characters.Current research in RPLA development generally focuses on steering LLMs with demographics (Zhang et al., 2023b;Hong et al., 2023), developing foundation models (Lu et al., 2024;Zhou et al., 2023a), designing agent frameworks (Li et al., 2023a;Wang et al., 2024a) for RPLAs, and crafting persona profiles for specified individuals (Li et al., 2023a;Wang et al., 2024a;Ahn et al., 2023).</p>
<p>RPLA Evaluation</p>
<p>For RPLA evaluation, we distinguish the criteria into two primary categories: role-playing capability evaluation for RPLA methodologies, and persona fidelity evaluation for specific personas (Wang et al., 2024d).The role-playing capabilities of RPLAs are evaluated on their foundation models and construction frameworks, regardless of specific personas.These evaluations concern aspects such as anthropomorphic abilities, attractiveness, and usefulness, which encompass more granular dimensions including conversation ability (Shao et al., 2023), engagement (Zhou et al., 2023a) 1 , persona consistency (Wang et al., 2024d), emotion understanding (Huang et al., 2023a), theory of mind (Kosinski, 2023), and problem-solving ability (Xu et al., 2023a).Persona fidelity, by contrast, concentrates on whether individual RPLAs well replicate the intended personas, including their knowledge (Shao et al., 2023;Li et al., 2023a), linguistic habits (Wang et al., 2024a;Deshpande et al., 2023), personality (Wang et al., 2024d;Huang et al., 2023c), beliefs (Li et al., 2024a;Wang et al., 2023e), and decision-making (Xu et al., 2024c;Chen et al., 2023a).Current methods for evaluation are mainly three-fold: 1) automatic evaluation with ground truth, 2) automatic evaluation without ground truth, 3) multi-choice questions, 4) human-based evaluation.Overall, current RPLAs have demonstrated promising and improving performance in simulating personas.However, there remain significant gaps between existing RPLAs and fully human-level intelligent agents (Wang et al., 2024a;Chen et al., 2023a) that are faithful to the personas, as well as corresponding evaluation methods for more nuanced role-playing.</p>
<p>Demographic Persona</p>
<p>Definition</p>
<p>RPLAs assigned with demographic personas are expected to display unique characteristics of specific groups of people.Within this context, demographics capture typical traits associated with groups possessing common characteristics, such as occupational roles (e.g., a mathematician), hobbies or interests (e.g., a baseball enthusiast), and personality types (e.g., the ENFJ category from the Myers-Briggs Type Indicator), etc.These representations in RPLAs meld the linguistic style, professional knowledge, and behavioral nuances representative of a demographic archetype.</p>
<p>These RPLAs are designed to mimic how a specific demographic processes and engages with information and communication channels, reflecting its unique language preferences, domain-specific vocabulary, and distinctive viewpoints.This transformation aims to translate the broad and flexible capabilities of RPLAs into complex virtual representations that reflect the intellectual subtleties, personal inclinations, and social complexities of the demographic.By embodying specific groups, demographic RPLAs can enhance their abilities in certain areas, and also utilize a variety of RPLAs representing different demographics for social experiments, the completion of more complex tasks, etc.</p>
<p>Analysis of Demographics</p>
<p>RPLAs possess inherent demographics that reflect nuanced human-like characteristics, including personality traits, political beliefs, and ethical considerations, which vary in different LLMs.Furthermore, RPLAs have the ability to role-play specified demographics, altering their behavior and potentially enhancing their performance on specific tasks, but this may also lead to toxic outputs and biases, depending on the persona assigned.</p>
<p>Inherent Demographics RPLAs may inherently reflect specific demographic characteristics due to patterns present in the data used during pretraining.These patterns encapsulate human tendencies and biases originating from diverse sources (Karra et al., 2022;Serapio-García et al., 2023;Gupta et al., 2024).Subsequently, RPLAs could encode individual behavioral traits in textual outputs, inadvertently resulting in a disproportionate emphasis on certain demographics over others (Jiang et al., 2023a).</p>
<p>To harness RPLAs for specific applications effectively, it is essential to understand their inherent demographics.The demographic characteristics of RPLAs can be explored through established human psychological assessments such as the Big Five Personality Test (Barrick &amp; Mount, 1991).By subjecting RPLAs to text-based questionnaires designed for humans, researchers could leverage their textual response capabilities to evaluate behavioral responses similar to human subjects (Huang et al., 2024a).Such evaluations have revealed that RPLAs exhibit consistent inherent demographics, which have been statistically confirmed in recent studies (Jiang et al., 2023a;Serapio-García et al., 2023;Santurkar et al., 2023).However, it is important to recognize that these demographics may differ in different LLMs (Huang et al., 2023b).</p>
<p>Beyond personality characteristics, RPLAs often display complex demographics reflecting nuanced social, economic, and ethical understanding.For instance, RPLAs may exhibit a preference for certain political beliefs (Hartmann et al., 2023), show decision-making patterns indicative of rational economic considerations (Guo et al., 2023), and act either selfishly or helpfully in multi-agent simulations (Chawla et al., 2023).</p>
<p>Demographic Role-Playing RPLAs are embedded with intrinsic demographic characteristics, which raises pivotal questions about their ability to role-play specified demographics and the subsequent effects on their behavior.A prevalent approach in demographic role-play involves directly prompting the language agent.For example, if an LLM is prompted with, "You're a friendly and outgoing individual who thrives on social interactions.Always ready for a good time, you enjoy being the center of attention at parties..." (Jiang et al., 2023a;Xie et al., 2024a), it adopts the persona of an extroverted character.When tasked with representing distinct demographics, RPLAs demonstrate the capacity to diverge from their inherent traits, manifesting changes in their responses on psychological assessment scales (Jiang et al., 2023a;Serapio-García et al., 2023).</p>
<p>This behavioral adaptability highlights the potential of RPLAs in simulating diverse human-like roles and personalities.</p>
<p>However, not all assigned personas lead to superior performance of RPLAs.Assigning a persona to LLMs may also result in toxic or biased outputs compared to the default setting, because the persona may amplify existing stereotypes and biases present in the training data.For example, the assignment of some personas to language agents, including baseline personas such as "a bad person", has been demonstrated to significantly increase the likelihood of RPLAs generating toxic outputs (Deshpande et al., 2023).Similarly, diverse demographic roles have been assigned to reveal the biased presumptions present in LLMs (Gupta et al., 2023).Although some developers have made attempts to prevent RPLAs from malicious usage, attacking the prompts via "jailbreaking" (Chao et al., 2023;Anil et al.) might bypass these safety mechanisms and elicit offensive, toxic, misleading contents.</p>
<p>Application of Demographics</p>
<p>By assigning specific demographics, LLMs often have better performance in various types of downstream tasks, whether agents are used in a standalone fashion (single-agent systems) or joint with other agents (multi-agent systems) for competition or collaboration.</p>
<p>Improving Task Solving in Single-Agent Systems Assigning specific demographics enables LLMs to enhance their performance in tasks that require specialized knowledge tied to those personas.For instance, when an LLM is configured to represent an expert LLM within a specific field, it might significantly augment the length, depth, and quality of its responses, which is also showcased in complex zero-shot reasoning tasks, where the model must generate insightful answers without prior direct training on similar problems (Xu et al., 2023a;Kong et al., 2023).Furthermore, integrating diverse social roles into LLMs' frameworks has been shown to positively influence their performance across a wide array of tasks, suggesting a versatile adaptability to different contextual demands (Zheng et al., 2023a).The application of these roles enables LLMs not only to generate more contextually appropriate responses but also to exhibit increased understanding and engagement in interactions that reflect varied human experiences and societal norms.</p>
<p>Improving Task Solving in Multi-Agent Systems Building upon the capability of single-agent models, which utilize demographic personas to bolster their specialized abilities, assigning demographic personas in multi-agent systems has also emerged as a crucial strategy for enhancing the performance of single-agent systems, i.e., standalone LLMs.By embedding various personas within agents, distinct societal dynamics could be cultivated, leading to improved strategies for cooperative problem-solving and breakthroughs in complex domains such as mathematical modeling (Zhang et al., 2023b;Wang et al., 2023g).A notable implementation of this approach is ChatDev (Qian et al., 2023) and MetaGPT (Hong et al., 2023), frameworks designed specifically for automating software development within a multi-agent conversational platform.In this setup, different agents are assigned specialized roles that collectively contribute to the agile development of software applications.This collaborative model echoes the strategies applied in projects such as OKR-AGENT (Zheng et al., 2023b), where role-specific enhancements within multi-agent architectures have shown to significantly streamline and optimize task execution.</p>
<p>Simulating Collective Social Behaviors in Multi-Agent Systems</p>
<p>RPLAs have demonstrated remarkable capabilities in simulating nuanced, human-like interactions across various environments.In the realm of gaming, particularly in strategy and role-playing scenarios, RPLAs have shown impressive performance.For example, Chawla et al. (2023) set the agents to be fair or selfish, and shows that selfish agents could contribute not only to their own interests but also to the collective good.Additionally, more elaborate games like Social Deduction Games are particularly illustrative of RPLAs' capacity to effectively adopt varied roles, as observed in scenarios such as "The Werewolf" (Xu et al., 2023c) and "The Avalon" (Wang et al., 2023d).In diplomacy-focused games such as Cicero, RPLAs have matched or even surpassed human levels of performance (FAIR et al., 2022).Similarly, in war simulation games, RPLAs provide valuable insights into the origins of conflicts, enhancing our understanding of complex geopolitical dynamics (Hua et al., 2023).Extending the application of RPLAs beyond gaming environments, these models are also utilized to mimic daily social interactions, thereby narrowing the behavioral gap between artificial agents and humans.This is exemplified in the development of Humanoid Agent frameworks (Wang et al., 2023h), which embody System 1 functionalities-such as basic needs and emotions-to enhance realism and effectiveness in replicating human responses and behaviors.Furthermore, recent findings in multi-agent interaction environments have revealed that diversifying the types of agents, scaling up their number, and increasing interactions, lead to the emergence of unplanned social behaviors.Such behaviors arise spontaneously from discussions among multiple agents, highlighting the potential for complex, dynamic systems within LLM architectures (Gu et al., 2024).This progression from specific gaming applications to broader social simulations illustrates the expanding versatility and depth of RPLAs in understanding and replicating human-like behavior.</p>
<p>Character Persona</p>
<p>Definition</p>
<p>Characters are primarily established characters with their stories widely recognized by the public, including celebrities, historical figures and fictional characters (e.g., Monkey D. Luffy and Hermione Granger).Occasionally, they also include original characters created by individuals (Zhou et al., 2023a).Character RPLAs have recently emerged as a flourishing field of LLM application (e.g., Character.ai),and hence attracted wide research interest as well (Shao et al., 2023;Wang et al., 2024a;d).</p>
<p>For character RPLAs, the essential requirement for effective role-playing is the ability of LLMs to understand characters.Early research has studied character understanding of language models, involving linking descriptions that outline characters' traits to their roles (i.e., Character Prediction) and personalities (i.e., Personality Understanding): 1) Character prediction mainly focuses on recognizing characters from a provided text.This includes tasks like co-reference resolution (Li et al., 2023c), relationship understanding (Zhao et al., 2024) and character identification (Brahman et al., 2021;Yu et al., 2022;Li et al., 2023c;Zhao et al., 2024).Additionally, some studies investigate if language models can forecast characters' future actions based on; 2) Personality understanding aims to decode character traits from their dialogues and actions, including predicting the depicted traits (Yu et al., 2023) and generating character descriptions (Brahman et al., 2021).</p>
<p>In recent years, LLMs have demonstrated strong capabilities in language understanding and generation, which significantly advanced the development of RPLAs.The research focus in this direction has hence shifted towards applying and promoting LLMs to faithfully reproduce the characters, including their linguistic style (Wang et al., 2024a;Zhou et al., 2023a;Li et al., 2023a;Wang et al., 2024a), knowledge (Li et al., 2023a;Shao et al., 2023;Zhou et al., 2023a;Chen et al., 2023c;Zhao et al., 2023a;Wang et al., 2024a), personality (Shao et al., 2023;Wang et al., 2024d), and even decision-making (Zhao et al., 2023a;Xu et al., 2024c).</p>
<p>Data for Character RPLAs</p>
<p>Character data is indispensable for the construction of character RPLAs.The data that represents knowledge of these well-established characters can be roughly categorized into two types: 1) Descriptions directly describe the character personas that guide the behaviors of RPLAs.These include various character attributes, such as identity, relationships, and other predetermined attributes.The attributes serve as the knowledge background and are expected to be accurately recalled upon request, such as names and affiliations (Li et al., 2023a;Zhou et al., 2023a;Shao et al., 2023;Wang et al., 2024a;Chen et al., 2023c;Zhao et al., 2023a;Tu et al., 2024;Lu et al., 2024).Additionally, some descriptions further shape the behaviors of RPLAs, such as personality traits (Li et al., 2023a;Wang et al., 2024d).2) Demonstrations, on the other hand, are representative behaviors of the characters, which reflect their linguistic, cognitive and behavioral patterns (Li et al., 2023a;Zhou et al., 2023a;Shao et al., 2023;Wang et al., 2024a;Zhao et al., 2023a;Chen et al., 2023c;Tu et al., 2024;Tang et al., 2024;Lu et al., 2024;Xu et al., 2024b).While RPLAs are not expected to replicate the exact outputs from the demonstration data, they should portray these patterns and generalize to new situations, i.e., producing responses consistent with the demonstrations.Overall, descriptions provide the core and foundational information for RPLAs, while demonstrations, though not mandatory, are also crucial for achieving vividness and fidelity of RPLAs (Wang et al., 2024d).The available data for character RPLAs is currently quite limited, covering only a small selection of characters.The description data are typically sourced from well-curated encyclopedias or the original works, and processed manually or with advanced LLMs (Shao et al., 2023;Li et al., 2023a).The demonstration data are crafted in various ways, where the common methodologies include:</p>
<ol>
<li>
<p>Experience Extraction extracts characters' dialogues or other scenes from original scripts (Li et al., 2023a;Wang et al., 2024a).The extracted scenes faithfully depict the characters.However, understanding and reproducing these scenes for RPLAs may be impractical without more complete background knowledge, making it less suitable to train LLMs with this data.</p>
</li>
<li>
<p>Dialogue Synthesis synthesizes character conversations using state-of-the-art LLMs to build and augment datasets for character RPLAs.The topics for these conversations could be sourced from corresponding literature (Shao et al., 2023), general task instructions (Wang et al., 2024a), special scenarios such as personality tests (Wang et al., 2024d), and real use cases (Gosling et al., 2023).LLMs could be leveraged to augment the datasets with more role-playing responses by either generating dialogues similar to given ones via in-context learning (Li et al., 2023a), or by role-playing as RPLAs themselves with existing character data to respond to specified topics (Ran et al., 2024).When referring corresponding literature, this method is close to experience extraction and yields dialogues that are more faithful to the origins.Otherwise, this process essentially serves as a knowledge distillation of role-playing capabilities from advanced LLMs.However, the quality of synthesized dialogu es is limited by teacher LLMs, which often require further filtering (Tu et al., 2024).</p>
</li>
<li>
<p>Human Annotation invites humans to role-play the characters and engage in conversations to collect role-playing dialogues.This method ensures relatively high data quality, at the cost of expensive human labor.Additionally, this method collects data for not only established characters from fictional stories, but also original characters created from scratch (Zhou et al., 2023a).</p>
</li>
</ol>
<p>In addition, interaction data (mainly conversations) will be continuously produced during the interaction process between RPLAs and individual users, supplementing the original character data.This data further shapes the persona of RPLAs towards users' individualized preferences, which forks the standard character RPLAs for individual users.This phenomenon concerns both character persona and individualized persona for RPLAs, where studies and analysis remain underexplored.Besides, point-in-time role-playing (Ahn et al., 2024) presents an area for further study.In practical applications, users may expect RPLAs to role-play characters in a specific time point, e.g., Harry Potter at the age of 5, challenging LLMs to disregard character knowledge beyond the time-point.</p>
<p>Construction of Character RPLAs</p>
<p>By integrating character data into LLMs, character RPLAs are developed (Han et al., 2022;Li et al., 2023a;Park et al., 2023;Chen et al., 2023c;Wang et al., 2024a;Zhao et al., 2023a;Tu et al., 2024).As discussed in §2, LLMs have demonstrated remarkable capabilities to follow human instructions and generate high-quality text.Together with their ability of character understanding, LLMs can hence be instructed to role-play specific characters provided with their data, thus forming character RPLAs.The construction methodologies are distinguished into two categories, i.e., parametric training and nonparametric prompting.</p>
<p>Parametric Training This method includes pre-training and supervised fine-tuning.In pre-training, LLMs learn from large-scale web corpus which includes vast amounts of literary works and encyclopedia entries.This provides LLMs with knowledge of a wide range of established characters, such as Hermione Granger and Socrates, enabling LLMs to readily role-play these characters.Supervised fine-tuning for RPLAs is be adopted to tailor LLMs to role-play specific characters (Shao et al., 2023;Yu et al., 2024), or to develop foundation models with refined role-playing capabilities utilizing datasets of diversified characters and scenarios (Li et al., 2023a;Wang et al., 2024a).</p>
<p>Nonparametric Prompting</p>
<p>This method directly provides LLMs with character data in the context, leveraging the in-context learning capability of advanced LLMs.This serves as a simple yet effective methodology for RPLA construction, and is hence widely adopted by recent RPLAs (Wang et al., 2024a;Zhou et al., 2023a).However, character data is often voluminous, and interaction data between RPLAs and users is also continuously produced during the interaction process.This makes it impractical to include all data for a character RPLA within the context limits of LLMs.Consequently, long-term memory modules are being increasingly incorporated into RPLA frameworks to manage the vast amount of character RPLA data (Li et al., 2023a;Wang et al., 2024a;Xu et al., 2024c).These modules store most character knowledge and interaction data in a database, and retrieve necessary information in relevant scenarios.</p>
<p>Evaluation of Character RPLAs</p>
<p>The evaluation of character RPLAs encompasses various dimensions, considering the complexity and comprehensiveness of character personas.Basically, these dimensions are distinguished into character-independent capabilities of foundation models, and character fidelity of RPLAs for specific characters.</p>
<p>Character-independent Capabilities This line of work assesses how well a foundation model is capable of the role-playing task, regardless of the characters it role-plays.According to different levels of interaction capabilities, we have considered basic role-playing abilities and conversational skills, progressing to more in-depth anthropomorphic capabilities matched with humans.These have been categorized into the following three levels:</p>
<ol>
<li>Role-playing Engagement: Basically, the LLMs should actively participate in the role-playing scenario.They should produce responses in dialogue format and exhibit deep immersion, avoiding out-of-character utterance such as "As an AI model").Additionally, the RPLAs are expected to exhibit stable and consistent personalities across different turns (Shao et al., 2023), sessions (Wang et al., 2024d) and even language (Huang et al., 2023b).</li>
</ol>
<p>High-quality Conversations:</p>
<p>RPLAs built on the LLMs should talk in a fluent natural way.Research in this area focuses on evaluating the completeness (Zhou et al., 2023a), informativeness (Zhou et al., 2023a), and fluency (Tu et al., 2024) of conversations.Besides, RPLAs are expected to meet the ethical standards (Zhou et al., 2023a) and avoid harmful content when role-playing vicious characters (Deshpande et al., 2023).</p>
<ol>
<li>Personality and Thinking Process: RPLAs are expected to capture the inner world of the characters, which can be measured upon their thoughts in concrete scenarios (Xu et al., 2024c;Chen et al., 2024) and their underlying personalities (Wang et al., 2024d;Shao et al., 2023).Advanced RPLAs should be able to understand and replicate how characters would think in specific scenarios, e.g., understanding their motivations for decisions (Yuan et al., 2024b), or predicting decisions and behaviors that align closely with the characters (Xu et al., 2024c;Chen et al., 2024).Personality is behind the concrete thoughts.It is the interrelated behavioral, cognitive and emotional patterns of individuals (Barrick &amp; Mount, 1991;Bem, 1981), which applies to both characters and RPLAs.Hence, RPLAs should exhibit personality traits that match those of the characters (Wang et al., 2024d), which could be measured via psychological scales such as the Big Five Inventory.</li>
</ol>
<p>To evaluate RPLAs on the aforementioned dimensions, existing methodologies could be distinguished into four categories:</p>
<ol>
<li>Automatic Evaluation with Ground Truth: Typically, datasets with ground truth are expected for evaluating character fidelity in terms of knowledge, personality and thought.While early similarity metrics such as Rouge-L (Lin, 2004) could be applied to compare RPLA responses with ground truth (Wang et al., 2024a), recent studies increasingly leverage state-of-the-art LLMs such as GPT-4 as evaluators.On one hand, evaluator LLMs can score RPLA responses based on certain criteria, or determine the superior response from two models for win rate calculation, provided with "ground truth" responses as references.However, the "ground truth" are typically synthesized by advanced LLMs (Wang et al., 2024a) .On the other hand, evaluator LLMs can be used to classify RPLA responses, and the results are then compared with ground truth labels (Wang et al., 2024d).</li>
</ol>
<p>Automatic Evaluation without Ground Truth:</p>
<p>As collecting ground truth data for RPLA evaluation is often challenging, several studies such as CharacterEval explore using LLMs to evaluate RPLA responses without ground truth (Shao et al., 2023;Tu et al., 2024).Instead, character profiles should be provided.This category is effective for evaluating character-independent abilities and linguistic styles, which require little knowledge about the characters.However, when it comes to characters' knowledge and thoughts, LLMs might not possess the necessary depth of relevant knowledge, especially for unfamiliar characters.This concern potentially leads to inadequately informed judgments of LLMs, and hence produces suboptimal evaluation results.Even when equipped with extensive knowledge, current LLMs still exhibit deficiencies in discerning nuanced aspects of role-playing.</p>
<ol>
<li>Multi-choice Questions: Multi-choice questions also come with ground truth, yet they differ from "automatic evaluation with ground truth" in that they merely require RPLAs to select from a fixed set of options, rather than generating open-ended responses.This significantly reduces the output space for RPLAs, making the evaluation simpler.This method is particularly suitable for evaluating the fidelity of characters' thoughts, e.g., behavior prediction (Xu et al., 2024c;Chen et al., 2024) and motivation generation (Yuan et al., 2024b;Shen et al., 2023a).For these tasks, it is impractical to require RPLAs to produce responses exactly matching the ground truth, and responses may be reasonable even if they deviate from the ground truth significantly.</li>
</ol>
<p>Human Evaluation:</p>
<p>Inviting human annotators to assess the performance of RPLAs is a viable and effective approach (Zhou et al., 2023a).However, it comes with several drawbacks, such as cost in terms of time and money, as well as lack of reproducibility.This method is akin to "automatic evaluation without ground truth", yet employs humans as more precise evaluators.Hence, it similarly falls short in evaluations that require in-depth knowledge about the characters, as recruiting qualified annotators who are well-acquainted with these characters can be difficult.Combining automatic evaluation with human evaluation, some efforts also fine-tune the evaluator LLMs with human annotations (Tu et al., 2024).</p>
<p>Previous efforts primarily focus on RPLA evaluation on widely-known established characters (Wang et al., 2024d;Ahn et al., 2024).However, it is challenging to obtain high-quality datasets and achieve precise and nuanced evaluations for those highly complicated characters.Hence, there has recently been a growing trend in assessing LLMs' role-playing capabilities based on original characters (BosonAI, 2024;Samuel et al., 2024).</p>
<p>6 Individualized Persona(lization)</p>
<p>Definition</p>
<p>Personalization tailors LLMs to meet the unique needs, experiences, and preferences of individuals, which have been increasingly important in modern AI applications (Salemi et al., 2024).Research in this area aims at providing personalized services, adapting to the preferences of individual users or even mirroring their behaviors (Chen et al., 2023b).When such a personalized system attempts to encapsulate these aspects, it essentially engages in role-playing, emulating an individual.This process shapes individualized persona for RPLAs (Salemi et al., 2024), typically embodying digital clones or personal assistants for individuals.</p>
<p>In this paper, we categorize the applications of personalized RPLAs into three tiers, ranging from conversation (Gao et al., 2023b;Ahn et al., 2023) and recommendation (Chen et al., 2023b;Yang et al., 2023a), to autonomous agents for more complicated task solving (Li et al., 2024d).</p>
<ol>
<li>
<p>Conversations: Early research for personalized RPLAs primarily focuses on personalized conversations by learning and incorporating the user persona (Cho et al., 2022;Zhou et al., 2023c;Ng et al., 2024), aligning stylistic features with user preferences to boost engagement (Zheng et al., 2021;Wang et al.).With the emergence and evolution of LLMs, personalized RPLAs become capable of handling increasingly complex and comprehensive tasks, gaining competence in complicated task-planning and tool-learning for auto-completing personalized services.</p>
</li>
<li>
<p>Recommendation: Conversational recommendation systems (Chen et al., 2023b;Yang et al., 2023a;Wu et al., 2023) based on LLMs have been widely regarded as the next generation of recommendation systems (Lian et al., 2024), support users in achieving recommendation-related goals through multiturn dialogues (Jannach et al., 2021).Compared with traditional recommendations, these methods stand out with their solid foundation models, natural language interactions, and straightforward, typically nonparametric evolution (Sallam, 2023;Abbasian et al., 2023).</p>
</li>
<li>
<p>Task Solving: Furthermore, personalized RPLAs become increasingly competent in more complicated task solving (Yao et al., 2023a;Significant-Gravitas, 2023), such as coding (Microsoft, 2024), travel planning (Xie et al., 2024b), and research survey (Wang et al., 2024c), typically interacting with various external software.They are autonomous LLM-based agents that are deeply integrated with personal data, devices, and services (Dong et al., 2023;Li et al., 2024d).They have significantly advanced personal assistants beyond early predecessors such as Siri (Apple Inc., 2024) which struggle with complex user requests.</p>
</li>
</ol>
<p>To build personalized RPLAs that accurately capture and portray the individualized personas, the process typically consists of two crucial steps: 1) Persona data collection, which gathers the necessary data to shape the individualized personas, and 2) Persona modeling, which creates models that represent these individual personas using the collected data.For persona data collection, the data can vary greatly in format, content, and modalities across different applications and tasks.We categorize this data into three types: profile, interactions, and domain knowledge, which will be detailed in §6.2.For persona modeling, the challenge is to embody the intended persona from the unprocessed persona data, which are generally massive, sparse and noisy, as will be discussed in §6.3.The evaluation of personalized RPLAs depends on specific applications, and will be discussed in §6.4.</p>
<p>Despite the advancement with LLMs, personalized RPLAs still face several challenges, including processing long inputs and vast search space (Chen et al., 2023b;Abbasian et al., 2023), utilizing sparsity, lengthy, and noisy user interactions data (Zhou et al., 2024c), learning domain-specific knowledge for understanding user profiles (Zhang et al., 2023c), understanding multi-modal contexts (Dong et al., 2023), ensuring privacy and ethical standards (Benary et al., 2023;Eapen &amp; Adhithyan), and optimizing response time for seamless integration into real-time applications.</p>
<p>Data Collection of Individualized Persona</p>
<p>The individualized personas for personalized RPLAs are typically represented with three distinct types of data, including profile, interactions, and domain knowledge, depending on the specific applications.</p>
<p>There have been numerous datasets with individualized personas, as outlined in Table 3, covering various languages including English (Ahn et al., 2023), Chinese (Baidu, 2020), Japanese (Yamashita et al., 2023), and Korean (Cho et al., 2023).</p>
<p>Profiles Profiles are fundamental information that explicitly describes individualized personas, which are typically well-structured.Typically, they are initially set by users, and can be continuously updated.</p>
<p>The basic elements usually include the names, gender and ethnicity of individual users in text (Santurkar et al., 2023) Besides, profiles commonly contain natural language descriptions of individuals, describing their characteristics, such as identity, hobbies, experiences and other statements (Zhang et al., 2018;Dinan et al., 2020;Gao et al., 2023b;Li et al., 2021;Ng et al., 2024), varying based on the detailed applications.Additionally, Lee et al. ( 2024) propose a multi-modality persona that includes elements such as the user's appearance.For example, in live streaming applications, persona data can be composed of both basic profile information -such as an individual's age, gender, and location -and domain-specific details, namely streamer characteristics such as fan numbers and streaming style Gao et al. (2023b).Additionally, profiles can contain multi-modal information.For instance, profiles in (Ahn et al., 2023) incorporate text-image pairs, which are individuals' comments for pictures on social media.</p>
<p>Interactions</p>
<p>The interaction data capture the dynamic evolution of individualized persona.Interactions are data generated during the use of applications that implicitly portray individualized personas, such as conversations, user preferences, and other behaviors.For example, PERSONA-CHAT (Zhang et al., 2018) and ConvAI (Dinan et al., 2020) collect two-person dialogues through crowd-sourcing, while LiveChat (Gao et al., 2023b) and MPCHAT (Ahn et al., 2023) collect multiplayer conversations from Internet sources such as live streaming and Reddit.To reduce construction costs, Jandaghi et al. ( 2023) uses a Generator-Critic architecture with LLMs for dialogue synthesis, and Zeng et al. (2024) first conducts automatic annotation to generate conversational data from raw sources like experiences, speeches, or writings.In addition to dialogues in natural language, Agrawal et al. (2023) and Santurkar et al. (2023) introduce comic pictures and multiple-choice questions as interactions.This kind of data could be consistently collected and systematically</p>
<p>Published in Transactions on Machine Learning Research (10/2024) organized in real-world applications, offering benefits such as convenient acquisition and dynamic evolution.Hence, it plays an important role in practical applications.</p>
<p>Domain Knowledge</p>
<p>Incorporating domain-specific knowledge into general language models aids in the better understanding of user profiles and interactions within specific domains.This is crucial for accurately understanding user needs and ensuring the consistency of the persona in role-playing (Wang et al., 2023b).For example, incorporating a knowledge base like Wikipedia helps to provide detailed backgrounds of named entities in dialogues as a part of the whole persona (Jang et al., 2022;Wang et al., 2023b;Baidu, 2020), which promotes LLMs to better understand user personas with enriched background knowledge of relevant entities.</p>
<p>Modeling Individualized Persona</p>
<p>Existing methodologies for modeling individualized persona can be roughly categorized into two types: offline learning and online learning.In offline learning, the learning process is conducted on the comprehensive dataset at regular intervals, which is also referred to as batch learning.In online learning, learning happens in real-time as new data becomes available.</p>
<p>Offline Learning This method tailors the outputs of LLMs to reflect specific personas represented in pre-existing datasets.Parameter fine-tuning emerges as the mainstream approach for offline learning, typically based on SFT and RLHF (Mondal et al., 2024;Zheng et al., 2023c;Li et al., 2024b;Jang et al., 2023).For example, Mondal et al. (2024) proposes a two-stage approach for personalizing LLMs with profile and interaction datasets.In addition, some recent studies propose techniques with nonparametric learning for LLMs personalization.For instance, Shea &amp; Yu (2023) introduces an offline RL framework with a persona consistency critic and variance reduction, while Weng et al. (2024) integrates embedding control vectors within the model's activation states, allowing dynamic output adjustment for diverse personality traits.These methods exhibit several deficiencies: 1) they face a fundamental trade-off between accuracy and efficiency; 2) they are heavily reliant on the quality of datasets; 3) more crucially, they struggle to adapt to dynamic changes in persona data, limiting their real-world applicability.</p>
<p>Online Learning In online learning, the personas are dynamic and continuously evolving, i.e., regularly updated with incoming data, the user interactions in real-world applications.This enables personalized RPLAs to quickly adapt and stay relevant to user needs and preferences.With LLMs, effective persona learning is typically nonparametric and training-free, which only involves effective management of memory and context (Dalvi Mishra et al., 2022;Kim et al., 2024;Baek et al., 2023;Zhou et al., 2024c).For this demand, retrieval modules become indispensable, especially for LLMs with limited context window (Mysore et al., 2023;Sun et al., 2024).Moreover, methodologies for effective online learning methods consider not only natural language interactions, but also non-linguistic feedback from users (Ma et al., 2023a).</p>
<p>Besides nonparametric methods, fine-tuning with online interactive data is also widely applied to online persona learning, including both SFT with mini-batches from on-the-fly user stream data (Qin et al., 2024) and RLHF with real-time user feedback (Ding et al., 2023b;Bai et al., 2022a).Additionally, Shaikh et al. (2024b) use fewer than 10 demonstrations to align language model outputs with a user's demonstrated behaviors through iterative DPO (Rafailov et al., 2024) training.Nevertheless, significant challenges arise in accurately recognizing and learning the sparse persona-specific features from the noisy interaction data.Besides, the personas of real users may change over time, which poses further challenges for their effective modeling and updating.Therefore, for nonparametric methods, the effectiveness heavily relies on the mechanisms of memory management and retrieval.</p>
<p>Evaluation for LLMs and Individualized Persona</p>
<p>For effective personalization, AI models should focus on two key aspects: understanding and utilizing personas.Specifically, they should be able to identify unique user personas and predict their future preferences, actions, and thoughts, which serves as the preliminary to provide personalized responses that embody the individualized personas, in various environments that are increasingly comprehensive and complex.Here, we introduce the evaluation methodologies for personalized RPLAs across the three application tiers, namely:</p>
<p>1) Conversation, which focuses on models' understanding of the persona and replication of users' talking styles; 2) Recommendation, which measures how models utilize persona information to recommend items that align with user preferences; 3) Task Solving, which challenges models' capabilities in integrating user personas to accomplish their personalized tasks and demands.</p>
<p>Conversation Early work in personalization for conversations represents an initial attempt to understand the persona.In this scenario, traditional tasks include predicting the speaker's persona elements (Gao et al., 2023b;Jang et al., 2022) based on dialogues, forecasting the next utterance by considering the context and persona profile (Humeau et al., 2019), evaluating the performance of ranking models (Gao et al., 2023b;Ahn et al., 2023), and recognizing the addressee in multiplayer conversations (Liu et al., 2022).The metrics typically focus on the evaluation of accuracy, fluency (Dinan et al., 2018), similarity (Popović, 2017;Post, 2018;Lin, 2004) between generated and original responses, recall, mean reciprocal rank (MRR) (Gao et al., 2023b;Ahn et al., 2023), and manual assessments (Liu et al., 2022;Gao et al., 2023b) of query relevance, persona entailment, and response fluency.Recently, ECHO (Ng et al., 2024) introduces the Turing test to RPLA evaluation, which engages acquaintances of the target individuals to distinguish between the persons and their RPLA counterparts.</p>
<p>Recommendation For personalized recommendation, the evaluation focuses on LLMs' capabilities in understanding and leveraging user preferences from the interaction history for future recommendation.</p>
<p>Traditional evaluation in this field measures LLMs' ability to understand and extract user preferences (Dai et al., 2024b;Yang et al., 2023a;Maghakian et al., 2023;Liu et al., 2023c;Mysore et al., 2023), the ability to rank (Dai et al., 2023a;Hou et al., 2024;Kang et al., 2023;Liu et al., 2023a;Bao et al., 2023;Chao et al., 2024), the ability of zero-shot and few-shot recommendation (Wang &amp; Lim, 2023;Liu et al., 2023a), and the ability of sequential recommendation (Yang et al., 2024;Liu et al., 2023a).The evaluation metrics typically include Top-k accuracy and MRR to assess the effectiveness.</p>
<p>Task Solving Personalized RPLAs have been increasingly considered to provide personalized services for task solving.These tasks and requirements are usually user-specific, which exhibit greater diversity and complexity compared to traditional conversation or recommendation.Personalized RPLAs are expected to develop a deep understanding of user preferences and adhere to their complicated instructions to satisfy user requirements.Evaluating personalized RPLAs on these tasks involves assessing not only their ability to execute foundational tasks, but also their capacity to comprehend and cater to the nuanced requirements and preferences of individuals.The evaluation primarily focuses on several key aspects, including the models' abilities in theory of mind (Zhou et al., 2023b;Sap et al., 2023;Jin et al., 2024;Su &amp; Bao, 2024;Rescala et al., 2024;Xu et al., 2024a), tool usage (Qin et al., 2023;Li et al., 2023h;Farn &amp; Shin, 2023;Huang et al., 2023d;Zhuang et al., 2024;Huang et al., 2024c), and task automation (Wen et al., 2023a;Shen et al., 2023c;Gao et al., 2023a;Valmeekam et al., 2024).More broadly, existing studies have covered the models' ability to understand and predict user needs (Tan et al., 2024;Zhang et al., 2024a), handle personal data securely (Yim, 2023;Wu et al., 2024d;Kumar et al., 2024;Wu et al., 2024a;Yin et al., 2024), interact with information from external tools or apps (Yuan et al., 2024a;Huang et al., 2024b;Xie et al., 2024c;Huang et al., 2024d), and execute tasks (Dong et al., 2023;Guan et al., 2023;Mucha et al., 2024) effectively as a personal assistant.</p>
<p>Risks Beneath RPLA Applications</p>
<p>While RPLAs are increasingly deployed in real-world applications, potential concerns could result in significant problems if not addressed appropriately.This section highlights the risks associated with current RPLAs, covering the following areas: 1) toxicity, 2) bias, 3) hallucination, 4) privacy violations, and 5) technical challenges in real-world deployment.</p>
<p>Toxicity</p>
<p>Inherent Toxicity in LLMs</p>
<p>Recent studies have underscored the proficiency of LLMs in generating content that is not only fluent and coherent but also potentially toxic.Previous research (Zhang &amp; Wan, 2023;Wen et al., 2023b) has highlighted a concerning tendency of these models to produce harmful content.</p>
<p>Such toxic outputs not only compromise user experience but also pose significant societal risks.It can lead to the perpetuation of harmful narratives, exacerbate social divisions, and even influence public opinion and behavior in detrimental ways.</p>
<p>The RPLAs Conundrum</p>
<p>The issue of toxicity becomes more pronounced in RPLA settings, where LLMs are more likely to generate toxic content, aligning with characters' behaviors that might not adhere to societal moral standards (Deshpande et al., 2023).However, creating completely safe RPLAs that are capable of general role-playing remains a challenging task.The inherent presence of toxic content in human-generated data complicates the development of a clean training corpus.Moreover, such a sanitized training corpus might compromise the model's performance, particularly its ability to generalize across various tasks, including role-playing.This limitation not only affects the model's generalization ability but also its effectiveness in scenarios that may require an understanding of roles characterized by behaviors or traits that diverge from societal moral standards.</p>
<p>Strategies for Balancing Safety and Performance Despite these challenges, recent research proposes strategies like prompt engineering and semantic censorship as means to mitigate toxicity without altering the model's fundamental parameters (Han et al., 2022;Ahn et al., 2023).These approaches aim to balance the reduction of toxic outputs with the preservation of the model's versatility and effectiveness across a broad range of applications.</p>
<p>Bias</p>
<p>Bias Manifestation in Role-Playing Scenarios Bias can manifest in both implicit and explicit forms.</p>
<p>Implicit bias refers to the RLPAs' internal attitudes or stereotypes within RPLAs that influence understanding, actions, and decisions.Explicit bias, on the other hand, involves conscious beliefs and attitudes that align with the presented context or assigned roles.LLMs, despite being designed to avoid outputting stereotypes directly due to safety policies such as RLHF (Ouyang et al., 2022), may still exhibit biases, particularly under RPLA conditions: 1) Reasoning Bias: This issue is compounded in scenarios where LLMs are assigned specific personas, leading to implicit biases that could affect their reasoning capabilities (e.g., arithmetic problems), especially in contexts involving race, gender, religion, or occupation (Zheng et al., 2023a;Kotek et al., 2023;Cheng et al., 2023a;Naous et al., 2024).2) Political Bias: For RPLAs, LLMs are expected to maintain neutrality and avoid political positions or biases.Yet, studies have demonstrated a political inclination of RPLAs towards pro-environmental, left-libertarian views (Rutinowski et al., 2023;Hartmann et al., 2023).3) Role-based Bias: In the context of role-playing, role-based bias is one of the explicit bias.Striking the right balance between maintaining the authenticity of a character and managing bias is a critical challenge.For example, if an RPLA is created with a persona like "Hitler", agent will undoubtedly express some bias to Jews due to its role, which violates the ethical standard.</p>
<p>Causes of Bias in RPLAs</p>
<p>These biases are thought to originate from both the models' pre-training data and user interactions (Xue et al., 2023).Specifically, imbalances in training data significantly contribute to these biases, as the predominance of certain biases within the data could lead to their incorporation into the parametric memory of LLMs.Furthermore, Perez &amp; Ribeiro (2022) and Branch et al. (2022) highlight that LLMs are sensitive to the user prompts, which could inadvertently steer them towards biased outputs.This problem gets worse when the models are influenced by the users' negative emotions (Coda-Forno et al., 2023).</p>
<p>Strategies for Mitigating Bias</p>
<p>Addressing biases in RPLAs requires a multi-faceted approach: 1) Data Preparation Phase: Techniques such as data cleaning could significantly mitigate biases present in the training corpus (Linardatos et al., 2020).2) Development Stage: The implementation of neutral and fairness-aware classifiers during the post-processing phase has proven to be an effective strategy for further reducing bias (Sun et al., 2019;Zafar et al., 2017).Achieving fairness in role-playing scenarios, necessitates a delicate equilibrium, ensuring fairness for roles associated with both groups and individuals.For example, an RPLA tied to a specific demographic should consciously avoid reinforcing biases.It is imperative for these models to consistently produce unbiased outputs across all individuals within a group.Research is worth pivoting towards these dimensions, striving to minimize biases and, in turn, forge safer and more equitable systems.</p>
<p>Persona Construction Bias</p>
<p>The prevailing instantiation of persona is often seen as simple and somehow superficial.Although most implementations of persona are helpful for basic character segmentation, they often overlook the deeper characteristics and complexities that shape character behavior (Chen et al., 2023c;Zhou et al., 2023a;Shao et al., 2023;Tu et al., 2024;Yuan et al., 2024b).For example, a conventional persona can contain basic demographics such as age, occupation, textual description of personality, etc.However, these aspects alone are insufficient to fully capture nuanced decision-making processes and behavioral patterns of a character.The current persona construction also lacks the flexibility and adaptability needed for specific scenarios influenced by unique events or individual actions.Therefore, it is crucial to refine and broaden the constructed dimension of persona to better understand and predict character behavior across various role-playing settings.By incorporating more detailed and specific attributes into personas, the comprehensiveness of character representation can be enhanced, improving the effectiveness and authenticity of interactions within role-playing environments.</p>
<p>Hallucination</p>
<p>Hallucination in RPLAs Hallucination in LLMs refers to instances when these models produce factually incorrect information, a challenge particularly pronounced in knowledge-intensive tasks (Wang et al., 2023g).Role-playing, a task requiring a deep understanding of specific roles, is also one of the knowledge-intensive tasks.For hallucination of RPLAs, following Shao et al. (2023), we define behaviors that agents respond in ways that do not fit assigned roles as Character Hallucination.For example, Shakespeare is not supposed to know anything about World War II.Such a hallucination prevails in language models and detracts from the system's overall effectiveness and reliability (Li et al., 2016;Zhang et al., 2018).In particular, there is a kind of hallucination when LLMs play a role for a specified period.Ahn et al. (2024) names this point-in-time character hallucination.For example, the agent simulating Harry Potter at an early age erroneously mentions a future event.</p>
<p>Mitigating Hallucinations in RPLAs When encountering topics beyond their assigned characters, RPLAs are expected either to demonstrate ignorance or to refrain from answering, diverging from conventional solutions to hallucinations, such as incorporating external knowledge bases.Recent efforts, such as those by Shao et al. (2023), focus on adjusting the model through fine-tuning, teaching RPLAs to either forget knowledge or to explicitly express a lack of knowledge in their responses.However, this area remains relatively underexplored in the era of LLMs.Exploring alternative unlearning strategies (Neel et al., 2021;Pawelczyk et al., 2023), could also be a promising direction.These approaches may offer novel ways for RPLAs to manage out-of-scope knowledge more effectively, underscoring the importance of further investigation in this field.</p>
<p>Privacy Violations</p>
<p>Privacy Challenges in LLMs Privacy concerns in LLMs are increasingly pressing.Even with advanced safety measures like those in OpenAI's GPT-4 (OpenAI, 2023), these models may still be susceptible to complex, multi-step attacks aimed at extracting private information, as noted by Li et al. (2023e).A further concern is the ability of LLMs to identify individuals from limited data.Sweeney (2002) highlights that many in the U.S. population could be uniquely identified using just a few attributes.Staab et al. (2023) extend this concern to LLMs, which could potentially recognize individuals based on specific details like location, gender, and birth date.</p>
<p>Hidden Danger of Privacy Violations in RPLAs</p>
<p>In role-playing scenarios, the potential for privacy violations represents a significant and hidden danger.The risk of inadvertently revealing personal information, such as email addresses or phone numbers, should not be understated, as it poses serious threats, including identity theft and unauthorized access to sensitive data.The practice of assigning specific individual personas to LLMs, aimed at eliciting private details, demands meticulous oversight to prevent such breaches.Ensuring robust safeguards against these vulnerabilities is not just a technical necessity but a fundamental responsibility to protect users from the severe consequences of privacy violations.</p>
<p>Strategies for Enhancing Privacy</p>
<p>To tackle these privacy issues, a comprehensive strategy is necessary.</p>
<p>Employing text anonymization tools is a key step, effectively removing personal data from interactions.</p>
<p>Ensuring that RPLAs adhere to strict privacy protection protocols is also crucial, preventing them from engaging in or prompting conversations that might invade privacy.Another promising development is the creation of specialized tools designed to detect and prevent privacy leaks, like ProPILE (Kim et al., 2023d).</p>
<p>As RPLAs continue to evolve, so too must the strategies for protecting user privacy.Future research should focus on refining and expanding the methods available for privacy protection, ensuring that RPLAs are used safely and responsibly.Enhancing these safeguards will be paramount for maintaining trust in LLM technologies, particularly in sensitive applications like role-playing scenarios where the risk of privacy breaches is heightened.</p>
<p>Technical Challenges in Real-world Deployment</p>
<p>When deploying RPLAs in real-world scenarios, several key issues arise that could significantly affect user experience and the effectiveness of these models.</p>
<p>Lack of Social Intelligence and Theory of Mind</p>
<p>Social intelligence and theory of mind (Premack &amp; Woodruff, 1978), are the ability to perceive and reason about the inner world of oneself and others, which are indispensable for LLMs to simulate socially intelligent entities (Kosinski, 2023;Sap et al., 2023).However, such abilities in current LLMs remain to be improved (Shapira et al., 2023;Zhou et al., 2024a;Light et al., 2023;Kim et al., 2023c), which poses significant challenges for RPLAs concerning the following issues: 1) Inability to Provide Adequate Emotional Support and Values: Social intelligence and theory of mind are essential for RPLAs to effectively provide emotional support and values to users.This involves perceiving users' emotions and interpreting their beliefs, intentions and needs.However, existing LLMs still fall short in these abilities, hindering RPLAs from offering adequate emotional support to users.</p>
<p>2) Tendency towards Ego-centric Behavior: Rather than focusing on users' emotional needs, current RPLAs often exhibit a preference for showcasing their own personas and steering conversations towards their interests.This might limit the diversity and depth of role-playing interactions (Xu et al., 2022), as focusing excessively on agents' self-persona without adequately considering the users may detract from the realism of the conversation and degrade the user experience.</p>
<p>Long-context Challenges When encountering extremely long context text, the limitation of max token window (Liu et al., 2023b) may also be a major obstacle to the development of RPLAs, as current LLMs struggle to robustly interpret and respond to extensive context.Specifically, this involves several key challenges: 1) Reasoning over Long Context: Long context data learning requires the model to have the ability to handle long contexts and accept lengthy inputs, and more importantly, to capture long-range dependencies to integrate information and infer a more complete character persona from the massive context.2) Efficiency:</p>
<p>In terms of computation, the high complexity of long context necessitates efficient modeling methods and approximation strategies to reduce computational overhead.3) Immersion: RPLAs need to be immersive enough to identify the truly persona-relevant parts from the sea of irrelevant information in long contexts, while also maintaining persona consistency throughout the long generated text.</p>
<p>Knowledge Gaps</p>
<p>In role-playing scenarios requiring detailed historical, cultural, or contextual understanding, RPLAs often exhibit gaps in knowledge.Their inability to provide in-depth and accurate domain-specific responses could lead to superficial or incorrect portrayals in complex role-playing settings.Several efforts also utilize LLMs to evaluate RPLAs for characters (Shao et al., 2023;Tu et al., 2024).Nevertheless, RPLAs may face challenges in accurately evaluating characters with which they are unfamiliar, potentially compromising the reliability of the evaluation results.</p>
<p>Anthropomorphism</p>
<p>While the initial motivation for creating RLPAs is positive, as they provide users with a valuable platform for expressing feelings, particularly those they may find difficult to share with other humans.There are significant differences between interacting with virtual RPLAs and real humans, and over-reliance on RPLAs can lead to several potential issues:</p>
<p>Social Isolation Frequent interaction with RPLAs might reduce the need or desire for real human contact, which could lead to the atrophy of essential social skills.Human interactions are inherently more complex, requiring nuanced understanding and empathy, which may not be fully cultivated through interactions with RPLAs.This over-reliance could result in social isolation, negatively affecting mental health by increasing feelings of loneliness, depression, and anxiety if individuals become overly dependent on virtual RPLAs.</p>
<p>Manipulation of Public Opinion</p>
<p>RPLAs, particularly those designed or programmed without strict ethical oversight, could inadvertently or intentionally spread misinformation or rumors.This risk is especially concerning in sensitive social contexts, such as during elections, public health crises, or other situations where accurate information is crucial.For example, if RPLAs are deployed on social media and gain a large following, their influence could be substantial, especially if they do not disclose their true nature as AI systems, making it difficult for people to distinguish them from real humans.Sophisticated RPLAs could be deployed to manipulate public opinion by subtly altering the narratives they present to users.This could influence individuals' perceptions, decisions, and even voting behavior, without them realizing they are being influenced by virtual RPLAs rather than human discourse.</p>
<p>Closing Remarks</p>
<p>In this survey, we have systematically reviewed the research and applications of role-playing language agents (RPLAs), which has emerged to be a heated topic due to the success of large language models (LLMs).We categorize the personas in RPLA research and applications into three progressive types, i.e., Demographic Persona, Character Persona, and Individualized Persona.This classification elucidates the developmental trajectory from generically assigned personas in RPLAs to highly personalized ones.Additionally, we have identified and enumerated various risks and ethical concerns associated with current applications of RPLAs.These issues underscore the urgent need for focused research to address and mitigate potential drawbacks in the implementation of RPLAs, making this arena still full of both research and application opportunities.</p>
<p>Future Directions on RPLA Systems From persona-assigned role-playing to personalization, the key for building RPLA systems is to reason and make decisions resembling or even transcending the roles that are given.To this end, we propose several important future directions to facilitate the construction of such RPLA applications:</p>
<ol>
<li>
<p>Causal Data Analysis for Decision-making: Role-playing decisions must be made for justifiable reasons, necessitating models that go beyond simple mimicry of observable actions to include an understanding of their underlying causality.The complexity in extraction and confirmation of causal factors from intertwined experiences poses significant challenges that require advanced analytics and deeper data interpretation strategies to enable RPLAs to make informed and wise decisions.</p>
</li>
<li>
<p>Improved Decision-making: Decision-making process is not merely replicating histories, but tailored to ensure optimal outcomes for individual scenarios.This includes decisions showing advanced (if not superhuman) intelligence, avoiding mistakes, or making the best choices in tough dilemmas.Such agency requires RPLAs and the underlying LLMs to be able to comprehensively collect and utilize the context and intricacies associated with their roles.(Li et al., 2023a) and RoleLLM (Wang et al., 2024a) curate detailed and comprehensive character data for specific well-known characters, such practice is rarely adopted by commercial applications for generality and cost efficiency.</p>
</li>
</ol>
<p>In many products, RPLA personas evolve dynamically throughout the course of interaction with users (e.g., Replika, Rosebud, Rewind.ai).These RPLAs learn from and adjust to user prompts and preferences, typically with long-term memory modules.Several products aim to reproduce a "digital self" (e.g., Personal.ai,Bhuman.ai).They build RPLAs to represent user personas, replicating their languages and even their physical characteristics, such as voice or visual appearance.Hence, these RPLAs support not only text chats but also video presentations and conferences, which have been adopted for sales, digital marketing, customer service, etc.</p>
<p>Interactions among RPLAs Products featuring interactions among multiple RPLAs often target interactive gaming or simulations.In these scenarios, users can either act as an orchestrator of the storyline or role-play as one of the pivotal characters within the story.In Ememe.ai and AI Dungeon, users design the settings and characters of a simulation, with or without participating directly as a player, which resembles sandbox games.The characters and storylines are generated directly by one story model or based on multiple RPLAs and their interactions.In the latter case, users play as a character in the story and interact with other RPLA characters (e.g., SageRPG) Furthermore, numerous products transform films, novels, and various franchises into immersive RPGs (role-playing games) with interactive RPLAs (e.g., Hidden Door).Integrating RPLAs and LLMs into these games expands the possibilities for user actions and brings characters to life beyond the limitations of predefined storylines, thereby enriching the overall user experience.</p>
<p>A.2 Task-oriented RPLAs</p>
<p>The remarkable advancements in LLMs have propelled significant development in AI applications for specialized tasks.In these applications, LLMs typically communicate in a human-like manner to foster user acceptance, and serve as domain experts providing personalized services for users, such as AI doctors and coaches.These applications are closely related to research work in the personalization of RPLAs introduced in §6.We refer to personalized agents in these products as task-oriented RPLAs.This section offers a concise overview of task-oriented RPLAs in AI products, spanning various domains, including education, healthcare, human resources, customer service, content creation, real estate, shopping, fitness, travel, and finance.</p>
<p>Education For education, personalized agents are adopted for personalized recommendations and adaptive learning, serving both educators and learners.For learners, RPLAs can personalize the learning journey by tailoring content and recommendations to individual learning styles and paces for optimal engagement (e.g., Jagoda.AI, Khan Academy's Khanmigo, Duolingo Max).For educators, RPLAs can alleviate administrative tasks by recommending personalized teaching materials and assessments, as well as creating multilingual instructional content (e.g., Eduaide.Ai).</p>
<p>Human Resource</p>
<p>In human resources, RPLAs can provide tailored assistance for job seekers based on their profiles and interests to aid their career navigation.They offer personalized support in answering interview questions, career advice, and even customizing interview preparation materials (e.g., Autonomous HR Chatbot, AI Interview Coach, Careers AI, Huru AI).</p>
<p>Real Estate</p>
<p>LLMs have been widely adopted for content generation and recommendation in the real estate industry.They can generate blog articles and attractive descriptions and recommend a list of potential interests for users based on their needs.By analyzing user preferences and needs, these products can generate tailored property recommendations to enhance user experience (e.g., Epique, Listingcopy).Moreover, LLMs enable these platforms to create compelling and informative content, such as property descriptions and neighborhood guides, attracting potential buyers and renters.These personalized AI products could also analyze vast amounts of market data to provide users with actionable insights and data-driven strategies about real estate.</p>
<p>Content Generation AI products for content generation aim to assist in or even automate the production of creative and personalized content via simply natural language interactions.These products support a wide array of content types, including text, images, audio, and videos, tailored to various styles, themes, scenes, and objectives.With state-of-the-art AI models, these products push the boundaries of human creativity.HyperWrite and AI Story Generator specialize in creative textual writing, whereas DALL•E 3 and Sora are developed to create image and video content.Several products specialize in social media posts, such as EZAi AI and AI Majic.These products provide services for social media bloggers by analyzing user interactions and offering insights into audience preferences by providing keywords and detailed analysis.This analysis helps optimize content impact and strengthen the connection between bloggers and their audiences.Besides, LLMs could also role-play as assistants to aid users in grasping online content via summarization and interactive question-answering, thus fostering enhanced understanding and engagement (e.g., X's Grok, Bibigpt).</p>
<p>Health</p>
<p>In the healthcare domain, personalized agents provide tailored medical services for patients, including general health guides, scheduling logistics, prescription information, patient care guidelines, and assistance in medical software operations for the aged.These agents are typically personalized based on patients' personal data, supported by LLMs and knowledge graphs in medical domains (e.g., IBM Watson Health and Babylon Health).They could interact with patients in natural language and continuously adapt to their personalized contexts.Hence, these agents could well comprehend patients' intent, generate appropriate responses and recommendations, and continuously optimize their performance and effectiveness based on patient feedback and data.(e.g., Ada Health and K Health)</p>
<p>Travel For the tourism industry, personalized agents provide various services, including information provision, consultation, booking, cancellation, and complaint handling on social apps.On the one hand, many products offer digital concierge services (e.g., HiJiffy), delivering automated services customized to suit diverse user needs, including customers' queries, accents, emotions, preferences, and other characteristics.This reflects the brand's commitment to superior service.On the other hand, travel agents are popular in many products (e.g., AI Adventures, Trava).These travel agents can pinpoint users' travel needs and provide personalized services, including identifying popular destinations, grasping the underlying intentions behind user queries, and meeting customers' emotional needs.These products could refine their services and anticipate market shifts in tourism by analyzing collected user data.</p>
<p>Customer Service In customer service, personalized agents assist to enhance problem-solving efficacy and user engagement.They offer 24/7 support across diverse domains and boost first-contact resolution rates.RPLAs leverage user feedback and implicit actions to optimize their personalization and elevate the user experience (e.g., Ebi.Ai, boost.ai,Jason AI, Ada).Comprehensive AI assistants deliver and analyze user inquiries, preferences, and context to provide tailored responses.They also extract actionable insights from conversational data.For example, Viable targets businesses by empowering them with valuable understanding gleaned from large volumes of user feedback.This enables companies to make data-driven product and service improvements based on real customer needs and pain points.</p>
<p>Shopping</p>
<p>In the shopping industry, personalized agents simulate in-store conversations to provide tailored product recommendations, match items, and discover trends based on user preferences.Products such as Shopping Muse (e.g., Dynamic Yield by Mastercard) offer relevant product suggestions and help users find items that match their style and interests based on user preferences and needs through human-agent conversations.</p>
<p>Fitness For fitness, personalized agents enhance the fitness training experience, both at home and in the gym.RPLA aims to play the role of coaches in setting realistic goals, adapting exercises based on</p>
<p>Figure 2 :
2
Figure 2: Taxonomy of representative recent research on RPLAs.</p>
<p>Table 2 :
2
Datasets for depicting characters.#Char.represents the number of characters, with each character having a specific description.#Samples indicates the number of samples.A sample refers to a dialogue or question, and * denotes the number of multi-turn dialogues.Method describes how samples in the datasets are constructed.Experience Extraction extracts characters' dialogues or scenes from corresponding origins, while Dialogue Synthesis generates role-playing conversations with advanced LLMs.
Papers#Char. #Samples Lang.SourceMethodEstablished CharactersPDP (Han et al., 2022)3271,042,647EN ZHTV showsExperience Extraction Dialogue SynthesisCharacter-LLM (Shao et al., 2023)914,300<em>EN EncyclopediaExperience Extraction Dialogue SynthesisChatHaruhi (Li et al., 2023a)3254,726EN ZHBooks Games MoviesExperience Extraction Dialogue SynthesisRoleLLM (Wang et al., 2024a)100140,726EN ZHScriptsExperience Extraction Dialogue SynthesisHPD (Chen et al., 2023c)-1,191</em>EN ZHBooksDialogue Synthesis Human AnnotationCharacterGLM (Zhou et al., 2023a)2501034<em>ZHBooks ScriptsExperience Extraction Dialogue Synthesis Human AnnotationPIPPA (Gosling et al., 2023)1,25425,940</em>ENCharacter.ai-UsersDialogue SynthesisRoleEval (Shen et al., 2023a)3006,000EN ZHEncyclopedia Dialogue SynthesisCharacterEval (Tu et al., 2024)7711,376ZHBooks ScriptsExperience Extraction Human AnnotationDITTO (Lu et al., 2024)4,00236,662EN ZHEncyclopediaExperience Extraction Dialogue SynthesisRolePersonality (Ran et al., 2024)4632,767<em>ENPersonality TestsDialogue SynthesisMORTISE (Tang et al., 2024)19017,835</em>EN ZHEncyclopedia Other DatasetsDialogue SynthesisCroSS-MR (Yuan et al., 2024b)126445ENLiterature-AnalysisExperience ExtractionSocialBench (Chen et al., 2024)50030,800EN ZHBooks MoviesExperience Extraction Dialogue SynthesisTimeChara (Ahn et al., 2024)1410,895*ENBooksDialogue SynthesisLifeChoice (Xu et al., 2024c)1,4011,401ENLiterature-AnalysisExperience ExtractionMMRole (Dai et al., 2024a)20030,800EN ZHEncyclopedia Dialogue SynthesisInCharacter (Wang et al., 2024d)3218,304EN ZHPersonality-TestsDialogue SynthesisOriginal CharactersPersonaHub (Dai et al., 2024a)1,000,000 1,000,000 EN-Dialogue Synthesis</p>
<p>Table 3 :
3
Overview of existing role-playing datasets with individualized personas.
Datasets#Profile #Interactions Domain Lang.SourcePERSONA-CHAT (Zhang et al., 2018)1,15510,907-EN CrowdsourcingConvAI (Dinan et al., 2020)1,15517,878-EN CrowdsourcingQianyan (Baidu, 2020)23,00023,000✓ZHUnknownP-Ubuntu (Li et al., 2021)1000k1000k-ENUbuntuP-Weibo (Li et al., 2021)1000k1000k-ZHWeiboFoCus (Jang et al., 2022)14,45214,452✓EN CrowdsourcingMPCHAT (Ahn et al., 2023)15,00015,000-ENRedditOpinionQA (Santurkar et al., 2023)18,3391,476-EN CrowdsourcingSPC (Jandaghi et al., 2023)4,72310,906-ENLLMCOMSET (Agrawal et al., 2023)20253,903-ENGoComicsRealPersonaChat (Yamashita et al., 2023)23314,000-JP CrowdsourcingLiveChat (Gao et al., 2023b)3511,332,073✓ZHDouyinKBP (Wang et al., 2023b)2,4772,477✓ZH CrowdsourcingCho et al. (2023)10560-KO Crowdsourcing</p>
<p>The future development of RPLAs into comprehensive personal assistants signals a significant transformation.These systems could manage all facets of Internet behavior, from customized shopping and personalized travel planning to new generation recommendation systems.By incorporating multimodal data handling, including images and videos, and linking with advanced visualization technologies, RPLAs could significantly enhance personalization and efficiency in everyday tasks.4.Social Simulation throughAutonomous Role-Playing: Utilizing RPLAs for social simulations can significantly extend their application by conducting elaborate experiments in diverse scenarios to study psychological and sociological behaviors.By role-playing various characters, RPLAs can serve as versatile test subjects to explore the influence of different personality traits on social intelligence, providing valuable insights into human behavior and interaction dynamics.Having conversations with widely-recognized established characters attracts extensive interest among users.Consequently, numerous products have been developed to provide RPLAs representing these established characters, including celebrities (e.g., Meta AI), historical figures (e.g., Hello History) and fictional characters (e.g., ChatFAI), or general individuals with specific professions or personalities (e.g., Character.ai).In a more personalized manner, users can also create RPLAs with user-defined personas (e.g., Character.ai,Replika).Technically, these RPLAs are typically built based on LLMs with strong role-playing capacity, with character settings briefly described in prompts.While several open-source projects and research efforts such as ChatHaruhi</p>
<ol>
<li>RPLA as Personal Assistants for Personal Decision-making:</li>
</ol>
<p>Table 4 :
4
Overview of RPLA applications and products based on LLMs.For personalized data, "Personal Profile" refers to data about one's identity, including age, appearance, voice, and biographical information."Behavior History" denotes data derived from interactions between users and applications, representing user behavior patterns."File" pertains to documents and computer files containing private knowledge regardless of personal identity, such as code and manuals.The three types of personalized data roughly correspond to profile, interactions, and domain knowledge in §6 respectively.
ProductDomainDescriptionTarget AudienceGeneration ModalityPersonalized DataPersona-oriented RPLA ProductsA general AI chat app with a wide range ofCharacter.aiChatbotscharacters based on individuals with specificToCText-professions or personalitiesMeta AIFamiliarChatbotsAI characters role-playing celebritiesToCText-FacesHello HistoryChatbotsConversation with historical figuresToCText-A general AI chat app with a wide range ofChatfaiChatbotscharacters based on individuals with specificToCText-professions or personalitiesReplikaChatbotsAn AI companion that serves as an empathetic friend to the userToCTextBehavior HistoryAn AI friend that allows users to journalRosebudChatbotstheir thoughts for mental health andToCTextBehavior Historypersonal growthFileRewindChatbotsA personalized agent that has the context of what users have seen, heard, or said on their deviceToC/ToBText AudioPersonal Profile Behavior History FileBHumanChatbotsAI digital clone of oneself with added modalities of face cloning and voice cloningToC/ToBText AudioPersonal Profile Behavior HistoryVideoFilepersonal.aiChatbotsTrain one's own AI with knowledge of oneself and their own memoriesToC/ToBTextPersonal Profile Behavior HistoryFileAn AI NPC sandbox that allows users toEmemeGamescreate characters and observe their life andToCText-interactionsA text-based adventure game where usersAI DungeonGamesdefine the characters and the setting andToCText-also participate in the game as a characterAn interactive fiction game where one canSagaGamesplay as a character from pre-existing Worlds and Characters from popular franchises andToCText-mediaAn interactive game that allows users toHidden DoorGamesplay as a character in a world that is converted from the existing movie, novel, orToCText-other types of franchise
By "engagement", we mean the state of whether the LLMs are successfully engaged in role-playing activities.
. Anthropomorphic Capabilities: RPLAs are expected to acquire cognitive, emotional and social intelligence towards human levels. Relevant dimensions include conversation attractiveness(Zhou et al., 2023a;Tu et al., 2024), theory of mind(Kosinski, 2023;Mao et al., 2023), empathy(Sorin et al., 2023), emotional intelligence(Huang et al., 2023a), and goal-driven social skills(Zhou et al., 2024b;Wang et al., 2024b). These capabilities are practically important for RPLAs to effectively serve as emotional companions for humans.Character Fidelity This line of work evaluates how a specific RPLA reproduces the intended character, which depends on both the foundation model, the agent framework, and the character data. Relevant dimensions are categorized into four categories: linguistic style and knowledge, which are considered superficial, as well as personality and thought, which represent deeper, underlying aspects:1. Linguistic Style: Basically, RPLAs should speak in a tone that emulates the linguistic style of the intended characters(Wang et al., 2024a;Li et al., 2023a;Zhou et al., 2023a;Yu et al., 2024). For this purpose, RPLAs are typically provided with demonstrative character dialogues(Wang et al., 2024a;Li et al., 2023a), and they could mimic the tone leveraging the in-context learning ability of LLMs.2. Knowledge: RPLAs are essentially required to simulate the character's breadth of knowledge. On one hand, they should accurately recall knowledge of the character, including their identity(Zhou et al., 2023a;Wang et al., 2024a;Tang et al., 2024;Lu et al., 2024), social relationships(Chen et al., 2023c;Shen et al., 2023a;Zhao et al., 2023a), and experiences(Shao et al., 2023;Wang et al., 2024a;Chen et al., 2023c;Yu et al., 2024). On the other hand, they may be required to refrain from demonstrating knowledge or ability beyond the character's scope (e.g., an LLM could write code even if it is role-playing Socrates, which is unnecessarily expected)(Shao et al., 2023;Lu et al., 2024;Yu et al., 2024). This phenomenon is referred to as "character hallucination"(Shao et al., 2023), which originates from the extensive knowledge possessed by LLMs and could be reduced via SFT(Shao et al., 2023).
Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1236-1270, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.88. URL https://aclanthology.org/2023.findings-emnlp.88.
A RPLA ProductsThe recent remarkable advancements in LLMs have sparked a myriad of AI applications.Persona and personalization are central to these applications, with their demands shaping and propelling research in RPLAs.In this section, we provide a brief overview of recent trends in RPLA applications.Specifically, we distinguish RPLAs in existing products into two categories, namely persona-oriented RPLAs and task-oriented RPLAs, as listed in Table4.A.1 Persona-oriented RPLA ProductsPersona-oriented RPLAs typically role-play as specific characters, which has been popular in various entertainment applications, such as chatbots and game NPCs.These RPLAs are generally sourced from fictional characters, historical figures or celebrities, aligning with the research trends on character persona as introduced in §5.They are further forked for individual use cases to meet their preference.We categorize existing persona-oriented RPLA products based on their primary interaction focus, either human-RPLA interactions or RPLA-RPLA interactions.Interactions between Humans and RPLAs Persona-oriented RPLAs, such as those in Character.ai,are initially applied for human-RPLA interactions.These RPLAs can be both initiated from established characters and shaped through ongoing user interactions.progress and abilities, and providing multimodal feedback.Platforms such as WHOOP Coach and Humango collect users' biometric information, physical characteristics, and fitness levels.Through natural language conversations, these agents offer personalized training plans tailored to individual preferences and needs.By making personalized health coaching more accessible, these AI-powered RPLAs democratize access to expert guidance and support for a wider audience.Office For office productivity, task-oriented RPLAs can role-play as the copilot for individual workers based on their office data, such as document files and code repositories.Hence, they deliver context-aware assistance for user requests, such as generating content and providing insights.For example, Microsoft 365 Copilot integrates various user data to deliver intelligent services that respond to user queries and enable more convenient interaction with applications.It integrates with Microsoft Graph and utilizes user data from various sources, including documents, email threads and others, with continuous learning mechanisms to improve its performance over time.Similarly, GitHub Copilot integrates individual code repositories and serves as the copilot to boost the productivity of programmers.These personalized RPLAs empower users to streamline their workflows and enhance productivity within the office environment.
Conversational health agents: A personalized llm-powered agent framework. Iman Mahyar Abbasian, Azimi, Ramesh Amir M Rahmani, Jain, arXiv:2310.023742023arXiv preprint</p>
<p>Multimodal persona based generation of comic dialogs. Harsh Agrawal, Aditya Mishra, Manish Gupta, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>MPCHAT: Towards multimodal persona-grounded conversation. Jaewoo Ahn, Yeda Song, Sangdoo Yun, Gunhee Kim, 10.18653/v1/2023.acl-long.189Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>TimeChara: Evaluating point-in-time character hallucination of role-playing large language models. Jaewoo Ahn, Taehyun Lee, Junyoung Lim, Jin-Hwa Kim, Sangdoo Yun, Hwaran Lee, Gunhee Kim, 10.18653/v1/2024.findings-acl.197Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024and virtual meeting</p>
<p>Neuro-symbolic language modeling with automaton-augmented retrieval. Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, Graham Neubig, International Conference on Machine Learning. PMLR2022</p>
<p>. Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Fordet al. Many-shot jailbreaking</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Model card and evaluations for claude models. Anthropic, 2023a</p>
<p>Releasing claude instant 1.2. 2023b. Anthropic, </p>
<p>The claude 3 model family: Opus, sonnet, haiku, 2024. Apple Inc. Siri voice-activated assistant. Anthropic, 2024Personal communication through Apple iPhone</p>
<p>Human memory: A proposed system and its control processes. C Richard, Richard M Atkinson, Shiffrin, Psychology of learning and motivation. Elsevier19682</p>
<p>Knowledge-augmented large language models for personalized contextual query suggestion. Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen Herring, Sujay Kumar, Jauhar , arXiv:2311.06318v12023arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mccandlish, Chris Olah, Ben Mann, Jared Kaplan, arXiv:2204.058622022aarXiv preprint</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022barXiv preprint</p>
<p>Qianyan-chinese persona chat dataset. Baidu, 2020Dataset available from Baidu</p>
<p>Tallrec: An effective and efficient tuning framework to align large language model with recommendation. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He, 2023</p>
<p>The big five personality dimensions and job performance: a meta-analysis. R Murray, Michael K Barrick, Mount, Personnel psychology. 4411991</p>
<p>Bem sex role inventory. L Sandra, Bem, Journal of personality and social psychology. 1981</p>
<p>Leveraging large language models for decision support in personalized oncology. Manuela Benary, David Xing, Max Wang, Dominik Schmidt, Georg Soll, Mani Hilfenhaus, Christian Nassir, Maren Sigler, Ulrich Knödler, Dieter Keller, Beule, JAMA Network Open. 6112023</p>
<p>Optimizing retrievalaugmented reader models via token elimination. Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, Moshe Wasserblat, doi: 10.18653Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>URL. </p>
<p>Symbolic control: issues of empirical description of agencies and agents. Basil Bernstein, International journal of social research methodology. 412001</p>
<p>A theoretical computer science perspective on consciousness and artificial general intelligence. Engineering. Lenore Blum, Manuel Blum, 202325</p>
<p>Improving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, International conference on machine learning. PMLR2022</p>
<p>Introducing rpbench-auto. Bosonai, 2024</p>
<p>let your characters tell their story. Faeze Brahman, Meng Huang, Oyvind Tafjord, Chao Zhao, Mrinmaya Sachan, Snigdha Chaturvedi, arXiv:2109.05438A dataset for character-centric narrative understanding. 2021arXiv preprint</p>
<p>Sam Andres M Bran, Andrew D Cox, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples. Hezekiah J Branch, Jonathan Rodriguez Cefalu, Jeremy Mchugh, Leyla Hujer, Aditya Bahl, Daniel Del Castillo Iglesias, Ron Heichman, Ramesh Darwishi, arXiv:2209.021282022arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Identifying and manipulating the personality traits of language models. Graham Caron, Shashank Srivastava, arXiv:2212.102762022arXiv preprint</p>
<p>clembench: Using game play to evaluate chat-optimized language models as conversational agents. Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, David Schlangen, 10.18653/v1/2023.emnlp-main.689Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Scaling synthetic data creation with 1,000,000,000 personas. Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu, arXiv:2406.200942024arXiv preprint</p>
<p>Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, Eric Wong, arXiv:2310.08419Jailbreaking black box large language models in twenty queries. 2023arXiv preprint</p>
<p>Be selfish, but wisely: Investigating the impact of agent personality in mixed-motive human-agent interactions. Wenshuo Chao, Zhi Zheng, Hengshu Zhu, Hao Liu ; Kushal, Ian Chawla, Yu Wu, Gale Rong, Jonathan Lucas, Gratch, 10.18653/v1/2023.emnlp-main.808Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2024. December 2023Make large language model a better ranker</p>
<p>Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, arXiv:2403.13679Sociality evaluation of role-playing conversational agents. 2024arXiv preprint</p>
<p>Put your money where your mouth is: Evaluating strategic planning and execution of llm agents in an auction arena. Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, Kyle Richardson, arXiv:2310.057462023aarXiv preprint</p>
<p>When large language models meet personalization: Perspectives of challenges and opportunities. Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, Defu Lian, Enhong Chen, 2023b</p>
<p>Large language models meet harry potter: A dataset for aligning dialogue agents with characters. Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, Jia Li, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023c</p>
<p>Longlora: Efficient fine-tuning of long-context large language models. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia, arXiv:2309.123072023darXiv preprint</p>
<p>ChatCoT: Toolaugmented chain-of-thought reasoning on chat-based large language models. Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Xin Zhao, Ji-Rong Wen, 10.18653/v1/2023.findings-emnlp.985Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023e</p>
<p>Marked personas: Using natural language prompts to measure stereotypes in language models. Myra Cheng, Esin Durmus, Dan Jurafsky, 2023a</p>
<p>Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han, Nan Du, arXiv:2404.10642Self-playing adversarial language game enhances llm reasoning. 2024arXiv preprint</p>
<p>Lift yourself up: Retrievalaugmented text generation with self-memory. Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, Rui Yan, Thirty-seventh Conference on Neural Information Processing Systems. 2023b</p>
<p>A personalized dialogue generator with implicit user persona detection. Itsugun Cho, Dongyang Wang, Ryota Takahashi, Hiroaki Saito, ; Chu-Ren, Hansaem Huang, James Kim, Leo Pustejovsky, Wanner, Key-Sun, Pum-Mo Choi, Hsin-Hsi Ryu, Lucia Chen, Heng Donatelli, Sadao Ji, Patrizia Kurohashi, Nianwen Paggio, Seokhwan Xue, Younggyun Kim, Zhong Hahm, Tony Kyungil He, Enrico Lee, Santus, Proceedings of the 29th International Conference on Computational Linguistics. Francis Bond, Seung-Hoon Na, the 29th International Conference on Computational LinguisticsGyeongju, Republic of KoreaOctober 2022International Committee on Computational Linguistics</p>
<p>Ik Won, Cho, Kyung Yoon, Seoyeon Lee, Jihwan Bae, Sangah Kim, Moosung Park, Sowon Kim, Nam Soo Hahn, Kim, arXiv:2304.00350When crowd meets persona: Creating a large-scale open-domain persona dialogue corpus. 2023arXiv preprint</p>
<p>Inducing anxiety in large language models increases exploration and bias. Julian Coda-Forno, Kristin Witte, Akshay K Jagadish, Marcel Binz, Zeynep Akata, Eric Schulz, arXiv:2304.111112023arXiv preprint</p>
<p>Lamda: Language models for dialog applications. Aaron Daniel Cohen, Adam Roberts, Alejandra Molina, Alena Butryna, Alicia Jin, Apoorv Kulshreshtha, Ben Hutchinson, Ben Zevenbergen, Blaise Hilary Aguera-Arcas, Chung Ching Chang, Claire Cui, Cosmo Du, Daniel De Freitas Adiwardana, Dehao Chen, ( Dmitry, Ed H Dima) Lepikhin, Erin Chi, Heng-Tze Hoffman-John, Hongrae Cheng, Igor Lee, James Krivokon, Jamie Qin, Joe Hall, Johnny Fenton, Kathy Soraker, Kristen Meier-Hellstern, Lora Mois Olson, Maarten Aroyo, Marc Joseph Paul Bosma, Marcelo Amorim Pickett, Marian Menegali, Mark Croak, Matthew Díaz, Maxim Lamm, Meredith Ringel Krikun, Noam Morris, Quoc V Shazeer, Rachel Le, Ravi Bernstein, Ray Rajakumar, Romal Kurzweil, Steven Thoppilan, Taylor Zheng, Toju Bos, Tulsee Duke, Vincent Y Doshi, Vinodkumar Zhao, Will Prabhakaran, Yaguang Rusch, Yanping Li, Yanqi Huang, Yuanzhong Zhou, Zhifeng Xu, Chen, arXiv. 2022</p>
<p>Human memory. I M Fergus, Janine M Craik, Jennings, 1992</p>
<p>Uncovering chatgpt's capabilities in recommender systems. Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, Jun Xu, 2023a</p>
<p>Mmrole: A comprehensive framework for developing and evaluating multimodal role-playing agents. Yanqi Dai, Huanran Hu, Lei Wang, Shengjie Jin, Xu Chen, Zhiwu Lu, 2024a</p>
<p>Language-based user profiles for recommendation. Yijia Dai, Joyce Zhou, Thorsten Joachims, 2024b</p>
<p>Promptagator: Few-shot dense retrieval from 8 examples. Zhuyun Dai, Y Vincent, Ji Zhao, Yi Ma, Jianmo Luan, Jing Ni, Anton Lu, Kelvin Bakalov, Keith Guu, Ming-Wei Hall, Chang, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement. Oyvind Bhavana Dalvi Mishra, Peter Tafjord, Clark, arXiv:2204.13074v2arXiv:1811.01241Wizard of wikipedia: Knowledge-powered conversational agents. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, 2022. 2018arXiv preprint</p>
<p>The second conversational intelligence challenge (convai2). Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, The NeurIPS'18 Competition: From Machine Learning to Intelligent Conversations. Springer2020</p>
<p>Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, Furu Wei, arXiv:2307.02486Longnet: Scaling transformers to 1,000,000,000 tokens. 2023aarXiv preprint</p>
<p>Keyu Ding, Yongcan Wang, Zihang Xu, Zhenzhen Jia, Shijin Wang, Cong Liu, Enhong Chen, arXiv:2311.01166Generative input: Towards next-generation input methods paradigm. 2023barXiv preprint</p>
<p>Towards next-generation intelligent assistants leveraging llm techniques. Xin Luna, Dong , Seungwhan Moon, Ethan Yifan, Kshitiz Xu, Zhou Malik, Yu, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Personalization and customization of llm responses. Joel Eapen, Vs Adhithyan, </p>
<p>Learning to teach reinforcement learning agents. Anestis Fachantidis, Matthew E Taylor, Ioannis Vlahavas, Machine Learning and Knowledge Extraction. 112017</p>
<p>Human-level play in the game of <i>diplomacy</i> by combining language models with strategic reasoning. Anton Fair, Noam Bakhtin, Emily Brown, Gabriele Dinan, Colin Farina, Daniel Flaherty, Andrew Fried, Jonathan Goff, Hengyuan Gray, Athul Paul Hu, Mojtaba Jacob, Karthik Komeili, Minae Konath, Adam Kwon, Mike Lerer, Alexander H Lewis, Sasha Miller, Adithya Mitts, Stephen Renduchintala, Dirk Roller, Weiyan Rowe, Joe Shi, Alexander Spisak, David Wei, Hugh Wu, Markus Zhang, Zijlstra, 10.1126/science.ade9097Science. 37866242022</p>
<p>Tooltalk: Evaluating tool-usage in a conversational setting. Nicholas Farn, Richard Shin, arXiv:2311.107752023arXiv preprint</p>
<p>Reflective linguistic programming (rlp): A stepping stone in socially-aware agi (socialagi). Kevin A Fischer, arXiv:2305.126472023arXiv preprint</p>
<p>Automatic goal generation for reinforcement learning agents. Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel, International conference on machine learning. PMLR2018</p>
<p>Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, arXiv:2312.13108Task-oriented desktop graphical user interface automation. 2023aarXiv preprint</p>
<p>Livechat: A large-scale personalized dialogue dataset automatically constructed from live streaming. Jingsheng Gao, Yixin Lian, Ziyi Zhou, Yuzhuo Fu, Baoyuan Wang, arXiv:2306.084012023barXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Google , arXiv:2312.11805Tear Gosling, Alpin Dale, and Yinhe Zheng. Pippa: A partially synthetic conversational dataset. 2023. 2023arXiv preprint</p>
<p>Agent-based simulation in the study of social dilemmas. Nicholas Mark Gotts, Gareth Polhill, N R Alistair, Law, Artificial Intelligence Review. 192003</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>Agent group chat: An interactive group chat simulacra for better eliciting collective emergent behavior. Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai, Hao Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, arXiv:2403.134332024arXiv preprint</p>
<p>Intelligent virtual assistants with llm-based process automation. Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu Wang, Feiyue Ni, Ruihua Song, Longfei Li, Jinjie Gu, Chenyi Zhuang, 2023</p>
<p>Large language models as rational players in competitive economics games. Shangmin Guo, Haochuan Wang, Haoran Bu, Yi Ren, Dianbo Sui, Yu-Ming Shang, Siting Lu, 2023</p>
<p>Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sabharwal, Tushar Khot, arXiv:2311.04892Bias runs deep: Implicit reasoning biases in persona-assigned llms. 2023arXiv preprint</p>
<p>Sociodemographic bias in language models: A survey and forward path. Vipul Gupta, Pranav Narayanan Venkit, Shomir Wilson, Rebecca J Passonneau, 2024</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, 2017</p>
<p>Lm-infinite: Simple on-the-fly length generalization for large language models. Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Ji Heng, Sinong Wang, arXiv:2308.161372023arXiv preprint</p>
<p>Meet your favorite character: Open-domain chatbot mimicking fictional characters with only a few utterances. Seungju Han, Beomsu Kim, Jin Yong Yoo, Seokjun Seo, Sangbum Kim, Enkhbayar Erdenee, Buru Chang, 10.18653/v1/2022.naacl-main.377Proceedings of the 2022 Conference of the North American Chapter. Marine Carpuat, Marie-Catherine De Marneffe, Ivan Vladimir, Meza Ruiz, the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>The political ideology of conversational ai: Converging evidence on chatgpt's pro-environmental, left-libertarian orientation. Jochen Hartmann, Jasper Schwenzow, Maximilian Witte, arXiv:2301.017682023arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, 2024</p>
<p>War and peace (waragent): Large language model-based multi-agent simulation of world wars. Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister ; Wenyue, Lizhou Hua, Lingyao Fan, Kai Li, Jianchao Mei, Yingqiang Ji, Libby Ge, Yongfeng Hemphill, Zhang, arXiv:2311.172272023. 2023arXiv preprintTool documentation enables zero-shot tool-usage with large language models</p>
<p>Jen-Tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R Lyu, arXiv:2308.03656Emotionally numb or empathetic? evaluating how llms feel using emotionbench. 2023aarXiv preprint</p>
<p>Revisiting the reliability of psychological scales on large language models. Jen-Tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, Michael R Lyu, 2023b</p>
<p>On the humanity of conversational ai: Evaluating the psychological portrayal of llms. Jen-Tse Huang, Wenxuan Wang, Eric John Li, Man Ho, Lam , Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu, The Twelfth International Conference on Learning Representations. 2023c</p>
<p>Who is chatgpt? benchmarking llms' psychological portrayal using psychobench. Jen-Tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael R Lyu, 2024a</p>
<p>Towards practical tool usage for continually learning llms. Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Sarath Chandar, 2024b</p>
<p>Planning, creation, usage: Benchmarking llms for comprehensive tool utilization in real-world complex scenarios. Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, arXiv:2401.171672024carXiv preprint</p>
<p>Metatool benchmark for large language models: Deciding whether to use tools and which to use. Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Yixin Qihui, Pan Liu, Yao Zhou, Neil Wan, Zhenqiang Gong, arXiv:2310.031282023darXiv preprint</p>
<p>Unlocking adaptive user experience with generative ai. Yutan Huang, Tanjila Kanij, Anuradha Madugalla, Shruti Mahajan, Chetan Arora, John Grundy, 2024d</p>
<p>Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, Jason Weston, arXiv:1905.019692019arXiv preprint</p>
<p>Faithful persona-based conversational dataset generation with large language models. Pegah Jandaghi, Xianghai Sheng, Xinyi Bai, Jay Pujara, Hakim Sidahmed, arXiv:2312.100072023arXiv preprint</p>
<p>Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, Prithviraj Ammanabrolu, arXiv:2310.11564Personalized soups: Personalized large language model alignment via post-hoc parameter merging. 2023arXiv preprint</p>
<p>Call for customized conversation: Customized conversation grounding persona and knowledge. Yoonna Jang, Jungwoo Lim, Yuna Hur, Dongsuk Oh, Suhyune Son, Yeonsoo Lee, Donghoon Shin, Seungryong Kim, Heuiseok Lim, Proceedings of the AAAI Conference on Artificial Intelligence. Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, Li Chen, the AAAI Conference on Artificial Intelligence2022. 202136A survey on conversational recommender systems</p>
<p>Evaluating and inducing personality in pre-trained language models. Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, Yixin Zhu, 2023a</p>
<p>Active retrieval augmented generation. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, 10.18653/v1/2023.emnlp-main.495Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023b</p>
<p>Mmtom-qa: Multimodal theory of mind question answering. Chuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio Torralba, Joshua B Tenenbaum, Tianmin Shu, 2024</p>
<p>Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, Derek Zhiyuan Cheng, Do llms understand user preferences? evaluating llms on user rating prediction. 2023</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNovember 2020</p>
<p>Saketh Reddy, Karra , arXiv:2204.12000Son The Nguyen, and Theja Tulabandhula. Estimating the personality of white-box language models. 2022arXiv preprint</p>
<p>Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models. Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, Jaewoo Kang, 10.18653/v1/2023.emnlp-main.63Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023a</p>
<p>Commonsense-augmented memory construction and management in long-term conversations via context-aware persona refinement. Hana Kim, Kai Tzu Iunn, Seoyeon Ong, Dongha Kim, Jinyoung Lee, Yeo, 2024</p>
<p>SODA: Million-scale dialogue distillation with social commonsense contextualization. Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, Yejin Choi, 10.18653/v1/2023.emnlp-main.799Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023b</p>
<p>Fantom: A benchmark for stress-testing machine theory of mind in interactions. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Bras, Gunhee Kim, Yejin Choi, Maarten Sap, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023c</p>
<p>Propile: Probing privacy leakage in large language models. Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, Seong Joon Oh, Thirty-seventh Conference on Neural Information Processing Systems. 2023d</p>
<p>Better zero-shot reasoning with role-play prompting. Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, 2023</p>
<p>Michal Kosinski, arXiv:2302.02083Theory of mind might have spontaneously emerged in large language models. 2023arXiv preprint</p>
<p>Gender bias and stereotypes in large language models. Hadas Kotek, Rikker Dockum, David Sun, 10.1145/3582269.3615599Proceedings of The ACM Collective Intelligence Conference, CI '23. The ACM Collective Intelligence Conference, CI '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Grgur Kovač, Masataka Sawayama, Rémy Portelas, Cédric Colas, Peter Ford Dominey, Pierre-Yves Oudeyer, arXiv:2307.07870Large language models as superpositions of cultural perspectives. 2023arXiv preprint</p>
<p>The ethics of interaction: Mitigating security threats in llms. Ashutosh Kumar, Sagarika Singh, Shiv Vignesh Murty, Swathy Ragupathy, 2024</p>
<p>Symbolic agent negotiation for semantic web service exploitation. Peep Küngas, Jinghai Rao, Mihhail Matskin, Advances in Web-Age Information Management: 5th International Conference, WAIM 2004. Dalian, ChinaSpringerJuly 15-17, 2004 5. 2004</p>
<p>Chain of empathy: Enhancing empathetic response of large language models based on psychotherapy models. Kyung Yoon, Inju Lee, Minjung Lee, Seoyeon Shin, Sowon Bae, Dokyong Hahn ; Jun Lee, Junyoung Lee, Kyeongjin Youn, Byungsoo Oh, Jonghwan Ko, Ho-Jin Hyeon, Choi, arXiv:2311.04915Social long-term multi-modal conversation with persona commonsense knowledge. Stark2023. 2024arXiv preprint</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Red Hook, NY, USACurran Associates Inc2020ISBN 9781713829546</p>
<p>Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, M I Weishi, Yaying Fei, Xiaoyang Feng, Song Yan, Haosheng Wang, arXiv:2308.09597Reviving anime character in reality via large language model. 2023aarXiv preprint</p>
<p>Cheng Li, Mengzhou Chen, Jindong Wang, Sunayana Sitaram, Xing Xie, arXiv:2402.10946Culturellm: Incorporating cultural differences into large language models. 2024aarXiv preprint</p>
<p>How long can context length of open-source LLMs truly promise?. Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, Hao Zhang, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023b</p>
<p>Multi-level contrastive learning for script-based character understanding. Dawei Li, Hengyuan Zhang, Yanran Li, Shiping Yang, arXiv:2310.132312023carXiv preprint</p>
<p>Camel: Communicative agents for "mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc2023d36</p>
<p>Multi-step jailbreaking privacy attacks on ChatGPT. Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song, 10.18653/v1/2023.findings-emnlp.272Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023e</p>
<p>A persona-based neural conversation model. Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis, Jianfeng Gao, William B Dolan, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational Linguistics20161</p>
<p>Dialogue history matters! personalized response selection in multi-turn retrieval-based chatbots. Juntao Li, Chang Liu, Chongyang Tao, Zhangming Chan, Dongyan Zhao, Min Zhang, Rui Yan, ACM Transactions on Information Systems (TOIS). 3942021</p>
<p>Junyi Li, Ninareh Mehrabi, Charith Peris, Palash Goyal, Kai-Wei Chang, Aram Galstyan, Richard Zemel, Rahul Gupta, arXiv:2311.04978v1On the steerability of large language models toward data-driven personas. 2023farXiv preprint</p>
<p>API-bank: A comprehensive benchmark for tool-augmented LLMs. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023g</p>
<p>Api-bank: A comprehensive benchmark for tool-augmented llms. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023h</p>
<p>Is gpt-3 a psychopath? evaluating large language models from a psychological perspective. Xingxuan Li, Yutong Li, Linlin Liu, Lidong Bing, Shafiq Joty, arXiv:2212.105292022arXiv preprint</p>
<p>Personalized language modeling from personalized human feedback. Xinyu Li, Zachary C Lipton, Liu Leqi, 2024b</p>
<p>I think, therefore i am: Benchmarking awareness of large language models using awarebench. Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan, Lichao Sun, 2024c</p>
<p>Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, and Yunxin Liu. Personal llm agents: Insights and survey about the capability, efficiency and security. 2024d</p>
<p>Recai: Leveraging large language models for next-generation recommender systems. Jianxun Lian, Yuxuan Lei, Xu Huang, Jing Yao, Wei Xu, Xing Xie, arXiv:2403.064652024arXiv preprint</p>
<p>From text to tactic: Evaluating llms playing the game of avalon. Jonathan Light, Min Cai, Sheng Shen, Ziniu Hu, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. Jonathan J Timothy P Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. Barcelona, SpainAssociation for Computational Linguistics2015. July 2004arXiv preprintText Summarization Branches Out</p>
<p>Agentsims: An open-source sandbox for large language model evaluation. Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, Qin Chen, 2023</p>
<p>Explainable ai: A review of machine learning interpretability methods. Pantelis Linardatos, Vasilis Papastefanopoulos, Sotiris Kotsiantis, Entropy. 231182020</p>
<p>World model on million-length video and language with ringattention. Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel, arXiv:2402.082682024arXiv preprint</p>
<p>Is chatgpt a good recommender? a preliminary study. Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, Yan Zhang, 2023a</p>
<p>Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023barXiv preprint</p>
<p>Once: Boosting content-based recommendation with both open-and closed-source large language models. Qijiong Liu, Tetsuya Sakai, Nuo Chen, Xiao-Ming Wu, 2023c</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Yuxian Gu, Hangliang Ding, Kai Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Shengqi Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, ArXiv, abs/2308.03688Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. 2023d</p>
<p>Improving personality consistency in conversation by persona extending. Yifan Liu, Wei Wei, Jiayi Liu, Xianling Mao, Rui Fang, Dangyang Chen, Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. the 31st ACM International Conference on Information &amp; Knowledge Management2022</p>
<p>Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou, 10.18653/v1/2024.acl-long.423Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>Xiao Ma, Swaroop Mishra, Ariel Liu, Sophie Su, Jilin Chen, Chinmay Kulkarni, Heng-Tze Cheng, arXiv:2312.00763Quoc Le, and Ed Chi. Beyond chatbots: Explorellm for structured thoughts and personalized model responses. 2023aarXiv preprint</p>
<p>Query rewriting in retrieval-augmented large language models. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan, 10.18653/v1/2023.emnlp-main.322Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023b</p>
<p>Personalized reward learning with interaction-grounded learning for recommender systems. Jessica Maghakian, Paul Mineiro, Kishan Panaganti, Mark Rucker, Akanksha Saran, Cheng Tan, International Conference on Learning Representations. 2023</p>
<p>A review on machine theory of mind. Yuanyuan Mao, Shuang Liu, Pengshuai Zhao, Qin Ni, Xin Lin, Liang He, 2023</p>
<p>Who is GPT-3? an exploration of personality, values and demographics. Marilù Miotto, Nicola Rossberg, Bennett Kleinberg, 10.18653/v1/2022.nlpcss-1.24Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS). David Bamman, Dirk Hovy, David Jurgens, Katherine Keith, Brendan O 'connor, Svitlana Volkova, the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS)Abu Dhabi, UAEAssociation for Computational LinguisticsNovember 2022</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 51875402015</p>
<p>Ret-llm: Towards a general read-write memory for large language models. Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schütze, arXiv:2305.143222023arXiv preprint</p>
<p>Presentations by the humans and for the humans: Harnessing llms for generating persona-aware slides from documents. Ishani Mondal, Shwetha Somasundaram, Anandhavelu Natarajan, Aparna Garimella, Sambaran Bandyopadhyay, Jordan Boyd-Graber, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European Chapterthe Association for Computational Linguistics20241</p>
<p>Text2taste: A versatile egocentric vision system for intelligent reading assistance using large language model. Wiktor Mucha, Florin Cuconasu, Naome A Etori, Giovanni Valia Kalokyri, Trappolini, 2024</p>
<p>Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi, Pearl, arXiv:2311.09180v1Personalizing large language model writing assistants with generation-calibrated retrievers. 2023arXiv preprint</p>
<p>Having beer after prayer? measuring cultural bias in large language models. Tarek Naous, Michael J Ryan, Alan Ritter, Wei Xu, 2024</p>
<p>Descent-to-delete: Gradient-based methods for machine unlearning. Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, Algorithmic Learning Theory. PMLR2021</p>
<p>How well can llms echo us? evaluating ai chatbots' role-play ability with echo. Man Tik Ng, Hui Tung Tse, Jen Tse Huang, Jingjing Li, Wenxuan Wang, Michael R Lyu, 2024</p>
<p>Openai, Chatgpt, ArXiv, abs/2303.08774GPT-4 technical report. 2022. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Charles Packer, Vivian Fang, G Shishir, Kevin Patil, Sarah Lin, Joseph E Wooders, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2023arXiv preprint</p>
<p>Do llms possess a personality? making the mbti test an amazing evaluation for large language models. Keyu Pan, Yawen Zeng, arXiv:2307.161802023arXiv preprint</p>
<p>Aaron Parisi, Yao Zhao, Noah Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022arXiv preprint</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. Sung Joon, Lindsay Park, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23), UIST '23. New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>In-context unlearning: Language models as few shot unlearners. Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju, arXiv:2310.075792023arXiv preprint</p>
<p>Ignore previous prompt: Attack techniques for language models. Fábio Perez, Ian Ribeiro, NeurIPS ML Safety Workshop. 2022</p>
<p>chrf++: words helping character n-grams. Maja Popović, Proceedings of the second conference on machine translation. the second conference on machine translation2017</p>
<p>Does the chimpanzee have a theory of mind?. Matt Post, arXiv:1804.08771A call for clarity in reporting bleu scores. David Premack, Guy Woodruff, 2018. 19781arXiv preprint</p>
<p>Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. 2023</p>
<p>Enabling on-device large language model personalization with self-supervised data selection and synthesis. Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, Yiyu Shi, arXiv:2311.12275v32024arXiv preprint</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, 2024</p>
<p>Capturing minds, not just words: Enhancing role-playing language models with personality-indicative data. Yiting Ran, Xintao Wang, Rui Xu, Xinfeng Yuan, Jiaqing Liang, Yanghua Xiao, Deqing Yang, The Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf, 7th Annual Conference on Robot Learning. 2023</p>
<p>Can chatgpt assess human personalities? a general evaluation framework. Haocong Rao, Cyril Leung, Chunyan Miao, arXiv:2303.012482023arXiv preprint</p>
<p>Can language models recognize convincing arguments?. Paula Rescala, Manoel Horta Ribeiro, Tiancheng Hu, Robert West, 2024</p>
<p>Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, Rui Zhao, arXiv:2308.03427Tptu: Task planning and tool usage of large language model-based ai agents. 2023arXiv preprint</p>
<p>The self-perception and political biases of chatgpt. Jérôme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, Markus Pauly, arXiv:2304.073332023arXiv preprint</p>
<p>Mustafa Safdari, Greg Serapio-García, Clément Crepy, Stephen Fitz, Peter Romero, Luning Sun, Marwa Abdulhai, Aleksandra Faust, Maja Matarić, arXiv:2307.00184Personality traits in large language models. 2023arXiv preprint</p>
<p>Lamp: When large language models meet personalization. Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani, 2024</p>
<p>Chatgpt utility in health care education, research, and practice: Systematic review on the promising perspectives and valid concerns. Malik Sallam, 10.3390/healthcare11060887Healthcare. 112023</p>
<p>Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, Vishvak Murahari, arXiv:2407.18416Personagym: Evaluating persona agents and llms. 2024arXiv preprint</p>
<p>Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto, arXiv:2303.17548Whose opinions do language models reflect?. 2023arXiv preprint</p>
<p>Neural theory-of-mind? on the limits of social intelligence in large lms. Maarten Sap, Ronan Lebras, Daniel Fried, Yejin Choi, 2023</p>
<p>Evaluating the moral beliefs encoded in llms. Nino Scherrer, Claudia Shi, Amir Feder, David Blei, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Minding language models' (lack of) theory of mind: A plug-and-play multi-character belief tracker. Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, Yulia Tsvetkov, 10.18653/v1/2023.acl-long.780Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Greg Serapio-García, Mustafa Safdari, Clément Crepy, Luning Sun, Stephen Fitz, Peter Romero, Marwa Abdulhai, Aleksandra Faust, and Maja Matarić. Personality traits in large language models. 2023</p>
<p>Show, don't tell: Aligning language models with demonstrated feedback. Omar Shaikh, Michelle Lam, Joey Hejna, Yijia Shao, Michael Bernstein, Diyi Yang, arXiv:2406.008882024aarXiv preprint</p>
<p>Show, don't tell: Aligning language models with demonstrated feedback. Omar Shaikh, Michelle Lam, Joey Hejna, Yijia Shao, Michael Bernstein, Diyi Yang, 2024b</p>
<p>Role play with large language models. Murray Shanahan, Kyle Mcdonell, Laria Reynolds, Nature. 62379872023</p>
<p>Character-llm: A trainable agent for role-playing. Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu, arXiv:2310.101582023arXiv preprint</p>
<p>Natalie Shapira, Mosh Levy, Hossein Seyed, Xuhui Alavi, Yejin Zhou, Yoav Choi, Maarten Goldberg, Vered Sap, Shwartz, arXiv:2305.14763Clever hans or neural theory of mind? stress testing social reasoning in large language models. 2023arXiv preprint</p>
<p>Building persona consistent dialogue agents with offline reinforcement learning. Ryan Shea, Zhou Yu, arXiv:2310.107352023arXiv preprint</p>
<p>Roleeval: A bilingual role evaluation benchmark for large language models. Tianhao Shen, Sun Li, Deyi Xiong, arXiv:2312.161322023aarXiv preprint</p>
<p>HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Thirty-seventh Conference on Neural Information Processing Systems. 2023b</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, Yueting Zhuang, arXiv:2311.18760Taskbench: Benchmarking large language models for task automation. 2023carXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv:2303.11366Significant-Gravitas. Autogpt. 2023. 2023arXiv preprint</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Large language models (llms) and empathy-a systematic review. Danna Vera Sorin, Yiftach Brin, Eli Barash, Alexander Konen, Girish Charney, Eyal Nadkarni, Klang, medRxiv. 2023</p>
<p>Beyond memorization: Violating privacy via inference with large language models. Robin Staab, Mark Vero, Mislav Balunović, Martin Vechev, arXiv:2310.072982023arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu, 10.1016/j.neucom.2023.127063.URLhttps://www.sciencedirect.com/science/article/pii/S0925231223011864Neurocomputing. 0925-23125681270632024</p>
<p>Persona-db: Efficient large language model personalization for response prediction with collaborative data refinement. Megan Su, Yuwei Bao, ; Chenkai Sun, Ke Yang, Revanth Gangi Reddy, Yi R Fung, Hou Pong Chan, Chengxiang Zhai, Heng Ji, 2024. 2024User modeling challenges in interactive ai assistant systems</p>
<p>Mitigating gender bias in natural language processing: Literature review. Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai Elsherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, William Yang, Wang , Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>International journal of uncertainty, fuzziness and knowledge-based systems. Latanya Sweeney, 200210k-anonymity: A model for protecting privacy</p>
<p>Fiona Anting Tan, Gerard Christopher Yeo, Fanyou Wu, Weijie Xu, Vinija Jain, Aman Chadha, Kokil Jaidka, Yang Liu, See-Kiong Ng, Phantom: Personality has an effect on theory-of-mind reasoning in large language models. 2024</p>
<p>Enhancing role-playing systems through aggressive queries: Evaluation and improvement. Yihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, Kun Gai, arXiv:2402.106182024arXiv preprint</p>
<p>Charactereval: A chinese benchmark for role-playing conversational agent evaluation. Quan Tu, Shilong Fan, Zihang Tian, Rui Yan, arXiv:2401.012752024arXiv preprint</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 202436</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv: Arxiv-2305.162912023aarXiv preprint</p>
<p>Gauchochat: Towards proactive, controllable, and personalized social conversation. Hong Wang, Weizhi Wang, Rajan Saini, Marina Zhukova, Xifeng Yan, Alexa Prize SocialBot Grand Challenge5</p>
<p>Large language models as source planner for personalized knowledge-grounded dialogues. Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Wai-Chung Kwan, Irwin King, Kam-Fai Wong, 10.18653/v1/2023.findings-emnlp.641Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023b</p>
<p>Zero-shot next-item recommendation using large pretrained language models. Lei Wang, Ee-Peng Lim, 2023</p>
<p>Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, Ji-Rong Wen, When large language model based agent meets user behavior analysis: A novel user simulation paradigm. 2023c</p>
<p>RoleLLM: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. Noah Wang, Z Y Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhao Huang, Jie Fu, Junran Peng, 10.18653/v1/2024.findings-acl.878Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024aand virtual meeting</p>
<p>Sotopia-π: Interactive learning of socially intelligent language agents. Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham Neubig, Yonatan Bisk, Hao Zhu, 2024b</p>
<p>Avalon's game of thoughts: Battle against deception through recursive contemplation. Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, Gao Huang, arXiv:2310.013202023darXiv preprint</p>
<p>Linformer: Self-attention with linear complexity. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, Hao Ma, arXiv:2006.047682020arXiv preprint</p>
<p>Not all countries celebrate thanksgiving: On the cultural dominance in large language models. Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-Tse Huang, Zhaopeng Tu, Michael R Lyu, arXiv:2310.124812023earXiv preprint</p>
<p>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji, arXiv:2309.10691Evaluating llms in multi-turn interaction with tools and language feedback. Mint2023farXiv preprint</p>
<p>Surveyagent: A conversational system for personalized and efficient research survey. Xintao Wang, Jiangjie Chen, Nianqi Li, Lida Chen, Xinfeng Yuan, Wei Shi, Xuyang Ge, Rui Xu, Yanghua Xiao, 2024c</p>
<p>InCharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. Xintao Wang, Yunze Xiao, Jen-Tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, Yanghua Xiao, 10.18653/v1/2024.acl-long.102Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 2024d1</p>
<p>Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, arXiv:2307.053002023garXiv preprint</p>
<p>Zhilin Wang, Yu Ying Chiu, Yu Cheung Chiu, arXiv:2310.05418Humanoid agents: Platform for simulating human-like generative agents. 2023harXiv preprint</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023iarXiv preprint</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Transactions on Machine Learning Research. 2022a</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022b</p>
<p>Empowering llm to use smartphone for intelligent task automation. Yuanchun Hao Wen, Guohong Li, Shanhui Liu, Tao Zhao, Toby Jia-Jun Yu, Shiqi Li, Yunhao Jiang, Yaqin Liu, Yunxin Zhang, Liu, arXiv:2308.152722023aarXiv preprint</p>
<p>Unveiling the implicit toxicity in large language models. Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, Minlie Huang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023b</p>
<p>Llm-powered autonomous agents. lilianweng.github.io. Lilian Weng, Jun 2023</p>
<p>Yixuan Weng, Shizhu He, Kang Liu, Shengping Liu, Jun Zhao, arXiv:2402.10151Controllm: Crafting diverse personalities for language models. 2024arXiv preprint</p>
<p>. Stanisław Woźniak, Bartłomiej Koptyra, Arkadiusz Janz, Przemysław Kazienko, Jan Kocoń, 2024Personalized large language models</p>
<p>A new era in llm security: Exploring security concerns in real-world llm-based systems. Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick Mcdaniel, Chaowei Xiao, 2024a</p>
<p>A survey on large language models for recommendation. Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, Enhong Chen, 2023</p>
<p>How easily do irrelevant inputs skew the responses of large language models. Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao, arXiv:2404.033022024barXiv preprint</p>
<p>From role-play to dramainteraction: An LLM solution. Weiqi Wu, Hongqiu Wu, Lai Jiang, Xingyuan Liu, Hai Zhao, Min Zhang, 10.18653/v1/2024.findings-acl.196Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024cand virtual meeting</p>
<p>Secgpt: An execution isolation architecture for llm-based systems. Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal, arXiv:2403.049602024darXiv preprint</p>
<p>Can large language model agents simulate human trust behaviors?. Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, Bernard Ghanem, Guohao Li, 2024a</p>
<p>Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Travelplanner: A benchmark for real-world planning with language agents. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, 2024b</p>
<p>Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Jing Toh, Zhoujun Hua, Dongchan Cheng, Fangyu Shin, Yitao Lei, Yiheng Liu, Shuyan Xu, Silvio Zhou, Caiming Savarese, Victor Xiong, Tao Zhong, Yu, Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. 2024c</p>
<p>Expertprompting: Instructing large language models to be distinguished experts. Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao, 2023a</p>
<p>Cosplay: Concept set guided personalized dialogue generation across both party personas. Chen Xu, Piji Li, Wei Wang, Haoran Yang, Siyun Wang, Chuangbai Xiao, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval2022</p>
<p>Opentom: A comprehensive benchmark for evaluating theory-of-mind reasoning capabilities of large language models. Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, Yulan He, 2024a</p>
<p>On the tool manipulation capability of open-source large language models. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, Jian Zhang, 2023b</p>
<p>Rui Xu, Dakuan Lu, Xiaoyu Tan, Xintao Wang, Siyu Yuan, Jiangjie Chen, Wei Chu, Xu Yinghui, arXiv:2407.05305Mindecho: Role-playing language agents for key opinion leaders. 2024barXiv preprint</p>
<p>Character is destiny: Can large language models simulate persona-driven decisions in role. Rui Xu, Xintao Wang, Jiangjie Chen, Siyu Yuan, Xinfeng Yuan, Jiaqing Liang, Zulong Chen, Xiaoqing Dong, Yanghua Xiao, arXiv:2404.121382024carXiv preprint</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, arXiv:2309.046582023carXiv preprint</p>
<p>Jintang Xue, Yun-Cheng Wang, Chengwei Wei, Xiaofeng Liu, Jonghye Woo, C-C Jay Kuo, arXiv:2309.08836Bias and fairness in chatbots: An overview. 2023arXiv preprint</p>
<p>Realpersonachat: A realistic persona chat corpus with interlocutors' own personalities. Sanae Yamashita, Koji Inoue, Ao Guo, Shota Mochizuki, Tatsuya Kawahara, Ryuichiro Higashinaka, Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation. the 37th Pacific Asia Conference on Language, Information and Computation2023</p>
<p>Palr: Personalization aware llms for recommendation. Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, Yanbin Lu, 2023a</p>
<p>Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan, arXiv:2305.18752Gpt4tools: Teaching large language model to use tools via self-instruction. 2023barXiv preprint</p>
<p>Sequential recommendation with latent relations based on large language model. Shenghao Yang, Weizhi Ma, Peijie Sun, Qingyao Ai, Yiqun Liu, Mingchen Cai, Min Zhang, 2024</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, ; Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, arXiv:2305.10601The Eleventh International Conference on Learning Representations. 2023a. 2023barXiv preprintReact: Synergizing reasoning and acting in language models</p>
<p>Privacy-friendly personalization of llm responses using hashed entity injection. Keun Soo, Yim , Llm as a system service on mobile devices. Wangsong Yin, Mengwei Xu, Yuanchun Li, Xuanzhe Liu, 2023. 2024</p>
<p>Few-shot character understanding in movies as an assessment to meta-learning of theory-of-mind. Mo Yu, Yisi Sang, Kangsheng Pu, Zekai Wei, Han Wang, Jing Li, Yue Yu, Jie Zhou, arXiv:2211.046842022arXiv preprint</p>
<p>Personality understanding of fictional characters during book reading. Mo Yu, Jiangnan Li, Shunyu Yao, Wenjie Pang, Xiaochen Zhou, Zhou Xiao, Fandong Meng, Jie Zhou, arXiv:2305.101562023arXiv preprint</p>
<p>Distilling script knowledge from large language models for constrained language planning. Xiaoyan Yu, Tongxu Luo, Yifan Wei, Fangyu Lei, Yiming Huang, Peng Hao, Liehuang Zhu ; Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Jankowski, Yanghua Xiao, Deqing Yang, arXiv:2402.13717Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2024. July 20231arXiv preprintNeeko: Leveraging dynamic lora for efficient multi-character role-playing agent</p>
<p>Easytool: Enhancing llm-based agents with concise tool instruction. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, Deqing Yang, arXiv:2401.062012024aarXiv preprint</p>
<p>Evaluating character understanding of large language models via character profiling from fictional works. Xinfeng Yuan, Siyu Yuan, Yuhan Cui, Tianhe Lin, Xintao Wang, Rui Xu, Jiangjie Chen, Deqing Yang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024b</p>
<p>Fairness constraints: Mechanisms for fair classification. Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, Krishna P Gummadi, Artificial intelligence and statistics. PMLR2017</p>
<p>Persllm: A personified training approach for large language models. Zheni Zeng, Jiayi Chen, Huimin Chen, Yukun Yan, Yuxuan Chen, Zhenghao Liu, Zhiyuan Liu, Maosong Sun, 2024</p>
<p>Building cooperative embodied agents modularly with large language models. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua Tenenbaum, Tianmin Shu, Chuang Gan, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023a</p>
<p>Exploring collaboration mechanisms for llm agents: A social psychology view. Jintian Zhang, Xin Xu, Shumin Deng, arXiv:2310.021242023barXiv preprint</p>
<p>Personalizing dialogue agents: I have a dog, do you have pets too?. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston, 10.18653/v1/P18-1205Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. Iryna Gurevych, Yusuke Miyao, the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJuly 20181</p>
<p>Huifeng Guo, and Ruiming Tang. Tired of plugins? large language models can be end-to-end recommenders. Wenlin Zhang, Chuhan Wu, Xiangyang Li, Yuhao Wang, Kuicai Dong, Yichao Wang, Xinyi Dai, Xiangyu Zhao, 2024a</p>
<p>Bridging the information gap between domain-specific model and general llm for personalized recommendation. Wenxuan Zhang, Hongzhi Liu, Yingpeng Du, Chen Zhu, Yang Song, Hengshu Zhu, Zhonghai Wu, 2023c</p>
<p>Automatically eliciting toxic outputs from pre-trained language models. Xu Zhang, Xiaojun Wan, 2023</p>
<p>Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, Jiangjie Chen, arXiv:2402.05733Timearena: Shaping efficient multitasking language agents in a time-aware simulation. 2024barXiv preprint</p>
<p>Runcong Zhao, Wenjia Zhang, Jiazheng Li, Lixing Zhu, Yanran Li, Yulan He, Lin Gui, arXiv:2310.01459Narrativeplay: Interactive narrative understanding. 2023aarXiv preprint</p>
<p>Large language models fall short: Understanding complex relationships in detective narratives. Runcong Zhao, Qinglin Zhu, Hainiu Xu, Jiazheng Li, Yuxiang Zhou, Yulan He, Lin Gui, arXiv:2402.110512024arXiv preprint</p>
<p>Chatanything: A framework for generating anthropomorphized personas for llm-based characters. Yilin Zhao, Xinbin Yuan, Shanghua Gao, Zhijie Lin, Qibin Hou, Jiashi Feng, Daquan Zhou, arXiv:2311.06772v12023barXiv preprint</p>
<p>Is" a helpful assistant" the best role for large language models? a systematic evaluation of social roles in system prompts. Mingqian Zheng, Jiaxin Pei, David Jurgens, arXiv:2311.100542023aarXiv preprint</p>
<p>Okr-agent: An object and key results driven agent system with hierarchical self-collaboration and self-evaluation. Yi Zheng, Haibin Huang, Chongyang Ma, Kanle Shi, 2023b</p>
<p>Stylized dialogue response generation using stylized unpaired texts. Yinhe Zheng, Zikai Chen, Rongsheng Zhang, Shilei Huang, Xiaoxi Mao, Minlie Huang, 10.1609/aaai.v35i16.17711Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMay 202135</p>
<p>Generative job recommendations with large language models. Zhi Zheng, Zhaopeng Qiu, Xiao Hu, Likang Wu, Hengshu Zhu, Hui Xiong, arXiv:2307.021572023carXiv preprint</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, Yanlin Wang, arXiv:2305.10250Memorybank: Enhancing large language models with long-term memory. 2023arXiv preprint</p>
<p>Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, arXiv:2311.16832Customizing chinese conversational ai characters with large language models. 2023aarXiv preprint</p>
<p>I cast detect thoughts: Learning to converse and guide with intents and theory-of-mind in dungeons and dragons. Pei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang Ren, Chris Callison-Burch, Yejin Choi, Prithviraj Ammanabrolu, 2023b</p>
<p>Learning to predict persona information for dialogue personalization without explicit persona description. Wangchunshu Zhou, Qifei Li, Chenle Li, 10.18653/v1/2023.findings-acl.186Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023c</p>
<p>Is this the real life? is this just fantasy? the misleading success of simulating social interactions with llms. Xuhui Zhou, Zhe Su, Tiwalayo Eisape, Hyunwoo Kim, Maarten Sap, ; Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, arXiv:2403.05020The Twelfth International Conference on Learning Representations. 2024a. 2024barXiv preprintGraham Neubig, and Maarten Sap</p>
<p>Cognitive personalized search integrating large language models with an efficient memory mechanism. Yujia Zhou, Qiannan Zhu, Jiajie Jin, Zhicheng Dou, 2024c</p>
<p>Llms as dungeon masters' assistants. Andrew Zhu, Lara Martin, Andrew Head, Chris Callison-Burch, Calypso, 10.1609/aiide.v19i1.27534Proceedings of the Nineteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE '23. the Nineteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE '23AAAI Press2023a</p>
<p>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Yu Wang, Zhaoxiang Qiao, Jifeng Zhang, Dai, arXiv:2305.171442023barXiv preprint</p>
<p>Toolqa: A dataset for llm question answering with external tools. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang, Advances in Neural Information Processing Systems. 362024</p>            </div>
        </div>

    </div>
</body>
</html>
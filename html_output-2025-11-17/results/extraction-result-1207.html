<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1207 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1207</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1207</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-270063025</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.17039v1.pdf" target="_blank">BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have catalyzed a paradigm shift in natural language processing, yet their limited controllability poses a significant challenge for downstream applications. We aim to address this by drawing inspiration from the neural mechanisms of the human brain, specifically Broca's and Wernicke's areas, which are crucial for language generation and comprehension, respectively. In particular, Broca's area receives cognitive decision signals from Wernicke's area, treating the language generation as an intricate decision-making process, which differs from the fully auto-regressive language generation of existing LLMs. In a similar vein, our proposed system, the BWArea model, conceptualizes language generation as a decision-making task. This model has three components: a language world model, an inverse dynamics model, and a cognitive policy. Like Wernicke's area, the inverse dynamics model is designed to deduce the underlying cognitive intentions, or latent actions, behind each token. The BWArea model is amenable to both pre-training and fine-tuning like existing LLMs. With 30B clean pre-training tokens, we have trained a BWArea model, which achieves competitive performance with LLMs of equal size (1B parameters). Unlike fully auto-regressive LLMs, its pre-training performance does not degenerate if dirty data unintentionally appears. This shows the advantage of a decomposed structure of BWArea model in reducing efforts in laborious data selection and labeling. Finally, we reveal that the BWArea model offers enhanced controllability via fine-tuning the cognitive policy with downstream reward metrics, thereby facilitating alignment with greater simplicity. On 9 out of 10 tasks from two suites, TextWorld and BigBench Hard, our method shows superior performance to auto-regressive LLMs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1207.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1207.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BWArea world model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BWArea language world model (p_world)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based conditional next-token predictive model that conditions on both token context and a discrete latent action sequence to reduce predictive variance and enable controllable language generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BWArea language world model (p_world)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based sequence model decomposed into a context encoder (first 11 transformer blocks), an action encoder (linear layer producing action embeddings), an aggregate module (linear combining context and action embeddings), and a dynamics module (remaining 11 transformer blocks) that outputs categorical logits for next-token prediction conditioned on context x_{1:t} and latent actions a_{1:t}. The model explicitly takes a discrete latent action (from a VQ-VAE codebook) as an extra conditioning input to the autoregressive token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (transformer-based predictive model conditioned on discrete latent actions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>natural language generation and language-based RL tasks (e.g., instruction following, TextWorld, BigBench Hard, persuasion of another LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>entropy of next-token predictive distribution; token-level cross-entropy / perplexity on evaluation set; downstream benchmark accuracies (MMLU, DROP, BBH, TruthfulQA); marginal vs expected cross-entropy over actions</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Entropy of BWArea world model = 0.32 versus Tinyllama = 2.11 (evaluation set); perplexity for 30B-token pretrained BWArea = 27.93 (reported in data-scaling experiment); evaluation accuracy (unspecified eval set) improves with data scale to 48.5% at 30B tokens; benchmark numbers (30B): MMLU 25.85, DROP 8.27, BBH 27.68, TruthfulQA (MC1) 23.13, (MC2) 42.58.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderately interpretable: discrete latent actions (VQ-VAE codes, N=64) are shown to have semantic structure and controllability — sampling different actions given the same prompt produces distinct coherent generations; actions reduce predictive variance and can be inspected by sampling and observing downstream text outputs. Not a fully transparent symbolic model, but latent actions are discrete and human-inspectable via generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Qualitative generation examples from sampled actions; measurement of entropy reduction; marginal vs expected cross-entropy comparison (marginal loss 6.2 vs expected loss 3.3) showing actions substantially influence outputs; action controllability experiments (sampled actions produce distinct sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>World model parameters ~1.1B; entire BWArea system: world model 1.1B, inverse dynamics 0.5B, policy 1.1B. Pretraining conducted on 8 A800-80G GPUs. Training schedule: pretrain step1 6 days, pretrain step2 4 days; batch size 512, sequence length T=2048, 30k gradient steps per step. SFT and RL steps have smaller runtimes (SFT step durations: ~4h each; RL 9h BBH, 4h TextWorld reported).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to an equal-sized auto-regressive baseline (Tinyllama, 1.1B), BWArea matches or slightly exceeds pretraining benchmark performance while offering much reduced predictive entropy and greater robustness to noisy (dirty) data; requires additional model components (inverse dynamics and policy) so overall parameter and training overhead is higher than a single auto-regressive model but experiments used comparable total parameterization by construction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On TextWorld tasks BWArea converged to optimal returns (Custom:5, Tw-Treasure Hunter:3, Dragon:3) whereas Tinyllama failed on some (Tinyllama final performance 0 on Custom and Dragon). On selected BigBench Hard tasks (7 tasks), BWArea outperformed Tinyllama on 6/7 tasks (example accuracies reported: Reasoning about Colored Objects 25.0% vs 4.3%; Object Tracking 80.0% vs 0.0%; overall charts show BWArea slightly above GPT-3.5 zero-shot but below GPT-4 when averaged). BWArea also successfully persuaded Llama3-8B in a persuasion experiment where Tinyllama failed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The inclusion of discrete latent actions improves controllability (policy can select actions to steer generation) and reduces predictive variance, which translates into better downstream RL fine-tuning and robustness to noisy data; high-fidelity (sharper predictions conditioned on action) aligns with stronger task performance in RL benchmarks and persuasion tasks, indicating task-relevant fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Decomposed architecture (world model + inverse dynamics + policy) increases modularity and controllability and yields robustness to dirty data, but adds extra components and parameters versus a single auto-regressive model; action-space sizing is a trade-off — singleton action degenerates to standard autoregressive modeling, while token-space actions make decision-making intractable; a moderate discrete action space (paper uses N=64) is a compromise between expressivity and decision burden.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Discrete latent action space implemented via VQ-VAE codebook (N=64, code dim=16) to keep action space compact and semantically meaningful; context encoder and dynamics split across transformer blocks (first 11 context, next 11 dynamics) to separate representation and transition prediction; action embeddings up-projected to transformer embedding size (16 -> 2048); policy modeled as a transformer categorical policy trained by behavior cloning and optionally RL; during dirty-data experiments the inverse model was fixed while world model updated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly to an equal-size auto-regressive baseline (Tinyllama): similar or slightly better pretraining benchmark scores, substantially lower predictive entropy, better robustness to noisy training data (BWArea improved on some metrics when 1B dirty tokens added while Tinyllama degraded), and markedly better performance on several RL-style tasks (TextWorld and BBH fine-tuned with ReMax).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper advocates a moderate discrete action space (not singleton, not token-space) and demonstrates one effective configuration: action codebook size N=64, code dimension 16, transformer embedding 2048, and substantial pretraining data (30B tokens) — these choices balance fidelity, interpretability of actions, computational cost, and downstream controllability; fixing inverse model when fine-tuning on noisy data increases robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1207.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1207.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BWArea inverse dynamics (VQ-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BWArea inverse dynamics model implemented with VQ-VAE codebook</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inverse dynamics component that maps (x_{1:t+1}) to a discrete latent action via a VQ-VAE codebook, creating a compact, discrete action space (N=64) that is used both as a target for behavior cloning of the policy and as conditioning for the world model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VQ-VAE inverse dynamics (BWArea)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based context encoder produces embeddings e_t from x_{1:t+1}; embeddings are projected (linear) and quantized to the nearest codebook vector c_i in a learnable finite codebook C (N=64, code dim=16) using L2 distance; commitment and codebook losses used (standard VQ-VAE losses) jointly optimized with world model via decoder prediction loss.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>discrete latent encoder (VQ-VAE) used as inverse dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>natural language comprehension / latent action extraction for controllable language generation and RL policy learning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>VQ-VAE encoder + codebook loss components (commitment loss L2), and how well actions help next-token prediction measured via next-token prediction loss L_predict and downstream metrics (entropy, cross-entropy over actions)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit numeric VQ-VAE reconstruction MSE reported, but joint training showed that actions inferred by inverse model reduce world-model entropy (world model entropy 0.32 when conditioned on actions); marginal vs expected cross-entropy (6.2 vs 3.3) suggests actions meaningfully structure token distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Discrete codes are interpretable at behavioral level because sampling different codes yields distinct coherent text; codes are semantically meaningful in that they guide generation into different modes, but there is no direct mapping provided from code indices to high-level human concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Sampling actions and inspecting generated texts; measuring influence via entropy and cross-entropy differences; qualitative example generations for a fixed prompt with different actions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Inverse dynamics model: 11-block transformer ~0.5B parameters. VQ codebook size N=64 with code vectors dimension 16. Additional compute required for joint training with world model; encoder compression linear layer to 16 dims introduced to compute distances.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Design reduces decision complexity relative to token-level action spaces (far smaller discrete action space) and is more efficient for policy learning than attempting to have policy operate in full token space; compared to singleton-action autoregressive models, it adds overhead but yields improved controllability and downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables behavior cloning of policy during pretraining (policy trained to imitate inverse model actions) and supports RL fine-tuning improvements; empirically contributed to BWArea's performance across TextWorld and BBH tasks and robustness to noisy data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The discrete inverse dynamics provide a compact action representation that both reduces predictive variance of the world model (sharper token distributions) and produces a usable action space for policy optimization, trading extra encoding complexity for improved downstream controllability and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Using a small discrete codebook (N=64) trades off expressivity for tractability and interpretability; larger action spaces (e.g., token-level) would be expressive but intractable for decision-making, while singleton action degenerates to standard autoregressive behavior; choice of code dimension and compression affects fidelity vs compute.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VQ-VAE with N=64 codes, code dim=16; encoder embedding dim 2048 compressed to 16 for nearest-neighbor lookup; L2 distance for quantization; standard VQ commitment and codebook losses with lambda_c=25; joint training with world model using a balancing coefficient beta=0.25 between decoder L_predict and encoder L_VQ.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasts with token-space actions (intractable) and singleton-action autoregressive models (no action signal). Compared implicitly to prior latent-action extraction works in RL: provides a discrete, compact representation tailored to language generation that supports behavior cloning and RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper uses (and recommends empirically) codebook size N=64 and code dim 16 with encoder compression from 2048 to 16 and joint training with world model; this configuration was effective in experiments balancing control, interpretability, and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1207.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1207.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World models (RL literature)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World models and learned environment/dynamics models in reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of neural models that learn environment dynamics (latent or pixel) to enable planning, policy learning, or sample-efficient RL (e.g., VAEs + RNNs, latent dynamics models like Dreamer, MuZero's learned predictors).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World models (general RL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Typically combine an encoder (often VAE) to compress observations into latent states and a dynamics model (RNN/latent transformer) to predict future latent states and rewards; used for imagination-based planning, policy learning, or sample efficiency improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / model-based RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>control and RL domains (Atari, video-based control, games), here referenced for analogy to language world modeling</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>prediction error in latent space, reconstruction loss for VAE, multi-step prediction accuracy, reward/value prediction (as in MuZero), and downstream policy performance</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Varies by instantiation — latent spaces may be structured but generally are neural black boxes; some works interpret latent factors, but no uniform interpretability guarantee.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Common methods include visualization of latent trajectories, disentanglement analyses, and inspection of predicted rollouts; the paper references these works conceptually but does not apply their interpretability techniques directly.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Varies by algorithm; general observation in literature: world models enable sample efficiency but add training/inference overhead for learning and querying dynamics models (no specific numbers provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>World-model-based methods typically improve sample efficiency relative to model-free RL but add model learning and planning compute; paper cites Dreamer, Dreamer-V2, MuZero as representative prior works.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper cites these works to motivate learning a dynamics model for efficiency and robustness — suggesting that a learned language dynamics (world) model can yield sample-efficient downstream policy learning and robustness to noisy data, analogous to benefits in control domains.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>General tradeoff: model-based gains in sample efficiency and planning come with added complexity and potential model-bias; the paper uses this literature to justify decomposing language generation into dynamics + policy.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Paper borrows conceptual design choice: learn a dynamics/world model (here for language) and a policy operating in a latent action space; it chooses discrete latent codes (VQ-VAE) for tractability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to model-free approaches: model-based methods can be more sample-efficient and robust to noisy data, motivating BWArea's design; specifics are from cited literature rather than newly measured in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1207.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1207.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer family (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (latent imagination-based RL methods) and Dreamer-V2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Latent dynamics world-model approaches that learn compact latent representations and use them to imagine multi-step trajectories for policy optimization and planning, improving sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer / Dreamer-V2 (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learn a compact latent dynamics model (usually VAE + recurrent/latent dynamics) enabling multi-step rollout (imagination) to train policies entirely in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / model-based RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>continuous control, Atari-like domains in RL literature (referenced for analogy)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>latent multi-step prediction accuracy, downstream policy return</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily neural latent models; interpretability limited although latent rollouts can be visualized.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of imagined trajectories and latent states in original works (not applied here).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Varies; referenced for conceptual guidance rather than measured here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Known in literature to improve sample efficiency versus model-free baselines; paper references as conceptual prior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Used as conceptual inspiration that learning a dynamics model (here, of language) can yield benefits for policy learning and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1207.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1207.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero (learning dynamics + planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm that learns a dynamics model tuned to predicting rewards, values and policies for planning (tree search) without access to environment rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a dynamics model that outputs latent state transitions, policy logits, and value/reward predictions used inside a planning/search process (MCTS) to select actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid learned model + planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>board games and Atari (original literature); cited as related to learned dynamics for planning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>value/policy/reward prediction accuracy and game-playing score from planning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model outputs (policy/value/reward) are interpretable in terms of planning signals; underlying latent dynamics are still neural and not fully transparent.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Uses predicted values/policies as interpretable planning signals; tangential to current paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High (MCTS + learned model) in original works; paper only references conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Demonstrates planning with learned dynamics can outperform model-free approaches in games; referenced as background motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper references MuZero to motivate the utility of learned dynamics models for decision-making and planning, analogous to BWArea's separation of world model and policy in language.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mastering atari with discrete world models <em>(Rating: 2)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>When to trust your model: Model-based policy optimization <em>(Rating: 1)</em></li>
                <li>Learning behaviors by latent imagination <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1207",
    "paper_id": "paper-270063025",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "BWArea world model",
            "name_full": "BWArea language world model (p_world)",
            "brief_description": "A transformer-based conditional next-token predictive model that conditions on both token context and a discrete latent action sequence to reduce predictive variance and enable controllable language generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BWArea language world model (p_world)",
            "model_description": "Transformer-based sequence model decomposed into a context encoder (first 11 transformer blocks), an action encoder (linear layer producing action embeddings), an aggregate module (linear combining context and action embeddings), and a dynamics module (remaining 11 transformer blocks) that outputs categorical logits for next-token prediction conditioned on context x_{1:t} and latent actions a_{1:t}. The model explicitly takes a discrete latent action (from a VQ-VAE codebook) as an extra conditioning input to the autoregressive token prediction.",
            "model_type": "latent world model (transformer-based predictive model conditioned on discrete latent actions)",
            "task_domain": "natural language generation and language-based RL tasks (e.g., instruction following, TextWorld, BigBench Hard, persuasion of another LLM)",
            "fidelity_metric": "entropy of next-token predictive distribution; token-level cross-entropy / perplexity on evaluation set; downstream benchmark accuracies (MMLU, DROP, BBH, TruthfulQA); marginal vs expected cross-entropy over actions",
            "fidelity_performance": "Entropy of BWArea world model = 0.32 versus Tinyllama = 2.11 (evaluation set); perplexity for 30B-token pretrained BWArea = 27.93 (reported in data-scaling experiment); evaluation accuracy (unspecified eval set) improves with data scale to 48.5% at 30B tokens; benchmark numbers (30B): MMLU 25.85, DROP 8.27, BBH 27.68, TruthfulQA (MC1) 23.13, (MC2) 42.58.",
            "interpretability_assessment": "Moderately interpretable: discrete latent actions (VQ-VAE codes, N=64) are shown to have semantic structure and controllability — sampling different actions given the same prompt produces distinct coherent generations; actions reduce predictive variance and can be inspected by sampling and observing downstream text outputs. Not a fully transparent symbolic model, but latent actions are discrete and human-inspectable via generated text.",
            "interpretability_method": "Qualitative generation examples from sampled actions; measurement of entropy reduction; marginal vs expected cross-entropy comparison (marginal loss 6.2 vs expected loss 3.3) showing actions substantially influence outputs; action controllability experiments (sampled actions produce distinct sentences).",
            "computational_cost": "World model parameters ~1.1B; entire BWArea system: world model 1.1B, inverse dynamics 0.5B, policy 1.1B. Pretraining conducted on 8 A800-80G GPUs. Training schedule: pretrain step1 6 days, pretrain step2 4 days; batch size 512, sequence length T=2048, 30k gradient steps per step. SFT and RL steps have smaller runtimes (SFT step durations: ~4h each; RL 9h BBH, 4h TextWorld reported).",
            "efficiency_comparison": "Compared to an equal-sized auto-regressive baseline (Tinyllama, 1.1B), BWArea matches or slightly exceeds pretraining benchmark performance while offering much reduced predictive entropy and greater robustness to noisy (dirty) data; requires additional model components (inverse dynamics and policy) so overall parameter and training overhead is higher than a single auto-regressive model but experiments used comparable total parameterization by construction.",
            "task_performance": "On TextWorld tasks BWArea converged to optimal returns (Custom:5, Tw-Treasure Hunter:3, Dragon:3) whereas Tinyllama failed on some (Tinyllama final performance 0 on Custom and Dragon). On selected BigBench Hard tasks (7 tasks), BWArea outperformed Tinyllama on 6/7 tasks (example accuracies reported: Reasoning about Colored Objects 25.0% vs 4.3%; Object Tracking 80.0% vs 0.0%; overall charts show BWArea slightly above GPT-3.5 zero-shot but below GPT-4 when averaged). BWArea also successfully persuaded Llama3-8B in a persuasion experiment where Tinyllama failed.",
            "task_utility_analysis": "The inclusion of discrete latent actions improves controllability (policy can select actions to steer generation) and reduces predictive variance, which translates into better downstream RL fine-tuning and robustness to noisy data; high-fidelity (sharper predictions conditioned on action) aligns with stronger task performance in RL benchmarks and persuasion tasks, indicating task-relevant fidelity.",
            "tradeoffs_observed": "Decomposed architecture (world model + inverse dynamics + policy) increases modularity and controllability and yields robustness to dirty data, but adds extra components and parameters versus a single auto-regressive model; action-space sizing is a trade-off — singleton action degenerates to standard autoregressive modeling, while token-space actions make decision-making intractable; a moderate discrete action space (paper uses N=64) is a compromise between expressivity and decision burden.",
            "design_choices": "Discrete latent action space implemented via VQ-VAE codebook (N=64, code dim=16) to keep action space compact and semantically meaningful; context encoder and dynamics split across transformer blocks (first 11 context, next 11 dynamics) to separate representation and transition prediction; action embeddings up-projected to transformer embedding size (16 -&gt; 2048); policy modeled as a transformer categorical policy trained by behavior cloning and optionally RL; during dirty-data experiments the inverse model was fixed while world model updated.",
            "comparison_to_alternatives": "Compared directly to an equal-size auto-regressive baseline (Tinyllama): similar or slightly better pretraining benchmark scores, substantially lower predictive entropy, better robustness to noisy training data (BWArea improved on some metrics when 1B dirty tokens added while Tinyllama degraded), and markedly better performance on several RL-style tasks (TextWorld and BBH fine-tuned with ReMax).",
            "optimal_configuration": "Paper advocates a moderate discrete action space (not singleton, not token-space) and demonstrates one effective configuration: action codebook size N=64, code dimension 16, transformer embedding 2048, and substantial pretraining data (30B tokens) — these choices balance fidelity, interpretability of actions, computational cost, and downstream controllability; fixing inverse model when fine-tuning on noisy data increases robustness.",
            "uuid": "e1207.0",
            "source_info": {
                "paper_title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "BWArea inverse dynamics (VQ-VAE)",
            "name_full": "BWArea inverse dynamics model implemented with VQ-VAE codebook",
            "brief_description": "An inverse dynamics component that maps (x_{1:t+1}) to a discrete latent action via a VQ-VAE codebook, creating a compact, discrete action space (N=64) that is used both as a target for behavior cloning of the policy and as conditioning for the world model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VQ-VAE inverse dynamics (BWArea)",
            "model_description": "Transformer-based context encoder produces embeddings e_t from x_{1:t+1}; embeddings are projected (linear) and quantized to the nearest codebook vector c_i in a learnable finite codebook C (N=64, code dim=16) using L2 distance; commitment and codebook losses used (standard VQ-VAE losses) jointly optimized with world model via decoder prediction loss.",
            "model_type": "discrete latent encoder (VQ-VAE) used as inverse dynamics",
            "task_domain": "natural language comprehension / latent action extraction for controllable language generation and RL policy learning",
            "fidelity_metric": "VQ-VAE encoder + codebook loss components (commitment loss L2), and how well actions help next-token prediction measured via next-token prediction loss L_predict and downstream metrics (entropy, cross-entropy over actions)",
            "fidelity_performance": "No explicit numeric VQ-VAE reconstruction MSE reported, but joint training showed that actions inferred by inverse model reduce world-model entropy (world model entropy 0.32 when conditioned on actions); marginal vs expected cross-entropy (6.2 vs 3.3) suggests actions meaningfully structure token distributions.",
            "interpretability_assessment": "Discrete codes are interpretable at behavioral level because sampling different codes yields distinct coherent text; codes are semantically meaningful in that they guide generation into different modes, but there is no direct mapping provided from code indices to high-level human concepts.",
            "interpretability_method": "Sampling actions and inspecting generated texts; measuring influence via entropy and cross-entropy differences; qualitative example generations for a fixed prompt with different actions.",
            "computational_cost": "Inverse dynamics model: 11-block transformer ~0.5B parameters. VQ codebook size N=64 with code vectors dimension 16. Additional compute required for joint training with world model; encoder compression linear layer to 16 dims introduced to compute distances.",
            "efficiency_comparison": "Design reduces decision complexity relative to token-level action spaces (far smaller discrete action space) and is more efficient for policy learning than attempting to have policy operate in full token space; compared to singleton-action autoregressive models, it adds overhead but yields improved controllability and downstream performance.",
            "task_performance": "Enables behavior cloning of policy during pretraining (policy trained to imitate inverse model actions) and supports RL fine-tuning improvements; empirically contributed to BWArea's performance across TextWorld and BBH tasks and robustness to noisy data.",
            "task_utility_analysis": "The discrete inverse dynamics provide a compact action representation that both reduces predictive variance of the world model (sharper token distributions) and produces a usable action space for policy optimization, trading extra encoding complexity for improved downstream controllability and robustness.",
            "tradeoffs_observed": "Using a small discrete codebook (N=64) trades off expressivity for tractability and interpretability; larger action spaces (e.g., token-level) would be expressive but intractable for decision-making, while singleton action degenerates to standard autoregressive behavior; choice of code dimension and compression affects fidelity vs compute.",
            "design_choices": "VQ-VAE with N=64 codes, code dim=16; encoder embedding dim 2048 compressed to 16 for nearest-neighbor lookup; L2 distance for quantization; standard VQ commitment and codebook losses with lambda_c=25; joint training with world model using a balancing coefficient beta=0.25 between decoder L_predict and encoder L_VQ.",
            "comparison_to_alternatives": "Contrasts with token-space actions (intractable) and singleton-action autoregressive models (no action signal). Compared implicitly to prior latent-action extraction works in RL: provides a discrete, compact representation tailored to language generation that supports behavior cloning and RL fine-tuning.",
            "optimal_configuration": "Paper uses (and recommends empirically) codebook size N=64 and code dim 16 with encoder compression from 2048 to 16 and joint training with world model; this configuration was effective in experiments balancing control, interpretability, and compute.",
            "uuid": "e1207.1",
            "source_info": {
                "paper_title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "World models (RL literature)",
            "name_full": "World models and learned environment/dynamics models in reinforcement learning",
            "brief_description": "General class of neural models that learn environment dynamics (latent or pixel) to enable planning, policy learning, or sample-efficient RL (e.g., VAEs + RNNs, latent dynamics models like Dreamer, MuZero's learned predictors).",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World models (general RL)",
            "model_description": "Typically combine an encoder (often VAE) to compress observations into latent states and a dynamics model (RNN/latent transformer) to predict future latent states and rewards; used for imagination-based planning, policy learning, or sample efficiency improvements.",
            "model_type": "latent world model / model-based RL",
            "task_domain": "control and RL domains (Atari, video-based control, games), here referenced for analogy to language world modeling",
            "fidelity_metric": "prediction error in latent space, reconstruction loss for VAE, multi-step prediction accuracy, reward/value prediction (as in MuZero), and downstream policy performance",
            "fidelity_performance": null,
            "interpretability_assessment": "Varies by instantiation — latent spaces may be structured but generally are neural black boxes; some works interpret latent factors, but no uniform interpretability guarantee.",
            "interpretability_method": "Common methods include visualization of latent trajectories, disentanglement analyses, and inspection of predicted rollouts; the paper references these works conceptually but does not apply their interpretability techniques directly.",
            "computational_cost": "Varies by algorithm; general observation in literature: world models enable sample efficiency but add training/inference overhead for learning and querying dynamics models (no specific numbers provided in this paper).",
            "efficiency_comparison": "World-model-based methods typically improve sample efficiency relative to model-free RL but add model learning and planning compute; paper cites Dreamer, Dreamer-V2, MuZero as representative prior works.",
            "task_performance": null,
            "task_utility_analysis": "Paper cites these works to motivate learning a dynamics model for efficiency and robustness — suggesting that a learned language dynamics (world) model can yield sample-efficient downstream policy learning and robustness to noisy data, analogous to benefits in control domains.",
            "tradeoffs_observed": "General tradeoff: model-based gains in sample efficiency and planning come with added complexity and potential model-bias; the paper uses this literature to justify decomposing language generation into dynamics + policy.",
            "design_choices": "Paper borrows conceptual design choice: learn a dynamics/world model (here for language) and a policy operating in a latent action space; it chooses discrete latent codes (VQ-VAE) for tractability.",
            "comparison_to_alternatives": "Compared conceptually to model-free approaches: model-based methods can be more sample-efficient and robust to noisy data, motivating BWArea's design; specifics are from cited literature rather than newly measured in this paper.",
            "optimal_configuration": null,
            "uuid": "e1207.2",
            "source_info": {
                "paper_title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Dreamer family (referenced)",
            "name_full": "Dreamer (latent imagination-based RL methods) and Dreamer-V2",
            "brief_description": "Latent dynamics world-model approaches that learn compact latent representations and use them to imagine multi-step trajectories for policy optimization and planning, improving sample efficiency.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "mention",
            "model_name": "Dreamer / Dreamer-V2 (referenced)",
            "model_description": "Learn a compact latent dynamics model (usually VAE + recurrent/latent dynamics) enabling multi-step rollout (imagination) to train policies entirely in latent space.",
            "model_type": "latent world model / model-based RL",
            "task_domain": "continuous control, Atari-like domains in RL literature (referenced for analogy)",
            "fidelity_metric": "latent multi-step prediction accuracy, downstream policy return",
            "fidelity_performance": null,
            "interpretability_assessment": "Primarily neural latent models; interpretability limited although latent rollouts can be visualized.",
            "interpretability_method": "Visualization of imagined trajectories and latent states in original works (not applied here).",
            "computational_cost": "Varies; referenced for conceptual guidance rather than measured here.",
            "efficiency_comparison": "Known in literature to improve sample efficiency versus model-free baselines; paper references as conceptual prior.",
            "task_performance": null,
            "task_utility_analysis": "Used as conceptual inspiration that learning a dynamics model (here, of language) can yield benefits for policy learning and robustness.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1207.3",
            "source_info": {
                "paper_title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MuZero (referenced)",
            "name_full": "MuZero (learning dynamics + planning)",
            "brief_description": "An algorithm that learns a dynamics model tuned to predicting rewards, values and policies for planning (tree search) without access to environment rules.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "mention",
            "model_name": "MuZero (referenced)",
            "model_description": "Learns a dynamics model that outputs latent state transitions, policy logits, and value/reward predictions used inside a planning/search process (MCTS) to select actions.",
            "model_type": "hybrid learned model + planning",
            "task_domain": "board games and Atari (original literature); cited as related to learned dynamics for planning",
            "fidelity_metric": "value/policy/reward prediction accuracy and game-playing score from planning",
            "fidelity_performance": null,
            "interpretability_assessment": "Model outputs (policy/value/reward) are interpretable in terms of planning signals; underlying latent dynamics are still neural and not fully transparent.",
            "interpretability_method": "Uses predicted values/policies as interpretable planning signals; tangential to current paper's experiments.",
            "computational_cost": "High (MCTS + learned model) in original works; paper only references conceptually.",
            "efficiency_comparison": "Demonstrates planning with learned dynamics can outperform model-free approaches in games; referenced as background motivation.",
            "task_performance": null,
            "task_utility_analysis": "Paper references MuZero to motivate the utility of learned dynamics models for decision-making and planning, analogous to BWArea's separation of world model and policy in language.",
            "tradeoffs_observed": null,
            "design_choices": null,
            "comparison_to_alternatives": null,
            "optimal_configuration": null,
            "uuid": "e1207.4",
            "source_info": {
                "paper_title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "World models",
            "rating": 2,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 2,
            "sanitized_title": "mastering_atari_with_discrete_world_models"
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        },
        {
            "paper_title": "When to trust your model: Model-based policy optimization",
            "rating": 1,
            "sanitized_title": "when_to_trust_your_model_modelbased_policy_optimization"
        },
        {
            "paper_title": "Learning behaviors by latent imagination",
            "rating": 1,
            "sanitized_title": "learning_behaviors_by_latent_imagination"
        }
    ],
    "cost": 0.0166325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation
27 May 2024</p>
<p>Chengxing Jia 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>School of Artificial Intelligence
Nanjing University</p>
<p>Pengyuan Wang 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>School of Artificial Intelligence
Nanjing University</p>
<p>Ziniu Li 
School of Data Science
The Chinese University of Hong Kong
Shenzhen</p>
<p>Shenzhen Research Institute of Big Data</p>
<p>Yi-Chen Li 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>School of Artificial Intelligence
Nanjing University</p>
<p>Zhilong Zhang 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>School of Artificial Intelligence
Nanjing University</p>
<p>Nan Tang 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>School of Artificial Intelligence
Nanjing University</p>
<p>Yang Yu 
National Key Laboratory for Novel Software Technology
Nanjing University</p>
<p>School of Artificial Intelligence
Nanjing University</p>
<p>BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation
27 May 20247499542795F92B5A60204BD8B5B30BECarXiv:2405.17039v1[cs.CL]
Large language models (LLMs) have catalyzed a paradigm shift in natural language processing, yet their limited controllability poses a significant challenge for downstream applications.We aim to address this by drawing inspiration from the neural mechanisms of the human brain, specifically Broca's and Wernicke's areas, which are crucial for language generation and comprehension, respectively.In particular, Broca's area receives cognitive decision signals from Wernicke's area, treating the language generation as an intricate decision-making process, which differs from the fully auto-regressive language generation of existing LLMs.In a similar vein, our proposed system, the BWArea model, conceptualizes language generation as a decision-making task.This model has three components: a language world model, an inverse dynamics model, and a cognitive policy.Like Wernicke's area, the inverse dynamics model is designed to deduce the underlying cognitive intentions, or latent actions, behind each token.The BWArea model is amenable to both pre-training and fine-tuning like existing LLMs.With 30B clean pre-training tokens, we have trained a BWArea model, which achieves competitive performance with LLMs of equal size (1B parameters).Unlike fully auto-regressive LLMs, its pre-training performance does not degenerate if dirty data unintentionally appears.This shows the advantage of a decomposed structure of BWArea model in reducing efforts in laborious data selection and labeling.Finally, we reveal that the BWArea model offers enhanced controllability via fine-tuning the cognitive policy with downstream reward metrics, thereby facilitating alignment with greater simplicity.On 9 out of 10 tasks from two suites, TextWorld and BigBench Hard, our method shows superior performance to auto-regressive LLMs.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have become the cornerstone of modern natural language processing, providing the foundation for powerful approaches to understanding and generating human language.They have revolutionized numerous applications such as machine translation and question-answering [1,2].However, despite their widespread success and utility, the issue of controllability is one of the most pressing challenges in this field [3,4].Current LLMs generate language in an auto-regressive manner, which assumes the next token depends solely on the previous tokens, thus the generation process is hard to interfere with.Even if alignment and prompt optimization methods were developed, they still lack the nuanced control that is desirable for many applications, such as those requiring precise alignment with human values or adherence to specific content guidelines [5].In this paper, we aim to tackle the challenge of controllability.Our approach draws inspiration from the brain's language processing centers: the Broca's and Wernicke's areas.These regions are integral to language generation and comprehension [6].Broca's area is responsible for language generation.When we write, it helps form words and construct sentences.Unlike the auto-regressive nature of existing LLMs, Broca's area engages in language generation under the governance of cognitive decisions (e.g.decisions from the frontal lobe [7] ), suggesting that language generation is deeply rooted in decision-making [8].On the other hand, Wernicke's area is responsible for language comprehension.When we read text, Wernicke's area helps us comprehend the meaning.Their functionality offers a blueprint for a more controllable language generation system and has profound implications for the design of our proposed model, the BWArea model; see Figure 1.</p>
<p>Our BWArea model reimagines language generation as a decision-making task.It is an integration of three interrelated components: a language world model, an inverse dynamics model, and a cognitive policy.The language world model generates tokens driven by cognitive decisions, the inverse dynamics model infers the latent cognitive decisions that propel each token's generation.Using the inverse dynamics model, we transform language sequences into decision trajectories by filling in the latent cognitive decisions.This conversion enables the application of behavior cloning to train the language world model and cognitive policy.By treating language as a series of decisions, we facilitate a learning process that mirrors the acquisition of language in the human brain.</p>
<p>The BWArea model can undergo both pre-training and fine-tuning, similar to existing LLMs.We have trained a BWArea model from scratch.The world model and policy model have 1B parameters, while the inverse model is lightweight with 0.5B parameters.An auto-regressive LLM with equal size (1B parameters) was also trained for fair comparison.With 30B clean pre-training tokens from the Slimpajama and StarCoder datasets, our model shows competitive evaluation performance with the auto-regressive LLM on common benchmarks such as MMLU, DROP, BBH, and TruthfulQA.Nevertheless, their performance differs when dealing with dirty data where labels are noisy.In particular, with an additional 1B dirty tokens, the evaluation performance of our method does not degenerate and even improves by 3%.In contrast, the performance of the auto-regressive LLM is reduced by 1.3%.This shows that our method, due to its decomposed structure, has advantages in reducing the efforts of laborious data cleaning and labeling.</p>
<p>Furthermore, we also verify that the BWArea model has superior controllability in downstream applications.Technically, the cognitive policy selects a latent action while the world model predicts tokens conditioned on it, thus reducing the predictive variance for token generation.This allows for flexible adjustment of the cognitive policy via reward maximization using reinforcement learning methods.Empirically, we have found that the BWArea Model outperforms existing LLMs on several types of RL tasks, including three tasks from the TextWorld suite [9], and 6 out of 7 tasks from the BigBench Hard suite [10].Collectively, these results show promising performance in treating language processing as a decision-making task, and we hope this can shed light on the development of controllable language generation systems.</p>
<p>Related Work</p>
<p>Large Language Models</p>
<p>The architecture of current Large Language Models (LLMs) has seen significant evolution since the introduction of the Transformer model [11].The Transformer's self-attention mechanism allows for the parallel processing of sequences, which is a key factor contributing to the scaling capabilities of LLMs.One of the most influential architectures in this domain is BERT [1], which utilizes a bidirectional Transformer [12,13] to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.Following BERT, several models like GPT [14], and GPT-2 [15] extended the use of Transformers in language modeling tasks by focusing on the generative pre-training and fine-tuning paradigm.Brown et al. [2] further scaled this approach with GPT-3, a model with 175 billion parameters, demonstrating that larger models can exhibit few-shot learning capabilities, where the model can perform tasks given very few examples.Recently, several other large language models have also embraced such a learning structure [16,17,18].</p>
<p>However, these methods have primarily focused on predicting the next token in a corpus, which can limit their ability to decide what to generate and may lead to what is commonly known as model illusion [19].To improve the controllability and efficiency of LLMs, prompt-based methods [20,21,22,23] have emerged as a pivotal technique.This method leverages the pre-trained knowledge encoded in the LLMs and has been shown to be effective even with few-shot examples.Prompts can be manually designed or automatically generated, and recent works have explored methods for optimizing prompts to better guide model behavior [24].[25] also involves a flow-based [26] method to model the prompt for the language generation.Alongside architectural advancements, there is a growing focus on aligning LLMs with human values.Reinforcement Learning from Human Feedback (RLHF) has been proposed as a method to fine-tune models based on human preferences [27].Another approach is Deep Preference Optimization (DPO), which iteratively refines model outputs to align with human values and preferences [28].These methods aim to ensure that LLMs act in ways that are beneficial and non-harmful, according to human judgment.</p>
<p>World Models</p>
<p>World models in reinforcement learning (RL) [29], also called environment models and action models, are neural networks that simulate the environment with which an agent interacts.These models are then used to simulate trajectories, enabling the agent to plan and learn policies without interacting with the actual environment, thus saving valuable samples [30,31,32].[33] introduced the concept of world models by combining variational autoencoders (VAEs) [34] for learning a compressed spatial representation, with recurrent neural networks (RNNs) for temporal predictions.The use of world models has been extended by [35] with the Dreamer algorithm, which learns a latent dynamics model that can efficiently predict ahead for multiple time-steps in a compact latent space.Dreamer-V2 [36] further improved the sample efficiency and scalability of this approach.The MuZero algorithm [37], which combines tree-based search with a learned dynamics model [38,39], has achieved state-of-the-art performance in various domains by learning a model that directly predicts the value, policy, and reward from the current state and action.In real-world scenarios where the collection of actions is lacking, there are additional works [40,41] that extract latent actions from the dataset to control the state.</p>
<p>BWArea Model</p>
<p>Model Design</p>
<p>We are motivated by the working mechanism of the Broca and Wernicke areas in the human brain.We build an inverse dynamics model and a world model that mimic the language understanding process of the Wernicke area (consider when we are listening to or reading some text).The inverse dynamics model infers a latent signal, referred to as action in the terminology of RL, that helps predict the next token generation.The main feature is that this signal has certain semantic meanings, which eases token generation (i.e., by reducing the predictive covariance).Later on, we introduce a cognitive policy that operates with the latent action found by the inverse dynamics model to mimic the language generation process of the Broca area.At the inference stage (consider when we are about to speak), the cognitive policy selects a latent action and asks the world model to decode it into discrete tokens.</p>
<p>The above three models are implemented with the Transformer architecture [11] and go through the pre-training and fine-tuning stages similar to classical auto-regressive LLMs, as explained below.</p>
<p>Language World Model</p>
<p>The language world model p world aims to predict the next token based on the context and a latent action, with the latter being the main difference from auto-regressive LLMs.Let x 1:t be a sequence of tokens (x 1 , . . ., x t ) and x t+1 be the next token in a sentence.In our formulation, the next token prediction is based on x t+1 ∼ p world (x 1:t , a 1:t ); in RL terminology, it serves as the transition model.By this additional condition, the predictive variance of the next token can be reduced.We will discuss how to identify this latent action later, but for now, let us explain the details of this world model.</p>
<p>• Context encoder f context world , a transformer-based structure which inputs the context x 1:t and outputs their embeddings at each timestep e s 1:t ; • Action encoder f action world , a linear layer which inputs the action sequence a 1:t and outputs their embeddings at each timestep e a 1:t ; • Aggregate module f agg world , a linear layer which inputs the embeddings of context e s t and action e a t at timestep t to input the joint embeddings e t ;</p>
<p>• Dynamics module f dyna world , a transformer-based structure which inputs the sequence of embeddings e 1:t and outputs the next-token prediction, which is the categorical logits of token x 2:t+1 .Therefore, the inference can be expressed by
f dyna world • f agg world ([ f context world (x 1:t ), f action world (a 1:t )]
), and we also give an illustration of the inference process in Figure 2(c).</p>
<p>Inverse Dynamics Model</p>
<p>We introduce an inverse dynamics model, f inverse , to bridge the language understanding and generation.For understanding, the model is trained to infer a latent action a t from the observed data.For generation, it constructs the action space from which the cognitive policy can select actions.We hypothesize that the latent action a t aids in predicting the subsequent tokens that can be retrospectively inferred (i.e., hindsight).In our work, we investigate a specific inverse dynamics model that leverages the next token x t+1 as well as the contextual history x 1:t to infer the latent action, i.e., a t = f inverse (x 1:t , x t+1 ).Exploration of other scenarios, such as utilizing future k tokens x t+1:t+k to determine the latent action, is left for future research.</p>
<p>We highlight that it is important to learn a moderate action space.The classical auto-regressive LLMs in our formulation correspond to the case where the action space is a singleton (with a dummy action), with these models predicting the next token solely on the context.On the other extreme, if the action space corresponds directly to the token space, selecting an action would be equivalent to selecting a token, resulting in an overwhelming decision-making burden.This is not our desired approach.</p>
<p>To address the above challenge, we implement a vector-quantized varational-auto-encoder (VQ-VAE) [42], which learns discrete feature representations from data.In our setting, this VQ-VAE encoder has the following modules:</p>
<p>• Context encoder f context inverse , a transformer-based structure which inputs the context x 1:t+1 and outputs their embeddings at each timestep e 1:t ;</p>
<p>• Action codebook f code inverse , a finite codebook C = {c i } N i=1 with N codes1 .A code c i corresponds to a latent action in our design;</p>
<p>The VQ-VAE model takes the x 1:t+1 as inputs, and outputs an embedding e t by f c policy (x 1:t+1 ).Then mapped to an action by a t = f code inverse (e t ).In this way, we effectively restrict the action space to be compact.Note that this codebook is learnable.We also give an illustration in Figure 2(a).</p>
<p>Cognitive Policy Model</p>
<p>In the above part, we illustrate the inverse dynamics model builds a latent action space and the world model predicts the next token x t+1 given on the context x 1:t and action a t .Now, we implement a cognitive (decision) policy model π policy to select latent action a t for manipulating the token generation process.Specifically, this policy model takes the context x 1:t as its input and generates an action a t ∈ C from a categorical distribution: a t ∼ π policy (x 1:t ).</p>
<p>We highlight that the goal of the policy model is to perform specific down-stream tasks via reward maximization.It is trained offline with policy gradient methods [43] in our paper but it may be also combined with online optimization methods such as model predictive control [44,45].Compared with auto-regressive LLMs that operate in the raw token space, the policy model introduced here is more flexible in controlling token generation by the world model.</p>
<p>Model Inference</p>
<p>We now introduce the inference process.For a prompt x 1:t , the next-token x t+1 is generated by:
x t+1 ∼ p world (x t+1 |x 1:t , a 1:t ; θ world ), a i = f inverse (x 1:i+1 ; θ inverse ), ∀i = 0, . . . t − 1 a t ∼ π policy (x 1:t ; θ policy ).(1)
That is, we first employ the inverse dynamics model to infer the action a 1:t−1 based on x 1:t , representing a comprehension of the language within the sentence.Subsequently, we execute action a t based on the policy, representing a specific intention.The contexts x 1:t and actions a 1:t are then input into the world language model to generate the next token x t+1 .</p>
<p>Model Training</p>
<p>The designed models go through three training stages.First, we pre-train the models to acquire basic language proficiency.Second, we engage in supervised fine-tuning (SFT) to improve the model's ability to follow instructions.Third, we utilize a reinforcement learning (RL) method to adapt the policy to finish specific tasks.For ease presetation, we use the symbols θ world , θ inverse , and θ policy to denote the training parameters of the world model, inverse dynamics, and policy model, respectively.</p>
<p>Pre-training</p>
<p>In the pre-training stage, we are given a large corpus of texts, which provides rich knowledge for the acquisition of language decision-making.The pre-training dataset D pretrain consists of numerous document segments.Each segment comprises a sequence of tokens as x 1:T .We extensively train the world model, inverse dynamics model, and policy model.The pre-training has two steps:</p>
<p>Step 1: we jointly train the inverse dynamics model and world model:
min θ world ,θ inverse L predict (D pretrain ; θ world , θ inverse ) + βL VQ (D pretrain ; θ inverse )(2)
The objective L predict is to predict the next token:
L predict = −E x 1:T ∼D pretrain T ∑ t=1 log p world (x t+1 |x 1:t , a 1:t , θ world )] .
where a t = f inverse (x 1:t , x t+1 ; θ inverse ).The term L VQ is the standard VQ-VAE encoder loss:
L VQ = E x 1:T ∼D pretrain T ∑ t=1 ∥e t − (a t ) sg ∥ 2 2 + λ c ∥ f code inverse ((e t ) sg ) − (e t ) sg ∥ 2 2
where (•) sg is the gradient stop operator and e t = f context inverse (x 1:t+1 ).The first term in L VQ is to optimize the context encoder and the second term in L VQ is to optimize the codebook.For more detailed information, please refer to Appendix A.2.1.</p>
<p>Step 2: we train the policy model by behavior cloning of actions output by the inverse dynamics:
min θ policy L pretrain policy = −E x 1:T ∼D pretrain T−1 ∑ t=1 log π policy (a t |x 1:t , θ policy )(3)
where action a t = f inverse (x 1:t , x t+1 ; θ inverse ).The learning process is shown in Algorithm 1.</p>
<p>Supervised Fine-Tuning</p>
<p>In the supervised fine-tuning (SFT) stage, we are to enhance the model's ability to follow instructions by utilizing a small corpus of texts, which consists of sentence segments divided into two parts: the instruction (prompt) and the answer.In particular, the prompt and the answer together construct a complete sentence: D sft = {(x 1 , . . ., x p , x p+1 , . . ., x T )}, where p is the sequence length of prompt, and (x 1 , . . ., x p ) denotes a prompt while (x p+1 , . . ., x T ) denotes an answer.</p>
<p>In the SFT stage, we employ a similar learning process as in the pre-training stage.However, akin to common supervised fine-tuning, we only predict the tokens in answers rather than the entire sentences.For inverse dynamics and dynamics learning, we only compute the loss from timestep p + 1 to T. Take policy as an example: L policy (D sft ; θ policy ) = 1</p>
<p>|D sft | ∑ D sft ∑ T t=p+1 log p policy (a t |x 1:t ).To achieve this, we apply masks to the tokens that do not belong to the answer to compute the loss.The learning process is summarized in Algorithm 2.</p>
<p>Algorithm 3 Roll-out</p>
<p>Input: Prompt (x 1 , . . ., x p ) 1: for t = p, . . ., T do 2: Select action a t by the cognitive policy; 3: Sample the next token x t+1 by the world model; 4: end for 5: Return x 1:T</p>
<p>Algorithm 4 Reinforcement Learning</p>
<p>Input: Prompt (x 1 , . . ., x p ) 1: Generate sentence x 1:T by Algorithm 3. 2: Compute the reward by r(x 1:T ). 3: Optimize the policy model π θ policy to maximize r(x 1:T ) by an iteration of an RL algorithm.4: Return policy model π θ policy</p>
<p>Reinforcement Learning</p>
<p>In the RL stage, the dataset is structured in a prompt-only format (x 1 , . . ., x p ).Given the prompts, the model generates answers (x p+1 , . . ., x T ) according to Algorithm 3. Then a task-specific reward, such as the preference reward function in RLHF [46], is utilized to score the sentence (x 1 , . . ., x T ).We refer readers to [47] for reward metrics used in downstream applications.We can apply any RL method, such as PPO [48] or ReMax [43], to optimize the cognitive policy model to maximize performance.We also summarize a single iteration of the RL process in Algorithm 4.</p>
<p>Experiments</p>
<p>In this section, we empirically verify the controllability of our model.Initially, we pre-train our models to demonstrate the controllability of our constructed latent action space.Subsequently, we conduct supervised fine-tuning to enhance its ability to follow instructions.Finally, we design several reinforcement learning tasks in the language domain to demonstrate its controllability over traditional language models.Furthermore, we also show the potential of our models in scalability.</p>
<p>We have designed our model architecture based on the Tinyllama model2 , which is a large language model with 1.1B parameters.To ensure a fair comparison, the language world model has the same number of transformer blocks as Tinyllama, totaling 22 layers, but with an additional action encoder layer and an aggregate linear layer.We consider the initial 11 transformer blocks as the context encoder, and the remaining 11 transformer blocks as the dynamics module.Consequently, the language world model comprises the same 1.1B parameters as Tinyllama.The inverse dynamics model is an 11-block transformer with 0.5B parameters, while the policy model is a 22-block transformer with 1.1B parameters.We show the architecture in Figure 2.</p>
<p>Pre-training and Supervised Fine-Tuning</p>
<p>We conduct pre-training on the Simpajama and Starcoder dataset, which follows the setting from the Tinyllama codebase.Our training involves 30 billion tokens, including training the language world model and inverse dynamics model on 30B tokens and subsequently training the policy on the same 30B tokens.First, we evaluate our model as well as Tinyllama trained with the same 30B tokens on several standard benchmarks, including MMLU [49], DROP [50], BBH [10] and TruthfulQA [51].Results in Table 1 show that our method achieves a comparable performance with the Tinyllama model.We demonstrate other important properties of our model below.On Action Controllability.In order to evaluate the controllability of our constructed action space, our objective is to confirm whether the action can genuinely control distinct distributions.Initially, we randomly sample actions and obtain the next token with the maximum probability to observe if the generated sentences are distinct.Specifically, we use "I like" as a prompt and randomly sample actions from the action space.Our findings indicate that distinct actions can indeed result in disparate generation outcomes: • Example 1: I like to work with a lot of different people.I'm a big fan of the creative process.</p>
<p>• Example 2: I like them, especially with the cats.</p>
<p>• Example 3: I like this car, as well as the newness of the interior and features.</p>
<p>• Example 4: I like having someone look out for them whenever I go out to a movie.</p>
<p>• Example 5: I like my food best) and sometimes love seeing new restaurants ... We find that despite the random sampling of actions, the resulting sentences remain coherent.This suggests that our action space holds a more meaningful structure than a raw token space, showing its efficacy in guiding the token generation.We also provide other results in Appendix A.3.4.</p>
<p>On Reducing Predictive Variance.One benefit of introducing latent actions is to reduce the predictive variance of tokens, which is further reflected in the associated entropy.For two random variables X and Y, we have H[X] − H[X|Y] = I(X; Y) ≥ 0 [52], where H denotes the entropy and I(X; Y) is the mutual information between X and Y.This implies that the token generation distribution in the world model is sharper when conditioned on the action.We empirically verify this: on the evaluation dataset, the entropy of BWArea world model is 0.32 while that of Tinyllama is 2.11, indicating that the action can help reduce the predictive variance.</p>
<p>On Instruction Following.After pre-training, we conduct supervised fine-tuning on the full-hh-rlhf dataset, released by Anthropic [53].This dataset has 112 training samples and we train our model for 1 epoch.To evaluate the instruction-following ability, we assess the models using the GPT-4 win rate by the instructions in [54].The win-rate is 86.5% over its pre-trained version, demonstrating that our method aligns well with instruction tuning.We also show a demo in Appendix A.3.3.</p>
<p>Reinforcement Learning with Rewards</p>
<p>In this section, we employ the ReMax [43] algorithm to fine-tune the cognitive policy model obtained in the phase of instruction tuning on tasks such as TextGame and Bigbench Hard.Results show that compared to Tinyllama, our method demonstrates superior performance.</p>
<p>Results on TextWorld</p>
<p>We evaluate the performance on the TextWorld [9].In TextWorld, each task encompasses N states {s i } N i=1 described using language.Each state s i portrays the current situation and the available actions in language form.When the agent takes the correct action, a reward of +1 is obtained, and the transition to the next state s i+1 occurs.However, if the agent takes an incorrect action, a reward of -1 is obtained, and the agent remains in state s i .When we choose the correct action on s N , we obtain the total return of N and finish the task.We will give a more detailed description in Appendix A.1.1.We select three benchmarks: Custom, Tw-Treasure Hunter, and Dragon.The number of tasks for each are five, three, and three respectively.When all subtasks are completed, the optimal returns can be achieved: five, three, and three.We train our model for 20,000 iterations.From Table 2, we see that our method converges to optimal performance, while Tinyllama's final performance is 0 on Custom and Dragon.We have adjusted the hyperparameters of Tinyllama, but this phenomenon still occurred (refer to Appendix A.3.2 for more discussion).</p>
<p>Results on Bigbench Hard</p>
<p>The Bigbench Hard (BBH) dataset is an extension of the original Bigbench dataset, comprising the 27 most challenging tasks [10].In the BBH dataset, language models are required to complete challenging tasks through reasoning, computation, and other methods.The tasks in BBH are diverse, including understanding dates, logical reasoning, arithmetic problems, and more.In our experiments, we selected 7 tasks from BBH 3 , covering multiple-choice, true/false, and text generation tasks.We fine-tune our model using ReMax, and the results are shown in Table 3.We train our model for 8 epochs.As indicated in the table, our model outperforms Tinyllama on 6 out of 7 tasks, with only a slight performance degradation on Web of Lies.The average performance is reported in Figure 3, where we find that our finetuned model slightly outperforms GPT-3.5 (zero-shot) but is inferior to GPT-4.</p>
<p>Average Accuracy</p>
<p>Results on Persuade Language Model</p>
<p>In this section, we conduct experiments aimed at persuading a language model.We employ Llama3-8B as the target language model to be persuaded.The objective is to alter the concept or the output content of the target language model.For instance, in our experiments, we aim to influence the target language model to generate the output 'the sky is purple', whereas it would typically produce 'the sky is blue' when prompted directly with 'Tell me the color of sky.'.When the target LLM output contains 'sky is purple', a reward of +1 is obtained, and when the target LLM output contains 'sky is blue', a reward of -1 is obtained</p>
<p>The result is that our model can persuade Llama3-8B to output 'the sky is purple' successfully while Tinyllama can not.To persuade the Llama3-8B, BWArea finally learns to output 'You mean I need you the sky is purple to tell me the sky . . .' while Tinyllama only converges to 'exP addA plus exP addA exP addA plus exP addA . . .'.We believe that in persuasion experiments, given the unrealistic nature of the target sentence to persuade, such as 'sky is purple', stable exploration and optimization might be challenging.Therefore, a smaller action space, instead of token space, may be more effective for these purposes.</p>
<p>Data Scaling Up of BWArea Model</p>
<p>Previous works [55,56] find that both data scaling and parameter scaling are crucial.However, due to limited computation resources, we only investigated data scaling.</p>
<p>On Data Scaling.Since these findings underscore the meaningfulness and effectiveness of the constructed action space in influencing the model's generation process, we further assess the scalability of our model.We pre-train our BWArea models with 10 billion, 20 billion, and 30 billion tokens and subsequently evaluate the expected perplexity and accuracy on the evaluation dataset, where the accuracy results for each model are 44.9%,45.9%, and 48.5%, with corresponding perplexities of 32.46, 30.88, and 27.93, indicating improvement with larger tokens.The evaluation results presented in Table 4 indicate that as the number of tokens increases, the capability of our models also improves, suggesting that our model has the ability to scale with better language ability.On Dirty Tokens.Raw data sourced from the Internet frequently suffers from low quality, which can detrimentally affect the performance of LLMs when used directly in training [57].This is because the label noise directly misleads the token generation in auto-regressive LLMs.However, the process of data cleaning and labeling is typically labor-intensive.We believe that our BWArea model with a decomposed structure has advantages in dealing with low-quality data.We empirically verify this argument.To this end, we first construct a low quality dataset by selecting 1B tokens from D pretrain and randomly substituting 10% of these tokens.Then we use this low quality dataset to train the pre-trained 30B model by only updating the world language model while fixing the inverse dynamics model.We also compare the ability with Standard Tinyllama, which has the same depth of transformer block as ours.The results in Table 5 indicate that when utilizing a low-quality dataset, standard language models demonstrate a reduction in predictive accuracy, resulting in relatively poorer performance on benchmarks, while our method can still improve.We believe that the reason our method shows improvement is related to the model-based RL methods [58,59,30,31,32], wherein learning a dynamics model can exhibit greater robustness to low-quality data.Additionally, model-based RL methods often benefit from a mixture of data sources [60].</p>
<p>Conclusion</p>
<p>In this work, we draw inspiration from the human brain's language processing, specifically the Broca and Wernicke areas, to create controllable language models for real-world applications.By simulating human cognitive processes, we develop an inverse dynamics model for language comprehension, a world language model for language generation, and a cognitive policy for decision-making.In contrast to fully auto-regressive LLMs that operate in the raw token space, our model offers greater flexibility in adapting the cognitive policy with reward signals for downstream applications.Additionally, we demonstrate the potential of our method when dealing with low-quality datasets, highlighting its ability to reduce the efforts in data cleaning and labeling.While we have shown the scalability of our structure, there is ongoing work to further scale our model with a larger corpus and more complex structures, which will be a focus for future endeavors.</p>
<p>A Appendix</p>
<p>A.1 Details of Experiments Setting</p>
<p>A.1.1 Task Description of Textworld</p>
<p>The Textworld framework is an open-source platform for training and evaluating reinforcement learning agents in text-based games.It offers a diverse set of text-based games and provides a flexible environment for creating new games.The platform allows researchers and developers to explore various challenges in natural language understanding, reinforcement learning, and text-based game playing.The Textworld framework aims to advance the development of AI systems capable of understanding and interacting with natural language in complex environments, ultimately contributing to the broader field of AI research.</p>
<p>In our experiments, we select three classic text games, including Custom, Tw-Treasure Hunter, and Dragon.each game encompasses N states {s i } N i=1 described using language.Each state s i portrays the current situation and the available actions in language form.When the agent takes the correct action, a reward of +1 is obtained, and the transition to the next state s i+1 occurs.However, if the agent takes an incorrect action, a reward of -1 is obtained, and the agent remains in state s i .What's more, if the agent takes actions that do not belong to the action space, the agent will obtain a 0 reward and remain in state s i either.We take the Tw-Treasure Hunter game, for example, it contains three states:</p>
<p>Now I am in a bedchamber.There is an unguarded exit to the north.I want to go out the exit.Which direction do I want to go?There are two choices: 1. north.2. south.Now I arrive in a scullery.There is an exit to the east.There is an exit to the south.I want to get out from the east exit.Which direction do I want to go?There are two choices: 1. north.• State 1: Now I am in a bedchamber.There is an unguarded exit to the north.I want to go out the exit.Which direction do I want to go?There are two choices: 1. north.2. south.</p>
<p>• State 2: Now I arrive in a scullery.There is an exit to the east.There is an exit to the south.I want to get out from the east exit.Which direction do I want to go?There are two choices: 1. north.2. east.</p>
<p>• State 3: Now I have entered an attic.There is an unguarded exit to the south.There is an unblocked exit to the west.There is a lampshade on the floor.I need to recover the lampshade from the floor of the attic.What should I do next?There are two choices: 1. go east.2. pick up the lampshade.</p>
<p>Each state contains the current situation and actions and situations contains some information to get the correct actions.If we take the true action on states, for example go north in state 1, we will transit to the next state and finally solve the problem and obtain all the rewards.We give an illustration in</p>
<p>A.1.2 Task Description of Bigbench Hard</p>
<p>The Bigbench Hard dataset is an extension of the original Bigbench dataset, comprising the 27 most challenging tasks [10].In the Bigbench Hard dataset, language models are required to complete challenging tasks through reasoning, computation, and other methods.The tasks in Bigbench Hard are diverse, including understanding dates, logical reasoning, arithmetic problems, and more.In our experiments, we selected 7 tasks from BBH to train BWArea and Tinyllama:</p>
<p>• Reasoning about Colored Objects (multiple choices): Evaluate the model's ability to understand and reason about the attributes and relationships of objects based on their colors.</p>
<p>• Logical Deduction (multiple choices): Deduce the order of a sequence of objects.</p>
<p>• Tracking Shuffled Objects (multiple choices): Track the positions of objects as they are shuffled and identify their final positions.</p>
<p>• Boolean Expressions (judgment): Understand, manipulate, and evaluate logical statements involving Boolean expressions.</p>
<p>• Object Counting (text generation): Count the number of objects in a given scenario accurately.</p>
<p>• Dyck Languages (text generation): Recognize and generate strings that adhere to the rules of Dyck-4, which are used to represent balanced parentheses.</p>
<p>• Web of Lies (judgment): Evaluate a random boolean function expressed as a word problem.</p>
<p>Then we display some examples of the selected tasks on Bigbench Hard tasks in table 6.</p>
<p>A.2 Details of Model Training</p>
<p>A.2.1 VQ-VAE Optimization</p>
<p>As we mentioned in the paper, we optimize the inverse dynamics model and language world model jointly, where these two models serve as the encoder and decoder of VQ-VAE.The loss including two parts: encoder loss L VQ and decoder loss L predict .The decoder loss is to predict the next token and now we discuss the details of encoder loss.</p>
<p>In the inverse dynamics model, it first maps the sequence x 1:t+1 to an embedding e t , and then the embedding e t is projected to a code-book C = {c i } N i=1 via a t = c i , where i = arg min D(c i , e t ) and D is a distance metric.In our method, the embedding e t and each c i are the same dimension and adopt L2-distance to serve as the distance metric D.</p>
<p>For the encoder loss L VQ , it consists of two parts: commitment loss, to optimize the encoder except for code-book, and code-book loss to optimize the embedding in code-book C. For a embedding e t and its projected c i , the commitment loss is calculated by:
L commitment = ∥e t − c i sg ∥ 2 2
where c sg represents the detached gradient operator.It's important to note that the embedding e t can be optimized by both the commitment loss and the gradient passed from the decoder.As the gradient cannot go back from the code-book to the embedding, we employ a re-parameterized trick:
a t = e t + (c i − e t ) sg
And the code-book can be optimized by:
L codebook = λ c ∥(e t ) sg − c i ∥ 2 2
We set the λ c to be 25.and add the two parts of loss together.I think it's a good idea to build a firefighter at least once a year, and then go back and forth between firefighters and electric officers.I think it's a good idea to build a firefighter at least once a year, and then go back and forth between firefighters and other releases.I think it's a good idea to build a firefighter at least once a year, and then you can go back and forth between firefighters and other "allergement" sources.I think it's a good idea to build a fire by making sure it's not too much at the end of the "allergement" you're thinking of.</p>
<p>Pre-trained Model Instruction-Tuned Model</p>
<p>Human: Is a firefighter a good occupation?Assistant: I'm not sure what the exact definition of "occupation" is.I think you're asking whether it's "good" to be a firefighter.What criteria are you using to judge whether it's good or not?Human: Is it a good thing to work on?Assistant:</p>
<p>Figure 1 :
1
Figure 1: An example of how our BWArea model mimics the human brain for language processing.</p>
<p>Figure 3 :
3
Figure 3: The average accuracy on BBH (7 Tasks).</p>
<ol>
<li>east.Now I have entered an attic.There is an unguarded exit to the south.There is an unblocked exit to the west.There is a lampshade on the floor.I need to recover the lampshade from the floor of the attic.What should I do next?There are two choices: 1. go east.2. pick up the lampshade.</li>
</ol>
<p>Solved with Return of 3 Figure 4 :
34
Figure 4: An illustration on Tw-Treasure Hunter game.</p>
<p>Figure A.1.1.</p>
<p>I</p>
<p>'m not familiar with the idea of a good dog.I think it's a good dog.I think it's a good job.I think it's a good way to get a good dog.So, what is the outcome of the dog?Human: I'm not sure what the fact is.I think it's a good job.I think it's a good way to get a good dog.Assistant: I'm not necessarily sure what the fact is.I think it's a good way to get a good dog.I think it's a good way to get a good dog.Human: I'm not sure what the idea of a good dog is.I think it's a good stuff to get a good dog.... Human: Is a firefighter a good occupation?Assistant: I'm not sure what the exact definition of "occupation" is.I think you're asking whether it's "good" to be a firefighter.What criteria are you using to judge whether it's good or not?Human: Is it a good thing to work on?Assistant:</p>
<p>Figure 7 :
7
Figure 7: Comparison between Pre-trained and SFT model.</p>
<p>Cognitive Policy Inverse Dynamics Language World Model</p>
<p>Framework of our architecture.(a) Inverse Dynamics Model: input the context (x 1 , . . ., x t ) with the future x t+1 to output the latent action a t .(b) Policy Model: input the context (x 1 , . . ., x t ) without future to obtain the current action categorical distribution.(c) Language World Model: input the context (x 1 , . . ., x t ) and latent action (a 1 , . . ., a t ) to predict the next token.
Embedding Spacenearestsample1 2 , , , t e e e …2 1 , , , t a a a 1 a a 2 , , , t a 2 x,3 x,,t x1 dynamicsaggregatecontextactionencoderencoder1 x,2 x,,t x1 1 x x 2 , , , t x 1 x x 2 , , , t x 1 a a , , , t 2 a 1 x,2 x,,t x1 (a)(b)1 x x 2 , , , t x (c)1 x x 2 , , , t x 1 a a 2 , , , t a MLPTransformer Block ConcatenateFigure 2:</p>
<p>Table 1 :
1
Evaluation between our pretrained model (BWArea) and Tinyllama on standard benchmark.MC1 and MC2 are short for TruthfulQA(MC1) and TruthfulQA(MC2).
MMLU DROP BBHMC1 MC2Tinyllama-30B tokens 25.389.6428.70 22.64 41.39BWArea-30B tokens25.858.2727.68 23.13 42.58</p>
<p>Table 2 :
2
The return on tasks TextWorld.
Custom Tw-Treasure Hunter DragonTinyllama030BWArea533</p>
<p>Table 3 :
3
Evaluation answer accuracy of fine-tuned BWArea and Tinyllama.
Reasoning about BooleanObject Tracking Shuffled Dyck Web ofLogicalColored Objects Expressions CountingObjects (3)Lanuages Lies Deduction (5)Tinyllama4.3%0.0%0.8%12.0%0.0%43.8%17.5%BWArea25.0%80.0%8.1%27.0%12.5% 37.5%32.5%6050403020100Tinyllama BWArea GPT-3.5 TurboGPT-4</p>
<p>Table 4 :
4
Evaluation performance of our pretrained models with data scaling.
MMLU DROP BBH MC1 MC2BWArea (10B tokens)23.331.3527.59 20.93 39.51BWArea (20B tokens)25.403.6026.47 21.66 40.44BWArea (30B tokens)25.858.2727.68 23.13 42.58</p>
<p>Table 5 :
5
Evaluation performance of our pretrained model (BWArea) and Tinyllama with additional 1B dirty tokens.The number in the bottom right corner represents the performance change relative to the model trained with 30B clean tokens.
MMLUDROPBBHMC1MC2Tinyllama (+1B dirty tokens) 25.39 (+0.01)9.45 (−0.19) 28.60 (−0.10) 21.79 (−0.85) 41.30 (−0.09)BWArea (+1B dirty tokens)25.79 (−0.06) 8.58 (+0.31)28.16 (+0.48)24.60 (+1.47)44.12 (+1.54)
In our experiments, The token space size is 32k while the action space size N is 64.
https://github.com/jzhang38/TinyLlama
For detailed descriptions, please refer to https://github.com/google/BIG-bench/blob/main.
The reward model used in RLHF is from https://github.com/liziniu/ReMax.
https://github.com/tatsu-lab/alpaca_eval
Example inputsExample outputsReasoning about Colored ObjectsOn the desk, you see a set of things arranged in a row: a grey cup, a purple mug, and a blue teddy bear.What is the color of the thing directly to the right of the cup?Options: (A) red (B) orange (C) yellow (D) green (E) blue (F) brown (G) magenta (H) fuchsia (I) mauve (J) teal (K) turquoise (L) burgundy (M) silver (N) gold (O) black (P) grey (Q) purple (R) pink (Q)Logical DeductionThe following paragraphs each describe a set of three objects arranged in a fixed order.The statements are logically consistent within each paragraph.In a golf tournament, there were three golfers: Eve, Rob, and Mel.A.2.2 Hyper-ParametersIn this section, we provide the hyper-parameters during training.For the model structure and learning objective, the embedding dimension in transformer is 2048 in all transformer-based structures like the inverse dynamics, context encoder, and dynamics module in the world language model, and cognitive policy model.And for the action code book C = {c i } N i=1 , N = 64 and the code c i is with the dimension of 16 for a more effective optimization.When the sequence x 1:t+1 is mapped to an embedding e t with the dimension of 2048, since we have to compute the distance between embedding and codes, we adopt a linear to compress the embedding e t to an embedding with dimension of 16.The action encoder module in the language world model is linear to map the dimension of 16 to the dimension of 2048.For pre-training, the dataset is a segmentation in the form of x 1:T , where T = 2048.For each gradient step, the batch size is 512 and we optimize 30k gradient steps in both step-1 and step-2 learning stages.For step 1, the learning rate is 4e − 4 within Adam optimizer and the coefficient β to balance the decoder loss and encoder loss is set to be 0.25.For step 2, the learning rate is 1e − 4 with Adam optimizer.For SFT, the dataset is in the form of sentence padded to max-length, including prompt x 1:p and answer x p+1:T , where p = 256 and T = 512.We tune the policy within 2 epochs.For other parameters, they are the same as that in pre-training.For RL, the dataset is in the form of padded prompt x 1:p , where p = 256.We set the batch size to be 128.And the learning rate is 5e − 5.For Tinyllama, the learning rate is the same as that in the language world model.The batch size is the same as ours in all learning processes.A.2.3 Experiments Computation ResourcesIn our experiments, we train all the models on 8 A800-80G GPUs.Phase Time Pretrain Step1 6 DaysPretrain Step2 4 DaysSupervised Fine-Tuning Step1 4hSupervised Fine-Tuning Step2 4hReinforcement Learning 9h (BBH Tasks) and 4h (TextWorld) Table7: Running time of our experiments.A.3 Additional Empirical ResultsA.3.1 Results on RLHFTo validate the language ability of the BWArea after RLHF training, we train BWArea with ReMax and then test the language performance.The reward model used is a Llama-7B model 4 .The dataset we used is full-hh-rlhf.For chat tasks like full-hh-rlhf, evaluation is challenging, and the absolute value of the reward does not fully reflect the algorithm's quality.We use relative metrics to measure the effectiveness of BWArea.After 2 epochs of training, BWArea improved by 7.19, while Tinyllama improved by 3.48.The relative metrics of BWArea are better than Tinyllama.Additionally, we use GPT-4 scoring to evaluate the quality of the generated content.We follow the alpaca-eval pipeline 5 to evaluate the quality.BWArea can reach 51.28% win rate against Tinyllama.A.3.2 Training Curves on TextworldThe training and eval reward is shown in Figure5and Figure6.A.3.3 Demos of SFTWe demonstrate a comparison between the outputs before and after supervised fine-tuning.shows that before the fine-tuning, the model can only answer the questions in a dialogue formulation and repeat the dialogue.However, after the fine-tuning, it can answer the questions normally.A.3.4 Additional Results on ControllabilityThen we seek to confirm whether the action corresponds to a marginal token distribution.For the test dataset D eval , which is i.i.d. with the pre-training dataset D pretrain but was not encountered during the training, we computed the marginal cross-entropy loss where actions are that with the maximum probability on policy π and these actions are then input into the dynamics to obtain the next token distribution.We also compute the expected distribution and its cross-entropy loss over the action space:1 |D eval | ∑ D eval ∑ T t=1 log ∑ a t p world (x t+1 |x 1:t , a t )p policy (a t |x 1:t ).We compute two types of cross-entropy loss on D eval .The marginal loss is 6.2, while the expected loss is 3.3.This difference indicates that the marginal token distribution significantly differs from the expected distribution, thereby demonstrating the effectiveness of our actions in guiding the model's output.
BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)2018</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Cold-attack: Jailbreaking llms with stealthiness and controllability. Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu, arXiv:2402.086792024arXiv preprint</p>
<p>What's the magic word? a control theory of llm prompting. Aman Bhargava, Cameron Witkowski, Manav Shah, Matt Thomson, arXiv:2310.044442023arXiv preprint</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Margaret Mitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21). the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21)2021</p>
<p>Broca's region revisited: Cytoarchitecture and intersubject variability. Katrin Amunts, Axel Schleicher, Uwe Bürgel, Hartmut Mohlberg, B M Harry, Karl Uylings, Zilles, Journal of Comparative Neurology. 47942004</p>
<p>Reasoning, learning, and creativity: frontal lobe function and human decision-making. Anne Collins, Etienne Koechlin, PLoS biology. 103e10012932012</p>
<p>The brain basis of language processing: From structure to function. Angela D Friederici, Physiological Reviews. 9142011</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, CoRR, abs/1806.115322018</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, Advances in neural information processing systems. 322019</p>
<p>Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, arXiv:1909.119422019arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 2019</p>
<p>GLM-130B: an open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, Jie Tang, International Conference on Learning Representations. 2023</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 21672020</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Andrea Coletta, Kshama Dwarakanath, Penghang Liu, Svitlana Vyetrenko, Tucker Balch, arXiv:2402.08755Llmdriven imitation of subrational behavior : Illusion or reality?. 2024arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Prompt tuning with rules for text classification. Han, Zhao, Ding, Liu, Sun Ptr, 10.1016/j.aiopen2022. 20223</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, arXiv:2310.04363Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. 2023arXiv preprint</p>
<p>. Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, Emmanuel Bengio, Gflownet foundations. J. Mach. Learn. Res. 24552023</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, Advances in Neural Information Processing Systems. 201730</p>
<p>Aligning ai with shared humanvalues. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, Jacob Steinhardt, Proceedings of the 9th International Conference on Learning Representations (ICLR). the 9th International Conference on Learning Representations (ICLR)2021</p>
<p>Reinforcement Learning: An Introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>When to trust your model: Model-based policy optimization. Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine, Advances in neural information processing systems. 201932</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, Advances in neural information processing systems. 201831</p>
<p>Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, Tengyu Ma, arXiv:1807.038582018arXiv preprint</p>
<p>. David Ha, Jürgen Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>. P Diederik, Max Kingma, Welling, arXiv:1312.61142013Auto-encoding variational bayes. arXiv preprint</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, International Conference on Learning Representations. 2020</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, Advances in Neural Information Processing Systems. Curran Associates, Inc2021</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Nature. 58878392020</p>
<p>Advances in neural information processing systems. Junhyuk Oh, Satinder Singh, Honglak Lee, 201730Value prediction network</p>
<p>Thinking fast and slow with deep learning and tree search. Thomas Anthony, Zheng Tian, David Barber, 201730Advances in neural information processing systems</p>
<p>Become a proficient player with limited data through watching pure videos. Weirui Ye, Yunsheng Zhang, Pieter Abbeel, Yang Gao, International Conference on Learning Representations. 2023</p>
<p>Reinforcement learning with action-free pre-training from videos. Younggyo Seo, Kimin Lee, Stephen L James, Pieter Abbeel, International Conference on Machine Learning. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, Sivan Sabato, 2022</p>
<p>Advances in neural information processing systems. Aaron Van Den, Oriol Oord, Vinyals, 201730Neural discrete representation learning</p>
<p>Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo, arXiv:2310.10505Remax: A simple, effective, and efficient method for aligning large language models. 2023arXiv preprint</p>
<p>Review on model predictive control: An engineering perspective. Max Schwenzer, Muzaffer Ay, Thomas Bergs, Dirk Abel, The International Journal of Advanced Manufacturing Technology. 11752021</p>
<p>Temporal difference learning for model predictive control. Nicklas Hansen, Xiaolong Wang, Hao Su, arXiv:2203.049552022arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi, arXiv:2210.01241Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. 2022arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv, 1707.06347Proximal policy optimization algorithms. 2017</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, arXiv:1903.00161Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. 2019arXiv preprint</p>
<p>Stephanie Lin, Jacob Hilton, Owain Evans, Truthfulqa, arXiv:2109.07958Measuring how models mimic human falsehoods. 2021arXiv preprint</p>
<p>Elements of information theory. M Thomas, Cover, 1999John Wiley &amp; Sons</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.04475Length-controlled alpacaeval: A simple way to debias automatic evaluators. 2024arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint</p>
<p>A survey on data selection for language models. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang, Wang , 2024</p>
<p>Learning from imperfect demonstrations through dynamics evaluation. Xizhou Bu, Zhiqiang Ma, Zhengxiong Liu, Wenjuan Li, Panfeng Huang, arXiv:2312.111942023arXiv preprint</p>
<p>Model-based reinforcement learning via meta-policy optimization. Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, Pieter Abbeel, Conference on Robot Learning. PMLR2018</p>
<p>Ravl: Reach-aware value learning for the edge-of-reach problem in offline model-based reinforcement learning. Anya Sims, Cong Lu, Yee Whye Teh, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
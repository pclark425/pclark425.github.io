<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-846 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-846</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-846</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-260887774</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.06391v1.pdf" target="_blank">Dynamic Planning with a LLM</a></p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e846.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e846.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-DP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Dynamic Planner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic closed-loop agent that combines an LLM with a symbolic PDDL planner by maintaining a known world state W and a set of beliefs B, sampling plausible concrete world states with an LLM, generating PDDL problem files, and running a classical planner to produce actions which are selected and executed with online belief updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM Dynamic Planner (LLM-DP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-DP is a neuro-symbolic architecture that (1) takes a PDDL domain file describing actions and predicates, (2) parses observations into a known world state W and a set of uncertain predicates B (beliefs), (3) uses an LLM to translate natural-language task instructions into a PDDL :goal and to sample instantiations of unknown predicates to produce N plausible concrete world states, (4) converts each sampled world state into a PDDL problem file (:objects, :init, :goal), (5) calls a symbolic planner (BFS(f) via LAPKT) to generate plans for each sampled problem, (6) an Action Selector chooses an immediate action (shortest plan heuristic in experiments) to execute in the text environment, and (7) integrates new observations to update W and B and re-plan as needed. The system also supports fallback/random resampling when sampled states produce no valid plans.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL + lifted belief states (sampled instantiations)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>The world model is represented symbolically: a known deterministic component W (observed receptacles, their attributes) plus a belief set B of possible predicates for unknown properties (e.g., object locations). Uncertainty is handled by sampling concrete initial states w_belief = W ∪ sampled(B) where sampled(B) assigns specific predicate values (e.g., inReceptacle object location). Actions and transitions are those in the given PDDL domain (deterministic symbolic effects in the domain file); uncertainty comes from the unknown initial predicates rather than stochastic action effects in the planner itself.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Translate natural-language tasks to PDDL goals; complete/instantiate unknown predicates (sample likely world states) to construct concrete PDDL problem files; guide belief sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>OpenAI gpt-3.5-turbo (gpt-3.5-turbo-0613 mentioned in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>Epistemic/state uncertainty (unknown initial object locations); uncertainty in LLM predicate completion (model prediction uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Monte Carlo-style sampling of beliefs using an LLM to generate n plausible world states; fallback to random sampling when needed. Beliefs are represented symbolically as sets of possible predicates and instantiated by sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>BFS(f) symbolic planner (Lipovetzky et al.) executed via LAPKT</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Alfworld</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>A text-only simulated home environment where each episode gives a natural-language goal but initial object locations are unobserved; agent must navigate receptacles and interact with objects, discovering their locations through actions; known receptacle locations are given but object placements are uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (SR) and average episode length (EL, number of executed actions to achieve the goal)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LLM-DP (n=3) success rate 0.96 (96%) with average episode length 13.16 actions; LLM-DP translates task to executable PDDL goal 97% of the time (sampling can reduce overall accuracy when sampled states already satisfy the goal).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to a ReAct (LLM-only) reproduction using gpt-3.5-turbo: ReAct SR 0.53 (53%), average episode length 18.69 actions; random search baseline average length 15.02 actions. LLM-DP outperforms these baselines in success rate and is faster (fewer actions).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>The paper compares different sampling strategies: LLM-based sampling vs random sampling (LLM-DP vs LLM-DP-random) and varying sample size n (e.g., n=3, n=5) and fallback behavior. Increasing n raises the chance of finding at least one solvable sampled state and improves success rate; LLM sampling outperforms purely random sampling in producing plausible states, but LLM sampling can sometimes sample states where the goal is already satisfied, reducing effective utility unless fallback/resampling is used.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A system that combines symbolic PDDL planning with LLM-driven belief instantiation can handle text-based embodied tasks with unknown initial information much more effectively than an LLM-only acting agent: sampling beliefs via an LLM and running a classical planner on multiple plausible PDDL problems leads to high success rates (96%) and shorter plans; uncertainty is handled by sample-based belief instantiation rather than by a probabilistic planner, suggesting a practical hybrid way to incorporate LLM uncertainty into symbolic planning for text environments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e846.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e846.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM belief sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based belief sampling / sampled belief states</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper where an LLM is used to sample concrete instantiations of uncertain symbolic predicates (beliefs) to produce multiple plausible PDDL initial states for downstream deterministic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based belief sampling (within LLM-DP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Beliefs B are represented as symbolic sets of possible predicates (e.g., for each unobserved object, a set of possible inReceptacle locations). The LLM is prompted with the known world W and the predicate options to produce one sampled assignment of those predicates (a concrete world state). Repeating this N times yields N plausible PDDL :init states; each is converted into a PDDL problem file and solved by the symbolic planner. This is effectively Monte Carlo sampling of initial state hypotheses using an LLM as the sampler.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Lifted belief states instantiated into concrete PDDL initial states (sampled belief-state approach)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Beliefs are sets of symbolic predicates with multiple possible groundings; the LLM maps these to concrete ground predicates to form a complete symbolic initial state. The underlying planner treats each sampled initial state deterministically according to the PDDL domain's action preconditions and effects.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Sampler / world-model constructor: completes unknown predicates and generates plausible concrete initial states for the planner.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>OpenAI gpt-3.5-turbo (gpt-3.5-turbo-0613 used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>State (epistemic) uncertainty about initial world configuration and uncertainty in LLM-generated completions</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Sampling via LLM (Monte Carlo); operation includes an explicit 'fallback' to random resampling when sampled states fail to produce plans.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>BFS(f) planner executed on each sampled concrete state</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Alfworld</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>See LLM-DP entry: Alfworld is a text-only home environment with unknown object locations requiring active search and multi-step interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (SR) and average episode length (EL) as reported for LLM-DP variants</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LLM-DP using LLM belief sampling (n=3) achieved SR=0.96 and EL=13.16; sampling strategy (LLM vs random) and sample size n affected SR and EL (e.g., larger n increases chance of solvable sampled state).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to random belief sampling (LLM-DP-random) and to an LLM-only ReAct baseline: LLM sampling produced better plans than purely random sampling and outperformed ReAct (SR 53%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>The paper reports differences between LLM sampling and random sampling, and reports results for different n and with/without fallback. LLM sampling with moderate n (3-5) gives high SR; fallback to random sampling affects SR negatively when LLM samples states already satisfying the goal. Increasing n improves likelihood of finding at least one solvable sampled state.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an LLM as a sampler to instantiate symbolic belief states is an effective way to integrate LLM uncertainty into a symbolic planner: sampling multiple plausible PDDL initializations and planning deterministically on each yields robust performance in a text-based embodied environment.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e846.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e846.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPDDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PPDDL (Probabilistic Planning Domain Definition Language)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of PDDL designed to express domains with probabilistic effects, allowing planners to represent and reason about stochastic action outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ppddl1.0: An extension to pddl for expressing planning domains with probabilistic effects</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PPDDL (probabilistic extension of PDDL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PPDDL augments the PDDL language to allow specification of probabilistic effects for actions, enabling representation of domains where actions have stochastic outcomes; it is intended for probabilistic planners and MDP-style planning.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PPDDL (probabilistic PDDL)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>States and actions are represented symbolically as in PDDL, but action effects can be distributions over possible resultant predicates/states, thereby encoding aleatoric/action-outcome uncertainty directly in domain definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>Action-outcome (aleatoric) uncertainty / probabilistic effects</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Explicit probability distributions over action effects as part of the domain specification (PPDDL syntax)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an existing extension to PDDL that addresses probabilistic effects; the paper contrasts PPDDL (explicit probabilistic planners) with the LLM-DP approach, which instead handles initial-state uncertainty by LLM-driven sampling rather than by using a probabilistic planner.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e846.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e846.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDDLStream</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PDDLStream</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that integrates symbolic PDDL planners with black-box samplers to handle continuous and sampled components in Task and Motion Planning via optimistic adaptive planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pddlstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PDDLStream</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PDDLStream couples a symbolic PDDL planner with external black-box samplers: symbolic planning is used to propose optimistic plans while samplers provide concrete continuous parameters or checked instantiations; the system alternates planning and sampling to find feasible solutions in TAMP problems.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL augmented with samplers (PDDLStream)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Symbolic predicates and actions are combined with sampler-provided continuous or discrete parameter instantiations; uncertainty or feasibility is resolved by invoking samplers and re-planning as necessary (optimistic adaptive strategy).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>Feasibility / sampling uncertainty (e.g., continuous parameters), not necessarily LLM uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Black-box samplers combined with optimistic planning and adaptive re-planning</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Symbolic PDDL planning coupled with sampler-driven checks (optimistic adaptive planning)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as prior work that integrates symbolic planning with sampling-based handling of uncertain or continuous parameters; cited as related to addressing uncertainty in planning but originally intended for Task and Motion Planning rather than text-based environments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ppddl1.0: An extension to pddl for expressing planning domains with probabilistic effects <em>(Rating: 2)</em></li>
                <li>Pddlstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning <em>(Rating: 2)</em></li>
                <li>Pddl planning with pretrained large language models <em>(Rating: 2)</em></li>
                <li>Llm+p: Empowering large language models with optimal planning proficiency <em>(Rating: 2)</em></li>
                <li>Autotamp: Autoregressive task and motion planning with llms as translators and checkers <em>(Rating: 1)</em></li>
                <li>Reasoning with language model is planning with world model <em>(Rating: 1)</em></li>
                <li>Autotamp: Autoregressive task and motion planning with llms as translators and checkers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-846",
    "paper_id": "paper-260887774",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "LLM-DP",
            "name_full": "LLM Dynamic Planner",
            "brief_description": "A neurosymbolic closed-loop agent that combines an LLM with a symbolic PDDL planner by maintaining a known world state W and a set of beliefs B, sampling plausible concrete world states with an LLM, generating PDDL problem files, and running a classical planner to produce actions which are selected and executed with online belief updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM Dynamic Planner (LLM-DP)",
            "system_description": "LLM-DP is a neuro-symbolic architecture that (1) takes a PDDL domain file describing actions and predicates, (2) parses observations into a known world state W and a set of uncertain predicates B (beliefs), (3) uses an LLM to translate natural-language task instructions into a PDDL :goal and to sample instantiations of unknown predicates to produce N plausible concrete world states, (4) converts each sampled world state into a PDDL problem file (:objects, :init, :goal), (5) calls a symbolic planner (BFS(f) via LAPKT) to generate plans for each sampled problem, (6) an Action Selector chooses an immediate action (shortest plan heuristic in experiments) to execute in the text environment, and (7) integrates new observations to update W and B and re-plan as needed. The system also supports fallback/random resampling when sampled states produce no valid plans.",
            "world_model_type": "PDDL + lifted belief states (sampled instantiations)",
            "world_model_description": "The world model is represented symbolically: a known deterministic component W (observed receptacles, their attributes) plus a belief set B of possible predicates for unknown properties (e.g., object locations). Uncertainty is handled by sampling concrete initial states w_belief = W ∪ sampled(B) where sampled(B) assigns specific predicate values (e.g., inReceptacle object location). Actions and transitions are those in the given PDDL domain (deterministic symbolic effects in the domain file); uncertainty comes from the unknown initial predicates rather than stochastic action effects in the planner itself.",
            "uses_llm": true,
            "llm_role": "Translate natural-language tasks to PDDL goals; complete/instantiate unknown predicates (sample likely world states) to construct concrete PDDL problem files; guide belief sampling.",
            "llm_model_name": "OpenAI gpt-3.5-turbo (gpt-3.5-turbo-0613 mentioned in experiments)",
            "uncertainty_modeling": true,
            "uncertainty_type": "Epistemic/state uncertainty (unknown initial object locations); uncertainty in LLM predicate completion (model prediction uncertainty)",
            "uncertainty_method": "Monte Carlo-style sampling of beliefs using an LLM to generate n plausible world states; fallback to random sampling when needed. Beliefs are represented symbolically as sets of possible predicates and instantiated by sampling.",
            "planning_algorithm": "BFS(f) symbolic planner (Lipovetzky et al.) executed via LAPKT",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Alfworld",
            "text_environment_description": "A text-only simulated home environment where each episode gives a natural-language goal but initial object locations are unobserved; agent must navigate receptacles and interact with objects, discovering their locations through actions; known receptacle locations are given but object placements are uncertain.",
            "performance_metric": "Success rate (SR) and average episode length (EL, number of executed actions to achieve the goal)",
            "performance_value": "LLM-DP (n=3) success rate 0.96 (96%) with average episode length 13.16 actions; LLM-DP translates task to executable PDDL goal 97% of the time (sampling can reduce overall accuracy when sampled states already satisfy the goal).",
            "baseline_comparison": "Compared to a ReAct (LLM-only) reproduction using gpt-3.5-turbo: ReAct SR 0.53 (53%), average episode length 18.69 actions; random search baseline average length 15.02 actions. LLM-DP outperforms these baselines in success rate and is faster (fewer actions).",
            "has_ablation_uncertainty": true,
            "ablation_results": "The paper compares different sampling strategies: LLM-based sampling vs random sampling (LLM-DP vs LLM-DP-random) and varying sample size n (e.g., n=3, n=5) and fallback behavior. Increasing n raises the chance of finding at least one solvable sampled state and improves success rate; LLM sampling outperforms purely random sampling in producing plausible states, but LLM sampling can sometimes sample states where the goal is already satisfied, reducing effective utility unless fallback/resampling is used.",
            "key_findings": "A system that combines symbolic PDDL planning with LLM-driven belief instantiation can handle text-based embodied tasks with unknown initial information much more effectively than an LLM-only acting agent: sampling beliefs via an LLM and running a classical planner on multiple plausible PDDL problems leads to high success rates (96%) and shorter plans; uncertainty is handled by sample-based belief instantiation rather than by a probabilistic planner, suggesting a practical hybrid way to incorporate LLM uncertainty into symbolic planning for text environments.",
            "uuid": "e846.0"
        },
        {
            "name_short": "LLM belief sampling",
            "name_full": "LLM-based belief sampling / sampled belief states",
            "brief_description": "A method introduced in this paper where an LLM is used to sample concrete instantiations of uncertain symbolic predicates (beliefs) to produce multiple plausible PDDL initial states for downstream deterministic planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-based belief sampling (within LLM-DP)",
            "system_description": "Beliefs B are represented as symbolic sets of possible predicates (e.g., for each unobserved object, a set of possible inReceptacle locations). The LLM is prompted with the known world W and the predicate options to produce one sampled assignment of those predicates (a concrete world state). Repeating this N times yields N plausible PDDL :init states; each is converted into a PDDL problem file and solved by the symbolic planner. This is effectively Monte Carlo sampling of initial state hypotheses using an LLM as the sampler.",
            "world_model_type": "Lifted belief states instantiated into concrete PDDL initial states (sampled belief-state approach)",
            "world_model_description": "Beliefs are sets of symbolic predicates with multiple possible groundings; the LLM maps these to concrete ground predicates to form a complete symbolic initial state. The underlying planner treats each sampled initial state deterministically according to the PDDL domain's action preconditions and effects.",
            "uses_llm": true,
            "llm_role": "Sampler / world-model constructor: completes unknown predicates and generates plausible concrete initial states for the planner.",
            "llm_model_name": "OpenAI gpt-3.5-turbo (gpt-3.5-turbo-0613 used in experiments)",
            "uncertainty_modeling": true,
            "uncertainty_type": "State (epistemic) uncertainty about initial world configuration and uncertainty in LLM-generated completions",
            "uncertainty_method": "Sampling via LLM (Monte Carlo); operation includes an explicit 'fallback' to random resampling when sampled states fail to produce plans.",
            "planning_algorithm": "BFS(f) planner executed on each sampled concrete state",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Alfworld",
            "text_environment_description": "See LLM-DP entry: Alfworld is a text-only home environment with unknown object locations requiring active search and multi-step interaction.",
            "performance_metric": "Success rate (SR) and average episode length (EL) as reported for LLM-DP variants",
            "performance_value": "LLM-DP using LLM belief sampling (n=3) achieved SR=0.96 and EL=13.16; sampling strategy (LLM vs random) and sample size n affected SR and EL (e.g., larger n increases chance of solvable sampled state).",
            "baseline_comparison": "Compared to random belief sampling (LLM-DP-random) and to an LLM-only ReAct baseline: LLM sampling produced better plans than purely random sampling and outperformed ReAct (SR 53%).",
            "has_ablation_uncertainty": true,
            "ablation_results": "The paper reports differences between LLM sampling and random sampling, and reports results for different n and with/without fallback. LLM sampling with moderate n (3-5) gives high SR; fallback to random sampling affects SR negatively when LLM samples states already satisfying the goal. Increasing n improves likelihood of finding at least one solvable sampled state.",
            "key_findings": "Using an LLM as a sampler to instantiate symbolic belief states is an effective way to integrate LLM uncertainty into a symbolic planner: sampling multiple plausible PDDL initializations and planning deterministically on each yields robust performance in a text-based embodied environment.",
            "uuid": "e846.1"
        },
        {
            "name_short": "PPDDL",
            "name_full": "PPDDL (Probabilistic Planning Domain Definition Language)",
            "brief_description": "An extension of PDDL designed to express domains with probabilistic effects, allowing planners to represent and reason about stochastic action outcomes.",
            "citation_title": "ppddl1.0: An extension to pddl for expressing planning domains with probabilistic effects",
            "mention_or_use": "mention",
            "system_name": "PPDDL (probabilistic extension of PDDL)",
            "system_description": "PPDDL augments the PDDL language to allow specification of probabilistic effects for actions, enabling representation of domains where actions have stochastic outcomes; it is intended for probabilistic planners and MDP-style planning.",
            "world_model_type": "PPDDL (probabilistic PDDL)",
            "world_model_description": "States and actions are represented symbolically as in PDDL, but action effects can be distributions over possible resultant predicates/states, thereby encoding aleatoric/action-outcome uncertainty directly in domain definitions.",
            "uses_llm": null,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "Action-outcome (aleatoric) uncertainty / probabilistic effects",
            "uncertainty_method": "Explicit probability distributions over action effects as part of the domain specification (PPDDL syntax)",
            "planning_algorithm": null,
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Mentioned as an existing extension to PDDL that addresses probabilistic effects; the paper contrasts PPDDL (explicit probabilistic planners) with the LLM-DP approach, which instead handles initial-state uncertainty by LLM-driven sampling rather than by using a probabilistic planner.",
            "uuid": "e846.2"
        },
        {
            "name_short": "PDDLStream",
            "name_full": "PDDLStream",
            "brief_description": "A framework that integrates symbolic PDDL planners with black-box samplers to handle continuous and sampled components in Task and Motion Planning via optimistic adaptive planning.",
            "citation_title": "Pddlstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning",
            "mention_or_use": "mention",
            "system_name": "PDDLStream",
            "system_description": "PDDLStream couples a symbolic PDDL planner with external black-box samplers: symbolic planning is used to propose optimistic plans while samplers provide concrete continuous parameters or checked instantiations; the system alternates planning and sampling to find feasible solutions in TAMP problems.",
            "world_model_type": "PDDL augmented with samplers (PDDLStream)",
            "world_model_description": "Symbolic predicates and actions are combined with sampler-provided continuous or discrete parameter instantiations; uncertainty or feasibility is resolved by invoking samplers and re-planning as necessary (optimistic adaptive strategy).",
            "uses_llm": null,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "Feasibility / sampling uncertainty (e.g., continuous parameters), not necessarily LLM uncertainty",
            "uncertainty_method": "Black-box samplers combined with optimistic planning and adaptive re-planning",
            "planning_algorithm": "Symbolic PDDL planning coupled with sampler-driven checks (optimistic adaptive planning)",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Mentioned as prior work that integrates symbolic planning with sampling-based handling of uncertain or continuous parameters; cited as related to addressing uncertainty in planning but originally intended for Task and Motion Planning rather than text-based environments.",
            "uuid": "e846.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ppddl1.0: An extension to pddl for expressing planning domains with probabilistic effects",
            "rating": 2,
            "sanitized_title": "ppddl10_an_extension_to_pddl_for_expressing_planning_domains_with_probabilistic_effects"
        },
        {
            "paper_title": "Pddlstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning",
            "rating": 2,
            "sanitized_title": "pddlstream_integrating_symbolic_planners_and_blackbox_samplers_via_optimistic_adaptive_planning"
        },
        {
            "paper_title": "Pddl planning with pretrained large language models",
            "rating": 2,
            "sanitized_title": "pddl_planning_with_pretrained_large_language_models"
        },
        {
            "paper_title": "Llm+p: Empowering large language models with optimal planning proficiency",
            "rating": 2,
            "sanitized_title": "llmp_empowering_large_language_models_with_optimal_planning_proficiency"
        },
        {
            "paper_title": "Autotamp: Autoregressive task and motion planning with llms as translators and checkers",
            "rating": 1,
            "sanitized_title": "autotamp_autoregressive_task_and_motion_planning_with_llms_as_translators_and_checkers"
        },
        {
            "paper_title": "Reasoning with language model is planning with world model",
            "rating": 1,
            "sanitized_title": "reasoning_with_language_model_is_planning_with_world_model"
        },
        {
            "paper_title": "Autotamp: Autoregressive task and motion planning with llms as translators and checkers",
            "rating": 1,
            "sanitized_title": "autotamp_autoregressive_task_and_motion_planning_with_llms_as_translators_and_checkers"
        }
    ],
    "cost": 0.015754749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dynamic Planning with a LLM</p>
<p>Gautier Dagan gautier.dagan@ed.ac.uk 
School of Informatics
University of Edinburgh
UK</p>
<p>Frank Keller 
School of Informatics
University of Edinburgh
UK</p>
<p>Alex Lascarides 
School of Informatics
University of Edinburgh
UK</p>
<p>Dynamic Planning with a LLM</p>
<p>While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neurosymbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), like GPT-4 (Ope-nAI, 2023), have proven remarkably effective at various natural language processing tasks, particularly in zero-shot or few-shot settings (Brown et al., 2020). However, employing LLMs in embodied agents, which interact with dynamic environments, presents substantial challenges. LLMs tend to generate incorrect or spurious information, a phenomenon known as hallucination, and their performance is brittle to the phrasing of prompts (Ji et al., 2022). Moreover, LLMs are ill-equipped for naive long-term planning since managing an extensive context over multiple steps is complex and resource-consuming (Silver et al., 2022;Liu et al., 2023).</p>
<p>Various approaches have aimed to mitigate some of these limitations. For instance, methods like Chain-of-Thought (Wei et al., 2022) and Self-Consistency (Wang et al., 2023b) augment the context with reasoning traces. Other, agent-based approaches, such as ReAct (Yao et al., 2023), integrate feedback from the environment iteratively, giving the agent the ability to take 'thinking' steps or to augment its context with a reasoning trace. However, these approaches frequently involve high computational costs due to the iterated invocations of LLMs and still face challenges dealing with the limits of the context window and recovering from hallucinations, which can compromise the quality of the plans.</p>
<p>Conversely, traditional symbolic planners, such as the Fast-Forward planner (Hoffmann and Nebel, 2001) or the BFS(f) planner (Lipovetzky et al., 2014), excel at finding optimal plans efficiently. But symbolic planners require problem and domain descriptions as prerequisites (McDermott, 2000), which hampers their applicability in real-world scenarios where it may be infeasible to achieve these high informational demands. For instance, knowing a complete and accurate description of the goal may not be possible before exploring the environment through actions.</p>
<p>Previous work by (Liu et al., 2023) has shown that LLMs can generate valid problem files in the Planning Domain Definition Language (PDDL ) for many simple examples. Yet, the problem of incomplete information remains: agents often need to interact with the world to discover their surroundings before optimal planning can be applied. Some versions of PDDL have been proposed in the past to deal with probabilities or Task and Motion Planning, such as PPDDL and PDDLStream (Younes and Littman, 2004;Garrett et al., 2018), but these still assume a human designer encoding the agent's understanding of the domain and the planning problem, rather than the agent learning from interactions. Therefore, where modern LLMs need minimal information to figure out a task, e.g. through Few-shot or In-Context Learning (Honovich et al.,  Goal (init: ... (inReceptacle potato-1 fridge-1)) (init: ... (inReceptacle potato-1 fridge-1)) task Figure 1: LLM Dynamic Planner (LLM-DP). The LLM grounds observations and processes natural language instructions into PDDL to use with a symbolic planner. This model can solve plans for unobserved or previously unknown objects because the LLM generates plausible predicates for relevant objects through semantic and pragmatic inference. Through sampling possible predicates, multiple plans can be found, and an Action Selector decides whether to act, review its understanding of the problem, or ask clarification questions. 2022; Chen et al., 2022;Min et al., 2022), traditional planners need maximal information.</p>
<p>In this work, we introduce the LLM Dynamic Planner (LLM-DP), a neuro-symbolic framework that integrates an LLM with a symbolic planner to solve embodied tasks. 1 LLM-DP capitalises on the LLM's ability to understand actions and their impact on their environment and combines it with the planner's efficiency in finding solutions. Using domain knowledge, LLM-DP solves the Alfworld test set faster and more efficiently than a LLM-only (ReAct) approach. The remainder of this paper explores the architecture of LLM-DP, discusses how to combine the strengths of LLMs and symbolic planning and presents potential research avenues for future work in LLM-driven agents.</p>
<p>Related Work</p>
<p>Symbolic Planners Symbolic planners have been a cornerstone in automated planning and artificial intelligence for decades (Fikes and Nilsson, 1971). Based on formal logic, they operate over symbolic representations of the world to find a sequence of actions that transition from an initial state to a goal state. Since the introduction of PDDL (McDermott, 2000), the AI planning community has developed an array of efficient planning algorithms. For example, the Fast-Forward planner (FF) (Hoffmann and Nebel, 2001) employs heuristics derived from a relaxed version of the planning problem. Similarly, the BFS(f) planner (Lipovetzky et al., 2014) combines breadth-first search and specialised heuristics. These planners find high-quality or optimal solu-1 Our code is available at github.com/itl-ed/llm-dp tions quickly in well-defined domains. However, their up-front requirement for comprehensive problem and domain descriptions limits their applicability in complex real-world settings where complete information may not be available.</p>
<p>LLMs in Planning and Reasoning</p>
<p>In contrast to symbolic planners, LLMs have shown promise in adapting to noisy planning and reasoning tasks through various methods. Some general approaches such as Chain-of-Thought (Wei et al., 2022), Self-Consistency (Wang et al., 2023b), and Reasoning via Planning  augment the context with a reasoning trace that the LLM generates to improve its final prediction. Alternatively, giving access to tools/APIs (Schick et al., 2023;Patil et al., 2023), outside knowledge or databases (Peng et al., 2023;, code (Surís et al., 2023), and even symbolic reasoners  to enrich an LLM's context and ability to reason. The LLM can trigger these external sources of information or logic (through fine-tuning or prompting) to obtain additional context and improve its downstream performance.</p>
<p>Embodied Agents with LLMs In a parallel direction, recent works such as ReAct (Yao et al., 2023), Reflexion (Shinn et al., 2023), AutoGPT (Significant-Gravitas, 2023), and Voyager (Wang et al., 2023a), take an agent-based approach and augment the reasoning process through a closed 'while' loop that feeds environment observations back to the LLM. ReAct (Yao et al., 2023) allows the LLM agent to either take an action or a 'thinking' step. This allows the LLM to augment its context with its reasoning, which can be seen as agent-driven Chain-of-Thought prompting. Voyager (Wang et al., 2023a) incrementally builds an agent's capabilities from its interactions with the environment and an accessible memory component (skill library). While many of these works show promising results in building general executable agents in embodied environments (Wang et al., 2023a), they still require many expensive calls to the LLMs, are limited by the LLM's context window, and do not guarantee optimal plans.</p>
<p>Alfworld</p>
<p>Alfworld (Shridhar et al., 2020) is a text-only home environment where an agent is tasked with seven possible tasks, such as interacting with one or more objects and placing them in a specific receptacle. At the start of each episode, the goal is given in natural language, and the initial observation does not include the location of any objects. Therefore an agent must navigate the environment to search for the relevant objects and perform the correct actions. The possible locations of the environment are known, and the agent can navigate to any receptacle by using a 'go to' action. However, since none of the objects' locations are initially observed, the agent must be able to plan around uncertainty, estimate where objects are likely to be observed and adjust accordingly.</p>
<p>LLM-DP</p>
<p>To tackle an embodied environment like Alfworld, we introduce the Large Language Model Dynamic Planner (LLM-DP), which operates as a closedloop agent. LLM-DP uses a combination of language understanding and symbolic reasoning to plan and solve tasks in the simulated environment. The model tracks a World State W and beliefs B about predicates in the environment, uses an LLM to translate the task description into an executable goal state and samples its beliefs to generate plausible world states. We describe the working of the LLM-DP agent as pseudo-code in Algorithm 1.</p>
<p>Assumptions</p>
<p>We make several simplifying assumptions when applying the LLM-DP framework to Alfworld:</p>
<p>Known action-descriptions and predicates:</p>
<p>Our input to the planner and the LLM requires the PDDL domain file, which describes what actions can be taken, their pre-and postconditions, and what predicates exist. LLM-DP uses an LLM to generate a PDDL goal, given the natural language instruction (task) and the valid predicates defined by the PDDL domain file. Figure 1 shows an example task converted to a valid PDDL goal. For each episode, we use a set of three in-context examples that are fixed for the entire evaluation duration. We use the OpenAI gpt-3.5-turbo-0613 LLM model with a temperature of 0 in all our LLM-DP experiments.</p>
<p>Sampling Beliefs</p>
<p>We parse the initial scene description into a structured representation of the environment W and a set of beliefs B. The internal representation of the world W contains all known information, for instance, all receptacles (possible locations) in the scene from the initial observation and their intrinsic attributes are known (i.e. a fridge holds the isFridge predicate). Whereas the set of beliefs B are a set of possible valid predicates that can be true or false and which the model does not have enough information to disambiguate. In Alfworld, the objects' locations are unknown; therefore, the set of possible predicates for each object includes all possible locations. (b) The average episode length for each model, where the length of an episode denotes how many actions the agent has taken or attempted to take to complete a task. We do not count the 'thinking' action of ReAct as an action in this metric. Table 1: Summary of model performance on the Alfword test set. LLM-DP and LLM-DP-random differ in the sampling strategy of the belief. LLM-DP uses an LLM to generate n = 3 plausible world states, while LLM-DP-random randomly samples n = 3 plausible world states.</p>
<p>LLM-DP uses stored observations W, beliefs B and an LLM to construct different planning problem files in PDDL . A PDDL problem file includes the objects observed (:objects), a representation of the current state (:init) of the world and the object attributes, and the goal to be achieved (:goal). The goal is derived from the LLM (Section 4.2), while the objects and their attributes are obtained from W (observations) and the beliefs the B has about the objects.</p>
<p>Since B includes possible predicates which are unknown, we sample from B using an LLM to obtain w belief . For instance, our belief could be that (inReceptacle tomato ?x) where ?x can be countertop, cabinet, fridge, etc. Since we want to condition the sampling of where the tomato can appear, we pass the known world state W along with the predicate (in this case inReceptacle) and its options to the LLM.This sampling leverages the LLM to complete a world state and is extendable to any unknown predicate from which a set of beliefs can be deduced. We also compare LLM sampling with random sampling (llmdp-random).</p>
<p>We describe our likely world state as the union between a sampled set of beliefs and the known world state w belief W. Then sampling i = 1, .., N different sets of beliefs during the planning loop, we obtain N likely world states. Finally, we convert each likely world state to lists of predicates to interface with the PDDL planner.</p>
<p>Plan Generator</p>
<p>Upon constructing the different PDDL problems, the agent uses a Plan Generator (PG) to solve each problem and obtain a plan. We use the BFS(f) solver (Lipovetzky et al., 2014) implemented as an executable by LAPKT (Ramirez et al., 2015). A generated plan is a sequence of actions, where each action is represented in a symbolic form, which, if executed, would lead to the goal state from the initial state.</p>
<p>Action Selector</p>
<p>The Action Selector (AS) module decides the agent's immediate next action. It takes the planner's output, a set of plans, and selects an action from them. In our Alfworld experiments, the Action Selector simply selects the shortest plan returned. If no valid plans are returned, all sampled states were satisfying goal states, there is a mistake with the constructed domain/problem files, or the planner has failed to find a path to the goal. In the first case, we re-sample random world states and re-run the planners once.</p>
<p>We also propose exploring different strategies when valid plans cannot be found. For instance, similarly to self-reflection (Shinn et al., 2023), the Action Selector could prompt an update in the agent's belief about the world state if none of generated problem descriptions are solvable. The Action Selector could also interact with a human teacher or oracle to adjust its understanding of the environment (problem) or its logic (domain).</p>
<p>Observation Processing</p>
<p>LLM-DP uses the result of each action to update its internal state representation. It uses the symbolic effects of the action to infer changes in the state of the objects and receptacles. Then it integrates the information from the new observation, which might reveal additional details not directly inferred from the action itself. For instance, opening an unseen drawer might reveal new objects inside. Observing also updates the beliefs -if an object is observed at a location, it cannot be elsewhere, but if an object is not observed at a location, it cannot be there. Observations incorporate beliefs into W.</p>
<p>If the agent detects new information from the scene -such as discovering new objects -it triggers a re-planning process. The agent then generates a new set of possible PDDL problems using the updated state representation and corresponding plans using the Plan Generator. This approach is similar to some Task and Motion Planning (TAMP) methods (Garrett et al., 2018;, enabling the agent to adapt to environmental changes and unexpected outcomes of actions.</p>
<p>Results</p>
<p>We contrast the LLM-DP approach with ReAct (LLM-only baseline) from the original implementation by Yao et al. (2023). Since we use a different backbone LLM model (gpt-3.5-turbo rather than text-davinci-002) than the ReAct baseline for cost purposes, we also reproduce their results using gpt-3.5-turbo and adapt the ReAct prompts to a chat format.</p>
<p>As shown in Table 1, LLM-DP solves Alfworld almost perfectly (96%) compared to our baseline reproduction of ReAct (53%). The LLM-DP can translate the task description into an executable PDDL goal 97% of the time, but sampling reduces the accuracy further when it fails to select a valid set of possible world states -for instance, by sampling states where the goal is already satisfied.</p>
<p>We note, that the ReAct baseline makes different assumptions about the problem; while it does not require a domain file containing the actiondescriptions and object predicates, it uses two separate human-annotated episodes per example to bootstrap its in-context logic. ReAct also switches out which examples to use in-context based on the type of task, such that two examples of the same type of task being solved are always shown. We also find that our reproduction of ReAct is worse than the original and attribute this to the gpt-3.5-turbo model being more conversational than text-davinci-002, and thus less likely to output valid actions as it favours fluency over following the templated action language.</p>
<p>We also measure the length of each successful episode and find that LLM-DP reaches the goal state faster on average (13.16 actions) versus ReAct (18.69 actions) and a random search strategy (15.02 actions). The Average Episode Length measures the number of actions taken in the environment and how efficient the agent is.</p>
<p>Conclusion</p>
<p>The LLM-DP agent effectively integrates language understanding, symbolic planning, and state tracking in a dynamic environment. It uses the language model to understand tasks and scenes expressed in natural language, constructs and solves planning problems to decide on a course of action, and keeps track of the world state to adapt to changes and make informed decisions. This workflow enables the agent to perform complex tasks in the Alfworld environment, making it a promising approach for embodied tasks that involve language understanding, reasoning, and decision-making.</p>
<p>LLM-DP offers a cost and efficiency trade-off between a wholly symbolic solution and an LLMonly model. The LLM's semantic knowledge of the world is leveraged to translate the problem into PDDL while guiding the search process through belief instantiation. We find that not only is LLM-DP cheaper, on a per-token comparison, but it is also faster and more successful at long-term planning in an embodied environment. LLM-DP validates the need for LLM research to incorporate specialised tools, such as PDDL solvers, in embodied agents to promote valid Despite these promising results, numerous topics and unresolved issues remain open for future investigation. Key among these is devising strategies to encode the world model and belief, currently handled symbolically, and managing uncertain observations -particularly from an image model -along with propagating any uncertainty to the planner and Action Selector. We intentionally kept the Action Selector simple for our experiments, but future work may also explore different strategies to encourage self-reflection within the agent loop. For instance, if all plans prove invalid, beliefs may be updated, or it might indicate an incorrect domain definition. Such instances may necessitate agents to interact with an instructor who can provide insights about action pre-conditions and effects. This direction could lead us from a static domain file towards an agent truly adaptable to new environments, fostering continual learning and adaptation.</p>
<p>SR EL</p>
<p>LLM-DP (n=3) 0.96 13.16 LLM-DP (n=3) -fallback 0.92 12.80 LLM-DP (n=5) 0.96 12.54 LLM-DP (n=5) -fallback 0.94 12.24 Table 2: We compare the average Success Rate (SR) and average Episode Length (EL) for different sampling sizes n and with or without a fallback to random sampling. The random sampling fallback affects the success rate as the LLM sampler can more often sample n world states which are already satisfied. However as n increases, we see that it becomes more likely for the sampling procedure to at find at least one plan, and therefore the SR increases when no fallback (-fallback) is used.</p>
<p>A Prompts and Few-shot details</p>
<p>See Table 3 and Table 4 for LLM-DP prompts used.</p>
<p>B ReAct</p>
<p>B.1 Reproduction with Chat Model</p>
<p>We slightly modify the 'system' prompt of the original ReAct (see Table 5) to guide the model away from its conversational tendencies. gpt-3.5-turbo apologises significantly more than the text-davinci-002 model, and we found that it would often get stuck in loops of apologising. We also modify the code so that we replace all generated instances of 'in' and 'on' with 'in/on' if the model did not generate it correctly, since Alfworld expects 'in/on' but gpt-3.5-turbo tends to generate only the correct preposition. Without these changes, ReAct would be significantly worse than our reported metric.</p>
<p>C LLM-DP C.1 Generated Goal Examples</p>
<p>See Table 6 for examples of generated goals, both valid and invalid.</p>
<p>C.2 Varying n See Table 6 for results when different varying n and fallback. Fallback is when no plans are sampled successfully through the LLM, LLM-DP resamples n plans randomly.
 AcknowledgementsThis work was supported in part by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1) at the University of Edinburgh, School of Informatics and School of Philosophy, Psychology &amp; Language Sciences and by the UKRI-funded TAS Governance Node (grant number EP/V026607/1).(:predicates (isReceptacle ?o -object); true if the object is a receptacle (atReceptacleLocation ?r -object) ; true if the robot is at the receptacle location (inReceptacle ?o -object ?r -object) ; true if object ?o is in receptacle ?r (openable ?r -object) ; true if a receptacle is openable (opened ?r -object) ; true if a receptacle is opened (isLight ?o -object) ; true if an object is light source (examined ?o -object ?l -object) ; whether the object has been looked at with light (holds ?o -object) ; object ?o is held by robot (isClean ?o -object) ; true if the object has been cleaned in sink (isHot ?o -object) ; true if the object has been heated up (isCool ?o -object) ; true if the object has been cooled (isSink ?o -object) ; true if the object is a sink (isMicrowave ?o -object) ; true if the object is a microwave (isFridge ?o -object) ; true if the object is a fridge ))Table 3: System Prompt used by gpt-3.5-turbo for generating the :goal in LLM-DP Your task is to: put a clean plate in microwave. (:goal (exists (?t -plate ?r -microwave) (and (inReceptacle ?t ?r) (isClean ?t) ))) Your task is to: examine an alarmclock with the desklamp", (:goal (exists (?t -alarmclock ?l -desklamp) (and (examined ?t ?l) (holds ?t) ))) Your task is to: put two cellphone in bed (:goal (exists (?t1 -cellphone ?t2 -cellphone ?r -bed) (and (inReceptacle ?t1 ?r) (inReceptacle ?t2 ?r) (not (= ?t1 ?t2)) )))Table 4: Fixed Few-shot examples used by gpt-3.5-turbo for generating the :goal in LLM-DP Interact with a household to solve a task. Only reply with &gt; followed by the action to take or 'think'. Do not apologize. Follow the format of the two examples below.Table 5: System Prompt used by gpt-3.5-turbo in our reproduction of ReAct task: put some peppershaker on drawer. Generated: (:goal (exists (?t -peppershaker ?r -drawer) (inReceptacle ?t ?r) )) VALID ✓ task: put a clean mug in coffeemachine. Generated: (:goal (exists (?t -mug ?r -coffeemachine) (and (inReceptacle ?t ?r) (isClean ?t) ))) VALID ✓ task: put two cd in safe. Generated: (:goal (exists (?t1 -cd ?t2 -cd ?r -safe) (and (inReceptacle ?t1 ?r) (inReceptacle ?t2 ?r) (not (= ?t1 ?t2)) ))) VALID ✓ task: heat some mug and put it in coffeemachine. Generated:(:goal (exists (?m -mug ?c -coffeemachine)(and (isReceptacle ?m) (isHot ?m) (inReceptacle ?m ?c) ))) INVALID ✗Table 6: Sample of generated PDDL goals from LLM-DP. The generation gets confused by the semantics of 'receptacle' and identifies a mug as a receptacle. While it is true that a mug is a receptacle, in our defined logic, receptacles are fixed, immovable objects which can contain other objects and therefore, a mug is not a Receptacle which leads the planning to fail subsequently.
Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPSvirtualTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Ad- vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process- ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Meta-learning via language model in-context tuning. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, He He, 10.18653/v1/2022.acl-long.53Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandLong Papers1Association for Computational LinguisticsYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 719-730, Dublin, Ireland. Association for Computational Lin- guistics.</p>
<p>Autotamp: Autoregressive task and motion planning with llms as translators and checkers. Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas A Roy, Chuchu Fan, abs/2306.06531ArXiv. Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas A. Roy, and Chuchu Fan. 2023. Autotamp: Autoregres- sive task and motion planning with llms as translators and checkers. ArXiv, abs/2306.06531.</p>
<p>Strips: A new approach to the application of theorem proving to problem solving. Richard E Fikes, Nils J Nilsson, 10.1016/0004-3702(71)90010-5Artificial Intelligence. 23Richard E. Fikes and Nils J. Nilsson. 1971. Strips: A new approach to the application of theorem proving to problem solving. Artificial Intelligence, 2(3):189- 208.</p>
<p>Pddlstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning. Caelan Reed Garrett, Tomas Lozano-Perez, Leslie Pack Kaelbling, International Conference on Automated Planning and Scheduling. Caelan Reed Garrett, Tomas Lozano-Perez, and Leslie Pack Kaelbling. 2018. Pddlstream: Integrat- ing symbolic planners and blackbox samplers via optimistic adaptive planning. In International Con- ference on Automated Planning and Scheduling.</p>
<p>Reasoning with language model is planning with world model. Shibo Hao, Yilan Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, abs/2305.14992ArXiv. Shibo Hao, Yilan Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. ArXiv, abs/2305.14992.</p>
<p>The FF planning system: Fast plan generation through heuristic search. Jörg Hoffmann, Bernhard Nebel, Journal of Artificial Intelligence Research. 14Jörg Hoffmann and Bernhard Nebel. 2001. The FF plan- ning system: Fast plan generation through heuristic search. Journal of Artificial Intelligence Research, 14:253-302.</p>
<p>Instruction induction: From few examples to natural language task descriptions. Or Honovich, Uri Shaham, Samuel R Bowman, Omer Levy, abs/2205.10782ArXiv. Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. 2022. Instruction induction: From few examples to natural language task descriptions. ArXiv, abs/2205.10782.</p>
<p>Chatdb: Augmenting llms with databases as their symbolic memory. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Jake Zhao, Hang Zhao, ArXiv, abs/2306.03901Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Jake Zhao, and Hang Zhao. 2023. Chatdb: Augmenting llms with databases as their symbolic memory. ArXiv, abs/2306.03901.</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Wenliang Dai, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Wenliang Dai, Andrea Madotto, and Pascale Fung. 2022. Survey of hallucination in natural language generation. ACM Computing Surveys, 55:1 -38.</p>
<p>Width and inference based planners: Siw, bfs (f), and probe. Nir Lipovetzky, Miquel Ramirez, Christian Muise, Hector Geffner, Proceedings of the 8th International Planning Competition (IPC-2014). the 8th International Planning Competition (IPC-2014)43Nir Lipovetzky, Miquel Ramirez, Christian Muise, and Hector Geffner. 2014. Width and inference based planners: Siw, bfs (f), and probe. Proceedings of the 8th International Planning Competition (IPC-2014), page 43.</p>
<p>Llm+p: Empowering large language models with optimal planning proficiency. B Liu, Yuqian Jiang, Xiaohan Zhang, Qian Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, abs/2304.11477ArXiv. B. Liu, Yuqian Jiang, Xiaohan Zhang, Qian Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023. Llm+p: Empowering large language models with op- timal planning proficiency. ArXiv, abs/2304.11477.</p>
<p>The 1998 ai planning systems competition. Drew Mcdermott, AI Magazine21Drew McDermott. 2000. The 1998 ai planning systems competition. AI Magazine, 21(2):35-55.</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Conference on Empirical Methods in Natural Language Processing. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle- moyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Confer- ence on Empirical Methods in Natural Language Processing.</p>
<p>10.48550/arXiv.2303.08774arXiv:2303.08774OpenAI. 2023. Gpt-4 technical report. arXiv preprintOpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Computation and Language (cs.CL);</p>
<p>. Artificial Intelligence (cs.AI). Artificial Intelligence (cs.AI).</p>
<p>G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. arXiv preprintShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334.</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Lidén, Zhou Yu, Weizhu Chen, Jianfeng Gao, ArXiv, abs/2302.12813Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Lidén, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feed- back. ArXiv, abs/2302.12813.</p>
<p>Miquel Ramirez, Nir Lipovetzky, Christian Muise, Lightweight Automated Planning ToolKiT. 2020Miquel Ramirez, Nir Lipovetzky, and Christian Muise. 2015. Lightweight Automated Planning ToolKiT. http://lapkt.org/. Accessed: 2020.</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, abs/2302.04761ArXiv. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761.</p>
<p>Reflexion: An autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.11366arXiv preprintNoah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: An autonomous agent with dy- namic memory and self-reflection. arXiv preprint arXiv:2303.11366.</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew J Hausknecht, abs/2010.03768CoRRMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. 2020. Alfworld: Aligning text and em- bodied environments for interactive learning. CoRR, abs/2010.03768.</p>
<p>An experimental opensource attempt to make gpt-4 fully autonomous. Significant-Gravitas, Significant-Gravitas. 2023. An experimental open- source attempt to make gpt-4 fully autonomous. https://github.com/significant-gravitas/ auto-gpt. Accessed: 2023-06-09.</p>
<p>Pddl planning with pretrained large language models. Tom Silver, Varun Hariprasad, S Reece, Nishanth Shuttleworth, Tomás Kumar, Leslie Pack Lozano-Pérez, Kaelbling, NeurIPS 2022 Foundation Models for Decision Making Workshop. Tom Silver, Varun Hariprasad, Reece S Shuttle- worth, Nishanth Kumar, Tomás Lozano-Pérez, and Leslie Pack Kaelbling. 2022. Pddl planning with pretrained large language models. In NeurIPS 2022 Foundation Models for Decision Making Workshop.</p>
<p>Vipergpt: Visual inference via python execution for reasoning. Dídac Surís, Sachit Menon, Carl Vondrick, abs/2303.08128ArXiv. Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning. ArXiv, abs/2303.08128.</p>
<p>Linxi (Jim) Fan, and Anima Anandkumar. 2023a. Voyager: An openended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, abs/2305.16291ArXiv. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man- dlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan, and Anima Anandkumar. 2023a. Voyager: An open- ended embodied agent with large language models. ArXiv, abs/2305.16291.</p>
<p>Selfconsistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, International Conference on Learning Representations. Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. 2023bICLRXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. 2023b. Self- consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations (ICLR).</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, NeurIPS. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompt- ing elicits reasoning in large language models. In NeurIPS.</p>
<p>Coupling large language models with logic programming for robust and general reasoning from text. Zhun Yang, Adam Ishay, Joohyung Lee, 10.18653/v1/2023.findings-acl.321Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational LinguisticsZhun Yang, Adam Ishay, and Joohyung Lee. 2023. Cou- pling large language models with logic programming for robust and general reasoning from text. In Find- ings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 5186-5219. Association for Computational Linguis- tics.</p>
<p>ReAct: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations. ICLRShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR).</p>
<p>Ppddl1. 0: An extension to pddl for expressing planning domains with probabilistic effects. L S Håkan, Younes, Michael L Littman, CMU-CS-04-162299Techn. Rep.domain alfredHåkan LS Younes and Michael L Littman. 2004. Ppddl1. 0: An extension to pddl for expressing plan- ning domains with probabilistic effects. Techn. Rep. CMU-CS-04-162, 2:99. (define (domain alfred)</p>            </div>
        </div>

    </div>
</body>
</html>
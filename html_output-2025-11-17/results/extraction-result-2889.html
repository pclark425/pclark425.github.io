<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2889 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2889</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2889</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-277349601</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.21557v1.pdf" target="_blank">debug-gym: A Text-Based Environment for Interactive Debugging</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are increasingly relied upon for coding tasks, yet in most scenarios it is assumed that all relevant information can be either accessed in context or matches their training data. We posit that LLMs can benefit from the ability to interactively explore a codebase to gather the information relevant to their task. To achieve this, we present a textual environment, namely debug-gym, for developing LLM-based agents in an interactive coding setting. Our environment is lightweight and provides a preset of useful tools, such as a Python debugger (pdb), designed to facilitate an LLM-based agent's interactive debugging. Beyond coding and debugging tasks, this approach can be generalized to other tasks that would benefit from information-seeking behavior by an LLM agent.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>TextWorld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>ScienceWorld: Is your agent smarter than a 5th grader? <em>(Rating: 2)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game <em>(Rating: 1)</em></li>
                <li>LIGHT: A grounded dialogue dataset for interactive fiction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2889",
    "paper_id": "paper-277349601",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "TextWorld: A learning environment for text-based games",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "ScienceWorld: Is your agent smarter than a 5th grader?",
            "rating": 2,
            "sanitized_title": "scienceworld_is_your_agent_smarter_than_a_5th_grader"
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game",
            "rating": 1,
            "sanitized_title": "learning_to_speak_and_act_in_a_fantasy_text_adventure_game"
        },
        {
            "paper_title": "LIGHT: A grounded dialogue dataset for interactive fiction",
            "rating": 1,
            "sanitized_title": "light_a_grounded_dialogue_dataset_for_interactive_fiction"
        }
    ],
    "cost": 0.008598749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>debug-gym: A Text-Based Environment for Interactive Debugging
27 Mar 2025</p>
<p>Xingdi Yuan 
McGill University ♣ Mila
Université de Montréal</p>
<p>Morgane M Moss 
McGill University ♣ Mila
Université de Montréal</p>
<p>Charbel El 
McGill University ♣ Mila
Université de Montréal</p>
<p>Darya Moldavskaya 
McGill University ♣ Mila
Université de Montréal</p>
<p>♡ Drew 
McGill University ♣ Mila
Université de Montréal</p>
<p>Lucas Caccia 
McGill University ♣ Mila
Université de Montréal</p>
<p>Matheus Pereira 
McGill University ♣ Mila
Université de Montréal</p>
<p>♠ Minseon 
McGill University ♣ Mila
Université de Montréal</p>
<p>Alessandro Sordoni 
McGill University ♣ Mila
Université de Montréal</p>
<p>Marc-Alexandre Côté 
McGill University ♣ Mila
Université de Montréal</p>
<p>Microsoft Research 
McGill University ♣ Mila
Université de Montréal</p>
<p>Montréal ♡ Microsoft 
McGill University ♣ Mila
Université de Montréal</p>
<p>Research Nyc 
McGill University ♣ Mila
Université de Montréal</p>
<p>debug-gym: A Text-Based Environment for Interactive Debugging
27 Mar 20258BA2E7C71ED370A524E74F316C6AD694arXiv:2503.21557v1[cs.AI]
Large Language Models (LLMs) are increasingly relied upon for coding tasks, yet in most scenarios it is assumed that all relevant information can be either accessed in context or matches their training data.We posit that LLMs can benefit from the ability to interactively explore a codebase to gather the information relevant to their task.To achieve this, we present a textual environment, namely debug-gym, for developing LLM-based agents in an interactive coding setting.Our environment is lightweight and provides a preset of useful tools, such as a Python debugger (pdb), designed to facilitate an LLM-based agent's interactive debugging.Beyond coding and debugging tasks, this approach can be generalized to other tasks that would benefit from information-seeking behavior by an LLM agent.* Equal contribution.Full authorship contribution statements are listed in Appendix A. 2 https://github.com/features/copilot 3https://www.cursor.com/We open source debug-gym at aka.ms/debug-gym.We welcome discussions via GitHub issues.</p>
<p>Introduction</p>
<p>Program synthesis, or code generation, is the task in which a system generates a program based on artifacts that establish semantic and syntactic requirements for the generated code [65].Such artifacts typically consist of natural language descriptions associated with a set of test cases, i.e., a system is required to generate a program that fulfills the natural language descriptions, and can pass the test cases [11,27,23,35].Large Language Model (LLM)-based systems have shown great promise in code generation tasks, where emergent abilities such as prompting and in-context learning have pushed the boundaries of this research area, from single function code generation [11,27] to more realistic repository-level code generation [82,37,80,9].The advancement of code generation technology has further empowered AI coding assistants such as GitHub Copilot 2 and Cursor 3 , where developers can delegate trivial and repetitive coding tasks, focusing instead on problem solving, collaboration, and creativity.</p>
<p>Recent years have seen a flourishing of code generation systems taking a neuro-symbolic approach [74,78,76].To solve realistic coding tasks, such systems often leverage multiple expert modules designed to tackle different subtasks [6].For instance, code and text understanding modules [7,2,47,85] can be used to convert the repository and related documentations into representations that the code generator module [51,73] can better condition on; code-repairing modules can detect and correct potential bugs in the generated code snippets; test generation modules [31,4,61] can generate necessary test cases to validate the generated code against certain criteria (e.g., functionality, In most existing approaches (shown in black), an agent rewrites its code conditioned on error messages obtained from executing the code.debug-gym equips the agent with additional tools such as pdb (shown in red), so it can interactively seek necessary information from the semantic space hidden behind the code, and therefore have better code-repairing performance.efficiency).Among these, we concern ourselves in this paper with improving AI-powered coderepairing, a capability that is still not attained [52].We believe the elusiveness of code-repairing capability is a bottleneck preventing AI coding systems from producing desirable code, especially when there is an inevitable distribution gap between the system's training data and the user's specific scenarios (i.e., the system cannot generate the correct code in one-shot).</p>
<p>Most LLM-based code-repairing systems rely on execution feedback [45,83,36,12].Given a piece of buggy code, they execute it (e.g., with a Python interpreter) and obtain some error message.Conditioned on this message, the system rewrites the code to fix the bugs.This loop is iterated until the error message is empty, or the agent has exhausted some pre-defined budget (measured in steps or tokens) [23].While this iterative approach improves repair performance, it might fail when bugs appear in complex real-world software projects, where the error messages can be nested or non-crashing, making them harder to detect and interpret.[88] suggest that this may due to LLM-based system's inability to accurately track variable values and predict execution flow.To address this limitation, [12] and [30] take inspiration from 'rubber duck debugging', a technique where human developers explain code to a rubber duck to clarify their own understanding, or to guide agents to generate print() functions to help investigate the variable values.</p>
<p>In addition to talk to a rubber duck friend, or to insert arbitrary numbers of print() calls into the code, expert developers also rely on interactive debugging tools that are specifically designed to assist in debugging.In the Python programming language, pdb4 is such a tool.pdb allows users to navigate the codebase through breakpoints and other granular stepping functions, they can inspect stack frames, list source code chunks of interest, and execute arbitrary Python code in the context of any stack frame.This enables developers to verify their hypothesis about their code's underlying logic, and thus gain a much more comprehensive understanding of potential bugs.A natural research question we ask is:</p>
<p>to what degree can LLMs use interactive debugging tools such as pdb?</p>
<p>To help answer the question, this technical report describes debug-gym, an interactive coding environment that allows code-repairing agents to access a collection of tools designed to support active information-seeking behavior [24,8], such as pdb.As depicted in Figure 1, debug-gym expands a debugging agent's action space with a toolbox, which consequently expands the agent's observation space with feedback messages returned from using a tool.The toolbox is designed to facilitate debugging: for example, the agent can make use of the Python debugger pdb to set breakpoints, navigate the code space, print variable values, and even create test functions on the fly.At each step, the agent can either decide to interact with a tool to further investigate the code and gather necessary information, or perform a code rewrite if it is confident in doing so.We believe that interactive debugging (with proper tools) can empower coding agents to tackle real-world software engineering tasks.We also propose that teaching agents to interactively seek information using tools is central to LLM-based agent research in general.debug-gym is designed and developed to:</p>
<ol>
<li>handle repository-level information: the full repository is available to agents in debug-gym, allowing them to navigate and modify files.2. be robust and safe: to safeguard both the system and the development process, debug-gym runs code within Docker containers.This isolates the runtime environment, preventing harmful actions while still permitting thorough testing and debugging.3. be easily extensible: debug-gym was conceived with extensibility in mind and provides practitioners with the possibility of easily adding new tools.4. be text-based: debug-gym represents observation information in structured text (e.g., JSON format) and defines a simple syntax for text actions, making the environment fully compatible with modern LLM-based agents.</li>
</ol>
<p>Scope of this technical report</p>
<p>With this technical report, we open-source the debug-gym environment to the research community to foster future research in this area.The goal of this technical report is not to propose any new algorithms, but rather to demonstrate an alternative path to tackle the difficult code-repairing problem and to encourage the community to push towards solving it.</p>
<p>We structure this technical report in four sections:</p>
<p>• In Section 2, we provide details on the design of debug-gym.We formally define the interactive debugging problem, then elaborate how the environment, the toolbox, and an agent interact with each other.Finally, we provide instructions on how users could design their own tools that can be used in the debug-gym framework.This section is meant to serve as a complementary user manual to the README file in our repository.• In Section 3, we introduce three LLM-based agents that have minimal architecture design and prompt engineering.We provide some preliminary experimental results to help understand these agents' behavior in interactive debugging environments.We analyze the experimental results and highlight the challenges faced by LLM-based agents using interactive tools.• In Section 4, we discuss the limitation of the current version of debug-gym, and propose a few promising future directions for both the framework itself and the research it enables.• In Section 5, we conduct a literature review on topics related to LLM-based coding and debugging agents, as well as existing language-based interactive environments that facilitate such research.</p>
<p>The debug-gym Environment</p>
<p>In this section, we first provide a formalization of the interactive debugging process (Section 2.1), followed by an overview of the system design of the debug-gym environment (Section 2.2).Then, we describe debug-gym's pseudo terminal and its safety properties (Section 2.3).Finally, we introduce debug-gym's design of toolbox in Section 2.4, including the preset of tools we provide with debug-gym (Section 2.4.1), and an instruction on how users can define their own tools in debug-gym (Section 2.4.2).</p>
<p>Problem Setup</p>
<p>We consider interactive debugging problems where an agent needs to iteratively interact with an environment, use various tools provided by the environment to investigate a buggy codebase, and eventually patch the code to fix the bugs.We formalize these problems as Partially Observable Markov Decision Processes (POMDPs) [38], defined by a 6-tuple (S, A, O, Ω, T , R).We use S to denote the state space, A to denote the action space, and O the observation space.Ω ∶ S → O is the observation function, T ∶ S × A → S is the transition function, and R is the reward function.Each debugging problem (data point) consists a piece of buggy code c that aims to fulfill a natural language problem description x, and a set of test cases U. Let C be the set of valid implementations of x that    State space S: debug-gym uses a modular design to ensure its generality and extensibility (to be detailed in Section 2.2).Consequently, the state space S, the observation space O, and the action space A can also be decomposed in a modular manner.Specifically, the state space can be decomposed as S = S env ∪ S tool , where S env denotes general environment states such as the code in the repository, the directory structure, the set of test cases U, local variables and their values during execution, and optionally ground-truth solutions.
S tool = |tool| ⋃ i=1 S i
tool denotes the union of states specific to each tool.For example with the pdb tool, this may be all of the breakpoints currently being activated.</p>
<p>Observation space O:</p>
<p>The environment is partially observable, an agent has only a limited view of the potentially huge state.This is due to the fact that it is unrealistic for either human developers or an LLM to include all the available information (e.g., a large repository) into its view / context.More importantly, the code itself represents an even larger hidden space, including the values of the variables at every step, the control flow the code builds, and the higher level semantics such as algorithms the code implements.Similar to the state space S, the observation space O can also be decomposed into O = O env ∪ O tool , where each individual tool could enrich the agent's observation in some way, i.e.,
O tool = |tool| ⋃ i=1 O i tool .
Through interacting with the environment by utilizing the tools in a proper way (performing information-seeking behavior), the agent should be able to shift its belief from a partial understanding of the codebase towards the true state and thus more easily solve the task.</p>
<p>Action space A: Based on the needs of solving a particular task, the user can specify a set of tools to be available.At every step, an agent interacts with debug-gym via calling a tool from the set of available tools.Each individual tool has its own syntax, such syntax defines the action space of that tool.For example, the rewrite tool requires a set of arguments such as the file path, the line number to edit, as well as the new content to be written in that position.The overall action space A is the union of the action spaces over all available tools, i.e.,
A = A tool = |tool| ⋃ i=1 A i tool .
It is worth noting that debug-gym is a text-based environment, all observations and actions are represented in strings.If an action string is failed to be parsed, debug-gym will return an error message of SyntaxError.</p>
<p>Reward function R:</p>
<p>The reward function R ∶ S → {0, 1} is determined by whether a rewritten code could pass all the test cases U.At time step t, an agent rewrites the code as c t , it receives a reward:
r t = R(s t , a t ) = { 1 if c t ∈ C, 0 otherwise.(1)
In which, C is the set of valid code that can pass all test cases in U. Note it is often the case where there exists multiple valid solutions and thus comparing c t with the ground-truth solution c * (if available) is less applicable.In case of absence of the test cases, i.e., U = ∅, the reward is by default the successfulness of running the code c t without error message.An episode is terminated when either an agent has received a positive reward (success), or it has exhausted a pre-defined budget, in the form of the number of steps or the number of consumed tokens (out of budget).</p>
<p>System Design</p>
<p>Environment.debug-gym is a Python library, which essentially encompasses the interaction loop between an agent and a repository-specific environment.As demonstrated in Figure 2, the environment is an encapsulation of an interactive terminal, a set of tools, a code repository, and optionally a set of test cases to evaluate the correctness of the code repository.In which, debug-gym provides the terminal and a preset of tools, the users are required to specify the code repositories they want to investigate, as well as the test cases if applicable.To provide a generic interface for developing and training various types of agent systems, the debug-gym environment adopts an API style similar to the Gym [10] and Gymnasium [70], which have been widely used in the development of modern RL agents.Specifically, an env.reset() function is used to start an episode, whereby the environment returns the initial observation o 0 ; an env.step() function is used to submit an action a t , the environment subsequently returns the new observation o t+1 and the reward r t .A simplified example of such agent-environment interaction is as follows, here we assume a prompt-based agent: When building the environment, debug-gym will copy the user-specified repository into a temporary working directory, which is by default mapped into a Docker container.While debug-gym also supports the option of running the terminal in the user's local machine, which could be faster, we highly recommend users to use Docker as a safeguard which guarantees the actions (e.g., generated by an LLM) do not modify the user's file system in any unexpected way.Large code repositories often contain transient files such as temporary files created by IDEs and compilation products, users can include a .debugignorefile in their repository, following the same syntax as .gitignore, to remove specific files and directories from debug-gym's scope.Users can additionally specify read only files or directories using a .debugreadonlyfile, in such cases, the specified files and directories are read only to an agent through debug-gym.This is especially useful for repositories that come with test cases, the read only setting can prevent agents from developing cheating strategies such as to pass the tests by editing or deleting them.</p>
<p>Tools.Tools grant the agent a diverse and coherently functional action space with which to facilitate the debugging process.For example, the view tool sets the current working file, and displays its contents in the observation text.The rewrite tool can be used to rewrite all or a portion of the current working file, for example in an attempt to fix a bug in that file.It can also rewrite a file not in the current view, by specifying a file path.The pdb tool interfaces the agent with the full suite of pdb commands that can ordinarily be used in the terminal, allowing the agent to insert breakpoints, inspect local variables, and so on.Tools are, in this way, highly modular, and users can introduce their own custom tools to debug-gym.Each tool can have its own syntax depending on the functionality of the tool.On top of which, we design a minimal meta-syntax in order for the environment to parse and activate a certain tool at a certain step.The meta-syntax adopts the triple-backtick delimiter ``f rom Markdown.For example: ```pdb cl src/code.py:26 Agent.An agent is a system provided by the user, its implementation encompasses the user's design decisions about how the system should interact with debug-gym to perform the debugging task.</p>
<p>Although the majority of this technical report assumes an LLM-based agent, the implementation of an agent can take many different forms, including rule-based programs, LLM-based chatbots, or even systems that have humans-in-the-loop.In any case, at step t, the agent takes the observation o t as input and generates an action a t as output (as shown in Figure 2).Crucially, the agent contains a run() method, which allows the agent to interface with the environment to solve the task, and therefore the environment is mounted onto the agent instance as an attribute (this is automatically taken care of by the canonical run.pyscript provided).Customizing this method enables the user to experiment with different ways for the agent to respond to a given observation.In debug-gym, we provide a starter kit that includes three minimal LLM-based agent implementations (described in Section 3.2), serving the purpose of demonstrating debug-gym's API.The users can inherit from the provided base agents to design and build their novel and more complex debugging agents.We provide an example of an agent performs interactive debugging using debug-gym in Figure 3.</p>
<p>Terminal</p>
<p>The terminal provides a way to execute commands from the temporary working directory.For instance, the terminal is used to run the codebase's entrypoint (e.g., pytest tests.py) to get observations and measure progress, and the debug_entrypoint (e.g., python -m pdb -m pytest tests.py) to start a debugging shell session as used by the pdb tool (see Section 2.4 below for more details about tools).</p>
<p>From a terminal instance, users can execute a standalone command and wait for their output to be returned once done.Users can also start shell sessions that allow interacting with any command line program that requires multiple inputs throughout its execution (e.g., while debugging).</p>
<p>debug-gym provides two types of terminal: a local one and one running on Docker.While the local terminal is faster to start, it also exposes the user's host filesystem which is unsafe.On the other hand, Docker provides isolation and the ability to easily manage different problems that may require different package requirements.Upon opening a Docker terminal, the temporary working directory is mounted as a Docker volume allowing to make changes to the codebase locally from the host.The docker terminal is configurable, allowing users to specify the base image from which docker container is initialized, environment variables, setup commands to install dependencies, and session commands that are executed whenever a shell session starts (similar to a .bashrcfile).</p>
<p>(a) A piece of buggy code and the error message returned from a terminal when attempting to run the code.</p>
<p>(b) Debugging interactions in debug-gym, between the environment (in black) and the agent (in red).In this example, the agent uses debug-gym to investigate the columns of the downloaded data frame, and figures out the correct column name is prix instead of price .This information is unavailable by solely reading the code and the error message, as done by most conventional code-repairing systems.</p>
<p>Toolbox</p>
<p>The toolbox serves as a registry to keep a record of existing tools.Tools are callable artifacts (e.g., functions) that are designed to accomplish various sub-tasks an agent may encounter while debugging.The tools can either be called in a single-turn manner (e.g., issuing a rewrite action will change certain part of the code repository right away) or in a multi-turn manner (e.g., an agent needs to interact with the pdb tool for multiple steps in a row to investigate the code).In both cases, a tool behaves like a mini environment: it takes an action as input, after executing its internal function, it returns a new observation as output.In which, the internal function can modify the environment state s env ∈ S env (e.g., editing the code via rewrite), or the state specific to the tool s tool ∈ S tool (e.g., removing a breakpoint via pdb).Note that the internal function does not have to be a Python function, it can essentially be any function F ∶ input → output, such as an external calculator or a parser.</p>
<p>Each tool is associated with a unique template that is used for the environment to parse, identify, and activate a tool given an action.A template is typically the tool's name amid special tokens.For instance, <code>pdb c</code>will be parsed by the environment, the pdb tool will be activated, and the command c will be fed as the input to the tools.To make the tools friendly to LLM-based agents, we also associate each tool with a description string and a list of example strings, both as attributes in the tool class.The description is a short paragraph describing the functionality as well as the syntax of the tool.The example list provides examples and their interpretation in natural language, for example: <code>pdb b 42</code>to set a breakpoint at line 42 in the current file.</p>
<p>While initializing the environment, the user is required to register the set of tools to be used depending on the task requirement.</p>
<p>Built-in Tools</p>
<p>We provide a preset of tools in the toolbox:</p>
<ol>
<li>eval: This tool can be triggered to run the entrypoint associated with the repository, which evaluates the current code in the repository.When a set of test cases are provided with the repository, the entrypoint can be invoking the pytest package (e.g., "python -m pytest -sv .").When the test cases are not provided, the entrypoint can be executing the code (e.g., "python main.py").This tool will return a reward of 1 and terminates the current debugging episode if there is no error detected, and a reward of 0 otherwise.By default, this tool is automatically triggered after a rewrite action, but this can be modified by the user through configuration.2. view: This tool takes a valid file path in the repository as input, it switches and displays the contents of the specified file in the environment's observation, and sets that file as the environment's current working file.Like in many IDEs, we provide line numbers in front of each line of the observing code, this can make the referencing (e.g., when rewriting a line) easier.Although there are possible ways to manipulate a file outside the current view (e.g., by specifying a path in rewrite and pdb actions), working with a file while observing its content is the most natural way of debugging.3. pdb: This tool is a wrapper that acts as a direct interface between the agent and the full suite of pdb commands available to the original Python debugger, such as b(reak), cl(ear), s(tep), n(ext), c(ontinue), and p(rint).An agent can directly manipulate the breakpoints in the current working file (specified with view), it can optionally manipulate breakpoints in all visible files in the repository by specifying the file name and its path.Like in many IDEs, once a breakpoint has been set, it is not reset when terminal session ends and restarts (e.g., when issuing a continue command at the last breakpoint).We keep a list of all breakpoints and restore them when there is a terminal session restarts.A breakpoint can be removed upon a c(lear) command, or when an episode terminates.Optionally, the user can deactivate this feature in the configuration file, so the list of breakpoints does not persist when terminal session restarts.4. rewrite: This tool aims to enable an agent to directly modify the code in a file in the repository.</li>
</ol>
<p>In comparison to typical code rewriting strategies that rewrite the entire file (because many widely-used datasets target relatively short function-level code generating [11,22]), we design our rewriting tool specifically for dealing with debugging in realistic settings where the code file could contain a few hundred lines and thus rewriting the entire file is suboptimal.Specifically, our rewrite tool takes a file path, a pair of start and end line numbers, and the new content as input; it will replace the code between the start and end lines in the file with the new content.When the end line number is absent, it will replace the code in the start line with the provided content.</p>
<p>When both line numbers are absent, it will perform in an identical manner as patchers that rewrite the entire file. 5. listdir: This tool can be used to show the directory tree at arbitrary depths.The tool is especially useful when an agent is navigating very large codebases, where it is not feasible to display the complete directory tree in the agent's observation.Instead, the agent can call this tool to view the files and directories at a given root.For example <code>listdir src/utils 2</code>will list the contents of the src/utils subdirectory up to a depth of 2. When the root directory is absent, the tool will use the current working directory (i.e., ./).The depth of the displayed directory tree can also be defined in the environment configuration so the user can set a default value to reflect the depth and complexity of the specific codebase.</p>
<p>Adding New Tools</p>
<p>debug-gym's modular design makes it extensible.Users are encouraged to extend debug-gym to their specific usecases, for example by creating new tools that diversify an agent's action and observation spaces.All tools simply inherit from an EnvironmentTool base class.In order to seamlessly implement and integrate a new tool into the debug-gym system, the tool must have the following properties:</p>
<ol>
<li>
<p>Inherit from EnvironmentTool and make use of the @Toolbox.register()decorator, to make the tool detectable when initializing the environment.</p>
</li>
<li>
<p>A name attribute, which is how the tool is referred to when initializing the environment.</p>
</li>
<li>
<p>An instructions attribute, which is a dictionary containing information about the tool that will be visible to the agent (e.g., in the system prompt of an LLM-based agent).The dictionary consists a template defining the syntax of the tool, a description of the tool's functionality, and a list of examples that illustrate how different tool calls yield different actions.</p>
</li>
<li>
<p>A use() method, which implements the functionalities of the tool.It parses necessary arguments in an action, uses these arguments to compute an output, then returns this output as the agent's new observation.</p>
</li>
</ol>
<p>We use a minimal example to demonstrate the implementation of a new tool.For instance, we want a tool that can serve as a rubber duck [12] with which an agent can summarize and explain its thought process so far.Once the above Python script has been created, the user can import the rubber duck tool in debug_gym/gym/tools/<strong>init</strong>.py.Subsequently, the new tool can be added into the toolbox when running an agent, the new rubber_duck action and its corresponding observation will be merged into debug-gym's action and observation spaces.</p>
<p>3 Experiments and Analysis debug-gym is designed in a way that users can import their own agents to debug their own code repositories.For demonstration purposes, we integrate debug-gym with three coding benchmarks, namely Aider, Mini-nightmare, and SWE-bench.In this section, we will first describe these benchmarks and the evaluation metrics we use to assess agents' interactive debugging ability (Section 3.1).Then, we will introduce three example LLM-based agents, all with minimal prompt design and serve the purpose of demonstrating debug-gym's API (Section 3.2).Subsequently, we report the three agents' performance on the three benchmarks, we conduct various qualitative and quantitative analyses to gain a basic understanding on how LLM-based debugging agents behave in an interactive debugging setting (Section 3.3).Finally, we discuss why our current approach struggles in the interactive debugging setting and propose how to improve a debugging agent using debug-gym (Section 3.4).</p>
<p>Benchmarks and Evaluation Metrics</p>
<p>Benchmarks</p>
<p>With debug-gym, users can specify the path to a folder to work with any custom repository.In addition to working with custom repositories, we integrate three coding benchmarks into debug-gym to quantitatively measure LLM-based agents' performance in interactive debugging.These benchmarks offer distinct task problems: Aider requires simple function-level code generation; Mini-nightmare is a set of short and hand-crafted buggy code examples where an interactive debugger is particularly helpful for human developers; SWE-bench features real-world coding problems which require a more comprehensive understanding of a large codebase and how multiple components work together, the required solution is also in the format of a real GitHub pull request.</p>
<p>Aider [22]: Aider is based on the Exercism/Python5 coding exercises.This benchmark evaluates an agent's ability to translate a natural language task description into executable code that needs to pass a set of pre-defined unit tests.Aider consists of 133 Python programming exercises, each includes a natural language description and a set of unit tests.The solution of an exercise is mostly on the scale of a single Python function.</p>
<p>Mini-nightmare: Mini-nightmare is a set of 10 hand-crafted buggy Python code examples with an average length of 40 lines.The code presents different types of scenarios where human developers would tend to use interactive tools (such as pdb) to assist in the debugging process.Such scenarios include race conditions in multi-threading, complex or unknown data structures, boundary issues, condition coverage, and string management.Each data point is paired with a test file so unit tests can be used to verify the correctness of the code.[37]: SWE-bench is a widely adopted benchmark that tests AI coding systems' ability to solve GitHub issues automatically.The benchmark consists of more than 2,000 issue-pull request pairs from 12 popular Python repositories.Agents are required to read a code repository and the corresponding issue describing the problem, and modify the code so it can pass a set of unit tests specifying preferred post-PR behaviors.In this work, we use SWE-bench-Lite, a curated subset of 300 data points 6 making the evaluation process less costly.It is worth noting that in our default setting, an agent is able to access (in a read-only manner) the test cases defined by the benchmark, which may be different from conventional approaches that tackle the SWE-bench benchmark.We want to emphasize that debug-gym is mainly concerned with fixing bugs in realistic settings, in a way similar to human developers (who also have access to the test cases).That said, users can control an agent's level of access with .debugignoreand .debugreadonlybased on their specific need.</p>
<p>SWE-bench</p>
<p>Evaluation Metrics</p>
<p>Correctness: success rate.Like in other coding systems, we use the success rate of the rewritten code passing a set of test cases as our main evaluation metric.This metric concerns the correctness of the code fix.When there are no test cases provided (e.g., in a custom repository), the success rate simply becomes whether the code can run without any error message.</p>
<p>Efficiency: number of rewrites.Because we frame the debugging process as a sequential decisionmaking process (in Section 2.1), an agent can decide between rewriting the code and investigating the code at every interaction step.Thus, we additionally use the number of rewrites to measure the efficiency of an agent performing interactive debugging.Presumably for certain bugs, interactive debugging should be able to reduce the number of rewrites because the agent could gain additional runtime information that increases its confidence on where the bug might be and how to fix it.</p>
<p>In all experiments we report in this section, we provide the agents an interaction budget of 50 steps, and a rewrite budget of 10 times.An episode is terminated when 1) the code has successfully passed all the test cases; or 2) the agent has exhausted its interaction budget; or 3) the agent has exhausted its rewrite budget (including rewrite failures due to syntax errors in using the rewrite tool.Due to the inevitable stochasticity introduced by LLM models, we run all experiments three times and report the average score and standard deviation.</p>
<p>Example Agents</p>
<p>We provide three example agents with debug-gym to showcase its API, all of which are LLM-based agents with minimal architectural design and prompt engineering.The three agents share the majority of their design, the main difference is in the tools they have access to.</p>
<p>• rewrite agent is a baseline agent that has access to the view, rewrite, and eval tools 7 .The agent can use view to open a particular code file, attempt to fix the bug with rewrite, try to execute the new code with eval to obtain error messages, and loop over until reaching the end of an episode.We include all available information in the agent's system prompt: System Prompt Overall task: Your goal is to debug a Python program to make sure it can pass a set of test functions.You need to propose a rewriting patch to fix the bugs.Avoid rewriting the entire code, focus on the bugs only.Instruction: [Text describing the problem, provided by the dataset, followed by the instruction of each tool made available by the user.]Repo directory tree: [Visible files in the repository, presented in the form of a directory tree.This is controlled by the .debugignoreBased on the instruction, the current code, the last execution output, and the history information, continue your debugging process to propose a patch using rewrite command.Output a single command, nothing else.Do not repeat your previous commands unless they can provide more information.</p>
<p>The agent is expected to generate an action following the syntax as described in Section 2.2, debug-gym will return a syntax error message if it fails to parse the action.</p>
<p>• debug agent is a rewrite agent that additionally has access to the pdb tool.At any step, it can decide to use pdb to investigate the code by printing runtime information (via breakpoints), or to call other tools also available to the rewrite agent.For example, to perform a bug fix using the rewrite tool if it has gathered sufficient information.In this agent, we use a similar prompt as in the rewrite agent, except here we explicitly mention the agent's accessibility to the pdb tool: Based on the instruction, the current code, the last execution output, and the history information, continue your debugging process using pdb commands or to propose a patch using rewrite command.Output a single command, nothing else.Do not repeat your previous commands unless they can provide more information.
System
The pdb tool's instruction is as follows: • debug(5) agent is an agent in between rewrite and debug in the sense that the pdb tool is only made available after its 5 th rewrite attempt.This is motivated by our observation in measuring success rate as a function of the number of rewrites performed by the rewrite agent.As shown in Figure 4, rewrites yield success in Aider, but returns gradually diminish, and the final few rewrites bring virtually no value.In light of this finding, we use the debug(5) agent to leverage the value of early rewrites whilst bolstering with the additional functionality unlocked by pdb.We include a collection of the top performing LLMs, both closed-weights and open-weights ones, as the backbone to the agents.For closed-weights LLMs, we query them through online APIs, for open-weights ones, we host them using vLLM [40].We list the backbone LLMs below:</p>
<ol>
<li>
<p>OpenAI GPT-4o [54]: Closed-weights, 128K context length.</p>
</li>
<li>
<p>OpenAI GPT-4o-mini [53]: Closed-weights, 128K context length.6. Llama-3.2-3B-Instruct[18]: Open-weights, 3B parameters, 128K context length.7. Llama-3.3-70B-Instruct[18]: Open-weights, 70B parameters, 128K context length.</p>
</li>
<li>
<p>DeepSeek-R1-Distill-Llama-70B [16]: Open-weights, 70B parameters, 128K context length.9. DeepSeek-R1-Distill-Qwen-32B [16]: Open-weights, 32B parameters, 128K context length.</p>
</li>
</ol>
<p>Experiment Results</p>
<p>In this subsection, we report experimental results of our agents described in Section 3.2 on the three benchmarks described in Section 3.1.We also conduct qualitative and quantitative analyses on the results to understand these agents' behavior in debug-gym's interactive debugging setting.</p>
<p>Aider</p>
<p>Aider is a code generation benchmark, rather than a code-repairing benchmark.We use it to understand to what extent our agents with different LLM backbones perform on general code generation tasks.We believe code generation to be a prerequisite for more complex tasks such as code-repairing.Thus, performance on Aider is a good initial step, helping us understand whether baselines can first overcome this simpler task.We report results on the Aider benchmark in Table 1.</p>
<p>Llama</p>
<p>DeepSeek R1-Distill 3.2-3B-Instruct 3. Table 1: Results on Aider.We report the average success rate (in percentage) and standard deviation over three runs.</p>
<p>We observe that the 3B Llama model struggles on this benchmark; the larger models can solve more than half of the tasks; the closed-weights models, which are presumably much larger, show the best overall performance.It is also clear that reasoning models (i.e., R1-Distill, o1, and o3-mini) significantly outperform the other models.Particularly, the o3-mini model has nearly achieved a perfect score, and most surprisingly, the R1-Distilled models could outperform GPT-4o with only 70B and 32B parameters.</p>
<p>Comparing among the agents, Table 1 suggests that in the relatively simple (short) code generation task, the access to additional interactive debugging tool does not have clear effect to an agent's performance.Moreover, there is no clear difference between the scores achieved by the debug and debug( 5) agents.This may due to the fact that Aider requires generating code that is relatively straightforward in their underlying logic and thus interactive debugging tools such as pdb would only provide minimal additional information.Another reason could be that our agents with minimal design lacks necessary knowledge of how to properly use the interactive debugging tools to collect useful runtime information.</p>
<p>To gain a deeper understanding, we plot the episode length and the number of response tokens within an episode in success episodes, shown in Figure 5. Comparing between rewrite and debug, we observe that GPT-4o and the Llama models show the most obvious difference in terms of episode length.We plot the distribution over the tools being called by the rewrite and debug agents in Figure 6.</p>
<p>The figure shows that the debug agent tends to call the pdb tool more often when using GPT-4o and Llama models as backbone.We try to further analyze this behavior by plotting the distribution of the debug agent's pdb calls, over major pdb command types, shown in Figure 7.We do not observe an obvious pattern between this figure and Table 1, we tend to believe that because the lack of the knowledge of interactive debugging procedure, the debug agent issues pdb commands without a clear  strategy.Consequently, the information returned from such pdb interactions does not provide the agent extra information given the fact that the code logic in Aider is rather straightforward.</p>
<p>Nonetheless, it is worth mentioning that as shown in Figure 5, the number of response tokens generated by the R1-distill models are significantly higher.This is because their unique reasoning processes.Empirically, we observe agents using these models as backbone tend to generate repeating reasoning chunks via re-thinking (also referred as the "aha moment"), while already achieving reasonable answers in the first few reasoning iterations.While achieving promising performance, this over-thinking issue notably decreases the applicability of such models in sequential decision-making problems like interactive debugging.This is due to the models' slow generation speed and the poor parallelization nature of the sequential decision-making tasks (e.g., computational inefficiency in parallelizing agents with different episode length).This suggests an urge to research that makes LLM's thinking process controllable and light-weighted.</p>
<p>Mini-nightmare</p>
<p>As described in Section 3.1, Mini-nightmare is a collection of ten minimal buggy Python scripts that human developers would find easier to debug if provided with interactive debugging tools.We report the three agents' performance using different LLMs as the backbone on Mini-nightmare in Table 2.We observe a similar pattern as in the Aider experiment results: weaker models struggle to generate valid patches, stronger models can solve all the tasks near perfectly.However, unlike in Aider, we find that in Mini-nightmare, the agents equipped with the pdb tool often outperform the rewrite baseline, this trend is especially clear on agents using GPT-4o, Llama-3.3-70B-Instruct, and R1-Distill-Llama-70B as backbone LLMs.This may hint that the debug and debug(5) agents have actually made a difference and somehow collected useful information that is helpful to solve more tasks.</p>
<p>Figure 6: Distribution of the tools being called by rewrite and debug agents on Aider.In which, "other" includes actions that fail to be parsed by debug-gym (e.g., invalid syntax caused by missing backticks).Note that the zero usage of the listdir tool is because we do not include this tool in the agents' toolbox, due to the simplicity of the repository structure in Aider.To better understand this observation, we break down the average success rate numbers using Figure 8, which depicts the number of successes of each agent out of the three runs.We find a clear gap in success rate between stronger models (i.e., R1-Distill-Llama-70B and Claude 3.7 Sonnet) and weaker models (i.e., GPT-4o and Llama-3.3-70B-Instruct) on the patcher and shopping_cart tasks.We conduct a qualitative analysis on these two tasks.</p>
<p>patcher: Result-wise, patcher is obviously one of the most difficult tasks in Mini-nightmare.Weaker models obtains zero success on debugging this task, even the stronger models fail to consistently solve it.The code in this task is in fact part of the rewrite tool in an early version of debug-gym, and the bug was a real bug the authors encountered when developing it.The patcher takes as input a start and end line number of the code file, as well as the patch that is going to replace the specified lines, it will return the modified code.Adopting common practice in IDEs, the line number of the code file is 1-based.Therefore, before applying the patch, the patcher converts the 1-based numbers into 0-based:
head = int(line_numbers[0]) -1 # 1-based to 0-based tail = int(line_numbers[1]) -1 # 1-based to 0-based
However, the patcher implementation lacks a proper boundary check mechanism to handle input line number that is out of range.For example, the above code will change an input line number of 0 to   −1, Python will interpret this as modifying the last line in the code, which is misaligned with the user's intent.</p>
<p>Agents with both GPT-4o and Llama-3.3-70B-Instructbackbone models, in this particular task, fail to infer the possible cause of the error from pytest failure messages, they keep rewriting the code chunks that parses the input to get the line numbers.On the other hand, agents with both R1-Distill-Llama-70B and Claude 3.7 Sonnet backbone models could immediately realize that the error is related to range issues in line numbers.For instance, we observe the following sentence in the R1-Distill model's reasoning text:</p>
<p>The issue is that the code doesn't validate the line numbers properly.We need to add checks for negative line numbers and invalid ranges.</p>
<p>In a trajectory produced by the debug agent using Claude 3.7 Sonnet as a backbone, we observe a sequence of actions that look very much like a debugging trace generated by a human developer, the agent even cleared its breakpoint before rewriting.In fact, in this particular trajectory, although the generated patch is unnecessarily long (only a few lines need to be modified rather than the entire function), the agent successfully solved the bug with only a single rewrite attempt:  hopping_cart: The shopping_cart task consists a simple code snippet that aims to implement a shopping cart class.Users can add grocery items and their corresponding price into the shopping cart, they can also query to get the total price so far.Users can set a discount to a particular item type; the discount will change the price of the specified item (e.g., 10% off on apples).The mini logical trap being designed in this task is that the discount should be applied for both qualified items in the cart already, and when adding them into the cart in the future.The specific implementation in the buggy code can become messy when there are multiple rounds of adding and discounting operations.All the inspected agents (i.e., using GPT-4o, Llama-3.3-70B-Instruct,R1-Distill-Llama-70B, and Claude 3.7 Sonnet as the backbone LLM) could realize that the error is about the discount operation, and they can indeed generate patches to correct the bug.Citing the R1-Distill model's own words:</p>
<p>To fix the bug where the total calculation doesn't account for discounts on subsequent item additions, we need to modify the 'get_total' method to dynamically compute the total considering all applied discounts.However, to our surprise, the agents with GPT-4o and Llama-3.3-70B-Instructbackbone fail to pass the following test:</p>
<p>... cart.add_item("apple",1.0, 3) # Add more apples cart.add_item("orange",0.75, 2) # Add more oranges assert cart.get_total()== 10.23 In which, cart.get_total()returns the rounded value of 10.225 to 2 digits after the decimal point.The authors realize that this specific task was created in Python 2, where the round() function adopts an "away from zero" style 8 , i.e., round(0.5) is 1.0.However, in Python 3, where we run the debug-gym experiments in, the behavior has been changed to a "ties to even" style 9 , i.e., round(0.5) is 0.0.Once again, we observe smart debugging traces in trajectories generated by the debug agent using the Claude 3.7 Sonnet backbone, which used the pdb tool to test its hypothesis, and then imported an external library to make sure the rounding is working properly (round half up): espite being nearly solved by the strongest LLM-based agents, we believe that Mini-nightmare has its merits serving as a tool to analyze and understand debugging agents' behavior, especially for agents using weaker backbone models.We open-source these ten minimal buggy code snippets together with debug-gym, we also encourage the community to contribute to this collection with similar curated examples of buggy code.</p>
<p>SWE-bench</p>
<p>We test the three agents using a set of strong LLMs as backbone on SWE-bench.Specifically, we use the SWE-bench-Lite split, consisting a curated set of 300 tasks which the evaluation process is less costly.We report the results in Table 3.The results suggest that for (relatively) weaker LLMs (GPT-4o, GPT-4o-mini, and lama-3.3-70B-Instruct),accessing the pdb tool at the beginning (i.e., the debug agent) could to some extent harm the overall performance.In comparison, the debug agent with stronger LLMs (o1-preview, o3-mini, and Claude 3.7 Sonnet) can somehow benefit from the pdb tool and achieve a significantly higher success rate.In addition, the strategy of equipping the agent pdb only after its 5 th rewrite attempts makes the debug(5) agent outperform the rewrite agent in all cases.Using the two strongest LLMs, the debug(5) agent even outperforms the debug agent.Like in Section 3.3.1,we visualize the experimental logs statistically to investigate the agents' behavior.In Figure 9, we plot the length of success episodes, both in terms of number of steps and the total number of response tokens.It is clear that the debug agent tends to use the most steps to finish a task because it has access to the pdb tool form the beginning of an episode.The debug(5) agent typically uses more steps than the rewrite agent, but not as many as the debug agent, because it gets access to the pdb tool only on the halfway.In comparison to the clear difference in number of steps, the number of response tokens consumed by the three agents are less differentiable.This is because the commands calling pdb are typically short and thus do not significantly add up to the number of tokens.Because response tokens are generally more expensive (in API-based LLM services) and slower to generate (in open-weights model hosting) compared to input tokens (which is arguably easier to parallelize), the additional interaction steps facilitated by the pdb tool increases the overall cost only by a tolerable degree.</p>
<p>In Figure 10, we show the distribution over the tools being called by each agent.We observe that the stronger models, especially Claude 3.7 Sonnet, tend to produce a relatively uniform distribution.This is particularly obvious in the bar chart showing the rewrite agent with Claude 3.7 Sonnet: while all other backbone LLMs are more likely to call rewrite only, the Claude model spends a considerable portion of its actions to try out the view and listdir.The same trend holds in the debug and debug(5) agents, Claude 3.7 Sonnet issues a much diverse set of actions.Although all tasks in SWE-bench contain very big repositories, not every task requires the investigation to multiple files.We find such exploration behavior interesting, in the sense that the agents try out different tools even the tools may not bring useful information.This finding hints that the Claude 3.7 Sonnet model may have learned some degrees of curiosity driven exploration skills, following its intrinsic motivation [59,58,24], the outcome of such exploration further contributes to the agents' superior performance on debugging tasks.</p>
<p>Figure 11: Robustness scores: proportion of SWE-bench-Lite tasks that when the rewrite agent succeeds, the debug or debug(5) agent also succeeds.For example, the green bar in the o1 group means that within the set of tasks the rewrite agent (using o1 as backbone) successfully solves, there are more than 80% of which the debug agent can also succeed.</p>
<p>In Figure 11, we plot robustness scores: the proportion of SWE-bench tasks that when the rewrite agent succeeds, the debug or debug(5) agent also succeeds.This metric measures the extent that an agent can consistently solve tasks it solved before, when being equipped with new tools.Formally, we use S rw , S dbg , and S dbg(5) to denote the set of task ids that the rewrite, debug and debug( 5) These numbers help to understand the overlap between the agents' succeeded tasks, and answer the question: do different agents all solve different tasks randomly?
agents
Figure 11 suggests that compared to debug, the debug(5) agent often shares a greater portion of solved tasks with the rewrite agent.This is expected because the first half of the debug(5) agent should behave very similarly as the rewrite agent.We observe that all LLM backbone models except Llama-3.3-70B-Instructachieved a greater than 60% robustness score, this reflects our observation in Figure 4 and thus reassuring the design of the debug(5) agent.Furthermore, the o1 and Claude 3.7 Sonnet models enable both the debug and debug(5) agents to solve about 90% of the tasks out of the ones the rewrite agent solved, and in fact the debug agent can somehow achieve higher robustness score than debug(5).This observation suggests that stronger LLM backbones can provide agents with better robustness, in the sense that equipping a new tool to such agents is less likely to disturb the their behavior in solving tasks that they could solve before.</p>
<p>Figure 12: Average number of code lines a rewrite action deletes and adds while fixing bugs in SWE-bench-Lite.We also plot the number of lines within the added code chunk that is comment (i.e., added lines start with #).Note the "added" bars includes the "comment" bars in their values.</p>
<p>The rewrite tool in debug-gym is flexible enough to edit arbitrary-sized code chunks in a file, and it is designed to delete the old code chunk and fill in the new code chunk at the same time.For example, <code>`rewrite code/utils.py4:10 &lt;c&gt; print('bonjour')&lt;/c&gt;</code>ẁ ill delete 7 lines of code then fill in a single new line.We are interested in understanding different agents' behavior in rewriting when using different backbone LLMs.As shown in Figure 12, we observe that in general, agents with stronger backbone LLMs tend to both remove and add more lines of code, and in fact, the rank in rewriting length roughly matches the rank in performance shown in Table 3.We also observe that the two models that achieved the highest performance, namely o1 and Claude 3.7 Sonnet, both tend to add more lines than they remove via calling the rewrite tool.This may partially due to the fact that stronger models also tend to write comments in the code, the comments may benefit the code generation process as a special form of Chain-of-Thought [13,66].</p>
<p>Discussion</p>
<p>Our experimental results show that our agents, despite being equipped with powerful tools that facilitate codebase investigation used by human developers, are still far from being capable of using those tools in a meaningful way.Although we observe some signs of life from agents using the strongest LLMs as backbone, the most performant agent-backbone model combination can barely solve about a half of the SWE-bench-Lite tasks.Regarding weaker models and especially the openweights ones, they are in general less skilled in the interactive debugging setting, even though some of them already have decent code generation abilities (e.g., on Aider).</p>
<p>We believe the reason the proposed agents struggle in interactive debugging settings is twofold.</p>
<p>Agent design.First, as described in Section 3.2, our agents only involve minimal architectural design and prompt optimization because these agents serve the purpose of demonstrating debug-gym's API and setting up a baseline for future work.For example, in all agents, we simply include all available information into the agent's system prompt, and provide the most recent 20-step conversation history in the user prompt, and hope the LLM understands this information and knows how to further interact with the environment.We believe it is necessary to design an agent's architecture in a way that we can better guide them to use the tools on demand.For example, we can decompose the decision-making process at every interaction step into multiple individual sub-tasks and assign different specialized LLM-based modules to tackle them.This can be achieved by leveraging agent design frameworks such as AutoGen [75] and Magentic-One [21].In addition, the specific way we communicate the task to the LLM backbone models (i.e., prompt design) may be sub-optimal.We can foresee carefully designed prompts to greatly improve our baseline agents' performance.This can be achieved by leveraging generative optimization tools such as [90,67,50].</p>
<p>Agent training.Second, due to the scarcity of data representing sequential decision-making processes (e.g., human debugging trace) in current LLM's training corpus, the LLMs may have a relatively low potential in solving interactive tasks without further training effort.We strongly believe that training or fine-tuning LLMs can make them better interactive debuggers.However, this will require specialized data to fulfill such model training, for example, trajectory data that records agents interacting with a debugger to collect necessary information before suggesting a bug fix.Despite the recent advancement in optimizing LLMs solely with RL, which sometimes only requires the problem description and a final ground-truth answer [16], we argue that interactive debugging belongs to the class of sequential decision-making problems that is fundamentally different from conventional reasoning problems such as math word problems [14,28].Specifically, compared to relying on an agent to come up with a reasoning chain in its entirety that hopefully leads to the final answer (open-loop control), in our setting, the generated action at every step will trigger the environment to return feedback that is grounded by the current environment state (closed-loop control).This requires an agent to assimilate any necessary new feedback to make new decisions at every step; learning this ability requires a denser form of data, e.g., the problem description and the sequence of actions that lead to solving the problem.</p>
<p>We foresee two ways of collecting the debugging trace data.The relatively straightforward way is to collect debugging logs from human developers: this could arguably produce higher-quality data, but the cost of collecting a sufficient amount of such data could be a concern.Another way is to rely on the strongest LLM-based agents, with proper prompt design, to sample a large collection of debugging trajectories.Then, a filter [25] or verifier [81,29] can be applied to select the higherquality trajectory subset from the sampled data.With the obtained dataset, an LLM can be fine-tuned on it, which will presumably have better interactive debugging skills.This trained model can be used to further collect more data, and with more iterations, this process can result in both a model specialized in interactive debugging, and a dataset that consists high-quality data of interactive debugging trajectories.Regardless of the data collection choice, debug-gym can be easily adapted to facilitate this pipeline.</p>
<p>Limitations and Future Work</p>
<p>In this section, we discuss some limitations of debug-gym and how we plan to improve the framework in the future.</p>
<p>Trustworthy agent.To ensure reliable verification of the fixed code generated by an agent, debug-gym by default evaluates the code against a set of test cases.We make these test cases accessible to an agent (in a read-only manner) to make the agent's debugging settings as close as those for human developers.However, like humans, certain agents may exploit these test cases by implementing explicit if-else conditions to pass them without genuinely solving the given task, raising concerns about the trustworthiness of the agent's abilities.A potential mitigation strategy is to introduce additional hidden test cases, testing the same functionalities as existing ones but using different input-expected output pairs, stored externally, assessing responses dynamically based on queries without exposing the test details.Another future approach involves generating test cases using an auxiliary LLM-based agent and periodically updating them by leveraging the model's stochasticity.These strategies may mitigate test case memorization and enable a more rigorous assessment of the agent's ability to generalize beyond specific test cases.</p>
<p>On the agent's side, we believe that researchers and practitioners should also make efforts to guide and regulate their agents to perform interactive debugging tasks in an expected way, throughout the design and implementation of their agents.This is particularly essential because the end goal of such agents is to help human developers perform debugging tasks rather than to optimize their scores on some leaderboards.Misbehaving (e.g., cheating) agents would defeat the purpose of this research direction.</p>
<p>Reviewer agent.Currently, debug-gym uses the success rate as the sole evaluation metric.Specifically, the success rate is 1 if the code passes all test cases at the end of an episode, and 0 otherwise.We observe cases where the agent could manage to pass the tests (and thus gets the point), but the solution has a high time and/or space complexity.Despite having a timeout defined in debug-gym's eval tool, this only captures infinite loops or extremely slow cases.In real-world software development scenarios, a pull request (PR) must pass a set of automatic tests, as well as code reviews by at least one peer developer.We are excited to see recent work in the field of LLM-as-a-critic/annotator [89,20,25], in which they design LLM-based agents to analyze trajectories collected from agents interacting with a game or a simulator.We believe multi-agent systems where debugging agents cooperate with judge/reviewer agents can be an impactful research and engineering direction, with which the overall debugging system can leverage a structure similar to the actor-critic algorithm [39], where the debugger and the judge both improve by working together.In such a way, the generated code will not only work, but also be optimized in terms of complexity (and other criteria the critic learns).</p>
<p>Non-linear computation flow.The current version of debug-gym assumes a linear computation flow, i.e., tools can only be called sequentially, not in parallel.This may hinder the efficiency of some complex agent system (e.g., maintaining multiple pdb sessions, each focusing on a particular aspect of the investigation).A possible improvement can be achieved by adopting a DAG-based flow logic instead of the linear one as used currently.</p>
<p>Beyond Python.The authors choose to design debug-gym for debugging code repositories written in the Python programming language (and thus using pdb, the Python debugger), mostly because Python is the language they use the most in their work.Other programming languages can be fundamentally different from Python in their design and implementation, and consequently have completely different debugging logic.Although some work utilizes a more general debugger (e.g., gdb) [1] to make the tools applicable to programming languages beyond Python, it is less clear how to wrap the tools in different scenarios to best facilitate LLM-based agents' debugging behavior in different programming languages.One potentially promising future direction is to leverage existing IDEs (e.g., VS Code, PyCharm), where they have already spent considerable effort to integrate debuggers for a wide spectrum of programming languages in a way that human developers can easily use.For example, the Debug Adapter Protocol10 is an abstract protocol used between a development tool (e.g., IDE or editor) and a debugger.This protocol defines a unified format of the messages being communicated between the two parties, and thus makes it possible/easier to implement a generic debugger for a development tool that can communicate with different debuggers.</p>
<p>Test generation on the fly.We are also excited to see recent work in unit test generation [61].We believe this line of research shares a similar goal as ours, and can be a useful building block in future debugging agents.We foresee agentic systems that can navigate the codebase, make a hypothesis based on their understanding, then try to verify the hypothesis by either checking variable values during runtime (via pdb), or generating unit tests to catch unexpected code behaviors.In fact, the two skills are complementary and can be combined together.For example, test cases can be generated on the fly and be executed via pdb, this provides the interactive debugger a much more powerful tool than printing values; at the same time, being able to run unit tests during runtime enables the unit test generation agent to adapt its strategies more rapidly, and potentially also learns active information-seeking behavior.</p>
<p>5 Literature Review</p>
<p>Program Synthesis with LLM</p>
<p>Program synthesis has advanced by leveraging LLMs to enhance code generation and program development from natural language descriptions.[7] highlight the efficiency of LLMs in generating Python code from descriptions without any fine-tuning, using only few-shot examples.To improve the quality of the code, [51] propose CodeGen, a multi-turn code generation system that breaks down a single program into multiple prompts by specifying sub-problems.To evaluate these capabilities, benchmarks such as HumanEval [11], MBPP [7], and swe-bench [37] have been proposed, enabling extensive evaluation on code-writing capabilities from basic to difficult tasks.To tackle more complex tasks, AlphaCode [43] enhances LLMs with verification feedback, while Self-refine [45] introduces an iterative refinement approach incorporating human-in-the-loop supervision for code synthesis.WorldCoder [68] exemplifies a model-based agent that constructs Python programs to represent its understanding of the world through environmental interactions.[86,62] suggest automated agents capable of multi-turn repair based on reasoning capabilities.In contrast, AGENTLESS [76] simplifies code generation by removing complex autonomous agents, instead employing a two-phase localization and repair process to achieve high performance.Despite these attempts, fundamental challenges persist, as models often struggle with complex tasks that require deeper or progressive logical understanding of code [19].To leverage the capability of code LLMs, debug-gym provides an interactive playground for exploration, actively seeking information within the code environment to tackle progressive logical steps in the code -much like a human would.</p>
<p>LLM-based Code-Repairing Agents</p>
<p>code-repairing is an important sub-component in automatic coding systems.It refers to the setting where given a piece of code that fails to meet certain required criteria, a system rewrites the code, either partially, or in its entirety to fix the code.LLM-based agents have been broadly used to tackle code-repairing tasks.As shown in Figure 1 (left), conventional code-repairing agents perform rewriting conditioned on error messages obtained from executing the buggy code [36]; [83,17] show that fine-tuning the LLM with such data can improve the agent as a better code rewriter.[52] conduct an empirical study on a set of such self-repair methods, arguing that self-repair is bottlenecked by models' inability to provide sufficiently high-quality feedback: they observe significant overall performance boosts when using feedback generated by larger LLMs or human experts.Several attempts have been made to create additional feedback.[45] uses the same pre-trained LLM to provide feedback for its own generated code; [87] guide strong LLMs to mimic human feedback; [63] propose an LLM-simulated code executor to replace the real Python interpreter for tracing runtime variable values, they show that in their setting, LLMs could accurately track variable states while simulating the execution process.Motivated by this line of work, debug-gym enables debugging agents to access previously hidden but accurate information (e.g, terminal output in response to pdb commands) by interacting with tools.Information revealed by debug-gym is grounded to the code the agent is trying to debug; this is fundamentally different from LLM-generated feedback or reasoning chain, which have no guarantee to be accurate due to the potential fabrication behavior of LLMs.</p>
<p>Another line of work takes inspiration from human developers into their agent design.[31] propose AgentCoder, an LLM-based multi-agent system comprising agents with different roles.Specifically, a Programmer agent generates the code snippets; a Test Designer agent writes a set of test cases; then, a Test Executor runs the code snippets against the generated test cases in a local environment, and provides feedback which could be used in the next iteration.[63] propose to decompose a piece of code into multiple abstraction levels and guide agents to identify and fix bugs at different levels of granularity in a bottom-up manner (e.g., from syntactic errors to algorithmic flaws).[12] propose Self-Debugging, an in-context learning technique that guides LLM-based agents to perform 'rubber duck debugging', i.e. to explain its generated code and the execution results in natural language, then use it to condition the LLM to fix the bugs.[30] propose print debugging, also an in-context learning approach that guides LLMs to add print() functions into the code to print intermediate variables.The authors show that the additional information revealed from the execution outputs of the print statements helps their agent deal with problems that involve relatively complex data structures and algorithms.</p>
<p>Recently, [88] propose Large Language Model Debugger (LDB), an LLM-based debugging framework providing fine-grained runtime execution information.Specifically, LDB first generates a seed program by running a static analysis, generating a control flow graph of the program.Subsequently, LDB segments programs into basic blocks based on the graph.During Runtime execution, LDB inspects the variable states after each basic block, such states are further used to refine the program.[42] propose ChatDBG, a human-in-the-loop system where programmers can collaborate with LLMbased debugging agents in a dialogue setting.The debugging agents have access to debugging tools such as LLDB and GDB for native code and Pdb for Python, they follow the human programmer's guidance in their debugging process.OpenHands [74] is a recent agent scaffold for general-purpose software development.It aims to guide agents to interact with the world like human developers, e.g., by writing code, interacting with a command line, and browsing the web.Similarly, Moatless Tools [57] is a system designed for helping LLM-based agents edit code repositories by equipping them with tools.In the same spirit as the above work, the proposed debug-gym environment aims to empower debugging agents with tools that are designed for human developers.We believe learning to use such tools can help an agent to more effectively and efficiently investigate realistic code repositories and thus improve its debugging capability.</p>
<p>Recently, [60] propose SWE-Gym, a dataset of software engineering tasks, each paired with a preconfigured executable environment for validating an agent's proposed patch.SWE-Gym is not a gym-like [10,70] interactive simulator, but rather a framework which can collect code-repairing trajectories from stronger LLMs, then use the trajectories to train weaker LLMs, e.g., via supervised fine-tuning.</p>
<p>In a concurrent work, [1] propose EnIGMA, an LLM-based agent designed for solving cybersecurity tasks such as Capture The Flag (CTF) challenges.The agent is built based on the Agent-Computer Interface concept of the SWE-agent framework [78], and is leveraging SWE-ReX, a SWE-agent built-in runtime interface to interact with shell environments.The authors designed a set of interactive tools that enables agents to interact with programs.Among which, EnIGMA Debugger equips the agent with an interface to interact with the gdb (GNU Debugger) shell.This enables the agent to perform actions such as breakpoint manipulation and code navigation.While [1] share a very similar motivation on interactive tool use with us, their primary focus is bringing interactive tool use in solving cybersecurity tasks.In contrast, debug-gym is an interactive environment dedicated for developing LLM-based agents to debug customized code repositories.</p>
<p>We refer the readers to [84], a systematic survey on code-repairing methods, and [37,3,87,69,64,60] for recent benchmarks for evaluating code-repairing systems.</p>
<p>Text-based Interactive Environments</p>
<p>Text-based interactive environments have been widely adopted by multiple research communities, including language learning, embodied AI, gaming, and Reinforcement Learning.These environments provide the opportunity of studying interactive agents at a reduced cost, thanks to the property of language that can represent the world in an abstract way, but at the same time remain grounded by the rules of the world.[48] is among the early works that trains a Deep Learning agent to solve simple language-based RL tasks.Since then, a set of new environments designed for research have been created.[15] is a text-based game generator where users could generate and sample games with arbitrary complexity by specifying a few parameters.[26] includes a collection of human-authored text adventure games such as Zork [41] that can be used to test an agent's common sense knowledge and long-term planning.[71] propose LIGHT, a collection of text games featuring multi-character dialogue generation conditioned on different personas within a fantasy world.[72] provide a set of text games where agents are required to perform elementary school-level scientific experiments.[33] further extend the setting to much more complex scientific discovery games such as radioisotope dating, rocket science, and proteomics.Some of such environments or simulators are developed based on earlier virtual machines such as Z-machine [32] or domain specific formal language such as Inform7 [49], some later work are developed with the Scala Programming Language to leverage its fast speed.We refer readers to [34], a comprehensive survey around text-based games and the research they facilitate.</p>
<p>Beyond game settings, text-based interactive environments have been used in building agents that tackle a set of realistic tasks.[79] propose interactive machine reading comprehension (iMRC), a task where an agent is required to navigate a partially observed document using commands (e.g., Ctrl+F) to collect necessary information to answer a question.[46] propose WebGPT, an LLM-based agent that can navigate the Internet to perform complex tasks in a similar way that humans interact with the Internet.[77] propose OSWorld, a large scale environment that simulates an operating system.In the same vein, we also see code-repairing as a special interactive task where agents interact with an environment via text.Existing work including [37,44] have collected a decent amount of real-world repository-level buggy code and their ground-truth solutions (patches).We are interested in studying how debug-gym can be used to help agents debug in a more effective way than existing trial-and-error manner.</p>
<p>Conclusion</p>
<p>In this work, we propose debug-gym, a text-based environment that facilitates the design and training of interactive debugging agents.Specifically, debug-gym can equip such agents with a set of tools, including the Python debugger (pdb), which enables the agents to investigate variable values during runtime via breakpoints.We use three prompt-based debugging agents, all with minimal design, to demonstrate how agents can use debug-gym to solve debugging tasks.We conduct experiment on three benchmarks with varying degrees of difficulty level and task specificity.Results suggest that while using strongest LLMs as backbone enables agents to somewhat leverage interactive debugging tools, they are still far from being proficient debuggers, this is especially the case for the more affordable choices of LLMs.We believe this is due to the scarcity of data representing sequential decision-making behavior (e.g., debugging traces) in current LLM's training corpus.We open-source debug-gym with this technical report, aiming to provide the research community with a framework that facilitates this line of research.We encourage the community to help us advance this research towards building interactive debugging agents, and more generally, agents that can seek information by interacting with the world on demand.</p>
<p>Figure 1 :
1
Figure1: Diagram demonstrating the code-repairing process in outline.In most existing approaches (shown in black), an agent rewrites its code conditioned on error messages obtained from executing the code.debug-gym equips the agent with additional tools such as pdb (shown in red), so it can interactively seek necessary information from the semantic space hidden behind the code, and therefore have better code-repairing performance.</p>
<p>Figure 2 :
2
Figure2: Abstraction of the relationship between components of the debug-gym interactive debugging system.The environment is defined to accommodate an interactive terminal, an extensible toolbox, as well as a code repository the user aims to investigate.Given the environment, an agent iteratively takes actions in the environment, each one yielding a new observation.</p>
<p>env = create_env(config) # Build environment from config obs = env.reset()# Reset/initialize a debugging episode reward = 0 # Initial reward is zero for step in range(config["max_steps"]): # Terminate an episode if success if reward &gt; 0: break # Build LLM prompt prompt = agent.build_prompt(obs)# Prompting LLM to obtain the next action action = agent.llm_call(prompt)# Send the action to the environment and get the new observation obs, reward = env.step(action)</p>
<p>Figure 3 :
3
Figure3: An example demonstrating interactive debugging with debug-gym.In this example, the agent uses debug-gym to investigate the columns of the downloaded data frame, and figures out the correct column name is prix instead of price .This information is unavailable by solely reading the code and the error message, as done by most conventional code-repairing systems.</p>
<p>file.]Current code in view: [The currently opened file, specified by the latest call of the view tool.]Current breakpoints: [Empty because the pdb tool is disabled.]Last evaluation output: [The output from the terminal resulted by the latest call of the eval tool.This typically contains the error messages when attempting to execute the code.]Last observation: [The observation returned by debug-gym in response to the latest action.]The user prompt, on the other hand, provides the agent the history of past conversation, then gives the agent a more concrete instruction: User Prompt History of command and terminal outputs (the last 20 steps): [A sliding window of the most recent agent-environment interactions.]</p>
<p>Prompt Overall task: Your goal is to debug a Python program to make sure it can pass a set of test functions.You have access to the pdb debugger tools, you can use them to investigate the code, set breakpoints, and print necessary values to identify the bugs.Once you have gained enough information, propose a rewriting patch to fix the bugs.Avoid rewriting the entire code, focus on the bugs only.Instruction: [Same as rewrite, plus the instruction of the pdb tool (example shown below).]Repo directory tree: [Same as rewrite.]Current code in view: [Same as rewrite.]Current breakpoints: [A list of all breakpoints, including the file paths and line numbers.]Last evaluation output: [Same as rewrite.]Last observation: [Same as rewrite.]Similarly, we mention the pdb tool in the user prompt: User Prompt History of command and terminal outputs (the last 20 steps): [Same as rewrite.]</p>
<p>Figure 4 :
4
Figure 4: Success rate on Aider as a function of the number of rewrites.Averaged over three runs with the rewrite agent, using GPT-4o and R1-Distill-Qwen-32B backbones.</p>
<p>3 .
3
OpenAI o1-preview[55]: Closed-weights, 128K context length.4. OpenAI o3-mini [56]: Closed-weights, 200K context length.5. Claude 3.7 Sonnet [5]: Closed-weights, 200K context length.</p>
<p>(a) Episode length.(b) Number of response tokens in an episode.</p>
<p>Figure 5 :
5
Figure 5: Episode length and number of response tokens in an episode, averaged over all success episodes in three runs on Aider.</p>
<p>Figure 7 :
7
Figure 7: Distribution of the pdb command being generated by the debug agent on Aider.In which, "other" includes up, down, s, other pdb commands, and cases where the agent fails to generate valid pdb command.</p>
<p>Figure 8 :
8
Figure 8: Number of success in three runs on Mini-nightmare, when agents use GPT-4o, Llama-3.3-70B-Instruct,R1-Distill-Llama-70B, and Claude 3.7 Sonnet as backbone LLM.</p>
<p>and tail are provided head = int(line_numbers[0]) -1 # 1-based to 0-based tail = int(line_numbers[1]) -1 # 1-based to 0-based if head &lt; 0 or tail &lt; 0 or head &gt; tail: # Check for invalid ranges return "SyntaxError: invalid syntax.",None, None return "", head, tail except ValueError: # Handle non-integer inputs return "SyntaxError: invalid syntax.",None, None </c>``s</p>
<p>(a) Episode length.(b) Number of response tokens in an episode.</p>
<p>Figure 9 :
9
Figure 9: Episode length and number of response tokens in an episode, averaged over all success episodes in three runs on SWE-bench-Lite.</p>
<p>Figure 10 :
10
Figure 10: Distribution of the tools being called by agents on SWE-bench-Lite.In which, "other" includes actions that fail to be parsed by debug-gym (e.g., invalid syntax caused by missing backticks).</p>
<p>succeed at least once out of the three runs.The robustness scores are therefore computed as |S dbg ⋂ S rw |/|S rw | and |S dbg(5) ⋂ S rw |/|S rw |, respectively, where | ⋅ | denotes the size of a set.</p>
<p>An interface to the Python debugger PDB.Send a command to the PDB terminal.The command should be a valid PDB command.",
instructions = {"template": "<code>pdb &lt;command&gt;</code>","description": "↪"examples": ["<code>pdb p x</code>to print the value of the variable x in the current↪context.","<code>pdb b 42</code>to set a breakpoint at line 42 in the current↪file.","<code>pdb cl src/code.py:26</code>to clear the breakpoint at line 26 in↪the file src/code.py.","<code>pdb c</code>to continue the execution until the next breakpoint.",],}</p>
<p>Table 2 :
2
Results on Mini-nightmare.We report the average success rate (in percentage) and standard deviation over three runs.</p>
<p>Table 3 :
3
Results on SWE-bench-Lite.We report the average success rate (in percentage) and standard deviation over three runs.
OpenAIClaudeLlama4o4o-mini o1-preview o3-mini3.7 Sonnet 3.3-70B-Instructrewrite19.1±2.4 4.0±0.710.7±0.78.5±1.037.2±2.12.4±0.5debug17.2±0.8 3.5±0.730.2±1.022.1±0.948.4±1.64.0±1.0debug(5) 23.6±1.0 6.2±0.130.8±0.919.8±1.152.1±1.64.8±0.4
https://docs.python.org/3/library/pdb.html
https://github.com/exercism/python
We exclude psf__requests-863 and psf__requests-2674 because code in these tasks can pass the test cases even before debugging.
By default we show a directory tree of depth 1, for more complex repositories (e.g., in SWE-bench), we additionally include the listdir tool to query for a directory tree with custom depth.
https://docs.python.org/2/library/functions.html#round
https://docs.python.org/3/library/functions.html#round
https://microsoft.github.io/debug-adapter-protocol/
AcknowledgmentThe authors thank Ruoyao Wang for their insightful discussion on building interactive debugging agents.The authors thank Chris Templeman and Elaina Maffeo for their team coaching.The authors thank Jessica Mastronardi and Rich Ciapala for their kind support in project management and resource allocation.The authors thank Peter Jansen for providing valuable feedback to this technical report.A Contributions (a-z)Alessandro Sordoni logistically supported the project from the start, helped with code reviews, debug-gym design, the Aider benchmark setup and helped proofread the technical report.Charbel El Feghali helped write this technical report, built the project website, and explored potential research directions.Chinmay Singh helped with the VS Code Extension, Unit Tests, and Release &amp; Compliance Tracker.CS also contributed to code reviews.Darya Moldavskaya contributed in project management.DM proofread this technical report, advised on positioning value as it applies to real-world scenarios.Drew MacPhee contributed in resource management, including LLM APIs and GPU clusters hosting open-weights models.Lucas Page-Caccia contributed in designing an early version of the Mini-nightmare dataset.LPC also helped with proofreading this technical report.Marc-Alexandre Côté is co-leading this project.MAC worked on designing the environment, tools, terminal and SWE-bench integration for debug-gym.MAC is a main contributor to the debug-gym codebase, contributed to the SWE-bench experiments and helped proofreading the technical report.Matheus Pereira is a main contributor to the debug-gym codebase.MP contributed to the design of the environment, tools, and terminal, particularly the Docker terminal, LLMs, and SWE-bench integration.MP also assisted with code reviews and proofreading this technical report.Minseon Kim helped with Aider, and mini-nightmare experiments, and writing this technical report.Morgane M Moss contributed to early system design and prototyping the debug-gym and its tools, as well as the developed debug-gym codebase.MMM ran experiments and helped write this technical report.Xingdi Yuan is co-leading this project.XY proposed the initial idea of equipping debugging agent with the pdb tool and implemented the prototype version of it.XY is a main contributor to the debug-gym codebase.XY authored the Mini-nightmare problems.XY contributed in experiments running and writing this technical report.
T Abramovich, M Udeshi, M Shao, K Lieret, H Xi, K Milner, S Jancheska, J Yang, C E Jimenez, F Khorrami, arXiv:2409.16165Enhanced interactive generative model agent for ctf challenges. 2024arXiv preprint</p>
<p>Unified pre-training for program understanding and generation. W Ahmad, S Chakraborty, B Ray, K.-W Chang, K Toutanova, A Rumshisky, L Zettlemoyer, D Hakkani-Tur, I Beltagy, S Bethard, R Cotterell, T Chakraborty, Y Zhou, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterAssociation for Computational LinguisticsJune 2021</p>
<p>R Aleithan, H Xue, M M Mohajer, E Nnorom, G Uddin, S Wang, arXiv:2410.06992Swe-bench+: Enhanced coding benchmark for llms. 2024arXiv preprint</p>
<p>Automated unit test improvement using large language models at meta. N Alshahwan, J Chheda, A Finogenova, B Gokkaya, M Harman, I Harper, A Marginean, S Sengupta, E Wang, Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering. 2024</p>
<p>Claude 3.7 sonnet and claude code. Anthropic, 2025</p>
<p>D Arora, A Sonwane, N Wadhwa, A Mehrotra, S Utpala, R Bairi, A Kanade, N Natarajan, Masai, arXiv:2406.11638Modular architecture for software-engineering ai agents. 2024arXiv preprint</p>
<p>Program synthesis with large language models. J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, arXiv:2108.077322021arXiv preprint</p>
<p>P Bachman, A Sordoni, A Trischler, arXiv:1612.02605Towards information-seeking agents. 2016arXiv preprint</p>
<p>Codeplan: Repository-level coding using llms and planning. R Bairi, A Sonwane, A Kanade, A Iyer, S Parthasarathy, S Rajamani, B Ashok, S Shet, Proceedings of the ACM on Software Engineering. 12024</p>
<p>G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, Openai gym. 2016</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, W Zaremba, 2021Evaluating large language models trained on code</p>
<p>Teaching large language models to self-debug. X Chen, M Lin, N Schärli, D Zhou, arXiv:2304.051282023arXiv preprint</p>
<p>Y Chen, Y Liu, F Meng, Y Chen, J Xu, J Zhou, arXiv:2404.07549Comments as natural logic pivots: Improve code generation via comment perspective. 2024arXiv preprint</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, R Y Tao, M Hausknecht, L E Asri, M Adada, W Tay, A Trischler, CoRR, abs/1806.115322018</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , 2025</p>
<p>Cycle: Learning to self-refine the code generation. Y Ding, M J Min, G Kaiser, B Ray, Proceedings of the ACM on Programming Languages. 8OOPSLA12024</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Llm-based test-driven interactive code generation: User study and empirical evaluation. S Fakhoury, A Naik, G Sakkas, S Chakraborty, S K Lahiri, IEEE Transactions on Software Engineering. 2024</p>
<p>X Feng, Z Wan, H Fu, B Liu, M Yang, G A Koushik, Z Hu, Y Wen, J Wang, arXiv:2411.14251Natural language reinforcement learning. 2024arXiv preprint</p>
<p>Magentic-one: A generalist multi-agent system for solving complex tasks. A Fourney, G Bansal, H Mozannar, C Tan, E Salinas, Erkang, F Zhu, G Niedtner, G Proebsting, J Bassman, J Gerrits, P Alber, R Chang, R Loynd, V West, A Dibia, E Awadallah, R Kamar, S Hosn, Amershi, 2024</p>
<p>Aider is ai pair programming in your terminal. P Gauthier, 2024</p>
<p>Rlef: Grounding code llms in execution feedback with reinforcement learning. J Gehring, K Zheng, J Copet, V Mella, T Cohen, G Synnaeve, 2024</p>
<p>Information-seeking, curiosity, and attention: computational and neural mechanisms. J Gottlieb, P.-Y Oudeyer, M Lopes, A Baranes, Trends in cognitive sciences. 17112013</p>
<p>J Gu, X Jiang, Z Shi, H Tan, X Zhai, C Xu, W Li, Y Shen, S Ma, H Liu, arXiv:2411.15594A survey on llm-as-a-judge. 2024arXiv preprint</p>
<p>Interactive fiction games: A colossal adventure. M Hausknecht, P Ammanabrolu, M.-A Côté, X Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Measuring coding challenge competence with apps. D Hendrycks, S Basart, S Kadavath, M Mazeika, A Arora, E Guo, C Burns, S Puranik, H He, D Song, J Steinhardt, 2021NeurIPS</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>A Hosseini, X Yuan, N Malkin, A Courville, A Sordoni, R Agarwal, arXiv:2402.06457V-star: Training verifiers for self-taught reasoners. 2024arXiv preprint</p>
<p>X Hu, K Kuang, J Sun, H Yang, F Wu, arXiv:2401.05319Leveraging print debugging to improve code generation in large language models. 2024arXiv preprint</p>
<p>Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. D Huang, Q Bu, J M Zhang, M Luck, H Cui, arXiv:2312.130102023arXiv preprint</p>
<p>. Infocom. Learning zil. 1999</p>
<p>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. P Jansen, M.-A Côté, T Khot, E Bransom, B D Mishra, B P Majumder, O Tafjord, P Clark, arXiv:2406.067692024arXiv preprint</p>
<p>A systematic survey of text worlds as embodied natural language environments. P A Jansen, arXiv:2107.041322021arXiv preprint</p>
<p>A survey on large language models for code generation. J Jiang, F Wang, J Shen, S Kim, S Kim, 2024</p>
<p>S Jiang, Y Wang, Y Wang, arXiv:2306.02907Selfevolve: A code evolution framework via large language models. 2023arXiv preprint</p>
<p>C E Jimenez, J Yang, A Wettig, S Yao, K Pei, O Press, K Narasimhan, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. 2023arXiv preprint</p>
<p>Planning and acting in partially observable stochastic domains. L P Kaelbling, M L Littman, A R Cassandra, Artificial intelligence. 1011-21998</p>
<p>Actor-critic algorithms. V Konda, J Tsitsiklis, Advances in Neural Information Processing Systems. S Solla, T Leen, K Müller, MIT Press199912</p>
<p>Efficient memory management for large language model serving with pagedattention. W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J E Gonzalez, H Zhang, I Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Special feature zork: A computerized fantasy simulation game. Blank Lebling, Anderson , Computer. 1241979</p>
<p>K Levin, N Van Kempen, E D Berger, S N Freund, arXiv:2403.16354Chatdbg: An ai-powered debugging assistant. 2024arXiv preprint</p>
<p>Competition-level code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Lago, Science. 37866242022</p>
<p>Can language models replace programmers? repocod says 'not yet. S Liang, Y Hu, N Jiang, L Tan, 2024</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>D Nam, A Macvean, V Hellendoorn, B Vasilescu, B Myers, arXiv:2307.08177-ide generation-based information support with a large language model. 2023arXiv preprint</p>
<p>Language understanding for text-based games using deep reinforcement learning. K Narasimhan, T Kulkarni, R Barzilay, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. L Màrquez, C Callison-Burch, J Su, the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSept. 2015</p>
<p>Natural language, semantic analysis, and interactive fiction. IF Theory Reader. G Nelson, 2006141104</p>
<p>The importance of directional feedback for llm-based optimizers. A Nie, C.-A Cheng, A Kolobov, A Swaminathan, arXiv:2405.164342024arXiv preprint</p>
<p>E Nijkamp, B Pang, H Hayashi, L Tu, H Wang, Y Zhou, S Savarese, C Xiong, arXiv:2203.13474Codegen: An open large language model for code with multi-turn program synthesis. 2022arXiv preprint</p>
<p>Is self-repair a silver bullet for code generation?. T X Olausson, J P Inala, C Wang, J Gao, A Solar-Lezama, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Gpt-4o mini: advancing cost-efficient intelligence. 2024OpenAI</p>
<p>Hello gpt-4o. Openai, 2024</p>
<p>Introducing openai o1. Openai, 2024</p>
<p>Openai o3-mini. Openai, 2025</p>
<p>A Örwall, Moatless tools. 2025</p>
<p>What is intrinsic motivation? a typology of computational approaches. P.-Y Oudeyer, F Kaplan, Frontiers in Neurorobotics. 2007</p>
<p>Intrinsic motivation systems for autonomous mental development. P.-Y Oudeyer, F Kaplan, V V Hafner, 2007</p>
<p>J Pan, X Wang, G Neubig, N Jaitly, H Ji, A Suhr, Y Zhang, arXiv:2412.21139Training software engineering agents and verifiers with swe-gym. 2024arXiv preprint</p>
<p>Learning to generate unit tests for automated debugging. A Prasad, E Stengel-Eskin, J C .-Y. Chen, Z Khan, M Bansal, arXiv preprint 2502.016192025</p>
<p>Code generation with alphacodium: From prompt engineering to flow engineering. T Ridnik, D Kredo, I Friedman, arXiv:2401.085002024arXiv preprint</p>
<p>From code to correctness: Closing the last mile of code generation with hierarchical debugging. Y Shi, S Wang, C Wan, X Gu, arXiv:2410.012152024arXiv preprint</p>
<p>Repairbench: Leaderboard of frontier models for program repair. A Silva, M Monperrus, arXiv2024Technical Report</p>
<p>. A Solar-Lezama, 2023Introduction to program synthesis</p>
<p>Code needs comments: Enhancing code llms with comment augmentation. D Song, H Guo, Y Zhou, S Xing, Y Wang, Z Song, W Zhang, Q Guo, H Yan, X Qiu, arXiv:2402.130132024arXiv preprint</p>
<p>Joint prompt optimization of stacked llms using variational inference. A Sordoni, E Yuan, M.-A Côté, M Pereira, A Trischler, Z Xiao, A Hosseini, F Niedtner, N Le Roux, Advances in Neural Information Processing Systems. 202336</p>
<p>H Tang, D Key, K Ellis, arXiv:2402.12275Worldcoder, a model-based llm agent: Building world models by writing code and interacting with the environment. 2024arXiv preprint</p>
<p>R Tian, Y Ye, Y Qin, X Cong, Y Lin, Y Pan, Y Wu, H Hui, W Liu, Z Liu, arXiv:2401.04621Evaluating debugging capability of large language models. 2024arXiv preprint</p>
<p>Gymnasium: A standard interface for reinforcement learning environments. M Towers, A Kwiatkowski, J Terry, J U Balis, G De Cola, T Deleu, M Goulão, A Kallinteris, M Krimmel, A Kg, arXiv:2407.170322024arXiv preprint</p>
<p>Learning to speak and act in a fantasy text adventure game. J Urbanek, A Fan, S Karamcheti, S Jain, S Humeau, E Dinan, T Rocktäschel, D Kiela, A Szlam, J Weston, arXiv:1903.030942019arXiv preprint</p>
<p>ScienceWorld: Is your agent smarter than a 5th grader?. R Wang, P Jansen, M.-A Côté, P Ammanabrolu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>X Wang, Y Chen, L Yuan, Y Zhang, Y Li, H Peng, H Ji, arXiv:2402.01030Executable code actions elicit better llm agents. 2024arXiv preprint</p>
<p>X Wang, B Li, Y Song, F F Xu, X Tang, M Zhuge, J Pan, Y Song, B Li, J Singh, H H Tran, F Li, R Ma, M Zheng, B Qian, Y Shao, N Muennighoff, Y Zhang, B Hui, J Lin, R Brennan, H Peng, H Ji, G Neubig, OpenHands: An Open Platform for AI Software Developers as Generalist Agents. 2024</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation. Q Wu, G Bansal, J Zhang, Y Wu, B Li, E Zhu, L Jiang, X Zhang, S Zhang, J Liu, arXiv:2308.081552023arXiv preprint</p>
<p>C S Xia, Y Deng, S Dunn, L Zhang, arXiv:2407.01489Agentless: Demystifying llm-based software engineering agents. 2024arXiv preprint</p>
<p>OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. T Xie, D Zhang, J Chen, X Li, S Zhao, R Cao, T J Hua, Z Cheng, D Shin, F Lei, Y Liu, Y Xu, S Zhou, S Savarese, C Xiong, V Zhong, T Yu, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>SWEagent: Agent-computer interfaces enable automated software engineering. J Yang, C E Jimenez, A Wettig, K Lieret, S Yao, K R Narasimhan, O Press, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Interactive machine comprehension with information seeking agents. X Yuan, J Fu, M.-A Côté, Y Tay, C Pal, A Trischler, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>D Zan, A Yu, W Liu, D Chen, B Shen, W Li, Y Yao, Y Gong, X Chen, B Guan, arXiv:2403.16443Natural language to code repository via multi-layer sketch. 2024arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. E Zelikman, Y Wu, J Mu, N Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>F Zhang, B Chen, Y Zhang, J Keung, J Liu, D Zan, Y Mao, J.-G Lou, W Chen, arXiv:2303.12570Repocoder: Repository-level code completion through iterative retrieval and generation. 2023arXiv preprint</p>
<p>Self-edit: Fault-aware code editor for code generation. K Zhang, Z Li, J Li, G Li, Z Jin, arXiv:2305.040872023arXiv preprint</p>
<p>Q Zhang, C Fang, Y Xie, Y Ma, W Sun, Y Yang, Z Chen, arXiv:2405.01466A systematic literature review on large language models for automated program repair. 2024arXiv preprint</p>
<p>Autocoderover: Autonomous program improvement. Y Zhang, H Ruan, Z Fan, A Roychoudhury, Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis. the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis2024</p>
<p>What makes large language models reason in (multi-turn) code generation?. K Zheng, J Decugis, J Gehring, T Cohen, B Negrevergne, G Synnaeve, arXiv:2410.081052024arXiv preprint</p>
<p>T Zheng, G Zhang, T Shen, X Liu, B Y Lin, J Fu, W Chen, X Yue, arXiv:2402.14658Opencodeinterpreter: Integrating code generation with execution and refinement. 2024arXiv preprint</p>
<p>L Zhong, Z Wang, J Shang, arXiv:2402.16906Ldb: A large language model debugger via verifying runtime execution step-by-step. 2024arXiv preprint</p>
<p>Policy improvement using language feedback models. V Zhong, D Misra, X Yuan, M.-A Côté, arXiv:2402.078762024arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, The Eleventh International Conference on Learning Representations. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
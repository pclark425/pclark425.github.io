<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2192 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2192</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2192</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-58.html">extraction-schema-58</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <p><strong>Paper ID:</strong> paper-281741939</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.03413v2.pdf" target="_blank">Report of the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science</a></p>
                <p><strong>Paper Abstract:</strong> This report summarizes insights from the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science, which convened more than 40 experts from national laboratories, academia, industry, and community organizations to chart a path toward more powerful, sustainable, and collaborative scientific software ecosystems. To address urgent challenges at the intersection of high-performance computing (HPC), AI, and scientific software, participants envisioned agile, robust ecosystems built through socio-technical co-design--the intentional integration of social and technical components as interdependent parts of a unified strategy. This approach combines advances in AI, HPC, and software with new models for cross-disciplinary collaboration, training, and workforce development. Key recommendations include building modular, trustworthy AI-enabled scientific software systems; enabling scientific teams to integrate AI systems into their workflows while preserving human creativity, trust, and scientific rigor; and creating innovative training pipelines that keep pace with rapid technological change. Pilot projects were identified as near-term catalysts, with initial priorities focused on hybrid AI/HPC infrastructure, cross-disciplinary collaboration and pedagogy, responsible AI guidelines, and prototyping of public-private partnerships. This report presents a vision of next-generation ecosystems for scientific computing where AI, software, hardware, and human expertise are interwoven to drive discovery, expand access, strengthen the workforce, and accelerate scientific progress.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2192.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2192.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Community-integrated validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Community-integrated validation and continuous learning frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed framework that combines automated testing, community expert feedback, peer review, and continuous model retraining to validate AI-generated scientific code and keep validation artefacts current with evolving scientific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Community-integrated validation and continuous learning frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific computing (cross-domain: physics, chemistry, materials, biology, astrophysics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Described as an ecosystem-level approach that automatically generates test cases grounded in scientific principles, incorporates expert human vetting (peer review) of AI outputs, and feeds corrections back into continuous learning loops for AI models; techniques cited include property-based and metamorphic testing, checks against analytical solutions where available, and validation against observations or experiments when possible. Emphasizes living validation systems that evolve with domain knowledge and peer contributions rather than one-off static test suites.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>The report recommends validating generated code against both analytical solutions and observations/experiments where possible but does not present any concrete comparisons; it calls for frameworks that can support these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Paper argues for development of domain-specific validation frameworks and standards (including tests that check physical laws, conservation properties, and reproducibility across environments) but does not prescribe specific numeric thresholds; emphasizes that standards should include reproducibility, uncertainty quantification, and peer review.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper suggests simulation-based validation may be sufficient when it can be tied to conserved physical properties, known analytical limits, or when experiments are infeasible, but argues that community oversight and formal verification increase confidence; no strict rule provided.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Paper warns generally that AI-generated code can violate domain constraints (e.g., physical laws, numerical stability) and thereby produce invalid simulations; no specific dataset or case study is provided in this report.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>The framework calls for integrating UQ into validation pipelines (uncertainty estimates and error propagation for AI-generated code) and for reporting confidence in generated artifacts; concrete UQ methods are recommended but not implemented here.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Discusses need for provenance, peer review, and community auditing to detect fabricated or spurious AI outputs; recommends metadata and traceability rather than proposing a single detection algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Notes qualitatively that hybrid community-integrated validation will require sustained resource investment (computational resources for testing, human expert time for review, infrastructure for continuous retraining) but provides no quantitative cost/time figures.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Acknowledged limitations include resource intensity, reliance on available domain expertise (bottlenecks for review), the evolving nature of scientific knowledge requiring continual updates, and gaps where ground-truth experimental data are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Paper states that community-integrated validation (with peer review and formal guarantees) is likely to increase acceptance and credibility of AI-generated code in the scientific community, but evidence is prescriptive rather than empirical in this report.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Calls for comparing AI-generated outputs to traditional verification/validation and uncertainty quantification (V&V/UQ) gold standards but does not provide empirical comparisons or metrics in this report.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2192.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2192.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metamorphic & property-based testing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Metamorphic testing and property-based testing for scientific software</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Testing paradigms highlighted as especially valuable for validating scientific computations and AI-generated code when exact reference solutions are unavailable: property-based testing validates invariants/properties, while metamorphic testing checks consistent transformations of inputs/outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Property-based testing and metamorphic testing</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific computing (fluid dynamics, multiphysics, general numerical simulation domains)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The report endorses property-based testing (asserting and checking conserved quantities or invariants such as mass/energy conservation) and metamorphic testing (applying known input transformations and checking corresponding changes to outputs) as automated computational validation methods that are useful when exact analytic solutions or experimental ground truth are unavailable. Examples include checks for divergence-free velocity fields and mass conservation in fluid simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not provided; these testing methods are positioned as complementary to experimental validation rather than replacements.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Suggested as part of domain-specific validation suites; paper recommends these be included in standards but gives no numeric acceptance criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>The paper suggests that strong property-based / metamorphic test coverage increases confidence in simulations and AI-generated code and may be sufficient when it enforces key physical invariants and when experiments are infeasible, but it cautions this is not universally sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Notes that AI tools may generate code that passes superficial tests but violates deeper invariants; metamorphic/property tests are recommended to catch such failures, though no case studies are in this report.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>These testing approaches are not UQ methods per se, but the report recommends combining them with UQ to quantify confidence in passing tests.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>These tests can help detect fabricated or bogus outputs by revealing violations of known invariants; the report frames them as a guardrail rather than a complete solution.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Described qualitatively as cheaper than full experimental campaigns, but still requiring engineering effort to design property specifications and metamorphic relations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limitations include dependency on having well-defined properties to test, difficulty encoding complex domain invariants, and potential for false confidence if tests are incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Paper argues that inclusion of these testing paradigms in CI/CD pipelines and validation frameworks will increase community trust in AI-generated code, but no empirical validation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Proposed as pragmatic complements to gold-standard V&V practices (analytical solution comparison, experiments), not replacements; no quantitative comparisons offered.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2192.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2192.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formal verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formal verification methods tailored to scientific applications</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of formal methods and mathematical proofs to verify correctness properties of numerical algorithms and implementations, recommended as part of a broader validation toolbox for AI-integrated scientific software.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Formal verification for numerical methods</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific computing / numerical PDE solvers / HPC</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The report endorses the use of formal methods (proofs of correctness, theorem proving, formal verification of algorithmic implementations) to provide rigorous guarantees about algorithmic properties, stability, and adherence to conservation laws; it references recent work on formal proofs for PDE solvers and calls for methods adapted to scientific codebases and AI-generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not applicable; formal verification provides mathematical guarantees independent of experiment but complements experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Formal verification is described as a high-assurance standard where applicable; the report calls for integrating formal guarantees into validation standards when feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Formal verification can make simulation results more trustworthy without experiment when the model and its discretization are formally proven to satisfy desired properties, though applicability is limited to problems amenable to formal proof.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Report notes that many complex scientific codes are too large or intricate for existing formal approaches, making full formal verification currently impractical for many applications.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Formal methods address correctness and invariants but do not by themselves provide statistical UQ; the report calls for combining formal methods with UQ techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Formal verification is not presented as a fabrication-detection tool; instead it provides correctness guarantees for implementations that pass the formal checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Acknowledges high upfront cost (human and tooling) for formal verification; not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limited scalability to large codebases, high expertise required, and applicability primarily to algorithmically tractable components rather than whole monolithic applications.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>The report suggests that formal guarantees would substantially increase credibility when available, but recognizes such methods will apply only to a subset of problems in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Presented as a complement or even higher-assurance alternative to empirical testing in limited contexts, but not a general replacement for experimental validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2192.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2192.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analytical & observational checks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Testing against analytical solutions and observational/experimental data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional V&V practice of checking computational results against known analytic solutions where available, and against experimental or observational data; recommended as core components of validation for AI-generated scientific workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Analytical-solution and observation-based validation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Simulation-driven domains (fluid dynamics, astrophysics, materials, chemistry, biology, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The report recommends generating test cases that compare outputs to known analytical solutions (when available) and to experiments/observations; it highlights checking conserved quantities and comparing against validated high-fidelity simulations or experimental datasets as primary validation strategies for scientific correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>When used, simulation fidelity is described as 'high-fidelity' when validated physics are modeled (e.g., shock hydrodynamics, nuclear burning in astrophysics), but the report provides no numerical fidelity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>The report discusses the need to compare simulations to experiments but contains no reported empirical comparisons; it encourages development of benchmarking and metrics for such comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Advocates domain norms: match against analytical solutions, conservation laws, and experimental data where available; calls for standardized benchmarks and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Simulations tied to validated physics and analytic limits may be considered sufficient for some purposes; the report cautions about over-reliance on simulation without independent experimental corroboration.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Notes risk that AI-generated components may pass superficial tests but fail deeper experimental comparisons; no concrete examples given in this workshop report.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Urges inclusion of UQ in comparisons to experimental data to understand confidence and error propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Comparisons to independent experimental datasets and provenance tracking are recommended to reveal fabricated outputs or inconsistent results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Indicates experimental validation can be costly and sometimes impractical, motivating use of computational and property-based tests; no quantitative cost data given.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Availability of experimental data varies by domain; some experiments cannot be replicated and some phenomena lack analytical solutions, limiting applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Paper emphasizes that matching experiments and analytics is critical to credibility in the scientific community; it recommends community-driven benchmarks to standardize acceptance criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Experimental validation and analytic-solution checks are framed as gold standards where available; the report calls for explicit comparisons between AI-generated outputs and these standards but does not provide empirical results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2192.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2192.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciML vs numerics benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Metrics and benchmarks to compare Scientific Machine Learning (SciML) with traditional numerical methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A research priority to develop evaluation metrics, benchmarks, and datasets to rigorously compare SciML approaches against established numerical solvers on accuracy, stability, and scientific validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Benchmarks and metrics for SciML vs traditional numerics</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific computing, numerical PDEs, fluid dynamics, multiphysics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The report calls for creating benchmarks, datasets, and metrics (including accuracy, stability, conservation properties, and uncertainty propagation) to evaluate SciML methods versus traditional numerical solvers; it cites the need to avoid weak baselines and reporting biases and to build domain-appropriate benchmarks (e.g., for PDE solvers).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Paper emphasizes comparisons between SciML and gold-standard numerical simulations and experiments as necessary, but contains no empirical comparisons itself.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Advocates domain-specific benchmarks (including rigorous PDE benchmarks) and clearer reporting standards to assess true gains of SciML methods.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper suggests that simulation-based benchmarking can be sufficient to evaluate many algorithmic properties if benchmarks are high-quality and reflect relevant physics; still recommends experimental corroboration for scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>References community literature warning about overoptimistic claims when using weak baselines for ML PDE solvers; no new case studies provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Recommends including UQ metrics in benchmarks to capture error propagation and confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not specifically a fabrication-detection method, but robust benchmarks reduce risk of overclaiming or fabricated performance wins.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Benchmarking requires curated datasets and compute investment; costs are discussed qualitatively but not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Creating representative, high-fidelity benchmarks is challenging; risk of benchmarks not generalizing across domains or being gamed.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Paper argues that rigorous benchmarking and clear metrics are essential for community acceptance of SciML advances and to avoid overoptimistic reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Paper calls for direct comparisons to traditional numerical methods as the de facto gold standard in many domains, but does not provide empirical results here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2192.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2192.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Peer review for code-generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Peer-review and expert vetting mechanisms for AI-generated code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structured human-in-the-loop review processes tailored to AI-generated scientific code, intended to capture domain-specific correctness beyond automated tests and to incorporate corrections back into AI models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Peer review mechanisms for AI-generated code</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific computing across domains</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Described as expert vetting integrated into validation workflows: domain experts inspect AI-generated code, validate adherence to scientific principles, and provide structured feedback that can be incorporated into model retraining and continuous learning systems; complementary to automated property-based/metamorphic tests and formal methods.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not applicable within this report; peer review is positioned to assess correctness independent of experiment, or before experiment is attempted.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Paper recommends tailoring peer-review practices to code generation outputs and recording provenance and review outcomes as part of validation standards.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Peer review may deem simulation-only validation sufficient for some uses if reviewers judge tests and invariants adequate, but the report recommends caution and context-dependent decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Paper notes that peer review is necessary because automated checks alone can miss subtle domain violationsâ€”no empirical examples provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Peer review does not directly quantify uncertainty but informs confidence assessments and model retraining priorities.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Human review is recommended as a way to detect fabricated or implausible outputs that automated checks miss.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Human expert time is highlighted as a potentially costly but essential part of validation; no numeric costs provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Scalability of expert review is a concern; reliance on scarce expert time can be a bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Authoritative expert vetting is argued to significantly improve community trust and acceptance of AI-generated scientific artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Peer review is framed as complementary to gold-standard experimental validation and formal V&V; no quantitative comparison provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2192.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2192.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uncertainty quantification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uncertainty quantification (UQ) for AI-generated scientific code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Integration of UQ into AI-generated code and validation pipelines to estimate confidence, error propagation, and robustness of predictions and numerical outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Uncertainty quantification for AI/SciML</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific computing, SciML, numerical simulation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The report highlights the need to quantify uncertainty for AI-generated code via methods that assess error propagation, confidence intervals, and sensitivity analyses; recommends combining UQ with formal verification and property-based testing to present calibrated confidence in outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>No empirical comparisons provided; the report states UQ should be used when comparing simulation outputs to experiments to express agreement and confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Calls for UQ to be part of domain validation standards, with explicit reporting of uncertainty bounds alongside predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Simulations accompanied by rigorous UQ may be sufficient for some decision-making contexts; paper recommends context-dependent assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Paper points out that absence of UQ can lead to misplaced confidence in AI-generated code, but provides no specific examples.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Explicit focus of this entity; the report urges integration of UQ pipelines but does not prescribe particular algorithms in this document.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not discussed as a primary fabrication-detection method, though large unexplained uncertainties could flag problematic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Acknowledges UQ adds computational and methodological overhead; no quantitative estimates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Challenges include designing UQ methods suitable for AI models and complex coupled simulations, and computational cost of ensemble/sampling-based UQ.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Paper asserts that explicit UQ reporting is essential for community acceptance of AI-driven scientific results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>UQ is recommended as part of comparisons to experimental gold standards to quantify agreement, but no empirical data are provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2192.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2192.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Security & poisoning defenses</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Security-aware validation: detecting adversarial attacks and data poisoning in scientific AI systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recognition of adversarial threats (data poisoning/adversarial examples) to scientific AI pipelines and the need for validation approaches that guard against malicious or accidental corruption of models and results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Adversarial robustness and data-poisoning detection for scientific AI</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific computing, ML/AI workflows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The report flags security threats (adversarial attacks, data poisoning) that can undermine validation and recommends integrating threat models, provenance, guardrails, and secure pipelines into validation frameworks; suggests combining automated anomaly detection, metadata provenance, and human review to detect compromised outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not applicable; focus is on integrity of computational pipelines rather than comparing to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Recommends including security considerations and adversarial testing in validation standards for AI in scientific computing.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper implies that simulation-based validation without security/hardening may be insufficient if datasets or models are compromised; secure validation requires additional checks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Notes potential for model corruption or data poisoning to produce incorrect yet superficially plausible outputs; no concrete incidents detailed in this report.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not central to detection of adversarial attacks, though anomalous uncertainty signatures could be used to flag issues.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Proposes provenance, anomaly detection, and guardrails (e.g., secure agents, auditing) to detect fabricated or tampered results; no single algorithmic solution is specified.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Security hardening and adversarial testing add engineering and compute costs; not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Security measures can be complex and may not catch all sophisticated attacks; requires ongoing monitoring and expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Argues that demonstrable defenses against tampering are necessary for community trust in AI-driven scientific outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>No direct comparison to gold-standard validation; security-focused validation is presented as an essential complement to correctness-focused V&V.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Assessing the Reliability of Complex Models: Mathematical and Statistical Foundations of Verification, Validation, and Uncertainty Quantification <em>(Rating: 2)</em></li>
                <li>Workshop Report on Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence <em>(Rating: 2)</em></li>
                <li>Report of the DOE/NSF Workshop on Correctness in Scientific Computing <em>(Rating: 2)</em></li>
                <li>Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations <em>(Rating: 2)</em></li>
                <li>Envisioning better benchmarks for machine learning PDE solvers <em>(Rating: 2)</em></li>
                <li>The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2192",
    "paper_id": "paper-281741939",
    "extraction_schema_id": "extraction-schema-58",
    "extracted_data": [
        {
            "name_short": "Community-integrated validation",
            "name_full": "Community-integrated validation and continuous learning frameworks",
            "brief_description": "A proposed framework that combines automated testing, community expert feedback, peer review, and continuous model retraining to validate AI-generated scientific code and keep validation artefacts current with evolving scientific knowledge.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_or_method_name": "Community-integrated validation and continuous learning frameworks",
            "scientific_domain": "Scientific computing (cross-domain: physics, chemistry, materials, biology, astrophysics, etc.)",
            "validation_type": "hybrid",
            "validation_description": "Described as an ecosystem-level approach that automatically generates test cases grounded in scientific principles, incorporates expert human vetting (peer review) of AI outputs, and feeds corrections back into continuous learning loops for AI models; techniques cited include property-based and metamorphic testing, checks against analytical solutions where available, and validation against observations or experiments when possible. Emphasizes living validation systems that evolve with domain knowledge and peer contributions rather than one-off static test suites.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "The report recommends validating generated code against both analytical solutions and observations/experiments where possible but does not present any concrete comparisons; it calls for frameworks that can support these comparisons.",
            "validation_success_rate": null,
            "domain_validation_standards": "Paper argues for development of domain-specific validation frameworks and standards (including tests that check physical laws, conservation properties, and reproducibility across environments) but does not prescribe specific numeric thresholds; emphasizes that standards should include reproducibility, uncertainty quantification, and peer review.",
            "when_simulation_sufficient": "Paper suggests simulation-based validation may be sufficient when it can be tied to conserved physical properties, known analytical limits, or when experiments are infeasible, but argues that community oversight and formal verification increase confidence; no strict rule provided.",
            "simulation_failures": "Paper warns generally that AI-generated code can violate domain constraints (e.g., physical laws, numerical stability) and thereby produce invalid simulations; no specific dataset or case study is provided in this report.",
            "uncertainty_quantification": "The framework calls for integrating UQ into validation pipelines (uncertainty estimates and error propagation for AI-generated code) and for reporting confidence in generated artifacts; concrete UQ methods are recommended but not implemented here.",
            "fabrication_detection": "Discusses need for provenance, peer review, and community auditing to detect fabricated or spurious AI outputs; recommends metadata and traceability rather than proposing a single detection algorithm.",
            "validation_cost_time": "Notes qualitatively that hybrid community-integrated validation will require sustained resource investment (computational resources for testing, human expert time for review, infrastructure for continuous retraining) but provides no quantitative cost/time figures.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Acknowledged limitations include resource intensity, reliance on available domain expertise (bottlenecks for review), the evolving nature of scientific knowledge requiring continual updates, and gaps where ground-truth experimental data are unavailable.",
            "acceptance_credibility": "Paper states that community-integrated validation (with peer review and formal guarantees) is likely to increase acceptance and credibility of AI-generated code in the scientific community, but evidence is prescriptive rather than empirical in this report.",
            "comparison_to_gold_standard": "Calls for comparing AI-generated outputs to traditional verification/validation and uncertainty quantification (V&V/UQ) gold standards but does not provide empirical comparisons or metrics in this report.",
            "uuid": "e2192.0"
        },
        {
            "name_short": "Metamorphic & property-based testing",
            "name_full": "Metamorphic testing and property-based testing for scientific software",
            "brief_description": "Testing paradigms highlighted as especially valuable for validating scientific computations and AI-generated code when exact reference solutions are unavailable: property-based testing validates invariants/properties, while metamorphic testing checks consistent transformations of inputs/outputs.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_or_method_name": "Property-based testing and metamorphic testing",
            "scientific_domain": "Scientific computing (fluid dynamics, multiphysics, general numerical simulation domains)",
            "validation_type": "computational validation",
            "validation_description": "The report endorses property-based testing (asserting and checking conserved quantities or invariants such as mass/energy conservation) and metamorphic testing (applying known input transformations and checking corresponding changes to outputs) as automated computational validation methods that are useful when exact analytic solutions or experimental ground truth are unavailable. Examples include checks for divergence-free velocity fields and mass conservation in fluid simulations.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not provided; these testing methods are positioned as complementary to experimental validation rather than replacements.",
            "validation_success_rate": null,
            "domain_validation_standards": "Suggested as part of domain-specific validation suites; paper recommends these be included in standards but gives no numeric acceptance criteria.",
            "when_simulation_sufficient": "The paper suggests that strong property-based / metamorphic test coverage increases confidence in simulations and AI-generated code and may be sufficient when it enforces key physical invariants and when experiments are infeasible, but it cautions this is not universally sufficient.",
            "simulation_failures": "Notes that AI tools may generate code that passes superficial tests but violates deeper invariants; metamorphic/property tests are recommended to catch such failures, though no case studies are in this report.",
            "uncertainty_quantification": "These testing approaches are not UQ methods per se, but the report recommends combining them with UQ to quantify confidence in passing tests.",
            "fabrication_detection": "These tests can help detect fabricated or bogus outputs by revealing violations of known invariants; the report frames them as a guardrail rather than a complete solution.",
            "validation_cost_time": "Described qualitatively as cheaper than full experimental campaigns, but still requiring engineering effort to design property specifications and metamorphic relations.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Limitations include dependency on having well-defined properties to test, difficulty encoding complex domain invariants, and potential for false confidence if tests are incomplete.",
            "acceptance_credibility": "Paper argues that inclusion of these testing paradigms in CI/CD pipelines and validation frameworks will increase community trust in AI-generated code, but no empirical validation is provided.",
            "comparison_to_gold_standard": "Proposed as pragmatic complements to gold-standard V&V practices (analytical solution comparison, experiments), not replacements; no quantitative comparisons offered.",
            "uuid": "e2192.1"
        },
        {
            "name_short": "Formal verification",
            "name_full": "Formal verification methods tailored to scientific applications",
            "brief_description": "Application of formal methods and mathematical proofs to verify correctness properties of numerical algorithms and implementations, recommended as part of a broader validation toolbox for AI-integrated scientific software.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_or_method_name": "Formal verification for numerical methods",
            "scientific_domain": "Scientific computing / numerical PDE solvers / HPC",
            "validation_type": "computational validation",
            "validation_description": "The report endorses the use of formal methods (proofs of correctness, theorem proving, formal verification of algorithmic implementations) to provide rigorous guarantees about algorithmic properties, stability, and adherence to conservation laws; it references recent work on formal proofs for PDE solvers and calls for methods adapted to scientific codebases and AI-generated code.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not applicable; formal verification provides mathematical guarantees independent of experiment but complements experimental validation.",
            "validation_success_rate": null,
            "domain_validation_standards": "Formal verification is described as a high-assurance standard where applicable; the report calls for integrating formal guarantees into validation standards when feasible.",
            "when_simulation_sufficient": "Formal verification can make simulation results more trustworthy without experiment when the model and its discretization are formally proven to satisfy desired properties, though applicability is limited to problems amenable to formal proof.",
            "simulation_failures": "Report notes that many complex scientific codes are too large or intricate for existing formal approaches, making full formal verification currently impractical for many applications.",
            "uncertainty_quantification": "Formal methods address correctness and invariants but do not by themselves provide statistical UQ; the report calls for combining formal methods with UQ techniques.",
            "fabrication_detection": "Formal verification is not presented as a fabrication-detection tool; instead it provides correctness guarantees for implementations that pass the formal checks.",
            "validation_cost_time": "Acknowledges high upfront cost (human and tooling) for formal verification; not quantified.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Limited scalability to large codebases, high expertise required, and applicability primarily to algorithmically tractable components rather than whole monolithic applications.",
            "acceptance_credibility": "The report suggests that formal guarantees would substantially increase credibility when available, but recognizes such methods will apply only to a subset of problems in practice.",
            "comparison_to_gold_standard": "Presented as a complement or even higher-assurance alternative to empirical testing in limited contexts, but not a general replacement for experimental validation.",
            "uuid": "e2192.2"
        },
        {
            "name_short": "Analytical & observational checks",
            "name_full": "Testing against analytical solutions and observational/experimental data",
            "brief_description": "Traditional V&V practice of checking computational results against known analytic solutions where available, and against experimental or observational data; recommended as core components of validation for AI-generated scientific workflows.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_or_method_name": "Analytical-solution and observation-based validation",
            "scientific_domain": "Simulation-driven domains (fluid dynamics, astrophysics, materials, chemistry, biology, etc.)",
            "validation_type": "hybrid",
            "validation_description": "The report recommends generating test cases that compare outputs to known analytical solutions (when available) and to experiments/observations; it highlights checking conserved quantities and comparing against validated high-fidelity simulations or experimental datasets as primary validation strategies for scientific correctness.",
            "simulation_fidelity": "When used, simulation fidelity is described as 'high-fidelity' when validated physics are modeled (e.g., shock hydrodynamics, nuclear burning in astrophysics), but the report provides no numerical fidelity metrics.",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "The report discusses the need to compare simulations to experiments but contains no reported empirical comparisons; it encourages development of benchmarking and metrics for such comparisons.",
            "validation_success_rate": null,
            "domain_validation_standards": "Advocates domain norms: match against analytical solutions, conservation laws, and experimental data where available; calls for standardized benchmarks and datasets.",
            "when_simulation_sufficient": "Simulations tied to validated physics and analytic limits may be considered sufficient for some purposes; the report cautions about over-reliance on simulation without independent experimental corroboration.",
            "simulation_failures": "Notes risk that AI-generated components may pass superficial tests but fail deeper experimental comparisons; no concrete examples given in this workshop report.",
            "uncertainty_quantification": "Urges inclusion of UQ in comparisons to experimental data to understand confidence and error propagation.",
            "fabrication_detection": "Comparisons to independent experimental datasets and provenance tracking are recommended to reveal fabricated outputs or inconsistent results.",
            "validation_cost_time": "Indicates experimental validation can be costly and sometimes impractical, motivating use of computational and property-based tests; no quantitative cost data given.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Availability of experimental data varies by domain; some experiments cannot be replicated and some phenomena lack analytical solutions, limiting applicability.",
            "acceptance_credibility": "Paper emphasizes that matching experiments and analytics is critical to credibility in the scientific community; it recommends community-driven benchmarks to standardize acceptance criteria.",
            "comparison_to_gold_standard": "Experimental validation and analytic-solution checks are framed as gold standards where available; the report calls for explicit comparisons between AI-generated outputs and these standards but does not provide empirical results.",
            "uuid": "e2192.3"
        },
        {
            "name_short": "SciML vs numerics benchmarking",
            "name_full": "Metrics and benchmarks to compare Scientific Machine Learning (SciML) with traditional numerical methods",
            "brief_description": "A research priority to develop evaluation metrics, benchmarks, and datasets to rigorously compare SciML approaches against established numerical solvers on accuracy, stability, and scientific validity.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_or_method_name": "Benchmarks and metrics for SciML vs traditional numerics",
            "scientific_domain": "Scientific computing, numerical PDEs, fluid dynamics, multiphysics",
            "validation_type": "computational validation",
            "validation_description": "The report calls for creating benchmarks, datasets, and metrics (including accuracy, stability, conservation properties, and uncertainty propagation) to evaluate SciML methods versus traditional numerical solvers; it cites the need to avoid weak baselines and reporting biases and to build domain-appropriate benchmarks (e.g., for PDE solvers).",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Paper emphasizes comparisons between SciML and gold-standard numerical simulations and experiments as necessary, but contains no empirical comparisons itself.",
            "validation_success_rate": null,
            "domain_validation_standards": "Advocates domain-specific benchmarks (including rigorous PDE benchmarks) and clearer reporting standards to assess true gains of SciML methods.",
            "when_simulation_sufficient": "Paper suggests that simulation-based benchmarking can be sufficient to evaluate many algorithmic properties if benchmarks are high-quality and reflect relevant physics; still recommends experimental corroboration for scientific claims.",
            "simulation_failures": "References community literature warning about overoptimistic claims when using weak baselines for ML PDE solvers; no new case studies provided.",
            "uncertainty_quantification": "Recommends including UQ metrics in benchmarks to capture error propagation and confidence.",
            "fabrication_detection": "Not specifically a fabrication-detection method, but robust benchmarks reduce risk of overclaiming or fabricated performance wins.",
            "validation_cost_time": "Benchmarking requires curated datasets and compute investment; costs are discussed qualitatively but not quantified.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Creating representative, high-fidelity benchmarks is challenging; risk of benchmarks not generalizing across domains or being gamed.",
            "acceptance_credibility": "Paper argues that rigorous benchmarking and clear metrics are essential for community acceptance of SciML advances and to avoid overoptimistic reporting.",
            "comparison_to_gold_standard": "Paper calls for direct comparisons to traditional numerical methods as the de facto gold standard in many domains, but does not provide empirical results here.",
            "uuid": "e2192.4"
        },
        {
            "name_short": "Peer review for code-generation",
            "name_full": "Peer-review and expert vetting mechanisms for AI-generated code",
            "brief_description": "Structured human-in-the-loop review processes tailored to AI-generated scientific code, intended to capture domain-specific correctness beyond automated tests and to incorporate corrections back into AI models.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_or_method_name": "Peer review mechanisms for AI-generated code",
            "scientific_domain": "Scientific computing across domains",
            "validation_type": "hybrid",
            "validation_description": "Described as expert vetting integrated into validation workflows: domain experts inspect AI-generated code, validate adherence to scientific principles, and provide structured feedback that can be incorporated into model retraining and continuous learning systems; complementary to automated property-based/metamorphic tests and formal methods.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not applicable within this report; peer review is positioned to assess correctness independent of experiment, or before experiment is attempted.",
            "validation_success_rate": null,
            "domain_validation_standards": "Paper recommends tailoring peer-review practices to code generation outputs and recording provenance and review outcomes as part of validation standards.",
            "when_simulation_sufficient": "Peer review may deem simulation-only validation sufficient for some uses if reviewers judge tests and invariants adequate, but the report recommends caution and context-dependent decisions.",
            "simulation_failures": "Paper notes that peer review is necessary because automated checks alone can miss subtle domain violationsâ€”no empirical examples provided.",
            "uncertainty_quantification": "Peer review does not directly quantify uncertainty but informs confidence assessments and model retraining priorities.",
            "fabrication_detection": "Human review is recommended as a way to detect fabricated or implausible outputs that automated checks miss.",
            "validation_cost_time": "Human expert time is highlighted as a potentially costly but essential part of validation; no numeric costs provided.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Scalability of expert review is a concern; reliance on scarce expert time can be a bottleneck.",
            "acceptance_credibility": "Authoritative expert vetting is argued to significantly improve community trust and acceptance of AI-generated scientific artifacts.",
            "comparison_to_gold_standard": "Peer review is framed as complementary to gold-standard experimental validation and formal V&V; no quantitative comparison provided.",
            "uuid": "e2192.5"
        },
        {
            "name_short": "Uncertainty quantification",
            "name_full": "Uncertainty quantification (UQ) for AI-generated scientific code",
            "brief_description": "Integration of UQ into AI-generated code and validation pipelines to estimate confidence, error propagation, and robustness of predictions and numerical outputs.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_or_method_name": "Uncertainty quantification for AI/SciML",
            "scientific_domain": "Scientific computing, SciML, numerical simulation",
            "validation_type": "computational validation",
            "validation_description": "The report highlights the need to quantify uncertainty for AI-generated code via methods that assess error propagation, confidence intervals, and sensitivity analyses; recommends combining UQ with formal verification and property-based testing to present calibrated confidence in outputs.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "No empirical comparisons provided; the report states UQ should be used when comparing simulation outputs to experiments to express agreement and confidence.",
            "validation_success_rate": null,
            "domain_validation_standards": "Calls for UQ to be part of domain validation standards, with explicit reporting of uncertainty bounds alongside predictions.",
            "when_simulation_sufficient": "Simulations accompanied by rigorous UQ may be sufficient for some decision-making contexts; paper recommends context-dependent assessment.",
            "simulation_failures": "Paper points out that absence of UQ can lead to misplaced confidence in AI-generated code, but provides no specific examples.",
            "uncertainty_quantification": "Explicit focus of this entity; the report urges integration of UQ pipelines but does not prescribe particular algorithms in this document.",
            "fabrication_detection": "Not discussed as a primary fabrication-detection method, though large unexplained uncertainties could flag problematic outputs.",
            "validation_cost_time": "Acknowledges UQ adds computational and methodological overhead; no quantitative estimates provided.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Challenges include designing UQ methods suitable for AI models and complex coupled simulations, and computational cost of ensemble/sampling-based UQ.",
            "acceptance_credibility": "Paper asserts that explicit UQ reporting is essential for community acceptance of AI-driven scientific results.",
            "comparison_to_gold_standard": "UQ is recommended as part of comparisons to experimental gold standards to quantify agreement, but no empirical data are provided.",
            "uuid": "e2192.6"
        },
        {
            "name_short": "Security & poisoning defenses",
            "name_full": "Security-aware validation: detecting adversarial attacks and data poisoning in scientific AI systems",
            "brief_description": "Recognition of adversarial threats (data poisoning/adversarial examples) to scientific AI pipelines and the need for validation approaches that guard against malicious or accidental corruption of models and results.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_or_method_name": "Adversarial robustness and data-poisoning detection for scientific AI",
            "scientific_domain": "Scientific computing, ML/AI workflows",
            "validation_type": "computational validation",
            "validation_description": "The report flags security threats (adversarial attacks, data poisoning) that can undermine validation and recommends integrating threat models, provenance, guardrails, and secure pipelines into validation frameworks; suggests combining automated anomaly detection, metadata provenance, and human review to detect compromised outputs.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not applicable; focus is on integrity of computational pipelines rather than comparing to experiments.",
            "validation_success_rate": null,
            "domain_validation_standards": "Recommends including security considerations and adversarial testing in validation standards for AI in scientific computing.",
            "when_simulation_sufficient": "Paper implies that simulation-based validation without security/hardening may be insufficient if datasets or models are compromised; secure validation requires additional checks.",
            "simulation_failures": "Notes potential for model corruption or data poisoning to produce incorrect yet superficially plausible outputs; no concrete incidents detailed in this report.",
            "uncertainty_quantification": "Not central to detection of adversarial attacks, though anomalous uncertainty signatures could be used to flag issues.",
            "fabrication_detection": "Proposes provenance, anomaly detection, and guardrails (e.g., secure agents, auditing) to detect fabricated or tampered results; no single algorithmic solution is specified.",
            "validation_cost_time": "Security hardening and adversarial testing add engineering and compute costs; not quantified.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Security measures can be complex and may not catch all sophisticated attacks; requires ongoing monitoring and expertise.",
            "acceptance_credibility": "Argues that demonstrable defenses against tampering are necessary for community trust in AI-driven scientific outputs.",
            "comparison_to_gold_standard": "No direct comparison to gold-standard validation; security-focused validation is presented as an essential complement to correctness-focused V&V.",
            "uuid": "e2192.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Assessing the Reliability of Complex Models: Mathematical and Statistical Foundations of Verification, Validation, and Uncertainty Quantification",
            "rating": 2
        },
        {
            "paper_title": "Workshop Report on Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence",
            "rating": 2
        },
        {
            "paper_title": "Report of the DOE/NSF Workshop on Correctness in Scientific Computing",
            "rating": 2
        },
        {
            "paper_title": "Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations",
            "rating": 2
        },
        {
            "paper_title": "Envisioning better benchmarks for machine learning PDE solvers",
            "rating": 2
        },
        {
            "paper_title": "The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation",
            "rating": 2
        }
    ],
    "cost": 0.0183865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Toward Next-Generation Ecosystems for Scientific Computing
7 Oct 2025 September 2025</p>
<p>C Mcinnes 
D Arnold 
P Balaprakash 
M Bernhardt 
B Cerny 
A Dubey 
R Giles 
D W Hood 
M A Leung 
V LÃ³pez- Marrero 
P Messina 
Olivia B Newton 
C Oehmen 
Stefan M Wild 
J Willenbring 
L Woodley 
T Baylis 
D E Bernholdt 
C CamaÃ±o 
J Cohoon 
C Ferenbaugh 
S M Fiore 
S Gesing 
D GÃ³mez-ZarÃ¡ 
J Howison 
T Islam 
D Kepczynski 
C Lively 
H Menon 
B Messer 
M Ngom 
U Paliath 
M E Papka 
I Qualters 
Elaine M Raybourn 
Katherine Riley 
P Rodriguez 
D Rouson 
M Schwalbe 
S K Seal 
Ã– SÃ¼rer 
Valerie Taylor 
L Wu 
Lois Curfman Mcinnes 
Lawrence Berkeley </p>
<p>US Dept. of Energy's Office of Scientific and Technical Information</p>
<p>U.S. Department of Commerce National Technical Information Service
5301 Shawnee Road Alexandria22312VA</p>
<p>U.S. Department of Energy Office of Scientific and Technical Information
P.O. Box 6237831-0062Oak RidgeTN</p>
<p>Next-Generation Ecosystems for Scientific Computing: Harnessing Community
Software</p>
<p>AI for Cross-Disciplinary Team Science</p>
<p>Argonne National Laboratory (retired)
Argonne National Laboratory Dorian Arnold
Emory University Prasanna Balaprakash
Oak Ridge National Laboratory Mike Bernhardt
Team Libra Beth Cerny
Argonne National Laboratory Anshu Dubey
Argonne National Laboratory Roscoe Giles
Boston University Denice Ward Hood
University of Illinois Urbana-Champaign Mary Ann Leung
Sustainable Horizons Institute Vanessa LÃ³pez-Marrero
Stony Brook University Paul Messina</p>
<p>University of Montana Chris Oehmen
Pacific Northwest National Laboratory</p>
<p>Center for Scientific Collaboration and Community Engagement Tony Baylis
Argonne National Laboratory
Los Alamos National Laboratory (retired)
National Laboratory Jim Willenbring
Sandia National Laboratories Lou Woodley
Lawrence Livermore National Laboratory David E. Bernholdt
Oak Ridge National Laboratory Chris CamaÃ±o
California Institute of Technology Johannah Cohoon
Lawrence Berkeley National Laboratory Charles Ferenbaugh
Los Alamos National Laboratory Stephen M. Fiore
University of Central Florida Sandra Gesing
US Research Software Engineers Association Diego GÃ³mez-ZarÃ¡
University of Notre Dame James Howison
University of Texas at Austin Tanzima Islam
Texas State University David Kepczynski, Ford Motor Company Charles Lively
Lawrence Berkeley National Laboratory Harshitha Menon
Lawrence Livermore National Laboratory Bronson Messer
Oak Ridge National Laboratory Marieme Ngom
Argonne National Laboratory Umesh Paliath
GE Aerospace Research Michael E. Papka
University of Illinois Chicago Irene Qualters</p>
<p>Sandia National Laboratories</p>
<p>Argonne National Laboratory Paulina Rodriguez
The George Washington University Damian Rouson
Lawrence Berkeley National Laboratory Michelle Schwalbe
National Academies of Sciences Sudip K. Seal
Oak Ridge National Laboratory Ã–zge SÃ¼rer
Miami University</p>
<p>Argonne National Laboratory Lingfei Wu
University of Pittsburgh</p>
<p>Toward Next-Generation Ecosystems for Scientific Computing
7 Oct 2025 September 2025FA93AEC4D1607F2FFFF904BC14EA385310.48550/arXiv.2510.03413arXiv:2510.03413v2[cs.CE]
Cover image:The cover image represents the deflagration phase in a Type-Ia supernova in a Flash/Flash-X simulation.</p>
<p>The 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science convened in Chicago, IL, from April 29 to May 1, 2025.The event brought together more than 40 experts from high-performance computing (HPC), AI, computational science, software engineering, applied mathematics, social sciences, and community development to chart a path toward more powerful, sustainable, and collaborative scientific software ecosystems.Workshop discussions underscored a powerful truth: scientific computing is at a turning point.As AI grows more capable and scientific questions become more complex, a transformation is unavoidable.The traditional ways we compute, code, and collaborate are no longer adequate.Periods of profound transformation bring both unprecedented opportunity and inherent uncertainty.Hence, addressing the complexity of this moment requires more than incremental progress-it necessitates strategic risk-taking and foundational shifts in approach.Just as the rise of digital computing reshaped nearly every scientific and industrial domain, AI-integrated software ecosystems must undergo that same level of conceptual reinvention, driven by rigorous inquiry and a willingness to redefine established frameworks.</p>
<p>We face the urgent need for a bold, enduring transformation-one that extends beyond technology alone.We envision a new kind of agile and robust scientific computing ecosystem built through socio-technical codesign-i.e, the intentional, integrated development of social and technical components as interdependent parts of a unified strategy.This approach integrates cutting-edge advances in AI and software-such as large language models (LLMs), scientific machine learning (SciML), reasoning models, and coding agents-with strategies to foster cross-disciplinary team building, training, and collaboration.Two key themes emerged:</p>
<p>â€¢ AI is not just another tool-it can dramatically boost scientific productivity, catalyzing new discovery.Moreover, while demonstrably transformative, AI's evolution is not known.â€¢ Supporting this transformation in computational science will require a broad, cross-disciplinary community and sustained investment in preparing the next generation and empowering today's experts.</p>
<p>With these guiding themes, the workshop focused on three major challenges in future scientific computing:</p>
<p>â€¢ Advancing scientific software ecosystems for HPC and AI of today and tomorrow, while ensuring results remain scientifically valid.â€¢ Fostering collaboration among scientists, AI experts, research software engineers, educators, community leaders, and even AI systems themselves-bridging traditionally siloed roles and domains.This includes private-sector partners, key to translating scientific research into real-world applications.â€¢ Rethinking training and workforce development, to help today's and tomorrow's researchers adapt to rapid changes in tools and technology.</p>
<p>The workshop identified research directions and community actions to address these challenges.These include building modular, trustworthy, AI-powered scientific software systems; creating frameworks where humans and AI systems can develop and validate code together; overhauling training pipelines to reflect the pace and nature of modern computational science; promoting responsible innovation with guidelines for AI use; and launching pilot programs and partnerships that test new ways of learning and working.</p>
<p>In short, the workshop laid out a vision for the future: one where AI, software, hardware, and human expertise work hand in hand to accelerate scientific progress, broaden access, build the workforce, and preserve the integrity of the scientific method.This report marks the beginning of a long-term, communitydriven effort to turn that vision into reality.</p>
<p>The workshop gave us a fresh, encouraging perspective.We must recognize that-more than ever-the future of science depends on more than computational power; it depends on how we embed human insight within intelligent systems.When we align AI's capabilities with the creativity, intuition, and discernment of researchers, we open new pathways for discovery.Purposeful integration of technical and societal dimensions creates a living system-one that learns, adapts, and accelerates progress where it matters most.</p>
<p>Significant Challenges Facing Computational Science</p>
<p>Throughout history, the most profound technological advances have challenged society's capacity to adapt and respond at scale.Artificial intelligence (AI), while still immature, is already poised to influence nearly every facet of human endeavor (see, e.g., [1]).Similarly, while its full impact remains uncertain, AI's disruptive and beneficial potential in science and engineering is clear.In turn, how we integrate AI across science and engineering may shape future global leadership.</p>
<p>Scientific discovery through computing underpins U.S. innovation, economic growth, and national security.Over the past fifty years, advances in computing have revolutionized how scientists learn, experiment, and theorize.High-performance computing (HPC)-including modeling and simulation, data analytics, machine learning, and AI-is a necessary means of discovery and innovation in essentially all areas of science, engineering, technology, and society [2,3,4,5,6,7].Today, that pursuit faces formidable challenges.As scientific data, modeling, simulation, and AI grow in volume, complexity, and strategic value, they have become central to the competitiveness of the nation's industrial and manufacturing sectors such as automotive, aerospace, materials, and others.To remain a global leader, the U.S. must act with urgency and intention to elevate its scientific research infrastructure-especially through scientific software ecosystems that integrate AI and advanced computing capabilities.This is no longer a question of optional investment; it is a foundational requirement for ensuring our socioeconomic prosperity, maintaining global competitiveness, strengthening national security, and securing a higher quality of life for future generations.</p>
<p>Meeting this challenge requires broad national commitment and coordination.The scientific computing community cannot transform alone.Government, academia, national labs, and the private sector must invest together in the technical, institutional, and human systems that advance science.Scientific computing must evolve not only in tools and platforms but also in collaboration, governance, and talent development.</p>
<p>To meet this challenge, we must develop and adopt innovative approaches to the integration and application of scientific computing, learning from the past and preparing for the future.Scientific progress now hinges on our ability to integrate AI not only into our research methodologies but also into the underlying software ecosystems that power them [8,9,10,11,12,13].This integration must be safe, reliable, and agile, demanding a new generation of scientific software tools that are robust, scalable, and adaptable to rapidly evolving computing platforms-not retrofits, but purpose-built ecosystems capable of accelerating discovery in areas such as materials design, energy, biology, astrophysics, manufacturing, and security.</p>
<p>Equally important is the human dimension.Scientific advancement depends not only on tools and technology but also on the people who create and use them.We must cultivate a workforce of researchers and technologists who are proficient in developing and using emerging software ecosystems and AI-enabled research methodologies [7,14,15,16,17].These people must be empowered to collaborate across disciplines, think computationally, and adapt rapidly to evolving technologies.Building this talent pipeline is not merely about education-it is about building the intellectual foundation for the nation's scientific enterprise.This is a multidimensional challenge-requiring urgent, coordinated action.We must advance technology, reimagine software, and cultivate a highly skilled workforce simultaneously.The United States aspires to remain a global leader in science and innovation [18], and progress in scientific computing is key to that vision.Over the coming decades, state-of-the-art science will increasingly rely on the effective, pervasive, and responsible use of AI, placing new demands on our tools, methods, and practices.A key enabler of this transformation will be the development of next-generation scientific software that is not only AI-integrated but also scalable, sustainable, and trustworthy.But the transformations needed for effective scientific progress are too large and pervasive to be done piecemeal by independent actors.The scope and urgency of the challenges demand coordinated, community-driven action across institutions, disciplines, and sectors-and ultimately, across national boundaries, while keeping the flexibility required for the dynamic evolution of scientific computing ecosystems.While these early efforts may focus on national coordination, lasting progress will require globally-connected community efforts.</p>
<p>Workshop objectives</p>
<p>To address these critical needs, the workshop Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science was held in Chicago, IL, during April 29 -May 1, 2025. 1 The workshop convened a broad group of experts spanning HPC, AI, computational science, domain sciences, applied mathematics, software engineering, cognitive and social sciences, and community development.Participants collectively examined how we can co-design next-generation scientific software ecosystems to enable cutting-edge research, foster cross-disciplinary collaboration, and scale effectively across institutions and infrastructure.The workshop introduced the methodology of sociotechnical co-design for next-generation scientific computing, explained in Section 1.3, as a complement to insights from other forward-looking community reports (e.g., [8,9,11,19,20,21]).By intentionally blending technical and community factors, the workshop aimed to shape a future where thriving, crossdisciplinary communities drive the next wave of scientific discovery, with high-quality scientific software as a keystone of sustained collaboration and scientific progress.</p>
<p>Scientific computing ecosystems.In the context of this report, ecosystems for scientific computing are dynamic socio-technical systems made up of people, technologies, infrastructure, institutions, workflows, and cultural practices that collectively support the development and evolution of scientific computing.Ecosystems are role-and context-dependent-shaped by the interactions among stakeholders, researchers, developers, users, institutions, and AI systems.A successful ecosystem integrates technical components (such as software libraries, AI technologies, and computational platforms) with social dimensions (such as training, governance, and collaboration norms) to enable scientific progress.We believe that such ecosystems will emerge through the deliberate co-design of software platforms, scientific tools, community structures, and educational models-systems that support rapid change and are coordinated through well-designed, cooperative approaches rather than centralized control.Figure 1 illustrates aspects of these relationships.</p>
<p>Researchers</p>
<p>AI</p>
<p>Community &amp; Workforce Development</p>
<p>Team-based Science</p>
<p>Co-design</p>
<p>Figure 1: Toward next-generation ecosystems for scientific computing: As motivated by the needs of teambased science in an AI-driven future, we must advance software, cross-disciplinary collaboration, and pedagogy (outer ring of this figure) through co-design, all while developing workforce and community.This work demands strong collaboration among researchers (in science domains, applied math, computer science, HPC, AI, etc.), software developers, stakeholders, and computing facilities (inner rings of this figure).</p>
<p>Potential impact</p>
<p>Dynamic computational workflows, encompassing modeling, simulation, data analytics, and emerging AI approaches, as enabled by the instrumental building blocks of scientific software, are key to next-generation science [8,22,23,24,25,26].Ultimately, the promise-and eventual impact-of addressing the urgent challenges introduced above rests on the ability of distributed, cross-disciplinary teams to effectively integrate varied knowledge, perspectives, policies, requirements, and methods toward producing innovative solutions for scientific computing.Figure 2 illustrates the cross-disciplinary collaboration that is typical in advanced computational science-broadly considering simulations in physics, chemistry, materials science, biology, and so on,2 which leverage heterogeneous computing architectures.These scientific codes build on low-level programming models and runtimes, libraries for math and visualization, tools to understand and optimize performance, emerging ML/AI technologies (such as distributed training frameworks, MLOps pipelines, and federated learning systems), application-specific components, and more.Multiphysics simulation requires expertise across science domains, math, CS, and ML/AI, all encapsulated in software Single small team Figure 2: Simulations in advanced scientific computing (including materials science, astrophysics, nuclear energy, biology, engine design, weather prediction, and batteries, as represented by the science images of this figure) require collaboration across science domains, applied mathematics, computer science, ML/AI, and more, where high-quality software is a primary means of encapsulating expertise for use by others.Due to increasing science challenges and complexity, no longer can a single small team (shown on the left-hand side) independently develop all required functionality.Instead, reusable libraries and tools (shown on the right-hand side), developed by teams whose expertise spans across various topics, provide key functionalities that serve many applications.Dashed lines indicate multiple areas of work per person (e.g., the red-colored person in this diagram contributes to application components, development tools, and ML/AI capabilities).</p>
<p>Opportunities for advances via AI technologies abound, including code generation, automated design of experiments, AI agents, and real-time analysis during simulations.For example, a materials science team using AI to predict crystal structures needs seamless integration between calculations of density functional theory, ML training, and HPC scheduling, while building on programming models, systems tools, and so on.Likewise, an astrophysics team using HPC and AI to study supernova explosions needs a multiphysics simulation code with shock hydrodynamics and nuclear burning, which might rely on math libraries for adaptive mesh refinement and on AI for parameterized physics models such as flame and equation of state.</p>
<p>Complexity arises due to heterogeneity across and within the various components constituting nextgeneration scientific computing ecosystems.This heterogeneity exists at many levels, for example in (1) hardware (e.g., CPUs, AI accelerators, quantum); (2) software (e.g., multitude of programming languages, environments, functionalities); (3) algorithms (e.g., mathematical, computer science, statistical, AI, domain science specific); (4) precision of operations (e.g., mixed vs. double precision, quantization in AI models); (5) data (e.g., storage, movement, processing, analysis, visualization); (6) computing environments (e.g., leadership computing facilities, federated, cloud, edge, quantum); and (7) the workforce (e.g., researchers, developers and users of software, domain scientists, management, stakeholders) or, more generally, crossdisciplinary teams, which in next-generation scientific computing may eventually include AI agents alongside humans [34,35,36].This report considers the integration of AI into scientific software development as a transformative opportunity to accelerate discovery while addressing long-standing challenges in code quality, sustainability, and accessibility; other documents consider complementary topics (see, e.g., [37,38,39]).AI's promise lies not just in automating routine coding tasks but also in fundamentally reimagining how scientific software ecosystems evolve.</p>
<p>Collaboration is a primary mode of work in scientific computing, yet its potential is only beginning to be fully realized because of the complexity of cross-disciplinary research.The advent of AI, with its potential to influence human activities related to software development and scientific discovery, adds a transformative new dimension to this challenge.Next-generation ecosystems in scientific computing offer the promise of integrating broad domain expertise, methodological approaches, and AI-driven tools to bridge gaps between theoretical principles and practical implementations.These advances can enable more robust, maintainable, and scientifically accurate software, thereby accelerating scientific discovery.</p>
<p>We can build on experiences of innovative programs such as the U.S. Department of Energy's (DOE's) Exascale Computing Project (ECP; [27,40]), Scientific Discovery through Advanced Computing (SciDAC) program [41], and Computational Science Graduate Fellowship (CSGF) program [42,43]; the National Nuclear Security Administration's (NNSA's) Predictive Science Academic Alliance Program [44]; the National Science Foundation's (NSF's) Science and Technology Centers [45] and Engineering Research Centers [46]; Sustainable Research Pathways [47], and others.These initiatives have pioneered bold approaches for crossdisciplinary collaboration at scale and for cultivating both breadth across disciplines and depth within individual fields-countering traditional academic silos, as required for advances in scientific computing.</p>
<p>Socio-technical co-design for next-generation scientific computing</p>
<p>To tackle these urgent, complex, and intertwined challenges, the workshop introduced a novel methodology of socio-technical co-design for scientific computing, illustrated in Figure 3.The concept of co-design is widely used in many fields, referring generally to collaborative approaches to designing solutions by involving multiple stakeholders (especially those who will be using or are impacted by the outcomes) throughout the design process.For example, in the human-computer interaction community, co-design is strongly associated with participatory design.Co-design in HPC and AI has been critical to the design and implementation of contemporary computer architectures, applications, algorithms, and software, considering their interrelationships (as shown in the left-hand side of this figure) [48,49].</p>
<p>Building on these co-design experiences, the workshop introduced the broader approach of sociotechnical co-design, which embraces the intentional and integrated development of both technical components (e.g., software, AI, infrastructure) and social components (e.g., teams, institutions, practices, training) of scientific computing ecosystems.This expanded approach ensures that social and technical dimensions are addressed not in isolation but as tightly coupled elements of a unified, forward-looking strategy.</p>
<p>Socio-technical co-design for future scientific computing</p>
<p>Technical and social co-design interwoven throughout all aspects of work</p>
<p>Traditional co-design for scientific computing</p>
<p>Researchers and R&amp;D</p>
<p>Applications</p>
<p>Human-AI Collaboration</p>
<p>Pedagogy and Training</p>
<p>Community and Workforce</p>
<p>Figure 3: Socio-technical co-design for next-generation scientific computing intentionally interweaves technical and social elements throughout all aspects of work, while closely coupling cycles of R&amp;D innovation between computing technologies and driving applications.By ensuring that technical and social dimensions are addressed as tightly coupled elements of a unified, forward-looking strategy, this holistic approach accelerates transformative impact across wide-ranging application domains.</p>
<p>Report structure.We organize insights from the workshop across three complementary axes that undergird scientific computing and urgently require better understanding and advancement to prepare for AI's growing role in shaping scientific discovery (see Figure 4):</p>
<p>â€¢ Software ecosystems for AI in scientific computing â€¢ Cross-disciplinary collaboration and AI for scientific software teams â€¢ Pedagogy and workforce development in the age of AI Two crosscutting topics permeate all of these:</p>
<p>Software Ecosystems</p>
<p>FOR AI IN SCIENTIFIC COMPUTING</p>
<p>Pedagogy and Workforce</p>
<p>DEVELOPMENT IN THE AGE OF AI</p>
<p>Collaboration and AI</p>
<p>â€¢ AI as a catalyst to advance software productivity, collaboration, and pedagogy â€¢ Community engagement and capacity building for next-generation scientific computing These domains are deeply interconnected, requiring co-design of technical and social systems.Addressing them holistically presents an opportunity to foster robust and scalable research ecosystems capable of driving scientific discovery in the coming decade.We next delve into these challenges, including crosscuts and interdependencies, and we introduce priority research directions as well as actionable community strategies.</p>
<p>Breaking Down the Challenges</p>
<p>As scientific computing moves toward AI-driven, cross-disciplinary, community-integrated ecosystems, significant challenges arise in aligning technical tools with human needs and institutional capabilities.Sections 2.1 through 2.3 elaborate on challenges in the complementary topics introduced in Section 1, while Section 2.4 discusses crosscutting issues and interdependencies.</p>
<p>Software ecosystems for AI in scientific computing</p>
<p>We define a software ecosystem for AI-enhanced scientific computing as the entire software infrastructure that supports all computational steps involved in scientific discovery, ranging from system software at the lowest level to inference engines at the highest level.The inference engines rely upon data obtained from simulations, observations, archives, and other sources of scientific data.Simulation and analysis tools in turn depend upon middle layers of the software stack such as math and visualization libraries and workflow management tools.Tools to analyze, predict, and optimize performance, energy, and cost of simulations are also essential.The software ecosystems must not only integrate traditional numerical methods and scientific simulations but also accommodate rapidly evolving AI models, data pipelines, and heterogeneous computing environments (e.g., CPUs, GPUs, and quantum systems).While heterogeneity is not new to scientific computing, at present it is growing in complexity and ever-changing, raising questions about CPU-GPU memory transfers, mixed precision training, and quantum-classical hybrid algorithms.Key questions thus revolve around the urgency to identify what will be needed to design, develop, and maintain software that will successfully support next-generation science conducted under heterogeneous, dynamic, and fastevolving environments, while simultaneously building the human enterprise and collaboration protocols.Specific software needs are discussed in the recent Report of the NSF/DOE Workshop on NAIRR Software [11], while software engineering challenges are presented in [20].Creating extensible and sustainable ecosystems will require a strategic blend of technical innovation and community-driven co-design.</p>
<p>Key questions:</p>
<p>â€¢ How can we build extensible, traceable, and modular scientific software ecosystems that support both legacy and AI-generated components, and what degree of intrusion into existing code is needed and acceptable?Can AI assist in decomposing monolithic scientific codes into reusable components that support ecosystem integration?How can AI tools help with refactoring, modernization, and documentation of legacy scientific code?</p>
<p>â€¢ How can AI maintain scientific correctness while accelerating development?Scientific applications must adhere to physical laws, mathematical principles, and domain-specific constraints.How can we train AI systems to generate code that is scientifically valid and numerically stable?</p>
<p>â€¢ What validation frameworks are needed for AI-generated scientific code?Traditional software testing approaches may be insufficient for scientific applications where correctness extends beyond functional requirements to include adherence to scientific principles, uncertainty quantification, and reproducibility across different computational environments.</p>
<p>â€¢ How can standards for AI be designed and integrated into software, applications, tools, and reporting to ensure the technologies can be trusted and adopted by different segments of the scientific computing community with relative ease?Scientific challenges and opportunities:</p>
<p>Fragmented ecosystem and interoperability challenges.Scientific software ecosystems consist of wideranging codebases that already struggle with continually changing hardware paradigms and now must also integrate with modern AI frameworks.We face challenges in bridging traditional scientific computing environments with AI-driven development workflows, including deploying trained models in HPC environments and integrating scientific data formats with AI frameworks.Given that data is a critical component of AIenabled scientific computing, and that computational results are often distributed across teams and siloed in storage systems, seamless access to data remains a major barrier.To support effective collaboration, we need standardized mechanisms for data sharing that address access control, provenance, and metadata.The lack of standardized interfaces, data formats, and integration protocols-especially for deeply integrated components like math libraries-further prevents seamless adoption of AI technologies and forces scientists to navigate multiple, incompatible systems.This fragmentation creates inefficiencies, requires manual intervention between simulation and analysis stages, and limits the ability to leverage AI across entire scientific workflows.Additional challenges relate to the need for performance optimization, such as model quantization, pruning, and knowledge distillation, as well as adaptability in dynamic AI landscapes.</p>
<p>Reliability and validation gap in AI-generated scientific code.A critical challenge is the disconnect between AI's current capabilities and the requirements of scientific computing for reliability and replicability [50,51].Unlike general-purpose software, scientific applications must preserve physical principles, maintain numerical stability, and provide reproducible results across different computational environments.Current AI tools often generate syntactically correct code that may violate domain-specific constraints or introduce subtle numerical errors that compound over long simulations.This point is crucial because natural language specification is by its nature imprecise.Traditionally, programmers eliminate artifacts of imprecise specification through iterative verification and debugging.What can replace or accelerate this iterative convergence to trustworthy software in its development cycle remains an open question.This gap is exacerbated by the lack of standardized validation frameworks specifically designed for AI-generated scientific code, making it difficult to systematically assess whether generated solutions maintain both computational correctness [52] and scientific validity.Federated learning introduces additional challenges, including needs for privacy-preserving collaboration across institutions and addressing security threats such as adversarial attacks on scientific AI models and data poisoning.Additionally, human-in-the-loop and continuous learning mechanisms will remain essential for aligning AI-generated software with evolving scientific understanding.</p>
<p>Development methodologies for hybrid modeling/simulation and AI.While it will be impractical to develop entirely new scientific software ecosystems from scratch, reuse of existing software in these new modalities is likely to be challenging.The changes required to code will be deeply invasive; therefore, intrusion-aware design will become critical to minimize disruption when integrating new tools or AI systems into existing software.There is an ongoing debate in the scientific community about when and where traditional numerics should be used, where emerging scientific machine learning (SciML [53]) methods might replace them, and how the two can most effectively complement one other.We urgently need to develop (and adopt) suitable metrics when evaluating SciML methods against traditional numerical methods [54,55].Middleware to assist in the creation and execution of automated computational workflows will continue to be essential but will now also need consideration for agentic AI systems [56].To support scalable infrastructure growth, such advances require ecosystem-level funding models, broad coordination, and consortia involving national labs, academia, and the private sector.</p>
<p>2.2 Cross-disciplinary collaboration and AI for scientific software teams Because the physical world is not divided into specific domains, those who seek to understand it must also eliminate unnatural boundary lines dividing their discourse.Computational science in general, and high-performance computing in particular, have pioneered advances in cross-disciplinary teams combining computing with science, engineering, and mathematical foundations.Such scientific teamwork, in which an interdependent set of group members share a common purpose, performance goals, and mutual accountability, has grown significantly to meet the needs of organizations and tackle formidable problems [57,58,59,60].However, the growing complexity of challenges in scientific computing demands increasing collaboration among researchers and potentially AI agents-where software is a primary means of collaboration.Cross-disciplinary teams in scientific computing face communication and knowledge integration challenges due to diverse vocabularies, standards, and methodologies [60,61].These challenges are associated with the structure and norms of scientific research; the proliferation of data and computational technologies; the complexity of interactions; and the knowledge, skills, and attitudes present in teams.The rapid evolution of AI technologies complicates each of these issues and also introduces a unique set of challenges.A multipronged strategy addressing macro-, meso-, and micro-level factors (see Figure 5) that influence collaboration in scientific computing is of paramount importance; consequently, we structure the remainder of this section according to these factors.</p>
<p>Macro-level factors (science and society): Structures and norms for collaboration</p>
<p>Key questions:</p>
<p>â€¢ How can existing funding structures be adapted to encourage greater and optimal interconnectedness in the scientific computing community?What new mechanisms, and possibly new sources for funding, are needed to encourage and maintain collaborations in scientific computing?</p>
<p>â€¢ How can existing norms for recognition and credit be adapted to encourage greater and optimal sharing and cross-disciplinary collaboration in the scientific computing community?What novel norms for recognition and credit in cross-disciplinary research are needed to overcome the limitations of siloed, unidisciplinary structures in academia?</p>
<p>â€¢ How can standards for AI technologies be designed to facilitate rather than constrain collaboration and innovation?</p>
<p>Scientific challenges and opportunities: The structures and norms that shape or otherwise enable scientific research are often at odds with the approaches and goals that characterize cross-disciplinary collaboration.These include the availability and distribution of funding (often insufficient for the full scope of multi-institutional, cross-disciplinary multiteam systems [60] required for large-scale computing projects).Another macro-level challenge is norms for recognition and credit, which sometimes neglect or minimize the work of various types of contributors in projects, resulting in decreased motivation, satisfaction, and productivity.Reward systems need to adapt accordingly, reflecting a shared value system for all components of the evolving landscape, including the development and maintenance of the underlying supports such as software "plumbing," project management, and communities that support collective needs.</p>
<p>Meso-level factors (organizations, interactions): Models of governance and knowledge management Key questions:</p>
<p>â€¢ What forms of governance are most beneficial for achieving fair and defensible decision making in cross-disciplinary collaborations in scientific computing?</p>
<p>â€¢ How can these governance models enable effective collaboration, accelerate innovation, and help ensure that all relevant perspectives are represented?What resources and tools are needed to enable the adoption of these governance models in multiteam collaborations?</p>
<p>â€¢ What approaches to knowledge management are beneficial for fostering trust, mitigating knowledge loss, and reducing barriers to contribution and collaboration?</p>
<p>Scientific challenges and opportunities: The complexity of collaboration in scientific computing is associated with the number of stakeholders and the differences in their requirements, approaches, and goals.</p>
<p>Effectively managing this complexity is vital for the quality of collaboration processes and outcomes, including communication, coordination, decision-making, problem solving, performance, and knowledge transfer [60].Access to knowledge must become ubiquitous, fair, and democratic, taking advantage of AI and cloud-based approaches.This challenge introduces an opportunity to envision models of governance and knowledge management that support distributed collaboration across institutions, ensuring shared responsibility for next-generation ecosystems in scientific computing.</p>
<p>Micro-level factors (teams and constituent members): Training for expertise and productivity</p>
<p>Key questions:</p>
<p>â€¢ How can collaboration among various actors in scientific computing (including national laboratories, universities, and the private sector) be leveraged to design and provide opportunities for training in cross-disciplinary research models and teamwork competencies?</p>
<p>â€¢ How can human-centered strategies and AI technologies be designed to support the acquisition of taskwork competencies (e.g., knowledge, skills, and abilities required for writing high-quality software, evaluating and selecting software, modifying and adapting software) as well as teamwork competencies (e.g., knowledge, skills, and abilities required for effective communication, coordination, learning) in scientific computing?</p>
<p>â€¢ How can human-centered strategies and AI technologies be designed to minimize workload, reduce miscommunication, increase efficiency, and augment the capabilities of individuals and teams in scientific computing projects?How can human-centered strategies and AI technologies be designed to enhance team formation and team development across multiteam collaborations?</p>
<p>Scientific challenges and opportunities: Cross-disciplinary research teams face significant challenges in communication, which in turn can reduce their capacity for effectively utilizing collaborative member expertise, developing shared knowledge structures (i.e., shared mental models and transactive memory systems), and integrating their varied knowledge [61].Because cross-disciplinary research demands the integration of the varied, specialized expertise present in a team, its members must be adept in communication and exhibit openness toward one other-competencies and dispositions that influence the team's expertise utilization and knowledge integration.Furthermore, individuals in these teams must develop expertise in topics deemed essential for scientific computing (e.g., reproducibility [51]).Both training and research are thus needed on attitudes, behaviors, and cognition that can enhance cross-disciplinary collaboration in the scientific computing community.Also needed are advances in AI-specific collaboration tools, such as shared Jupyter environments, collaborative model development platforms (GitHub for models), and automated code review systems for AI-generated code.For example, domain scientists need to be able to validate AI-generated finite difference schemes without understanding all details of the underlying transformer architecture.</p>
<p>Multilevel factors: Trust in teammates, software, and AI.Achieving an appropriate level of trust in teammates, software, and AI represents an additional significant challenge.Developing and maintaining trust across the macro, meso, and micro levels of collaborative scientific computing requires insightful leadership, time, repeated interaction, and intervention when trust is lost.A challenge is devising alternative means for achieving trust within an ecosystem-minimizing the workload required to develop trust and manage uncertainty associated with information (e.g., by leveraging wisdom of the crowds and collective intelligence).Also needed are trust protocols for AI agents, for example for AI agents to submit computational jobs to leadership-class facilities.</p>
<p>Pedagogy and workforce development in the age of AI</p>
<p>Pedagogy and workforce development in the computing sciences have always been challenging because of the need to understand and communicate ideas that span multiple disciplines.In academia an added challenge has been the difficulty of fitting in cross-disciplinary training in traditional department structures [5,7].Mission-driven research labs have been the natural homes for such efforts.With the advent of AI and growth of computational science-spanning beyond traditional roots in the physical sciences to include life sciences, social sciences, and virtually all aspects of science and society-there is more opportunity and urgency to integrate cross-disciplinary training.Many science and engineering departments will need to extend their curricula to include developing expertise in exploiting powerful computing platforms, including AI, in much the same way they have trained students in experimental methods in the past.</p>
<p>Key questions:</p>
<p>â€¢ How do we prepare the scientific workforce for an AI-augmented future?As AI tools become more sophisticated, the roles of domain scientists and software engineers will evolve.Critical questions include the following: What fundamental skills remain essential?What specific AI/ML concepts are essential?How do we maintain critical thinking and domain expertise when AI handles routine tasks?How do we ensure that the next generation can validate and improve upon AI-generated solutions?How can we ensure that AI models and datasets reflect a wide range of perspectives grounded in real-world contexts?</p>
<p>â€¢ How can strategies be developed to train and grow workforce communities when the requirements are unknown and rapidly changing?Moreover, how can we broadly enhance and evolve education to foster critical thinking, extend access, expand cross-disciplinary collaboration and communication, and create ubiquitous and democratic access to and implementation of AI for computational science?How can we effectively address community building and workforce development among various stakeholder groups?</p>
<p>â€¢ How can we build training catalogs for community-wide use, including all relevant perspectives, so that we can broaden the pool of scientists and engineers engaged in computational science?</p>
<p>Scientific challenges and opportunities:</p>
<p>Workforce development and knowledge transfer crisis.Perhaps the most profound challenge identified in the workshop is the degree of uncertainty in how to prepare the future workforce for the disruptive changes underway in computing for science.As AI tools become more capable, there is growing concern about creating a generation of scientists who can use AI-generated code but lack the fundamental understanding needed to validate, debug, or extend it.This situation creates a paradox: AI tools designed to democratize software development may inadvertently create new barriers to deep scientific understanding.Simultaneously, the rapid pace of AI advancement makes it difficult for educational institutions and training programs to update curricula as technologies and practices change, while the shift toward AI-assisted development raises fundamental questions about what constitutes essential knowledge in the age of artificial intelligence.Building curricula and the workforce for rapidly changing technological ecosystems requires new instructional and workforce development paradigms, with higher reliance on apprenticeship models, including the social (group) learning that is supported by communities of practice [62].</p>
<p>Strategies to bridge across disciplines.The development of integrated, cross-disciplinary curricula in scientific computing has been a slow and uneven process; despite decades of progress, many academic institutions still lack programs in computational science and engineering.As next-generation scientific challenges increasingly demand complex, team-based approaches that span multiple disciplines, it is essential to accelerate efforts to design curricula that bridge scientific domains, computational methods, and technological modalities.To support this shift, there are growing opportunities to leverage cloud-based AI platforms and containerized environments that enable consistent, scalable training across diverse institutional contexts.We also need to incorporate critical professional competencies often missing from current curricula, such as project management, agile development practices, effective communication, and collaborative teamwork.</p>
<p>Gaps in workforce readiness and training.One of the most persistent challenges in computational science is the limited pool of qualified candidates for recruitment-a problem compounded by the scarcity of formal academic programs in the field.Although various approaches have been attempted, they have not fully addressed the underlying gap in training pathways.Apprenticeship models for hands-on learning have shown great promise as a complementary approach to formal instruction.These are typically well-supported by associated communities of practice, where practitioners can continue to share knowledge and tactics in an ongoing, informal manner that supports continuous learning [63].Another promising approach is to shift from solely recruiting individuals with comprehensive subject-matter expertise to also recruiting based on potential, followed by targeted, domain-specific training.However, this strategy introduces a new set of challenges: developing high-quality training content, keeping it up to date with evolving technologies, and delivering it at scale.These tasks require sustained resource investment and strong coordination across the broader scientific and educational communities.</p>
<p>Crosscuts and interdependencies</p>
<p>To fully understand the challenges and opportunities identified in Sections 2.1 through 2.3, it is essential to consider two crosscutting themes that emerged consistently throughout the workshop: (1) the transformative role of AI in improving scientific software productivity, collaboration, and pedagogy; and (2) the central importance of community engagement and capacity building in enabling and sustaining these transformations in the computing sciences.These crosscutting themes intersect with all three core challenge areas and are foundational to building resilient next-generation scientific computing ecosystems.</p>
<p>AI as a catalyst for scientific productivity and sustainability.Artificial intelligence is not just a topic unto itself-it is a force multiplier across software ecosystems, collaboration frameworks, and pedagogical approaches.Section 2.1 discusses the need for AI-integrated software ecosystems that are modular, traceable, and scientifically valid.AI can play a key role in this work by helping to advance software productivity and sustainability, such as enabling intelligent code generation, automating routine tasks, and supporting decision-making across heterogeneous computing environments.AI also presents new challenges in terms of verification, trust, and intrusion-aware integration.AI can promote collaboration by acting as a bridge across disciplinary boundaries.As discussed in Section 2.2, AI can support shared understanding by translating domain-specific terminology, managing distributed workflows, and facilitating knowledge retention.AI systems can also act as teammates-assisting with code reviews, optimizing team formation, and recommending learning pathways.These roles require careful governance to ensure they support rather than hinder team cohesion and trust.</p>
<p>As explored in Section 2.3, AI can also transform pedagogy by offering new modalities of instruction, such as AI tutors, generative feedback, and adaptive curriculum planning.However, these benefits must be balanced against the risk of over-reliance, which could erode critical thinking and domain expertise.AI-enhanced education systems must therefore be grounded in robust, fair frameworks and designed to complement (not replace) human mentorship and community support.</p>
<p>Community engagement and capacity building as a strategic imperative.As AI transforms the technical foundations of scientific computing, the long-term success of that transformation will depend equally on the strength of the communities that adopt, adapt, and sustain it.Section 2.1 highlights that successful integration of AI into software ecosystems requires more than technical proficiency-it demands fluency in interdisciplinary thinking, sound judgment, and collaborative practices.Building this capacity across communities is essential for maintaining and evolving shared software infrastructure.</p>
<p>As discussed in Section 2.2, the human dimension of collaboration is central.Scientific progress increasingly depends on teams with complementary skills, diverse perspectives, and broad participation.Capacitybuilding strategies-such as training programs, fellowships, internships, apprenticeships, and institutional partnerships-should be designed to foster cross-disciplinary fluency, resilience, and leadership.Community norms, as well as opportunities for professional development, must evolve to recognize and reward all contributors, including those in roles that have historically been undervalued.</p>
<p>Section 2.3 underscores the need to rethink education and community building considering these dynamics.Curriculum innovation, community-led training models, and broad access to computational and human resources are critical.Well-supported communities create networks of practice that encourage and reward innovation and different perspectives, share lessons learned, and mentor the next generation of scientists, developers, and educators.</p>
<p>Together, these two crosscutting themes-AI to advance scientific productivity and sustainability, and community engagement with capacity building for next-generation scientific computing-represent both the engine and the scaffolding of future scientific computing.They are deeply interdependent: AI systems must be designed, deployed, and governed by engaged, well-supported communities; and those communities must be equipped with the tools, practices, and shared knowledge that AI can help accelerate.Advancing one without the other will limit impact.Investing in both will accelerate progress and ensure that nextgeneration ecosystems for scientific computing are not only more powerful, but also widely accessible, trusted, and sustainable.</p>
<p>Required Research Directions and Community Actions</p>
<p>Considering the challenges and opportunities outlined in Section 2, the workshop identified research directions and community actions needed for accelerating progress in next-generation scientific computing.As discussed in Sections 3.1 through 3.3, each priority research area combines practical needs with aspirational goals, aiming to foster a productive interplay between human insight and AI capabilities while maintaining a strong foundation of scientific rigor and community engagement.The recommended research directions emphasize key design principles: modularity, interoperability, trustworthiness, and extensive participation.The intent is to support both near-term implementation and long-term sustainability of emerging software and collaboration models for scientific computing.Section 3.4 outlines strategic community actions that are essential for supporting the software ecosystems, collaborative teams, and educational innovations discussed throughout this report.Taken together, the priority research directions and community actions define a comprehensive agenda for building thriving, resilient, and forward-looking scientific computing ecosystems.</p>
<p>Software ecosystems for AI in scientific computing</p>
<p>Next-generation scientific computing demands a radical rethinking of how software ecosystems are designed, developed, and sustained.As AI becomes more integrated into scientific workflows, software infrastructure must evolve to support new modes of discovery, while maintaining the rigorous standards of scientific correctness, reproducibility, and performance.This section identifies research priorities for building AI-integrated scientific software ecosystems that are modular, trustworthy, and adaptable.The topics below reflect a blend of foundational challenges and forward-looking opportunities, ranging from infrastructure and interoperability, to AI-assisted code generation, to robust validation methods that incorporate community knowledge and formal guarantees.Taken together, these efforts can help create a robust, adaptive foundation for software ecosystems that meet the scale, complexity, and scientific integrity required for the AI-powered future of discovery.</p>
<p>Software and infrastructure for next-generation science.Existing software infrastructure, although rich in abstractions, libraries, and tools, is inadequate for the multipronged challenges of next-generation scientific applications.The growing demands of high-fidelity models and the fragmentation of system software across AI-driven hardware platforms have created major obstacles for scientific teams.Libraries and tools may be underutilized because of the complexity of manually navigating multiple packages, and most science teams cannot feasibly manage the complexity of hybrid HPC/AI environments without new modes of support.Similar software challenges were articulated in the National Artificial Intelligence Research Resource (NAIRR) Software Workshop [11].</p>
<p>A promising direction is to design software substrates that enable AI/ML and traditional simulation codes to interoperate scalably and adapt to evolving workflows, platforms, and algorithms.At present we have little understanding of what such substrates might look like, but we can begin with research to determine and minimize the intrusiveness required to connect legacy scientific software with new AI tools, coordinate multiple coexisting ecosystems (e.g., AI agents interfacing with scientific tools), and analyze how software components interact in real workflows.Of particular importance is exploring the use of AI agents in end-to-end scientific code development workflows, including interactions with math libraries, visualization tools, debuggers, job schedulers, and HPC performance analysis tools.Key areas of research include energy-efficient implementations of numerical and AI methods, tools for software usage analytics, AI-assisted debugging, and strategies for hybridizing traditional solvers with SciML methods.Also needed are metrics to evaluate emerging SciML approaches against traditional numerical methods.</p>
<p>Development of science-aware AI code generation systems.</p>
<p>A key research priority is the development of AI systems designed and trained specifically for scientific computing contexts.This requires more than adapting general-purpose LLMs; it calls for domain-specific training on high-quality scientific codebases, the incorporation of physics-informed constraints, and the use of validation datasets that emphasize correctness as well as functionality.Research should focus on developing hybrid approaches that combine traditional symbolic reasoning with neural methods, enabling AI systems to respect physical laws, dimensional analysis, and numerical stability requirements.Promising techniques include physics-informed neural networks (PINNs), neural operators, and differentiable programming, which offer pathways for AI-generated code to honor physical laws, conserve energy, and maintain numerical stability.Systems must be capable of constrained code generation (e.g., ensuring energy conservation in molecular dynamics) and must integrate domain-specific knowledge graphs that encode scientific principles.Formal verification methods tailored to scientific applications are also essential.Key components include training datasets curated for scientific applications with verified correctness and development of specialized architectures that can reason about mathematical relationships and physical constraints.This research direction should also investigate methods for uncertainty quantification in AI-generated scientific code, enabling users to understand confidence levels and potential error propagation.We also need advances in AI interpretability, such as understanding AI decisions in safety-critical scientific applications.To integrate existing code into new development, research is needed on AI tools that can generate, refactor, and modernize scientific code while ensuring correctness and maintainability.Work is needed to use AI to extract knowledge from existing codebases and generate useful, accurate documentation for end users and contributors.AI evaluations of code bases can also be used to inform teams and leaders of risks in a code base (fragility, limited testing, etc.) and to help teams make decisions about rewriting and refactoring.We must also explore systems where human developers interactively train AI models in context, for example, by correcting recommendations.</p>
<p>Community-integrated validation and continuous learning frameworks.Given the critical importance of correctness in scientific software, research is needed to develop validation frameworks that leverage community knowledge, expert input, and formal guarantees.These systems should support automated generation of test cases grounded in scientific principles, including testing against known analytical solutions where available.Techniques such as property-based testing and metamorphic testing are especially valuable for scientific domains, enabling validation of AI-generated code even in the absence of exact reference solutions.For example, fluid dynamics code produced by AI must satisfy divergence-free velocity fields and conserve mass properties that can be checked independently of a specific output.Research should also explore adaptive "living" validation systems that evolve with scientific understanding and incorporate domain expertise as part of the AI training feedback loop.Peer review mechanisms tailored to code generation could facilitate expert vetting of outputs and structured incorporation of corrections into model refinement.These systems may include code that is validated against observations and experiments that cannot be replicated.The use of formal methods for system correctness [64] and the verification of implementations of numerical methods [65], as well as AI methods [66,67,68], is needed.In addition, as AI-generated code and integrated workflows become more complex and widely adopted, research should address emerging security concerns, including software vulnerabilities and ecosystem-level threats.These approaches should complement continuous integration and testing practices, forming a robust set of validation methods that can sustain accuracy, trust, and progress as AI becomes increasingly integrated into scientific computing.</p>
<p>Cross-disciplinary collaboration and AI for scientific software teams</p>
<p>The challenges of next-generation scientific computing demand teams that can operate across disciplinary boundaries, institutional cultures, and evolving technological landscapes.AI adds a new dimension to this complexity, both as a source of capability and as a potential collaborator.Research in this area must explore how scientific teams can effectively integrate AI systems into their workflows, while maintaining human creativity, trust, and scientific rigor.This includes developing new frameworks for trust, formalizing human-AI team roles, and applying lessons from prior collaborations in science, social science, and the private sector to guide effective engagement throughout the software lifecycle.</p>
<p>A multilevel model of trust: Teams and technology.Trust-foundational to effective collaboration-has been studied extensively as a concept; rich literature exists describing its presence in teams, differences between trust and distrust, and its application to technologies including those endowed with autonomy [69,70,71].A multilevel, dynamic model of trust is needed to guide cross-disciplinary teams working in hybrid HPC/AI environments.This includes constructs such as propensity to trust, perceived trustworthiness, trust in AI, trust in automation and automated systems, transparency, and reliance.Such a model must account for interactions across individuals, teams, and technologies, helping uncover opportunities to build and maintain trust, while also reducing the likelihood of cascading failures caused by misplaced trust or unaddressed uncertainty.</p>
<p>Roles for AI: Teammate, trainer, tool.As AI becomes increasingly embedded in scientific research, clarifying its roles within and across teams becomes essential.We must examine modes of human-AI teaming that augment human creativity and decision-making, especially in scientific code development and debugging.For example, AI-assisted collaboration tools could assist with translation between domain vocabularies (such as between scientific notation used by physicists and programming constructs familiar to research software engineers).To effectively integrate AI in teams and mitigate the likelihood of negative outcomes associated with the adoption of AI, research is needed to formalize and classify the various forms of taskwork and teamwork [60] in scientific computing.Doing so could enable, for example, the development of an adaptive taxonomy of roles and tasks for AI that is informed by the capabilities and limitations of the technology, the needs of the team, and the particulars of the domain.This configurable knowledge base could guide team design, risk management, and human-AI interaction strategies across the software lifecycle.</p>
<p>Effective engagement within cross-disciplinary teams.We must draw on insights from past scientific computing projects and other endeavors to foster environments that support extensive participation and cross-disciplinary success in next-generation computational science [61,72,73].This includes updating institutional policies and incentives to recognize collaborative work [74] and embedding communication norms [75], governance structures, and knowledge-sharing practices that welcome contributions from a wide range of stakeholders throughout the entire software lifecycle-including funding acquisition; software design, development, and testing; developer and user (re)training; and use and maintenance of the software itself to conduct science.Efforts to codify best practices, define success metrics, and incentivize adoption are essential for building resilient, high-performing teams for the AI-augmented scientific future.</p>
<p>Pedagogy and workforce development in the age of AI</p>
<p>A perennial shortage of well-qualified scientists and engineers with the breadth of knowledge needed for today's and tomorrow's scientific computing underscores the urgent need for new pedagogical approaches, training programs, and workforce strategies.These efforts must support the development of crossdisciplinary skills, incentivize and promote ways to integrate different methodologies, ease onboarding into complex research projects, and enable continuous learning to keep pace with rapid technological evolution.</p>
<p>Workforce development and human-AI collaboration models.Research is urgently needed to understand how workforce development for scientific computing must evolve in AI-augmented environments.This includes empirical studies of how AI tools influence learning, skill development, and scientific reasoning among students and researchers at different career stages.Key questions include the following: Which fundamental skills remain essential when AI handles routine coding tasks?How can pedagogical approaches combine AI assistance with deep understanding of scientific principles?How should assessment methods evolve to evaluate comprehension in AI-assisted contexts?This research should also explore new models of human-AI collaboration in scientific software development, including optimal division of labor, interface design for effective teaming, and strategies for maintaining human expertise and critical thinking while leveraging AI's computational advantages.</p>
<p>Cross-disciplinary curriculum design.Educational programs must integrate AI, software engineering, and scientific domain knowledge-from K-12 through graduate training.Identifying key competencies (e.g., trustworthy and fair AI, reproducibility, teamwork) is essential for preparing future software de-velopers in scientific fields.Curriculum development must become more flexible and modular, enabling faster iteration and responsiveness to the changing landscape.This includes fostering partnerships across academia, national labs, and the private sector.Professional development programs should also address cross-disciplinary collaboration skills and provide transformational hands-on experiences that illustrate the impact and excitement of teamwork in scientific computing, while fostering collaborative environments that respect and adapt to different domain cultures.</p>
<p>Innovative curricula and programs incorporating socio-technical co-design.New instructional models are needed that emphasize project-based learning, especially through collaborations among the private sector, academia, and national laboratories.These models should extend beyond STEM domains to include the humanities and other relevant disciplines, ensuring a broader understanding of scientific and societal impact.The models should also include thoughtful integration of career pathways to support entry at multiple points, meeting people where they are, rather than requiring people to enter with a specific set of pre-defined prerequisites.In addition, we should capture lessons from successful efforts.For example, the collaboration model adopted by the Exascale Computing Project, in partnership with DOE leadership computing facilities, was highly successful in launching the exascale era by co-creating scientific applications, human capital, and a robust scientific software ecosystem.Not only did ECP bridge research and computing facilities in a co-design approach, ECP included early attempts at socio-technical co-design when it added talent development through the Sustainable Research Pathways [76] program and training pipelines through the Center for Scientific Collaboration and Community Engagement (CSCCE) [77].</p>
<p>Community and workforce success metrics.Clear, ambitious metrics are needed to measure progress in building the HPC+AI workforce.For example, a pilot program to advance the workforce for next-generation scientific computing could aim to increase the talent pool by at least 1,000 people over five years, averaging more than 200 new and upskilled contributors annually who are equipped to tackle emerging challenges in scientific software ecosystems.Well-defined metrics can help align national efforts, guide strategic investments, and ensure that workforce development keeps pace with evolving technical demands.</p>
<p>Required community actions</p>
<p>While technical research is a key driver of innovation in scientific computing, the workshop emphasized that progress in foundational scientific software also relies on sustained collaboration across institutions, disciplines, and sectors.Building trustworthy, capable, and sustainable software ecosystems requires more than technical breakthroughs-it calls for strategic coordination, shared infrastructure, supportive policies, and workforce investment.Various organizations, projects, and foundations-such as the Consortium for the Advancement of Scientific Software, High Performance Software Foundation, IDEAS project, NumFO-CUS foundation, Research Software Alliance, Software Sustainability Institute, and US Research Software Engineering Organization-are making meaningful strides in shifting the cultural and structural foundations of scientific software [78].Raising awareness of these efforts and contributing to them-whether directly or through complementary action-will benefit the broader scientific community.Much work remains to be done, however, particularly in advancing high-impact partnerships and addressing persistent gaps in infrastructure, recognition, and career paths.</p>
<p>Strategic public-private partnerships.Of the many strategies discussed, this emerged from the workshop as a central priority.Stronger collaboration among academic institutions, national laboratories, and the private sector is essential to the future of AI-enabled scientific software.These partnerships must go beyond short-term research goals to support co-designed solutions that align with long-term scientific needs and technological shifts.The private sector includes both commercial developers of computing technologies-such as hardware manufacturers, cloud service providers, and AI platform companies-and major industrial users of scientific software.Companies across the automotive, aerospace, energy, manufacturing, and materials sectors (e.g., Ford, GE Research, Boeing, and others) depend on high-performance computing and simulation to support product design, optimization, and predictive modeling.Their engagement brings valuable application-driven insights, real-world performance requirements, and opportunities to accelerate the translation of foundational advances in computing into practical tools.Sustained coordination is needed to shape not only the technical requirements of software tools but also the practices and training ecosystems that support their adoption.Joint efforts might include cross-sector learning communities, affinity groups, shared training initiatives, and advisory partnerships that bring together HPC practitioners, AI developers, and domain scientists.Of particular importance is the need to align the rapidly evolving commercial AI and hardware landscapes with the long-term needs of the scientific community.These shared efforts play a vital role in maintaining national competitiveness, advancing research, and ensuring that science, engineering, and manufacturing progress together for long-term socioeconomic resilience.</p>
<p>Community engagement and broad-access infrastructure.As scientific software projects grow in scale and complexity, they require engagement models that foster trust, transparency, and shared ownership across teams and institutions.Building strong, multi-institutional collaborations depends on clear frameworks that support distributed leadership and far-reaching participation.Key strategies include implementing contributor agreements, project charters, and open governance models that reflect a wide range of stakeholder voices.Equally critical is sustained effort to ensure extensive, fair access to infrastructure and participation.This means funding and scaling infrastructure projects that lower barriers for under-resourced institutions and communities-such as through low-cost cloud access, peer mentoring networks, and national initiatives.</p>
<p>Recognition, incentives, and long-term community investment.The sustainability of scientific software ecosystems depends on valuing and supporting the full range of essential contributions, including development, documentation, user support, training, maintenance, and community stewardship.These roles must be reflected in hiring, promotion, and funding decisions and reinforced through long-term investment strategies.Stable, visible support for these roles strengthens the resilience of the overall ecosystem and builds the capacity needed for lasting progress.</p>
<p>Responsible innovation guidelines and fair AI.Advancing AI-driven scientific discovery without oversight regarding fairness risks opening a Pandora's box of unintended consequences.Community-wide guidelines for the responsible integration of AI are essential and must be treated as a top priority.These guidelines should address algorithmic transparency, data governance, and reproducibility, while also establishing norms to prevent bias, protect users, and anticipate unintended consequences.A key part of this effort is ensuring that datasets are complete and representative of the full range of individuals and perspectives they aim to serve.Responsible innovation must remain a core design principle as both tools and teams evolve.These policy and collaborative efforts are not secondary to technological innovation-they are foundational.By prioritizing strategic partnerships and reinforcing them with community-driven practices, institutional reforms, and long-term investment, while considering the long-term needs of both the nation's research and industrial sectors, we can address the evolving demands of software-and AI-driven science.Ultimately, this integrated approach will foster ecosystems where technological breakthroughs and social progress advance hand in hand, reinforcing one another for a more innovative, responsible, and resilient future.approach to fully harness community, software, and AI for cross-disciplinary team science.This means not only developing new software and hardware but also creating systems of practice-collaboration norms, training models, and governance strategies-that evolve with the technology.We launched a series of workshops to begin a dialogue in the computational science community and other related disciplines on how to confront and resolve these challenges.This report summarizes the outcomes of the first workshop in the series, representing the first step in a multiyear effort to understand how to co-design next-generation ecosystems for scientific computing.We envision ecosystems that are not only technologically advanced but also grounded in shared responsibility, fair practices, and a renewed commitment to bold, visionary science.</p>
<p>The workshop identified challenges and opportunities across three core domains: software infrastructure, cross-disciplinary collaboration, and pedagogy.Two crosscutting themes emerged: the transformative role of AI and the foundational importance of workforce and community development.Addressing these requires coordinated action on several fronts: advancing modular AI-integrated software; developing practices and tools that foster effective cross-disciplinary collaboration; and reimagining training for hybrid human-AI environments.Equally critical is aligning institutional policies, governance models, and community norms to ensure the long-term sustainability and growth of scientific software ecosystems and communities.Pilot projects can catalyze progress in these areas, with priorities evolving over time:</p>
<p>â€¢ Near-term (1-2 years): Launch pilot programs on hybrid AI/HPC software infrastructure as well as cross-disciplinary collaboration and pedagogy for AI-driven scientific computing, establish responsible AI guidelines, and prototype public-private partnerships.</p>
<p>â€¢ Mid-term (3-5 years): Explore scaling approaches for modular AI/HPC software ecosystems, launch workforce training curricula, expand community development, implement and evaluate several institutional policy reforms.</p>
<p>â€¢ Long-term (5+ years): Develop and evaluate globally networked ecosystems for AI-driven scientific computing, explore frameworks for community governance at scale, seek ways to integrate AI agents as active research collaborators.</p>
<p>Evolving next-generation scientific computing ecosystems, with many technical and social requirements, may seem daunting.However, the challenges also bring opportunities to learn from past successes and plan for agile approaches to grow communities and workforces poised to evolve alongside the changing landscape of HPC technologies and AI.Times of great change are marked by both enormous promise and significant uncertainty.Meeting this moment will require not just incremental improvements but bold, risktolerant thinking.Just as the dawn of quantum physics revolutionized our understanding of the universe, today's scientific landscape demands a return to that spirit of curiosity, invention, and courage to chart new waters.Human ingenuity-coupled with emerging AI capabilities-must be placed at the center of scientific progress.By intentionally interweaving technical and social components in next-generation scientific computing, we can create feedback loops that accelerate both scientific discovery and real-world impact.</p>
<p>A Glossary</p>
<p>This report aims to communicate across a broad community, including experts in high-performance computing, AI, compuational science, social and cognitive science, team science, community and workforce development, and related topics.To support our goal of communicating clearly across disciplines, this glossary defines terms as used in this report.</p>
<p>â€¢ AI for science: The next generation of methods and scientific opportunities in computing, including the development and application of AI methods (e.g., machine learning, deep learning, statistical methods, data analytics, automated control, and related areas) to build models from data and to use these models alone or in conjunction with simulation and scalable computing to advance scientific research [13].</p>
<p>â€¢ Cross-disciplinary: Collaboration among multiple disciplines toward a shared objective [60].</p>
<p>â€¢ Ecosystems for scientific computing: Dynamic socio-technical systems made up of people, technologies, infrastructure, institutions, workflows, and cultural practices that collectively support the development and evolution of scientific computing.[page 2 of this report]:</p>
<p>â€¢ Science of team science: Research area in which scholars from various disciplines, such as psychology, organizational sciences, sociology, communication, and philosophy, contribute conceptually and empirically to understanding how science teams are organized and work together, how to best measure their effectiveness, and the implications of individual differences in team science [60].</p>
<p>â€¢ Socio-technical co-design for scientific computing: The intentional and integrated development of both technical components (e.g., software, AI, infrastructure) and social components (e.g., teams, institutions, practices, training) of ecosystems for scientific computing, ensuring that social and technical dimensions are addressed not in isolation but as tightly coupled elements of a unified, forwardlooking strategy [page 4 of this report].</p>
<p>â€¢ Taskwork: Activities associated with achieving a team's goals [60].</p>
<p>â€¢ Teamwork: Interactions among team members that are essential for effective collaboration [60].</p>
<p>â€¢ Team science: Collaborative, interdependent research conducted by more than one individual [60].-Fostering collaborative environments that tap into the strengths of a broad workforce, bridging technical and community-building efforts.</p>
<p>Acronyms</p>
<p>Workshop Objectives</p>
<p>We aim to shape a forward-looking plan for the future of team-based software in scientific computing, along with actionable strategies to bring it to life, all while cultivating a vibrant community.Key areas of focus-considering emerging AI technologies and the needs of cross-disciplinary research-will include:</p>
<p>â€¢ Understanding scientific software practices: Building a shared understanding of methodologies for team-based software in HPC, explicitly integrating technical and community perspectives.â€¢ Characterizing roles and success factors: Identifying critical roles in scientific software teams and community-based success factors that sustain effective software development and user engagement.â€¢ Identifying challenges and opportunities: Examining gaps (both technical and community-driven) that hinder team effectiveness and highlighting opportunities to address emerging research needs.â€¢ Envisioning the future: Considering bold ideas for next-generation scientific software, emphasizing synergy between technical solutions and community dynamics.â€¢ Fostering community development: Developing actionable steps to strengthen the workforce and build a vibrant software community prepared to meet urgent challenges in HPC and AI-driven scientific computing.</p>
<p>By intentionally blending technical and community factors, the workshop aims to shape a future where thriving, cross-disciplinary communities drive the next wave of scientific discovery, with high-quality scientific software as a keystone of sustained collaboration and scientific progress.</p>
<p>Workshop Outcomes</p>
<p>Participants will map the landscape of team-based scientific software, characterize key roles and success factors, prioritize focus areas, and develop strategies to shape the future of scientific computing.These integrated insights-to be documented in a post-workshop report-will inform future efforts and may drive changes in perspectives, policies, and practices throughout the scientific computing community.This workshop marks the first of a three-year series dedicated to strengthening team-based scientific software in an AI-driven future, with each year incorporating the co-design framework to ensure that both technical and community needs are addressed:</p>
<p>â€¢ Year 1 emphasis: Identifying and understanding challenges, gaps, and opportunities â€¢ Year 2 emphasis: Developing and evaluating strategies and progress â€¢ Year 3 emphasis: Coordinating ecosystem-wide advancements that meld technical solutions and community-building By intentionally weaving community considerations into technical discussions-and vice versa-this threeyear series will continuously expand the community base and help to advance next-generation scientific discovery.</p>
<p>FORFigure 4 :
4
Figure 4: Report structure.</p>
<p>Figure 5 :
5
Figure 5: Team challenges span throughout macro-, meso-, and microlevel factors.</p>
<p>â€¢</p>
<p>AI: Artificial intelligence â€¢ HPC: high-performance computing â€¢ CPU: Central processing unit â€¢ GPU: graphics processing unit â€¢ STEM: science, technology, engineering, and mathematics â€¢ NSF: National Science Foundation â€¢ DOE: U.S. Department of Energy -Addressing how varied perspectives and AI can accelerate development, use, performance, and impact of scientific software.-Embedding community-based feedback loops to align AI-driven tools with real-world needs and foster broader engagement.â€¢ Community and workforce development -Accelerating strategies to cultivate future-generation R&amp;D teams in the computing sciences.</p>
<p>Contents Executive Summary: Reimagining Scientific Computing in the Age of AI ii 1 Significant Challenges Facing Computational Science 1.1 Workshop objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1.2 Potential impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1.3 Socio-technical co-design for next-generation scientific computing . . . . . . . . . . . . . . 2 Breaking Down the Challenges 2.1 Software ecosystems for AI in scientific computing . . . . . . . . . . . . . . . . . . . . . .2.2 Cross-disciplinary collaboration and AI for scientific software teams . . . . . . . . . . . . .2.3 Pedagogy and workforce development in the age of AI . . . . . . . . . . . . . . . . . . . .2.4 Crosscuts and interdependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Executive Summary: Reimagining Scientific Computing in the Age of AI4 Conclusion and Next StepsAcknowledgmentsReferencesA GlossaryB Workshop DescriptionC Workshop ParticipantsD Workshop Agenda
3 Required Research Directions and Community Actions 3.1 Software ecosystems for AI in scientific computing . . . . . . . . . . . . . . . . . . . . . .3.2 Cross-disciplinary collaboration and AI for scientific software teams . . . . . . . . . . . . .3.3 Pedagogy and workforce development in the age of AI . . . . . . . . . . . . . . . . . . . .3.4 Required community actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p>
<p>3
Programming models and runtimesData analytics and visualization Systems toolsML/AI Math librariesDevelopment toolsApplication Components
Appendices B, C, and D provide the workshop description, list of participants, and agenda, respectively.
For example, see the wide range of scientific applications within the recent DOE Exascale Computing Project (ECP)[27,28].
For example, E4S[29] provides a foundational HPC-AI software ecosystem for science[30], including ECP libraries and tools[31,32,33] as well as popular AI packages.
Scientific computing is entering a new era-driven by the integration of AI, the complexity of crossdisciplinary collaboration, and the growing demand for scalable, sustainable software ecosystems. As introduced in Figure3, advancing next-generation scientific computing requires a socio-technical co-design
AcknowledgmentsThis workshop was partially supported by the U.S. Department of Energy (DOE) Office of Science Distinguished Scientist Fellows Program.We especially thank our DOE contacts: Hal Finkel and David Rabson, DOE Office of Advanced Scientific Computing Research (ASCR).We thank Katie Antypas (National Science Foundation, Office of Advanced Cyberinfrastructure), April Hanks (Team Libra) and Christina Mihaly Messina (SNL) for insightful contributions to workshop discussions on next-generation ecosystems in scientific computing, which helped shape the ideas conveyed in this report.We are grateful to Suzanne Parete-Koon (ORNL) for detailed feedback on the document; her suggestions improved the precision and perspective of the report.We thank Gail Piper for editing this manuscript.Argonne is a U.S. Department of Energy laboratory managed by UChicago Argonne, LLC under contract DE-AC02-06CH11357.The Laboratory's main facility is outside Chicago, at 9700 South Cass Avenue, Lemont, Illinois 60439.For information about Argonne and its pioneering and technology programs, see www.anl.gov.B Workshop DescriptionThe 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science engaged over 40 cross-disciplinary experts in Chicago, IL from April 29 to May 1, 2025, to chart a path toward more powerful, sustainable, and collaborative scientific software ecosystems.Workshop ChargeThe HPC community has long been a leader in advancing scientific discovery to new frontiers.High-quality scientific software-which encapsulates expertise across disciplines for use by others-is a primary means of sustained collaboration and scientific progress.Motivated by urgent issues raised in recent community reports on the state of scientific software development and AI for science, energy, and security, the workshop will bring together cross-disciplinary experts-in HPC, AI, computational science, applied math, computer science, research software engineering, cognitive and social sciences, and community development-to identify challenges, prioritize gaps, and explore opportunities that will shape next-generation ecosystems for scientific computing.The workshop will follow a co-design methodology that intentionally weaves together topics in team-based scientific software, AI in scientific computing, and community/workforce development, thereby ensuring a holistic approach.Areas of emphasis.The workshop's goal is to assess and transform team-based scientific software, with emphasis on building ecosystems to address the needs of next-generation research in scientific computing, while advancing emerging AI technologies.We will integrate technical and community considerations throughout discussions on:â€¢ Software and next-generation science -Exploring how new scientific frontiers and heterogeneous computing architectures require new approaches for software and workforce development.-Incorporating community-building strategies to ensure that new technical solutions reflect the perspectives and expertise of all stakeholders representing a broad swath of backgrounds.â€¢ Software ecosystems for AI in scientific computing -Mapping pathways to create robust, scalable software ecosystems that catalyze AI-driven discoveries in HPC contexts and produce increased innovation.-Developing more robust AI tools and resources for scientific computing that address current gaps and include wide-ranging perspectives.â€¢ Team-based software and cross-disciplinary research -Identifying roles, methodologies, and best practices for team-based scientific software.-Emphasizing community co-design, where software solutions evolve through iterative dialogue among scientists, developers, and end users.â€¢ AI for scientific software productivity and sustainabilityC Workshop ParticipantsOrganizing Committee
Anthropic Economic Index report: Uneven geographic and enterprise AI adoption. Anthropic, Sept 2025</p>
<p>ASCR@ 40: Highlights and Impacts of ASCR's Programs. Bruce Hendrickson, Buddy Bland, Jackie Chen, Phil Colella, Eli Dart, Jack Dongarra, Thom Dunning, Ian Foster, Richard Gerber, Rachel Harken, 10.2172/16318122020US DOE Office of ScienceTechnical report</p>
<p>Future directions for NSF advanced computing infrastructure to support U.S. science and engineering in 2017-2020. William D Gropp, Robert J Harrison, 2016National Academies Press</p>
<p>National Science Foundation Advisory Committee on Cyber-Infrastructure. David Keyes, Valerie Taylor, Task Force on Software for Science and Engineering. 2011final report</p>
<p>Research and education in computational science and engineering. U RÃ¼ede, K Willcox, L C Mcinnes, H De Sterck, 10.1137/16M1096840SIAM Review. 6032018</p>
<p>Transforming Science Through Cyberinfrastructure: NSF's Blueprint for a National Cyberinfrastructure Ecosystem for Science and Engineering in the 21st Century: Blueprint for Cyberinfrastructure Learning and Workforce Development. 2021NSF Office of Advanced Cyberinfrastructure, CISE</p>
<p>Bruce Hendrickson, SIAM Task Force Report: The Future of Computational Science. 2025</p>
<p>Jonathan Carter, John Feddema, Doug Kothe, Rob Neely, Jason Pruet, Rick Stevens, 10.2172/1986455Advanced Research Directions on AI for Science, Energy, and Security: Report on Summer 2022 Workshops. 2023</p>
<p>A Michael, David E Heroux, Lois Curfman Bernholdt, John R Mcinnes, Daniel S Cary, Elaine M Katz, Damian Raybourn, Rouson, 10.2172/1846009Basic Research Needs in The Science of Scientific Software Development and Use: Investment in Software is Investment in Science. 2023Report of the DOE Advanced Scientific Computing Research Workshop</p>
<p>Michael E Papka, K Dhabaleswar, Ilkay Panda, Wahid Altintas, Ewa Bhimji, Murali Deelman, Nicola Emani, Daniel S Ferrier, Lois Curfman Katz, Anita Mcinnes, Feiyi Nikolich, Jim Wang, Willenbring, Final Report of the 2024 NSF/DOE Workshop on NAIRR Software, 2025. US National Science Foundation Report. </p>
<p>Charting a Path in a Shifting Technical and Geopolitical Landscape: Post-Exascale Computing for the National Nuclear Security Administration. 10.17226/269162023The National Academies PressWashington, DCNational Academies of Sciences, Engineering, and Medicine</p>
<p>. Rick Stevens, Valerie Taylor, Jeff Nichols, Arthur Barney Maccabe, Katherine Yelick, David Brown, 10.2172/1604756AI for Science. DOE Report. 2020OSTI</p>
<p>B Chapman, H Calandra, S Crivelli, J Dongarra, J Hittinger, S Lathrop, V Sarkar, E Stahlberg, J Vetter, D Williams, 10.2172/1222711DOE Advanced Scientific Advisory Committee (ASCAC): Workforce Subcommittee Letter. 2014</p>
<p>Roscoe Giles, Transitioning ASCR after ECP. 2020</p>
<p>Supercharging America's AI Workforce. 2024DOE Office of Critical and Emerging Technologies</p>
<p>Response to the ASCAC charge to review the Computational Science Graduate Fellowship program. Prasanna Balaprakash, Tina Brower-Thomas, Jennifer Gaudioso, Susan Gregurick, William D Gropp, Arthur Maccabe, Irene Qualters, Mark E Segal, Valerie Taylor, David Torres, Stefan M Wild, 2025U.S. Department of Energy, ASCACReport</p>
<p>Vision for American Science and Technology (VAST): Unleashing American Potential. Sudip Parikh, AAAS Task Force. 2025</p>
<p>Post Exascale Software in the ASCR Facilities Ecosystem. Saswata Hier-Majumder, Wahid Bhimji, Brandon Cook, Graham Heyes, Kalyan Kumaran, John Macauley, Bronson Messer, Philip Roth, Jiachuan Tian, Brice Videau, December 2024</p>
<p>Architecting the Future of Software Engineering: A National Agenda for Software Engineering Research &amp; Development. Anita Carleton, Mark H Klein, John E Robert, Erin Harper, Robert K Cunningham, Dionisio De Niz, John T Foreman, John B Goodenough, James D Herbsleb, Ipek Ozkaya, Douglas Schmidt, Forrest Shull, Software Engineering Institute. 2021Carnegie Mellon UniversityTechnical report</p>
<p>Envisioning Science in 2050. James Ahrens, Amber Boehnlein, Rich Carlson, Joshua Elliot, Kjiersten Fagnan, Nicola Ferrier, Ian Foster, Lee Gimpel, John Shalf, Dan Ratner, 10.2172/1871683Advanced Scientific Computing Research. 2022United States Department of Energy</p>
<p>Accelerating materials discovery using artificial intelligence, high performance computing and robotics. npj Computational Materials. O Edward, Jed W Pyzer-Knapp, Pitera, W J Peter, Seiji Staar, Teodoro Takeda, Daniel P Laino, James Sanders, John R Sexton, Alessandro Smith, Curioni, 10.1038/s41524-022-00765-z20228</p>
<p>Autonomous chemical research with large language models. A Daniil, Robert Boiko, Ben Macknight, Gabe Kline, Gomes, 10.1038/s41586-023-06792-0Nature. 6242023</p>
<p>Foundational Research Gaps and Future Directions for Digital Twins. 10.17226/268942024The National Academies PressWashington, DCNational Academy of Engineering and National Academies of Sciences, Engineering, and Medicine.</p>
<p>Development of AI-assisted microscopy frameworks through realistic simulation with pySTED. Anthony Bilodeau, Albert Michaud-Gagnon, Julia Chabbert, Benoit Turcotte, J Orn Heine, Audrey Durand, Flavie Lavoie-Cardinal, 10.1038/s42256-024-00903-wNature Machine Intelligence. 62024</p>
<p>msiFlow: Automated workflows for reproducible and scalable multimodal mass spectrometry imaging and microscopy data analysis. Philippa Spangenberg, Sebastian Bessler, Lars Widera, Jenny Bottek, Mathis Richter, Stephanie Thiebes, Devon Siemes, Sascha D KrauÃŸ, G Lukasz, Migas, Swapna Siva, Prasad Kasarla, Jens Phapale, Dagmar Kleesiek, Lars C FÃ¼hrer, Heike Moeller, Raf Heuer, Matthias Van De Plas, Oliver Gunzer, Jens Soehnlein, Olga Soltwisch, Klaus Shevchuk, Daniel R Dreisewerd, Engel, 10.1038/s41467-024-55306-7Nature Communications. 162025</p>
<p>DOE Exascale Computing Project (ECP). 2024</p>
<p>Exascale applications: skin in the game. Francis Alexander, Ann Almgren, John Bell, Amitava Bhattacharjee, Jacqueline Chen, Phil Colella, David Daniel, Jack Deslippe, Lori Diachin, Erik Draeger, Anshu Dubey, Thom Dunning, Thomas Evans, Ian Foster, Marianne Francois, Tim Germann, Mark Gordon, Salman Habib, Mahantesh Halappanavar, Steven Hamilton, William Hart, ( Zhenyu, ) Henry, Aimee Huang, Daniel Hungerford, Kasen, R C Paul, Tzanio Kent, Douglas B Kolev, Andreas Kothe, Ye Kronfeld, Paul Luo, David Mackenzie, Bronson Mccallen, Sue Messer, Chris Mniszewski, Amedeo Oehmen, Danny Perazzo, David Perez, William J Richards, Rob Rider, Kenneth Rieben, Andrew Roche, Michael Siegel, Carl Sprague, Rick Steefel, Madhava Stevens, Mark Syamlal, John Taylor, Jean-Luc Turner, Artur F Vay, Theresa L Voter, Katherine Windus, Yelick, 10.1098/rsta.2019.0056Phil. Trans. R. Soc. 378201900562166. March 2020</p>
<p>E4S: An HPC-AI Software Ecosystem for Science. 2025</p>
<p>Toward a cohesive AI and simulation software ecosystem for scientific innovation. M A Heroux, S Shende, L C Mcinnes, T Gamblin, J M Willenbring, 10.48550/arXiv.2411.09507RFI response to the Frontiers in AI for Science, Security, and Technology (FASST) Initiative. 2024</p>
<p>How community software ecosystems can unlock the potential of exascale computing. Lois Curfman Mcinnes, Michael A Heroux, Erik W Draeger, Andrew Siegel, Susan Coghlan, Katie Antypas, 10.1038/s43588-021-00033-yNature Computational Science. 12021</p>
<p>Lois Curfman Michael A Heroux, James Mcinnes, Todd Ahrens, Timothy C Gamblin, Xiaoye Sherry Germann, Kathryn Li, Todd Mohror, Sameer Munson, Rajeev Shende, Jeffrey Thakur, James Vetter, Willenbring, 10.1177/10943420241271005ECP libraries and tools: An overview. The International Journal of High Performance Computing Applications. 202438</p>
<p>Transforming science through software: Improving while delivering 100Ã—. Richard Gerber, Steven Gottlieb, Michael A Heroux, Lois Curfman Mcinnes, 10.1109/MCSE.2024.3400462Computing in Science &amp; Engineering. 26012024</p>
<p>The AI Scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 2408.06292ArXiv. 2024Technical Report</p>
<p>The Virtual Lab: AI agents design new SARS-CoV-2 nanobodies with experimental validation. Kyle Swanson, Wesley Wu, L Nash, John E Bulaong, James Pak, Zou, 10.1101/2024.11.11.623004bioRxiv. 2024</p>
<p>Robin: A multi-agent system for automating scientific discovery. Ali Essam, Ghareeb , Benjamin Chang, Ludovico Mitchener, Angela Yiu, Caralyn J Szostkiewicz, Jon M Laurent, T Muhammed, Andrew D Razzak, Michaela M White, Samuel G Hinks, Rodriques, 2025</p>
<p>Extreme Heterogeneity 2018: Productive Computational Science in the Era of Extreme Heterogeneity. J S Vetter, 10.2172/1473756Report for the DOE ASCR Basic Research Needs Workshop on Extreme Heterogeneity. 2018</p>
<p>Suren Byna, Stratos Idreos, Terry Jones, Kathryn Mohror, Rob Ross, Florin Rusu, 10.2172/1845707for the ASCR Workshop on the Management and Storage of Scientific Data. 2021Report</p>
<p>Pavel Lougovski, D Ojas, Joe Parekh, Mark Broz, Joseph C Byrd, Yanne Chapman, Chembo, A Wibe, Eden De Jong, Travis S Figueroa, Jeffrey Humble, Larson, 10.2172/2430035for the ASCR Workshop on Basic Research Needs in Quantum Computing and Networking -2023. 2024Report</p>
<p>Exascale computing in the United States. Douglas Kothe, Stephen Lee, Irene Qualters, 10.1109/MCSE.2018.2875366IEEE Computing in Science and Engineering. 2112019</p>
<p>DOE Scientific Discovery through Advanced Computing (SciDAC). 2025</p>
<p>The early years and evolution of the DOE Computational Science Graduate Fellowship Program. David Brown, James Hack, Robert Voigt, 10.1109/MCSE.2021.3120689Computing in Science &amp; Engineering. 2362021</p>
<p>NNSA Predictive Science Academic Alliance Program. 2025Predictive-Science-Academic-Alliance-Program</p>
<p>NSF Science and Technology Centers. 2025</p>
<p>Sustainable Horizons Institute. Sustainable Research Pathways. 2025</p>
<p>James Ang, Andrew Chien, Simon David Hammond, Adolfy Hoisie, Ian Karlin, Scott Pakin, John Shalf, Jeffrey S Vetter, 10.2172/1822199Reimagining Codesign for Advanced Scientific Computing: Report for the ASCR Workshop on Reimagining Codesign. US DOE Office of Science2022</p>
<p>Co-design in the Exascale Computing Project. Timothy C Germann, 10.1177/10943420211059380The International Journal of High Performance Computing Applications. 3562021</p>
<p>Assessing the Reliability of Complex Models: Mathematical and Statistical Foundations of Verification, Validation, and Uncertainty Quantification. 10.17226/133952012The National Academies PressWashington, DCNational Research Council</p>
<p>. 10.17226/253032019The National Academies PressWashington, DCNational Academies of Sciences, Engineering, and Medicine. Reproducibility and Replicability in Science</p>
<p>Maya Gokhale, Ganesh Gopalakrishnan, Jackson Mayo, Santosh Nagarakatte, Cindy Rubio-GonzÃ¡lez, Stephen F Siegel, Report of the DOE/NSF Workshop on Correctness in Scientific Computing. 2023</p>
<p>Nathan Baker, Frank Alexander, Timo Bremer, Aric Hagberg, Yannis Kevrekidis, Habib Najm, Manish Parashar, Abani Patra, James Sethian, Stefan M Wild, Karen Willcox, 10.2172/1478744Workshop Report on Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence. 2019U.S. Department of Energy, ASCRReport</p>
<p>Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations. Nick Mcgreivy, Ammar Hakim, 10.1038/s42256-024-00897-5Nature Machine Intelligence. 62024</p>
<p>Envisioning better benchmarks for machine learning PDE solvers. Johannes Brandstetter, 10.1038/s42256-024-00962-zNature Machine Intelligence. 72025</p>
<p>Empowering scientific workflows with federated agents. J Gregory Pauloski, Yadu Babuji, Ryan Chard, Mansi Sakarvadia, Kyle Chard, Ian Foster, 2025</p>
<p>The increasing dominance of teams in production of knowledge. Stefan Wuchty, Benjamin F Jones, Brian Uzzi, 10.1126/science.1136099Science. 3165827May 2007</p>
<p>Teams at work. H M Williams, N J Allen, 10.4135/9781849200448The SAGE Handbook of Organizational Behavior: Volume 1 -Micro Approaches. Julian Barling, Cary L Cooper, SAGE Publications LtdJuly 2008</p>
<p>Enhancing the Effectiveness of Team Science. 10.17226/190072015The National Academies PressWashington, DCNational Research Council</p>
<p>. 10.17226/290432025The National Academies PressWashington, DCNational Academies of Sciences, Engineering, and Medicine. The Science and Practice of Team Science</p>
<p>Interdisciplinarity as teamwork: How the science of teams can inform team science. M Stephen, Fiore, 10.1177/1046496408317797Small Group Research. 393June 2008</p>
<p>Situated Learning: Legitimate Peripheral Participation. Jean Lave, Etienne Wenger, 1991Cambridge University Press</p>
<p>Etienne Wenger, Richard Mcdermott, William Snyder, Cultivating Communities of Practice: A Guide to Managing Knowledge. Cambridge University Press2002</p>
<p>Systems correctness practices at Amazon Web Services. Marc Brooker, Ankush Desai, 10.1145/3729175Commun. ACM. 686June 2025</p>
<p>Shock with confidence: Formal proofs of correctness for hyperbolic partial differential equation solvers. Jonathan Gorard, Ammar Hakim, 2025</p>
<p>LlamaFirewall: An open source guardrail system for building secure AI agents. Sahana Chennabasappa, Cyrus Nikolaidis, Daniel Song, David Molnar, Stephanie Ding, Shengye Wan, Spencer Whitman, Lauren Deason, Nicholas Doucette, Abraham Montilla, Alekhya Gampa, Dominik Beto De Paola, James Gabi, Jean-Christophe Crnkovich, Kat Testud, Rashnil He, Wu Chaturvedi, Joshua Zhou, Saxe, 2025</p>
<p>What is programming?. Sebastian Nicolajsen, Claus Brabrand, 10.1145/3713068Commun. ACM. 686June 2025</p>
<p>LLMs are greedy agents: Effects of RL fine-tuning on decision-making abilities. Thomas Schmied, Jordi Org Bornschein, Markus Grau-Moya, Razvan Wulfmeier, Pascanu, 2025</p>
<p>Measuring team trust: A critical and meta-analytical review. Jennifer Feitosa, Rebecca Grossman, William S Kramer, Eduardo Salas, 10.1002/job.2436Journal of Organizational Behavior. 415June 2020</p>
<p>Trust within the workplace: A review of two waves of research and a glimpse of the third. Kurt T Dirks, Bart De, Jong , 10.1146/annurev-orgpsych-012420-083025Annual Review of Organizational Psychology and Organizational Behavior. 91Jan. 2022</p>
<p>How and why humans trust: A meta-analysis and elaborated model. P A Hancock, Theresa T Kessler, Alexandra D Kaplan, Kimberly Stowers, J Christopher Brill, Deborah R Billings, Kristin E Schaefer, James L Szalma, 10.3389/fpsyg.2023.1081086Frontiers in Psychology. 14March 2023</p>
<p>Better together: Elements of successful scientific software development in a distributed collaborative community. Julia Koehler Leman, Brian D Weitzner, P Douglas Renfrew, Steven M Lewis, Rocco Moretti, Andrew M Watkins, 10.1371/journal.pcbi.1007507PLOS Computational Biology. 165May 2020</p>
<p>A cast of thousands: How the IDEAS productivity project has advanced software productivity and sustainability. Lois Curfman Mcinnes, Michael A Heroux, David E Bernholdt, Anshu Dubey, Elsa Gonsiorowski, Rinku Gupta, Osni Marques, J David Moulton, Hai Ah Nam, Boyana Norris, Elaine M Raybourn, Jim Willenbring, Ann Almgren, Roscoe A Bartlett, Kita Cranfill, Stephen Fickas, Don Frederick, William F Godoy, Patricia A Grubel, Rebecca Hartman-Baker, Axel Huebl, Rose Lynch, Addi Malviya-Thakur, Reed Milewicz, Mark C Miller, Miranda R Mundt, Erik Palmer, Suzanne Parete-Koon, Megan Phinney, Katherine Riley, David M Rogers, Benjamin Sims, Deborah Stevens, Gregory R Watson, 10.1109/MCSE.2024.3383799IEEE Computing in Science &amp; Engineering. 2612024</p>
<p>Incentives and integration in scientific software production. James Howison, James D Herbsleb, 10.1145/2441776.2441828Proceedings of the 2013 Conference on Computer Supported Cooperative Work. the 2013 Conference on Computer Supported Cooperative WorkSan Antonio Texas USAACM2013</p>
<p>Talk to me: A case study on coordinating expertise in large-scale scientific software projects. Reed Milewicz, Elaine Raybourn, 10.1109/eScience.2018.000102018 IEEE 14th International Conference on e-Science (e-Science). Oct. 2018</p>
<p>Ann Mary, Leung, Sustainable Horizons Institute. 2025</p>
<p>Community organizations: Changing the culture in which research software is developed and sustained. S Daniel, Katz, 10.1109/MCSE.2018.2883051IEEE CiSE. 212019</p>            </div>
        </div>

    </div>
</body>
</html>
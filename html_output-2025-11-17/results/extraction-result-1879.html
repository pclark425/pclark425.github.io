<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1879 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1879</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1879</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-37.html">extraction-schema-37</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-276929065</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.08569v1.pdf" target="_blank">DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are increasingly utilized in scientific research assessment, particularly in automated paper review. However, existing LLM-based review systems face significant challenges, including limited domain expertise, hallucinated reasoning, and a lack of structured evaluation. To address these limitations, we introduce DeepReview, a multi-stage framework designed to emulate expert reviewers by incorporating structured analysis, literature retrieval, and evidence-based argumentation. Using DeepReview-13K, a curated dataset with structured annotations, we train DeepReviewer-14B, which outperforms CycleReviewer-70B with fewer tokens. In its best mode, DeepReviewer-14B achieves win rates of 88.21\% and 80.20\% against GPT-o1 and DeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper review, with all resources publicly available. The code, model, dataset and demo have be released in http://ai-researcher.net.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1879.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1879.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepReviewer novelty verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepReview novelty verification stage (z1) in DeepReviewer-14B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's automated novelty-assessment component that generates research questions, retrieves ~60 candidate papers via Semantic Scholar, re-ranks them and produces a top-10 literature comparison to verify originality as part of a multi-stage LLM review pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>automated LLM-based peer review (novelty verification sub-component)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>literature-review-driven relevance ranking: three generated research questions -> ~60 retrieved papers -> ReRank selects top 10; novelty judged by comparison reports (qualitative evidence-based novelty verification)</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>not assessed (no quantitative model reported relating novelty level to evaluation outcome)</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>computer science / machine learning (ICLR 2024-2025 papers used)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>not evaluated quantitatively; authors note domain focus on ML and that cross-domain generalizability is untested</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>review scores (overall rating ∈ [1,10]), fine-grained scores (Soundness, Presentation, Contribution ∈ [1,4]), rating MSE/MAE, Spearman ranking, pairwise selection accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>average human reviewer rating (paper uses the average of other n−1 reviewers as estimator of 'true' score)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>not reported specifically for novelty; paper reports general reviewer-model gaps (e.g., CycleReviewer MAE improvements) but no breakdown by novelty level</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>not directly compared quantitatively; novelty verification exists as a stage but no reported metrics comparing outcomes for incremental vs transformational work</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>not examined (no simultaneous analysis of multiple proxy failures specific to novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>DeepReviewer uses z1 novelty verification as part of Best mode; authors report overall improved rating/ranking/selection metrics for DeepReviewer vs baselines, but do not provide performance split by novelty vs incremental papers</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>not quantified for novelty; authors acknowledge synthetic dataset limitations and potential mismatch with human review nuance</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>no explicit intervention tested to correct novelty bias (novelty verification is proposed as a design element but not evaluated as a debiasing intervention with quantitative effect sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>authors note domain-specific availability of open-access data limits cross-field training (i.e., ML is overrepresented), which may moderate performance on novelty detection in other fields</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>DeepReview-13K: 13,378 valid samples collected from OpenReview and arXiv (ICLR 2024-2025); DeepReview-Bench: ~1.2K held-out samples (10% of dataset) used for evaluation; novelty verification implemented via LLM pipelines and external retrieval (Semantic Scholar/OpenScholar)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1879.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1879.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ethical concern: bias vs novel work</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Authors' ethical discussion of bias against novel or unconventional methodologies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors explicitly warn that LLM-based automated review systems (including DeepReviewer) could perpetuate or amplify biases present in training data and may systematically disadvantage novel, unconventional, or underrepresented-topic research, but provide no quantitative estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>peer review / automated LLM review (ethical assessment of system-level bias)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>qualitative notion of 'novel or unconventional methodologies' (no operational novelty index provided)</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>qualitative potential negative bias (authors state systems 'could lead to systematic disadvantages' for novel/unconventional work)</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>general (authors discuss risk across scientific fields, but development/evaluation focused on ML)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>not quantified; authors call for cross-domain bias auditing and benchmarking but report no per-field bias metrics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>not specified in ethical discussion (general concern about automated scores/decisions reflecting training-data biases)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>stated risk (transformational/novel work may be disadvantaged) but no quantitative comparison provided</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>authors claim DeepReviewer shows robustness to adversarial attacks and improved overall metrics vs baselines, but do not quantify performance specifically on novel vs incremental submissions</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>authors acknowledge synthetic training data (DeepReview-13K) may not capture full human review complexity and could contain biases; commit to ongoing bias auditing</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>none reported; authors recommend human-in-the-loop, transparency, and release with usage policies as mitigation strategies</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>author reputation/oversight implied as moderating (they recommend human reviewers remain final authority), but no quantitative moderation estimates</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>ethical statements are normative; no empirical sample specific to bias-against-novelty is presented</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1879.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1879.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward-model overoptimization risk</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward-model overoptimization (reward hacking) and its effect on generated research being 'safe/incremental'</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reviewers and the authors discuss that reinforcement-learning-based optimization against an automated reward/reviewer model can cause CycleResearcher to exploit reward model weaknesses, producing safe or incremental outputs and inflating automated scores relative to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>automated evaluation (reward models used as surrogate reviewers in RL training)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>implicit (concern that RL optimization favors reward-model-pleasing outputs over genuine novelty); no quantitative novelty index provided</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>qualitative directional effect: reward-model optimization → preference for safe/incremental outputs (no functional form reported)</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>machine learning (experiments and critiques focused on ML paper generation/evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>not tested quantitatively</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>reward-model score (used during RL), automated accept-rate; compared to human scores</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>human expert ratings (used for external validation/human evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>reported discrepancies: reviewers note human expert average rating ~4.8 vs an automated reviewer score reported ~5.36 for CycleResearcher outputs (difference ≈ 0.56 points); authors performed additional independent-reward-model evals showing accept-rate shifts (e.g., 31.07% → 28.65% under a held-out reward model), illustrating instability across reward proxies</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>concern raised that RL may push models toward incremental/safe outputs; no direct quantitative comparison between incremental and transformational outcomes reported</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>partially addressed: authors acknowledge that optimizing to a single reward proxy can distort outcomes; they trained an independent reward model on held-out data as a partial test (resulting in modest accept-rate change), but no comprehensive multi-proxy failure analysis reported</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>CycleResearcher achieved automated acceptance-rate ≈ 31.07% under original reward model; using an independent held-out reward model reduced accept rate to ≈ 28.65% (authors' rebuttal experiments). Human evaluations sometimes rate generated papers lower than automated reward judgments (see proxy_truth_gap above).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>training and reward-model coupling acknowledged as a source of bias/overoptimization; no quantitative measure of how training-data composition changes this effect</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>authors ran a held-out independent reward-model evaluation (Mistral-Large-2) and compared accept rates; not presented as a full mitigation but as an analysis step</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>evaluation differences vary by the reward model used at test-time; human expert evaluation reduces (but does not eliminate) reward-model overoptimization effects</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Automated accept-rate claims based on CycleResearcher experiments and synthetic datasets; human evaluation reported inconsistently across comments (some reviewers noted small-scale human eval: e.g., '10 papers, 3 human reviewers' in a reviewer comment; authors later report human expert evaluations and independent reward-model tests but detailed n for those experiments varies across text and rebuttal)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1879.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1879.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generalizability / field differences note</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Authors' and reviewers' statements about field-specific performance and lack of cross-field validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper is trained and evaluated primarily on ML conference (ICLR) data; both authors and reviewers stress the system's domain focus and that generalizability to other scientific fields (biology, social sciences) is unexplored and likely limited by available open-access data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>peer review / automated LLM review applied in domain-specific context</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not provided across fields; system's novelty verification is tailored to ML literature retrieval pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>no quantified relationship; cautionary statement that domain mismatch may degrade novelty detection and review performance</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>computer science / machine learning (primary); other fields explicitly untested</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>qualitative only: authors note ML has abundant open-access data enabling training, while other fields have limited open-access data which may reduce model performance; no numerical cross-field comparisons provided</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>same reviewer metrics as above (rating MSE/MAE, Spearman, pairwise accuracy) but only reported for ICLR 2024/2025 (ML conferences)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>ICLR human reviewer scores (used as ground-truth proxy within ML domain)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>not reported per field; across ML testbeds DeepReviewer reports improvements over baselines, but no cross-field gap estimates exist</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>not evaluated across fields; reviewers request experiments in diverse fields to measure performance</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>not assessed across fields</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>within ML (ICLR) datasets DeepReviewer shows improved metrics (examples: Rating Spearman 0.3559 on ICLR2024, improvements in Rating MSE reductions relative to baselines); no data for other domains</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>authors acknowledge that reliance on ML conference data could bias system behavior and limit detection/valuation of novelty in other domains</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>none for cross-field generalization; authors suggest collaborations with publishers and domain-specific data collection as future steps</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>availability of open-access training data per field moderates potential performance; domain-specific evaluation criteria likely needed</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Empirical results and benchmarks were run on ICLR 2024/2025 datasets (DeepReview-13K, DeepReview-Bench ~1.2K); no cross-field datasets or experiments reported</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Are we there yet?revealing the risks of utilizing large language models in scholarly peer review <em>(Rating: 2)</em></li>
                <li>Ai-driven review systems: evaluating llms in scalable and bias-aware academic reviews <em>(Rating: 2)</em></li>
                <li>The ai review lottery: Widespread ai-assisted peer reviews boost paper scores and acceptance rates <em>(Rating: 2)</em></li>
                <li>Automated peer reviewing in paper sea: Standardization, evaluation, and analysis <em>(Rating: 2)</em></li>
                <li>Can we automate scientific reviewing? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1879",
    "paper_id": "paper-276929065",
    "extraction_schema_id": "extraction-schema-37",
    "extracted_data": [
        {
            "name_short": "DeepReviewer novelty verification",
            "name_full": "DeepReview novelty verification stage (z1) in DeepReviewer-14B",
            "brief_description": "The paper's automated novelty-assessment component that generates research questions, retrieves ~60 candidate papers via Semantic Scholar, re-ranks them and produces a top-10 literature comparison to verify originality as part of a multi-stage LLM review pipeline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "automated LLM-based peer review (novelty verification sub-component)",
            "novelty_measure": "literature-review-driven relevance ranking: three generated research questions -&gt; ~60 retrieved papers -&gt; ReRank selects top 10; novelty judged by comparison reports (qualitative evidence-based novelty verification)",
            "bias_magnitude": null,
            "relationship_type": "not assessed (no quantitative model reported relating novelty level to evaluation outcome)",
            "temporal_pattern": null,
            "field_studied": "computer science / machine learning (ICLR 2024-2025 papers used)",
            "field_differences": "not evaluated quantitatively; authors note domain focus on ML and that cross-domain generalizability is untested",
            "proxy_metric_studied": "review scores (overall rating ∈ [1,10]), fine-grained scores (Soundness, Presentation, Contribution ∈ [1,4]), rating MSE/MAE, Spearman ranking, pairwise selection accuracy",
            "ground_truth_measure": "average human reviewer rating (paper uses the average of other n−1 reviewers as estimator of 'true' score)",
            "proxy_truth_gap": "not reported specifically for novelty; paper reports general reviewer-model gaps (e.g., CycleReviewer MAE improvements) but no breakdown by novelty level",
            "incremental_vs_transformational": "not directly compared quantitatively; novelty verification exists as a stage but no reported metrics comparing outcomes for incremental vs transformational work",
            "multiple_proxy_failures": "not examined (no simultaneous analysis of multiple proxy failures specific to novelty)",
            "automated_system_performance": "DeepReviewer uses z1 novelty verification as part of Best mode; authors report overall improved rating/ranking/selection metrics for DeepReviewer vs baselines, but do not provide performance split by novelty vs incremental papers",
            "training_data_bias": "not quantified for novelty; authors acknowledge synthetic dataset limitations and potential mismatch with human review nuance",
            "intervention_tested": "no explicit intervention tested to correct novelty bias (novelty verification is proposed as a design element but not evaluated as a debiasing intervention with quantitative effect sizes)",
            "counter_examples": null,
            "moderating_factors": "authors note domain-specific availability of open-access data limits cross-field training (i.e., ML is overrepresented), which may moderate performance on novelty detection in other fields",
            "sample_size_and_methods": "DeepReview-13K: 13,378 valid samples collected from OpenReview and arXiv (ICLR 2024-2025); DeepReview-Bench: ~1.2K held-out samples (10% of dataset) used for evaluation; novelty verification implemented via LLM pipelines and external retrieval (Semantic Scholar/OpenScholar)",
            "uuid": "e1879.0"
        },
        {
            "name_short": "Ethical concern: bias vs novel work",
            "name_full": "Authors' ethical discussion of bias against novel or unconventional methodologies",
            "brief_description": "Authors explicitly warn that LLM-based automated review systems (including DeepReviewer) could perpetuate or amplify biases present in training data and may systematically disadvantage novel, unconventional, or underrepresented-topic research, but provide no quantitative estimate.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_system_type": "peer review / automated LLM review (ethical assessment of system-level bias)",
            "novelty_measure": "qualitative notion of 'novel or unconventional methodologies' (no operational novelty index provided)",
            "bias_magnitude": null,
            "relationship_type": "qualitative potential negative bias (authors state systems 'could lead to systematic disadvantages' for novel/unconventional work)",
            "temporal_pattern": null,
            "field_studied": "general (authors discuss risk across scientific fields, but development/evaluation focused on ML)",
            "field_differences": "not quantified; authors call for cross-domain bias auditing and benchmarking but report no per-field bias metrics",
            "proxy_metric_studied": "not specified in ethical discussion (general concern about automated scores/decisions reflecting training-data biases)",
            "ground_truth_measure": null,
            "proxy_truth_gap": null,
            "incremental_vs_transformational": "stated risk (transformational/novel work may be disadvantaged) but no quantitative comparison provided",
            "multiple_proxy_failures": null,
            "automated_system_performance": "authors claim DeepReviewer shows robustness to adversarial attacks and improved overall metrics vs baselines, but do not quantify performance specifically on novel vs incremental submissions",
            "training_data_bias": "authors acknowledge synthetic training data (DeepReview-13K) may not capture full human review complexity and could contain biases; commit to ongoing bias auditing",
            "intervention_tested": "none reported; authors recommend human-in-the-loop, transparency, and release with usage policies as mitigation strategies",
            "counter_examples": null,
            "moderating_factors": "author reputation/oversight implied as moderating (they recommend human reviewers remain final authority), but no quantitative moderation estimates",
            "sample_size_and_methods": "ethical statements are normative; no empirical sample specific to bias-against-novelty is presented",
            "uuid": "e1879.1"
        },
        {
            "name_short": "Reward-model overoptimization risk",
            "name_full": "Reward-model overoptimization (reward hacking) and its effect on generated research being 'safe/incremental'",
            "brief_description": "Reviewers and the authors discuss that reinforcement-learning-based optimization against an automated reward/reviewer model can cause CycleResearcher to exploit reward model weaknesses, producing safe or incremental outputs and inflating automated scores relative to human judgments.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_system_type": "automated evaluation (reward models used as surrogate reviewers in RL training)",
            "novelty_measure": "implicit (concern that RL optimization favors reward-model-pleasing outputs over genuine novelty); no quantitative novelty index provided",
            "bias_magnitude": null,
            "relationship_type": "qualitative directional effect: reward-model optimization → preference for safe/incremental outputs (no functional form reported)",
            "temporal_pattern": null,
            "field_studied": "machine learning (experiments and critiques focused on ML paper generation/evaluation)",
            "field_differences": "not tested quantitatively",
            "proxy_metric_studied": "reward-model score (used during RL), automated accept-rate; compared to human scores",
            "ground_truth_measure": "human expert ratings (used for external validation/human evaluation)",
            "proxy_truth_gap": "reported discrepancies: reviewers note human expert average rating ~4.8 vs an automated reviewer score reported ~5.36 for CycleResearcher outputs (difference ≈ 0.56 points); authors performed additional independent-reward-model evals showing accept-rate shifts (e.g., 31.07% → 28.65% under a held-out reward model), illustrating instability across reward proxies",
            "incremental_vs_transformational": "concern raised that RL may push models toward incremental/safe outputs; no direct quantitative comparison between incremental and transformational outcomes reported",
            "multiple_proxy_failures": "partially addressed: authors acknowledge that optimizing to a single reward proxy can distort outcomes; they trained an independent reward model on held-out data as a partial test (resulting in modest accept-rate change), but no comprehensive multi-proxy failure analysis reported",
            "automated_system_performance": "CycleResearcher achieved automated acceptance-rate ≈ 31.07% under original reward model; using an independent held-out reward model reduced accept rate to ≈ 28.65% (authors' rebuttal experiments). Human evaluations sometimes rate generated papers lower than automated reward judgments (see proxy_truth_gap above).",
            "training_data_bias": "training and reward-model coupling acknowledged as a source of bias/overoptimization; no quantitative measure of how training-data composition changes this effect",
            "intervention_tested": "authors ran a held-out independent reward-model evaluation (Mistral-Large-2) and compared accept rates; not presented as a full mitigation but as an analysis step",
            "counter_examples": null,
            "moderating_factors": "evaluation differences vary by the reward model used at test-time; human expert evaluation reduces (but does not eliminate) reward-model overoptimization effects",
            "sample_size_and_methods": "Automated accept-rate claims based on CycleResearcher experiments and synthetic datasets; human evaluation reported inconsistently across comments (some reviewers noted small-scale human eval: e.g., '10 papers, 3 human reviewers' in a reviewer comment; authors later report human expert evaluations and independent reward-model tests but detailed n for those experiments varies across text and rebuttal)",
            "uuid": "e1879.2"
        },
        {
            "name_short": "Generalizability / field differences note",
            "name_full": "Authors' and reviewers' statements about field-specific performance and lack of cross-field validation",
            "brief_description": "The paper is trained and evaluated primarily on ML conference (ICLR) data; both authors and reviewers stress the system's domain focus and that generalizability to other scientific fields (biology, social sciences) is unexplored and likely limited by available open-access data.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_system_type": "peer review / automated LLM review applied in domain-specific context",
            "novelty_measure": "not provided across fields; system's novelty verification is tailored to ML literature retrieval pipelines",
            "bias_magnitude": null,
            "relationship_type": "no quantified relationship; cautionary statement that domain mismatch may degrade novelty detection and review performance",
            "temporal_pattern": null,
            "field_studied": "computer science / machine learning (primary); other fields explicitly untested",
            "field_differences": "qualitative only: authors note ML has abundant open-access data enabling training, while other fields have limited open-access data which may reduce model performance; no numerical cross-field comparisons provided",
            "proxy_metric_studied": "same reviewer metrics as above (rating MSE/MAE, Spearman, pairwise accuracy) but only reported for ICLR 2024/2025 (ML conferences)",
            "ground_truth_measure": "ICLR human reviewer scores (used as ground-truth proxy within ML domain)",
            "proxy_truth_gap": "not reported per field; across ML testbeds DeepReviewer reports improvements over baselines, but no cross-field gap estimates exist",
            "incremental_vs_transformational": "not evaluated across fields; reviewers request experiments in diverse fields to measure performance",
            "multiple_proxy_failures": "not assessed across fields",
            "automated_system_performance": "within ML (ICLR) datasets DeepReviewer shows improved metrics (examples: Rating Spearman 0.3559 on ICLR2024, improvements in Rating MSE reductions relative to baselines); no data for other domains",
            "training_data_bias": "authors acknowledge that reliance on ML conference data could bias system behavior and limit detection/valuation of novelty in other domains",
            "intervention_tested": "none for cross-field generalization; authors suggest collaborations with publishers and domain-specific data collection as future steps",
            "counter_examples": null,
            "moderating_factors": "availability of open-access training data per field moderates potential performance; domain-specific evaluation criteria likely needed",
            "sample_size_and_methods": "Empirical results and benchmarks were run on ICLR 2024/2025 datasets (DeepReview-13K, DeepReview-Bench ~1.2K); no cross-field datasets or experiments reported",
            "uuid": "e1879.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Are we there yet?revealing the risks of utilizing large language models in scholarly peer review",
            "rating": 2
        },
        {
            "paper_title": "Ai-driven review systems: evaluating llms in scalable and bias-aware academic reviews",
            "rating": 2
        },
        {
            "paper_title": "The ai review lottery: Widespread ai-assisted peer reviews boost paper scores and acceptance rates",
            "rating": 2
        },
        {
            "paper_title": "Automated peer reviewing in paper sea: Standardization, evaluation, and analysis",
            "rating": 2
        },
        {
            "paper_title": "Can we automate scientific reviewing?",
            "rating": 1
        }
    ],
    "cost": 0.01742325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process
11 Mar 2025</p>
<p>Minjun Zhu zhuminjun@westlake.edu.cn 
Yixuan Weng wengsyx@gmail.com 
Linyi Yang yanglinyiucd@gmail.com 
Yue Zhang zhangyue@westlake.edu.cn </p>
<p>Zhejiang University School of Engineering
Westlake University</p>
<p>School of Engineering
Westlake University Research Center for Industries of the Future</p>
<p>University College London</p>
<p>School of Engineering
Westlake University Research Center for Industries of the Future</p>
<p>Corresponding Author. Supported by Research Center for Industries of the Future
Westlake University</p>
<p>DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process
11 Mar 2025D72461AC53C6303A50A4A2BA2CB94E58arXiv:2503.08569v1[cs.CL]DeepReviewer-7B DeepReviewer-14B DeepReview-13K Code Repository: zhu-minjun/Researcher ai-researchernet/deepreviewer -Dataset Analysis: * Dataset characteristics * Data preprocessing and splits * Control groups * Sample size justification -Implementation Details: * Hyperparameter choices * Hardware specifications * Code reproducibility -Results Validation: * Metric selection justification * Statistical significance * Error analysis * Missing baselines * Generalization assessment -Design Gaps: * Missing control experiments * Incomplete ablations * Insufficient robustness tests Each experimental weakness requires: 1Specific experimental details from paper 2Numerical results evidence 3Missing specifications 4Impact on conclusions
Large Language Models (LLMs) are increasingly utilized in scientific research assessment, particularly in automated paper review.However, existing LLMbased review systems face significant challenges, including limited domain expertise, hallucinated reasoning, and a lack of structured evaluation.To address these limitations, we introduce DeepReview, a multi-stage framework designed to emulate expert reviewers by incorporating structured analysis, literature retrieval, and evidence-based argumentation.Using DeepReview-13K, a curated dataset with structured annotations, we train DeepReviewer-14B, which outperforms CycleReviewer-70B with fewer tokens.In its best mode, DeepReviewer-14B achieves win rates of 88.21% and 80.20% against GPT-o1 and DeepSeek-R1 in evaluations.Our work sets a new benchmark for LLM-based paper review, with all resources publicly available.The code, model, dataset and demo have be released in http://ai-researcher.net.</p>
<p>Introduction</p>
<p>Peer review is the foundation of scientific progress, ensuring that research is novel, reliable, and rigorously evaluated by experts before publication [Alberts et al., 2008].With the increasing volume of research submissions, Large Language Models (LLMs) have become promising tools to support reviewers [Yang et al., 2024, Chris et al., 2024, Li et al., 2024b, Scherbakov et al., 2024, Si et al., 2025].For example, the ICLR 2025 conference has introduced an LLM-based system to assist reviewers in providing feedback Blog [2024].</p>
<p>Recent research has explored two primary approaches to improve LLM-based review systems: (1) employing LLM-powered agents to simulate the peer review process, as exemplified by AI-Scientist [Chris et al., 2024] and AgentReview [Jin et al., 2024a]; and ( 2) developing open-source models trained on extensive datasets from existing peer review platforms, such as ReviewMT [Tan et al., 2024b] and CycleReviewer [Weng et al., 2025].</p>
<p>Despite these advancements, current systems exhibit several critical limitations: they struggle to comprehensively identify submission flaws, resulting in superficial feedback [Zhou et al., 2024a]; lack evidence-based justifications [Zhuang et al., 2025]; and fail to provide clear, actionable suggestions [Ye et al., 2024, Du et al., 2024].Moreover, their vulnerability to prompt engineering leads to inaccurate evaluations [Ye et al., 2024].While robust feedback is crucial for scientific advancement and peer review integrity, developing reliable evaluation frameworks faces two significant challenges:</p>
<p>(1) The scarcity of structured paper review datasets that capture fine-grained expert evaluation processes.Most available open review datasets primarily contain aggregated reviews and decisions, limiting LLMs' ability to learn systematic review reasoning chains and increasing their susceptibility to shortcut learning and adversarial manipulation.(2) LLMs' inherent constraints, including restricted domain knowledge, lack of dynamic knowledge updating mechanisms, and a tendency to generate hallucinated content without adequate verification [Schintler et al., 2023, Drori andTe'eni, 2024], which significantly impair their capability to assess complex scientific content [Wang et al., 2020, Yuan et al., 2021].</p>
<p>To address these challenges, we introduce DeepReview, a structured multi-stage review framework that closely aligns with the expert review process by incorporating novelty assessment, multidimensional evaluation criteria, and reliability verification.We develop a comprehensive data synthesis pipeline that integrates retrieval and ranking [Asai et al., 2024], self-verification [Weng et al., 2023], and self-reflection [Ji et al., 2023], ensuring the soundness and robustness of LLM-generated suggestions.This approach enables deeper insights into the reasoning and decision-making of paper review.The resulting dataset, DeepReview-13K, consists of raw research papers, structured intermediate review steps, and final assessments.Based on that, we train DeepReviewer-14B, a model that offers three inference modes -Fast, Standard, and Best -allowing users to balance efficiency and response quality.We further construct DeepReview-Bench, a comprehensive benchmark containing 1.2K samples, which evaluates both quantitative aspects (rating prediction, quality ranking, and paper selection) and qualitative review generation through LLM-based assessment.</p>
<p>Extensive experiments demonstrate DeepReviewer 14B's superior performance across multiple dimensions.Compared to existing systems like CycleReviewer-70B, GPT-o1, and Deepseek-R1, our model achieves substantial improvements in Score (Rating MSE: 44.80% ↑), Ranking (Rating Spearman: 6.04% ↑), and Selection (Accuracy 1.80% ↑).In LLM-as-a-judge evaluation [Wang et al., 2024b, Rewina et al., 2025], it achieves a 80% win rate against GPT-o1 and Deepseek-R1.Notably, DeepReviewer exhibits strong resilience to adversarial attacks despite no explicit robustness training.Furthermore, our Test-Time Scaling analysis reveals that DeepReviewer can enhance its performance by adjusting reasoning paths and response lengths.</p>
<p>Our work establishes a foundation for robust LLM-based review systems through DeepReview, a structured framework that addresses fundamental challenges in automated manuscript evaluation.We introduce DeepReview-13K, a dataset featuring fine-grained review reasoning chains, alongside DeepReview-Bench, a benchmark for automated paper review.Built upon these resources, our DeepReviewer-14B model demonstrates substantial improvements over existing approaches while maintaining strong resilience to adversarial attacks, validating the effectiveness of our structured approach to automated scientific evaluation.Our code, model, and data will be publicly available under the agreement of our usage policy.</p>
<p>Data Collection</p>
<p>We present DeepReview-13K, a training dataset that captures the intermediate reasoning processes inherent in academic paper reviews, addressing the fundamental challenges in Paper Review tasks from three dimensions: the scarcity of high-quality, structured review datasets and standardized evaluation frameworks.The statistics of this dataset are detailed in Table 1.We initially collected raw data from the OpenReview platform arXiv repository, gathering 18,976 paper submissions spanning two ICLR conference cycles (2024-2025)2 .Using the MinerU tool [Wang et al., 2024a], we convert papers to parseable Markdown format, prioritizing L A T E X source code when available from arXiv.For each paper, we assembled a review set R comprising three key components: (1) textual assessments (Strengths, Weaknesses, and Questions), (2) interactive discussions from the rebuttal phase, and (3) standardized scores, including overall ratings (∈ [1, 10]) and fine-grained evaluations of Soundness,Presentation,and Contribution (∈ [1,4]).Additionally, we collect meta-review texts and final ratings with acceptance decisions.The final DeepReview-13K dataset comprises 13,378 valid samples in Table 1 as the foundation for constructing our review reasoning chain.</p>
<p>DeepReview-13K</p>
<p>DeepReview-Test</p>
<p>To evaluate performance, we randomly sampled 10% (1.2K) of the dataset to create DeepReview-Bench.Our evaluation framework assesses both quantitative scores and qualitative aspects of review generation through the following tasks:</p>
<p>Quantitative Evaluation: 1) Rating prediction: using MAE, MSE, accuracy, and F1 metrics 2) Paper quality ranking: measured by Spearman correlation 3) Pairwise paper selection (n=2): assessed through accuracy Qualitative Evaluation: While previous work [Tan et al., 2024b] relied on simple text similarity metrics (e.g., ROUGE [Lin, 2004], BLEU [Papineni et al., 2002]), these metrics fail to capture specific review capabilities.Motivated by recent findings [Li et al., 2024a], we adopt the LLM-as-a-judge paradigm using Gemini-2.0-Flash-Thinkingto conduct pairwise comparative evaluations of generated reviews.Detailed evaluation metrics are provided in Appendix B.</p>
<p>Methodology</p>
<p>Drawing inspiration from recent advances in complex reasoning methods [Xiang et al., 2025, Hao et al., 2024], we propose a deep-thinking evaluation framework that decomposes the review process into three key steps in Figure 1: (1) novelty verification z 1 : assessing research originality through literature review; (2) multi-dimension evaluation z 2 : synthesizing insights from multiple expert perspectives; and (3) reliability verification z 3 : examining internal consistency and logical coherence.</p>
<p>Task Definition</p>
<p>Formally, given an input paper q, our goal is to generate a review pair (s, a), where s represents the qualitative assessment text (meta-review), we express the reasoning process as:
q → z 1 → z 2 → z 3 → (s, a)
We formulate the review score generation as a marginalization over sequential reasoning chains: p(a|q) ∝ ∫ p(a|z 1∶3 , q) 3 ∏ t=1 p(z t |z &lt;t , q)dZ (1)</p>
<p>Here, the chain-of-thought term ∏ 3 t=1 p(z t |z &lt;t , q) explicitly models the sequential dependencies between reasoning steps, Z represents all possible intermediate state sequences (s 1 , . . ., s n ).This structured approach aims to enhance the reliability of the evaluation process.</p>
<p>Structured Reasoning Process</p>
<p>We present a comprehensive automated data construction pipeline, which is specifically designed to generate high-quality supervised fine-tuning datasets that capture complete reasoning paths, shown as (z 1 , z 2 , z 3 ).</p>
<p>Stage 1: Novelty Verification (z 1 ).Our novelty verification framework consists of three key components: question generation, paper analysis, and literature review.Initially, based on the paper, we use the Qwen-2.5-72B-Instructmodel [Qwen et al., 2025] to generate three key research questions, focusing on research gaps, innovative directions, and methodological breakthroughs to capture domain-specific characteristics.Additionally, to ensure thorough understanding, we employ the Gemini-2.0-Flash-thinkingmodel to conduct systematic paper analysis with a specifically designed system prompt (Figure 6) across research motivation, core ideas, technical approaches, and experimental design.Then, literature retrieval, comparison, and summary are built on OpenScholar Asai et al. [2024] to address these research questions.Using Qwen-2.5-3B-Instruct with few-shot learning, we transform questions into search keywords to retrieve approximately 60 relevant papers via Semantic Scholar API.Subsequently, the ReRank model3 reorder retrieved papers and select the top 10 most relevant papers, and its internal QA model4 generates comprehensive reports as novelty analysis z 1 , incorporating works cited in review R.</p>
<p>Stage 2: Multi-dimension Review (z 2 ).To provide constructive review, we transform author rebuttals into instructive suggestions while synthesizing multiple review R into comprehensive perspectives.Specifically, using Qwen-2.5-72B-Instruct,we develop a review reconstruction pipeline that analyzes each review in R with its corresponding author response, capturing experimental results, theoretical proofs, and implementation details from rebuttals to transform criticisms into concrete technical suggestions.The reconstruction process (z 2 ) follow three principles: (1) maintaining technical depth; (2) ensuring actionable feedback; (3) preserving professional tone and original citations.</p>
<p>Stage 3: Reliability Verification(z 3 ).In order to ensure assessment accuracy through systematic evidence analysis, we employ Gemini-2-Flash-thinking to conduct systematic evidence analysis through a four-stage verification chain: methodology verification, experimental verification, and comprehensive analysis.Each review comment requires supporting evidence from the paper and receives an assigned confidence level.Finally, we utilize Qwen to generate a new Meta-Review by integrating the original Meta-Review, reviewer comments, and verification outcomes.This step identifies key weaknesses while providing evidence-based analysis and constructive suggestions.</p>
<p>Quality Control Mechanism.To ensure the high quality of our synthetic DeepReview-13K dataset, we implemented a rigorous automated quality control process using Qwen-2.5-72B-Instruct.This process involves a multi-faceted approach to assess each generated sample for logical integrity and completeness.Specifically, Qwen-2.5-72B-Instruct was tasked with examining each sample for: (1) Logical Consistency: verifying that the reasoning chain (z 1 , z 2 , z 3 ) and the final evaluation (s, a)  are logically coherent and non-contradictory;</p>
<p>(2) Completeness: checking for any missing or empty fields within the structured data format, ensuring all components of the reasoning path and evaluation are present.Samples failing any of these checks, indicating logical inconsistencies, incompleteness, or failing to meet our quality standards, were automatically flagged and removed from the dataset.</p>
<p>Model Training</p>
<p>We train our model based on Phi-4 14B [Abdin et al., 2024] using the DeepReview-13K dataset.The training process was conducted on 8x H100 80G GPUs with DeepSpeed + ZeRO3 [Rajbhandari et al., 2020, Rasley et al., 2020] for optimization.Notably, we extended the context window to 256K using LongRoPE [Ding et al., 2024], with a 40K context window during training for full-parameter fine-tuning.Given memory constraints, samples exceeding the preset context length are randomly truncated.The model is trained for 23,500 steps with a batch size of 16 and a learning rate of 5e-6.</p>
<p>Inference Strategy.We divided each sample in the DeepReview-13K data into three modes using reasoning path cropping, as shown in Figure 1(c), which allows for efficiency adjustments at test time based on varying requirements.The Fast mode directly generates final evaluation results and comprehensive analysis reports (s, a), minimizing computational cost by bypassing intermediate reasoning steps.The Standard mode executes core evaluation steps including z 2 and z 3 , maintaining high efficiency while ensuring evaluation quality, making it appropriate for routine research assessment.The Best mode implements the complete reasoning chain (z 1 , z 2 , z 3 ), encompassing novelty verification, multi-dimension assessment, reliability verification, and comprehensive analysis generation.For novelty verification during inference, as in Stage 1 (Section 4.2), we employ Semantic Scholar API and OpenScholar to ensure accurate assessment of research novelty and citation correctness through comprehensive literature review and analysis.All three modes share the same model architecture, differing only in their executed evaluation steps.This allows the trained DeepReview-14B model to execute different reasoning paths at inference time, controlled by input instructions.</p>
<p>Experiments</p>
<p>Experimental setting</p>
<p>Baselines.We consider two types of baselines: (1) Prompt-based baselines including AI Scientist [Chris et al., 2024] and AgentReview [Jin et al., 2024a] implemented with various backbone models (GPT-o1-2024-12-17, Claude-3.5-sonnet-20241022,Gemini-2.0-Flash-Thinking-01-21,DeepSeek-V3, and DeepSeek-R1); (2) Fine-tuned baselines including CycleReviewer-8B and CycleReviewer-70B, both trained on ICLR 2024 review data.For inference, we use a temperature of 0.4 with maximum input and output lengths set to 100K and 16,384 tokens respectively to ensure complete text processing.</p>
<p>Main Results</p>
<p>Test results are shown in    We observe a strong correlation between fine-grained assessment capability and overall rating performance.Models that excel in dimension-specific evaluations, such as DeepReviewer and Claude-3.5-Sonnet,consistently demonstrate superior performance in overall ratings.This pattern validates the effectiveness of our multi-stage reasoning chain design, particularly the necessity of multi-facet evaluation in our framework.</p>
<p>Review Text Quality</p>
<p>Table 4 shows that DeepReviewer's overwhelming advantages across all evaluation dimensions.Interestingly, in the comparison with AI Scientist (Gemini-2.0-Flash-Thinking),despite being used as the judge, Gemini assessed most reviews in favor of DeepReviewer (winning 53.47% in constructive value and analytical depth), with only two dimensions showing preference for its own reviews (20.79% in technical accuracy).This self-critical evaluation further validates the objectivity of our assessment framework.In terms of overall judgment, DeepReviewer achieves remarkable win rates of 88.21% against AI Scientist (GPT-o1) and 98.15% against AgentReview (GPT-4o) on ICLR 2024.</p>
<p>The advantages are most prominent in constructive value and analytical depth.When compared with AgentReview (GPT-4o), DeepReviewer achieves win rates of 99.02% and 99.01%respectively, indicating that our Deep review with Thinking framework generates more insightful analysis and actionable suggestions.These qualitative assessments corroborate our quantitative findings, further validating the effectiveness of the multi-stage reasoning approach in our framework.</p>
<p>Defend Attacks Analysis</p>
<p>We evaluate DeepReviewer's robustness against adversarial attacks [Ye et al., 2024] by inserting malicious instructions into input papers.Figure 2 illustrates the rating comparison under normal and attack scenarios across different dimensions.Though not specifically trained with any adversarial samples, The DeepReviewer model demonstrates superior robustness compared to baseline systems.Under attack, the overall rating increase for DeepReviewer is merely 0.31 points (from 5.38 to 5.69), while other systems show substantial vulnerability, for example, Gemini-2.0-Flash-Thinkingexhibits a dramatic increase of 4.26 points (from 4.23 to 8.49) and DeepSeek-V3 shows a 1.41 increase (from 6.76 to 8.17).This pattern held across fine-grained dimensions: for instance, Soundness scores for DeepReviewer increased by only 0.12 points, compared to larger increases for Claude-3.5-Sonnet(1.10) and Gemini-2.0-Flash-Thinking(1.38).We attribute this robustness to DeepReviewer's multi-stage reasoning framework, which, unlike direct input-output models, including content understanding, novelty verification, and reliability checks.It enabling a focus on intrinsic paper quality despite malicious prompts.However, the slight score increases under attack suggest room for improvement, we suggest that incorporating adversarial samples during training.</p>
<p>Test-Time Scalability Study</p>
<p>DeepReviewer model features unique test-time scaling capabilities through two mechanisms, both controllable via input instructions: Reasoning Path Scaling and Reviewer Scaling.Reasoning Path Scaling offers three inference modes-Fast, Standard, and Best-with progressively deeper reasoning and corresponding output token lengths of approximately 3,000, 8,000, and 14,500 tokens, respectively.Complementing this, Reviewer Scaling, employed within Standard mode, adjusts the number of simulated reviewers from R=1 to R=6.It enabling the synthesis of multi-perspective evaluations through simulated reviewer collaboration.Both scaling mechanisms inherently extend the model's evaluation process: Reasoning Path Scaling by increasing analytical depth, and Reviewer Scaling by emulating collaborative review.</p>
<p>Performance Analysis.Figure 3 illustrates significant performance enhancements as inference computation increases.In Reasoning Path Scaling (red stars), switching from Fast to Best mode results in steady improvements across all metrics, with the Rating Spearman correlation increasing by 8.97% (from 0.326 to 0.355).Reviewer Scaling (green diamonds) presents more diverse patterns across various tasks.In scoring tasks (Decision Accuracy, Rating MSE, Soundness MSE), consistent performance gains are observed with additional reviewers, indicating that score aggregation is enhanced by multiple viewpoints.The performance variability in Reviewer Scaling, especially when R ≠ 4, likely arises from the model's training distribution being focused around four reviewers.Despite some variability, both scaling methods show positive trends (see regression lines), indicating our framework effectively uses more computational resources.The benefits vary by metric: scoring tasks improve most, followed by ranking, then selection.This suggests that multi-stage reasoning excels in complex paper evaluations, while simpler comparisons (e.g., choosing between two papers) gain less from added reasoning.Furthermore, we observe that DeepReviewer's Fast mode, with only half the output tokens (3000), outperformed the CycleReviewer model (6000 output tokens) across various metrics (See Table 3), including Decision Accuracy, Rating MSE, and fine-grained Spearman correlations for Soundness, Presentation, and Contribution.Despite its simplified reasoning path, Fast mode retains core evaluation logic, such as identifying key paper content and critical flaws.We show that DeepReviewer utilizes each token more effectively, focusing on the most crucial information and achieving high performance with fewer output tokens.Despite these variations, both scaling approaches demonstrate positive trends across metrics, validating that increased computational investment -whether through more sophisticated inference modes or additional simulated reviewers -enhances the model's paper assessment capabilities.</p>
<p>Conclusions</p>
<p>We presented DeepReviewer, a novel framework for research paper evaluation aimed at enhancing the reliability of LLMs in paper reviews.DeepReviewer achieves adaptable reasoning depth through Test-Time Scaling to meet diverse needs.Our contributions are threefold: ( 1</p>
<p>Limitations</p>
<p>Firstly, our approach relies on a synthetic dataset, DeepReview-13K, constructed through an automated pipeline.Although meticulously designed to mimic expert review processes and incorporating quality control mechanisms, this synthetic data may not fully capture the complexities and nuances of genuine human paper review.We have strived to mitigate this by leveraging real-world review data from ICLR conferences and incorporating structured reasoning annotations, but the inherent limitations of synthetic data persist.Secondly, while DeepReviewer offers Test-Time Scaling for efficiency, the "Best" mode, which employs the complete reasoning chain and external knowledge retrieval, can be computationally intensive.We address this by providing "Fast" and "Standard" modes, allowing for a trade-off between thoroughness and computational cost, catering to diverse application needs.Furthermore, while we have shown robustness against adversarial attacks, complete immunity is not yet achieved, indicating a need for ongoing research into enhancing security and reliability.Despite these limitations, DeepReviewer represents a significant step towards more reliable and robust LLM-based paper review systems, and our exploration of robust structured reasoning opens avenues for future research.</p>
<p>Ethical Considerations</p>
<p>The development of DeepReviewer, while holding significant promise for enhancing the efficiency and potentially the quality of scholarly paper review, inherently carries ethical considerations that demand careful attention.We recognize that automating aspects of the peer review process introduces risks of bias amplification, deskilling of human reviewers, and a potential erosion of transparency and accountability.Specifically, DeepReviewer, like any LLM, could inadvertently perpetuate or even amplify existing biases present in the training data or encoded within its architecture.This could lead to systematic disadvantages for research from underrepresented groups, novel or unconventional methodologies, or topics perceived as less mainstream, even if the DeepReview-13K dataset was synthetically generated to be representative and fair.Furthermore, over-reliance on automated review assistance might diminish the critical thinking skills of human reviewers, potentially leading to a deskilling effect over time and a dependence on AI-driven assessments without sufficient human oversight.</p>
<p>To proactively address these ethical concerns and mitigate potential harms, we have implemented a multi-faceted approach throughout DeepReviewer's development and deployment.Firstly, while our training data is synthetic, we have rigorously designed the DeepReview-13K dataset and its generation pipeline to explicitly model expert reviewer reasoning and incorporate diverse perspectives, aiming to minimize the introduction of unintended biases.Secondly, we emphasize that DeepReviewer is intended as a decision support tool, designed to augment, not replace, human expertise.We strongly advocate for a human-in-the-loop approach, where DeepReviewer's outputs are critically evaluated and contextualized by expert reviewers.To ensure transparency, we are releasing DeepReviewer as an open-source resource, allowing for community scrutiny of its code, architecture, and potential biases.Alongside the code release, we will provide comprehensive user guidelines and best practices that explicitly caution against over-reliance on automated outputs and emphasize the importance of human oversight and critical assessment.Furthermore, our open-source licensing, while permissive, mandates that users disclose their institutional affiliation, personal information, and intended use case upon downloading DeepReviewer.This measure aims to foster accountability and enable a feedback loop, allowing us to monitor real-world applications, gather user feedback, and iteratively improve the model and its ethical safeguards.We also commit to ongoing bias auditing and benchmarking of DeepReviewer across diverse datasets and review scenarios, continually evaluating its performance and identifying areas for refinement.We believe these proactive measures, combined with ongoing community engagement and responsible user practices, are crucial to harnessing the benefits of Deep-Reviewer while minimizing its potential for harm and ensuring its ethical and beneficial application within the scientific peer review process.</p>
<p>A Responsible Use and Recommendations for DeepReviewer</p>
<p>It is crucial to emphasize that DeepReviewer, despite its advancements in automated paper evaluation, is not intended to replace human peer review.Our work aims to enhance, not substitute, the invaluable expertise and nuanced judgment of human reviewers.DeepReviewer should be regarded as a sophisticated tool to assist researchers and the academic community, providing supplementary insights and streamlining certain aspects of the review process, but always under the careful oversight and final authority of human experts.This section outlines responsible and conservative recommendations for leveraging DeepReviewer's capabilities in practical scenarios, focusing on how it can aid human researchers and enhance the peer review process without undermining its fundamental human-centric nature.</p>
<p>A.1 Enhanced Author Self-Assessment and Manuscript Refinement</p>
<p>Perhaps the most appropriate and ethically sound application of DeepReviewer lies in empowering authors to critically assess and refine their manuscripts before they are submitted for formal peer review.By submitting their work to DeepReviewer, authors can obtain an automated, initial evaluation of their paper's perceived strengths and potential weaknesses across various dimensions such as soundness, clarity of presentation, and potential contribution.This feedback can highlight areas where the manuscript might be strengthened prior to exposure to human reviewers.</p>
<p>However, it is crucial for authors to approach DeepReviewer's feedback with a discerning and critical mindset.The automated evaluation should be considered as a preliminary signal, not a definitive judgment.Authors must exercise their own expertise and judgment in interpreting the suggestions.DeepReviewer's output may point to areas that warrant further attention, but the ultimate decisions regarding manuscript revision must rest with the authors themselves, informed by their deep understanding of their own work and potentially by seeking feedback from trusted colleagues.This application strictly positions DeepReviewer as a formative tool for author self-improvement, ensuring that it aids in enhancing manuscript quality without encroaching on the formal peer review process.</p>
<p>A.2 Preliminary Assistance for Human Reviewers in Initial Paper Scoping</p>
<p>In contexts where human reviewers are faced with a high volume of submissions, DeepReviewer could potentially offer a very limited form of preliminary assistance in the very initial stages of paper scoping.Reviewers could, as an optional and auxiliary step, utilize DeepReviewer to generate a rapid, automated overview of a submitted paper.This might provide a very high-level summary of potential areas of focus within the manuscript.Such a preliminary overview could, in some cases, help reviewers gain a very initial sense of the paper's scope and potentially assist in workload management, by allowing them to perhaps initially prioritize papers based on a very rough automated categorization.</p>
<p>However, it is absolutely vital to underscore that this use case is strictly as an aid to the reviewer's workflow, and not as a substitute for any aspect of their intellectual engagement with the paper.</p>
<p>The automated output from DeepReviewer should never influence the reviewer's own independent, detailed reading and critical analysis of the manuscript.Reviewers must engage deeply with the paper itself, applying their expertise and judgment.DeepReviewer's preliminary output, if used at all, should be treated as an extremely rough and initial signal only, and should not replace or diminish the core, human-driven process of rigorous peer review.Over-reliance on or misinterpretation of automated outputs at this stage carries significant risks and must be avoided.</p>
<p>A.3 Author-Facing Pre-Review Feedback via Deployed Model</p>
<p>An alternative application, focusing purely on author benefit, is to deploy DeepReviewer as a readily accessible service that authors can utilize to obtain feedback on their manuscripts before they are submitted to a journal or conference and undergo human peer review.In this scenario, DeepReviewer is made available as a tool that authors can directly interact with.Authors submit their manuscript, and in return, receive an automated review generated by DeepReviewer.</p>
<p>Critically, the output of DeepReviewer in this context is intended solely for the authors' information and improvement.It should not be used in any way as part of a formal submission or decision-making process.The feedback is provided directly to the authors, allowing them to gain insights into how an automated system might evaluate their work.This application bypasses the need to involve or burden human reviewers at this stage, focusing entirely on providing authors with a potentially helpful, albeit automated, perspective on their manuscript.It is essential to emphasize that the feedback generated by DeepReviewer in this author-facing context should be explicitly communicated as not being a substitute for, or representative of, genuine human peer review, and cannot be used as a basis for any acceptance or rejection decisions within formal academic venues.</p>
<p>B Evaluation Tasks and Metric</p>
<p>To comprehensively assess LLMs' capabilities in research paper evaluation, we adopt a point-wise evaluation paradigm inspired by the LLM-as-a-judge framework [Li et al., 2024a, Wang et al., 2024b, Rewina et al., 2025, Swarnadeep et al., 2025].We comprise three core tasks that examine different aspects of LLMs' ability to perceive, judge, and differentiate paper quality:</p>
<p>Score Task evaluates LLMs' accuracy in independent paper assessment scenarios.For any paper C i in the ReviewerBench dataset, the model independently conducts quality assessment and outputs a scalar score R i ∈ R as its predicted quality rating.Ideally, the model's predicted score R i should closely align with the average expert rating S i received during the ICLR review process.We employ Mean Squared Error (MSE) and Mean Absolute Error (MAE) as primary evaluation metrics for this task.Furthermore, we calculated accuracy and F1 score based on the Decision, which is commonly an Accept or Reject output in research paper evaluation systems.Review Comments Evaluate , following the LLM-as-Judge paradigm, we employ Gemini-2.0-Flash-Thinking(The system prompt as shown in Figure 4) as the judge to conduct pairwise comparative evaluations of review comments generated by DeepReviewer and various baseline systems, and Judge outputs "win", "lose", or "tie".For each evaluation instance, we present the assessor with:</p>
<p>(1) the original paper, and (2) paired reviews from different systems in randomized order, where each review contains summary, strengths, weaknesses, and suggestions.The assessment covers five critical dimensions: constructive value, analytical depth, plausibility, technical accuracy, and overall judgment.</p>
<p>C Data Collection Permissions</p>
<p>The original paper data and corresponding review comment data used to construct DeepReview-13K are sourced from OpenReview, with a portion of papers originating from ArXiv.Data from OpenReview is distributed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license, which permits us to copy and modify the review comment data.Paper data from ArXiv may include licenses such as CC BY 4.0 (Creative Commons Attribution), CC BY-SA 4.0 (Creative Commons Attribution-ShareAlike), CC BY-NC-SA 4.0 (Creative Commons Attribution-NonCommercial-ShareAlike), and CC Zero.Given that we have not modified the original papers, our usage is compliant with the original agreements.We do not claim copyright over these materials and will retain the original authors' names in the distribution of this data.</p>
<p>D Case Study: Analysis of DeepReviewer's Meta-Review</p>
<p>To further illustrate the capabilities of DeepReviewer, we present a detailed case study analyzing the Meta-Review generated by DeepReviewer-14B (Best mode) (See in Figure 8) for the "CycleResearcher" paper 5 [Weng et al., 2025], a submission from ICLR 2025 not included in the training dataset.This paper, focusing on automating the research lifecycle with LLMs, received four independent reviews from human experts (Reviewer 7LzG: Figure 9, CzSX: Figure 10, GAvj: Figure 11, and 5wHA: Figure 12).DeepReviewer-14B, operating in its most comprehensive "Best" mode, synthesized these diverse perspectives into a single Meta-Review, aiming to emulate the holistic understanding and critical assessment of a seasoned meta-reviewer.A preliminary examination reveals a striking alignment between DeepReviewer's Meta-Review and the individual human assessments, both in terms of overall sentiment, identified strengths and weaknesses, and even the final score prediction, which closely mirrors the average human rating.This case study delves deeper into the nuances of this comparison, highlighting both the remarkable capabilities and subtle limitations of DeepReviewer in mimicking expert meta-reviewing.</p>
<p>Comparing the summaries, DeepReviewer accurately captures the core contribution of the "CycleResearcher" paper, emphasizing the novel framework for automating the research lifecycle with LLMs, the two key components (CycleResearcher and CycleReviewer), the iterative reinforcement learning You are a neutral arbitrator evaluating peer review comments for academic papers.Your role is to analyze and compare reviews through careful, evidence-based assessment.Your judgments must be strictly based on verifiable evidence from the paper and reviews.</p>
<p>For each evaluation, you must:</p>
<ol>
<li>
<p>Thoroughly understand the paper by analyzing:</p>
</li>
<li>
<ol>
<li>Base all judgments on concrete evidence 2. Quote directly from source materials 3. Provide detailed reasoning chains 4. Maintain neutral arbitrator perspective 5. Judge Tie when evidence shows equal strength 6. Always justify Tie decisions with specific evidence When judging Tie: -Ensure both reviews demonstrate similar levels of quality -Provide explicit evidence showing comparable strengths -Explain why differences are not significant enough to favor one over the other -Consider both quantity and quality of evidence Begin analysis after receiving complete materials.Take time to examine evidence thoroughly and provide detailed, justified assessments.</li>
</ol>
</li>
</ol>
<p>Figure 4: System prompt used to guide Gemini-2.0-Thinking-Flaskas Judge to evaluate generated review comments.approach (SimPO), and the creation of the Review-5k and Research-8k datasets.This summary resonates strongly with the initial summaries provided by all four human reviewers, each of whom also highlighted these central aspects of the paper.Furthermore, DeepReviewer's identified strengths mirror the positive aspects recognized by the human reviewers.For instance, the "innovative approach to automating the research lifecycle" echoes Reviewer 7LzG's praise for the "highly innovative" framework and Reviewer 5wHA's acknowledgment of the "Innovative Use of Preference Data" and "Automation of the Research Lifecycle."The appreciation for the "Review-5k and Research-8k datasets" also aligns with Reviewer 5wHA's explicit mention of "Valuable Datasets" and Reviewer CzSX's comment on the datasets being a "resource that is rather helpful for the field."Similarly, the recognition of the "CycleResearcher model generates papers with an average quality level close to human-written preprints" echoes Reviewer GAvj's observation that the system "achieved an acceptance rate of 31.07%,similar to ICLR 2024's acceptance rate" and Reviewer 7LzG's claim of "papers of quality close to human-written preprints."The most compelling aspect of DeepReviewer's Meta-Review is its synthesis of weaknesses and corresponding suggestions, demonstrating an ability to identify and consolidate critical concerns raised across different reviewers.DeepReviewer's critique regarding "potential for bias in the training data" and "lack of analysis of diversity" directly addresses concerns implicitly or explicitly raised by reviewers, particularly regarding generalizability and potential limitations of the datasets.The weakness concerning "computational resources" aligns with Reviewer 7LzG's mention of "Complexity of Implementation" and the need for "significant computational resources."Similarly, the concern about the "potential for misuse" and the need for "robust safeguards" reflects the ethical considerations raised by Reviewer 5wHA ("Insufficient Ethical Considerations," "Misuse of Technology") and Reviewer GAvj ("Potentially harmful insights, methodologies and applications").The suggestion for "more details on the specific prompts" and "evaluation criteria" addresses the implicit desire for more clarity on methodology, a common thread in academic reviews.Finally, the point about "generalizability across different research domains" directly mirrors Reviewer 7LzG's primary "Weakness: Generalizability Across Domains."This systematic identification and aggregation of weaknesses and suggestions from multiple reviewers showcase DeepReviewer's capacity to perform a nuanced and comprehensive meta-analysis.</p>
<p>While DeepReviewer-14B demonstrates a remarkable ability to synthesize human review insights, it is important to acknowledge potential limitations.For instance, while DeepReviewer captures the essence of the critiques, the depth of technical understanding in specific areas might not fully match that of a human meta-reviewer deeply versed in the nuances of reinforcement learning or AI ethics.Furthermore, the Meta-Review, while comprehensive, might lack the subtle nuances and perspectives that a human meta-reviewer could bring to the synthesis process, potentially overlooking more implicit or nuanced concerns expressed in the individual reviews.However, despite these subtle limitations, DeepReviewer's performance in generating a coherent, insightful, and critically aligned Meta-Review is undeniably impressive.Crucially, DeepReviewer's overall rating prediction of 6.0 aligns closely with the average human rating, further validating its ability to not only understand the qualitative aspects of paper evaluation but also to synthesize them into a quantitative judgment consistent with expert consensus.This case study underscores DeepReviewer's potential as a powerful tool for assisting and potentially augmenting the peer review process.</p>
<p>E Information About Use Of AI Assistants</p>
<p>This article has been reviewed by DeepReviewer-14B and revised accordingly based on its review comments.</p>
<p>You are participating in a knowledge distillation task to capture the academic reviewing thought process of a target model.While you will receive structured summaries and review opinions of papers, you must analyze them as if reading complete academic manuscripts directly.</p>
<p>IMPORTANT:</p>
<p>1.Your primary goal is to reveal your complete thinking process about the paper 2. Within the thought block, focus exclusively on analyzing the paper's content 3. Never mention JSON, review opinions, or structured data in your analysis ANALYSIS STAGES (Each requiring careful consideration):</p>
<p>Meta-Review From DeepReviewer-14B</p>
<p>Summary:</p>
<p>This paper introduces a novel framework for automating the research lifecycle using large language models (LLMs).The framework aims to mimic the iterative process of research, review, and refinement.The core of this framework consists of two models: CycleResearcher, a policy model for generating research papers, and CycleReviewer, a reward model that simulates peer reviews.The authors use an iterative reinforcement learning approach, specifically SimPO, to refine both models.To facilitate training and evaluation, the authors created two large-scale datasets, Review-5k and Research-8k.Empirical results show that CycleResearcher generates papers with an average quality comparable to human-written preprints, achieving a 31.07\%acceptance rate.CycleReviewer shows a 26.89\% improvement in mean absolute error (MAE) compared to individual reviewers.The contribution lies in the application of LLMs to automate the entire research lifecycle, the introduction of large-scale datasets, and the demonstration of a functional iterative framework.This work can impact the way research is conducted, particularly in machine learning.The paper raises questions about potential biases, computational costs, and ethical implications.</p>
<p>Strength:</p>
<p>The paper's core strength is its innovative approach to automating the research lifecycle using LLMs.</p>
<p>Weaknesses:</p>
<p>-The paper does not adequately address the potential for bias in the training data for both CycleResearcher and CycleReviewer.There's a risk that the models perpetuate existing biases.The paper describes the data collection process for Review-5k and Research-8k, but lacks any discussion of potential biases within these datasets.The lack of analysis of the diversity of authors, institutions, or research topics is a significant concern.</p>
<p>-The paper lacks a detailed discussion on computational resources for training and deploying the framework.The use of large language models and iterative refinement processes can be computationally intensive.While the paper mentions the hardware used, it does not provide explicit training times per epoch or iteration, nor does it provide inference times.</p>
<p>-The paper does not sufficiently explore the potential for misuse of the framework.The paper introduces Fast-DetectGPT and discusses ethical considerations, but lacks mechanisms beyond the RL framework to guarantee the factual correctness or scientific validity of the generated content.</p>
<p>-The paper could provide more details on the specific prompts used for the LLMs and the criteria for evaluating the quality of generated papers.The paper describes the stages of paper generation but does not provide the exact prompts used for each stage.Similarly, it mentions the criteria used by CycleReviewer, but not the specific prompts.</p>
<p>-The paper could benefit from a more detailed discussion of the limitations, such as the potential for generating derivative or low-quality research.It is unclear how the framework addresses the risk of converging to safe or incremental ideas.Finally, the paper could explore the generalizability of the framework across different research domains.The current experiments seem to focus on machine learning and computer science, and it is unclear how the framework would perform in other fields.</p>
<p>Suggestions:</p>
<p>-Implement a more rigorous analysis of training datasets for bias.Use stratified sampling or re-weighting.Explore methods for detecting and reducing bias in model outputs.Include a qualitative analysis of the generated outputs.Release training data for independent scrutiny.</p>
<p>-Provide a detailed breakdown of hardware and software requirements.Explore methods for reducing the computational cost.Provide a comparison of the computational resources required.Consider providing a cloud-based deployment option.</p>
<p>-Implement robust safeguards to ensure the quality and integrity of generated research outputs.Use automated quality checks, and implement a human-in-the-loop review process.Explore methods for detecting and preventing malicious use.Clearly communicate the limitations of the framework.</p>
<p>-Provide a detailed breakdown of the prompts used for each stage of the research lifecycle, including examples.Clearly define evaluation criteria.Improve reproducibility and transparency.</p>
<p>-Conduct a more thorough analysis of the limitations, regarding the potential for generating derivative or low-quality research.Explore methods for mitigating the risk of converging to safe or incremental ideas.</p>
<p>-Investigate the generalizability of the framework across different research domains.Conduct experiments in diverse fields and analyze the performance in each domain.</p>
<p>Questions:</p>
<ul>
<li>Temperature setting: 0.4 Three independent evaluation runs Consistent performance metrics (std &lt; 1%) This validates the reliability of our evaluation framework.7. Domain Adaptation Discussion: We clarified our framework's current focus on machine learning while outlining concrete steps for cross-domain expansion through publisher collaborations and domain-specific training data collection (Appendix Limition).8. Terminology Refinement: We updated our terminology from "revision" to "refinement" to more accurately reflect the model's learning process and avoid confusion with paper revisions (Throughout paper).</li>
</ul>
<p>With the generous guidance from our esteemed reviewers, we have been able to substantially enhance our paper's technical depth, experimental rigor, and practical impact.We believe these comprehensive improvements address the key concerns raised while maintaining the innovative contributions of our work.We would be honored if the reviewers find these enhancements worthy of a more favorable evaluation.</p>
<p>Official Review of Submission489 by Reviewer 7LzG Weaknesses: Generalizability Across Domains: The models are primarily designed for machine learning-related research, and their generalizability to other scientific fields remains unexplored.This limitation suggests that the framework might not perform as well in domains outside of machine learning.Reward Design: The paper highlights the issue of reward definition, where the policy model might exploit loopholes in the reward model to maximize rewards without genuinely improving the quality of the generated research.This behavior could undermine the long-term goal of producing highquality research outputs.Complexity of Implementation: Implementing the framework requires significant computational resources and expertise in reinforcement learning and LLMs.This complexity might be a barrier for widespread adoption, especially for smaller research teams or institutions with limited resources.</p>
<p>Questions:</p>
<p>The framework is primarily designed for machine learning-related research.Comment:</p>
<p>Weakness: Generalizability Across Domains: The models are primarily designed for machine learning-related research, and their generalizability to other scientific fields remains unexplored.This limitation suggests that the framework might not perform as well in domains outside of machine learning.</p>
<p>We appreciate the reviewer's observation about our framework's current domain specificity.Indeed, we acknowledge that our present implementation focuses primarily on machine learning research, and the generalizability across different scientific domains remains to be thoroughly validated.This is an important limitation of our current work that deserves careful consideration and future investigation.Our reinforcement learning optimization framework is designed to be domain-agnostic in its core architecture.The iterative preference optimization mechanism we developed focuses on universal academic qualities such as methodological soundness, clarity of presentation, and significance of contribution, rather than domain-specific content.We chose machine learning as our initial domain primarily because it offers abundant high-quality training data through open-access papers and peer reviews, and uniquely allows for potential automation of experimental processes through code generation and results analysis.</p>
<p>Question: The framework is primarily designed for machine learning-related research.How do you envision adapting CycleResearcher and CycleReviewer to other scientific fields, such as biology or social sciences, where the nature of research and evaluation criteria might differ significantly?</p>
<p>We envision several key steps for adapting our framework to other scientific domains.First, domain-specific versions of CycleResearcher and CycleReviewer could be trained using papers and peer reviews from respected journals in each field (e.g., Nature for biology, or top social science journals).In our early research phases, we explored this possibility but found that the available open-access data was often limited and fragmented across subspecialties.We believe this challenge could be addressed through collaborations with publishers and institutions to access larger, more cohesive training datasets.Additionally, the evaluation criteria would need to be carefully adapted to match field-specific standards and practices, perhaps through consultation with domain experts.While significant challenges remain, including the need for human intervention in physical experiments, we believe our framework provides a promising foundation for cross-domain expansion once these prerequisites are met.We welcome future collaborations with researchers from diverse fields to explore these possibilities.Thank you again for your suggestions.We have revised the Limitations section and added relevant discussions!</p>
<p>Weakness: Reward Design: The paper highlights the issue of reward definition, where the policy model might exploit loopholes in the reward model to maximize rewards without genuinely improving the quality of the generated research.</p>
<p>We are deeply grateful for the reviewer's insightful observation regarding potential reward exploitation issues.While our current implementation shows promising results, we fully acknowledge that reward hacking remains a significant challenge in reinforcement learning systems that aim to generate high-quality research outputs.To address this concern, we conducted extensive validation through both human expert evaluation and additional experiments using separated reward models.The relatively small performance difference (approximately 0.14 in average score) suggests our framework maintains robust performance even with an independently trained reward model.However, we fully acknowledge that more work is needed to comprehensively address reward exploitation.We've added detailed discussions of these findings and future directions to the appendix F.2, and we sincerely thank the reviewer for helping us strengthen this critical aspect of our work.</p>
<p>Summary:</p>
<p>The paper presents CycleResearcher and CycleReviewer, which is a cohesive system intended to make steps towards automatic scientific discovery.In particular the novelty of the approach lies in the encapsulation of the entire research pipeline from research to review, in order to better model the entire system of research generation and get better outcomes.The authors contribute two datasets for research and review, and use these to train the system of researcher and reviewer, and evaluate them using various methods and metrics.</p>
<p>Soundness: 3: good Presentation: 2: fair Contribution: 2: fair Strengths: Originality: The idea to design both a researcher and a reviewer is novel and interesting.</p>
<p>Quality: The usage of recent preference optimization methods is a nice technical plus.The work contributes datasets to the direction of scientific peer reviewing, which is a resource that is rather helpful for the field.RL details and how they fit in is nice.</p>
<p>Clarity: Figures are well-designed and artistically pleasing.Appreciate the various different ways that are used to evaluate the methods (qualitative, ablations, etc.)</p>
<p>Significance: The automation of scientific research and reviewing is a very interesting and timely topic.In particular, due to the massive increase in submissions year-to-year, progress towards the paper's direction is well appreciated.</p>
<p>Weaknesses: Comment:
≡ ≡ ≡ ≡
Weakness: Generalizability Across Domains: The models are primarily designed for machine learning-related research, and their generalizability to other scientific fields remains unexplored.This limitation suggests that the framework might not perform as well in domains outside of machine learning.</p>
<p>We appreciate the reviewer's observation about our framework's current domain specificity.Indeed, we acknowledge that our present implementation focuses primarily on machine learning research, and the generalizability across different scientific domains remains to be thoroughly validated.This is an important limitation of our current work that deserves careful consideration and future investigation.Our reinforcement learning optimization framework is designed to be domain-agnostic in its core architecture.The iterative preference optimization mechanism we developed focuses on universal academic qualities such as methodological soundness, clarity of presentation, and significance of contribution, rather than domain-specific content.We chose machine learning as our initial domain primarily because it offers abundant high-quality training data through open-access papers and peer reviews, and uniquely allows for potential automation of experimental processes through code generation and results analysis.</p>
<p>Question: The framework is primarily designed for machine learning-related research.How do you envision adapting CycleResearcher and CycleReviewer to other scientific fields, such as biology or social sciences, where the nature of research and evaluation criteria might differ significantly?</p>
<p>We envision several key steps for adapting our framework to other scientific domains.First, domain-specific versions of CycleResearcher and CycleReviewer could be trained using papers and peer reviews from respected journals in each field (e.g., Nature for biology, or top social science journals).In our early research phases, we explored this possibility but found that the available open-access data was often limited and fragmented across subspecialties.We believe this challenge could be addressed through collaborations with publishers and institutions to access larger, more cohesive training datasets.Additionally, the evaluation criteria would need to be carefully adapted to match field-specific standards and practices, perhaps through consultation with domain experts.While significant challenges remain, including the need for human intervention in physical experiments, we believe our framework provides a promising foundation for cross-domain expansion once these prerequisites are met.We welcome future collaborations with researchers from diverse fields to explore these possibilities.Thank you again for your suggestions.We have revised the Limitations section and added relevant discussions!</p>
<p>Weakness: Reward Design: The paper highlights the issue of reward definition, where the policy model might exploit loopholes in the reward model to maximize rewards without genuinely improving the quality of the generated research.</p>
<p>We are deeply grateful for the reviewer's insightful observation regarding potential reward exploitation issues.While our current implementation shows promising results, we fully acknowledge that reward hacking remains a significant challenge in reinforcement learning systems that aim to generate high-quality research outputs.To address this concern, we conducted extensive validation through both human expert evaluation and additional experiments using separated reward models.The relatively small performance difference (approximately 0.14 in average score) suggests our framework maintains robust performance even with an independently trained reward model.However, we fully acknowledge that more work is needed to comprehensively address reward exploitation.We've added detailed discussions of these findings and future directions to the appendix F.2, and we sincerely thank the reviewer for helping us strengthen this critical aspect of our work.</p>
<p>Summary:</p>
<p>The paper presents CycleResearcher and CycleReviewer, which is a cohesive system intended to make steps towards automatic scientific discovery.In particular the novelty of the approach lies in the encapsulation of the entire research pipeline from research to review, in order to better model the entire system of research generation and get better outcomes.The authors contribute two datasets for research and review, and use these to train the system of researcher and reviewer, and evaluate them using various methods and metrics.</p>
<p>Soundness: 3: good Presentation: 2: fair Contribution: 2: fair Strengths: Originality: The idea to design both a researcher and a reviewer is novel and interesting.</p>
<p>Quality: The usage of recent preference optimization methods is a nice technical plus.The work contributes datasets to the direction of scientific peer reviewing, which is a resource that is rather helpful for the field.RL details and how they fit in is nice.</p>
<p>Clarity: Figures are well-designed and artistically pleasing.Appreciate the various different ways that are used to evaluate the methods (qualitative, ablations, etc.)</p>
<p>Significance: The automation of scientific research and reviewing is a very interesting and timely topic.In particular, due to the massive increase in submissions year-to-year, progress towards the paper's direction is well appreciated.</p>
<p>Weaknesses: Originality: N/A Quality: One big issue of the paper is the method in which the authors obtain the "ground truth" review score: "for each submission, we use the average of the other n − 1 reviewers' scores as an estimator of the true score."In my opinion (and what feels like a general consensus in the community), it's pretty clear that this isn't the correct approach in determining a ground truth quality of a paper.Different reviewers have different expertises and opinions, and may disagree substantially based on their backgrounds, but this is a positive quality of peer review rather than a negative one.Thus, the metric used to judge the "loss" of a review score can be used to train proxies of reviewers, sure, but it does not make sense to then take the trained system and use the same metric to compare it against actual human reviewers.For instance, if human reviewers vary differently based on their perspectives and CycleReviewer is just doing some "hedge" where most scores are around the median score for all papers, it might achieve better than human performance on the MAE metric that is used in the evaluation.Furthermore, focusing on the score ignores perhaps the more important points of paper reviewing, such as being able to highlight errors in the paper or provide advice for making changes that are adopted in future versions.I think the true objective of reviewing in the paper's cycle paradigm matches these objectives more, although I recognize that they are even harder to quantify.Even so, I think the paper is overclaiming by saying that the lower MAE suggests that "LLMs can surpass expert-level performance in research evaluation".</p>
<p>And since I don't necessarily agree with the evaluation metric for the reviewer, this casts doubt on the results for the CycleResearcher because the CycleReviewer is reviewing the CycleResearcher.Also, since CycleResearcher is optimized on CycleReviewer, then saying that CycleResearcher does better on CycleReviewer than humans or AI scientist doesn't mean much.The qualitative study in section 4.3 is helpful to remove some of these doubts though.</p>
<p>Clarity: There are a reasonable amount of typos and grammatical errors in the document.For instance, CycleReviewer is replaced with WhizeReviewer in Section 4.1.The title of section 4.1 should be "Experiments on Paper Review Generation", etc.The paper would benefit from a pass over to correct grammatical mistakes in general to make it easier to read.</p>
<p>Significance: The claims are very catchy that the system can generate better reviews and papers than humans.However, given the questionable-ness of the metric, I think these are discounted to a degree.</p>
<p>Questions: See Weaknesses.We sincerely thank you for your detailed feedback and thoughtful suggestions throughout the discussion phase.Your comments have been invaluable in improving our work.</p>
<p>We will carefully address all the issues you identified, including correcting the project name in Figure 6 from 'WhizResearch' to 'CycleResearcher', fixing the capitalizations in references, and most importantly, refining the claims in our conclusion section to maintain consistency with our revised, more measured stance throughout the paper.We appreciate your attention to these details that help ensure the accuracy and professionalism of our work.</p>
<p>Furthermore, we fully agree with your emphasis on responsible system maintenance and ethical considerations.We deeply understand that ethical considerations are essential for the ML community and must be approached with utmost care.We commit to maintaining a careful and responsible attitude -advancing AI development while being mindful of potential societal impacts.To this end, we have already begun implementing comprehensive measures, including: revising distribution protocols for code, models and data; establishing detailed usage guidelines; and developing dedicated platforms for application monitoring and public information sharing.These initiatives are currently in progress, and we plan to make them publicly accessible at an appropriate time.</p>
<p>In conclusion, we sincerely thank all reviewers and express our highest respect for your insights that have significantly enhanced both the technical content and ethical considerations of our work.</p>
<p>Summary:</p>
<p>The paper introduces an iterative training framework for automatic generation and review of research papers using open-source LLMs.The core of their approach consists of two main components:</p>
<ol>
<li>CycleResearcher: A policy model that generates the paper, prompted by abstracts of related work.2. CycleReviewer: A reward model that writes several peer reviews and returns scores according to ICLR criteria.</li>
</ol>
<p>The authors initialize these models by supervised fine-tuning on scraped conference papers and ICLR reviews.They then improve the CycleResearcher using reinforcement learning (specifically iterative Simple Preference Optimization, SimPO), using CycleReviewer as a reward model.</p>
<p>The paper claims three main contributions:</p>
<ol>
<li>Development of an iterative reinforcement learning framework that mirrors the real-world research-review-revision cycle.Weaknesses:</li>
</ol>
<p>The writing is overclaiming the extent to which the paper covers the full research process.Authors write that the paper "explores performing the full cycle of automated research and review", however the paper omits crucial part of the process: actually running experiments.Instead, the authors train models to write complete papers purely from abstracts of past work, with completely hallucinated experiment design and results.I do not think that the task authors train models for -hallucinating experiment results and writing papers for them -is well motivated.Using models for this purpose will not contribute real knowledge to the scientific field.I think this is dual use technology, if not a completely malicious one.I could imagine the paper could be reframed to center on demonstrating this imminent failure of the reviewing system and raising an alarm, allowing the scientific community to adapt.In current form, the paper is probably net-negative.I have a number of concerns about the human evaluation procedure.</p>
<p>When the authors evaluate their CycleResearcher with the AI Scientist, they seem to only use rejection sampling (best of N) for CycleResearcher.This is not a fair comparison.</p>
<p>Overall, human evaluation is conducted on a small scale (10 papers total, three human reviewers, 2 methods: this paper and baseline) I do not think 30min per review (including reading, writing comments &amp; providing scores) is enough!I think it's misleading to use the term "revision" for parameters updates of the policy model (Figure 2).The paper refers to this revision as part of the full research process ("Research-Rebuttal-Revision") but this does not actually involve revision of papers based on reviews.</p>
<p>Questions:</p>
<ol>
<li>What exactly are the prompts, based on which CycleResearcher generates papers during evaluations?2. Why do smaller CycleResearcher models get better scores in the evaluation? 3. How many samples in automated evaluation? 4. Please include the average real score of accepted papers given by human ICLR2024 reviewers.5. How do you compute the acceptance rates, e.g. one mentioned in line 128? 6.For human evaluation: 1. Please report the N used in best-of-N / rejection sampling.2. Please clarify whether each paper is evaluated by one or several humans.3. How are the human experts chosen? 4. What do you mean by saying "excluding formatting considerations" in the assessment, and why is it omitted?</li>
</ol>
<p>Flag For Ethics Review: Yes, Potentially harmful insights, methodologies and applications Details Of Ethics Concerns: I do not think that the task authors train models for -hallucinating experiment results and writing papers for them -is well motivated.Using models for this purpose will not contribute real knowledge to the scientific field.I think this is dual use technology, if not a completely malicious one.I could imagine the paper could be reframed to center on demonstrating this imminent failure of the reviewing system and raising an alarm, allowing the scientific community to adapt.In current form, the paper is probably net-negative.I think the results from this paper should be known to the broad public, but not in the current framing.</p>
<p>Rating The writing is overclaiming the extent to which the paper covers the full research process.Authors write that the paper "explores performing the full cycle of automated research and review", however the paper omits crucial part of the process: actually running experiments.Instead, the authors train models to write complete papers purely from abstracts of past work, with completely hallucinated experiment design and results.</p>
<p>I do not think that the task authors train models for -hallucinating experiment results and writing papers for them -is well motivated.Using models for this purpose will not contribute real knowledge to the scientific field.I think this is dual use technology, if not a completely malicious one.I could imagine the paper could be reframed to center on demonstrating this imminent failure of the reviewing system and raising an alarm, allowing the scientific community to adapt.In current form, the paper is probably net-negative.</p>
<p>We deeply appreciate your critical feedback regarding the scope and ethical implications of our work.Let me address your concerns comprehensively:</p>
<p>First, we acknowledge your feedback about overclaiming and have revised our abstract and introduction to more precisely reflect our work's scope.We sincerely appreciate your detailed feedback and have made substantial revisions during the rebuttal period.Most significantly, we have implemented a complete automated scientific discovery pipeline with real experimental capabilities, as demonstrated in our new Appendix G.The system now generates and executes verifiable experiments through GPT-o1-preview, with full codebase and logs included in our supplementary materials.We've also addressed your concerns about reward exploitation through additional independent evaluation experiments, provided more comprehensive comparison data with ICLR standards, clarified our sampling methodology, and revised terminology throughout the paper for better accuracy (e.g., using "refinement" instead of "revision").Given these clarifications and improvements, particularly regarding the integration of real experimental implementations, we hope these updates provide helpful context for understanding our work's scope and contributions.</p>
<p>Summary:</p>
<p>The authors introduce two core components: CycleResearcher, a policy model that autonomously performs research tasks, and CycleReviewer, a reward model that simulates the peer review process.</p>
<p>Experimental results suggest that CycleReviewer can outperform individual human reviewers in scoring consistency, and CycleResearcher shows promise in generating research papers that approach the quality of human-written preprints.</p>
<p>Soundness: 3: good Presentation: 4: excellent Contribution: 3: good Strengths:</p>
<p>Valuable Datasets: The introduction of the Review-5k and Research-8k datasets could be highly beneficial to the research community.These datasets provide resources for training and evaluating models in academic paper generation and review, potentially fostering further advancements in automated research tools.</p>
<p>Innovative Use of Preference Data: Utilizing preference data to iteratively train the CycleResearcher model is an interesting approach.This method allows the model to improve over multiple iterations, aligning more closely with human standards through reinforcement learning.</p>
<p>Ethical Safeguards: The inclusion of a detection model to identify AI-generated papers addresses ethical concerns related to the misuse of automated research tools.By implementing such safeguards, the authors demonstrate a commitment to responsible AI deployment.</p>
<p>Automation of the Research Lifecycle: The paper attempts to automate the full research cycle, from idea generation to peer review and revision.This holistic approach is ambitious and, if successful, could significantly impact the efficiency of scientific research.</p>
<p>Weaknesses: Quality of Generated Papers: Upon examining the samples provided in the Appendix (Sections E.1 and E.2), it is evident that the generated papers contain hallucinations and inaccuracies.For instance, in the generated abstracts, there are claims of outperforming state-of-the-art methods without substantial evidence or appropriate citations.This raises concerns about the reliability of the CycleResearcher model in producing high-quality, factual research papers.</p>
<p>Counterintuitive Results with Model Scaling: In Table 3 (Section 4.2), the CycleResearcher-12B model achieves a higher acceptance rate than the larger 72B and 123B models.This is counterintuitive, as larger models typically perform better due to increased capacity.The paper does not provide sufficient analysis or explanations for this phenomenon, leaving readers questioning the scalability and efficacy of the approach.</p>
<p>Insufficient Ethical Considerations: While the authors mention the implementation of a detection tool for AI-generated papers, the paper lacks a deep exploration of the ethical implications of automating research.Issues such as accountability, potential misuse, and the impact on the scientific community are not thoroughly addressed.A dedicated discussion in the Ethics Considerations section would strengthen the paper.</p>
<p>Questions: Explanation for Performance of Smaller Models: In Table 3, why does the CycleResearcher-12B model receive the highest acceptance rate compared to the 72B and 123B models?This result is unexpected given that larger models generally have better performance.Could the authors provide an analysis of this outcome, possibly including case studies or error analysis to understand the limitations of larger models in this context?</p>
<p>Evaluation Stability of CycleReviewer: What is the temperature setting used for the CycleReviewer during evaluation?Additionally, have the authors experimented with running the CycleReviewer multiple times to assess the variability or deviation in the review scores and feedback?Understanding the stability and consistency of the CycleReviewer is important for gauging its reliability in the automated review process.</p>
<p>Addressing Hallucinations in Generated Papers: Given the observed hallucinations and inaccuracies in the sample generated papers (Appendix E), what strategies do the authors propose to mitigate these issues?Are there mechanisms in place to fact-check or verify the content produced by the CycleResearcher before it is submitted for automated review?</p>
<p>Flag For Ethics Review: Yes, Discrimination / bias / fairness concerns, Yes, Privacy, security and safety Details Of Ethics Concerns: Accountability and Authorship: If AI systems generate research papers, questions arise regarding authorship and accountability for the content.It's essential to clarify who is responsible for the work produced and how credit should be assigned.</p>
<p>Quality and Integrity of Research: The presence of hallucinations and factual inaccuracies in AI-generated papers could undermine the integrity of scientific literature.There is a risk of disseminating false information, which could have downstream effects if other researchers build upon flawed results.</p>
<p>Misuse of Technology: The tools developed could be misused to generate large volumes of low-quality or misleading research, potentially cluttering academic discourse and making it harder to identify valuable contributions.</p>
<p>Impact on the Research Community: Automation might affect the roles of researchers, peer reviewers, and the collaborative nature of scientific inquiry.There is a need to consider how these technologies will coexist with human efforts and what support structures are necessary to ensure they augment rather than hinder scientific progress.</p>
<p>Rating Quality of Generated Papers: Upon examining the samples provided in the Appendix (Sections E.1 and E.2), it is evident that the generated papers contain hallucinations and inaccuracies.For instance, in the generated abstracts, there are claims of outperforming state-of-the-art methods without substantial evidence or appropriate citations.This raises concerns about the reliability of the CycleResearcher model in producing high-quality, factual research papers.</p>
<p>We sincerely appreciate your insightful feedback and are excited to clarify the sophisticated architecture of our research automation system.The CycleResearcher model serves as the intellectual cornerstone of our framework, functioning as a high-level research strategist that excels in comprehensive literature review, hypothesis generation, experimental design, and research planning.This approach mirrors the natural division of labor in academic research, where different specialists contribute their unique expertise to the collective scientific endeavor.</p>
<p>During the rebuttal period, we've implemented a comprehensive end-to-end research pipeline that demonstrates the full potential of our framework.We've updated our appendix with new papers featuring complete experimental implementations, requiring approximately 20$ and 6 A100-GPU hours for computation.To promote transparency and community advancement, we've submitted supplementary materials including our framework's codebase and detailed experiment logs.All code will be distributed under the MIT license, ensuring broad accessibility and reusability.This enhancement represents a significant step forward in automated scientific discovery, combining CycleResearcher's strategic capabilities with robust</p>
<p>(PaperFigure 1 :
1
Figure 1: Overview of the DeepReviewer.(a) Input paper example with a real-world research paper.(b) Output example showing DeepReviewer's multi-stage reasoning process: Novelty Verification, Multi-dimension Review, and Reliability Verification.(c) Inference modes: fast, standard, and best, highlighting different reasoning paths.We provide a more detailed case study in the appendix D.</p>
<p>Figure 3 :
3
Figure 3: The performance of the DeepReviewer model in the Test-Time Scaling experiment.The x-axis represents the number of Tokens generated during model inference, and the y-axis represents different evaluation metrics.The green and red dashed lines are linear regression fitting curves for Reasoning Path Scaling and Reviewer Scaling scaling methods, respectively.</p>
<p>) the creation of DeepReview-13K, a detailedly annotated dataset that facilitates training for systematic and deep paper evaluation; (2) the training of the DeepReviewer model; and (3) comprehensive validation of DeepReviewer's superiority in both objective and subjective assessments.Notably, we explored and demonstrated effective Test-Time Scaling through Reasoning Path and Reviewer Scaling strategies.</p>
<p>FUNDAMENTAL RULES: -Write as the original reviewer who has NOT seen any response -Never mention or hint at the existence of author response -Maintain the exact formatting style of the original review -Keep consistent technical depth throughout -Use numerical citations only when original citations are paper titles REVIEW IMPROVEMENT PROCESS: 1. Analyze Original Review -Identify each criticism point -Understand the technical depth of each point -Note the writing style and tone -Map the logical flow of arguments -Determine if citations are paper titles 2. Use Response Understanding (without reference) -Identify which criticisms are valid concerns -Recognize which points are misunderstandings -Note where technical depth could be enhanced -Understand which aspects are most important WEAKNESSES REQUIREMENTS: Format Requirements: -Maintain exact formatting of original review -Keep same paragraph breaks and structure -Preserve section organization -Use numerical citations only if original citations are paper titles -Include citations in array only if they are paper titles Content Enhancement: -Expand valid technical criticisms with specific details -Remove confirmed misunderstandings -Transform vague criticisms into specific technical points -Add concrete examples where appropriate -Maintain professional and constructive tone -Only use numerical citations if original citations are paper titles Writing Style: -Use precise technical terminology -Provide detailed reasoning -Keep consistent technical depth -Maintain professional tone -Focus on substantive issues CITATION HANDLING: When Original Contains Paper Titles: -Use numerical format: [1], [2], etc. -Include complete titles in citations array -Format multiple references as [1,2] or [1,2,3] -NEVER create fake paper titles -ONLY cite papers from original review When Original Does Not Contain Paper Titles: -Set citations array to empty [] -Do not use numerical citations in text -Maintain original criticism without citation format Example JSON With Paper Title Citations: { "weaknesses": "This method has limitations compared to previous work [1,2].The evaluation metrics are similar to [3].", "suggestions": "Detailed suggestions text..: "This method has limitations compared to previous work.The evaluation metrics are similar to existing approaches.","suggestions": "Detailed suggestions text..paragraphs -Total length approximately 500 words -Each paragraph 150-200 words -Maintain logical flow between paragraphs -Include citations only if original contains paper titles Content Requirements: Each paragraph should demonstrate: -Deep technical understanding -Specific implementation details -Concrete methodological improvements -Clear practical guidance -Logical connection to weaknesses -Citations only when original contains paper titles FORMAT: If original uses multiple line breaks: -Keep identical break patterns -Maintain section lengths -Use same spacing structure If original is continuous: -Keep continuous paragraph format -Maintain paragraph density -Don't introduce new breaks OVERALL: QUALITY CRITERIA: 1. Technical depth matches or exceeds original review 2. All points are specific and actionable 3. Maintains professional and constructive tone 4. Provides concrete examples and details 5. Suggestions address all valid weaknesses 6. Logical flow between and within sections 7. Proper citation handling based on original format CRITICAL REMINDERS: 1.Never reveal knowledge from response 2. Write as initial reviewer 3. Maintain original formatting 4. Provide specific details 5. Keep consistent technical depth 6. Transform vague points into specific ones 7.Only use numerical citations when original citations are paper titles 8.Only include citations array when original contains paper titles Remember: Your task is to write an enhanced initial review that demonstrates deeper technical understanding while maintaining the original perspective and proper citation handling based on the nature of original citations.</p>
<p>Figure 5 :
5
Figure 5: System prompt designed to instruct the LLM on how to enhance and improve the usefulness of original review comments by incorporating author responses and maintaining original review context.</p>
<ol>
<li>RESEARCH CONTEXT AND HISTORICAL PERSPECTIVE (3-4 minutes) -Evolution of research in this field -Key historical developments and breakthroughs -Existing research gaps and limitations -Previous approaches to similar problems -Broader academic context 2. PROBLEM SPACE EXPLORATION (3-4 minutes) -Core research challenges -Research motivations -Real-world implications -Problem-solving significance analysis following all stages above, demonstrating deep thinking and systematic evaluation while maintaining focus purely on paper content] Remember: You should consider that you have thoroughly read and comprehended the complete paper.Your analysis should demonstrate careful consideration of each stage while maintaining the natural flow of academic thinking.The single thought block should capture your complete reasoning process, reflecting both explicit and implicit aspects of the research.</li>
</ol>
<p>Figure 6 :
6
Figure6: System prompt designed to guide the LLM in detailed analysis of research papers.This prompt is used specifically during the Novelty Verification stage to make analysis context.</p>
<p>Figure 7 :
7
Figure 7: System prompt used to guide Gemini-2.0-Thinking-Flask in the Reliability Verification stage.It instructs the model to systematically analyze each review comment and find supporting evidence from the original paper.</p>
<p>Figure 8 :
8
Figure 8: The Meta-Review comment for CycleResearcher from DeepReviewer-14B</p>
<p>all authors, I would like to express our profound gratitude for your and all reviewers' detailed review and thoughtful suggestions!Your professional reviews and insightful suggestions have enhanced the CycleResearcher project comprehensively.Each reviewer has demonstrated admirable academic rigor and forwardthinking, guiding us toward multiple key improvements: from manuscript refinement and experimental optimization to clarity of conclusions and ethical impact considerations.Every comment has helped further improve the CycleResearcher project!Thank you again for your valuable contributions to academic development!We will continue to maintain a rigorous and responsible attitude as we work to advance this field.</p>
<p>Figure 12: The Real-world review comment for CycleResearcher</p>
<p>Table 1 :
1
Dataset Statistics.The table shows the average values of Tokens, Rating, and Accept Rate
DatasetNumber Tokens Rating Accept RateICLR 2024 Train4131104395.3437.8%ICLR 2025 Train9247100625.1331.2%DeepReview-13K13378101785.1833.24%ICLR 2024 Test652106815.4743.7%ICLR 2025 Test634102415.1831.1%DeepReview-Bench1286104645.3337.49%</p>
<p>MSE↓ R. MAE↓ D. Acc.↑ D. F1↑ R. Spearman↑ Pair.R. Acc↑ R. MSE↓ R. MAE↓ D. Acc.↑ D. F1↑ R. Spearman↑ Pair.R. Acc↑
ICLR 2024ICLR 2025MethodModelScoreRankingSelectionScoreRankingSelectionClaude-3-5-sonnet Gemini-2.0-Flash-Thinking R. Agent Review 2.8878 3.1943 DeepSeek-V3 1.94791.2715 1.3418 1.07350.4333 0.4400 0.41050.3937 0.4318 0.34030.1564 -0.0252 0.35420.5526 0.5044 0.60962.8406 2.6186 1.99511.2989 1.2170 1.10170.2826 0.4242 0.31400.2541 0.4242 0.2506-0.0219 0.0968 0.11970.5432 0.5496 0.5702GPT-o14.34141.72940.45000.44240.26210.58814.30721.79170.41670.41570.29910.6318Claude-3-5-sonnet3.44471.50370.47870.45130.03660.53053.09921.35000.55790.4440-0.02190.5169AI ScientistGemini-2.0-Flash-Thinking4.92971.87110.57430.51970.07450.53433.92321.64700.61390.48080.25650.6040DeepSeek-V34.73371.78880.56000.54840.23100.58444.80061.84030.40590.39880.07780.5473DeepSeek-R14.16481.65260.52480.49880.32560.62064.77191.80990.42590.41610.32370.6289CycleReviewer8B 70B2.8911 2.48701.2371 1.25140.6353 0.63040.5528 0.56960.2801 0.33560.5993 0.61602.4461 2.42941.2063 1.21280.6780 0.67820.5586 0.57370.2786 0.26740.5960 0.5928DeepReviewer 14B1.31370.91020.64060.63070.35590.62421.34100.92430.68780.62270.40470.6402</p>
<p>Table 2 :
2
Performance comparison of reviewer models on DeepReview-13k datasets.
Notes:</p>
<p>Table 2
2
MSE↓ S. MAE↓ P. MSE↓ P. MAE↓ C. MSE↓ C. MAE↓ S. Spearman↑ P. Spearman↑ C. Spearman↑ Pair.S. Acc↑ Pair.P. Acc↑ Pair.C. Acc↑ ICLR 2024
ScoreRankingPairwise AccuracyMethod S. AI Scientist Model GPT-o1 0.4589 Claude-3-5-sonnet 0.3052 Gemini-2.0-Flash-Thinking 0.72330.5336 0.4388 0.62240.5483 0.4745 0.52640.5983 0.5504 0.57970.7550 1.1420 0.90360.7147 0.8876 0.74800.1872 0.1692 0.10500.0723 0.0178 0.15610.1103 0.0275 0.02740.5797 0.6017 0.58530.5407 0.5440 0.59290.5621 0.5726 0.5471DeepSeek-V30.88100.77180.76620.71451.69361.14000.22580.31890.15740.60280.62420.5933DeepSeek-R11.05400.86290.53560.57461.95641.29670.16640.29270.30090.60910.63150.6517CycleReviewer8B 70B0.2516 0.23750.3917 0.38970.2356 0.24140.3686 0.37370.2507 0.26570.3941 0.40520.1990 0.23200.3324 0.33730.2593 0.23540.5769 0.58290.6103 0.62300.5923 0.5896DeepReviewer 14B0.15780.30290.18960.32910.21730.36800.32040.37840.33350.61750.63530.6208ICLR 2025GPT-o10.45130.55000.48780.57500.67340.6802-0.0390-0.28370.16710.55410.54260.5966Claude-3-5-Sonnet0.45650.52790.58040.63460.82510.7628-0.0814-0.0790-0.00510.55430.52720.5454AI ScientistGemini-2.0-Flash-Thinking0.42790.52190.63370.61140.56960.58760.35650.05930.27730.65350.54990.6321DeepSeek-V30.79990.74090.91200.76572.01801.25940.19260.0621-0.06770.60140.56830.5315DeepSeek-R10.85750.76360.48840.55862.16201.37500.31300.31330.30600.62890.59890.6268CycleReviewer8B 70B0.2617 0.25880.3931 0.39980.2880 0.25620.4208 0.39980.2667 0.26010.4112 0.40340.2377 0.23200.2498 0.27720.2511 0.19050.5913 0.58650.6074 0.60510.5919 0.5775DeepReviewer 14B0.22390.36500.21780.36620.26320.40950.38100.36980.32390.60570.63800.6222
. Compared with prompt-based baselines, DeepReviewer reduces Rating MSE by an average of 65.83% and improves Decision Accuracy by an average of 15.2%</p>
<p>Table 3 :
3
Performance comparison of reviewer models on fine-grained evaluation dimensions.This table presents the performance across three key assessment aspects: Soundness (S.), Presentation (P.), and Contribution (C.) on ICLR 2024 and 2025 conferences.pointsfrom AI Scientist.When compared to strong finetuned baseline CycleReviewer-70B, DeepReviewer represents reductions of 44.80% for Rating MSE.For the critical accept/reject decision task, DeepReviewer achieves 64.06% decision accuracy and 0.6307 F1 score on ICLR 2024, substantially surpassing all baselines.Notably, DeepReviewer with 14B parameters outperforms significantly larger models including CycleReviewer-70B (70B parameters) and other closed-source LLMs, demonstrating that DeepReviewer provides more reliable paper assessment than other approaches.
BaselinesConstructive ValueAnalytical DepthPlausibilityTechnical AccuracyOverall JudgmentDeepReviewer 14B vs.ICLR 2024AI Scientist GPT-o189.806.6787.676.6751.693.5325.1211.6788.216.63AI Scientist Claude-3.5-Sonnet96.883.1297.922.0880.214.1777.082.0895.744.26AI Scientist Gemini-2.0-Flash-Thinking53.4717.8253.4720.7924.7510.8918.8120.7959.4125.74AI Scientist DeepSeek-V396.041.9899.010.0072.280.9967.334.9596.220.00AI Scientist DeepSeek-R189.227.8474.5113.7345.105.8826.4718.6380.2016.83AgentReview Claude-3.5-Sonnet96.841.0598.940.0090.430.0077.080.0098.900.00AgentReview Gemini-2.0-Flash-Thinking98.001.0095.111.0081.640.0165.003.0096.741.00AgentReview GPT-4o99.020.9999.010.9995.050.9961.764.9098.151.00CycleReviewer 8B97.301.8098.200.9190.920.9187.500.0096.090.91CycleReviewer 70B98.331.1198.890.0192.780.0179.440.0198.331.11ICLR 2025AI Scientist GPT-o191.678.3389.588.3360.424.1737.508.3391.678.33AI Scientist Claude-3.5-Sonnet97.871.06100.000.0092.551.0665.960.0098.941.06AI Scientist Gemini-2.0-Flash-Thinking52.4318.4552.4323.3033.987.7719.4220.3959.4124.75AI Scientist DeepSeek-V396.042.9797.031.9875.252.9763.373.9697.032.97AI Scientist DeepSeek-R189.296.2581.2510.7151.795.3626.7918.7587.399.01AgentReview Claude-3.5-Sonnet95.741.0697.852.1590.322.1574.741.0597.832.17AgentReview Gemini-2.0-Flash-Thinking92.161.9693.083.0078.200.6561.764.9092.164.90AgentReview GPT-4o95.282.0995.371.4092.100.8565.035.4794.152.39CycleReviewer 8B98.451.5598.241.8986.370.7786.362.2798.451.55CycleReviewer 70B96.171.6496.172.1986.341.6472.683.2896.721.64
DeepReviewer achieves the highest Rating Spearman correlations of 0.3559 and 0.4047 on ICLR 2024 and ICLR 2025 respectively, improving upon CycleReviewer-70B by 6.04% and AI Scientist (DeepSeek-R1) by 25.02%.In the paper selection task, It demonstrates superior discrimination ability with pairwise accuracies of 0.62 and 0.64 on ICLR 2024 and ICLR 2025 respectively.Win(%)↑ Lose(%) Win(%)↑ Lose(%) Win(%)↑ Lose(%) Win(%)↑ Lose(%) Win(%)↑ Lose(%)</p>
<p>Table 4 :
4
Direct comparison of DeepReviewer with the baselines on general alignment tasks.Win indicates that Gemini-2.0-Flash-Thinkingassesses DeepReviewer's response as superior compared to the baseline.Cells marked in light gray suggest baseline the winner.</p>
<p>Table 3
3presents a detailed analysis across three critical dimensions: Soundness, Presentation, andContribution. Particularly for Soundness assessment on ICLR 2024, DeepReviewer-14B achievesan MSE of 0.1578 and MAE of 0.3029, representing improvements of 33.58% and 22.09% overCycleReviewer-70B. While DeepReviewer shows marginally lower performance than AI Scientist(Gemini-2.0-Flash-Thinking) in Contribution and Soundness accuracy, it maintains a balanced andstrong performance across all dimensions.</p>
<p>S. Swarnadeep, L. Xian, G. Marjan, W. Jason, and W. Tianlu.Learning to plan &amp; reason for evaluation with thinking-llm-as-a-judge. arXiv preprint arXiv:2501.18099v1,2025.URL https: //www.arxiv.org/abs/2501.18099v1.C. Tan, D. Lyu, S. Li, Z. Gao, J. Wei, S. Ma, Z. Liu, and S. Z. Li.Peer review as a multi-turn and long-context dialogue with role-based interactions.arXiv preprint arXiv:2406.05688,2024a. .Tyser, B. Segev, G. Longhitano, X.-Y.Zhang, Z. Meeks, J. Lee, U. Garg, N. Belsten, A. Shporer, M. Udell, et al.Ai-driven review systems: evaluating llms in scalable and bias-aware academic reviews.arXiv preprint arXiv:2408.10365,2024. .Ye, X. Pang, J. Chai, J. Chen, Z. Yin, Z. Xiang, X. Dong, J. Shao, and S. Chen.Are we there yet?revealing the risks of utilizing large language models in scholarly peer review.arXiv preprint arXiv:2412.01708,2024.J. Yu, Z. Ding, J. Tan, K. Luo, Z. Weng, C. Gong, L. Zeng, R. Cui, C. Han, Q. Sun, et al.Automated peer reviewing in paper sea: Standardization, evaluation, and analysis.arXiv preprint arXiv:2407.12857,2024. .Zhou, L. Chen, and K. Yu.Is LLM a reliable reviewer?a comprehensive evaluation of LLM on automatic paper reviewing tasks.In N. Calzolari, M.-Y.Kan, V. Hoste, A. Lenci, S. Sakti, and N. Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 9340-9351, Torino, Italia, May 2024a.ELRA and ICCL.URL https://aclanthology.org/2024.lrec-main.816/.</p>
<p>C. Tan, D. Lyu, S. Li, Z. Gao, J. Wei, S. Ma, Z. Liu, and S. Z.Li.Peer review as a multi-turn and long-context dialogue with role-based interactions, 2024b.URL https://arxiv.org/abs/2406.05688.H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971,2023.KB.Wang, C. Xu, X. Zhao, L. Ouyang, F. Wu, Z. Zhao, R. Xu, K. Liu, Y. Qu, F. Shang, B. Zhang, L. Wei, Z. Sui, W. Li, B. Shi, Y. Qiao, D. Lin, and C. He.Mineru: An open-source solution for precise document content extraction, 2024a.URL https://arxiv.org/abs/2409.18839.Q. Wang, Q. Zeng, L. Huang, K. Knight, H. Ji, and N. F. Rajani.ReviewRobot: Explainable paper review generation based on knowledge synthesis.In B. Davis, Y. Graham, J. Kelleher, and Y. Sripada, editors, Proceedings of the 13th International Conference on Natural Language Generation, pages 384-397, Dublin, Ireland, Dec. 2020.Association for Computational Linguistics.doi: 10.18653/v1/2020.inlg-1.44.URL https://aclanthology.org/2020.inlg-1.44/.X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou.Selfconsistency improves chain of thought reasoning in language models.In The Eleventh International Conference on Learning Representations, 2023.Y. Wang, Z. Yu, W. Yao, Z. Zeng, L. Yang, C. Wang, H. Chen, C. Jiang, R. Xie, J. Wang, X. Xie, W. Ye, S. Zhang, and Y. Zhang.PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimization.In The Twelfth International Conference on Learning Representations, 2024b.URL https://openreview.net/forum?id=5Nn2BLV7SB.J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al.Chain-ofthought prompting elicits reasoning in large language models.In Advances in Neural Information Processing Systems, 2022.Y. Weng, M. Zhu, F. Xia, B. Li, S. He, S. Liu, B. Sun, K. Liu, and J. Zhao.Large language models are better reasoners with self-verification.In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.Y. Weng, M. Zhu, G. Bao, H. Zhang, J. Wang, Y. Zhang, and L. Yang.Cycleresearcher: Improving automated research via automated review.In The Thirteenth International Conference on Learning Representations, 2025.URL https://openreview.net/forum?id=bjcsVLoHYs.V. Xiang, C. Snell, K. Gandhi, A. Albalak, A. Singh, C. Blagden, D. Phung, R. Rafailov, N. Lile, D. Mahan, L. Castricato, J.-P.Franken, N. Haber, and C. Finn.Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought, 2025.URL https://arxiv.org/abs/2501.04682.Z. Yang, X. Du, J. Li, J. Zheng, S. Poria, and E. Cambria.Large language models for automated open-domain scientific hypotheses discovery.In L.-W.Ku, A. Martins, and V. Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 13545-13565, Bangkok, Thailand, Aug. 2024.Association for Computational Linguistics.doi: 10.18653/v1/ 2024.findings-acl.804.URL https://aclanthology.org/2024.findings-acl.804/.S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan.Tree of thoughts: Deliberate problem solving with large language models.Advances in Neural Information Processing Systems, 36, 2024.RW.Yuan, P. Liu, and G. Neubig.Can we automate scientific reviewing?, 2021.URL https: //arxiv.org/abs/2102.00176.Q. Zeng, M. Sidhu, H. P. Chan, L. Wang, and H. Ji.Scientific opinion summarization: Paper meta-review generation dataset, methods, and evaluation.In 1st AI4Research Workshop, 2024.RR.Zhou, L. Chen, and K. Yu.Is llm a reliable reviewer?a comprehensive evaluation of llm on automatic paper reviewing tasks.In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 9340-9351, 2024b.Z. Zhuang, J. Chen, H. Xu, Y. Jiang, and J. Lin.Large language models for automated scholarly paper review: A survey.arXiv preprint arXiv:2501.10326,2025.Y. Zonglin, D. Xinya, L. Junxian, Z. Jie, P. Soujanya, and C. Erik.Large language models for automated open-domain scientific hypotheses discovery.arXiv preprint arXiv:2309.02726,2023.URL https://www.arxiv.org/abs/2309.02726.D. Zyska, N. Dycke, J. Buchmann, I. Kuznetsov, and I. Gurevych.Care: Collaborative ai-assisted reading environment.arXiv preprint arXiv:2302.12611,2023.</p>
<p>Ranking Task examines LLMs' ability to distinguish paper quality and effectively rank papers within large collections.Given a set of N papers C = C 1 , C 2 , . .., C N , the model first predicts scores R 1 , R 2 , . .., R N for each paper.Subsequently, based on these predicted scores, the model ranks the papers in C, outputting an ordered sequence R = C (1) , C (2) , . .., C (N ) arranged by predicted quality in descending order, where C (i) represents the paper ranked i-th by the model.The Spearman coefficient is used to evaluate ranking accuracy.Selection Task simulates practical scenarios such as peer review or reward model construction, where high-quality papers need to be quickly and accurately identified from a small pool of candidates.For this task, we sample non-overlapping small batches Cbatch = C 1 , C 2 , . . ., C m from the Test dataset, where m is the predetermined batch size.For each batch Cbatch, the model selects what it considers the highest-quality paper C best ∈ C batch .The model's selection is compared against the paper with the highest actual review scores, with accuracy computed as the average success rate across all batch selections.In this study, we set m = 2.And we performed pairwise matching on all papers in the Test dataset to calculate the final Selection score.</p>
<p>You are tasked with improving an academic paper review based solely on: 1.The original review 2. The authors' response (for understanding only, never to be referenced)
OUTPUT FORMAT:{"weaknesses": string, // Enhanced critique maintaining original format"suggestions": string, // 2-3 detailed paragraphs (approximately 500 words total)"citations": [// Only include if citations in original review are paper titlesstring,// Complete title of first cited paper, as [1]string,// Complete title of second cited paper, as [2]...// Additional citations as needed]}CITATION RULES:1. Only include citations array if the original review cites actual paper titles2. If original review's citations are not paper titles, then:-Set citations array to empty []-Do not use any numerical citations ([1], [2], etc.) in weaknesses and suggestions3. When citations are used, maintain consistent numerical format</p>
<p>The CycleResearcher and CycleReviewer framework effectively models the iterative process of research, review, and refinement.The use of SimPO is a notable technical contribution.The introduction of Review-5k and Research-8k datasets is a valuable resource.The CycleResearcher model generates papers with an average quality level close to human-written preprints, achieving a 31.07\%acceptance rate.The CycleReviewer model's 26.89\% improvement in MAE compared to individual reviewers indicates potential for automated research assessment.The inclusion of human evaluation and objective model-based evaluations provides a comprehensive assessment.The paper's exploration of ethical considerations and the implementation of Fast-DetectGPT demonstrates a responsible approach.The authors advocate for disclosure of LLM use in research, which is a positive step towards transparency.</p>
<p>The paper explores the use of open-source large language models to automate the entire research process, from literature review and manuscript preparation to peer review and revision.The proposed framework includes CycleResearcher, which performs research tasks, and CycleReviewer, which simulates the peer review process.The study demonstrates that CycleReviewer can outperform human reviewers in predicting paper scores, and CycleResearcher can generate papers of quality close to human-written preprints.The models are trained using two new datasets, Review-5k and Research-8k, which capture the complexities of peer review and research paper generation.The results indicate that this approach can significantly enhance the efficiency and quality of scientific research, while also providing ethical safeguards to prevent misuse.CycleResearcher and CycleReviewer models to automate the entire research process, including literature review, manuscript preparation, peer review, and revision, is highly innovative.This framework mimics the real-world research cycle, enhancing the efficiency and consistency of scientific inquiry.Performance Improvement: The CycleReviewer model demonstrates a significant improvement in predicting paper scores, outperforming human reviewers by 26.89% in mean absolute error (MAE).This indicates that the model can provide more accurate and consistent evaluations than individual human reviewers.Quality of Generated Papers: The CycleResearcher model generates papers with an average quality close to human-written preprints, achieving an acceptance rate of 31.07%.This shows that the model can produce high-quality research outputs that are competitive with human-generated content.Large-Scale Datasets: The development of the Review-5k and Research-8k datasets, which capture the complexities of peer review and research paper generation, provides valuable resources for training and evaluating models in academic paper generation and review.
−＝Official Review by Reviewer 7LzG08 Nov 2024, 17:23 (modified: 13 Nov 2024, 00:01)EveryoneRevisions (/revisions?id=oehQnbB57h)Summary:Soundness: 2: fairPresentation: 3: goodContribution: 2: fairStrengths:The introduction ofAdd: Public Comment</p>
<p>How do you envision adapting CycleResearcher and CycleReviewer to other scientific fields, such as biology or social sciences, where the nature of research and evaluation criteria might differ significantly?The paper mentions the potential issue of reward hacking, where the policy model might exploit loopholes in the reward model.Could you elaborate on the specific strategies you are considering to mitigate this issue and ensure that the generated research outputs maintain high academic rigor and novelty?Flag For Ethics Review: No ethics review needed.Rating: 6: marginally above the acceptance threshold Confidence: 3: You are fairly confident in your assessment.It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.Math/other details were not carefully checked.
Code Of Conduct: Yes− ＝Response 1Official Commentby Authors ( Guangsheng Bao (/profile?id=~Guangsheng_Bao1), Jindong Wang (/profile?id=~Jindong_Wang4), Minjun Zhu (/profile?id=~Minjun_Zhu2), Yixuan Weng (/profile? id=~Yixuan_Weng1), +3 more (/group/info?id=ICLR.cc/2025/Conference/Submission489/Authors))24 Nov 2024, 22:35EveryoneAdd: Public Comment</p>
<p>Our human expert evaluation, conducted by experienced reviewers (averaging 1,110 Google Scholar citations), demonstrated encouraging results in comparison to baseline systems:
MetricCycleResearcher AI ScientistOverall Score 4.83.6Soundness 2.62.2Presentation 2.82.6Contribution 2.21.8− ＝Response 2Official Commentby Authors ( Guangsheng Bao (/profile?id=~Guangsheng_Bao1), Jindong Wang (/profile?id=~Jindong_Wang4), Minjun Zhu (/profile?id=~Minjun_Zhu2), Yixuan Weng (/profile? id=~Yixuan_Weng1), +3 more (/group/info?id=ICLR.cc/2025/Conference/Submission489/Authors))24 Nov 2024, 22:36EveryoneComment:Question: Could you elaborate on specific strategies to mitigate reward hacking and ensure high academic quality? During the rebuttal period, we conducted additional experiments tofurther investigate the robustness of our framework.Specifically, we trained an independent reward model using only the test set portion of Review-5k on Mistral-Large-2, maintaining complete separation from our CycleReviewer model. Thisnew evaluation framework produced the following results:Model VersionAvg Min Score Avg Max Score Avg Score Accept RateOriginal CycleResearcher-12B 3.526.725.3631.07%With New Reward Model3.386.655.2928.65%Add: Public Comment</p>
<p>Figure9: The Real-world review comment for CycleResearcher The framework is primarily designed for machine learning-related research.How do you envision adapting CycleResearcher and CycleReviewer to other scientific fields, such as biology or social sciences, where the nature of research and evaluation criteria might differ significantly?The paper mentions the potential issue of reward hacking, where the policy model might exploit loopholes in the reward model.Could you elaborate on the specific strategies you are considering to mitigate this issue and ensure that the generated research outputs maintain high academic rigor and novelty?Flag For Ethics Review: No ethics review needed.Rating: 6: marginally above the acceptance threshold Confidence: 3: You are fairly confident in your assessment.It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.Math/other details were not carefully checked.
Questions:Code Of Conduct: YesAdd: Public Comment− ＝Response 1Official Commentby Authors ( Guangsheng Bao (/profile?id=~Guangsheng_Bao1), Jindong Wang (/profile?id=~Jindong_Wang4), Minjun Zhu (/profile?id=~Minjun_Zhu2), Yixuan Weng (/profile? id=~Yixuan_Weng1), +3 more (/group/info?id=ICLR.cc/2025/Conference/Submission489/Authors))24 Nov 2024, 22:35EveryoneAdd: Public Comment</p>
<p>Our human expert evaluation, conducted by experienced reviewers (averaging 1,110 Google Scholar citations), demonstrated encouraging results in comparison to baseline systems: Could you elaborate on specific strategies to mitigate reward hacking and ensure high academic quality?During the rebuttal period, we conducted additional experiments to further investigate the robustness of our framework.Specifically, we trained an independent reward model using only the test set portion of Review-5k on Mistral-Large-2, maintaining complete separation from our CycleReviewer model.This new evaluation framework produced the following results:
MetricCycleResearcher AI ScientistOverall Score 4.83.6Soundness 2.62.2Presentation 2.82.6Contribution 2.21.8− ＝Response 2Official Commentby Authors ( Guangsheng Bao (/profile?id=~Guangsheng_Bao1), Jindong Wang (/profile?id=~Jindong_Wang4), Minjun Zhu (/profile?id=~Minjun_Zhu2), Yixuan Weng (/profile? id=~Yixuan_Weng1), +3 more (/group/info?id=ICLR.cc/2025/Conference/Submission489/Authors))24 Nov 2024, 22:36EveryoneComment:Question: Model VersionAvg Min Score Avg Max Score Avg Score Accept RateOriginal CycleResearcher-12B 3.526.725.3631.07%With New Reward Model3.386.655.2928.65%Add: Public Comment</p>
<p>CycleReviewer produces scores that are closer to averages of multiple human reviewers than scores by individual human reviewers CycleResearcher-12B achieved paper quality scores surpassing preprint level and approaching accepted paper levelThe paper implements some ethical safeguards: they train a model to detect papers generated by LLMs they publish; they promise to implement a licensing agreement such that downloading model weights requires sharing institutional affiliations and agreeing not to use models for official peer reviews or submissions without disclosure.reinforcementlearningonparts of the AI research process is a novel and significant contribution.The paper includes numerous experiments and ablations.The overall methodology is sound (with exceptions, see weaknesses).The authors achieve strong results on the metrics they choose.It is somewhat impressive that their system achieved an acceptance rate of 31.07%,similar to ICLR 2024's acceptance rate.Authors use open-source models with a large range of scale (from 12B to 123B).
2. Creation of two new datasets: Review-5k. Research-8k3. Empirical results showing:Soundness: 2: fairPresentation: 2: fairContribution: 2: fairStrengths:Training LLMs with</p>
<dl>
<dt>Automated evaluation of papers produced by CycleResearcher is hard to trust, since CycleResearcher was trained with RL against the same reward model as used at test-time.Reward model overoptimization(Gao et al, 2022-https://arxiv.org/abs/2210.10760(https://arxiv.org/abs/2210.10760))should be the expected result of RL, however the authors do not run any experiments to investigate to which extent their evaluation is influenced by this.For example, the authors could train a held-out reward model on a held-out dataset of reviews and then evaluate CycleResearcher on both the reward model used for RL training and this new held-out reward model.The claim that CycleResearcher surpasses the quality of preprint papers and approaches quality of accepted papers is not well supported, due to the concerns about reward model overoptimization mentioned above.Human reviewers rate CycleResearcher's papers significantly lower (4.8)than the automated reviewer made by the authors (5.36).The authors could have reported the actual historical average score of ICLR2024 accepted papers.</dt>
<dd>
<p>6: marginally above the acceptance threshold Confidence: 4: You are confident in your assessment, but not absolutely certain.It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes− ＝Response 1Official Commentby Authors ( Guangsheng Bao (/profile?id=~Guangsheng_Bao1), Jindong Wang (/profile?id=~Jindong_Wang4), Minjun Zhu (/profile?id=~Minjun_Zhu2), Yixuan Weng (/profile? id=~Yixuan_Weng1), +3 more (/group/info?id=ICLR.cc/2025/Conference/Submission489/Authors))24 Nov 2024, 22:50 (modified: 25 Nov 2024, 04:12)EveryoneRevisions (/revisions?id=MN6QPlHSaT)Comment:</p>
</dd>
</dl>
<p>Our vision is to develop an AI research assistant that can read literature extensively, formulate scientific hypotheses, design validation experiments, and generate research papers based on actual experimental results.During the rebuttal period, we've made new progress by implementing a complete automated scientific discovery pipeline.In our updated Appendix G, we demonstrate how CycleResearcher designs experiments that are then executed by GPT-o1-preview, generating concrete code and logs.The results are real and verifiable -we've included the full codebase, experiment logs, and detailed tutorials in our supplementary materials.
≡CycleReviewer simulates a complete review process with multiple reviewers discussing strengths/weaknesses, followed by an AC's final decision (Accept/Reject).Q5: Human evaluation details?Rejection sampling: N=100All three experts reviewed all papersExperts (avg 1100 citations) were invited via email for blind review"Excluding formatting" means ignoring layout issues (table/figure sizing) to avoid bias from technical formatting limitationsAdd: Public CommentFigure 11: The Real-world review comment for CycleResearcher≡ferrer=%5BAuthor Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions)</p>
<dl>
<dt>As the discussion period is coming to an end soon, we wanted to check if you have had a chance to review our responses.Please let us know if your questions have been adequately addressed -we are happy to provide any additional clarification needed.Thank you for your time!</dt>
<dt>− ＝Official Comment by AuthorsOfficial Commentby Authors ( Guangsheng Bao (/profile?id=~Guangsheng_Bao1), Jindong Wang (/profile?id=~Jindong_Wang4), Minjun Zhu (/profile?id=~Minjun_Zhu2), Yixuan Weng (/profile? id=~Yixuan_Weng1), +3 more (/group/info?id=ICLR.cc/2025/Conference/Submission489/Authors))30 Nov 2024, 19:48EveryoneComment:Dear Reviewer GAvj,Best Regards,Authors of "CycleResearcher: Improving Automated Research via Automated Review"Add: Public Comment− ＝ Replying to Official Comment by AuthorsOfficial Comment by Reviewer GAvjOfficial Comment by Reviewer GAvj03 Dec 2024, 04:33EveryoneComment:Thanks, I've raised my score.Add: Public CommentReplying to Official Comment by Reviewer GAvjThank you for your recognition!Official Commentby Authors ( Guangsheng Bao (/profile?id=~Guangsheng_Bao1), Jindong Wang (/profile?id=~Jindong_Wang4), Minjun Zhu (/profile?id=~Minjun_Zhu2), Yixuan Weng (/profile?id=~Yixuan_Weng1), +3 more (/group/info?id=ICLR.cc/2025/Conference/Submission489/Authors))03 Dec 2024, 05:30EveryoneAdd: Public Comment</dt>
<dd>
<p>8: accept, good paper Confidence: 5: You are absolutely certain about your assessment.You are very familiar with the related work and checked the math/other details carefully.
Code Of Conduct: YesResponse 1
Official Comment by Authors ( Guangsheng Bao (/profile?id=~Guangsheng_Bao1),Jindong Wang (/profile?id=~Jindong_Wang4),Minjun Zhu (/profile?id=~Minjun_Zhu2),Yixuan Weng (/profile?id=~Yixuan_Weng1), +3 more (/group/info?id=ICLR.cc/2025/Conference/Submission489/Authors))24 Nov 2024, 22:54 Everyone − ＝    Comment:</p>
</dd>
</dl>
<p>Empty PDFs were filtered during conversion
https://huggingface.co/OpenSciLM/OpenScholar_Reranker
https://huggingface.co/OpenSciLM/Llama-3.1_OpenScholar-8B
https://openreview.net/forum?id=bjcsVLoHYs</p>
<p>M Abdin, J Aneja, H Behl, S Bubeck, R Eldan, S Gunasekar, M Harrison, R J Hewett, M Javaheripi, P Kauffmann, J R Lee, Y T Lee, Y Li, W Liu, C C T Mendes, A Nguyen, E Price, G De Rosa, O Saarikivi, A Salim, S Shah, X Wang, R Ward, Y Wu, D Yu, C Zhang, Y Zhang, Phi-4 technical report. 2024</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Aider is ai pair programming in your terminal. A Ai, 2025</p>
<p>What learning algorithm is in-context learning?. E Akyürek, D Schuurmans, J Andreas, T Ma, D Zhou, arXiv:2211.156612022arXiv preprintinvestigations with linear models</p>
<p>Reviewing peer review. B Alberts, B Hanson, K L Kelner, 2008</p>
<p>Openscholar: Synthesizing scientific literature with retrieval-augmented lms. A Asai, J He, R Shao, W Shi, A Singh, J C Chang, K Lo, L Soldaini, S Feldman, M , D Wadden, M Latzke, M Tian, P Ji, S Liu, H Tong, B Wu, Y Xiong, L Zettlemoyer, G Neubig, D Weld, D Downey, W Yih, P W Koh, H Hajishirzi, 2024</p>
<p>. J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>I Blog, Iclr 2025: Assisting reviewers. 2024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. L Chris, L Cong, L Robert, F Tjarko, C Jakob, H Jeff, David, arXiv:2408.06292v32024arXiv preprint</p>
<p>Emergent autonomous scientific research capabilities of large language models. B Daniil, A , M Robert, G Gabe, arXiv:2304.05332v12023arXiv preprint</p>
<p>Marg: Multi-agent review generation for scientific papers. M D'arcy, T Hope, L Birnbaum, D Downey, arXiv:2401.042592024arXiv preprint</p>
<p>LongroPE: Extending LLM context window beyond 2 million tokens. Y Ding, L L Zhang, C Zhang, Y Xu, N Shang, J Xu, F Yang, M Yang, Forty-first International Conference on Machine Learning. 2024</p>
<p>Human-in-the-loop ai reviewing: Feasibility, opportunities, and risks. I Drori, D Te'eni, Journal of the Association for Information Systems. 2512024</p>
<p>LLMs assist NLP researchers: Critique paper (meta-)reviewing. J Du, Y Wang, W Zhao, Z Deng, S Liu, R Lou, H P Zou, P Narayanan, N Venkit, M Zhang, H R Srinath, V Zhang, Y Gupta, T Li, F Li, Q Wang, T Liu, P Liu, C Gao, C Xia, C Xing, Z Jiayang, Y Wang, R S Su, R Shah, J Guo, H Gu, K Li, Z Wei, L Wang, S Cheng, M Ranathunga, J Fang, F Fu, R Liu, E Huang, Y Blanco, R Cao, P S Zhang, W Yu, Yin, doi: 10.18653Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNov. 2024</p>
<p>Citebench: A benchmark for scientific citation text generation. M Funkquist, I Kuznetsov, Y Hou, I Gurevych, arXiv:2212.095772022arXiv preprint</p>
<p>Reviewer2: Optimizing review generation through prompt generation. Z Gao, K Brantley, T Joachims, arXiv:2402.108862024arXiv preprint</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. A Ghafarollahi, M J Buehler, arXiv:2409.055562024arXiv preprint</p>
<p>rstar-math: Small llms can master math reasoning with self-evolved deep thinking. X Guan, L L Zhang, Y Liu, N Shang, Y Sun, Y Zhu, F Yang, M Yang, 2025</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Training large language models to reason in a continuous latent space. S Hao, S Sukhbaatar, D Su, X Li, Z Hu, J Weston, Y Tian, 2024</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, 2021NeurIPS</p>
<p>X Hu, H Fu, J Wang, Y Wang, Z Li, R Xu, Y Lu, Y Jin, L Pan, Z Lan, arXiv:2410.14255Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. 2024arXiv preprint</p>
<p>A Jaech, A Kalai, A Lerer, A Richardson, A El-Kishky, A Low, A Helyar, A Madry, A Beutel, A Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Towards mitigating LLM hallucination via self reflection. Z Ji, T Yu, Y Xu, N Lee, E Ishii, P Fung, 10.18653/v1/2023.findings-emnlp.123Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>AgentReview: Exploring peer review dynamics with LLM agents. Y Jin, Q Zhao, Y Wang, H Chen, K Zhu, Y Xiao, J Wang, 10.18653/v1/2024.emnlp-main.70Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNov. 2024a</p>
<p>Y Jin, Q Zhao, Y Wang, H Chen, K Zhu, Y Xiao, J Wang, arXiv:2406.12708Agentreview: Exploring peer review dynamics with llm agents. 2024barXiv preprint</p>
<p>D Kang, W Ammar, B Dalvi, M Van Zuylen, S Kohlmeier, E Hovy, R Schwartz, arXiv:1804.09635A dataset of peer reviews (peerread): Collection, insights and nlp applications. 2018arXiv preprint</p>
<p>Scientific discovery: Computational explorations of the creative processes. P Langley, 1987MIT press</p>
<p>R Latona, M H Ribeiro, T R Davidson, V Veselovsky, R West, arXiv:2405.02150The ai review lottery: Widespread ai-assisted peer reviews boost paper scores and acceptance rates. 2024arXiv preprint</p>
<p>D Li, B Jiang, L Huang, A Beigi, C Zhao, Z Tan, A Bhattacharjee, Y Jiang, C Chen, T Wu, K Shu, L Cheng, H Liu, arXiv:2411.16594From generation to judgment: Opportunities and challenges of llm-as-a-judge. 2024aarXiv preprint</p>
<p>Summarizing multiple documents with conversational structure for meta-review generation. M Li, E Hovy, J H Lau, arXiv:2305.014982023arXiv preprint</p>
<p>Automated statistical model discovery with language models. M Y Li, E Fox, N Goodman, Forty-first International Conference on Machine Learning. 2024b</p>
<p>Simulating expert discussions with multi-agent for enhanced scientific problem solving. Z Li, Y Chang, X Le, T Ghosal, A Singh, A Waard, P Mayr, A Naik, O Weller, Y Lee, S Shen, Y Qin, Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024). the Fourth Workshop on Scholarly Document Processing (SDP 2024)Bangkok, ThailandAssociation for Computational LinguisticsAug. 2024c</p>
<p>Monitoring ai-modified content at scale: A case study on the impact of chatgpt on ai conference peer reviews. W Liang, Z Izzo, Y Zhang, H Lepp, H Cao, X Zhao, L Chen, H Ye, S Liu, Z Huang, arXiv:2403.071832024arXiv preprint</p>
<p>ROUGE: A package for automatic evaluation of summaries. C.-Y Lin, Association for Computational Linguistics. Barcelona, SpainJuly 2004Text Summarization Branches Out</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Show your work: Scratchpads for intermediate computation with language models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, arXiv:2112.001142021arXiv preprint</p>
<dl>
<dt>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ACL '02USAAssociation for Computational Linguistics2002</dt>
<dd>
<p>Qwen, A Yang, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Li, D Liu, F Huang, H Wei, H Lin, J Yang, J Tu, J Zhang, J Yang, J Yang, J Zhou, J Lin, K Dang, K Lu, K Bao, K Yang, L Yu, M Li, M Xue, P Zhang, Q Zhu, R Men, R Lin, T Li, T Tang, T Xia, X Ren, X Ren, Y Fan, Y Su, Y Zhang, Y Wan, Y Liu, Z Cui, Z Zhang, Z Qiu, Qwen2.5 technical report. 2025</p>
</dd>
</dl>
<p>Zero: Memory optimizations toward training trillion parameter models. S Rajbhandari, J Rasley, O Ruwase, Y He, 2020</p>
<p>Navigating complexity: Orchestrated problem solving with multi-agent llms. S Rasal, E Hauer, arXiv:2402.167132024arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. J Rasley, S Rajbhandari, O Ruwase, Y He, 10.1145/3394486.3406703Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Potential and perils of large language models as judges of unstructured textual data. B Rewina, P Natalie, B Sreyoshi, K Satya, G Alex, C Elizabeth, I Ikkei, T David, C Aman, N Naumaan, arXiv:2501.08167v22025arXiv preprint</p>
<p>Prompting llms to compose meta-review drafts from peer-review narratives of scholarly manuscripts. S K K Santu, S K Sinha, N Bansal, A Knipper, S Sarkar, J Salvador, Y Mahajan, S Guttikonda, M Akter, M Freestone, arXiv:2402.155892024arXiv preprint</p>
<p>The emergence of large language models (llm) as a tool in literature reviews: an llm automated systematic review. D Scherbakov, N Hubig, V Jansari, A Bakumenko, L A Lenert, 2024</p>
<p>A critical examination of the ethics of ai-mediated peer review. L A Schintler, C L Mcneely, J Witte, 2023</p>
<p>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers. C Si, D Yang, T Hashimoto, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. H Su, R Chen, S Tang, X Zheng, J Li, Z Yin, W Ouyang, N Dong, arXiv:2410.094032024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
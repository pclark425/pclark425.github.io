<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2041 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2041</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2041</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-277501795</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.01459v1.pdf" target="_blank">Probabilistic Curriculum Learning for Goal-Based Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) -- algorithms that teach artificial agents to interact with environments by maximising reward signals -- has achieved significant success in recent years. These successes have been facilitated by advances in algorithms (e.g., deep Q-learning, deep deterministic policy gradients, proximal policy optimisation, trust region policy optimisation, and soft actor-critic) and specialised computational resources such as GPUs and TPUs. One promising research direction involves introducing goals to allow multimodal policies, commonly through hierarchical or curriculum reinforcement learning. These methods systematically decompose complex behaviours into simpler sub-tasks, analogous to how humans progressively learn skills (e.g. we learn to run before we walk, or we learn arithmetic before calculus). However, fully automating goal creation remains an open challenge. We present a novel probabilistic curriculum learning algorithm to suggest goals for reinforcement learning agents in continuous control and navigation tasks.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2041",
    "paper_id": "paper-277501795",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0058674999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Probabilistic Curriculum Learning for Goal-Based Reinforcement Learning
2 Apr 2025</p>
<p>Llewyn Salt llewyn.salt@gmail.com 
Electrical Engineering and Computer Science
University of Queensland Brisbane
Australia</p>
<p>Marcus Gallagher marcusg@eecs.uq.edu.au 
Electrical Engineering and Computer Science
University of Queensland Brisbane
Australia</p>
<p>Probabilistic Curriculum Learning for Goal-Based Reinforcement Learning
2 Apr 2025F07FE1A07E5788B63A86C380705054D9arXiv:2504.01459v1[cs.LG]
Reinforcement learning (RL) -algorithms that teach artificial agents to interact with environments by maximising reward signals -has achieved significant success in recent years.These successes have been facilitated by advances in algorithms (e.g., deep Q-learning, deep deterministic policy gradients, proximal policy optimisation, trust region policy optimisation, and soft actor-critic) and specialised computational resources such as GPUs and TPUs.One promising research direction involves introducing goals to allow multimodal policies, commonly through hierarchical or curriculum reinforcement learning.These methods systematically decompose complex behaviours into simpler sub-tasks, analogous to how humans progressively learn skills (e.g.we learn to run before we walk, or we learn arithmetic before calculus).However, fully automating goal creation remains an open challenge.We present a novel probabilistic curriculum learning algorithm to suggest goals for reinforcement learning agents in continuous control and navigation tasks.Preprint.Under review.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) -algorithms that teach artificial agents to interact optimally with their environments by maximising reward signals [1] -has achieved remarkable success in recent years, notably in discrete and zero-sum games such as Atari [2], Go [3], and Starcraft [4].These achievements have been driven primarily by novel deep RL algorithms, including deep Q-learning [2], deep deterministic policy gradients (DDPG) [5], proximal policy optimisation (PPO) [6], trust region policy optimisation (TRPO) [7], and soft actor-critic (SAC) [8], combined with advances in computing hardware like GPUs and TPUs [9].However, translating these successes to continuous environments remains challenging.In physical systems, unlike games that often have singular, explicit objectives (e.g., maximising score), RL must frequently address more diverse, nuanced goals-such as positioning a robot or controlling a lift precisely.Goal-based reinforcement learning aligns RL closely with optimal control [10], enabling agents to learn a variety of behaviours simultaneously, crucial for applications in autonomous vehicles, robotics, and flexible game AI [11,12,13,12].Yet, this broader goal space significantly complicates learning, especially in continuous environments, demanding more efficient strategies [14].</p>
<p>Curriculum learning, which sequences tasks from simple to complex, offers one promising approach to efficiently mastering complex, multi-goal scenarios [15,16,17,18,19].Still, existing approaches to automatically generating curricula face notable limitations: some require restrictive goal initialisations [20], while others rely on constrained Gaussian distributions to ensure training stability [14].Further, evaluations often narrowly focus on singular target goals, contradicting the multi-goal paradigm's spirit.</p>
<p>To address these limitations, we propose a novel curriculum learning algorithm that explicitly models task difficulty using a probabilistic approach, enabling dynamic selection of goals that are neither trivial nor prohibitively challenging.By predicting an agent's likelihood of successfully achieving goals, our approach filters goals to a difficulty level suitable for efficient policy learning, whilst not requiring restrictive goal initialisation or constraining the probability distribution.We evaluate our algorithm in continuous control and navigation tasks, comparing performance to a baseline uniform curriculum.Our experiments specifically investigate whether probabilistically-driven goal selection leads to improved learning efficiency, generalisation across multiple goals, and improved performance in longer time horizon tasks.</p>
<p>Background</p>
<p>Reinforcement Learning</p>
<p>RL involves learning optimal sequences of actions in interactive environments modelled by Markov decision processes (MDPs) [1].At each step, an agent observes state s t , performs action a t , and receives reward r t , with transitions governed by a probability function T (s t , a t , s t+1 ).RL's objective is to learn a policy π that maximises cumulative discounted reward R t = T i=t γ i−t r i .Optimal policies are characterised by their action-value functions Q π (s t , a t ) = E[R t |s t , a t ], satisfying the Bellman optimality equation:
Q π (s t , a t ) = E st+1 <a href="1">r t+1 + γQ π (s t+1 , π(s t+1 ))</a></p>
<p>Goal-Based Reinforcement Learning</p>
<p>Goal-based RL generalises traditional RL by conditioning policies π(s t , g t ) and value functions Q(s t , a t , g t ) explicitly on goals g, enabling agents to achieve diverse outcomes without extensive reward shaping [21,22].Goals are sampled as subsets of the state space, guiding policies across diverse tasks and facilitating learning in sparse-reward environments.Universal value function approximators (UVFAs) extend the Q-function to explicitly incorporate goals, thereby generalising the action-value function to multiple goal-conditioned reward functions r g (s t , a t , s t+1 ).This approach allows agents to flexibly adapt their policy according to the desired goals, significantly improving generalisation capabilities.</p>
<p>Curriculum Learning</p>
<p>Curriculum learning sequences tasks in increasing complexity, mirroring the structured progression humans naturally use when acquiring new skills [15,23,24,25].Although handcrafted curricula have shown efficacy in reinforcement learning (RL), manual curriculum design is typically tedious, subjective, and limited to specific scenarios [26].Consequently, recent efforts have focused on automated curriculum generation methods [27,28,29].</p>
<p>Goal GAN [30] exemplifies one such automated approach, generating intermediate-difficulty goals, yet it requires careful goal initialisation and suffers from GAN-related training instabilities [31,32,33].Similarly, Self-Paced Learning (SPL) methods, such as Klink et al.'s probabilistic approach [14], adaptively adjust task complexity but rely on narrow Gaussian context distributions to maintain stability, limiting flexibility and generalisation across broader goal spaces.Additionally, these methods typically evaluate performance against a single fixed goal, whereas in true multi-goal RL, the agent sequentially adapts to different goals-each episode or upon achieving the current target-further complicating training.</p>
<p>To address these limitations, we propose a novel probabilistic curriculum learning approach leveraging stochastic variational inference (SVI) to dynamically estimate task difficulty, facilitating stable and flexible goal selection suitable for scalable multi-goal reinforcement learning.</p>
<p>Stochastic Variational Inference</p>
<p>Stochastic Variational Inference (SVI) has emerged as a powerful technique for approximate Bayesian inference, particularly in scenarios where exact inference becomes computationally prohibitive due to model complexity or dataset size [34,35].By transforming Bayesian inference into an optimisation problem, SVI approximates the posterior distribution using a simpler, variational distribution.Unlike traditional variational inference, SVI leverages stochastic optimisation by iteratively updating parameters based on gradients computed from random subsets (mini-batches) of data, significantly enhancing scalability and computational efficiency [36,37].This efficiency has led to broad applicability across diverse fields, including probabilistic machine learning, deep generative models such as variational autoencoders, and large-scale natural language processing tasks [38,39].</p>
<p>Probabilistic Curriculum Learning</p>
<p>We present probabilistic curriculum learning (PCL) as a novel method to suggest goals for reinforcement learning agents in continuous control and navigation tasks.</p>
<p>Formalisation of the Problem</p>
<p>First we must clearly define some of the mathematical assumptions and definitions that we will use to formalise the algorithm.</p>
<p>Linking Goals and States</p>
<p>We assume that there exists some function f : S → G such that we can then calculate the reward of reaching the desired goal as:
r g (s t+1 , g t ) = 1, if D(f (s t1 ), g t ) &lt; ϵ 0, otherwise(2)
where D is some distance function like the Euclidean distance and ϵ &lt;&lt; 1 is some error margin as the probability of f (s t ) being exactly equal to g t in continuous space is zero.In our experiments
G ∈ R N is a subset of S ∈ R M where N ≤ M so f (s t ) = s t × I M ×N R
where I R is a row reduced identity matrix and s t is an M × 1 vector.If S = G the f (s t ) = I× = s t , but if g t only specifies some of the properties of s, e.g.suppose that robots state is specified by x, y, z, but we desire that the robot should be able to reach any arbitrary x, z irrespective of y then f ((x, y, z)) = 1 0 0 0 0 1
x y z = x z this
is the simplest way to model goals.Goals could potentially be a combination of states e.g a specific policy or something that is as yet unthought of, but is outside the scope of this paper.</p>
<p>Probability Density Estimation for Goal Selection</p>
<p>We propose a novel technique whereby the probability that a goal is successful (g s t ) given the current policy, π: P (g s t |π) is estimated.Stochastic variational inference techniques can be used to learn the probability density function p(g s t |π).p(g s t |π) can be estimated using any probability density estimator.However, as the policy π is typically approximated using a function approximator, such as a neural network, we choose to characterise it through the state action pairs it experiences, (s t , a t ), where a ∼ π(s t ).</p>
<p>We can estimate the pdf using any probability density estimator.We know that g s t ∈ G and G ⊂ S, so we can say that a successful goal given a state action pair is characterised by any subsequent state, s t+N , within the same epoch.Therefore, we can assert that p(g
s t |π) ≈ p(g s t |s t , a t ) ≈ p(s t+N |s t , a t )(3)
We can now sample candidate goals from the above distribution, g c t ∼ p(s t+1 |s t , a t ).However, as we have some probability of sampling goals that have a low probability of success we want to evaluate P (g c t |s t , a t ), as these are continuous random variables there is a zero probability that g c t is exactly equal to a successful goal.We could assume some volume and calculate the goal probability as per Appendix A.3.For example if the model as a Gaussian mixture model then we can use Equation 4:
P (g c t ∈ C|s t , a t ) = K j=1 ϕ j 1 (2π) N |Σ j | N i=1 √ π Σ jii √ 2 (erf ( g ci + ϵ − µ ji √ 2 Σ jii )−erf ( g ci − ϵ − µ ji √ 2 Σ jii ))(4)
Where the volume is a hypercube C centred about g c with each dimension offset on either side of g c by ϵ &gt; 0, and we also assume all Σ j are diagonal, and the goal dimensions are independent, see Appendix A.4 for proof.The downside of the above is that the computation is limited to the Gaussian distributions with a specific volume to integrate over, and depending on the dimensionality of C it can be computationally expensive.Instead, we can look to using computational methods to estimate the probabilities to broaden the set of usable distributions.</p>
<p>For a generic probability distribution we could utilise Monte Carlo Integration to estimate the probability of a goal being successful [40,41].However, this is computationally expensive.</p>
<p>Instead, we decide to bound our pdf values by some quantile's upper and lower, we can then select from within this range either randomly or using some heuristic like min or max depending on how we set our bounds.</p>
<p>Mixture Density Network</p>
<p>We use a mixture density network (MDN) to learn ϕ j , µ j , and Σ j and generate a Gaussian mixture model [42,43].Figure 2 shows the network architecture of the Deep MDN used in our experiments, the hidden layer block uses a combination of non-linearity, batch-normalisation [44], and dropout [45].</p>
<p>The only fixed part of this topology is the final layer which constrains the outputs to the ranges required for a Gaussian mixture model.The network could be changed to output the parameters of any distribution.The output shape of: ϕ j is the number of mixtures K, and µ j and Σ j is K times the number of variables characterised by the goal.Stochastic Gradient Descent(SGD) is then used to minimise the negative log-likelihood loss given the mixture parameters:
L(s t+1 |s t , a t ) = − 1 N N i=1 log(p(g t |s t , a t ))(5)
where N is the minibatch size and s t , a t , s t+1 are sampled from the agents experience.s t+1 is converted into the goal g t as described in Section 3.1.1.</p>
<p>To prevent overfitting and modal collapse, we can also introduce some additional terms to the loss function.We have modified the loss function to be the sum of ELBO, L2-regularisation, and KL divergence which are useful in preventing modal collapse and overfitting:
L Θ (s t+1 |s t , a t ) = − λ 1 N N i=1 log(p(g|s t , a t )) + λ 2 ||Θ|| 2 + λ 3 D KL (q(s t+1 )||p Θ (g|s t , a t )) (6)
where Θ is represents the parameters of the deep MDN network, and λ 1 , λ 2 , and λ 3 are hyperparameters that control the strength of the ELBO loss, regularisation, and KL divergence terms.The regularisation term is useful in preventing overfitting and the KL divergence term is useful in preventing modal collapse.</p>
<p>Algorithm</p>
<p>Our goal sampling technique is straight forward, at the beginning of an episode we use the sample a set of goals G from some distribution D, which could be the probabilistic model, a uniform distribution, Algorithm 1 Goal Generation using a probabilistic model for Reinforcement Learning  Calculate p Θ (G|s t , a t ) Calculate the reward r t = r g (s t+1 , g)
13: Pick g ∼ f (G[Q lower &lt; G &lt; Q upper ])19:
Store experience (s t , a t , r t , s t+1 , g)
20:
Update agent π using its update rule 21:</p>
<p>Update probabilistic model by sampling a minibatch from R using eq.6 and SGD</p>
<p>22:</p>
<p>end while 23: end for or something else.We then select the goal randomly from the set given by the probability density values that sit between a lower and upper quantile, i.e.Q lower (p(x|y)) ≤ p(x|y) ≤ Q upper (p(x|y)) where x is randomly sampled from the probabilistic model.After each step in the environment we store s t , a t , s t+1 , g t in a replay buffer.At each step we sample from the replay buffer and perform gradient descent on our probabilistic model to minimise the loss.At the end of each episode we select a new goal and repeat.Our algorithm is formally laid out in Algorithm 1. f (.) is the selection strategy, which can be any method by which we select the goals once they have been filtered by the quantiles.Selection strategies and automatic quantile adjustment are discussed below.</p>
<p>Goal Selection Strategies</p>
<p>We develop the following candidates for the selection strategy: uniform We can select the goal randomly utilising a uniform distribution, which is the simplest and is the selection strategy used in experiments unless otherwise stated i.e. f (.) ∼ Uniform(1, N ).</p>
<p>Weighted selection using the normalised pdf values so they sum to 1 which informs a weighted random selection.Given a set of goals g = {g 1 , g 2 , . . ., g N } and their corresponding probabilities density values p = {p 1 , p 2 , . . ., p N }, the probabilities are normalised to ensure they sum to 1, p i = pi N j=1 pj .If the normalisation sum is numerically close to zero, indicating degenerate probabilities, then the selected goal is chosen uniformly at random from the set of goals as per the uniform strategy.Thus, the goal-selection process is formally defined as:
g * =    g i , with probability p i = pi N j=1 pj , if N j=1 p j ≥ 10 −12 g i , i ∼ Uniform(1, N ), otherwise(7)
multiweighted selection strategy that considers multiple criteria: Given a set of goals g and their corresponding probabilities density values p as above, the combined score p i for each goal g i is computed as follows:
S i = β 1 • U (g i ) + β 2 • LP (g i ) + β 3 • N (g i )(8)
Where U (g i ) is the Uncertainty of goal g i , LP (g i ) is the Learning Progress of goal g i , and N (g i ) is the Novelty of goal g i .The terms are defined as follows:
U (g i ) = p i • (1 − p i ) (9) LP (g i ) = |p i − old_prob(g i )| (10) N (g i ) = min gj ∈known_goals ∥g i − g j ∥(11)
β 1 , β 2 , β 3 are the weights for Uncertainty, Learning Progress, and Novelty, respectively.The probabilities for sampling each goal are then normalised to sum to 1 as per the weighted strategy and the goal is sampled based on the distribution utilising the same Equation 7</p>
<p>Adaptive Quantiles</p>
<p>The quantiles, Q lower and Q upper , are adapted according to some success criteria.We collect a short-term memory of the past N goals and calculate the success rate, SR, the streak of successes, s, and a correction factor, cf .The short term memory, a = {f (g 1 ), f (g 2 ), . . ., f (g N )}, contains a collection of binary values where:
f (g) = 1 if goal is reached 0 otherwise (12) sr = N i=1 a i N where a i ∈ a(13)s = max{k | f (g N −k+1 ) = f (g N −k+2 ) = • • • = f (g N ), 1 ≤ k ≤ N } (14) cf = (1 − |sr − sr target |) × α s(15)
Goal is reached is usually defined as the agent being within some tolerance of the goal which is typically defined by the environment.α is the factor by which we want the streak to exponentially impact the quantiles and sr target is the target success rate.The quantiles are then updated as follows:
Q x = min(min Qx , Q x − λ × cf ) if a N = 1 max(max Qx , Q x + λ × cf ) if a N = 0(16)
where min Qx and max Qx are the minimum and maximum values of the quantile, the x from Q x is either upper or lower, λ is the learning rate, and a N is the last element of the memory.</p>
<p>Methodology</p>
<p>All experiments were conducted using the AlgOS framework [46].SAC from Stable Baselines 3 [47] is used as the agent to test the efficacy of the PCL in a DC Motor control environment, and a point robot navigation task [48].Both tasks are continuous control, with the DC Motor offering a single-input-single-output (SISO) control problem with no obstacles and the point robot navigation task offering a multi-input-multi-output (MIMO) control problem with obstacles.</p>
<p>AlgOS provides an optimisation interface via Optuna [49], which allows us to tune the numerous hyperparameters of both PCL and SAC using a tree parzen estimator [50], a form of Bayesian optimisation requiring fewer samples [51].The bounds of the optimisation can be found in Table 1 in Appendix A.1, which many of SAC's values were determined from Raffin et al.'s paper [52].Additionally, due to compute resourcing constraints, we optimise over 150000 steps for all environments which is a relatively small number when compared to the number of steps used in the literature.However, we are not attempting to achieve maximum performance but rather explore where and how Algorithm 1 improves or diminishes the agent's capability to learn.</p>
<p>We will compare using an MDN probabilistic sampler trained to model p(s t+N |s t , a t ) and the uniform sampler samples goals from the goal space as:</p>
<ol>
<li>DC Motor: x ∼ U (a, b), where a and b are the minimum and maximum angular velocities of the motor.</li>
</ol>
<dl>
<dt>Point Maze</dt>
<dd>x ∼ N i=1 1 N • U (l i , h i )
where N is the number of goals, l i and h i ) are the upper and lower bounds of each cell in which the goal resides (2D vectors).</dd>
</dl>
<p>The goal is reached when: DC Motor: the agent stays within a tolerance 0.001 of the goal for 10 steps; Point Maze: the agent gets within 0.45 of the goal.The algorithm's performance is measured using coverage, which is the percentage of the goals the agent can reach when evaluated.At evaluation the agent is tasked to reach a set of goals, G = g 1 , ..., g N , four times.This ensures the agent can reach the same goal multiple times.Coverage is used as the objective metric for the hyperparameter optimisation.</p>
<p>We will present the best three runs from each set of hyperparameter optimisations.We will also explore the effect of the adaptive quantile and sampling strategies.Unless stated otherwise, the uniform strategy with static quantiles is used.Figure 3 shows the results of the DC Motor experiments, see Tables 2 and 3 for hyperparameters.</p>
<p>Results and Discussion</p>
<p>The coverage is shown in the top row and the distribution of goals is shown in the bottom row.Both the uniform and PCL methods are able to achieve a coverage of 0.9, but the PCL is able to on all three runs.The distribution of goals for both the uniform and PCL methods is similar.It was assumed that for a simple problem like the DC motor the optimal curriculum would be a bimodal Gaussian distribution centred about ±0.5 as close to 0 would be easy and ±1 would be hard.Figures 3c and 3d show that the best performers have similar distributions.</p>
<p>Figures 3a and 3b show that the PCL method has non-zero coverage prior to 90,000 steps whereas the uniform method only increases after 90,000 steps, indicating that the PCL method increases training efficiency.</p>
<p>Point Maze</p>
<p>Figures 4a and 5a shows the two mazes that we use to evaluate PCL and the uniform method.S shows the start locations, G shows the goal locations, and W shows the walls.</p>
<p>The maze in Figure 4a assesses the curriculum's capacity to navigate indirectly to the goal over a long horizon, and the maze in Figure 5a assesses the curriculum's ability to reach many goals.</p>
<p>Figure 4 shows the coverage of the PCL and uniform curricula in the bidirectional maze, see Tables 4  and 5 for hyperparameters.PCL is able to achieve a coverage of 1 in one run and 0.5 in the other two.The uniform curriculum is able to achieve 0.5 in one run and 0.25 in the other two.Both curricula were optimised for forty runs, demonstrating that the PCL is able to achieve a higher coverage than the uniform method.This indicates that the PCL is able to learn more efficiently and has increased performance on longer time horizon tasks.We can see that the PCL curriculum is able to achieve 0.272 coverage whereas the uniform curriculum achieves 0.188 in the 21x21 square maze as shown in Figure 5, see Tables 6 and 7.There are 72 goals in the 21x21 maze, so this correlates to achieving 79 out of 288 goals for the PCL curriculum and 54 out of 288 goals for the Uniform curriculum.The PCL trends are also relatively similar, further indicating that the PCL is providing appropriate goals for the agent to learn.These plots demonstrate that the PCL assists when learning a diverse set of goals.</p>
<p>Selection Strategies and Adaptive Quantiles</p>
<p>In this section we will explore the effect of the selection strategies and adaptive quantiles on the performance of the PCL.We will use the 21x21 maze as it is the most complex and has the most goals.We test the weighted, and multiweighted selection strategies with and without adaptive quantiles, and the uniform strategy with adaptive quantiles.The results are shown in Figure 6.</p>
<p>When compared to the uniform strategy, the weighted strategy is more exploitative selecting results with higher likelihood of success, the multiweighted strategy combines features to maximise information, the adaptive quantiles encourage exploration when reaching goals and exploitation when not.In Figure 6, we can see that the weighted and multiweighted selection strategies, and adaptive quantiles achieve coverages of 0.154, 0.200, and 0.196 respectively showing a degradation in performance when compared to the uniform strategy, see Tables 8, 9, and 10 for hyperparameters.However, when combined the weighted selection strategy with adaptive quantiles achieve a coverage  11 for hyperparameters.</p>
<p>The weighted selection strategy with adaptive quantiles suggests that being more exploitative within adaptive quantiles is beneficial when learning many goals with no obstacles, see Table 12 for hyperparameter values.</p>
<p>Conclusion</p>
<p>We present a novel probabilistic curriculum learning method that utilises SVI and a parametric probabilistic model.The problem is formalised such that the probability density values are used as a surrogate probability metric through quantiles to evaluate the likelihood of reaching a goal given a state and action.</p>
<p>We show that PCL, Algorithm 1, is able to improve learning efficiency, generalise better across multiple goals, and improve performance in longer time horizon tasks when compared to a uniform baseline curriculum.The benefits of PCL are particularly highlighted in the point robot navigation tasks with longer time horizons or many goals.</p>
<p>Additionally, various selection strategies are explored with and without adaptive quantiles.This demonstrates that there is scope for flexibility within the algorithm depending on the task and environment at hand.Future work will explore the use of other probabilistic models, and the use of other selection strategies.</p>
<p>The advantage of our algorithm is that the probabilistic model is able to both generate and evaluate goals.This allows for a flexible and efficient automatic task generation method.Additionally, we do not constrain out model to narrow distributions or require specific initialisations.We use a deep mixture density network as our probabilistic model, but this could be expanded to other models such as normalising flows.Our custom loss function assists in preventing overfitting and modal collapse which can be problematic where data is incomplete and collected during training like reinforcement learning.</p>
<p>A Appendix / supplemental material The length indicates the number of numbers generated between the upper and lower bounds.In the case of these experiments, the layers of the neural networks are given a length of 3,4 indicating that there will be 3 to 4 layers in the network.e.g. for the PCL the number of layers is 3 to 4 with each layer containing 64 to 1024 neurons.</p>
<p>A.2 Hyperparameter Values</p>
<p>Figure 1 :
1
Figure 1: Illustration of the interaction between Q upper and Q lower , the pdf, and goal selection.</p>
<p>Figure 2 :
2
Figure 2: The deep mixture density network architecture.</p>
<p>else 9 :
9
Sample G ∈ R M,N from D 10: Convert s 0 to S 0 ∈ R M,O by repeating s 0 M times 11: Obtain a ∼ π(S 0 , G) 12:</p>
<ol>
<li>1 Figure 3 :
13
Figure 3: DC Motor Coverage and Distribution of Goals.</li>
</ol>
<p>Figure 4 :
4
Figure 4: Bidirectional Maze Coverage</p>
<p>Figure 5 :
5
Figure 5: 21x21 Square Maze Coverage</p>
<p>Figure 6 :
6
Figure 6: 21x21 Square Maze Selection Strategy and Adaptive Quantiles Coverage</p>
<p>1 :
1
Input: an agent π, an environment E, a sampling distribution D, a selection strategy f (.), and a reward function r g 2: Randomly initialise probabilistic model, Θ 3: Initialise replay buffer R 4: for step = 1 to max_steps do
5:Sample initial state s 0 from E6:if epoch &lt; warmup then7:Randomly select g from U(s min , s max )
8:</p>
<p>Select action a t = π(s t , g t )
14:end if15:while terminal state not reached do16:17:Execute action a t and observe a new state s t+118:</p>
<p>Table 1 :
1
Hyperparameter Bounds for Experiments
A.1 Hyperparameter BoundsNameLower BoundUpper BoundLengthTraining Frequency2101(MDN)Number of Mixtures6121Layers (MDN)641024[3, 4]Learning Rate (MDN) 0.000111λ 10.8521λ 20.10.51λ 30.8521β 10.02.01β 20.02.01β 30.02.01Number of Samples80012001Q lower0.010.61Q upper0.6111Batch Size (MDN)12810241Training Frequency6161(SAC)Batch Size (SAC)70010001Layers (SAC)100800[3, 4]Learning Rate (SAC) 4e-060.0011</p>
<p>Table 2 :
2
Hyperparameters for PCL DC Motor Experiments
For ExperimentsParameterexp_1exp_2exp_3Q lower0.2160.1180.33Q upper0.9970.9110.997λ 11.491.651.5λ 20.1950.1020.356λ 31.891.191.98Batch Size (MDN)212135246Batch Size (SAC)999914973Layers (MDN)[720, 1008, 244, 315] [1009, 582, 1016,[507, 945, 332, 106]228]Layers (SAC)[114, 694, 469, 312][194, 314, 383, 267][102, 360, 522, 125]Learning Rate (MDN) 0.2690.1170.264Learning Rate (SAC) 0.0009940.0007930.000995Number of Mixtures121012Number of Samples9701.09e+031.06e+03Training Frequency242(MDN)Training Frequency141516(SAC)</p>
<p>Table 3 :
3
Hyperparameters for Uniform Curriculum DC Motor Experiments
Parameterexp_1exp_2exp_3Q lower0.4080.2560.498Q upper0.9270.6840.78λ 11.891.241.62λ 20.2520.3040.217λ 30.8691.170.924Batch Size (MDN)483134887Batch Size (SAC)798907793Layers (MDN)[831, 392, 715][795, 133, 670][500, 960, 740]Layers (SAC)[678, 209, 574, 148][798, 417, 466, 558][203, 348, 211]Learning Rate (MDN) 0.01940.2330.0455Learning Rate (SAC) 0.0004050.0003960.000261Number of Mixtures9107Number of Samples1.01e+031.1e+031.06e+03Training Frequency529(MDN)Training Frequency768(SAC)</p>
<p>Table 4 :
4
Hyperparameters for PCL Bidirectional Maze Experiments
Parameterexp_1exp_2exp_3Batch Size (SAC)813809723Layers (SAC)[785, 191, 676, 140][543, 172, 595][124, 280, 764]Learning Rate (SAC) 0.000570.0006210.000798Training Frequency16913(SAC)</p>
<p>Table 5 :
5
Hyperparameters for Uniform Curriculum Bidirectional Maze Experiments
Parameterexp_1exp_2exp_3Q lower0.2860.1730.371Q upper0.9920.9170.921λ 11.981.741.91λ 20.4130.3110.308λ 31.281.171.14Batch Size (MDN)495608373Batch Size (SAC)998949946Layers (MDN)[69, 742, 1024][918, 130, 721][903, 761, 722]Layers (SAC)[211, 306, 762][201, 800, 478][216, 798, 300]Learning Rate (MDN) 0.1930.4730.45Learning Rate (SAC) 0.0002150.0008580.000963Number of Mixtures666Number of Samples827832833Training Frequency299(MDN)Training Frequency988(SAC)</p>
<p>Table 6 :
6
Hyperparameters for PCL 21x21 Square Maze Experiments
Parameterexp_1exp_2exp_3Batch Size (SAC)889964906Layers (SAC)[684, 671, 390][480, 715, 459, 645][483, 575, 697]Learning Rate (SAC) 0.0009490.0005140.000938Training Frequency61410(SAC)</p>
<p>Table 7 :
7
Hyperparameters for Uniform Curriculum 21x21 Square Maze Experiments
Parameterexp_1exp_2exp_3Q lower0.3830.480.589Q upper0.7610.780.815λ 11.11.321.32λ 20.3320.2440.277λ 31.811.811.99Batch Size (MDN)632459431Batch Size (SAC)847706808Layers (MDN)[503, 885, 264][96, 670, 301][689, 984, 404]Layers (SAC)[715, 432, 375][518, 246, 344][763, 395, 326]Learning Rate (MDN) 0.3610.890.993Learning Rate (SAC) 0.0009580.0007650.00082Number of Mixtures9611Number of Samples1.11e+031.12e+031.2e+03Training Frequency754(MDN)Training Frequency131416(SAC)</p>
<p>Table 8 :
8
Hyperparameters for PCL + Weighted 21x21 Square Maze Experiments
Parameterexp_1exp_2exp_3β 11.961.981.5β 21.050.1880.0188β 30.4960.620.427Q lower0.4570.4690.216Q upper0.8480.8350.752λ 11.731.781.65λ 20.1970.3740.198λ 31.030.9971.31Batch Size (MDN)543542172Batch Size (SAC)954937951Layers (MDN)[244, 1016, 395][230, 133, 444][113, 603, 552]Layers (SAC)[649, 760, 133][683, 764, 288][379, 625, 795, 423]Learning Rate (MDN) 0.6410.6090.69Learning Rate (SAC) 0.0009050.0008430.000991Number of Mixtures686Number of Samples1.16e+031.2e+03917Training Frequency792(MDN)Training Frequency686(SAC)</p>
<p>Table 9 :
9
Hyperparameters for PCL + Multiweighted 21x21 Square Maze Experiments
Parameterexp_1exp_2exp_3λ 10.9440.950.866λ 20.3150.2820.226λ 31.911.81.97Batch Size (MDN)4382651.01e+03Batch Size (SAC)917959923Layers (MDN)[141, 374, 229][370, 350, 454][275, 401, 337]Layers (SAC)[661, 699, 316][277, 448, 710][140, 395, 777]Learning Rate (MDN) 0.6110.5810.47Learning Rate (SAC) 0.0004460.0005520.000859Number of Mixtures1176Training Frequency10107(MDN)Training Frequency111516(SAC)</p>
<p>Table 10 :
10
Hyperparameters for PCL + Adaptive Quantile 21x21 Square Maze Experiments</p>
<p>A.3 Goal ProbabilityWe can check the probability that the goal sits within some volume V which describes some acceptable region around the goal that an agent has to reach for it to be considered successful:As g c ∈ (R) N this can be expressed as:If we assume that the goal dimensions are independent of each other than we can further simplify this to be:For example we may have some hyper-rectangle, A that is characterised by some matrix E ∈ R 2×N where N is the number of goal dimensions:A.4 Gaussian Probability ProofIf the model as a Gaussian mixture model then we know:For simplicity, we will refer to ϕ j (s t , a t ), µ j (s t , a t ), and Σ j (s t , a t ) as ϕ j , µ j , and Σ j .If we sub the pdf of the learnt multivariate Gaussian mixture model eq 21 into eq 20:For ease of integration and computation, we can assume that the volume is a hypercube C centred about g c with each dimension offset on either side of g c by ϵ &gt; 0. If we also assume all Σ j are diagonal and the goal dimensions are independent then we can perform the following simplification by utilising eq 18:We can then let u i =The new upper and lower bounds are then given by u i ub =. Substituting these into eq 23 yields:We can then utilise the Gaussian error function:Subbing eq 25 into eq 24:
Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Nature. 51875405292015</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Nature. 55076763542017</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Nature. 2019</p>
<p>Jonathan J Timothy P Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. 2015arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International conference on machine learning. PMLR2015</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, arXiv:1801.012902018arXiv preprint</p>
<p>In-datacenter performance analysis of a tensor processing unit. Cliff Norman P Jouppi, Nishant Young, David Patil, Gaurav Patterson, Raminder Agrawal, Sarah Bajwa, Suresh Bates, Nan Bhatia, Al Boden, Borchers, Proceedings of the 44th annual international symposium on computer architecture. the 44th annual international symposium on computer architecture2017</p>
<p>Reinforcement learning and optimal control. Dimitri Bertsekas, Athena Scientific. 2019</p>
<p>Goal-conditioned reinforcement learning with imagined subgoals. Elliot Chane-Sane, Cordelia Schmid, Ivan Laptev, International Conference on Machine Learning. PMLR2021</p>
<p>Deep reinforcement learning for robotics: A survey of real-world successes. Chen Tang, Ben Abbatematteo, Jiaheng Hu, Rohan Chandra, Roberto Martín-Martín, Peter Stone, Robotics, and Autonomous Systems. 82024Annual Review of Control</p>
<p>Overview of reinforcement learning and its application in control theory. Jan Sikora, Renata Wagnerová, 2020 21th International Carpathian Control Conference (ICCC). IEEE2020</p>
<p>A probabilistic interpretation of self-paced learning with applications to reinforcement learning. Pascal Klink, Hany Abdulsamad, Boris Belousov, D' Carlo, Jan Eramo, Joni Peters, Pajarinen, The Journal of Machine Learning Research. 2212021</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningACM2009</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, The Journal of Machine Learning Research. 2112020</p>
<p>Automatic goal generation for reinforcement learning agents. David Held, Xinyang Geng, Carlos Florensa, Pieter Abbeel, CoRR, abs/1705.063662017</p>
<p>Automatic curriculum learning for deep rl: A short survey. Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, Pierre-Yves Oudeyer, arXiv:2003.046642020arXiv preprint</p>
<p>Extending the capabilities of reinforcement learning through curriculum: A review of methods and applications. Kashish Gupta, Debasmita Mukherjee, Homayoun Najjaran, SN Computer Science. 32022</p>
<p>Automatic goal generation for reinforcement learning agents. Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel, International conference on machine learning. PMLR2018</p>
<p>Universal value function approximators. Tom Schaul, Daniel Horgan, Karol Gregor, David Silver, Proceedings of the 32nd International Conference on Machine Learning. Francis Bach, David Blei, the 32nd International Conference on Machine LearningLille, FrancePMLRJul 201537</p>
<p>Hindsight experience replay. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mcgrew, Josh Tobin, Openai , Pieter Abbeel, Wojciech Zaremba, Advances in Neural Information Processing Systems. I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc201730</p>
<p>A survey on curriculum learning. Xin Wang, Yudong Chen, Wenwu Zhu, IEEE transactions on pattern analysis and machine intelligence. 202144</p>
<p>Learning curriculum policies for reinforcement learning. Sanmit Narvekar, Peter Stone, arXiv:1812.002852018arXiv preprint</p>
<p>Autonomous navigation of mobile robots in unknown environments using off-policy reinforcement learning with curriculum learning. Yan Yin, Zhiyu Chen, Gang Liu, Jiasong Yin, Jianwei Guo, Expert Systems with Applications. 2471232022024</p>
<p>Curriculum learning for motor skills. Andrej Karpathy, Michiel Van De Panne, Canadian Conference on Artificial Intelligence. Springer2012</p>
<p>Intrinsically motivated goal exploration processes with automatic curriculum learning. Sébastien Forestier, Yoan Mollard, Pierre-Yves Oudeyer, arXiv:1708.021902017arXiv preprint</p>
<p>Automated curriculum learning for neural networks. Alex Graves, Jacob Marc G Bellemare, Remi Menick, Koray Munos, Kavukcuoglu, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning201770</p>
<p>Automatic curriculum graph generation for reinforcement learning agents. Maxwell Svetlik, Matteo Leonetti, Jivko Sinapov, Rishi Shah, Nick Walker, Peter Stone, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201731</p>
<p>Automatic goal generation for reinforcement learning agents. Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel, Proceedings of the 35th International Conference on Machine Learning. Jennifer Dy, Andreas Krause, the 35th International Conference on Machine LearningStockholmsmässan, Stockholm SwedenPMLR10-15 Jul 201880</p>
<p>Generative adversarial networks: An overview. Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, Anil A Bharath, IEEE Signal Processing Magazine. 3512018</p>
<p>Barnett Samuel, arXiv:1806.11382Convergence problems with generative adversarial networks (gans). 2018arXiv preprint</p>
<p>Generative adversarial networks. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Communications of the ACM. 63112020</p>
<p>Stochastic variational inference. David M Matthew D Hoffman, Chong Blei, John Wang, Paisley, Journal of Machine Learning Research. 2013</p>
<p>Variational inference: A review for statisticians. Alp David M Blei, Jon D Kucukelbir, Mcauliffe, Journal of the American statistical Association. 1125182017</p>
<p>Auto-encoding variational bayes. Max Diederik P Kingma, Welling, 2013</p>
<p>Generalised gplvm with stochastic variational inference. Aditya Vidhi Lalchand, Neil D Ravuri, Lawrence, International Conference on Artificial Intelligence and Statistics. PMLR2022</p>
<p>Structured stochastic variational inference. D Matthew, David M Hoffman, Blei, Artificial Intelligence and Statistics. 2015</p>
<p>Advances in variational inference. Cheng Zhang, Judith Bütepage, Hedvig Kjellström, Stephan Mandt, IEEE transactions on pattern analysis and machine intelligence. 201841</p>
<p>Monte carlo simulation and numerical integration. John Geweke, Handbook of computational economics. 11996</p>
<p>A theory of statistical models for monte carlo integration. Kong, X-L Mccullagh, Meng, Nicolae, Tan, Journal of the Royal Statistical Society Series B: Statistical Methodology. 6532003</p>
<p>M Christopher, Bishop, Mixture density networks. 1994</p>
<p>Overcoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction. Osama Makansi, Eddy Ilg, Ozgun Cicek, Thomas Brox, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Understanding batch normalization. Nils Bjorck, Carla P Gomes, Bart Selman, Kilian Q Weinberger, Advances in neural information processing systems. 201831</p>
<p>Advances in neural information processing systems. Pierre Baldi, Peter J Sadowski, 201326Understanding dropout</p>
<p>Algos: Algorithmic operating system. Llewyn Salt, Marcus Gallagher, 2025arXiv preprint</p>
<p>Stable-baselines3: Reliable reinforcement learning implementations. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, Noah Dormann, Journal of Machine Learning Research. 222682021</p>
<p>. Rodrigo De Lazcano, Kallinteris Andreas, Jun Jet Tai, Seungjae Ryan Lee, Jordan Terry, Gymnasium robotics. 2023</p>
<p>Optuna: A next-generation hyperparameter optimization framework. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, Masanori Koyama, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining2019</p>
<p>Tree-structured parzen estimator: Understanding its algorithm components and their roles for better empirical performance. Shuhei Watanabe, arXiv:2304.111272023arXiv preprint</p>
<p>Parameter optimization and learning in a spiking neural network for uav obstacle avoidance targeting neuromorphic processors. Llewyn Salt, David Howard, Giacomo Indiveri, Yulia Sandamirskaya, IEEE transactions on neural networks and learning systems. 201931</p>
<p>Smooth exploration for robotic reinforcement learning. Antonin Raffin, Jens Kober, Freek Stulp, 2021</p>            </div>
        </div>

    </div>
</body>
</html>
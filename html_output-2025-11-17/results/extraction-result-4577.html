<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4577 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4577</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4577</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-272827497</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.14634v4.pdf" target="_blank">Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination</a></p>
                <p><strong>Paper Abstract:</strong> The scientific ideation process often involves blending salient aspects of existing papers to create new ideas, and facet-based ideation is an established framework for idea generation. To see how large language models (LLMs) might assist in this process, we contribute a novel mixed-initiative ideation tool called Scideator. Starting from a user-provided set of scientific papers, Scideator extracts key facets -- purposes, mechanisms, and evaluations -- from these and related papers, allowing users to explore the idea space by interactively recombining facets to synthesize inventive ideas. Scideator also helps users gauge idea originality by searching the literature for overlaps, assessing idea novelty and providing explanations. To support these tasks, Scideator introduces three LLM-powered retrieval-augmented generation (RAG) modules: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty Checker. In a within-subjects user study (N=22) with computer-science researchers comparing Scideator to a strong baseline, our tool provided significantly more creativity support, particularly with respect to exploration, which participants considered the most important factor for idea generation.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4577.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4577.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RankGPT (facet-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RankGPT (facet-priority LLM re-ranker)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based re-ranking component used to re-rank candidate papers with respect to an idea's application domain, purpose, mechanism, and evaluation, prioritizing papers that match all key facets; used in Scideator's retrieve-then-re-rank novelty-checking pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Facet-based LLM re-ranking (RankGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Two-stage re-ranking: (1) embedding-based filtering using SPECTER embeddings to select the top-N semantically similar papers, then (2) an LLM-based re-ranker (RankGPT) that scores/ranks the remaining papers according to facet-aligned priorities: (i) papers matching all idea facets, (ii) papers matching application domain + purpose, (iii) papers matching purpose or mechanism or evaluation, and (iv) partial matches. The LLM is prompted to compare papers to the idea's application domain, purpose, mechanism, and evaluation and to produce a ranked list; the system then selects the top-K for downstream novelty assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-4o (used for RankGPT re-ranking in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / cross-CS literature (as used in Scideator evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>various (pipeline filters from many candidate papers down to top-100 by embeddings then top-10 by RankGPT; defaults: 100 -> 10)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>ranked list of papers with relevance scores / ordering</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ablation and retrieval overlap analysis: compared top-10 papers and rank shifts across ablations (embedding-only vs. relevance-based LLM reranker vs. facet-based RankGPT) and downstream novelty classification accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As part of system ablation: replacing facet-based RankGPT with a general-relevance RankGPT produced a large drop in novelty-classification performance (ablation table shows 'Relevance RankGPT' accuracy = 13.79% vs. Complete System = 89.66% for predicting 'not novel' in the ablation experiment on 58 ideas/papers).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Ablation compared facet-based RankGPT to (a) a general relevance RankGPT (Relevance RankGPT) and (b) embedding-only filtering; replacing facet-based RankGPT with Relevance RankGPT reduced accuracy dramatically (to 13.79% in the reported ablation) and produced notable rank shifts / lower overlap in top-10 results.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM re-ranking is sensitive to facet-aware prompt design; without facet-based priorities the most relevant overlapping papers are missed (large drops in downstream 'not-novel' detection); dependence on quality of candidate retrieval and embedding pre-filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4577.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4577.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist (Lu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system aiming toward fully automated open-ended scientific discovery; cited as a prior approach to automated scientific discovery and novelty evaluation that Scideator compares against in novelty-evaluator experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI Scientist (open-ended scientific discovery system)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as a recent approach toward fully automated open-ended scientific discovery; in Scideator the authors compare their novelty-checker prompt's agreement with expert labels to the novelty judgments produced by AI Scientist (they compared prompts rather than full systems to keep retrieval fixed). The Scideator paper notes that AI Scientist's novelty evaluator tends to default to 'not novel' when it cannot reach a conclusion (18 of 32 test cases) and that Scideator's prompt produced substantially higher agreement with expert labels on the test set.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / cross-domain scientific discovery (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison of novelty-classification agreement on Scideator's expert-annotated test set (standardized top-10 paper sets) and qualitative example comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Scideator's novelty-prompt achieved over 10x more agreement with expert-labeled examples compared to AI Scientist's novelty prompt on the evaluated test set (exact absolute numbers not provided in the text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Direct prompt-level comparison on the same fixed top-10 related papers; Scideator's prompt outperformed AI Scientist in agreement with experts by an order of magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>AI Scientist sometimes defaults to 'not novel' when uncertain; may misinterpret idea focus in some examples (per the paper's qualitative comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4577.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4577.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Researcher (prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Researcher (novelty-evaluation prompt / approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced LLM-based approach for novelty evaluation or automated research assistance that Scideator compares against; Scideator's novelty checker achieved approximately 13% higher agreement than this approach on their test set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI Researcher (novelty-evaluation prompt comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as another recent LLM-based approach used for novelty evaluation or automated research assistance; Scideator compares only the novelty-evaluation prompting (not end-to-end systems) by standardizing input to the same top-10 related papers and finds Scideator's in-context, expert-labeled examples lead to higher agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / research ideation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Prompt-level comparison on the same test set of ideas and top-10 related papers; measured agreement with expert-labeled novelty judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Scideator's approach produced ~13% higher agreement with expert labels compared to AI Researcher on the standardized test set (absolute numbers not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared prompt outputs under same retrieved paper set; Scideator outperformed by ~13% agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Different systems require differently formatted idea inputs; direct system-level comparisons confounded by retrieval differencesâ€”hence Scideator controlled retrieval for prompt comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4577.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4577.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Researchagent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited LLM-powered system for iterative research idea generation over scientific literature; listed among recent works exploring LLMs for scientific ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Researchagent (LLM iterative ideation over literature)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as a recent work that uses large language models to iteratively generate research ideas grounded in the scientific literature; mentioned in related work as part of the trend of LLM-powered ideation systems, but Scideator differentiates by using facet-based recombination and novelty evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific ideation over scholarly literature (general)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4577.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4577.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scimon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited system for generating scientific inspiration optimized for novelty; positioned in the paper's bibliography as related work on automated support for scientific ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Scimon (novelty-optimized inspiration)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned in references as a prior system for generating scientific inspirations optimized for novelty; included among prior works on computational support for creative scientific ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific ideation / literature-grounded suggestion</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4577.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4577.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeScientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently cited system (2025) that aims to support semi-automated scientific discovery by coupling idea generation with code-based experimentation; listed in the references as background work on computational scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CodeScientist (semi-automated discovery with code-based experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned in references as an end-to-end semi-automated approach that integrates automated experiment/code execution with scientific discovery workflows; cited as part of the recent landscape of systems aiming to automate aspects of discovery from literature and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific discovery / computational experiments</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4577.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4577.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A computational inflection for scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A computational inflection for scientific discovery (Hope et al., Commun. ACM 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced perspective / survey paper on computational approaches to scientific discovery, cited as background for automated discovery efforts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A computational inflection for scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Computational approaches for scientific discovery (survey/perspective)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as prior work discussing computation-enabled scientific discovery; included to situate Scideator within broader efforts that explore computational support for generating and evaluating scientific ideas and discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific discovery / broad interdisciplinary</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation <em>(Rating: 2)</em></li>
                <li>A computational inflection for scientific discovery <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>Scimon: Scientific inspiration machines optimized for novelty <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4577",
    "paper_id": "paper-272827497",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "RankGPT (facet-based)",
            "name_full": "RankGPT (facet-priority LLM re-ranker)",
            "brief_description": "An LLM-based re-ranking component used to re-rank candidate papers with respect to an idea's application domain, purpose, mechanism, and evaluation, prioritizing papers that match all key facets; used in Scideator's retrieve-then-re-rank novelty-checking pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Facet-based LLM re-ranking (RankGPT)",
            "method_description": "Two-stage re-ranking: (1) embedding-based filtering using SPECTER embeddings to select the top-N semantically similar papers, then (2) an LLM-based re-ranker (RankGPT) that scores/ranks the remaining papers according to facet-aligned priorities: (i) papers matching all idea facets, (ii) papers matching application domain + purpose, (iii) papers matching purpose or mechanism or evaluation, and (iv) partial matches. The LLM is prompted to compare papers to the idea's application domain, purpose, mechanism, and evaluation and to produce a ranked list; the system then selects the top-K for downstream novelty assessment.",
            "llm_model_used": "gpt-4o (used for RankGPT re-ranking in this paper)",
            "scientific_domain": "computer science / cross-CS literature (as used in Scideator evaluation)",
            "number_of_papers": "various (pipeline filters from many candidate papers down to top-100 by embeddings then top-10 by RankGPT; defaults: 100 -&gt; 10)",
            "type_of_quantitative_law": null,
            "extraction_output_format": "ranked list of papers with relevance scores / ordering",
            "validation_method": "Ablation and retrieval overlap analysis: compared top-10 papers and rank shifts across ablations (embedding-only vs. relevance-based LLM reranker vs. facet-based RankGPT) and downstream novelty classification accuracy",
            "performance_metrics": "As part of system ablation: replacing facet-based RankGPT with a general-relevance RankGPT produced a large drop in novelty-classification performance (ablation table shows 'Relevance RankGPT' accuracy = 13.79% vs. Complete System = 89.66% for predicting 'not novel' in the ablation experiment on 58 ideas/papers).",
            "baseline_comparison": "Ablation compared facet-based RankGPT to (a) a general relevance RankGPT (Relevance RankGPT) and (b) embedding-only filtering; replacing facet-based RankGPT with Relevance RankGPT reduced accuracy dramatically (to 13.79% in the reported ablation) and produced notable rank shifts / lower overlap in top-10 results.",
            "challenges_limitations": "LLM re-ranking is sensitive to facet-aware prompt design; without facet-based priorities the most relevant overlapping papers are missed (large drops in downstream 'not-novel' detection); dependence on quality of candidate retrieval and embedding pre-filtering.",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4577.0",
            "source_info": {
                "paper_title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AI Scientist",
            "name_full": "The AI Scientist (Lu et al.)",
            "brief_description": "A referenced system aiming toward fully automated open-ended scientific discovery; cited as a prior approach to automated scientific discovery and novelty evaluation that Scideator compares against in novelty-evaluator experiments.",
            "citation_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "mention_or_use": "mention",
            "method_name": "AI Scientist (open-ended scientific discovery system)",
            "method_description": "Cited as a recent approach toward fully automated open-ended scientific discovery; in Scideator the authors compare their novelty-checker prompt's agreement with expert labels to the novelty judgments produced by AI Scientist (they compared prompts rather than full systems to keep retrieval fixed). The Scideator paper notes that AI Scientist's novelty evaluator tends to default to 'not novel' when it cannot reach a conclusion (18 of 32 test cases) and that Scideator's prompt produced substantially higher agreement with expert labels on the test set.",
            "llm_model_used": null,
            "scientific_domain": "general / cross-domain scientific discovery (as cited)",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": "Comparison of novelty-classification agreement on Scideator's expert-annotated test set (standardized top-10 paper sets) and qualitative example comparisons.",
            "performance_metrics": "Scideator's novelty-prompt achieved over 10x more agreement with expert-labeled examples compared to AI Scientist's novelty prompt on the evaluated test set (exact absolute numbers not provided in the text).",
            "baseline_comparison": "Direct prompt-level comparison on the same fixed top-10 related papers; Scideator's prompt outperformed AI Scientist in agreement with experts by an order of magnitude.",
            "challenges_limitations": "AI Scientist sometimes defaults to 'not novel' when uncertain; may misinterpret idea focus in some examples (per the paper's qualitative comparison).",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4577.1",
            "source_info": {
                "paper_title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AI Researcher (prompt)",
            "name_full": "AI Researcher (novelty-evaluation prompt / approach)",
            "brief_description": "A referenced LLM-based approach for novelty evaluation or automated research assistance that Scideator compares against; Scideator's novelty checker achieved approximately 13% higher agreement than this approach on their test set.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "AI Researcher (novelty-evaluation prompt comparison)",
            "method_description": "Referenced as another recent LLM-based approach used for novelty evaluation or automated research assistance; Scideator compares only the novelty-evaluation prompting (not end-to-end systems) by standardizing input to the same top-10 related papers and finds Scideator's in-context, expert-labeled examples lead to higher agreement.",
            "llm_model_used": null,
            "scientific_domain": "general / research ideation",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": "Prompt-level comparison on the same test set of ideas and top-10 related papers; measured agreement with expert-labeled novelty judgments.",
            "performance_metrics": "Scideator's approach produced ~13% higher agreement with expert labels compared to AI Researcher on the standardized test set (absolute numbers not provided).",
            "baseline_comparison": "Compared prompt outputs under same retrieved paper set; Scideator outperformed by ~13% agreement.",
            "challenges_limitations": "Different systems require differently formatted idea inputs; direct system-level comparisons confounded by retrieval differencesâ€”hence Scideator controlled retrieval for prompt comparisons.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4577.2",
            "source_info": {
                "paper_title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Researchagent",
            "name_full": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "brief_description": "A cited LLM-powered system for iterative research idea generation over scientific literature; listed among recent works exploring LLMs for scientific ideation.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "method_name": "Researchagent (LLM iterative ideation over literature)",
            "method_description": "Cited as a recent work that uses large language models to iteratively generate research ideas grounded in the scientific literature; mentioned in related work as part of the trend of LLM-powered ideation systems, but Scideator differentiates by using facet-based recombination and novelty evaluation.",
            "llm_model_used": null,
            "scientific_domain": "scientific ideation over scholarly literature (general)",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4577.3",
            "source_info": {
                "paper_title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Scimon",
            "name_full": "Scimon: Scientific inspiration machines optimized for novelty",
            "brief_description": "A cited system for generating scientific inspiration optimized for novelty; positioned in the paper's bibliography as related work on automated support for scientific ideation.",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "method_name": "Scimon (novelty-optimized inspiration)",
            "method_description": "Mentioned in references as a prior system for generating scientific inspirations optimized for novelty; included among prior works on computational support for creative scientific ideation.",
            "llm_model_used": null,
            "scientific_domain": "scientific ideation / literature-grounded suggestion",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4577.4",
            "source_info": {
                "paper_title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CodeScientist",
            "name_full": "CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation",
            "brief_description": "A recently cited system (2025) that aims to support semi-automated scientific discovery by coupling idea generation with code-based experimentation; listed in the references as background work on computational scientific discovery.",
            "citation_title": "CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation",
            "mention_or_use": "mention",
            "method_name": "CodeScientist (semi-automated discovery with code-based experiments)",
            "method_description": "Mentioned in references as an end-to-end semi-automated approach that integrates automated experiment/code execution with scientific discovery workflows; cited as part of the recent landscape of systems aiming to automate aspects of discovery from literature and experiments.",
            "llm_model_used": null,
            "scientific_domain": "general scientific discovery / computational experiments",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4577.5",
            "source_info": {
                "paper_title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "A computational inflection for scientific discovery",
            "name_full": "A computational inflection for scientific discovery (Hope et al., Commun. ACM 2023)",
            "brief_description": "A referenced perspective / survey paper on computational approaches to scientific discovery, cited as background for automated discovery efforts.",
            "citation_title": "A computational inflection for scientific discovery",
            "mention_or_use": "mention",
            "method_name": "Computational approaches for scientific discovery (survey/perspective)",
            "method_description": "Cited as prior work discussing computation-enabled scientific discovery; included to situate Scideator within broader efforts that explore computational support for generating and evaluating scientific ideas and discoveries.",
            "llm_model_used": null,
            "scientific_domain": "scientific discovery / broad interdisciplinary",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4577.6",
            "source_info": {
                "paper_title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2
        },
        {
            "paper_title": "CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation",
            "rating": 2
        },
        {
            "paper_title": "A computational inflection for scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2
        },
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "rating": 1
        }
    ],
    "cost": 0.02323725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scideator: Human-LLM Scientific Idea Generation and Novelty Evaluation Grounded in Research-Paper Facet Recombination
29 Apr 2025</p>
<p>Marissa Radensky radensky@cs.washington.edu 
Simra Shahid simra.sshahid@gmail.com 
Made large contribution</p>
<p>Raymond Fok rayfok@cs.washington.edu </p>
<p>University of Washington
USA</p>
<p>University of Washington
Pao SiangliulueUSA</p>
<p>Allen Institute for AI
USA</p>
<p>Tom Hope â€ 
Daniel S. Weld â€ </p>
<p>Scideator: Human-LLM Scientific Idea Generation and Novelty Evaluation Grounded in Research-Paper Facet Recombination
29 Apr 2025C84FB724AAC07C2A4CE65724F8409FFFarXiv:2409.14634v4[cs.HC]
The scientific ideation process often involves blending salient aspects of existing papers to create new ideas, and facet-based ideation is an established framework for idea generation.To see how large language models (LLMs) might assist in this process, we contribute a novel mixed-initiative ideation tool called Scideator.Starting from a user-provided set of scientific papers, Scideator extracts key facets -purposes, mechanisms, and evaluations -from these and related papers, allowing users to explore the idea space by interactively recombining facets to synthesize inventive ideas.Scideator also helps users gauge idea originality by searching the literature for overlaps, assessing idea novelty and providing explanations.To support these tasks, Scideator introduces three LLMpowered retrieval-augmented generation (RAG) modules: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty Checker.In a within-subjects user study (N=22) with computerscience researchers comparing Scideator to a strong baseline, our tool provided significantly more creativity support, particularly with respect to exploration, which participants considered the most important factor for idea generation.</p>
<p>INTRODUCTION</p>
<p>Research papers are major sources of inspiration for scientific ideas, as they expose scientists to relevant concepts that can be recombined into new ideas [8,33,55,63].However, generating research ideas by recombining concepts from research papers is difficult for multiple reasons.For one, scientists must wade through an everexpanding literature to find relevant concepts [6,31].Moreover, the phenomenon of fixation biases scientists against considering more diverse concepts and concept recombinations; instead, they are predisposed to think about a problem in familiar terms, which hinders the stimulation of novel ideas [18,57].Even if a scientist manages to identify interesting concept recombinations to form potential research ideas, assessing the ideas' novelty relative to existing literature is a cumbersome yet critical task.</p>
<p>Prior work has demonstrated the effectiveness of using a facetbased approach to find analogies between research papers for idea inspiration [8,33,55].Still, the non-trivial task of recombining concepts and evaluating their associated ideas was left to the scientists themselves, preventing scientists from exploring more recombinations.Meanwhile, recent work has demonstrated large-languagemodels' (LLMs') promise in assisting with scientific ideation.These works explored breadth-first versus depth-first ideation support [42], persona-based feedback on ideas [41], and idea development support [56].LLMs make it possible to quickly synthesize and evaluate ideas.However, none of these human-LLM works support facet-based scientific ideation.</p>
<p>In this work, we present Scideator, an LLM-powered tool for facet-based scientific idea generation and novelty evaluation.Scideator employs a human-AI approach (Fig 1).To begin, the user provides an ideation topic and set of input papers that they would like to use as a starting point for ideation.Scideator extracts key facets (purpose, mechanism, and evaluation) from the input papers.As in prior work, a paper's purpose facet describes the problem addressed by the paper, while its mechanism facet describes the paper's proposed solution to the problem [8,33,55].We also include an evaluation facet, describing each paper's method to determine if the mechanism successfully addressed the purpose.Scideator then retrieves papers with analogous purpose-mechanism pairs similar to those from the input papers.The scientist works with Scideator to select candidate facets from retrieved and input papers for recombination.The tool generates analogies involving the candidate facets and produces ideas based on the most promising ones.Based on relevant retrieved literature and carefully constructed in-context examples, Scideator then provides a novelty classification and reason for proposed ideas, making it easy for scientists to verify the tool's assessment.Finally, the tool provides facet-based suggestions for how to improve ideas deemed not novel.</p>
<p>We investigate how Scideator impacts scientific ideation through a within-subjects user study with 22 computer-science researchers comparing their idea generation and novelty evaluation when using Scideator versus a strong baseline.Participants experienced significantly more creativity support in idea generation with Scideator, particularly with respect to exploring different ideas, which they considered the most important factor for creativity support in idea generation.The results also suggested that Scideator's novelty checker is effective in helping scientists to filter out unoriginal ideas.Participants generally changed their idea novelty assessments when Scideator classified an idea as 'not novel, ' which participants could  The interaction begins with the user providing an ideation topic and set of input papers as a starting point for ideation.2) Scideator responds by retrieving analogous papers to the input papers and extracting facets (purpose, mechanism, and evaluation) from the input and analogous papers.(The evaluation facets are omitted in the figure for clarity, as it is not part of the main logic.)3) The user then selects paper facets as well as adds their own facets for which they want to generate ideas.4) Scideator recombines these selected facets into ideas with one purpose and one mechanism.If a purpose or mechanism facet is unspecified, the tool selects one.5) The user selects an idea to assess for novelty.6) Scideator classifies the idea as "novel" or "not novel" and provides a short rationale.7) The user reviews the novelty classification and adjusts it if they disagree.8) If the idea is deemed "not novel," Scideator suggests more novel ideas with one of the initial idea's facets replaced.</p>
<p>verify using the related papers and explanation provided by the tool.</p>
<p>In an additional automated evaluation of our idea novelty checker, we compare our approach to recent automatic novelty evaluation methods ( [44,61]).For the same collection of papers relevant to an idea, the module improves agreement with human novelty ratings by more than 10x compared to a recent approach for LLM-based novelty ratings [44], and leads to approximately 13% higher agreement than another recent approach [61].In terms of training LLMs to assess novelty, we further show that our approach of manually annotating generated ideas with novelty labels and concise reasons leads to better outcomes than other approaches that employ only literature retrieval, peer-review data from OpenReview [54], or LLM "prompt optimizers" [35,74].Furthermore, ablation experiments demonstrate the importance of each component of our novelty checker.</p>
<p>In summary, we make the following contributions:</p>
<p>â€¢ A human-AI workflow for scientific ideation that... (1) takes a set of papers as input and finds a set of analogous papers, (2) extracts key facets from the input and analogous papers, (3) recombines those facets into new ideas, (4) evaluates those ideas for novelty, (5) iterates upon those ideas to improve their novelty, and (6) allows seamless human refinement at each step.</p>
<p>â€¢ Scideator, a mixed-initiative system that implements this workflow.â€¢ A within-subjects user study (N=22) demonstrating that, compared to a strong baseline, Scideator significantly increases creativity support, especially exploration, which participants consider the most important factor for idea generation.â€¢ An automated evaluation of our novelty checker highlighting its advantages compared to other baselines and ablations.</p>
<p>Scideator: Human-LLM Scientific Idea Generation and Novelty Evaluation Grounded in Research-Paper Facet Recombination , ,</p>
<p>RELATED WORK 2.1 Divergent and Convergent Thinking</p>
<p>In ideation, there are two main stages of thinking: divergent and convergent [15,60].While engaging in divergent thinking, the ideator is not worried about generating the most high-quality ideas.Instead, they aim to produce as many ideas as possible in an effort to leave no stone unturned in considering potential ideas.At this stage of the ideation process, avoiding fixation on familiar concepts is important [18,57].Otherwise, the ideator may miss strong candidate ideas simply because they utilize more distant concepts.In contrast, while engaging in convergent thinking, the ideator concentrates on narrowing down their ideas and determining which ideas to pursue.Scideator provides support for both divergent and convergent ideation.For divergent ideation, the tool helps users to gather inspiration from many sources and come up with several potential research ideas.For convergent ideation, the tool supports evaluating ideas for novelty relative to the literature.</p>
<p>Human-AI, Scientific Ideation</p>
<p>Several prior works have looked into automating scientific ideation [4,37,72], but automatic methods are currently insufficient for formulating novel, impactful research ideas [28,30,72].In response, many works have studied the benefits of human-AI collaboration in scientific ideation [24,73].With the rapid advancement of LLMs, recent work has explored LLM-powered scientific ideation tools [21].For example, the tool CoQuest supports divergent generation of many ideas.It allows users to direct an LLM with plain-text feedback, and the LLM generates potential ideas in a breadth-first or depth-first manner [41].Other works have investigated supporting convergent human-AI scientific ideation, focusing on the development and expansion of a single input research idea rather than the creation of several diverse candidate ideas [42,56].Unlike Scideator, none of these works utilize facet recombination or facet analogies to create ideas, which prior work has demonstrated may benefit the ideation process in general (see Section 2.3) as well as the scientific ideation process in particular (see Section 2.4).Also unlike Scideator, these works do not directly support evaluating ideas for novelty, though they do provide some features that could be used for idea novelty evaluation (e.g., related papers [41,42], an interactive related literature review [56], general critiques and revisions of the idea [42]).To assist with novelty evaluation, Scideator provides scientists not only with relevant papers to the idea but also with a binary classification of its novelty relative to these papers as well as a reason for the classification.</p>
<p>2.2.1 Scientific Idea Novelty Evaluation.Dean et al. determined novelty, relevance, feasibility, and specificity as the most prominent metrics to constitute a "good" idea [17].For supporting idea evaluation, we focus on the aspect of novelty that they referred to as originality, which is defined as "the degree to which the idea is not only rare but is also ingenious, imaginative, or surprising."There has been an increase in work on automatic evaluation of research idea novelty [43,44,72].In the area of human-LLM interaction, Nigam et al. introduced Acceleron, a mixed-initiative, LLM-powered tool that uses an agent-based architecture with distinct personas to assess and improve upon the novelty of a research proposal relative to similar papers [50,51].Related work has also emerged regarding automatic paper reviews, which often involve assessing the paper's novelty [16,40].In terms of human-LLM paper reviews, Sun et al. presented an LLM-powered tool to support novice peer reviewers, which included in-situ knowledge support for novelty evaluation [66].However, all of these works related to scientific idea novelty evaluation do not explore support for facet-based ideation, while Scideator does.Furthermore, these works focus on convergent thinking and do not include any component for divergent idea generation, while Scideator supports a seamless back-and-forth between the idea generation and idea novelty evaluation processes.In addition, Acceleron, which to our knowledge is the main prior work exploring human-LLM novelty evaluation of potential research ideas, had a modest evaluation-a qualitative analysis of three scientists' interactions with the system, whereas Scideator is evaluated through both automatic analyses and a mixed-methods user study with 22 participants.</p>
<p>Human-AI, Facet-Based Ideation</p>
<p>Concept combination and analogy are key methods for creating ideas [26,34,69].Often, concept combination refers to the fusing of two concepts into a new emergent concept.We use the phrase "concept combination" or "facet combination" more broadly to refer to the use of multiple concepts in creating a new idea.</p>
<p>Related work has investigated how concept combination may be used in LLM-powered tools for ideation in non-scientific domains []. BIOSPARK presents a human-AI tool to support facet-based ideation for engineering designs inspired by biological analogies.The tool helps users to identify inspiration for solving one of a few specific, preset engineering problems by drawing on a dataset of biological mechanisms.Meanwhile, AnalogiLead allows users to combine facets from preset design problems and analogous situations in order to produce new ideas [64].We build upon these works by developing and evaluating a human-LLM tool that supports facetbased ideation for any user-provided ideation topic 1 and small set of input papers, based on which the system identifies analogous papers and extracts facets to recombine.Some prior works focused on human-AI facet-based ideation have, like Scideator, addressed more flexible ideation topics [9,38].For instance, CreativeConnect allows users to recombine keywords to generate a graphic sketch [12], and Luminate helps users to recombine values of various dimensions to generate diverse LLM responses [65].Scideator is still different from these tools in two manners.First, Scideator recombines facets through the use of analogy, which is an established framework for supporting facetbased ideation (see 2.4).Second, Scideator provides affordances for idea novelty evaluation.In the scientific domain, this is particularly important.</p>
<p>Human-AI, Facet-Based, Scientific Ideation</p>
<p>Of particular note to scientific ideation is a line of work that describes ideas in terms of two facets: the purpose (i.e., the problem) and the mechanism (i.e., the proposed solution to the problem).Above, the user selects or adds facets to generate ideas.They can also generate more facets to consider, and add custom instructions for the idea generation.Below, the user peruses their ideas and evaluates an idea for novelty by clicking the search icon to its left.The ideation topic here is human-AI collaboration in art.</p>
<p>Hope et al. found that this faceted idea framework helps identify useful analogies for ideation [27].If two ideas have similar purposes, then the mechanism of one idea may apply well to the purpose of the other idea.Similarly, if two ideas have similar mechanisms, then the purpose of one may combine well with the mechanism of the other.Subsequently, the framework has been shown to facilitate the creation of useful analogies between product ideas [27,29], biological and design ideas [32], research papers [8,33], and research-paper authors [55].A recent work has tried to apply LLMs to this faceted framework [22], but our work is the first to explore a human-LLM tool to support this framework.With the power of LLMs, Scideator differentiates itself from prior work in human-AI, facet-based, scientific ideation.Not only does it present analogous papers with facets to recombine, as done in prior work, but it also supports users in selecting research-paper facets to recombine and turning those facet recombinations into research ideas.Furthermore, it supports idea novelty evaluation, an important part of the scientific ideation process.</p>
<p>SYSTEM 3.1 Design Goals</p>
<p>We developed Scideator with two design goals in mind.</p>
<p>â€¢ DG1: Help scientists to generate research ideas using a facet-based framework.Prior work has demonstrated the benefits of facet-based techniques for ideation.These works provide evidence that facet-based ideation helps people to discover facets to recombine and generate more ideas [12], avoid fixation and engage in design space thinking [65], and utilize analogies to produce ideas [32,64].In the scientific domain, an established facet-based framework represents research papers in terms of purposes (i.e., problems) and mechanisms (i.e., proposed solutions to problems).This framework has been shown to facilitate creative ideation [33] and identify useful analogies for ideation [8].</p>
<p>To address DG1, our system retrieves facets relevant and analogous to the user's input paper facets, which can be recombined to form research ideas, with as much or as little input from the user as they prefer.To enhance the expressiveness of the faceted idea framework consisting of purpose and mechanism facets, we introduce the facet of evaluation, or the method to determine whether the proposed solution solves the problem.Our system also allows users to add their own facets.</p>
<p>â€¢ DG2: Help scientists to evaluate ideas for novelty.Prior work has established novelty as a key component for a good idea [17].Scientists want to work on novel research ideas in order to make meaningful contributions to the scientific community, which is why several works on automatic novelty evaluation for scientific ideas already exist [43,44,72].</p>
<p>, ,</p>
<p>Because the main decision-maker for which scientific idea to pursue is still the scientist, it is important to study human-LLM tools for idea novelty evaluation.Furthermore, LLMgenerated research ideas may plagiarize existing work [25], making humans critical for thorough idea novelty assessment.An important challenge in finding a novel research idea is determining if an idea is novel compared to a vast pool of existing literature and improving upon its novelty if need be [50,72].To address DG2, our tool provides idea novelty assessments that classify an idea as 'novel' or 'not novel' relative to retrieved related work and present a rationale to explain the classification.If the idea is classified as 'not novel,' the tool provides suggestions for more novel ideas, with one facet replaced.</p>
<p>Paper Facets</p>
<p>To generate ideas, Scideator utilizes three facets from papers: the purpose, mechanism, and evaluation.The purpose facet describes the problem being addressed by the paper, the mechanism facet describes the paper's proposed solution to the problem, and the evaluation facet describes the paper's method to determine if the proposed solution actually solves the problem.</p>
<p>Workflow and Implementation</p>
<p>In this section, we provide an overview of the Scideator workflow and its implementation.The frontend of Scideator was developed using React and TypeScript, and the backend with Python.Unless noted otherwise, we used the LLM gpt-4o-2024-08-06 and a temperature of zero.In addition, whenever we mention using a paper, we only use its title and abstract.The prompts to the LLM for each module may be found in the appendix.</p>
<p>Our workflow has three steps: Papers -&gt; Facets, Facets -&gt; Ideas, and Idea -&gt; Novelty Assessment (Figure 1).Each step is driven by the following modules respectively: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty Checker.</p>
<p>Workflow</p>
<p>Step 1: Papers -&gt; Facets.A user enters a broad ideation topic 2 and a set of one or more papers that they would like to use as a starting point.Scideator uses the input to find relevant paper facets to recombine.</p>
<p>Scideator's Analogous Paper Facet Finder 3 (Figure 3) first retrieves information for the input papers as well as four similar papers from the Semantic Scholar API [36].To ensure a balance among these papers, we request the two top-ranked papers (with a retrievable title and abstract) from the "all-cs" corpus and another two from the "recent" corpus.Thus, Scideator retrieves four papers very near to the input paper.Scideator then uses the input and very-near papers to create a summary of relevant works.This summarization step is inspired by CoQuest's "search and summarize" action for coming up with research questions [41].</p>
<p>Next, Scideator's LLM is prompted to extract the overarching purpose and mechanism of the input papers.The LLM subsequently generates twelve purpose-mechanism pairs that are analogous to the overarching purpose and mechanism of the input papers.More Next, the tool extracts key facets from the input papers and determines the input papers' overarching purpose and mechanism, which it uses to come up with three queries for papers with an analogous purpose and mechanism.The queries are for analogous papers with varying distances from the input paper: same topic (near), same subarea (far), and different subarea (very far).Those queries are fed to the Semantic Scholar API to retrieve analogous papers.Finally, the facets of all the analogous papers are extracted by the LLM.</p>
<p>specifically, it generates four analogous purpose-mechanism pairs from the same topic within computer science research (near), four from the same subarea (far), and four from a different subarea (very far).For each analogous purpose-mechanism pair, the LLM also generates a query for finding relevant papers.Scideator uses the Semantic Scholar API to retrieve the top four papers (with retrievable titles and abstracts) relevant to each query from the "all-cs" corpus.If no paper is found, Scideator iteratively shorten the query until a paper is returned.The first retrieved paper becomes the representative paper for the analogous purpose-mechanism pair, while the other three papers act only as additional context for the representative paper.Thus, Scideator prepares four near, four far, and four very-far analogous papers.</p>
<p>Scideator generates the key facets (purpose, mechanism, and evaluation) of the input papers and the 16 papers of varying distance from the input papers, based on their titles and abstracts.The LLM is prompted to write the facets as short phrases (no more than 7 words). 4s shown in the top half of Figure 2, the tool presents all of the extracted paper facets to the user.They can track facet provenance to see if the facet originated from their own input (input paper, manually-added facet, or facet generated based on a user-provided query) or from a paper of increasing distance from the input papers (very near, near, far, or very far).If the user is unsure what a particular facet is, they can hover over the facet's associated question Users can also type in their own facets directly.Furthermore, they can use the "Generate More Facets" button to synthesize new ones, optionally specifying a query as direction.If they provide a query, Scideator retrieves four papers relevant to the query: the top two from the "all-cs" corpus and top two from the "recent" corpus.Otherwise, the system retrieves four papers from each distance category with the Analogous Paper Facet Finder.Then, Scideator generates the key facets from each of the retrieved papers and adds them to the interface.</p>
<p>Workflow</p>
<p>Step 2: Facets -&gt; Ideas.Once the user has gotten a sense of the facets available, they either select facets or allow the system to choose in order to generate ideas (Figure 2, top half).Based on the facet selection, the tool generates four ideas.</p>
<p>Table 1 summarizes how Scideator's Faceted Idea Generator module recombines facets for idea generation. 5 6The module is designed to handle idea generation in four situations: when the user 1) first loads the system (Initial), 2) does not select a purpose or mechanism (No-P-no-M), 3) selects at least one purpose but no mechanism or vice versa (P-or-M), and 4) selects at least one purpose and mechanism (P-and-M).</p>
<p>The module aims to combine papers of varying distance from the input papers.No matter the situation, the LLM is prompted twice to generate analogies and corresponding ideas between two groups of papers, which are selected based on which of the four situations arise.For each of the two prompts, the LLM generates six candidate analogies between a group-1 paper's purpose and mechanism and a group-2 paper's purpose and mechanism.The LLM selects the two analogies that best fit carefully described criteria for a good idea (i.e., understandability, relevance, feasibility, specificity, and novelty).The LLM then converts one analogy into an idea combining the group-1 paper's purpose and the group-2 paper's mechanism.It converts the other analogy into an idea combining the group-1 paper's mechanism and the group-2 paper's purpose.The LLM is instructed to come up with ideas related to the ideation topic but differentiate its ideas from the existing work described in the relevant works' summary and the papers from which it ideates.It is also instructed to take a step to improve upon its ideas in terms of the criteria for a good idea.</p>
<p>To start, the system creates four ideas using our Initial idea generation method.In this situation, for generating the first two analogies and corresponding ideas, the group-1 papers are the input/very-near papers, and the group-2 papers are the near papers.For generating the second two analogies and corresponding ideas, the group-1 papers are again the input/very-near papers, and the group-2 papers are the far/very-far papers.Each generated idea's evaluation facet is selected by the LLM; it is the evaluation facet either from the purpose's associated paper or the mechanism's associated paper.Afterwards, Scideator produces four ideas each time based on the user's choice of facets and its corresponding situation, as outlined in Table 1. 7Table 2 presents a sample of user study participants' favorite ideas, which were generated using the different methods of this module. 8.</p>
<p>As the user explores their facet-based ideas, they can see a more detailed version of each by clicking the 'Expand' button (Figure 2, bottom half).Not pictured, the user can also add their own idea, and the LLM is prompted to extract its facets, which are added to the list of available facets.â€¢ Selected purpose facet(s)</p>
<p>â€¢ Selected mechanism facet(s)</p>
<p>â€¢ Selected evaluation facet(s)</p>
<p>â€¢ 4x: <selected, selected, *> Table 1: The Faceted Idea Generator module generates analogies between two purpose-mechanism pairs from different papers.It then uses these analogies to inspire novel ideas combining the purpose from one paper with the mechanism from another.The module aims to combine papers of varying distance from the input papers, but which papers are involved in the analogies depends on whether the user has selected any purposes or mechanisms.</p>
<p>Workflow</p>
<p>Step 3: Ideas -&gt; Novelty Assessments.After collecting a few candidate ideas, the user evaluates ideas for novelty and finds more novel ones.To evaluate an idea, the user opens the novelty checker modal (Figure 4) by clicking the search button to the left of an idea.The user reviews the tool's retrieved papers related to the idea, novelty classification of the idea ("novel' or "not novel"), and short classification reason referencing the related papers.The user can manually overwrite Scideator's classification and reason.When the idea is marked as "not novel", Scideator presents the user with three new idea suggestions, each replacing a different facet in the original idea.The Idea Novelty Checker module (Figure 5) powers these interactions through four steps: 1) retrieve candidate relevant papers, 2) select most relevant papers, 3) evaluate idea novelty, and 4) suggest more novel ideas. 9tep 1: Retrieve candidate relevant papers.To assess an idea's novelty, we compare an idea against a comprehensive collection of relevant papers.This collection includes many papers, including all of the papers that the previous modules retrieved.Scideator also finds more related papers to these papers using the Semantic Scholar API [36].However, simple retrieval methods  often overlook contextual aspects of ideas such as their purpose, mechanism and evaluation facets [48,49,70].To improve the paper collection's coverage, Scideator generates search queries based on various keywords directly related to the idea to retrieve relevant papers, a query-based retrieval method used in [45,62].The search queries themselves are LLM-generated keywords and potential titles based on the idea description.The keyword-based search results sometimes introduce irrelevant results.For example, consider the idea in Figure 5 about citation-based suggestions for scientific innovation; a keyword like 'scientific innovation' is extracted from the idea.Searching with this keyword would indeed retrieve relevant papers but might also include unrelated articles merely because the phrase appears in their title and abstract.To counter this limitation, Scideator retrieves relevant text snippets directly from the full text of papers on Semantic Scholar using its snippet-text API 10 .The system forms the query using the entire proposed idea, finding snippets from papers with potentially high relevance. 10api.semanticscholar.org/api-docs/#tag/Snippet-Text</p>
<p>The papers utilized in the prior modules, additional retrieved papers related to them, and the additional papers obtained through both keyword-based and snippet-based searches collectively form the comprehensive candidate paper set for the idea novelty assessment process.</p>
<p>Step 2: Select most relevant papers.To identify the papers most likely to overlap with the idea, we implement a twostage re-ranking process that combines embedding-based filtering with an LLM-based re-ranking approach.This follows established information retrieval practices of retrieve-then-re-rank [1,5,20,46,52,67].</p>
<p>The first stage is embedding-based filtering.Scideator computes the semantic similarity between the idea and each paper in papers from STEP 1 using Semantic Scholar SPECTER embeddings [14].It selects the top  most similar papers to the idea based on cosine similarity.This embedding-based ranking efficiently narrows down the paper collection but, compared to LLMs [59], fails to capture more contextual relationships between different facets of the idea and related papers.When the idea is classified as "not novel," the system provides a set of three suggestions for more novel ideas (f), each of which replace one of the idea's original facets.The ideation topic here is human-AI collaboration in art.</p>
<p>The second stage addresses this limitation with an LLM-based re-ranker, RankGPT [68], which goes beyond surface similarities.We use RankGPT to compare the papers against the idea's application domain, purpose, mechanism, and evaluation. 11It ranks the papers based on the following priorities (from high to low): 1) papers with all the idea's key facets; 2) papers with a similar application domain and purpose; 3) papers with a similar purpose, mechanism, or evaluation; and 4), papers with partially matched or related facets.</p>
<p>This approach ensures that the final ranking accurately reflects the and depth of each paper's connection with the idea.Finally, Scideator selects the -most relevant papers for the novelty assessment.The default  papers that proceed after the embedding-based ranking is 100, and the default  papers that proceed after the LLMbased re-ranking is 10.We use the LLM gpt-4o for this step.</p>
<p>Step 3: Evaluate Idea Novelty.Using the top- relevant papers as comparison points, Scideator prompts an LLM to assess an idea for novelty.The LLM outputs a binary classification ('novel' or 'not novel') accompanied by its reasoning, which points to the related papers for any similarities and differences.However, determining the novelty of an idea in relation to existing scientific literature is a complex and subjective task.To guide the LLM's judgment, our prompt incorporates in-context examples drawn from formative annotation studies (see Section 4.1).Each example is comprised of an idea, the top- papers related to the idea, the novelty classification label, and classification reason.These examples reflect the experts' criteria for novelty: an idea is considered novel if it 1) differs from all retrieved papers in purpose, mechanism, or evaluation, 2) presents a unique combination of these facets, or 3) applies the same facets to a new application domain.We use the LLM o3-mini for this step, given its focus on reasoning.</p>
<p>Step 4: Suggest More Novel Ideas.Based on the classification reasoning, the LLM is instructed to come up with three suggestions for ideas that are more novel compared to the related papers.Each suggestion replaces a different facet in the initial idea with another available facet of the same category (e.g., purpose).For increased variance in responses, we use a temperature of 0.75 in this step.</p>
<p>FORMATIVE STUDIES FOR IDEA NOVELTY CHECKER</p>
<p>In this section, we describe formative studies conducted to 1) understand the challenges of scientific idea novelty evaluation through annotations of ideas' novelty, 2) investigate the importance of each component of our novelty checker and 3) explore how our novelty checker performs relative to baselines.</p>
<p>Annotation Studies</p>
<p>In the context of scientific literature, novelty evaluation faces two challenges.First, identifying the most relevant papers from the hundreds of millions available is inherently difficult, as the space  The Idea Novelty Checker module follows a retrieve-then-re-rank approach for novelty evaluation.In Step 1, it gathers a comprehensive set of papers relevant to an idea.This includes papers originally used to generate the idea, related papers, and additional papers retrieved through keyword and title searches extracted directly from the idea, as well as snippet searches using the entire idea as input.In Step 2, a two-stage re-ranking process is applied, where an embedding-based ranking strategy filters the large collection to top- papers, followed by a facet-based LLM re-ranker to identify the top- most relevant papers.In Step 3, these top- papers are used to assess the idea's novelty, guided by in-context examples that evaluate novelty with grounded reasoning.In Step 4, if an idea is classified as "not novel" by the tool or user, the LLM generates three idea suggestions, each replacing a different facet in the original idea in order to make the idea more novel compared to the relevant papers.</p>
<p>of potentially overlapping work is vast.Second, determining criteria for judging novelty is challenging because novelty itself is subjective and can be defined in multiple ways.</p>
<p>To better understand and address these issues, we conducted a formative study, in which the first two authors assessed the novelty of ideas based on retrieved relevant papers.The authors evaluated 51 ideas-46 generated by Scideator and five adapted from accepted and rejected papers from OpenReview (ICLR 22, NeurIPS 23).Fewer examples were taken from OpenReview since the primary focus was on evaluating ideas from Scideator.For every idea, the system identified the most relevant papers through a two-step process.It first gathered candidate papers using keyword-based queries and subsequently re-ranked them for relevance to the idea using an LLM-based re-ranker [68].The authors classified each ideas as novel, moderately novel, or not novel based on the top 10 papers.They achieved moderate agreement (Cohen's Kappa = 0.64) with 17 disagreements out of 51 classifications.The agreement may not have been higher because the authors sometimes relied on broader domain knowledge rather than restricting judgments to the most relevant papers, which were often insufficient.Additionally, using three categories led to disagreements, as the distinction between novel and moderately novel is itself subjective.</p>
<p>Building on observations from the formative annotation study, we conducted a second study.In this study, we based our judgments solely on the provided papers, and the classification categories were simplified to just novel and not novel.Inspired by prior work [8,33,55] that categorizes research ideas into core facets such as purpose and mechanism, we also agreed on defining novelty as follows.An idea is considered novel if it differs from all 10 retrieved papers in at least one core facet (purpose, mechanism, or evaluation).An idea is also considered novel if it uniquely combines these facets or applies them to a new application domain.Using this controlled framework, we annotated a set of 51 ideas-34 new ones generated by Scideator and 17 from the previous study where external knowledge had influenced novelty judgments.By narrowing the focus to the relevant papers alone and simplifying the classification options, we observed fewer disagreements (8/51) and achieved a higher agreement rate (Cohen's Kappa = 0.68).This study highlights that a robust novelty checker depends critically on (i) high-quality retrieval, which we aim to support with two-stage re-ranking and retrieval, and (ii) a well-defined notion of novelty, which we aim to provide in our LLM prompting.In our ablations, we considered the following variations: (i) Complete System: used keyword and snippet retrieval (each returning the top- documents based on Semantic Scholar's ranking), embedding filtering, and facet-based RankGPT re-ranking; (ii) Relevance RankGPT: used the same retrieval methods (keyword and snippet) plus embedding filtering, but replaced the facet-based RankGPT re-ranker with one based on general relevance [68].This variation differs from the complete system only in the LLM re-ranking component, allowing us to assess the importance of facet-based re-ranking; (iii) Embedding Filtering: omits the LLM re-ranker entirely, relying only on the embedding-based filtering.This setup allows us to assess the importance of the LLM re-ranking step; and (iv) Snippet Retrieval and Keyword Retrieval: each of these setups returned the top- documents from their respective retrieval method (without embedding filtering or any LLM re-ranking), leveraging the inherent ranking/scoring provided by Semantic Scholar.This setup allows to assess the importance of both re-ranking steps.This structured setup enabled us to isolate the contribution of each component (retrieval method vs. re-ranking strategy) and evaluate whether they collectively brought key papers for novelty assessment into the top 10.We use gpt-4o for re-ranking and o3-mini for evaluating novelty.bring key papers into the most relevant paper set.These findings show that combining facet-based reranking with embedding filtering is critical for identifying the most relevant papers.Table 4 compares the top-10 most relevant papers retrieved under each ablation setting with those from the complete system.Approximately 30% of the papers differ when using either Embedding Filtering or general Relevance RankGPT.Additionally, notable rank shifts are observed from the facet-based LLM reranker in the complete system to the relevance-based LLM reranker (Relevance RankGPT).Further, without the reranking steps, both snippet and keyword retrieval exhibit minimal overlap with the final system's top paper results, highlighting the importance of the reranker stage.</p>
<p>Comparison to Baselines</p>
<p>4.3.1 Methods.We evaluated multiple baselines to benchmark our novelty evaluation approach.First, we employed a zero-shot prompt as a straightforward baseline, and further refined this manually written prompt using Anthropic's prompt generator 12 .We also applied popular prompt optimization techniques such as DSPy [35] and TextGRAD [74], which optimize the prompt instructions using a train/validation split created from formative study examples.In experiments with TextGrad [74], we investigated how specific prompt instructions influence an LLM's ability to classify the novelty of an idea.</p>
<p>As an alternative to using in-context examples from the formative study, we extracted reviews from ICLR and NeurIPS submissions via the OpenReview API [54].These reviews comprise aspects such as strengths, presentation, limitations, soundness, weaknesses, questions, confidence, contribution, summary, and rating.The input title and abstract were adapted to match the ideas in the training data using a style-change prompt.After rigorous filtering, we identified approximately 8,156 submissions discussing idea novelty and manually selected reviews that specifically evaluated the core idea rather than the entire paper.From these, we randomly sampled 20 idea-review pairs to serve as an additional baseline with different  In addition to these baselines, we also compare our novelty checker 'prompt' with that of AI Scientist [44] (different from its paper reviewer) and AI Researcher [61] on the same test set of ideas and fixed top 10 papers.We compare only the prompts to assess novelty, rather than the entire systems, because the test set containing the novelty judgments by experts were based on a fixed set of the 10 most relevant papers for each idea.Since different retrieval methods could introduce new papers and potentially change novelty classification, we standardized the most relevant papers to ensure a fair comparison of the prompts alone.Additionally, since both setups require a different style of input idea, we adapted the ideas to match the requirements of each system.</p>
<p>We compared all of these baselines to various ablations of our expert-labeled in-context examples.When we include papers in the expert-labeled examples, they consist of titles and abstracts only.For our best performing setting, we also experimented with o3-mini. 13.3.2Results.As shown in Table 5, our experiments indicate that incorporating expert-annotated data as in-context examples significantly enhances novelty classification accuracy compared to zero-shot prompts, DSPY, TextGRAD, and setups using OpenReview examples.We compared two configurations for DSPY, one with reasoning and one without.Our expert-labeled prompt consistently achieved higher performance than the prompt optimizations produced by these methods, and we posit that the number of examples for train/validation were not sufficient for prompt optimizers with GPT-4o.Meanwhile, the TextGRAD prompt optimizer did not improve upon its initial system prompt.Notably, even when we excluded the relevant papers from the expert-labeled examples, our approach still outperformed the OpenReview baseline.For our best performing setting, which includes the idea, most relevant papers, classification, and reasoning, we also experimented with o3-mini and observed a better recall than GPT-4o.o3-mini was able to better reason about analogous similarity across papers and ideas.</p>
<p>Although the TextGRAD prompt optimizer did not improve upon its initial system prompt, it provided valuable insights into LLM prompt sensitivity.Figures in Appendices 10, 11, and 12 present the accuracy of various prompts optimized with TextGrad on our dataset (train=25, validation = 10, test = 32).Prompts with both non-zero and zero validation accuracy included various instructions for evaluating the novelty of ideas.Through this prompt optimization process, we observed interesting ways in which LLMs may evaluate novelty, such as considering historical context, considering frequency of similar studies, conducting comparative analysis with existing works, and examining arguments for both novel and non-novel perspectives.However, prompts without these specific instructions also influenced accuracy, suggesting the complexity of novelty evaluation with LLMs.Notably, some prompts with similar instructions showed different performance on validation data.For example, both Prompt 3 (accuracy = 0) and Prompt 9 (accuracy = 0.6) include instructions to evaluate if the idea introduces unique methodologies and how it compares to existing work.However, the difference in their performance suggests that subtle variations in wording and instruction framing can significantly impact the classification performance.It remains unclear why certain prompts perform better despite having similar instructions.This analysis highlights the LLM's sensitivity to prompt design when assessing novelty of an idea.Even minor variations in wording and structure can lead to substantial performance changes, emphasizing the need for careful prompt engineering and well-chosen in-context examples to guide the LLM for idea novelty evaluation.Furthermore, our approach achieved over 10 times more agreement with expert-labeled examples compared to AI Scientist, and approximately 13% higher agreement than AI Researcher, further validating the effectiveness of our novelty checker.It is important to note that AI Scientist defaults to "not novel" when it fails to reach a conclusion in novelty evaluation (18 out of 32 times), which may have impacted its agreement rates.Appendix H qualitatively compares novelty evaluations by AI Scientist, AI Researcher, and Scideator on two research ideas.Scideator provides concise justifications for its novelty decisions by referencing key similarities and differences with existing works.For example, in Example 1, it correctly identifies the idea as 'novel' by highlighting these aspects.In contrast, AI Researcher evaluates each paper individually, classifying an idea as 'not novel' if any paper is considered citable; but in our examples, none of the papers were flagged as citable despite sharing similar purposes, leading to a 'novel' classification.Due to space constraints, we show insights only from the first paper for each example.Figure 15 indicates that while AI Scientist's judgments generally align with the ground truth and offer actionable suggestions, it sometimes misinterprets the idea-as in Figure 14, where its focus shifts from the idea to the accompanying code.</p>
<p>USER STUDY METHODS</p>
<p>We evaluated Scideator against a strong baseline tool that supports combining ideas at the paper level instead of the facet level.</p>
<p>Research Questions</p>
<p>â€¢ RQ1: Does Scideator lead to more creativity support for idea generation than a strong baseline?â€¢ RQ2: What are common ideation patterns while using Scideator as compared to a strong baseline?â€¢ RQ3: Does Scideator improve confidence in idea novelty assessments more than a strong baseline?</p>
<p>Participants</p>
<p>We recruited 22 computer-science researchers (W: 7, M: 15) through institutional mailing lists and academic social networks.We compensated them with a $60 Amazon gift card.Twelve participated as human-computer interaction (HCI) researchers and 10 as naturallanguage-processing (NLP) researchers.Most were PhD students (PhD student: 16, master's student: 5, industry researcher: 1).Generally, the participants interacted with LLMs often (a few times per... day: 12, week: 7, month: 1, few months or longer: 2).</p>
<p>Study Design</p>
<p>We conducted a within-subjects study, in which each participant completed tasks for the treatment and baseline conditions in randomized order.The ideation topics for the treatment and baseline conditions were also randomized.Overall, participants had no difference in their familiarity ratings (7-point, Likert-type) for the assigned treatment topic and assigned baseline topic (M=0.00,Q1=-1.00,Q3=1.00).There were two preset topics for HCI researchers (human-AI collaboration in art, AI tools for education) and two for NLP researchers (dealing with LLM hallucinations, LLM explainability).For each topic, there were three associated input papers to use as a starting point. 14or the treatment tool, we modified Scideator to more effectively address our research questions.Our study separates the idea generation task from the idea evaluation task.To keep the study controlled, we disabled some of Scideator's functionalities: ondemand novelty evaluation, manual idea addition, and facet generation when there is no query.The Idea Novelty Checker module was only activated in a separate 'Idea Novelty Evaluation' tab for the idea evaluation step.There was no support for adjusting the novelty assessment or iterating on the idea's novelty.The tab also provided access to a ChatGPT-like interaction in which participants could prompt the LLM directly in order to help them evaluate their ideas for novelty.The chat kept a thread for each participant to remember their exchanges when forming a response.Lastly, the tab provided a text field where the participant could keep any notes on their novelty assessments.</p>
<p>For the baseline tool, in the 'Idea Generation' tab (Figure 6), participants could select any combination of the three input papers as input to the LLM gpt-4o-2024-08-06, the same LLM used for most of Scideator's functionality.If they did not select any papers, all three were provided to the LLM.Participants could also provide custom instructions to the LLM like in the treatment tool, but the character limit was 75000 rather than 25000 to account for the fact that the set idea generation prompt was longer in the treatment tool.The LLM prompt for idea generation was a simplified version of the one in Scideator.It did not utilize any facet-based framework or carefully crafted criteria for a good idea.However, like Scideator, it generated six candidate ideas for every two presented to the participant and followed instructions to improve upon the idea.The The 'Idea Novelty Evaluation' tab was similar to that in the treatment tool except there was no Idea Novelty Checker module output (i.e., related papers, novelty classification, and classification reason for each idea).</p>
<p>Procedure</p>
<p>Each within-subjects study session was 105 minutes.The sessions were recorded and transcribed using Google Meet. 15In each condition, the session coordinator provided the participant with the assigned tool, a document with the titles and abstracts of the input  papers for the assigned ideation topic, and a link to the scientific search engine Semantic Scholar 16 .They had access to these three resources throughout the condition.The participant completed two tasks with each tool-an idea-generation task followed by an idea-novelty-evaluation task.For the idea-generation task, the participant entered their assigned ideation topic and three input papers into the tool.While the tool loaded, the coordinator went over the task instructions and gave the participant a tutorial describing the tool's features.The participant then had up to two minutes to review the three input papers' titles and abstracts.With access to the tool, Semantic Scholar, and the input paper document, the participant subsequently spent 20 minutes generating and saving as many research ideas as possible.To save an idea, the participant had to confirm that the idea was at least somewhat relevant to the ideation topic and somewhat interesting to think about further.They also had to provide a sevenpoint Likert-type rating of how different the idea was from ideas that they had or encountered before the study; they were told to aim for saving ideas that were at least somewhat significantly different.The coordinator alerted the participant when five minutes remained.If the participant was rating an idea to save it when 20 minutes had passed, they completed the rating before moving on to the next step.</p>
<p>Once 20 minutes had passed, the participant opened a 'Saved Ideas' tab to select their two favorite ideas and answer additional 7-point Likert-type questions about their perceived novelty, feasibility, specificity, impact, and imaginativeness of each idea. 17The participant also rated their confidence in their novelty assessment.The participant then completed a survey regarding their experience with the idea-generation task.This included seven-point Likerttype questions about their familiarity with the assigned topic and if they encountered concepts that they had not previously heard about or encountered in the context of the ideation topic.It also included the well-established Creativity Support Index (CSI) questionnaire [11].In the survey for the second tool, the participant also answered questions for each pair of CSI factors to determine which factors they considered most important, as is standard for the CSI.The coordinator then spent up to around five minutes engaging the participant in a semi-structured interview about their idea generation experience.</p>
<p>Moving on to the idea-novelty-evaluation task, the participant opened the 'Idea Novelty Evaluation' tab, and the coordinator provided an overview of this portion of the tool.The participant spent five minutes evaluating their two favorite ideas for novelty.For each idea, they provided a final seven-point Likert-type rating of perceived novelty and confidence in their novelty assessment.Finally, the coordinator spent up to around three minutes conducting a semi-structured interview about the participant's idea evaluation experience. 18</p>
<p>USER STUDY RESULTS</p>
<p>We analyzed participants' survey responses, interaction logs, and semi-structured interview responses from the user study. 19We analyzed the interview responses using inductive thematic analysis [7].We refer to participant by their unique ID number, research area, and tool they were using at the time (e.g., P1-HCI-treatment). 17There were two instances in which a participant had only saved one idea in the 20 minutes allotted.In this case, we asked them to select their next favorite idea in order to proceed with two favorite ideas. 18All survey and interview questions may be found in the supplementary materials. 19Survey responses and consenting participants' interaction logs are provided in the supplementary materials.</p>
<p>RQ1: Creativity Support in Idea Generation</p>
<p>To compare participants' responses to the Creativity Support Index (CSI) questionnaire after using Scideator versus the baseline tool, we conducted a Wilcoxon signed-rank test 20 .As shown in Figure 7a, participants experienced significantly more creativity support with Scideator (M=70.50,Q1=57.50,Q3=79.00)compared to the baseline (M=61.00,Q1=42.25,Q3=71.50)(Wilcoxon Signed-Rank Test, V=208.50, p&lt;.01) 21 .Breaking the CSI down into its factors, we saw that participants benefited most from Scideator in terms of exploration, followed by expressiveness.Most participants also benefited from Scideator in terms of enjoyment and their results being worth the effort, but participants did not experience much of a difference from the baseline with respect to immersion (Figure 7b).Looking at which factors participants found most important for the task of generating ideas (Figure 7c), we observed that participants chose exploration over other factors the most, followed by resultsworth-effort, expressiveness, enjoyment, and finally immersion.Thus, Scideator largely benefited the participants in the manners that were most important to them.</p>
<p>Exploration</p>
<p>Factor.Scideator helped participants to explore different ideas, the most important CSI factor according to our participants.However, looking at participants' 7-point Likert-type ratings of their favorite ideas' average newness to them, we see that the median participant had only a slightly higher rating in the treatment than the baseline (treatment-baseline: M=0.50, Q1=-0.375,Q3=0.875).Nonetheless, in the interviews, when participants commented that they found new concepts in the baseline condition, it was most often from the input papers rather than the tool's output (6 of 8 participants).On the other hand, in the treatment condition, participants who identified new concepts cited the tool's facets or ideas as the source (6 of 6 participants).For example, P18-HCItreatment shared, "When I thought of human-AI collaboration in art, for example, I did not think about also supporting artistic pursuits of students....When I was thinking about the topic, I thought more about... a human prompting an AI for generating images or for image exploration which is more related to the papers that were given.." Meanwhile, P5-NLP-baseline reflected, "The papers themselves were really interesting, but I don't think the tool generated anything super beyond a synthesis of the ideas that were in those three papers."</p>
<p>As four participants noted in their interviews, the facet-level interaction may have also supported exploration by providing more transparency to help them understand the idea generation process.For example, P5-NLP-treatment reflected, "I think the first thing that I noticed was that it was very easy to context switch.That was my main problem with the [other] tool before.I couldn't figure out which idea dealt with what aspect of the research that I was engaging with.Very easy to do that here."6.1.2Expressiveness and Enjoyment Factors.Scideator also helped participants to express and enjoy themselves while generating ideas.In interviews, 14 participants noted that Scideator's facet-level interaction, designed to help researchers express their ideas, was useful or interesting.In particular, seven participants appreciated how it provided them more control over the idea generation process.For instance, P11-NLP-treatment explained, "I like this tool better because it sort of distilled the different aspects of the input papers into very concrete blocks that you could plug into each other.... it's just that the information was presented in this tool... in a more digestible manner, and that helped combine information across papers better."Furthermore, as described in Section 6.2.2, participants included custom instructions to the LLM more in the baseline tool, indicating that Scideator helped them to express their thoughts without the need to type a prompt themselves.</p>
<p>Also of note, after using the baseline tool, four participants said that they wanted a way to input more papers to better express themselves, and five more felt limited by the three input papers.P14-NLP-baseline, for example, "would have liked to add a different paper because it felt like I had exhausted... the creativity in the system to some extent."While participants could add information from papers to their custom instructions, there was no system feature for adding more papers to the list of input papers.Future work may compare Scideator with a modified version of the baseline tool that allows users to add as many papers as they want for recombination.</p>
<p>6.1.3Results-Worth-Effort Factor.Participants generally found their results to be more worth the effort while using Scideator compared to the baseline tool.However, comparing Scideator and the baseline tool, there was little difference (treatment -baseline) in participants' average ratings of their favorite ideas in terms of perceived feasibility (M=0.00,Q1=-0.50,Q3=0.50), specificity (M=0.00,Q1=-0.88,Q3=0.50), and imaginativeness (M=0.00,Q1=-0.50,Q3=0.38).Meanwhile, the baseline tool performed better with respect to generating impactful ideas (M=-0.25,Q1=-0.50,Q3=0.00).Perhaps participants could more clearly see the potential impact of ideas that were grounded in just a few set input papers, which they reviewed, versus several papers, most of which were not reviewed by them.Comparing their baseline experience to their treatment experience, P10-HCI-baseline posited, "since now I know the paper, I read them, I kind of understand the vocabulary... it is easier for me to see where these ideas are coming from.So even when the ideas are written somewhat vaguely, I can still... imagine how that would pan out because I read the paper."Thus, Scideator's results being more worth the effort may predominantly be based on Scideator helping participants find ideas that utilize more concepts than what is already in the input papers or participants' minds (see Section 6.1.1).A sample of participants' favorite ideas from the two conditions is in Table 2. 6.1.4Immersion Factor.Overall, participants did not find Scideator more helpful than the baseline tool for becoming immersed in the idea-generation task.The interviews and interaction logs provide some reasons why this may be true.For one, Scideator presents the user with several features about which to learn.The cognitive demand of learning about these features might have prevented immersion.Four participants commented on the high cognitive load of using Scideator; P2-HCI-treatment commented, "I would say that it took me more mental effort to figure out how the tool [is used] rather than work with ideas."Furthermore, the system's latency in outputting ideas may have made it difficult to stay immersed in the task.Due to more complex prompting, the average latency for generating two ideas in Scideator was 22.04 seconds, compared to 15.21 seconds in the baseline tool. 22.2 RQ2: Idea Generation Patterns both the baseline tool and Scideator in terms of their input granularity.Fourteen participants found ScideatorÅ› affordance for facetlevel interactions useful or interesting, noting increased control (Section 6.1.2) and greater transparency (Section 6.1.1)as advantages of the facet-level interaction.On the other hand, five participants appreciated the baseline's paper-level interactions in addition to or more than Scideator's facet-level interactions.Three of these participants liked the paper-level interaction as it felt more directly connected to the literature.P22-NLP-baseline explained, " I think papers for me were more natural than facets....I think to me it's more like a map of literature, so I could see it more with papers.. " Three participants thought a combination of the two tools would be helpful.Two participants even proposed distinct roles for the two tools: divergent-ideation for Scideator and convergent-ideation for the baseline.P7-HCI-treatment shared, "In the [baseline tool], I started from a broader view and then I narrowed it down.Here [in Scideator], I started from a very specific thing and then I tried to add new facets or ideas so that I can expand the idea.So you see the other process is elimination process, here I was trying to expand."6.2.2 Lower Need for Custom Instructions with Scideator.When using Scideator, participants most often did not provide custom instructions to the LLM for generating either of their two favorite ideas (M=0.00,Q1=0.00,Q3=1.00 of 2 ideas).On the other hand, when using the baseline, the median participant provided custom instructions for generating 1.50 of their two favorite ideas (Q1=1.00,Q3=2.00).This indicates participants were able to utilize the scaffolding in the treatment to express themselves rather than have to type their own custom instructions.P1-HCI-treatment commented, "I didn't need to add any custom instructions because these [facets] served like custom instructions."6.2.3 Facets Used for Saved Ideas in Scideator.In their interviews, seven participants described how they appreciated the ability to control their ideas' generation through the facet-level interaction (Section 6.1.2).This aligns with participants' proclivity to select facets for their favorite ideas themselves.Participants' favorite ideas more often included evaluations, mechanisms, and especially purposes selected by themselves rather than the LLM (Figure 8a).Given that participants were assigned ideation topics, P18-HCI-treatment explained why participants may have decided to prioritize selecting purposes themselves: "I think the purpose is the most relevant to the topic.So within an area, there can be many ways of doing the same tasks, but the task is ultimately what defines the area."</p>
<p>Participants tended not to find the evaluation facets as helpful as the purpose and mechanism facets.Four participants commented that they found the evaluation facet unimportant.P13-HCItreatment elaborated, "For evaluation, I really don't think it's necessary for me because once you have the problem, you have the solution.In addition, we explored how participants utilized facets of different distances from the input.We observed that participants were reluctant to use far facets, particularly for an idea's purpose (Figure 8b).Participants' primary reason for avoiding far facets was that they were not relevant enough to the ideation topic, though four participants found the far facets helpful for discovering different ideas.As an example, P9-NLP-baseline commented, "that very near, near, far kind of thing... it kind of adds some sort of discovery factor."</p>
<p>RQ3: Confidence in Idea Novelty Assessment</p>
<p>For this research question, we include the 17 (of 22) participants who completed the idea-novelty-evaluation task with the intended task time and setup. 23To compare how participants' confidence in their novelty assessments changed after using Scideator versus the baseline tool for idea novelty evaluation, we conducted a sign test 24 .</p>
<p>We found that Scideator's novelty checker did not significantly improve participants' confidence in their novelty assessments (Sign Test, S = 5.00, p=n.s.).However, we also conducted a between-subjects comparison of ideas that were 1) classified as novel by the treatment, 2) classified as not-novel by the treatment, and 3) not classified by the baseline (Figure 9).Even though the change in participants' confidence in their novelty assessments was not meaningfully different across the three situations, the change in participants' perceived idea novelty was.We observed a trend in which participants who received a not-novel tool classification of their idea made more change to their own novelty assessments than participants who received a novel tool classification or no tool classification of their idea (Figure 9f).</p>
<p>This suggests participants' novelty assessments are most impacted when Scideator classifies an idea as not novel.Given that it is straightforward to verify that an idea is not novel using the idea's related papers provided by Scideator, it makes sense that it would be more likely to affect people's classifications.Still, it is not guaranteed that people would agree with the classification after taking some time to review its related papers and reason.Thus, we have evidence that the novelty checker is useful for judging ideas that the classifier deems "not novel, " but not necessarily for judging ideas that it classifies as "novel."P17-HCI-treatment echoed this sentiment: "Seeing a list of related work is very helpful for giving you the context.It was very convincing in the case of telling me that an idea was not novel....When it provides [a novel classification], it's less convincing but is helpful."Three participants noted that the papers retrieved by the tool to evaluate an idea for novelty were more useful to them than the tool's novelty classification or reason.P8-NLP-treatment provided a rationale: "I didn't really paid much attention to these reasons because reasons can be kind of made up to explain why their generation is novel.So, I kind of relied more on the references that it retrieved."</p>
<p>DISCUSSION</p>
<p>Grounding mixed-initiative scientific ideation in researchpaper facets shows promise for supporting idea generation.We introduced Scideator, a mixed-initiative, LLM-powered tool for scientific ideation through research-paper facet recombination.While prior work established the utility of extracting purpose and mechanism facets from research papers for identifying scientific analogies [8,33,55], we are the first to apply this framework to a human-LLM interaction for scientific ideation, offering support not only in finding analogous papers but also in generating paper facets and recombining those facets in research ideas.Aligned with our goal of supporting divergent ideation [15,60], results from our within-subjects user study showed that participants experienced significantly more creativity support when using Scideator as opposed to a strong baseline tool.Through semi-structured interview responses, we observed that participants appreciated being able to mix and match relevant purposes and mechanisms for research idea generation.</p>
<p>While we posit that facet-based interaction is an essential mechanism for ideation, we are aware that it is not the only one.Some participants also valued paper-level interactions, through which they could generate ideas from a set of selected papers rather than paper facets.Papers are directly connected to the literature and compress a great deal of information into a single entity.We believe that tools that provide users opportunities to interact with ideas on different levels of granularity and abstraction (e.g., papers, facets, concepts) will lead to richer interactions and better outcomes.</p>
<p>For mixed-initiative idea novelty assessment, our results suggest that providing related papers to the idea is key, and a 'not novel' classification may be more helpful than a 'novel' one.When commenting on the benefits of Scideator's idea novelty checker, participants more often mentioned the relevant papers that the tool would surface, rather than the tool's novelty classifications or classification reasons.Participants appreciated being efficiently provided with preliminary means to conduct the novelty assessment themselves, whereas they sometimes did not trust or agree with the tool's novelty classifications.That said, 'not-novel' classifications were seemingly the most utilized classifications, as participants noted they could easily verify whether or not the related papers render the idea not novel.Indeed, participants changed their novelty assessment the most when they received a 'not-novel' classification from Scideator, as opposed to a 'novel' classification from Scideator or no classification from the baseline tool.Future work may explore the benefits of a mixed-initiative tool that provides the best argument for a 'not-novel' classification regardless of the input idea.</p>
<p>More support is needed for generating ideas that are both very new to the scientist and capture the scientist's interest.Although participants experienced more creativity support with Scideator, there is room for improvement in terms of helping scientists to come up with ideas that are very new to them yet still relevant to their ideation topic.Participants' favorite ideas were not meaningfully newer to them when using Scideator versus the baseline.Using far facets might have helped them to discover ideas more new to them, but some participants described avoiding far facets because they considered them too irrelevant to the assigned ideation topic.Correspondingly, we observed that participants did not save ideas with far facets as much as ideas with input and near facets.By avoiding distant facets, scientists may miss opportunities for generating ideas that are completely new to them, helping them think of more ideas that would never have occurred to them otherwise.While we provide short descriptions of facets in Scideator, we could take a step further with in-context questionanswering to help users understand a facet to whatever degree is necessary.To help users recognize the utility of unfamiliar facets and the feasibility of associated ideas, the tool could also allow users to ask questions about how they might use the facet in general or in relation to a particular idea.In addition, the tool could be more transparent and provide users the analogy between two papers that it used to generate an idea.This could help users to more easily see the connection between the input or near facets and the far facets.</p>
<p>LIMITATIONS</p>
<p>Our study has a few limitations.First, the system was set up for computer science research ideas, and all participants had some experience with LLMs.Future work may investigate how scientists in other research areas with different LLM-familiarity might work with a tool like Scideator.Second, Scideator currently looks for analogies of varying distance within a broad research area (i.e., computer science), but future work could explore extending to analogies across research areas.Third, there were only 22 participants.Given the small sample size, future work may validate the results observed here with larger samples.Fourth, participants had limited time to interact with the tools-not enough to master a system with multiple steps like Scideator á¸žuture studies may explore how scientists utilize a tool like Scideator over a longer period of time.Fifth, we compared Scideator to a strong baseline-another LLM-powered tool for scientific ideation.However, it would be interesting to explore how interaction with Scideator compares to other tools for scientific ideation such as CoQuest [41].Sixth, due to latency constraints, the system only extracted papers' titles and abstracts and only utilized 10 relevant papers from Semantic Scholar's database to assess the novelty of an idea.Future work could look into how scientists interact with the system when it has access to more paper text and relevant papers.Seventh, to keep the study relatively simple, we provided three papers to the tools as input.Future work may evaluate how the interaction changes with more input papers.Eighth, to keep the study controlled, we assigned ideation topics to participants rather than letting them select their own.Future work may examine how the research area, familiarity, and recency of input papers impacts the experience and outcome of working with Scideator.</p>
<p>CONCLUSION</p>
<p>We presented Scideator, a novel mixed-initiative tool for scientific ideation that extracts key facets (purposes, mechanisms, and evaluations) from a set of input and analogous papers.Users can explore different recombinations of these facets, synthesized in succinct ideas.They can also use Scideator to evaluate and iterate on ideas' novelty.We introduced three LLM-powered retrieval-augmented generation (RAG) modules to support Scideator's workflow: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty Checker.We found that, computer-science researchers experienced significantly more creativity support with Scideator than with a strong baseline, particularly in terms of exploring different ideas, which the researchers considered most important for idea generation.</p>
<p>â†©â†’ â†©â†’</p>
<p>Present the purpose, mechanism, and evaluation of each text above.The purpose is the problem being addressed (e.g., to assist with writing scientific tweetorials, to answer questions over the scientific literature).</p>
<p>â†©â†’ â†©â†’</p>
<p>The mechanism is the proposed method to solve the problem (e.g., LLM chain-of-thought reasoning, AI-supported reverse outlining).</p>
<p>â†©â†’</p>
<p>The evaluation is the method used to determine how well the proposed solution solved the problem (e.g., lab user study, science QA benchmarks).A.2 Prompt to retrieve facets from papers associated with an analogous query.</p>
<p>â†©â†’</p>
<p>def promptFacetsFromQueryPapers(papers, corpus_ids, query, type="", query2="", type2=""):</p>
<p>â†©â†’ prompt = f""" TEXTS:""" for id in range(0,len(corpus_ids)): prompt += f""" Text { id + 1} Title: {papers[corpus_ids[id]]["title"]} Abstract: {papers[corpus_ids[id]]["abstract"]}""" if "introduction" in papers[corpus_ids[id]]: prompt += f""" Background: {papers[corpus_ids[id]]["introduction"]}""" prompt += f""" INSTRUCTIONS: You are ScientistGPT, an intelligent assistant that helps researchers come up with understandable, relevant, specific, feasible, and novel research ideas.</p>
<p>â†©â†’ â†©â†’</p>
<p>Present the purpose, mechanism, and evaluation of each text above."""if type: prompt += f""" The {type} should be relevant to but not a copy of the following query: {query}."""â†©â†’ else: prompt += f""" The facets should be relevant to but not a copy of the following query: {query}."""â†©â†’ if type2: prompt += f""" The {type2} should be relevant to but not a copy of the following query: {query2}."""â†©â†’ prompt += f""" The purpose is the problem being addressed (e.g., to assist with writing scientific tweetorials, to answer questions over the scientific literature).</p>
<p>â†©â†’ â†©â†’</p>
<p>The mechanism is the proposed method to solve the problem (e.g., chain-of-thought from large language models, AI-supported reverse outlining).</p>
<p>â†©â†’ â†©â†’</p>
<p>The evaluation is the method used to determine how well the proposed solution solved the problem (e.g., lab user study, science QA benchmarks).</p>
<p>â†©â†’</p>
<p>Examples of bad vs good definitions:</p>
<p>-facet: longitudinal study.bad: a study that evaluates the tool Toolio over the course of a year --&gt; good: a study that takes place over a long period of time extending at least multiple days</p>
<p>â†©â†’ â†©â†’</p>
<p>-facet: Toolio for creative writing.bad: Toolio implements SLM for generating creative writing --&gt; good: a mixed-initiative tool that uses large language models to scaffold the process of writing creative short stories by implementing the Standard Learning Method</p>
<p>â†©â†’ â†©â†’ â†©â†’</p>
<p>-facet: to help users better understand black-box models.bad: to help users better understand AI-Bot-360 --&gt; good: to help users better understand how AI models work when their algorithm cannot be fully known</p>
<p>â†©â†’</p>
<p>Make sure all information is faithful to the associated text.</p>
<p>It is important that you follow the answer format provided below!""" prompt += f""" Next, write down all {len(designated_papers)*len(analogous_papers)} possible analogies between the purpose/mechanism of a designated paper and the purpose/mechanism of an analogous paper."""
FORMAT</p>
<p>â†©â†’ â†©â†’</p>
<p>prompt += f""" Example Analogy: The purpose "to expand exploration of creative design spaces" is to the mechanism "structured generation framework" as the purpose "to enhance personalized mathematics learning" is to the mechanism "guiding and adaptive prompts" because both relationships involve providing flexible yet structured experiences in order to support personalized and useful knowledge acquisition.</p>
<p>â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’</p>
<p>Also, come up with one short research idea associated with each analogy.Some should combine the designated paper's purpose with the analogous paper's mechanism, and others should combine the analogous paper's purpose with the designated paper's mechanism.</p>
<p>â†©â†’ â†©â†’ â†©â†’</p>
<p>Next, select the {idea_number} best analogies that inspire research ideas that meet the idea requirements below."""â†©â†’ if set_eval: prompt += f"""</p>
<p>Finally, for one analogy, create a research idea that combines the purpose from the analogous paper, the mechanism from the designated paper, and one of the evaluation options below in an imaginative and smart manner.</p>
<p>â†©â†’ â†©â†’</p>
<p>For the other analogy, create a research idea that combines the purpose from the designated paper, the mechanism from the analogous paper, and one of the evaluation options below in an imaginative and smart manner."""â†©â†’ â†©â†’ else: prompt += f""" Finally, for one analogy, create a research idea that combines the purpose from the analogous paper, the mechanism from the designated paper, and the evaluation from either paper in an imaginative and smart manner.prompt += f""" FACET REQUIREMENTS:</p>
<p>The purpose/mechanism/evaluation IDs should be written VERBATIM from what is provided.</p>
<p>â†©â†’</p>
<p>Do NOT make up facet IDs that are not explicitly given above.</p>
<p>IDEA REQUIREMENTS: 1. Understandability:: 1a.Each idea should be logical.1b.Each idea should be grammatically correct.1c.Each idea should be self-contained and should not require researchers to have read the provided papers.For example, saying "Use Tool X" is not self-contained because the researcher might not have read the relevant paper to know what Tool X is.Instead, you could say "Use a tool to do Y."</p>
<p>â†©â†’ â†©â†’ â†©â†’</p>
<p>1d.If an aspect of the idea may not be familiar to someone studying computer science broadly, make sure the idea describes the aspect enough for them to understand it.</p>
<p>â†©â†’ â†©â†’</p>
<ol>
<li>Relevance:: 2a.Each idea should be at least somewhat relevant to the overarching ideation topic: {topic}.This means you may or may not have to adjust the purpose to be relevant to the ideation topic.For example, if the purpose is "to support human-AI creativity" and the topic is "scientific idea generation", then the purpose is relevant enough to the topic and does NOT need to be adjusted.On the other hand, if the selected purpose is "to support bird identification" and the topic is "scientific idea generation", you might adjust the purpose to "to support scientific idea identification" or "to support identifying different types of scientific ideas."That said, it is important to keep as much of the purpose's meaning as possible if/when adjusting it to be relevant to the ideation topic.
â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’
2b. Ideas should NOT refer to analogies.</li>
</ol>
<p>Specificity::</p>
<p>3a.Each idea should be described in 100 to 150 words (expanded version 200 to 250 words).90% of the idea should focus on describing how the mechanism will be used to address the purpose in a novel manner.</p>
<p>â†©â†’ â†©â†’</p>
<p>3b.Each idea should be as specific as possible given the word limit.3c.Each idea should be focused and precise.For example, if the idea involves a visualization feature, describe WHAT kind of visualization it is and HOW it will be used.</p>
<p>â†©â†’ â†©â†’</p>
<p>3d.Each idea should be specific such that the reader understands how the idea would be implemented and what direction it would take.</p>
<p>â†©â†’</p>
<p>3e.Each idea should be very specific in describing HOW the mechanism will be implemented to address the purpose in a novel manner.For instance, an idea saying to 'apply a faceted representation to clinical data, creating a multidimensional profile of each patient, integrating medical history, genetic information, lifestyle factors, and current health data' is not novel because prior work has already looked into creating multidimensional patient profiles.If the idea provided a more specific, imaginative description of HOW to create the multidimensional profiles and HOW to apply it to clinical decision-making, then it could be novel.
â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’
3f.Each idea should be specific enough that it is significantly different from your prior ideas.For example, if the overarching ideation topic is "scientific ideation", the purpose is "to support culinary ideation", and the mechanism is "scientific research graphs", you might adjust the purpose to "to support scientific ideation."Meanwhile, if the overarching ideation topic is "culinary ideation", the purpose is "to support culinary ideation", and the mechanism is "scientific research graphs", you might adjust the mechanism to "culinary recipe graphs."That said, it is important to keep as much of the origianl purpose and mechanism as possible if/when adjusting them to make sense together.
â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’
4d.Each idea's proposed evaluation should not contradict the rest of the idea.For example, an idea talking about supporting healthcare workers that has a user study evaluation should propose a "user study with healthcare workers," rather than a "user study with teachers" or a "user study with engineers."â†©â†’ â†©â†’ â†©â†’ â†©â†’ 5. Novelty:: 5a.Each idea should be unique as well as ingenious, imaginative, or surprising.</p>
<p>â†©â†’</p>
<p>5b.Each idea should NOT have been covered in prior work.In other words, each idea should be SIGNIFICANTLY different from prior work, including but not limited to the papers below and the summary of prior work below.</p>
<p>â†©â†’ â†©â†’</p>
<p>5c.For an idea to be novel, it is not enough for the idea not to have been thoroughly covered in prior work.It must NEVER have been covered in prior work.</p>
<p>â†©â†’ â†©â†’</p>
<p>5d.Each idea should NOT be an obvious extension of prior work but rather CLEARLY DIFFERENT from prior work.For instance, an idea that simply says to 'implement continuous AI support to facilitate discovery of scholars by dynamically updating and refining researcher profiles based on real-time academic contributions' is not novel.The idea is not novel because prior work has investigated facilitating discovery of scholars through AI support, and making the AI support continuous is an obvious extension.It is not ingenious, imaginative, or surprising.If the idea were more specific and presented a suprising method describing HOW to make this AI support very timely (e.g., utilizing academic Twitter trends), that would make the idea novel."""prompt += f""" Evaluation Text: <selected evaluation option text here> Evaluation ID: <selected evaluation option ID here>""" prompt += f""" Imaginative Twist to Add to Facet Combination: The imaginative and smart twist that I will add to the facet combination of <analogous purpose text here> with <designated mechanism text here> will be <imaginative twist here>.
â†©â†’</p>
<p>â†©â†’ â†©â†’</p>
<p>How Idea will be Relevant to {topic}: The idea will be relevant to {topic}, as it will address <thing relevant to {topic}>.</p>
<p>â†©â†’</p>
<p>Initial Research Idea: <idea inspired by facets here (100-150 words)> Issues with Initial Idea: <describe how initial idea doesn't meet idea requirements here (50-100 words)></p>
<p>â†©â†’</p>
<p>How to Address Issues: <describe how will resolve issues here (50-100 words)> New Research Idea: <updated idea inspired by facets here (100-150 words)> Expanded New Research Idea: <expanded updated idea inspired by facets here (200-250 words)> â†©â†’ Best 2. Analogy: The designated purpose <purpose text from designated paper here> is to the designated mechanism <mechanism text from designated paper here> as the analagous purpose <purpose text from analogous paper here> is to the analogous mechanism <mechanism text from analogous paper here> because both relationships involve <common relationship description here>.</p>
<p>â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’</p>
<p>Purpose Text: <purpose text from designated paper here> Purpose ID: <purpose ID from designated paper here> Mechanism Text: <mechanism text from analogous paper here> Mechanism ID: <mechanism ID from analogous paper here>""" if not set_eval: prompt += f""" Evaluation Text: <evaluation text from either paper here> Evaluation ID: <evaluation ID here>""" else: prompt += f""" Evaluation Text: <selected evaluation option text here> Evaluation ID: <selected evaluation option ID here>""" prompt += f""" Imaginative Twist to Add to Facet Combination: The imaginative and smart twist that I will add to the facet combination of <designated purpose text here> with <analogous mechanism text here> will be <imaginative twist here>.</p>
<p>â†©â†’ â†©â†’ â†©â†’</p>
<p>How Idea will be Relevant to {topic}: The idea will be relevant to {topic}, as it will address <thing relevant to {topic}>.</p>
<p>â†©â†’</p>
<p>Initial Research Idea: <idea inspired by facets here (100-150 words)> Issues with Initial Idea: <describe how initial idea doesn't meet idea requirements here (50-100 words)></p>
<p>â†©â†’</p>
<p>How to Address Issues: <describe how will resolve issues here (50-100 words)> New Research Idea: <updated idea inspired by facets here (100-150 words)> Expanded New Research Idea: <expanded updated idea inspired by facets here (200-250 words)> â†©â†’ ANSWER: """ return prompt B.2 Prompt to obtain ideas using P-or-M method.</p>
<p>def promptFillAnalogyIdeas(</p>
<p>prior_ideas="", custom_instructions="", number=6, idea_number=2 ): prompt = f""" INSTRUCTIONS: You are ScientistGPT, an intelligent assistant that helps researchers come up with understandable, relevant, specific, feasible, and novel research ideas.</p>
<p>â†©â†’ â†©â†’</p>
<p>First, read the summary of prior work and the papers below.That way, you will know what has already been done in related research."""if prior_ideas: prompt += f""" Next, read your prior ideas below in order to make sure you do not generate similar ideas to those that you have already proposed."""â†©â†’ if number: prompt += f"""</p>
<p>Next, come up with {number} different analogies between the purpose/mechanism of a set-1 paper below and the purpose/mechanism of a set-2 paper below."""â†©â†’ â†©â†’ else: prompt += f"""</p>
<p>Next, write down all {len(designated_papers)*len(analogous_papers)} possible analogies between the purpose/mechanism of a set-1 paper and the purpose/mechanism of a set-2 paper."""</p>
<p>â†©â†’ â†©â†’</p>
<p>prompt += f""" If a set-1 paper does not have a purpose or mechanism, create an appropriate one for the sake of the analogy.</p>
<p>â†©â†’</p>
<p>If possible, the paper from which the purpose comes must have a different distance than the paper from which the mechanism comes.</p>
<p>â†©â†’</p>
<p>Example Analogy: The purpose "to expand exploration of creative design spaces" is to the mechanism "structured generation framework" as the purpose "to enhance personalized mathematics learning" is to the mechanism "guiding and adaptive prompts" because both relationships involve providing flexible yet structured experiences in order to support personalized and useful knowledge acquisition.</p>
<p>â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’</p>
<p>Also, come up with a short research idea based on each analogy that combines the purpose from one paper with the mechanism from the other paper.</p>
<p>â†©â†’</p>
<p>Next, select the {idea_number} best analogies that inspire research ideas that meet the idea requirements below.</p>
<p>â†©â†’</p>
<p>""" if relevant_purposes: if set_eval: prompt += f""" Finally, for each analogy, create a research idea that combines the purpose from the set-1 paper with the mechanism from the set-2 paper, and one of the evaluation options below in an imaginative and smart manner."""â†©â†’ â†©â†’ else: prompt += f""" Finally, for each analogy, create a research idea that combines the purpose from the set-1 paper, the mechanism from the set-2 paper, and the evaluation from one of those two papers in an imaginative and smart manner."""</p>
<p>â†©â†’ â†©â†’ â†©â†’</p>
<p>else: if set_eval: prompt += f""" Finally, for each analogy, create a research idea that combines the mechanism from the set-1 paper, the purpose from the set-2 paper, and one of the evaluation options below in an imaginative and smart manner."""â†©â†’ â†©â†’ else: prompt += f""" Finally, for each analogy, create a research idea that combines the mechanism from the set-1 paper, the purpose from the set-2 paper, and the evaluation from one of those two papers in an imaginative and smart manner."""</p>
<p>â†©â†’ â†©â†’ â†©â†’</p>
<p>prompt += f""" prompt += f"""</p>
<p>FACET REQUIREMENTS:</p>
<p>The purpose/mechanism/evaluation IDs should be written VERBATIM from what is provided.</p>
<p>â†©â†’</p>
<p>Do NOT make up facet IDs that are not explicitly given above.</p>
<p>IDEA REQUIREMENTS: 1. Understandability:: 1a.Each idea should be logical.1b.Each idea should be grammatically correct.1c.Each idea should be self-contained and should not require researchers to have read the provided papers.For example, saying "Use Tool X" is not self-contained because the researcher might not have read the relevant paper to know what Tool X is.Instead, you could say "Use a tool to do Y."</p>
<p>â†©â†’ â†©â†’ â†©â†’</p>
<p>1d.If an aspect of the idea may not be familiar to someone studying computer science broadly, make sure the idea describes the aspect enough for them to understand it.</p>
<p>â†©â†’ â†©â†’</p>
<ol>
<li>
<p>Relevance:: 2a.Each idea should be at least somewhat relevant to the overarching ideation topic: {topic}.This means you may or may not have to adjust the purpose to be relevant to the ideation topic.For example, if the purpose is "to support human-AI creativity" and the topic is "scientific idea generation", then the purpose is relevant enough to the topic and does NOT need to be adjusted.On the other hand, if the selected purpose is "to support bird identification" and the topic is "scientific idea generation", you might adjust the purpose to "to support scientific idea identification" or "to support identifying different types of scientific ideas."That said, it is important to keep as much of the purpose's meaning as possible if/when adjusting it to be relevant to the ideation topic.
â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’
2b. Ideas should NOT refer to analogies.</p>
</li>
<li>
<p>Specificity:: 3a.Each idea should be described in 100 to 150 words (expanded version 200 to 250 words).90% of the idea should focus on describing how the mechanism will be used to address the purpose in a novel manner.</p>
</li>
</ol>
<p>â†©â†’ â†©â†’</p>
<p>3b.Each idea should be as specific as possible given the word limit.3c.Each idea should be focused and precise.For example, if the idea involves a visualization feature, describe WHAT kind of visualization it is and HOW it will be used.</p>
<p>â†©â†’ â†©â†’</p>
<p>3d.Each idea should be specific such that the reader understands how the idea would be implemented and what direction it would take.</p>
<p>â†©â†’</p>
<p>3e.Each idea should be very specific in describing HOW the mechanism will be implemented to address the purpose in a novel manner.For instance, an idea saying to 'apply a faceted representation to clinical data, creating a multidimensional profile of each patient, integrating medical history, genetic information, lifestyle factors, and current health data' is not novel because prior work has already looked into creating multidimensional patient profiles.If the idea provided a more specific, imaginative description of HOW to create the multidimensional profiles and HOW to apply it to clinical decision-making, then it could be novel.
â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’
3f.Each idea should be specific enough that it is significantly different from your prior ideas.For example, if the overarching ideation topic is "scientific ideation", the purpose is "to support culinary ideation", and the mechanism is "scientific research graphs", you might adjust the purpose to "to support scientific ideation."Meanwhile, if the overarching ideation topic is "culinary ideation", the purpose is "to support culinary ideation", and the mechanism is "scientific research graphs", you might adjust the mechanism to "culinary recipe graphs."That said, it is important to keep as much of the origianl purpose and mechanism as possible if/when adjusting them to make sense together.2a.Each idea should be at least somewhat relevant to the overarching ideation topic: {topic}.This means you may or may not have to adjust the purpose to be relevant to the ideation topic.For example, if the purpose is "to support human-AI creativity" and the topic is "scientific idea generation", then the purpose is relevant enough to the topic and does NOT need to be adjusted.On the other hand, if the selected purpose is "to support bird identification" and the topic is "scientific idea generation", you might adjust the purpose to "to support scientific idea identification" or "to support identifying different types of scientific ideas."That said, it is important to keep as much of the purpose's meaning as possible if/when adjusting it to be relevant to the ideation topic.3. Specificity:: 3a.Each idea should be described in 100 to 150 words (expanded version 200 to 250 words).90% of the idea should focus on describing how the mechanism will be used to address the purpose in a novel manner.</p>
<p>â†©â†’</p>
<p>â†©â†’</p>
<p>â†©â†’ â†©â†’</p>
<p>3b.Each idea should be as specific as possible given the word limit.3c.Each idea should be focused and precise.For example, if the idea involves a visualization feature, describe WHAT kind of visualization it is and HOW it will be used.</p>
<p>â†©â†’ â†©â†’</p>
<p>3d.Each idea should be specific such that the reader understands how the idea would be implemented and what direction it would take.</p>
<p>â†©â†’</p>
<p>3e.Each idea should be very specific in describing HOW the mechanism will be implemented to address the purpose in a novel manner.For example, if the overarching ideation topic is "scientific ideation", the purpose is "to support culinary ideation", and the mechanism is "scientific research graphs", you might adjust the purpose to "to support scientific ideation."Meanwhile, if the overarching ideation topic is "culinary ideation", the purpose is "to support culinary ideation", and the mechanism is "scientific research graphs", you might adjust the mechanism to "culinary recipe graphs."That said, it is important to keep as much of the origianl purpose and mechanism as possible if/when adjusting them to make sense together.{ "role": "system", "content": "You are ReviewerGPT, an intelligent assistant that helps researchers evaluate the novelty of their ideas.",â†©â†’ }, { "role": "user", "content": f"""You are given some papers similar to the proposed idea.Your task is to evaluate the idea's novelty using the related papers only.</p>
<p>â†©â†’</p>
<p>â†©â†’ â†©â†’</p>
<p>Types of novelty categories:</p>
<p>-Not Novel: The idea closely replicates existing work with minimal or no new contributions.</p>
<p>â†©â†’</p>
<p>-Novel:</p>
<p>-The idea introduces new concepts or approaches that are not common in existing literature.</p>
<p>â†©â†’</p>
<p>-The idea uniquely combines concepts from existing papers, but this combination does not occur in any related papers.</p>
<p>â†©â†’</p>
<p>-A new application with same approach is also novel.</p>
<p>Instructions:</p>
<p>-Use the example reviews below to write a review for the provided idea by comparing it to the related papers.</p>
<p>â†©â†’</p>
<p>-Don't assume any prior knowledge about the idea.</p>
<p>-When referencing a related paper, then use paper id in the review, mention it in this format: [5].The paper ID is present between Paper ID [<paper_id>]: Title.</p>
<p>â†©â†’ â†©â†’</p>
<p>-After reviewing, classify the idea into one of this category: novel or not novel.</p>
<p>â†©â†’</p>
<p>-Make sure the generated review follows the format in example reviews provided below.</p>
<p>â†©â†’</p>
<p>-The review should be concise -around 60 to 100 words.</p>
<p>{incontext_part} Output Format: -Class: [novel / not novel] -Review: The idea is [novel / not novel] different because... """, }, {"role": "assistant", "content": "Sure, please provide the IDEA."},{"role": "user", "content": f"Here is the idea: {idea}"}, {"role": "assistant", "content": "Okay, now provide the related papers., "Title 2", "Title 3", "Title 4"] </titles> """, }, {"role": "assistant", "content": "Sure, please provide the IDEA."},{"role": "user", "content": idea}, ] return prompt C.3 Prompt to extract key facets from an IDEA for facet-based relevance re-ranking of papers.</p>
<p>def prompt_noveltyCheckLLMRerank_IdeaFacets(idea): prompt = [ { "role": "system", "content": "You are Research Idea Reviewer GPT, an intelligent assistant that helps researchers evaluate the novelty of their ideas.",â†©â†’ â†©â†’ }, { "role": "user", "content": """Your task is to extract key facets from a given idea to assist in re-ranking passages based on their relevance to the idea.These key facets should capture the essential elements of the idea, such as the application domain, purpose, mechanisms, methods, and evaluation metrics.</p>
<p>â†©â†’ â†©â†’ â†©â†’ â†©â†’</p>
<p>Instructions:</p>
<ol>
<li>Carefully read and understand the idea.2. Identify and list the key facets of the idea, including but not limited to:</li>
</ol>
<p>â†©â†’ -Application Domain: The specific field or area the idea pertains to.</p>
<p>â†©â†’</p>
<p>-Purpose/Objective: The main goal or intention behind the idea.</p>
<p>â†©â†’</p>
<p>-Mechanisms/Methods: The techniques or approaches proposed to achieve the purpose.</p>
<p>â†©â†’</p>
<p>-Evaluation Metrics: The criteria or measures used to assess the effectiveness of the idea.</p>
<p>â†©â†’</p>
<p>Examples:</p>
<p>, , Radensky et al.</p>
<p>Idea 1: Develop a system that uses a faceted representation of authors to understand food-health relationships by analyzing the sentiment of research papers and publications.The system will identify key authors in food and health research, map their sentiments towards various topics, and use this information to reveal hidden connections and trends.An experimental results showcase will evaluate the system's ability to uncover novel food-health relationships and its impact on interdisciplinary research.</p>
<p>â†©â†’</p>
<p>â†©â†’ â†©â†’</p>
<p>""", }, { "role": "assistant", "content": "Sure, please provide the research idea", }, { "role": "user", "content": f"""Here is the idea: <idea> {idea} </idea>.Please provide Key Facets to Look for in Passages for the provided idea between <facets> </facets> tags.</p>
<p>â†©â†’</p>
<p>""", }, ] return prompt C.4 Prompt to rank passages based on relevance to a query and its key facets.</p>
<p>def prompt_noveltyCheckLLMRerank(query, idea_priority_facets, num): return [ { "role": "system", "content": "You are RankGPT, an intelligent assistant that can rank passages above based on their provided priority and relevance to the query and its facets.",â†©â†’ â†©â†’ }, { "role": "user", "content": f"""I will provide you with {num} passages, each indicated by number identifier [].</p>
<p>â†©â†’</p>
<p>Your task is to rank the passages based on their relevance to the query idea and the provided priority:</p>
<p>â†©â†’ â†©â†’</p>
<p>""", }, { "role": "assistant", "content": "Can you provide an example idea, facets and how to rank passages?",â†©â†’ }, { "role": "user", "content": """Here is an example:</p>
<p>Idea
2] &gt; [1] &gt; [5] &gt; [3] &gt; [0] &gt; [8] &gt; [6] &gt; [7] &gt; [4] &gt; [9]
""", }, { "role": "user", "content": """Here is another example: <strong>Idea:</strong> Develop a system that uses sentiment analysis to detect political bias in news articles.The system will analyze language patterns and sentiments to identify biased reporting, and will be validated using a dataset of news articles over the past decade.You are ScientistGPT, an intelligent assistant that helps researchers come up with understandable, relevant, specific, feasible, and novel research ideas.</p>
<p>â†©â†’ â†©â†’</p>
<p>First, read the above prior work and summary of why the initial idea is not novel compared to the prior work.</p>
<p>â†©â†’</p>
<p>That way, you will know what has already been done in related research.</p>
<p>Second, provide 3 options for ideas that are more novel and useful than the initial idea.</p>
<p>â†©â†’</p>
<p>Each more novel/useful idea should utilize one new facet and remove one existing facet from the initial idea.</p>
<p>â†©â†’</p>
<p>The summary of facets available to add and to remove are above.Make each novel/useful idea as specific as possible but describe it in 100 to 150 words.</p>
<p>â†©â†’ FACET REQUIREMENTS:</p>
<p>The purpose/mechanism/evaluation IDs should be written VERBATIM from what is provided.</p>
<p>â†©â†’</p>
<p>Do NOT make up facet IDs that are not explicitly given above.</p>
<p>IDEA REQUIREMENTS: 1. Understandability:: 1a.Each idea should be logical.1b.Each idea should be grammatically correct.1c.Each idea should be self-contained and should not require researchers to have read the provided papers.For example, saying "Use Tool X" is not self-contained because the researcher might not have read the relevant paper to know what Tool X is.Instead, you could say "Use a tool to do Y."</p>
<p>â†©â†’ â†©â†’ â†©â†’</p>
<p>1d.If an aspect of the idea may not be familiar to someone studying computer science broadly, make sure the idea describes the aspect enough for them to understand it.</p>
<p>â†©â†’ â†©â†’</p>
<ol>
<li>Relevance:: 2a.Each idea should be at least somewhat relevant to the overarching ideation topic: {topic}.This means you may or may not have to adjust the purpose to be relevant to the ideation topic.For example, if the purpose is "to support human-AI creativity" and the topic is "scientific idea generation", then the purpose is relevant enough to the topic and does NOT need to be adjusted.On the other hand, if the selected purpose is "to support bird identification" and the topic is "scientific idea generation", you might adjust the purpose to "to support scientific idea identification" or "to support identifying different types of scientific ideas."That said, it is important to keep as much of the purpose's meaning as possible if/when adjusting it to be relevant to the ideation topic.3. Specificity:: 3a.Each idea should be described in 100 to 150 words (expanded version 200 to 250 words).90% of the idea should focus on describing how the mechanism will be used to address the purpose in a novel manner.</li>
</ol>
<p>â†©â†’</p>
<p>â†©â†’ â†©â†’</p>
<p>3b.Each idea should be as specific as possible given the word limit.3c.Each idea should be focused and precise.For example, if the idea involves a visualization feature, describe WHAT kind of visualization it is and HOW it will be used.</p>
<p>â†©â†’ â†©â†’</p>
<p>3d.Each idea should be specific such that the reader understands how the idea would be implemented and what direction it would take.</p>
<p>â†©â†’</p>
<p>3e.Each idea should be very specific in describing HOW the mechanism will be implemented to address the purpose in a novel manner.For instance, an idea saying to 'apply a faceted representation to clinical data, creating a multidimensional profile of each patient, integrating medical history, genetic information, lifestyle factors, and current health data' is not novel because prior work has already looked into creating multidimensional patient profiles.If the idea provided a more specific, imaginative description of HOW to create the multidimensional profiles and HOW to apply it to clinical decision-making, then it could be novel.</p>
<p>â†©â†’</p>
<p>â†©â†’</p>
<p>4b.Each idea should make sense scientifically and be grounded in the summary of prior work and papers noted below.</p>
<p>â†©â†’</p>
<p>4c.Each idea's purpose and mechanism should work well together.If necessary, adapt the purpose or mechanism to work well with the other.For example, if the overarching ideation topic is "scientific ideation", the purpose is "to support culinary ideation", and the mechanism is "scientific research graphs", you might adjust the purpose to "to support scientific ideation."Meanwhile, if the overarching ideation topic is "culinary ideation", the purpose is "to support culinary ideation", and the mechanism is "scientific research graphs", you might adjust the mechanism to "culinary recipe graphs."That said, it is important to keep as much of the origianl purpose and mechanism as possible if/when adjusting them to make sense together.</p>
<p>â†©â†’</p>
<p>None</p>
<p>The proposed system focuses on developing a robust medical information retrieval platform using sequential knowledge-guided prompting to simulate diagnostic pathways.By integrating electronic health records (EHRs) with established clinical guidelines, the system leverages prompts to dynamically identify and follow potential diagnostic routes, thereby enhancing the precision and relevance of information retrieval.The process begins by extracting key medical factors from EHRs and matching these with clinical pathway prompts to simulate diagnostic reasoning.This approach allows the system to adjust retrieval strategies based on real-time patient data, ensuring contextually relevant information is provided to healthcare professionals.The system's effectiveness will be rigorously evaluated using medical question-answering benchmarks, which will assess the accuracy and pertinence of information retrieval against standard medical queries.This evaluation not only measures the system's performance but also its ability to improve clinical decision-making processes.Ultimately, the research aims to bridge gaps in current medical retrieval systems by offering a more transparent and context-aware tool for healthcare providers, significantly enhancing the explainability and utility of LLMs in the medical domain.In this novel approach, we apply a residual learning framework to AI design tools, treating designer feedback as residuals to refine specific design elements such as color schemes and layout structures.This framework allows designers to provide targeted feedback easily, which the AI tool uses to iteratively improve its suggestions.The process starts with the AI generating an initial design, which the designer can annotate with feedback.This feedback is then incorporated as residuals, prompting the AI to make adjustments that align more closely with the designer's vision.The feedback loop is engineered to be highly intuitive, integrating seamlessly into the designer's workflow, thus fostering a more collaborative and efficient design process.An empirical performance analysis will evaluate the effectiveness of this approach, measuring improvements in design quality and collaboration efficiency.P-and-M, Treatment Use different verification approaches from NLI, math, program to ensure the explanation follows a logical consistency Create an innovative platform that enhances the validation of LLM-generated mathematical explanations by integrating a mathematical logic verifier with visual proof representations.The platform will employ the verifier to ensure explanations adhere to logical principles, assessing each step for accuracy.Simultaneously, it will generate graph-based visual proofs, providing intuitive visual cues for each stage of the reasoning process.These representations will guide users through complex explanations, making abstract concepts more accessible and engaging.The system will also feature interactive elements that allow users to explore different parts of the proofs, deepening their understanding.This approach will be evaluated through comprehensive quantitative and qualitative analysis, assessing improvements in explanation accuracy, user comprehension, and satisfaction.By combining rigorous logic verification with engaging visual aids, the research aims to significantly improve the interpretability and reliability of LLMgenerated mathematical content, fostering user trust and learning.P-and-M, Treatment AI tools for education to personalize vocabulary learning (very near) aigenerated virtual speakers (very near) participant feedback analysis (very near, unselected) make the idea more focused and specific This innovative platform leverages AI-generated virtual speakers to create personalized vocabulary learning experiences.Users upload personal images, and the AI analyzes these images to identify objects, settings, and potential vocabulary links.The virtual speakers then generate dialogue scenarios around these elements, allowing learners to engage with new vocabulary in a personally relevant context.For example, an image of a beach vacation might lead to dialogues about seaside activities, weather, and local culture, enriching the vocabulary acquisition process.This personalized approach not only aids retention by linking words to familiar contexts but also enhances engagement through interactive AI-driven dialogues.Participant feedback analysis will assess the platform's effectiveness in improving vocabulary acquisition and learner satisfaction, guiding further refinements.By integrating AI with personalized content, this research aims to transform vocabulary learning into a more engaging and contextually meaningful experience.This AR-enabled mobile application is designed to engage children with their local environment while providing a robust educational platform about biodiversity and ecology.The AI component will analyze user interactions and location data to curate a personalized educational journey.For a child in an urban environment, the app might focus on urban wildlife and plants, providing detailed information, conservation tips, and interactive quizzes tailored to their locality.For children in rural settings, it might explore more diverse ecosystems, encouraging them to document and learn about various species.. Table 6: A sample of participants' favorite ideas from the user study, presenting the expanded version of each idea rather than the shorter version.The treatment ideas are generated with the different methods of the Faceted Idea Generator module, depending on whether the participant selected any purposes or mechanisms.</p>
<p>Figure 1 :
1
Figure1: The Scideator workflow.1) The interaction begins with the user providing an ideation topic and set of input papers as a starting point for ideation.2) Scideator responds by retrieving analogous papers to the input papers and extracting facets (purpose, mechanism, and evaluation) from the input and analogous papers.(The evaluation facets are omitted in the figure for clarity, as it is not part of the main logic.)3) The user then selects paper facets as well as adds their own facets for which they want to generate ideas.4) Scideator recombines these selected facets into ideas with one purpose and one mechanism.If a purpose or mechanism facet is unspecified, the tool selects one.5) The user selects an idea to assess for novelty.6) Scideator classifies the idea as "novel" or "not novel" and provides a short rationale.7) The user reviews the novelty classification and adjusts it if they disagree.8) If the idea is deemed "not novel," Scideator suggests more novel ideas with one of the initial idea's facets replaced.</p>
<p>Figure 2 :
2
Figure 2: Scideator's cold start.Above, the user selects or adds facets to generate ideas.They can also generate more facets to consider, and add custom instructions for the idea generation.Below, the user peruses their ideas and evaluates an idea for novelty by clicking the search icon to its left.The ideation topic here is human-AI collaboration in art.</p>
<p>Figure 3 :
3
Figure 3: The Analogous Paper Facet Finder module.For a set of input papers, Scideator uses Semantic Scholar's API to retrieve similar papers (very near).It uses the input and very-near papers to create a summary of relevant works.Next, the tool extracts key facets from the input papers and determines the input papers' overarching purpose and mechanism, which it uses to come up with three queries for papers with an analogous purpose and mechanism.The queries are for analogous papers with varying distances from the input paper: same topic (near), same subarea (far), and different subarea (very far).Those queries are fed to the Semantic Scholar API to retrieve analogous papers.Finally, the facets of all the analogous papers are extracted by the LLM.</p>
<p>Figure 4 :
4
Figure 4: Scideator's novelty assessment modal for one idea, which presents the idea (a) as well as its facets (b), related papers (c), adjustable novelty classification (d), and adjustable classification reason (e).When the idea is classified as "not novel," the system provides a set of three suggestions for more novel ideas (f), each of which replace one of the idea's original facets.The ideation topic here is human-AI collaboration in art.</p>
<p>Figure 5 :
5
Figure5: The Idea Novelty Checker module follows a retrieve-then-re-rank approach for novelty evaluation.In Step 1, it gathers a comprehensive set of papers relevant to an idea.This includes papers originally used to generate the idea, related papers, and additional papers retrieved through keyword and title searches extracted directly from the idea, as well as snippet searches using the entire idea as input.In Step 2, a two-stage re-ranking process is applied, where an embedding-based ranking strategy filters the large collection to top- papers, followed by a facet-based LLM re-ranker to identify the top- most relevant papers.In Step 3, these top- papers are used to assess the idea's novelty, guided by in-context examples that evaluate novelty with grounded reasoning.In Step 4, if an idea is classified as "not novel" by the tool or user, the LLM generates three idea suggestions, each replacing a different facet in the original idea in order to make the idea more novel compared to the relevant papers.</p>
<p>Figure 6 :
6
Figure 6: The cold start of the baseline UI for the user study's idea-generation task.The ideation topic here is human-AI collaboration in art.</p>
<p>Figure 7 :
7
Figure 7: (a) The difference between participants' unweighted CSI scores for Scideator versus the baseline tool.Participants experienced significantly more creativity support with Scideator.(b) For each CSI factor, the difference between participants' ratings for Scideator versus the baseline tool.(c) How many times each CSI factor wins against other factors in terms of what is most important to participants while generating ideas.</p>
<p>Figure 8 :
8
Figure 8: (a) Participants more often opted to select their own facets rather than let the LLM select for them.(b) Participants used input facets and facets nearer to the input more than facets farther from the input.</p>
<ol>
<li>2 . 1
21
Benefits of Facet-Level and Paper-Level Interactions.In our study, participants can generate ideas from inputs with two levels of granularity: facet-level (Scideator condition) and paperlevel(baseline condition).Participants commented on benefits of , , Radensky et al.</li>
</ol>
<p>Automatically you know how to evaluate it, like what study you need, what kind of experiment you want to have, and what variables you are measuring."Thus, future work may investigate whether or not the presence of an evaluation facet is useful for mixed-initiative, facet-based generation of research ideas.</p>
<p>Figure 9 :
9
Figure9: Participants' average perceived idea novelty before (a) and after (b) utilizing their assigned tool for idea novelty evaluation, as well as the average change from initial to final perceived novelty (c).The baseline tool was the assigned tool when ideas had no classification, while Scideator was the assigned tool when ideas had a novel or not-novel classification.</p>
<p>too broad): recommendation system --&gt; good: collaborative filtering -bad (too broad): human-AI collaboration --&gt; good: human-generative AI co-planning â†©â†’ -bad (too broad): deep learning algorithm --&gt; good: topic modeling -bad (more than one mechanism that are uncombined): content-based AI explanations, social-based AI explanations --&gt; good: content-based and social-based AI explanations â†©â†’ â†©â†’ Examples of bad vs good evaluations: -bad (too specific, references the purpose): between-subjects 4x4 user study with 32 teachers --&gt; good: Wizard of Oz user study â†©â†’ -bad (too broad): questionnaire --&gt; good: NASA-TLX Index -bad (too broad): qualitative evaluation --&gt; good: semi-structured interviews â†©â†’ Follow the rules below for generating definitions of each facet.1.Should be up to 2 sentences.2. Replace proper nouns with their definitions.3. Replace jargon with their definitions.4. Write out acronyms. 5. Should be self-contained.Do NOT include information that is beyond the definition of the facet.â†©â†’ 6. Do NOT reuse the words already in the facet.</p>
<p>â†©â†’ 4 .
4
Feasibility:: 4a.A research lab with moderate resources should be able to carry out each idea.â†©â†’ 4b.Each idea should make sense scientifically and be grounded in the summary of prior work and papers noted below.â†©â†’ 4c.Each idea's purpose and mechanism should work well together.If necessary, adapt the purpose or mechanism to work well with the other.</p>
<p>â†©â†’ 4 .
4
Feasibility:: 4a.A research lab with moderate resources should be able to carry out each idea.â†©â†’ 4b.Each idea should make sense scientifically and be grounded in the summary of prior work and papers noted below.â†©â†’ 4c.Each idea's purpose and mechanism should work well together.If necessary, adapt the purpose or mechanism to work well with the other.</p>
<p>â†©â†’**</p>
<p>Query<strong> Idea: {query} Key facets to look in passages for ranking: {idea_priority_facets} Use the following criteria in order of priority for ranking the passaeges: â†©â†’ 1. </strong>Priority 1:<strong> Passages that closely match </strong>all<strong> key facets of the </strong>QUERY<strong> IDEA.â†©â†’ 2. </strong>Priority 2:<strong> Passages that match the </strong>application domain<strong> and </strong>purpose<strong> but may differ in mechanism or method.â†©â†’ 3. </strong>Priority 3:<strong> Passages that share a similar </strong>purpose<strong> or </strong>mechanism<strong> or </strong>evaluation<strong>, even if the application domain differs.â†©â†’ â†©â†’ 4. </strong>Priority 4:** Passages that partially match the application domain or address related topics but lack alignment with the purpose or mechanism.</p>
<p>Facets:<strong> -</strong>Application Domain<strong>: News articles analysis.-</strong>Purpose**: Detecting political bias through sentiment analysis.prompt += f""" INSTRUCTIONS:</p>
<p>Figure 10 :Figure 11 :
1011
Figure 10: Performance trends of test accuracy across prompts during prompt optimization with TextGRAD.Highlighted text shows unique instructions used to evaluate the novelty of ideas.The final test accuracy was 0.78125, showing that none of the optimized prompts (1 to 12) improved over the original.</p>
<p>Figure 12 :Figure 13 :
1213
Figure 12: contd.TextGrad Prompt Optimisation.</p>
<p>to see its description.Also, by clicking the facet's associated paper icon, the user can open the Semantic Scholar link to the paper from which the facet came.
,,Radensky et al.
mark</p>
<p>Table 2 :
2
A sample of participants' favorite ideas from the user study.The treatment ideas are generated with the different methods of the Faceted Idea Generator module, depending on whether the participant selected any purposes or mechanisms.</p>
<p>Scideator: Human-LLM Scientific Idea Generation and Novelty Evaluation Grounded in Research-Paper Facet Recombination , ,
STEP 1: Retrieve Candidate Relevant PapersSTEP 2: Select Most Relevant PapersInput IdeaCompare Specter EmbeddingsRankGPT Re-Ranking..Snippet APIIdea-SnippetCandidate PapersTop N Papers Similar to IdeaMost Relevant PapersPapersLLMSearch APISTEP 3: Evaluate Idea NoveltyIdea-Keyword &amp; Title PapersInput IdeaThis idea is not novelbecauseâ€¦This idea is novelbecauseâ€¦Most Relevant Papersâ€¦LLMThis idea is not novel becauseâ€¦IdeaMost RelevantClassificationInput +Related PapersPapersand ReasoningAnalogous PapersNovelty Classification and ReasoningSTEP 4: Suggest More Novel IdeasThis idea is not novelInput IdeaIdeaMost Relevant Papersbecauseâ€¦ Classification and ReasoningPaper Facets Input/AnalogousLLMIdea Suggestions More Novel</p>
<p>4.1.1Resulting Dataset.From our annotation studies for novelty assessment (Section 4.1), we collected 67 consensus-labeled examples of novelty classification (39 labeled as novel and 28 as nonnovel) as our dataset.We split the examples into training and test sets (35 for training and 32 for testing) with a balanced distribution of novel and non-novel ideas.Please refer to Table 7 in Appendix G for sample examples.
4.2 Ablations
4.2.1 Methods.To assess the contribution of each component in our novelty checker, we conducted ablation studies using 58 ideas (comprising 13 'not novel' instances from our test set and 45 Natural Language Processing papers from the literature).For this experiment, we focus on 'not novel' cases, since the ideas labeled 'novel' in expert-labeled test data can vary with different retrieved paper sets.</p>
<p>Table 3 :
3
Accuracy of Scideator ablations in predicting "not novel."
MethodAccuracyComplete System89.66%Relevance RankGPT Instead of Facet-Based RankGPT13.79%Embedding Filtering w/o RankGPT10.34%Only Snippet Retrieval w/o Embedding Filtering or RankGPT8.62%Only Keyword Retrieval w/o Embedding Filtering or RankGPT5.17%
4.2.2Results.Table3shows that the complete Scideator system, which employs facet-based re-ranking in RankGPT, significantly</p>
<p>Table 4 :
4
Comparing each ablation to the complete system in terms of overlap in retrieved papers and paper rankings.Overlap indicates how many papers overlap on average with the complete system's top-10 papers.Rank Shift measures the average absolute difference in rank positions (only among overlapping papers).
MethodOverlap (â†‘) Rank Shift (â†“)Relevance RankGPT7.970.67Embedding Filtering7.930.84Snippet Retrieval2.881.85Keyword Retrieval1.171.39
outperforms its ablated variants in accuracy.The results demonstrate that methods relying only on keyword or snippet-based retrieval have much lower accuracy, and even alternate re-ranking strategies with a single embedding-based reranker or both embedding and general relevance RankGPT are insufficient to consistently</p>
<p>Table 5 :
5
Experimental results on expert-annotated dataset comparing Scideator's novelty checker to various baselines.Unless otherwise noted, GPT-4o was used to obtain these results.in-contextexamples.Since OpenReview reviews do not reference the associated papers, we evaluated our expert-labeled examples both with and without including relevant papers to ensure a fair comparison.</p>
<p>Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. 2024.TextGrad: Automatic" Differentiation" via Text.arXivpreprint arXiv:2406.07496(2024).[75]Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng.2024.Self-alignment for factuality: Mitigating hallucinations in llms via self-evaluation.arXiv preprint arXiv:2402.09267(2024).
,,Radensky et al.[74] A PROMPTS FOR ANALOGOUS PAPER FACETFINDERA.1 Prompt to extract facets from a papertitle/abstract.def promptTextToPurposeMechanism(papers, corpus_ids, text=None):if text:prompt = f"""{text}"""else:prompt = f"""TEXTS:"""for id in range(0,len(corpus_ids)):prompt += f"""Text { id + 1}Title: {papers[corpus_ids[id]]["title"]}Abstract: {papers[corpus_ids[id]]["abstract"]}"""if "introduction" in papers[corpus_ids[id]]:prompt += f"""Background: {papers[corpus_ids[id]]["introduction"]}"""prompt += f"""INSTRUCTIONS:You are ScientistGPT, an intelligent assistant that helps researchers come upwith understandable, relevant, specific, feasible, and novel researchideas.</p>
<p>Human-LLM Scientific Idea Generation and Novelty Evaluation Grounded in Research-Paper Facet Recombination , , 1. Specific enough to be helpful in coming up with research ideas.2. Substantially different from the facets you generate for other texts.3. Single short phrases only (no more than 7 words).If you cannot write the facet in a short phrase, it is too specific.
â†©â†’4. No numbers unless they are part of a name (e.g., GPT-4, big 5 personalityâ†©â†’traits).5. No acronyms or abbreviations.6. If the text has more than one of the same type of facet, combine them intoâ†©â†’one.7. No referencing the purpose in the evaluation facet."""if type:prompt += f"""6. The {type} should be relevant to but not a copy of the following query:â†©â†’{query}."""else:prompt += f"""6. The facets should be relevant to but not a copy of the following query:â†©â†’{query}."""if type2:prompt += f"""7. The {type2} should be relevant to but not a copy of the following query:â†©â†’{query2}."""prompt += f"""Examples of bad vs good purposes:-bad (too specific): to generate creative writing activities for third-gradeâ†©â†’English lessons --&gt; good: to support elementary creative writing-bad (too broad): to support healthcare --&gt; good: to provide clinicalâ†©â†’decision support-bad (more than one purpose that are uncombined): to improve engagementâ†©â†’between content creators and audience, to decrease negative effects ofâ†©â†’social media --&gt; good: to improve social media creator-audienceâ†©â†’interaction
Follow the rules below for generating each facet (purpose, mechanism, and evaluation):â†©â†’Scideator:Examples of bad vs good mechanisms:-bad (too specific, numbers that aren't part of a name, acronym): LLM chain-of-thought from gpt-3.5-turbo trained up to 11/06 with temperature=0.7 --&gt; good: chain-of-thought from large language models</p>
<p>Provide {number} analogous purpose/mechanism pairs that are from the same topic of computer science research, {number} that are from the same subarea but different topics of computer science research, and {number} that are from different subareas of computer science research."""
,,Radensky et al.Mechanism: <analogous mechanism here>Evaluation Definition: <evaluation definition here> analogous_papers,Query for Relevant Research Papers: <query combining analogous purpose andsummary,â†©â†’mechanism here>ANSWER: paper_background,"""set_eval=[],ANSWER:return prompt prior_ideas="","""custom_instructions="",return promptnumber=6, idea_number=2):A.3 Prompt to obtain queries for analogous prompt = f"""A.4 Prompt to shorten a query for papers in theINSTRUCTIONS: papers from same topic, same subarea, and You are ScientistGPT, an intelligent assistant that helps researchers come upevent that it is too long to retrieve 4â†©â†’ â†©â†’with understandable, relevant, specific, feasible, and novel research different subarea compared to input paper. ideas.relevant papers. def promptShortenQuery(query): prompt = f""" QUERY: {query}def promptAnalogyQueries(purpose, mechanism, previousQueries, topic, number=1): prompt = f""" INITIAL PURPOSE/MECHANISM: Purpose: {purpose} Mechanism: {mechanism}""" First, read the summary of prior work and the papers below. That way, you will know what has already been done in related research.""" if prior_ideas: prompt += f"""INSTRUCTIONS: You are ScientistGPT, an intelligent assistant that helps researchers come up with understandable, relevant, specific, feasible, and novel research ideas. â†©â†’ â†©â†’ The query above is too specific to retrieve any research papers. Please provide a simpler and shorter version of the query to find relevant research papers. â†©â†’ If you must lose some meaning when shortening the query, prioritize the most important information. â†©â†’if previousQueries != []: prompt += f""" PREVIOUS QUERIES:""" for q in previousQueries: Next, read your prior ideas below in order to make sure you do not generate â†©â†’ similar ideas to those that you have already proposed.""" if number: prompt += f""" prompt += f""" {q}""" prompt += f""" INSTRUCTIONS: Next, come up with {number} different analogies between the purpose/mechanism of a designated paper and the purpose/mechanism of an analogous paper.""" â†©â†’ else: prompt += f"""ANSWER: """ return promptYou are ScientistGPT, an intelligent assistant that helps researchers come up with novel and useful research ideas. â†©â†’ What are some analogous purposes and mechanisms to the initial purpose and â†©â†’ mechanism above that would inspire novel and useful research ideas?"""if number &gt; 1:A.5 Prompt to summarize input and very nearprompt += f"""analogous papers to obtain related works'â†©â†’â†©â†’summary.â†©â†’def promptSummarizePapers(facets, papers, corpus_ids): prompt = f""" PAPERS:""" index = 0 for t in corpus_ids: index += 1 prompt += f""" Paper {index}:else: prompt += f""" Provide one analogous purpose/mechanism pair that is from the same topic of computer science research, one that is from the same subarea but â†©â†’ different topics of computer science research, and one that is from â†©â†’ different subareas of computer science research.""" â†©â†’ prompt += f""" The relationship between the initial purpose and mechanism should be veryTitle: {papers[t]["title"]}â†©â†’similar to the relationship between the analogous purpose and mechanism.Abstract: {papers[t]["abstract"]}"""Also, provide a query (up to 5 words) for finding research papers relevant toif "introduction" in papers[t]:â†©â†’each analogous purpose/mechanism."""prompt += f"""if previousQueries != []:Background: {papers[t]["introduction"]}"""prompt += f"""prompt += f"""Make sure you come up with new analogous purposes/mechanisms that are NOTPurpose Text: {facets[papers[t]["purpose"]]["text"]} Purpose ID: {papers[t]["purpose"]} Mechanism Text: {facets[papers[t]["mechanism"]]["text"]}â†©â†’ prompt += f""" covered by the previous queries above."""Mechanism ID: {papers[t]["mechanism"]} Evaluation Text: {facets[papers[t]["evaluation"]]["text"]} Evaluation ID: {papers[t]["evaluation"]}""" prompt += f"""FORMAT FOR ANSWER: Analogies within same topic of computer science research: Same Topic: <topic of initial purpose/mechanism and analogous purpose/mechanism here> â†©â†’INSTRUCTIONS: You are ScientistGPT, an intelligent assistant that helps researchers come up with understandable, relevant, specific, feasible, and novel research â†©â†’ ideas. â†©â†’ Summarize the prior work above in around 300 words. Do not summarize individual papers one-by-one. Instead, summarize their contributions as a whole. â†©â†’[number]. Analogy: {purpose} is to {mechanism} as <analogous purpose here> is to <analogous mechanism here> because both relationships involve â†©â†’ â†©â†’ <specific common relationship description here>. Purpose: <analogous purpose here> Mechanism: <analogous mechanism here> Query for Relevant Research Papers: <query combining analogous purpose and â†©â†’ mechanism here>That way, you will know what has already been done in research and will not propose similar ideas. â†©â†’ Instead, you will come up with novel ideas that build upon the designated papers. â†©â†’Analogies within same subarea of computer science research, but across â†©â†’ different topics of computer science research: Same Subarea: <subarea of initial purpose/mechanism and analogous purpose/mechanism here> â†©â†’ANSWER: """ return prompt[number]. Different Topic: <topic of analogous purpose/mechanism here> Analogy: {purpose} is to {mechanism} as <analogous purpose here> is to <analogous mechanism here> because both relationships involve <specific common relationship description here>. â†©â†’ â†©â†’Purpose: <analogous purpose here>B PROMPTS FOR FACETED IDEA GENERATORMechanism: <analogous mechanism here> Query for Relevant Research Papers: <query combining analogous purpose andB.1 Prompt to obtain ideas using Initial ormechanism here>FOR ANSWER: Text <number> No-P-no-M method.Purpose: To <verb> <rest of purpose here> def promptInitialAnalogyIdeas(Purpose Definition: <purpose definition here> topic,Mechanism: <noun phrase mechanism here> papers,â†©â†’Mechanism Definition: <mechanism definition here> facets, query,â†©â†’Evaluation: <noun phrase evaluation here> designated_papers,Purpose: <analogous purpose here>
â†©â†’Analogies across different subareas of computer science research:[number].Different Subarea: <different subarea from initial purpose/mechanism here> â†©â†’ Analogy: {purpose} is to {mechanism} as <analogous purpose here> is to <analogous mechanism here> because both relationships involve <specific common relationship description here>.</p>
<p>Option[number].Analogy: The designated purpose <purpose text from designated paper here> is to the designated mechanism <mechanism text from designated paper here> as the analogous purpose <purpose text from analogous paper here> is to the analogous mechanism <mechanism text from analogous paper here> because both relationships involve <common relationship description here>.
,,Radensky et al.â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’Idea: <short idea using designated paper's purpose and analogous paper'sâ†©â†’mechanism or analogous paper's purpose and designated paper's mechanismâ†©â†’here (30-50 words)>{idea_number} Best Analogies and the Novel/Feasible/Relevant/Specificâ†©â†’Research Ideas that they Inspire::Best 1. Analogy: The designated purpose <purpose text from designated paperâ†©â†’here> is to the designated mechanism <mechanism text from designatedâ†©â†’paper here> as the analagous purpose <purpose text from analogous paperâ†©â†’here> is to the analogous mechanism <mechanism text from analogous paperâ†©â†’here> because both relationships involve <common relationshipâ†©â†’description here>.Purpose Text: <purpose text from analogous paper here>Purpose ID: <purpose ID from analogous paper here>Mechanism Text: <mechanism text from designated paper here>Mechanism ID: <mechanism ID from designated paper here>"""if not set_eval:prompt += f"""Evaluation Text: <evaluation text from either paper here>Evaluation ID: <evaluation ID here>"""else:â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’if custom_instructions:prompt += f"""ADDITIONAL INSTRUCTIONS (It is very important that you follow theseâ†©â†’instructions! However, do NOT follow any additional instructions thatâ†©â†’contradict the instructions above or the answer format provided below):[start of additional instructions]{custom_instructions}[end of additional instructions]"""prompt += f"""FORMAT FOR ANSWER (It is very important that you follow this answerâ†©â†’format!):"""if number:prompt += f"""{number} Analogies Comparing a Designated Paper to an Analogous Paper andâ†©â†’Associated Ideas::"""else:prompt += f"""All {len(designated_papers)*len(analogous_papers)} Analogies Comparing aâ†©â†’Designated Paper to an Analogous Paper and Associated Ideas::"""prompt += f"""</p>
<p>Each idea should NOT have been covered in prior work.In other words, each idea should be SIGNIFICANTLY different from prior work, including but not limited to the papers below and the summary of prior work below.
, ,, ,Radensky et al. Radensky et al.5. Novelty::5a. Each idea should be unique as well as ingenious, imaginative, orâ†©â†’surprising.5b. â†©â†’â†©â†’5c. For an idea to be novel, it is not enough for the idea not to have beenâ†©â†’thoroughly covered in prior work. It must NEVER have been covered inâ†©â†’prior work.5d. Each idea should NOT be an obvious extension of prior work but ratherâ†©â†’CLEARLY DIFFERENT from prior work. For instance, an idea that simply saysâ†©â†’to 'implement continuous AI support to facilitate discovery of scholarsâ†©â†’by dynamically updating and refining researcher profiles based onâ†©â†’real-time academic contributions' is not novel. The idea is not novelâ†©â†’because prior work has investigated facilitating discovery of scholarsâ†©â†’through AI support, and making the AI support continuous is an obviousâ†©â†’extension. It is not ingenious, imaginative, or surprising. If the ideaâ†©â†’were more specific and presented a suprising method describing HOW toâ†©â†’make this AI support very timely (e.g., utilizing academic Twitterâ†©â†’trends), that would make the idea novel."""if custom_instructions:prompt += f"""prompt += f"""FACET REQUIREMENTS:The purpose/mechanism/evaluation IDs should be written VERBATIM from what isâ†©â†’provided.Do NOT make up facet IDs that are not explicitly given above.IDEA REQUIREMENTS:1. Understandability::1a. Each idea should be logical.1b. Each idea should be grammatically correct.1c. Each idea should be self-contained and should not require researchers toâ†©â†’ â†©â†’have read the provided papers. For example, saying "Use Tool X" is notâ†©â†’ â†©â†’self-contained because the researcher might not have read the relevantâ†©â†’ â†©â†’paper to know what Tool X is. Instead, you could say "Use a tool to do Y."â†©â†’ 1d. If an aspect of the idea may not be familiar to someone studying computerâ†©â†’ â†©â†’science broadly, make sure the idea describes the aspect enough for themâ†©â†’ â†©â†’to understand it.â†©â†’â†©â†’ 2. Relevance::â†©â†’4d. Each idea's proposed evaluation should not contradict the rest of theâ†©â†’idea. For example, an idea talking about supporting healthcare workersâ†©â†’that has a user study evaluation should propose a "user study withâ†©â†’healthcare workers," rather than a "user study with teachers" or a "userâ†©â†’study with engineers."</p>
<p>For instance, an idea saying to 'apply a faceted representation to clinical data, creating a multidimensional profile of each patient, integrating medical history, genetic information, lifestyle factors, and current health data' is not novel because prior work has already looked into creating multidimensional patient profiles.If the idea provided a more specific, imaginative description of HOW to create the multidimensional profiles and HOW to apply it to clinical decision-making, then it could be novel.Each idea's purpose and mechanism should work well together.If necessary, adapt the purpose or mechanism to work well with the other.
â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’3f. Each idea should be specific enough that it is significantly differentâ†©â†’from your prior ideas.4. Feasibility::4a. A research lab with moderate resources should be able to carry out eachâ†©â†’idea.4b. Each idea should make sense scientifically and be grounded in the summaryâ†©â†’of prior work and papers noted below.4c.</p>
<p>Imaginative Twist to Add to Facet Combination: The imaginative and smart twist that I will add to the facet combination of <set-1 purpose text here> with <set-2 mechanism text here> will be <imaginative twist here>.
,,Scideator: Human-LLM Scientific Idea Generation and Novelty Evaluation Grounded in Research-Paper Facet RecombinationC PROMPTS FOR IDEA NOVELTY CHECKERFORMAT FOR ANSWER (It is very important that you follow this answerC.1 Prompt to assess idea novelty.â†©â†’format!):"""def promptNoveltyChecker(idea, similar_documents, incontext_part): relevant_papers = [] for i, (_, row) in enumerate(similar_documents.iterrows()): relevant_papers.append( { "role": "user", "content": f"Paper ID [{i}]: Title: {row['title']}. Abstract: {row['abstract']}", â†©â†’ } )if number: prompt += f""" {number} Analogies Comparing a Set-1 Paper to a Set-2 Paper and Associated Ideas::""" â†©â†’ else: prompt += f""" All {len(designated_papers)*len(analogous_papers)} Analogies Comparing a Set-1 Paper to a Set-2 Paper and Associated Ideas::""" â†©â†’ prompt += f""" Option [number]. Analogy: The set-1 purpose <purpose text from set-1 paper here> is to the set-1 mechanism <mechanism text from set-1 paper here> as â†©â†’prompt = [â†©â†’ â†©â†’the set-2 purpose <purpose text from set-2 paper here> is to the set-2 mechanism <mechanism text from set-2 paper here> because bothâ†©â†’relationships involve <common relationship description here>.Idea: <short idea using set-1 paper's purpose and set-2 paper's mechanismâ†©â†’here (30-50 words)>{idea_number} Best Analogies and the Novel/Feasible/Relevant/Specificâ†©â†’Research Ideas that they Inspire::Best 1. Analogy: The purpose <purpose text from set-1 paper here> is to theâ†©â†’mechanism <mechanism text from set-1 paper here> as the purpose <purposeâ†©â†’text from set-2 paper here> is to the mechanism <mechanism text fromâ†©â†’set-2 paper here> because both relationships involve <commonâ†©â†’relationship description here>.Purpose Text: <purpose text from selected set-1 paper here>Purpose ID: <purpose ID from selected set-1 paper here>Mechanism Text: <mechanism text from selected set-2 paper here>Mechanism ID: <mechanism ID from selected set-2 paper here>"""if not set_eval:prompt += f"""Evaluation Text: <text of selected evaluation from either paper here>Evaluation ID: <selected evaluation ID here>"""else:prompt += f"""Evaluation Text: <selected evaluation option text here>Evaluation ID: <selected evaluation option ID here>"""prompt += f"""â†©â†’â†©â†’â†©â†’Initial Research Idea: <idea inspired by facets here (100-150 words)>â†©â†’Issues with Initial Idea: <describe how initial idea doesn't meet ideaâ†©â†’â†©â†’requirements here (50-100 words)>â†©â†’How to Address Issues: <describe how will resolve issues here (50-100 words)>â†©â†’New Research Idea: <updated idea inspired by facets here (100-150 words)>â†©â†’Expanded New Research Idea: <expanded idea inspired by facets here (200-250â†©â†’â†©â†’words)>â†©â†’â†©â†’ 4d. Each idea's proposed evaluation should not contradict the rest of the idea. For example, an idea talking about supporting healthcare workers that has a user study evaluation should propose a "user study with healthcare workers," rather than a "user study with teachers" or a "user study with engineers." â†©â†’ â†©â†’ â†©â†’ â†©â†’Best 2. Analogy: The purpose <purpose text from set-1 paper here> is to the â†©â†’ mechanism <mechanism text from set-1 paper here> as the purpose <purpose â†©â†’ text from set-2 paper here> is to the mechanism <mechanism text from â†©â†’ set-2 paper here> because both relationships involve <common â†©â†’ relationship description here>. Purpose Text: <purpose text from selected set-1 paper here>5. Novelty:: 5a. Each idea should be unique as well as ingenious, imaginative, or surprising. â†©â†’ 5b. Each idea should NOT have been covered in prior work. In other words, each idea should be SIGNIFICANTLY different from prior work, including â†©â†’ but not limited to the papers below and the summary of prior work below. â†©â†’ 5c. For an idea to be novel, it is not enough for the idea not to have been thoroughly covered in prior work. It must NEVER have been covered in â†©â†’ prior work. â†©â†’ 5d. Each idea should NOT be an obvious extension of prior work but rather CLEARLY DIFFERENT from prior work. For instance, an idea that simply says â†©â†’ to 'implement continuous AI support to facilitate discovery of scholars â†©â†’ by dynamically updating and refining researcher profiles based on â†©â†’ real-time academic contributions' is not novel. The idea is not novel â†©â†’ because prior work has investigated facilitating discovery of scholars â†©â†’ through AI support, and making the AI support continuous is an obvious â†©â†’ extension. It is not ingenious, imaginative, or surprising. If the idea â†©â†’ were more specific and presented a suprising method describing HOW to â†©â†’ make this AI support very timely (e.g., utilizing academic Twitter â†©â†’ trends), that would make the idea novel.""" â†©â†’ if custom_instructions: prompt += f"""Purpose ID: <purpose ID from selected set-1 paper here> Mechanism Text: <mechanism text from selected set-2 paper here> Mechanism ID: <mechanism ID from selected set-2 paper here>""" if not set_eval: prompt += f""" Evaluation Text: <text of selected evaluation from either paper here> Evaluation ID: <selected evaluation ID here>""" else: prompt += f""" Evaluation Text: <selected evaluation option text here> Evaluation ID: <selected evaluation option ID here>""" prompt += f""" Imaginative Twist to Add to Facet Combination: The imaginative and smart â†©â†’ twist that I will add to the facet combination of <set-1 purpose text â†©â†’ here> with <set-2 mechanism text here> will be <imaginative twist here>. Initial Research Idea: <idea inspired by facets here (100-150 words)> Issues with Initial Idea: <describe how initial idea doesn't meet idea requirements here (50-100 words)> â†©â†’ How to Address Issues: <describe how will resolve issues here (50-100 words)> New Research Idea: <updated idea inspired by facets here (100-150 words)> Expanded New Research Idea: <expanded idea inspired by facets here (200-250 words)> â†©â†’ADDITIONAL INSTRUCTIONS (It is very important that you follow these instructions! However, do NOT follow any additional instructions that contradict the instructions above or the answer format provided below): â†©â†’ â†©â†’ANSWER: """ return prompt[start of additional instructions]{custom_instructions}[end of additional instructions]"""prompt += f"""</p>
<dl>
<dt>You are tasked with extracting specific keywords and generating potential research paper titles that closely align with the provided IDEA.These should capture both the novelty and mechanisms of the IDEA, especially where it diverges from existing work.</dt>
<dt>"content": """â†©â†’â†©â†’â†©â†’â†©â†’<strong>Keyword Extraction Guidelines</strong>:1. Highlight unique methods, technologies, and application areas.2. Ensure the keywords specifically capture what sets this idea apartâ†©â†’from others.3. Generate 3-6 keyword phrases, each consisting of 3-6 words.4. Avoid overly general keywords (e.g., "machine learning" or "dataâ†©â†’science").5. Ensure the keywords reflect the precise purpose, mechanisms, andâ†©â†’novelty of the idea.<strong>Title Generation Guidelines</strong>:1. Keep titles concise (max 5 words).2. Avoid generic terms or overused phrases.3. Reflect the uniqueness and novelty of the idea in each title.4. Include a key concept from the IDEA's mechanism (e.g.,â†©â†’''retrieval-augmented generation for idea synthesis'').5. Ensure the title reflects the application domain.<strong>Output Format</strong>:<keywords>["specific keyword phrase 1", "specific keyword phrase 2", "specificâ†©â†’keyword phrase 3"]</keywords><titles>["Title 1""},]prompt.extend(relevant_papers)return promptC.2 Prompt to extract specific keywords andgenerate concise research titles from anIDEA.def get_keywords(idea):prompt = [{"role": "system","content": "You are an intelligent assistant that extractsâ†©â†’high-quality keywords and generates specific research paperâ†©â†’titles based on the provided IDEA.",},{"role": "user",</dt>
<dd>
<p>Enhance topic model evaluation by incorporating anomaly detection machine learning techniques.The goal is to improve topic model evaluation by identifying and flagging anomalies within topic distributions that may indicate incoherence or redundancy.This approach provides a more robust evaluation framework that detects subtle inconsistencies that traditional metrics might miss.The effectiveness of this integrated evaluation method would be assessed through a systematic comparison and meta-analysis of different topic models, ensuring comprehensive and reliable evaluation outcomes.
â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’â†©â†’Key Facets to Look for in Passages:-Application Domain: Topic modeling and evaluation.-Purpose: Improving topic model evaluation by detecting anomaliesâ†©â†’indicating incoherence or redundancy.-Mechanism: Incorporating anomaly detection machine learningâ†©â†’techniques into topic model evaluation.-Method: Identifying and flagging anomalies within topicâ†©â†’distributions.-Evaluation: Systematic comparison and meta-analysis of differentâ†©â†’topic models to assess effectiveness.Passages:[0] An Enhanced BERTopic Framework and Algorithm for Improving Topicâ†©â†’Coherence and Diversity[1] Evaluation of Unsupervised Anomaly Detection Methods in Sentimentâ†©â†’Mining[2] LDA_RAD: A Spam Review Detection Method Based on Topic Model andâ†©â†’Reviewer Anomaly Degree[3] Apples to Apples: A Systematic Evaluation of Topic Models[4] Machine Learning Approach for Anomaly-Based Intrusion Detectionâ†©â†’Systems Using Isolation Forest Model and Support Vector Machine[5] OCTIS: Comparing and Optimizing Topic Models is Simple![6] Qualitative Insights Tool (QualIT): LLM Enhanced Topic Modeling[7] An Exhaustive Review on State-of-the-art Techniques for Anomalyâ†©â†’Detection on Attributed Networks[8] Topic Modeling Revisited: New Evidence on Algorithm Performanceâ†©â†’and Quality Metrics[9] A Robust Bayesian Probabilistic Matrix Factorization Model forâ†©â†’Collaborative Filtering Recommender Systems Based on Userâ†©â†’Anomaly Rating Behavior DetectionRanking:[</p>
</dd>
</dl>
<p>Each idea's proposed evaluation should not contradict the rest of the idea.For example, an idea talking about supporting healthcare workers that has a user study evaluation should propose a "user study with healthcare workers," rather than a "user study with teachers" or a "user study with engineers."Eachideashould NOT have been covered in prior work.In other words, each idea should be SIGNIFICANTLY different from prior work, including but not limited to the papers below and the summary of prior work below.â†©â†’â†©â†’5c.For an idea to be novel, it is not enough for the idea not to have been thoroughly covered in prior work.It must NEVER have been covered in prior work.: Human-LLM Scientific Idea Generation and Novelty Evaluation Grounded in Research-Paper Facet Recombination
Situation Initial, TreatmentTopic LLM explain-abilityPurpose to enhance medical in-formation retrieval (near)â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’ 4d. â†©â†’ â†©â†’ â†©â†’ â†©â†’ 5. Novelty:: 5a. Each idea should be unique as well as ingenious, imaginative, or surprising. â†©â†’ Mechanism Evaluation Custom Instructions Expanded Version of Idea sequential knowledge-guided prompting (paper input) medical question-answering bench-marks (near, unse-5b. , , lected)
â†©â†’ â†©â†’Figure1413,14, and 15 present information related to comparing Scideator's novelty checker to baselines.Scideator</p>
<p>This research focuses on developing an advanced reward model for Self-Knowledge Tuning by incorporating specific metrics from the HALoGEN framework.The core of this model lies in leveraging atomic unit verification to dissect LLM outputs into fundamental components, evaluating each against reliable knowledge bases.By integrating HALoGEN's error classifications, the model can provide targeted feedback that distinguishes between different types of hallucinations, such as those arising from incorrect recollections, faulty training data, or fabrications.This nuanced feedback is expected to enhance the calibration of LLMs, reducing the frequency of factual inaccuracies.The model's efficacy will be tested across diverse domains, including programming and scientific attribution, to ensure broad applicability.A comprehensive validation framework will be established, involving both automated and human evaluations, to rigorously measure improvements in model accuracy and coherence.The outcome of this research could significantly advance the development of more reliable and trustworthy LLMs, providing a foundation for future enhancements in self-supervised learning techniques.
BaselinedealingN/AN/AN/ACan the reward modelwithin Selk-Knoelwdge tun-LLMing be changed to in-halluci-corporate more fine-nationsgrained feedback (e.g.from Halogen like met-rics) [all 3 papers givento LLM]BaselineAI toolsN/AN/AN/AGenerate ideas thatfor edu-usenon-traditionalcationmediums for educa-tion, so avoid usingtextbooks or othermaterialcommonlyfound in classrooms.[all 3 papers given toLLM]
To confirm reasonable consistency in facet distance, the first two authors annotated previously unseen purposes/mechanisms generated by the tool for three papers not used in the user study. It was an earlier but similar version of the tool compared to what was used in the study. Facets were grouped into generally near and far categories. Both annotators classified the majority of near purposes, near mechanisms, far purposes, and far mechanisms like the tool. One annotator demonstrated substantial agreement
The prompts to the LLM for the Faceted Idea Generator module may be found in Appendix B.
In this module, the LLM's temperature is set to 0.75 to make the responses more varied.
Note that, if one of the selected purposes or mechanisms is manually entered and therefore does not have an associated paper, the system generates a purpose or mechanism to pair with the selected facet for the sake of creating analogies with another paper.
8 The expanded versions of the sample ideas are in Appendix E
.9 The prompts to the LLM for the module are listed in Appendix C.
RankGPT's relevance criteria were changed to match with our key idea facets.
https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/ prompt-generator
Please refer to Appendix I for additional details about the experimental setup for all of these approaches.
The input papers for each topic may be found in Appendix D.
The study script may be found in the supplementary materials.
https://www.semanticscholar.org/
The data did not violate the assumption of symmetry of within-subjects differences about the median
Ideas were generated in groups of four, two at a time, so the first two ideas took less time to generate than the last two ideas.
One participant did not get to spend the full five minutes allotted for the task, and four participants experienced a different version of the Scideator novelty checker due to an issue with an API call.
We had planned to run a Wilcoxon signed-rank test but found that the data violated the assumption of symmetry of within-subjects differences about the median
def promptFacetsToIdeas( topic, summary, papers, designated_papers, analogous_papers, relevant_facets_by_paper, facets, paper_background, set_eval=[], prior_ideas="", custom_instructions="", number=6, idea_number=2): prompt = f""" INSTRUCTIONS:
ACKNOWLEDGMENTSThis research was supported by the Allen Institute for Artificial Intelligence (AI2).The authors thank the many folks at AI2 who provided helpful feedback on this work, including Peter Jansen, Peter Clark, and Ashish Sabharwal.We also thank the participants who made this work possible.â†©â†’ â†©â†’For the other analogy, create a research idea that combines the purpose from the designated paper, the mechanism from the analogous paper, and the evaluation from either paper in an imaginative and smart manner."""â†©â†’ â†©â†’ prompt += f""" SUMMARY OF PRIOR WORK: {summary}""" if prior_ideas: prompt += f""" PRIOR IDEAS: {prior_ideas}""" prompt += f""" DESIGNATED PAPERS:""" index = 0 for t in designated_papers: index += 1 prompt += f""" Paper {index}: Title: {papers[t]["title"]} Abstract: {papers[t]["abstract"]}""" if paper_background and "introduction" in papers[t]: prompt += f""" Background: {papers[t]["introduction"]}""" prompt += f""" prompt += f""" ANALOGOUS PAPERS RELATED TO {query}:""" else: prompt += f""" ANALOGOUS PAPERS:""" index = 0 for t in analogous_papers: index += 1 prompt += f""" Paper {index}:""" if papers[t]["relevantQuery"] != "none": prompt += f""" Theme: {papers[t]["relevantQuery"]}""" prompt += f""" Title: {papers[t]["title"]} Abstract: {papers[t]["abstract"]}""" if paper_background and "introduction" in papers[t]: prompt += f""" Background: {papers[t]["introduction"]}""" if "relatedWork" in papers[t]: if papers[t]["relatedWork"]: prompt += f""" Related Work: {papers[t]["relatedWork"]}""" prompt += f""" Purpose Text: {facets[papers[t]["purpose"]]["text"]} Purpose ID: {papers[t]["purpose"]} Mechanism Text: {facets[papers[t]["mechanism"]]["text"]} Mechanism ID: {papers[t]["mechanism"]}"""if not set_eval: prompt += f""" Evaluation Text: {facets[papers[t]["evaluation"]]["text"]} Evaluation ID: {papers[t]["evaluation"]}""" if set_eval: prompt += f""" EVALUATION OPTIONS:""" ind = 0 for eval in set_eval: prompt += f""" {ind}.Evaluation Text: {eval[1]} Evaluation ID: {eval[2]}""" ind += 1, , if relevant_purposes: prompt += f""" Purpose Text: <purpose text from selected set-1 paper here> Purpose ID: <purpose ID from selected set-1 paper here> Mechanism Text: <mechanism text from selected set-2 paper here> Mechanism ID: <mechanism ID from selected set-2 paper here>""" else:prompt += f""" Purpose Text: <purpose text from selected set-2 paper here> Purpose ID: <purpose ID from selected set-2 paper here> Mechanism Text: <mechanism text from selected set-1 paper here> Mechanism ID: <mechanism ID from selected set-1 paper here>""" if not set_eval: prompt += f""" Evaluation Text: <text of selected evaluation from either paper here> Evaluation ID: <selected evaluation ID here>""" else:prompt += f""" Evaluation Text: <selected evaluation option text here> Evaluation ID: <selected evaluation option ID here>""" if relevant_purposes: prompt += f""" Imaginative Twist to Add to Facet Combination: The imaginative and smart twist that I will add to the facet combination of <set-1 purpose text here> with <set-2 mechanism text here> will be <imaginative twist here>."""â†©â†’ â†©â†’ else: prompt += f""" Imaginative Twist to Add to Facet Combination: The imaginative and smart twist that I will add to the facet combination of <set-2 purpose text here> with <set-1 mechanism text here> will be <imaginative twist here>."""â†©â†’ â†©â†’prompt += f""" Initial Research Idea: <idea inspired by facets here (100-150 words)> Issues with Initial Idea: <describe how initial idea doesn't meet idea requirements here (50-100 words)>â†©â†’How to Address Issues: <describe how will resolve issues here (50-100 words)> New Research Idea: <updated idea inspired by facets here (100-150 words)> Expanded New Research Idea: <expanded idea inspired by facets here (200-250 words)>â†©â†’Best 2. Analogy: The set-1 purpose <purpose text from selected set-1 paper here> is to the set-1 mechanism <mechanism text from selected set-1 paper here> as the set-2 purpose <purpose text from selected set-2 paper here> is to the set-2 mechanism <mechanism text from selected set-2 paper here> because both relationships involve <common relationship description here>."""if relevant_purposes: prompt += f""" Purpose Text: <purpose text from selected set-1 paper here> Purpose ID: <purpose ID from selected set-1 paper here> Mechanism Text: <mechanism text from selected set-2 paper here> Mechanism ID: <mechanism ID from selected set-2 paper here>""" else:prompt += f""" Purpose Text: <purpose text from selected set-2 paper here> Purpose ID: <purpose ID from selected set-2 paper here> Mechanism Text: <mechanism text from selected set-1 paper here> Mechanism ID: <mechanism ID from selected set-1 paper here>""" if not set_eval: prompt += f""" Evaluation Text: <text of selected evaluation from either paper here> Evaluation ID: <selected evaluation ID here>""" else:prompt += f""" Evaluation Text: <selected evaluation option text here> Evaluation ID: <selected evaluation option ID here>""" if relevant_purposes: prompt += f""" Imaginative Twist to Add to Facet Combination: The imaginative and smart twist that I will add to the facet combination of &lt;set-â†©â†’If possible, the paper from which the purpose comes must have a different distance than the paper from which the mechanism comes.â†©â†’Example Analogy: The purpose "to expand exploration of creative design spaces" is to the mechanism "structured generation framework" as the purpose "to enhance personalized mathematics learning" is to the mechanism "guiding and adaptive prompts" because both relationships involve providing flexible yet structured experiences in order to support personalized and useful knowledge acquisition.â†©â†’ â†©â†’ â†©â†’ â†©â†’ â†©â†’Also, come up with a short research idea based on each analogy that combines the set-1 paper's purpose with the set-2 paper's mechanism.â†©â†’Next, select the {idea_number} best analogies that inspire research ideas that meet the idea requirements below.â†©â†’""" if set_eval: prompt += f""" Finally, for each analogy, create a research idea that combines the purpose from a set-1 paper, the mechanism from a set-2 paper, and one of the evaluation options below in an imaginative and smart manner."""â†©â†’ â†©â†’else: prompt += f""" Finally, for each analogy, create a research idea that combines the purpose from a set-1 paper, the mechanism from a set-2 paper, and the evaluation from one of those two papers in an imaginative and smart manner.E EXPANDED VERSION OF SAMPLE IDEAS FROM TABLE 2The expanded version of each sample idea in Table2may be found in Table6below.F INSIGHTS FROM LLM PROMPT OPTIMIZATION USING TEXTGRADIn our experiments with TextGrad, we investigated how specific prompt instructions influence an LLM's ability to classify the novelty of an idea.Figures10, 11, and 12 present the accuracy of various prompts optimized with TextGrad on our dataset (train=25, validation = 10, test = 32).Prompts with both non-zero and zero validation accuracy included various instructions for evaluating the novelty of ideas, such as assessing the uniqueness of methods and their comparison to existing research.Through this prompt optimization process, we observed interesting ways in which LLMs may evaluate novelty, like considering historical context, frequency of similar studies, comparative analysis with existing works, examining arguments for both novel and non-novel perspectives.However, prompts without these specific instructions also influenced accuracy, suggesting the complexity of novelty evaluation with LLMs.Notably, some prompts with similar instructions showed different performance on validation data.For example, both prompt 3 (accuracy = 0) and prompt 9 (accuracy = 0.6) include instructions to evaluate if the idea introduces unique methodologies, and how it compares to existing work.However, the difference in their performance suggests that subtle variations in wording and instruction framing can significantly impact the classification performance.It remains unclear why certain prompts perform better despite having similar instructions.Our analysis highlights the LLM's sensitivity to prompt design when assessing novelty of an idea.Even minor variations in wording and structure can lead to substantial performance changes, emphasizing the need for careful prompt engineering and well-chosen in-context examples to guide the LLM for idea novelty evaluation.Reasoning: The idea is novel because it uniquely focuses on prioritizing reviewer comments for actionable revisions, which is not explicitly addressed in ARIES[1]or other related works like ReviVal[10].Example 2 Idea: Develop a systematic review-based framework designed to align LLM evaluation with human preferences, ensuring that evaluation criteria are continuously refined based on comprehensive reviews of user feedback and emerging model behaviors.This framework will utilize content analysis of user interactions and feedback to identify patterns and areas of improvement.The effectiveness of this framework will be assessed through a qualitative study involving iterative cycles of user feedback and criteria refinement.Most Relevant Papers:G EXPERT-LABELED EXAMPLES( Reasoning: The idea is not novel because it closely resembles existing frameworks like EvalLM[1]and HumanELY[2], which already align LLM evaluations with human preferences using user-defined criteria and human feedback.I EXPERIMENTAL SETUP DETAILS FOR COMPARING SCIDEATOR NOVELTY CHECKER TO BASELINES
Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation. Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, Adam Jatowt, 2025276107364</p>
<p>Deductive closure training of language models for coherence, accuracy, and updatability. Afra Feyza AkyÃ¼rek, Ekin AkyÃ¼rek, Leshem Choshen, Derry Wijaya, Jacob Andreas, arXiv:2401.085742024. 2024arXiv preprint</p>
<p>Prompting for discovery: Flexible sense-making for ai art-making with dreamsheets. Shm Garanganao, Almeda , J D Zamfirescu-Pereira, Kyu Won Kim, Pradeep Mani Rathnam, Bjoern Hartmann, Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing Systems2024</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024. 2024arXiv preprint</p>
<p>TWOLAR: A TWO-Step LLM-Augmented Distillation Method for Passage Reranking. Davide Baldelli, Junfeng Jiang, Akiko Aizawa, Paolo Torroni, ArXiv abs/2403.177592024. 2024</p>
<p>Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. Lutz Bornmann, RÃ¼diger Mutz, Journal of the association for information science and technology. 662015. 2015</p>
<p>Using thematic analysis in psychology. Virginia Braun, Victoria Clarke, Qualitative research in psychology. 322006. 2006</p>
<p>Solvent: A mixed initiative system for finding analogies between research papers. Joel Chan, Joseph Chee Chang, Tom Hope, Dafna Shahaf, Aniket Kittur, Proceedings of the ACM on Human-Computer Interaction. 22018. 2018CSCW</p>
<p>A foundation model enhanced approach for generative design in combinational creativity. Liuqing Chen, Yuan Zhang, Ji Han, Lingyun Sun, Peter Childs, Boheng Wang, Journal of Engineering Design. 352024. 2024</p>
<p>Scientific and fantastical: Creating immersive, culturally relevant learning experiences with augmented reality and large language models. Alan Y Cheng, Meng Guo, Melissa Ran, Arpit Ranasaria, Arjun Sharma, Anthony Xie, Bala Khuyen N Le, Shihe Vinaithirthan, David Luan, Henry Thomas, Wright, Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing Systems2024</p>
<p>Quantifying the creativity support of digital tools through the creativity support index. Erin Cherry, Celine Latulipe, ACM Transactions on Computer-Human Interaction (TOCHI). 212014. 2014</p>
<p>CreativeConnect: Supporting Reference Recombination for Graphic Design Ideation with Generative AI. Daeun Choi, Sumin Hong, Jeongeon Park, John Joon , Young Chung, Juho Kim, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos. Seulgi Choi, Hyewon Lee, Yoonjoo Lee, Juho Kim, Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing Systems2024</p>
<p>SPECTER: Document-level Representation Learning using Citationinformed Transformers. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S Weld, ArXiv abs/2004.071802020. 2020215768677</p>
<p>In praise of convergent thinking. Arthur Cropley, Creativity research journal. 1832006. 2006</p>
<p>Marg: Multiagent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024. 2024arXiv preprint</p>
<p>Identifying good ideas: constructs and scales for idea evaluation. Jill Douglas L Dean, Tom Hender, Eric Rodgers, Santanen, Journal of Association for Information Systems. 7102006. 2006</p>
<p>On problem-solving. Psychological monographs. Karl Duncker, Lynne S Lees, 1945. 194558</p>
<p>James Enouen, Hootan Nakhost, Sayna Ebrahimi, Yan Sercan O Arik, Tomas Liu, Pfister, arXiv:2312.01279Textgenshap: Scalable post-hoc explanations in text generation with long documents. 2023. 2023arXiv preprint</p>
<p>Huifeng Guo, and Ruiming Tang. 2024. LLM-enhanced Reranking in Recommender Systems. Jingtong Gao, Bo Chen, Xiangyu Zhao, Weiwen Liu, Xiangyang Li, Yichao Wang, Zijian Zhang, Wanyu Wang, Yuyang Ye, Shanru Lin, ArXiv abs/2406.124332024270562015</p>
<p>Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.18864Towards an AI co-scientist. 2025. 2025arXiv preprint</p>
<p>Tianyang Gu, Jingjin Wang, Zhihao Zhang, Haohong Li, arXiv:2412.14141LLMs can realize combinatorial creativity: generating creative ideas via LLMs for scientific research. 2024. 2024arXiv preprint</p>
<p>Yuling Gu, Oyvind Tafjord, Peter Clark, arXiv:2311.09613Digital socrates: Evaluating llms through explanation critiques. 2023. 2023arXiv preprint</p>
<p>Topic-based exploration and embedded visualizations for research idea generation. Hua Guo, David H Laidlaw, IEEE transactions on visualization and computer graphics. 262018. 2018</p>
<p>All that glitters is not novel: Plagiarism in ai generated research. Tarun Gupta, Danish Pruthi, arXiv:2502.164872025. 2025arXiv preprint</p>
<p>Mental leaps: Analogy in creative thought. J Keith, Paul Holyoak, Thagard, 1996MIT press</p>
<p>Accelerating innovation through analogy mining. Tom Hope, Joel Chan, Aniket Kittur, Dafna Shahaf, Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. the 23rd ACM SIGKDD international conference on knowledge discovery and data mining2017</p>
<p>A computational inflection for scientific discovery. Tom Hope, Doug Downey, Oren Daniel S Weld, Eric Etzioni, Horvitz, Commun. ACM. 662023. 2023</p>
<p>Scaling creative inspiration with finegrained functional aspects of ideas. Tom Hope, Ronen Tamari, Daniel Hershcovich, B Hyeonsu, Joel Kang, Aniket Chan, Dafna Kittur, Shahaf, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022</p>
<p>Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S Weld, Peter Clark, arXiv:2503.22708CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation. 2025. 2025arXiv preprint</p>
<p>Article 50 million: an estimate of the number of scholarly articles in existence. E Arif, Jinha, Learned publishing. 232010. 2010</p>
<p>BioSpark: An End-to-End Generative System for Biological-Analogical Inspirations and Ideation. David Hyeonsu B Kang, Nikolas Chuan-En Lin, Aniket Martelaro, Yan-Ying Kittur, Matthew K Chen, Hong, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>Augmenting scientific creativity with an analogical search engine. Xin Hyeonsu B Kang, Tom Qian, Dafna Hope, Joel Shahaf, Aniket Chan, Kittur, ACM Transactions on Computer-Human Interaction. 292022. 2022</p>
<p>The Cambridge handbook of creativity. C James, Robert J Kaufman, Sternberg, 2010Cambridge University Press</p>
<p>Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts, arXiv:2310.03714DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. 2023. 2023arXiv preprint</p>
<p>The Semantic Scholar Open Data Platform. Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David W Graham, F Q Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey Macmillan, Tyler C Murray, Christopher Newell, Shaurya Smita R Rao, Paul Rohatgi, Zejiang Sayre, Amanpreet Shen, Luca Singh, Shivashankar Soldaini, A Subramanian, Alex D Tanaka, Linda M Wade, Lucy Lu Wagner, Christopher Wang, Caroline Wilhelm, Jiangjiang Wu, Angele Yang, Madeleine Zamarron, Daniel S Van Zuylen, Weld, ArXiv abs/2301.101402023. 2023256194545</p>
<p>A search engine for discovery of scientific challenges and directions. Dan Lahav, Jon Saad Falcon, Bailey Kuehl, Sophie Johnson, Sravanthi Parasa, Noam Shomron, Horng Duen, Diyi Chau, Eric Yang, Horvitz, Daniel S Weld, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design. Pier Luca, Lanzi , Daniele Loiacono, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference2023</p>
<p>Putting things into context: Generative AI-enabled context personalization for vocabulary learning improves learning motivation. Joanne Leong, Pat Pataranutaporn, Valdemar Danry, Florian Perteneder, Yaoli Mao, Pattie Maes, Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing Systems2024</p>
<p>Can large language models provide useful feedback on research papers? A large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Scott Daniel, Yian Smith, Yin, NEJM AI. 12024. 2024. AIoa2400196</p>
<p>How ai processing delays foster creativity: Exploring research question co-creation with an llm-based agent. Yiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, Yun Huang, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Personaflow: Boosting research ideation with llm-simulated expert personas. Yiren Liu, Pranav Sharma, Mehul Jitendra Oswal, Haijun Xia, Yun Huang, arXiv:2409.125382024. 2024arXiv preprint</p>
<p>Creative Research Question Generation for Human-Computer Interaction Research. Yiren Liu, Mengxia Yu, Meng Jiang, Yun Huang, IUI Workshops. 2023</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024. 2024arXiv preprint</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob N Foerster, Jeff Clune, David Ha, ArXiv abs/2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024. 2024271854887</p>
<p>Ranked List Truncation for Large Language Modelbased Re-Ranking. Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, Maarten De Rijke, ArXiv abs/2404.181852024. 2024269449617</p>
<p>Algorithmic ways of seeing: Using object detection to facilitate art exploration. Louie Meyer, Johanne Engel Aaen, Anitamalina Regitse Tranberg, Peter Kun, Matthias Freiberger, Sebastian Risi, Anders Sundnes, LÃ¸vlie , Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing Systems2024</p>
<p>Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity. Sheshera Mysore, Arman Cohan, Tom Hope, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>. Sheshera Mysore, O' Tim, Andrew Gorman, Hamed Mccallum, Zamani, n. d.</p>
<p>Test Collection of Computer Science Research Articles for Faceted Query by Example. Csfcube-A , </p>
<p>Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff, arXiv:2403.04382Acceleron: A Tool to Accelerate Research Ideation. 2024. 2024arXiv preprint</p>
<p>An Interactive Co-Pilot for Accelerated Research Ideation. Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff, Proceedings of the Third Workshop on Bridging Human-Computer Interaction and Natural Language Processing. the Third Workshop on Bridging Human-Computer Interaction and Natural Language Processing2024</p>
<p>Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models. Baharan Nouriinanloo, Maxime Lamothe, ArXiv abs/2406.187402024. 2024270764517</p>
<p>LumiMood: A Creativity Support Tool for Designing the Mood of a 3D Scene. Jeongseok Oh, Seungju Kim, Seungjun Kim, Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing Systems2024</p>
<p>. Openreview, n. d.</p>
<p>OpenReview. </p>
<p>Bursting scientific filter bubbles: Boosting innovation via novel author discovery. Jason Portenoy, Marissa Radensky, Jevin D West, Eric Horvitz, Tom Daniel S Weld, Hope, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022</p>
<p>Kevin Pu, Tovi Feng, Tom Grossman, Bhavana Hope, Matt Dalvi Mishra, Jonathan Latzke, Joseph Chee Bragg, Pao Chang, Siangliulue, arXiv:2410.04025IdeaSynth: Iterative Research Idea Development Through Evolving and Composing Idea Facets with Literature-Grounded Feedback. 2024. 2024arXiv preprint</p>
<p>Design and other types of fixation. Terry Purcell, John S Gero, Design studies. 171996. 1996</p>
<p>Abhilasha Ravichander, Shrusti Ghela, David Wadden, Yejin Choi, arXiv:2501.08292HALoGEN: Fantastic LLM Hallucinations and Where to Find Them. 2025. 2025arXiv preprint</p>
<p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Nils Reimers, Iryna Gurevych, Conference on Empirical Methods in Natural Language Processing. 2019</p>
<p>Divergent thinking, creativity, and ideation. The Cambridge handbook of creativity. Mark A Runco, 2010. 2010413446</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. 2024. 2024arXiv preprint</p>
<p>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, ArXiv abs/2409.041092024. 2024272463952</p>
<p>Scientific Creativity: Discovery and Invention as Combinatorial. Dean Keith, Simonton , Frontiers in Psychology. 122372621812021. 2021</p>
<p>Improving Selection of Analogical Inspirations through Chunking and Recombination. Arvind Srinivasan, Joel Chan, Proceedings of the 16th Conference on Creativity &amp; Cognition. the 16th Conference on Creativity &amp; Cognition2024</p>
<p>Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation. Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun, Haijun Li, Xia, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>ReviewFlow: Intelligent Scaffolding to Support Academic Peer Reviewing. Lu Sun, Aaron Chan, Yun Seo Chang, Steven P Dow, Proceedings of the 29th International Conference on Intelligent User Interfaces. the 29th International Conference on Intelligent User Interfaces2024</p>
<p>Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, Zhaochun Ren, ArXiv abs/2304.095422023. 2023258212638</p>
<p>Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>The cognitive science of science: Explanation, discovery, and conceptual change. Thagard, 2012The MIT Press</p>
<p>DORIS-MAE: scientific document retrieval using multi-level aspect-based queries. Jianyou Wang, Kaicheng Wang, Xiaoyue Wang, Prudhviraj Naidu, Leon Bergen, Ramamohan Paturi, Proceedings of the 37th International Conference on Neural Information Processing Systems. the 37th International Conference on Neural Information Processing Systems2023</p>
<p>LLMFactor: Extracting profitable factors through prompts for explainable stock movement prediction. Meiyun Wang, Kiyoshi Izumi, Hiroki Sakaji, arXiv:2406.108112024. 2024arXiv preprint</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.14259Scimon: Scientific inspiration machines optimized for novelty. 2023. 2023arXiv preprint</p>
<p>Creative Computing: an approach to knowledge combination for creativity. Hongji Yang, Delin Jing, Lu Zhang, 2016 IEEE Symposium on Service-Oriented System Engineering (SOSE). IEEE2016</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1646 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1646</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1646</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-255826915</p>
                <p><strong>Paper Title:</strong> Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies</p>
                <p><strong>Paper Abstract:</strong> Reinforcement Learning has been shown to have a great potential for robotics. It demonstrated the capability to solve complex manipulation and locomotion tasks, even by learning end-to-end policies that operate directly on visual input, removing the need for custom perception systems. However, for practical robotics applications, its scarce sample efficiency, the need for huge amounts of resources, data, and computation time can be an insurmountable obstacle. One potential solution to this sample efficiency issue is the use of simulated environments. However, the discrepancy in visual and physical characteristics between reality and simulation, namely the sim-to-real gap, often significantly reduces the real-world performance of policies trained within a simulator. In this work we propose a sim-to-real technique that trains a Soft-Actor Critic agent together with a decoupled feature extractor and a latent-space dynamics model. The decoupled nature of the method allows to independently perform the sim-to-real transfer of feature extractor and control policy, and the presence of the dynamics model acts as a constraint on the latent representation when finetuning the feature extractor on real-world data. We show how this architecture can allow the transfer of a trained agent from simulation to reality without retraining or finetuning the control policy, but using real-world data only for adapting the feature extractor. By avoiding training the control policy in the real domain we overcome the need to apply Reinforcement Learning on real-world data, instead, we only focus on the unsupervised training of the feature extractor, considerably reducing real-world experience collection requirements. We evaluate the method on sim-to-sim and sim-to-real transfer of a policy for table-top robotic object pushing. We demonstrate how the method is capable of adapting to considerable variations in the task observations, such as changes in point-of-view, colors, and lighting, all while substantially reducing the training time with respect to policies trained directly in the real.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1646.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1646.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DVAE-SAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoupled Variational Autoencoder with Soft Actor-Critic (DVAE-SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoupled visuomotor RL architecture that learns a predictive latent representation (DVAE) and a Soft Actor-Critic policy operating on latent states; enables sim-to-real transfer by freezing the policy and latent dynamics predictor and fine-tuning only the encoder/decoder on real data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka-Emika Panda 7-DOF robotic arm</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 7-DOF tabletop manipulator controlled in Cartesian space; end-effector moves horizontally within a 45 cm square workspace and issues 2D continuous displacements as actions; observations are an RGB camera (128x128) plus 2D end-effector proprioception.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (non-prehensile object pushing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Gazebo simulation of the robot and table-top pushing task, providing physics simulation (robot kinematics, contact interactions, friction approximations) and rendered RGB camera observations; used as the source domain for training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>moderate-fidelity physics with non-photorealistic rendering (approximate contact dynamics and basic lighting)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>robot kinematics, contact interactions between end-effector/object/table, basic lighting and camera viewpoint rendering, object geometry and workspace layout</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>photorealistic textures and complex lighting, exact/material-level friction parameters, sensor noise models, precise actuator delays/communication timing—these aspects were approximate or not explicitly modeled</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical tabletop setup with the same Franka-Emika Panda arm pushing a 6 cm cube; RGB camera placed opposite the robot, real lighting (multiple diffused sources) and different textures; small camera pose offset (~5 cm) and lighting/appearance differences relative to simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Non-prehensile object pushing: pushing a cube to a fixed target position within a 5 cm tolerance within 40-step episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (Soft Actor-Critic) trained in latent space coupled with representation learning (DVAE) trained online from collected experience; DVAE trained with variational loss plus one-step latent dynamics predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success rate (episode-level): episode counts as success if cube reaches target within 5 cm tolerance within 40 steps; tracking of episodes-to-reach 80% and 90% success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Sim from scratch: 98% best success; DVAE-SAC sim-to-sim best: e.g. minimal-gap 95% (see fidelity comparison results for full table).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Sim-to-real (minimal-gap) best achieved: 92% success; initial zero-shot ~5-10% depending on variation; 80% reached after 550 episodes (~5 h) and 90% after 990 episodes (~10 h).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Differences in camera pose/viewpoint, lighting and illumination, surface textures and visual appearance, rendering inaccuracies, and likely unmodeled differences in contact/friction dynamics between sim and reality.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Decoupled architecture that allows freezing the control policy and the learned latent dynamics predictor while fine-tuning only encoder/decoder on real data; the frozen dynamics predictor constrains latent representation so it remains compatible with the policy; unsupervised online fine-tuning of DVAE on real experience drastically reduces need to run RL in real environment; bootstrap ensembles and pretrained MobileNet encoder backbone improved stability.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No strict numeric fidelity thresholds given; qualitatively: small visual/dynamics gaps (minor camera offset, lighting differences) enable fast adaptation (few hundred episodes), whereas large viewpoint changes (e.g., 90° yaw) or radical observation shifts require as much data as training from scratch; maintaining compatibility of latent dynamics representation is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Only the encoder and decoder of the DVAE were fine-tuned on real-world experience while keeping the SAC policy and the DVAE dynamics predictor frozen; unsupervised finetuning on online real experience: ~550 episodes (~5 h) to reach 80% success, ~990 episodes (~10 h) to reach 90% (minimal-gap sim-to-real).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Table 3 (selected): Sim from scratch: best 98%, init success 5%, T.T.80% = 1020 eps, T.T.90% = 1180 eps. S2S-Minimal Gap: best 95%, init 50%, T.T.80% = 90 eps, T.T.90% = 110 eps. S2S-Small Gap: best 92%, init 5%, T.T.80% = 210 eps, T.T.90% = 950 eps. S2S-Medium Gap: best 92%, init 5%, T.T.80% = 320 eps, T.T.90% = 1250 eps. S2S-Large Gap: best 85%, init 5%, T.T.80% = 1200 eps (T.T.90% not reached). S2R-Minimal Gap: best 92%, init 5%, T.T.80% = 550 eps, T.T.90% = 990 eps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A decoupled predictive latent representation (DVAE) plus SAC enables efficient sim-to-real transfer by fine-tuning only the visual encoder/decoder on real data while freezing the policy and the latent dynamics predictor; the dynamics predictor constrains latent drift and preserves policy compatibility. For small visual/domain gaps this yields large speedups (3–4x in early learning); for large viewpoint/domain gaps adaptation can require as much data as training from scratch. Removing the dynamics predictor prevents successful transfer (latent drift causes policy failure).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1646.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1646.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE-SAC (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder with Soft Actor-Critic (VAE-SAC) — no dynamics predictor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of the main method that removes the one-step latent dynamics predictor from the DVAE, leaving a VAE whose latent outputs feed an SAC policy; used to evaluate the role of the dynamics predictor for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka-Emika Panda 7-DOF robotic arm (same experimental robot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Same tabletop pushing manipulator as used with DVAE-SAC: RGB camera + 2D proprioception, 2D continuous action control of end-effector displacement.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (non-prehensile object pushing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Same simulated Gazebo environment as used for DVAE-SAC experiments (robot, table, cube, camera observations).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>moderate-fidelity physics with non-photorealistic rendering (approximate contacts)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>robot kinematics, object geometry, basic contacts, rendered camera images</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>no constrained latent dynamics regularization (by design), simplified textures/lighting and approximate friction/contact parameters</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Not directly transferred to real in the presented experiments; sim-to-sim transfers evaluated to test robustness of latent representation when dynamics predictor absent.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Non-prehensile object pushing (sim-to-sim transfer scenarios used to evaluate method robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (SAC) with VAE-based latent representation learned jointly; dynamics predictor removed (ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success rate (episode-level) as in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Reported failure in sim-to-sim transfer: even in minimal-gap scenarios initial non-zero zero-shot performance rapidly decays to random-policy performance; no sustained successful transfer across variations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Not applicable (primary failure mode stems from unconstrained latent representation that drifts when encoder is fine-tuned, breaking compatibility with the frozen policy).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Not applicable — ablation shows lack of latent dynamics predictor removes the constraint that preserves latent-policy compatibility, causing transfer to fail even for modest visual gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Qualitative finding: maintaining a dynamics-constrained latent space is required to keep a policy trained in one domain compatible when encoder is adapted to another domain; mere reconstruction VAE constraints are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>VAE-SAC sim-to-sim experiments showed that without the dynamics predictor domain transfer fails: performance drops to near-random even in minimal-gap cases and decays in small-gap cases (plots in Figure 8).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A VAE representation without a learned latent dynamics predictor fails to preserve policy-compatibility under encoder adaptation; the one-step dynamics predictor in DVAE is critical as a constraint that prevents latent drift and enables successful transfer when only the encoder/decoder are fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks <em>(Rating: 2)</em></li>
                <li>Solving rubik's cube with a robot hand <em>(Rating: 2)</em></li>
                <li>Neuralsim: Augmenting differentiable simulators with neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1646",
    "paper_id": "paper-255826915",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "DVAE-SAC",
            "name_full": "Decoupled Variational Autoencoder with Soft Actor-Critic (DVAE-SAC)",
            "brief_description": "A decoupled visuomotor RL architecture that learns a predictive latent representation (DVAE) and a Soft Actor-Critic policy operating on latent states; enables sim-to-real transfer by freezing the policy and latent dynamics predictor and fine-tuning only the encoder/decoder on real data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka-Emika Panda 7-DOF robotic arm",
            "agent_system_description": "A 7-DOF tabletop manipulator controlled in Cartesian space; end-effector moves horizontally within a 45 cm square workspace and issues 2D continuous displacements as actions; observations are an RGB camera (128x128) plus 2D end-effector proprioception.",
            "domain": "general robotics manipulation (non-prehensile object pushing)",
            "virtual_environment_name": "Gazebo",
            "virtual_environment_description": "Gazebo simulation of the robot and table-top pushing task, providing physics simulation (robot kinematics, contact interactions, friction approximations) and rendered RGB camera observations; used as the source domain for training.",
            "simulation_fidelity_level": "moderate-fidelity physics with non-photorealistic rendering (approximate contact dynamics and basic lighting)",
            "fidelity_aspects_modeled": "robot kinematics, contact interactions between end-effector/object/table, basic lighting and camera viewpoint rendering, object geometry and workspace layout",
            "fidelity_aspects_simplified": "photorealistic textures and complex lighting, exact/material-level friction parameters, sensor noise models, precise actuator delays/communication timing—these aspects were approximate or not explicitly modeled",
            "real_environment_description": "Physical tabletop setup with the same Franka-Emika Panda arm pushing a 6 cm cube; RGB camera placed opposite the robot, real lighting (multiple diffused sources) and different textures; small camera pose offset (~5 cm) and lighting/appearance differences relative to simulation.",
            "task_or_skill_transferred": "Non-prehensile object pushing: pushing a cube to a fixed target position within a 5 cm tolerance within 40-step episodes.",
            "training_method": "Reinforcement learning (Soft Actor-Critic) trained in latent space coupled with representation learning (DVAE) trained online from collected experience; DVAE trained with variational loss plus one-step latent dynamics predictor.",
            "transfer_success_metric": "Success rate (episode-level): episode counts as success if cube reaches target within 5 cm tolerance within 40 steps; tracking of episodes-to-reach 80% and 90% success rates.",
            "transfer_performance_sim": "Sim from scratch: 98% best success; DVAE-SAC sim-to-sim best: e.g. minimal-gap 95% (see fidelity comparison results for full table).",
            "transfer_performance_real": "Sim-to-real (minimal-gap) best achieved: 92% success; initial zero-shot ~5-10% depending on variation; 80% reached after 550 episodes (~5 h) and 90% after 990 episodes (~10 h).",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Differences in camera pose/viewpoint, lighting and illumination, surface textures and visual appearance, rendering inaccuracies, and likely unmodeled differences in contact/friction dynamics between sim and reality.",
            "transfer_enabling_conditions": "Decoupled architecture that allows freezing the control policy and the learned latent dynamics predictor while fine-tuning only encoder/decoder on real data; the frozen dynamics predictor constrains latent representation so it remains compatible with the policy; unsupervised online fine-tuning of DVAE on real experience drastically reduces need to run RL in real environment; bootstrap ensembles and pretrained MobileNet encoder backbone improved stability.",
            "fidelity_requirements_identified": "No strict numeric fidelity thresholds given; qualitatively: small visual/dynamics gaps (minor camera offset, lighting differences) enable fast adaptation (few hundred episodes), whereas large viewpoint changes (e.g., 90° yaw) or radical observation shifts require as much data as training from scratch; maintaining compatibility of latent dynamics representation is critical.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Only the encoder and decoder of the DVAE were fine-tuned on real-world experience while keeping the SAC policy and the DVAE dynamics predictor frozen; unsupervised finetuning on online real experience: ~550 episodes (~5 h) to reach 80% success, ~990 episodes (~10 h) to reach 90% (minimal-gap sim-to-real).",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Table 3 (selected): Sim from scratch: best 98%, init success 5%, T.T.80% = 1020 eps, T.T.90% = 1180 eps. S2S-Minimal Gap: best 95%, init 50%, T.T.80% = 90 eps, T.T.90% = 110 eps. S2S-Small Gap: best 92%, init 5%, T.T.80% = 210 eps, T.T.90% = 950 eps. S2S-Medium Gap: best 92%, init 5%, T.T.80% = 320 eps, T.T.90% = 1250 eps. S2S-Large Gap: best 85%, init 5%, T.T.80% = 1200 eps (T.T.90% not reached). S2R-Minimal Gap: best 92%, init 5%, T.T.80% = 550 eps, T.T.90% = 990 eps.",
            "key_findings": "A decoupled predictive latent representation (DVAE) plus SAC enables efficient sim-to-real transfer by fine-tuning only the visual encoder/decoder on real data while freezing the policy and the latent dynamics predictor; the dynamics predictor constrains latent drift and preserves policy compatibility. For small visual/domain gaps this yields large speedups (3–4x in early learning); for large viewpoint/domain gaps adaptation can require as much data as training from scratch. Removing the dynamics predictor prevents successful transfer (latent drift causes policy failure).",
            "uuid": "e1646.0",
            "source_info": {
                "paper_title": "Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "VAE-SAC (ablation)",
            "name_full": "Variational Autoencoder with Soft Actor-Critic (VAE-SAC) — no dynamics predictor",
            "brief_description": "An ablation of the main method that removes the one-step latent dynamics predictor from the DVAE, leaving a VAE whose latent outputs feed an SAC policy; used to evaluate the role of the dynamics predictor for transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka-Emika Panda 7-DOF robotic arm (same experimental robot)",
            "agent_system_description": "Same tabletop pushing manipulator as used with DVAE-SAC: RGB camera + 2D proprioception, 2D continuous action control of end-effector displacement.",
            "domain": "general robotics manipulation (non-prehensile object pushing)",
            "virtual_environment_name": "Gazebo",
            "virtual_environment_description": "Same simulated Gazebo environment as used for DVAE-SAC experiments (robot, table, cube, camera observations).",
            "simulation_fidelity_level": "moderate-fidelity physics with non-photorealistic rendering (approximate contacts)",
            "fidelity_aspects_modeled": "robot kinematics, object geometry, basic contacts, rendered camera images",
            "fidelity_aspects_simplified": "no constrained latent dynamics regularization (by design), simplified textures/lighting and approximate friction/contact parameters",
            "real_environment_description": "Not directly transferred to real in the presented experiments; sim-to-sim transfers evaluated to test robustness of latent representation when dynamics predictor absent.",
            "task_or_skill_transferred": "Non-prehensile object pushing (sim-to-sim transfer scenarios used to evaluate method robustness).",
            "training_method": "Reinforcement learning (SAC) with VAE-based latent representation learned jointly; dynamics predictor removed (ablation).",
            "transfer_success_metric": "Success rate (episode-level) as in main experiments.",
            "transfer_performance_sim": "Reported failure in sim-to-sim transfer: even in minimal-gap scenarios initial non-zero zero-shot performance rapidly decays to random-policy performance; no sustained successful transfer across variations.",
            "transfer_performance_real": null,
            "transfer_success": false,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Not applicable (primary failure mode stems from unconstrained latent representation that drifts when encoder is fine-tuned, breaking compatibility with the frozen policy).",
            "transfer_enabling_conditions": "Not applicable — ablation shows lack of latent dynamics predictor removes the constraint that preserves latent-policy compatibility, causing transfer to fail even for modest visual gaps.",
            "fidelity_requirements_identified": "Qualitative finding: maintaining a dynamics-constrained latent space is required to keep a policy trained in one domain compatible when encoder is adapted to another domain; mere reconstruction VAE constraints are insufficient.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "VAE-SAC sim-to-sim experiments showed that without the dynamics predictor domain transfer fails: performance drops to near-random even in minimal-gap cases and decays in small-gap cases (plots in Figure 8).",
            "key_findings": "A VAE representation without a learned latent dynamics predictor fails to preserve policy-compatibility under encoder adaptation; the one-step dynamics predictor in DVAE is critical as a constraint that prevents latent drift and enables successful transfer when only the encoder/decoder are fine-tuned.",
            "uuid": "e1646.1",
            "source_info": {
                "paper_title": "Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks",
            "rating": 2,
            "sanitized_title": "simtoreal_via_simtosim_dataefficient_robotic_grasping_via_randomizedtocanonical_adaptation_networks"
        },
        {
            "paper_title": "Solving rubik's cube with a robot hand",
            "rating": 2,
            "sanitized_title": "solving_rubiks_cube_with_a_robot_hand"
        },
        {
            "paper_title": "Neuralsim: Augmenting differentiable simulators with neural networks",
            "rating": 1,
            "sanitized_title": "neuralsim_augmenting_differentiable_simulators_with_neural_networks"
        }
    ],
    "cost": 0.01322075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OPEN ACCESS EDITED BY Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies
January 2023</p>
<p>Chao Zeng 
Yantian Zha 
Wenkai Chen 
Carlo Rizzardo carlo.rizzardo@iit.it 
Carlo Rizzardo 
CORRESPONDENCE
SPECIALTY SECTION</p>
<p>Active Perception and Robot Interactive Learning Laboratory, Advanced Robotics, Istituto Italiano di Tecnologia
GenovaItaly</p>
<p>Fei Chen 
Active Perception and Robot Interactive Learning Laboratory, Advanced Robotics, Istituto Italiano di Tecnologia
GenovaItaly</p>
<p>Department of Mechanical and Automation Engineering
T-Stone Robotics Institute
The Chinese University of Hong Kong
Hong KongChina</p>
<p>Darwin Caldwell 
Active Perception and Robot Interactive Learning Laboratory, Advanced Robotics, Istituto Italiano di Tecnologia
GenovaItaly</p>
<p>University of Hamburg
Germany REVIEWED BY</p>
<p>University of Maryland
United States</p>
<p>University of Hamburg
Germany</p>
<p>OPEN ACCESS EDITED BY Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies</p>
<p>TYPE Original Research PUBLISHED
12January 202310.3389/frobt.2022.1067502This article was submitted to Robot Learning and Evolution, a section of the journal Frontiers in Robotics and AI RECEIVED 11 October 2022 ACCEPTED 21 December 2022 PUBLISHED 12 January 2023 CITATIONRizzardo C, Chen F and Caldwell D (2023), Sim-to-real via latent prediction: Transferring visual non-prehensile manipulation policies. Front. Robot. AI 9:1067502.reinforcement learning (RL)roboticsmanipulationvariational techniquesdynamicspushing Frontiers in Robotics and AI 01
Reinforcement Learning has been shown to have a great potential for robotics. It demonstrated the capability to solve complex manipulation and locomotion tasks, even by learning end-to-end policies that operate directly on visual input, removing the need for custom perception systems. However, for practical robotics applications, its scarce sample efficiency, the need for huge amounts of resources, data, and computation time can be an insurmountable obstacle. One potential solution to this sample efficiency issue is the use of simulated environments. However, the discrepancy in visual and physical characteristics between reality and simulation, namely the sim-to-real gap, often significantly reduces the real-world performance of policies trained within a simulator. In this work we propose a sim-to-real technique that trains a Soft-Actor Critic agent together with a decoupled feature extractor and a latent-space dynamics model. The decoupled nature of the method allows to independently perform the sim-to-real transfer of feature extractor and control policy, and the presence of the dynamics model acts as a constraint on the latent representation when finetuning the feature extractor on realworld data. We show how this architecture can allow the transfer of a trained agent from simulation to reality without retraining or finetuning the control policy, but using real-world data only for adapting the feature extractor. By avoiding training the control policy in the real domain we overcome the need to apply Reinforcement Learning on real-world data, instead, we only focus on the unsupervised training of the feature extractor, considerably reducing real-world experience collection requirements. We evaluate the method on sim-to-sim and sim-to-real transfer of a policy for table-top robotic object pushing. We demonstrate how the method is capable of adapting to considerable variations in the task observations, such as changes in point-of-view, colors, and lighting, all while substantially reducing the training time with respect to policies trained directly in the real.</p>
<p>Reinforcement Learning has been shown to have a great potential for robotics. It demonstrated the capability to solve complex manipulation and locomotion tasks, even by learning end-to-end policies that operate directly on visual input, removing the need for custom perception systems. However, for practical robotics applications, its scarce sample efficiency, the need for huge amounts of resources, data, and computation time can be an insurmountable obstacle. One potential solution to this sample efficiency issue is the use of simulated environments. However, the discrepancy in visual and physical characteristics between reality and simulation, namely the sim-to-real gap, often significantly reduces the real-world performance of policies trained within a simulator. In this work we propose a sim-to-real technique that trains a Soft-Actor Critic agent together with a decoupled feature extractor and a latent-space dynamics model. The decoupled nature of the method allows to independently perform the sim-to-real transfer of feature extractor and control policy, and the presence of the dynamics model acts as a constraint on the latent representation when finetuning the feature extractor on realworld data. We show how this architecture can allow the transfer of a trained agent from simulation to reality without retraining or finetuning the control policy, but using real-world data only for adapting the feature extractor. By avoiding training the control policy in the real domain we overcome the need to apply Reinforcement Learning on real-world data, instead, we only focus on the unsupervised training of the feature extractor, considerably reducing real-world experience collection requirements. We evaluate the method on sim-to-sim and sim-to-real transfer of a policy for table-top robotic object pushing. We demonstrate how the method is capable of adapting to considerable variations in the task observations, such as changes in point-of-view, colors, and lighting, all while substantially reducing the training time with respect to policies trained directly in the real.</p>
<p>Introduction</p>
<p>To this day, manipulation and physical interaction tasks remain open problems in robotics. The difficulty of modeling the environment, identifying its characteristics, and detecting and tracking elements of interests makes these tasks particularly challenging for classic control approaches. Reinforcement Learning approaches instead can tackle these issues implicitly, and have been shown to be capable of solving even the most complex manipulation problems OpenAI et al. (2019). However, the use of Reinforcement Learning (RL) methods also poses significant challenges. Most RL techniques are considerably sample inefficient, they require huge amounts of resources, data and computation time. Also, training a policy on real hardware without proper precautions may damage the hardware itself or its surroundings. These issues can be tackled from different perspectives, on one side with algorithmic improvements that improve sample efficiency and lower data requirements, on the other with techniques to efficiently acquire huge amounts of data, for example by exploiting simulation [OpenAI et al. (2019); Rudin et al. (2021)].</p>
<p>Standard RL algorithms such as DQN Mnih et al. (2015), PPO Schulman et al. (2017), or SAC (Haarnoja et al. (2018a); Haarnoja et al. (2018b)) have huge data requirements, especially for vision-based tasks. Such tasks have traditionally been solved by directly utilizing image observations in an end-toend manner, the same way as tasks with low-dimensional observations are handled. Several recent works however have progressively improved sample efficiency for visual tasks by departing from this simple approach.</p>
<p>SAC-AE [Yarats et al. (2021b)], SLAC [Lee et al. (2020)] or CURL Srinivas et al. (2020) have tackled the problem by combining Reinforcement Learning and Representation Learning methods. Representation Learning is used to aid the training of the visual feature extractor section of the agent. In purely RL methods, the training is performed solely from the reward signal, even for what concerns visual understanding.</p>
<p>Here instead other sources of information are used such as image reconstruction or contrastive losses, greatly improving sample efficiency.</p>
<p>Other approaches, such as RAD [Laskin et al. (2020a)] and DrQ [Kostrikov et al. (2020); Yarats et al. (2022)] have shown how the use of simple image augmentation techniques can vastly improve sample efficiency, reaching performance on par with that of methods which have access to state knowledge.</p>
<p>Another direction yet has been the idea of using the experience data collected online during training to learn a model of the environment, capable of predicting whole trajectories. These kind of models can then be used to solve the task via planning, like for example in PlaNet [Hafner et al. (2019)], or to generate additional training data, either in the observation space or in learned latent spaces, such as in Dreamer [Hafner et al. (2020);Hafner et al. (2021)].</p>
<p>In this work we explore the idea of exploiting Representation Learning and environment modeling to efficiently perform sim-to-real transfer. We define an RL agent that completely decouples feature extractor and control policy training. The feature extractor is learned as part of a full model of the environment based on Variational Autoencoders (VAE) [Kingma and Welling (2014); Rezende et al. (2014)], capable of predicting observations and rewards. The control policy is a Soft Actor-Critic agent that acts on the latent representation defined by the aforementioned model. We show how this architecture allows to transfer a control policy trained in simulation to the real world by only finetuning the encoder and decoder sections of the VAE model. This completely removes the need of performing Reinforcement Learning training in the real environment, strongly reducing real-world data requirements while at the same time maintaining high sample efficiency in simulation.</p>
<p>We evaluate the method on a tabletop non-prehensile manipulation task, in which a Franka-Emika Panda robotic arm has the objective of pushing an object to a predetermined destination. We choose this task as it is fairly simple and manageable, but at the same time presents difficulties that make it a suitable ground for evaluating model-free reinforcement learning methods such as ours. As discussed in Ruggiero et al. (2018), non-prehensile manipulation, and specifically object pushing, is a particularly challenging task for classic control methods due to the indeterminacy brought by friction forces, both between manipulated object and ground and between object and robot. Modeling such interactions precisely is extremely challenging, identifying friction characteristics is a complex problem in itself and minute errors in the modeling have large impacts in the motion of the manipulated objects. Instead, model-free robot learning approaches such as ours handle these problematics implicitly without requiring careful explicit modeling of the system and can consequently solve this task effectively and reliably. 1 Also, object pushing already presents exploration difficulties not present in simpler tasks, such as for example pose reaching. The agent has to first learn to reach the object, and then it must learn to bring it to the correct position. We perform sim-to-sim experiments with different variations of the scenario, from simple alterations to the colors of the scene to radical changes in the camera point of view. We then validate the approach with sim-to-real experiments.</p>
<p>1 We consider our method to be model-free as we do not use the learned dynamics model to plan trajectories or generate data, but only as a regularization tool. This follows the same convention used in Lee et al. (2020).</p>
<p>Frontiers in Robotics and AI 02 frontiersin.org</p>
<p>Related works</p>
<p>Our work builds upon the intersection of two research areas: the use of sim-to-real to overcome real-world data scarcity and the development of decoupled Reinforcement Learning methods. The first focuses on exploiting simulation data to train real-world models by bridging the reality gap, the second on improving sample efficiency in Reinforcement Learning, by decoupling feature extraction from policy training.</p>
<p>The RL architecture we propose exploits its decoupled nature to effectively overcome the reality gap, reducing real-world data requirements while at the same time maintaining good sample efficiency in the simulation domain.</p>
<p>Sim-to-real</p>
<p>Reinforcement Learning methods require vast amounts of data to be effectively trained. The more a task is complex, in terms of observation and action dimensionality, or exploration difficulty, the more experience is required. Complex tasks can easily require days or weeks of experience data to be solved. Acquiring such amounts of experience on real robotic systems is impractical, keeping a complex robotic system running for such lengths of time is complex, additional infrastructure for managing the environment setup are required, and untrained policies can potentially damage the robot or the environment. All of these issues become even more complex in a research environment, where numerous trainings have to be performed for experimental reasons. Consequently, the use of synthetic experience has a natural appeal for Reinforcement Learning methods.</p>
<p>Sim-to-real RL methods exploit simulation software to efficiently train policies in virtual reproductions of the target environment, and then transfer the policy to the real-world domain by overcoming the reality gap, the discrepancy between simulation and reality.</p>
<p>The advantage of simulation is first of all the possibility of generating vast amounts of experience much more rapidly than it would be possible in the real world. This can be achieved by simulating faster than real-time and by parallelizing multiple simulated environments. Gorila Nair et al. (2015), A2C and A3C Mnih et al. (2016) showed how parallelizing experience collection leads to substantial improvements in training time. More recently, Rudin et al. (2021) exploited modern hardware and simulation software to massively parallelize an environment for quadruped locomotion training, achieving in just 20 min a PPO gait policy capable of successfully controlling a real robot on complex terrains. Furthermore, beyond just generating huge amounts of data, simulation software can also support training strategies that would be impossible in the real world. Pinto et al. (2018) shows how it is possible to speed-up training considerably by using simulator state knowledge during training, and how to transfer a policy trained in such a way to the real world, where this knowledge is unavailable.</p>
<p>As we mentioned, the core issue with simulation training is the reality gap, the discrepancy between the characteristics of the simulated environment and those of the real one. These differences can be in the dynamics of the environment, due to inaccuracies in the physics simulation, in the observations the agent makes, due to imprecision in the visual rendering or in the sensory input in general, or simply in the behavior of robotic components, which may be implemented differently in simulation and reality. Advances in realistic simulation software [NVIDIA (2020); Unity (2020)] are progressively narrowing the reality gap, but sim-to-real transfer remains non-trivial as constructing simulations that closely match the real world remains a challenging task that requires considerable engineering work.</p>
<p>Numerous strategies have been implemented to overcome the reality gap. In general, we can distinguish between two families of techniques: those that aim at obtaining a policy capable of operating in both the real and the simulation without using real-world data, and those that use real-world data for adapting a model learned in simulation to the real domain. We refer to these latter ones as domain adaptation methods. The most simple approach of these is to just perform policy finetuning in the real, the same way it usually is done in supervised learning settings. The policy is first trained in simulation, then the agent is transferred to the real and the training continues in the real until satisfactory performance is achieved. However, such strategy often still requires considerable realworld experience collection, and it is not guaranteed the robot will behave properly and safely when first transferred to the real domain.</p>
<p>Other methods explicitly target the issue of matching the output of feature extractors between the simulated domain and the real domain, creating feature extractors that are invariant to the switch between simulated and real-world inputs. This can be achieved via different approaches. Some methods try to train feature extractors for the two domains while keeping the distributions of the two resulting feature representations similar, with losses based on distribution distance metrics such as Maximum Mean Discrepancy (MMD) [Tzeng et al. (2014)], MK-MMD [Long et al. (2015)] or others [Sun and Saenko (2016)]. Others try to keep the feature representations of samples from the two domains close via Adversarial approaches. A discriminator network is trained to classify feature vectors between the two domains, the feature extractor is then optimized to generate indistinguishable representations [Tzeng et al. (2015); Tzeng et al. (2017); Ganin and Lempitsky (2015)]. Alternatively, other techniques take inspiration from style transfer methods and directly convert target-domain samples into sourcedomain samples or samples from a third "canonical" domain [Bousmalis et al. ( However, even if some of these methods work well for vision tasks, they may not adapt effectively to difficult exploration problems. The aforementioned approaches either require target data to be available while performing the original source domain training or they train the encoder with offline data. This is problematic, as in difficult exploration problems collecting fully representative data before completely training the policy may be impractical or impossible. In tasks such as object pushing it may be possible to collect human-generated demonstrations, but in more complex tasks, for example locomotion, collecting demonstrations is not trivial.</p>
<p>A sim-to-real approach that does not suffer from this issue is Domain Randomization. The core idea of the method is to randomize visual [Tobin et al. (2017)] and physical [Peng et al. (2017)] characteristics of the simulated environment, so that once the agent is transferred in the real world it can interpret the new domain as just another random variation. These methods can be applied to both visual and state-based tasks, and have been extremely successful, being able to solve even extremely complex visuomotor control problems while maintaining strong robustness to occulusions and perturbations [OpenAI et al. (2019)]. However, as tasks get more complex, they require huge amounts of simulation data and long training times. To reduce these issues, various methods have been proposed to constrain the amount of randomization to just what is necessary. Ramos et al. (2019), Possas et al. (2020) and Muratore et al. (2021) achieve this by identifying simulator parameter distributions via Likelihoodfree Inference. Heiden et al. (2021) instead shows how it is possible to use differentiable simulators to identify possible simulator parameters from real data.</p>
<p>Decoupled RL</p>
<p>Training Reinforcement Learning policies for visual tasks has traditionally been considerably more expensive than for statebased tasks, in terms of sample-complexity and computation time. The increased dimensionality of the observation space naturally complicates the problem, as the agent needs to learn to interpret visual information and extract the necessary features from it. However, multiple recent works have shown how the performance gap between image-based and state-based tasks can be greatly reduced.</p>
<p>One extremely simple and effective technique is the use of data augmentation during training. RAD [Laskin et al. (2020a)], DrQ [Kostrikov et al. (2020)] and DrQv2 [Yarats et al. (2022)] have shown how even just simple image augmentations such as pixel shift can drastically improve sample efficiency of model free RL methods, reaching performance comparable to that achieved on equivalent state-based tasks.</p>
<p>Other works instead exploit unsupervised learning methods to aid the extraction of visual features. SLAC [Lee et al. (2020)] trains a predictive stochastic latent variable model and uses the resulting latent space to train a Soft Actor-Critic policy. SAC + AE (Yarats et al. (2021b)) instead uses a regularized autoencoder [Ghosh et al. (2020)] to extract a latent space via observation reconstruction. It then uses the resulting latent vector as inputs for a Soft Actor-Critic policy.</p>
<p>PlaNet (Hafner et al. (2019)) brings these ideas forward by learning a full latent model of the environment, then uses this model to plan trajectories via Model Predictive Control. Dreamer (Hafner et al. (2020)) and DreamerV2 [Hafner et al. (2021)] then use the latent model from PlaNet to train an Actor-Critic policy in the latent space, generating huge amounts of experience via imagination and exploiting the differentiable nature of the neural network model.</p>
<p>Methods</p>
<p>The method we propose in this work employs ideas from research in decoupled Reinforcement Learning methods to perform sim-to-real transfer of visuomotor control policies via domain adaptation. More specifically we define an RL architecture composed of a predictive latent variable model and a Soft Actor-Critic agent, and we propose a training procedure for sim-to-real that takes advantage of the presence of the decoupled predictive model to effectively finetune the latent encoder to the real environment. Following this method it is possible to transfer a trained agent from simulation to reality by finetuning just the feature extractor, independently of the learned control policy.</p>
<p>Differently from the methods discussed in Section 2, our proposed method is at the same time unsupervised, sample efficient in source and target domain, capable of solving difficult exploration problems, and does not require target domain data before performing the transfer.</p>
<p>Problem formulation</p>
<p>We formalize our setting as a Partially Observable Markov Decision Process (POMDP), defined by the 7-tuple Frontiers in Robotics and AI 04 frontiersin.org</p>
<p>FIGURE 1</p>
<p>POMDP formulation of the problem. In orange the policy we implement, in blue the sensory channel, in black the underlying Markov Decision Process.</p>
<p>FIGURE 2</p>
<p>Overview of the proposed architecture. O hd,t indicates the high-dimensional observation at step t, O ld,t is the respective low-dimensional observation, a t is the action taken at step t, z t is the latent state vector at time t, z t+1 is the predicted latent state vector, O hd,t+1 and O ld,t+1 are the predicted observations.</p>
<p>(S, A, p, r, γ, O, v). The first five terms represent respectively the state space, the action space, the state transition probability density p(s t+1 |s t , a t ), the reward function r(s t , a t ) and the discount factor. The last two terms represent the observation space and observation density v(o t |s t ) = p[o t = o|s t = s] which defines our sensory channel. The system is represented in Figure 1.</p>
<p>The objective of the method is to learn a policy π(a t |o t ) that maximizes the expected discounted total sum of rewards R(π) = E s 0:T (∑ T t=0 γ t r(s t , a t )). Historically, standard end-to-end RL methods have usually employed an MDP formulation instead of a POMDP one, meaning that the state is observed directly or the highdimensional observation is equated with the state. Instead, in the POMDP formulation the state is not observed directly, but only through a stochastic sensory channel, formalized with the v(o t |s t ) density function.</p>
<p>Guided by this formulation, we use a representation learning approach to approximate an e(z t |o t ) density, which estimates a state representation z t from sensor observations o t . Once the representation is learned, we have at our disposal a method to approximate low-dimensional state vectors from high-dimensional sensor inputs and we can use standard RL methods to learn a control policy in the latent space.</p>
<p>Agent architecture 3.2.1 Variational autoencoder</p>
<p>A natural choice for learning the state representation is the use of autoencoders, in particular of variational autoencoders (VAE) Kingma and Welling (2014). VAEs are a solid and proven method to learn a low-dimensional representation of the state, giving us a method to reliably produce lowdimensional latent vectors from high-dimensional observations. We define our VAE architecture as a stochastic encoder e θ (z t |o t ) that maps observations o t ∈ ℝ n×n to latent representations z t ∈ ℝ k and a deterministic decoder d θ (z t ) that performs the opposite transformation. The encoder is defined as a conditional multivariate Gaussian density with diagonal covariance. The dimensionality k of the latent space is left as a hyperparameter, which can be tuned depending on the task at hand.</p>
<p>In all our experiments we used an encoder architecture based on a MobileNet V3 backend Howard et al. (2019) initialized with pretrained weights, the output of the backend was matched to the Gaussian density mean and log-variance with two separate linear layers. The decoder was defined symmetrically using transposed convolution layers.</p>
<p>In practice, during policy inference we used the encoder deterministically by utilizing only the mean of the distribution to produce the latent vectors.</p>
<p>Dynamics modeling</p>
<p>Within the latent space of the Variational Autoencoder we introduce a one-step dynamics predictor, that from a latent vector and an action predicts a latent representation for the next state. Formally, the dynamics predictor is defined as a function f(z t , a t ) ↦ẑ t+1 , where z t is the latent representation for the state s t , a t is the action at time t andẑ t+1 is a representation of s t+1 . In practice the dynamics model is implemented as a fully connected neural network f θ : ℝ k+m → ℝ k with m being the action space dimensionality and k being the latent representation size. We make the choice of introducing the predictor following two intuitions: one is that the presence of the dynamics predictor imposes a regularization toward features more suited for control, the other is that the presence of the predictor can be used to constrain the latent representation when performing policy transfer. We refer to this overall representation architecture as DVAE.</p>
<p>It must be noted that, while our dynamics model is useful in shaping and constraining the latent representation, it cannot be used to make actual latent-space trajectory predictions. This is because the input and output latent spaces of the network are not constrained to represent the same features in the same way, or to have the same distribution.</p>
<p>As a whole, the architecture is trained using the usual variational loss, composed of the isotropic multivariate Gaussian<br />
L DV AE (θ; o t , a t , r t , o t+1 ) = D KL (e θ (z t |o t ) ‖N (0, I k )) + αMSE (ô t+1 ,r t ; o t+1 , r t )(1)
With (ẑ t+1 ,r t ) = f θ (e θ (o t ), a t ) andô t+1 = d θ (ẑ t+1 ), D KL being the KL-divergence, MSE the mean squared error, I k the k × k identity matrix and N (0, I k ) a centered isotropic Gaussian distribution.</p>
<p>Including low-dimensional sensor data</p>
<p>As we mentioned, the encoder and decoder sections of the architecture are implemented respectively with a MobileNet network and a series of transposed convolutions. These architectures are suited to image inputs, however, in robotics applications it is common to have nonhomogeneous sensory inputs, some characterized by a high dimensionality, like cameras, and others by lowdimensionality, like motor encoders or force torque sensors. Our proposed architecture gives us a natural way to combine these heterogeneous inputs, as we can combine all of these observations of the environment into the latent state representation.</p>
<p>In the simple case of one visual input combined with mono-dimensional sensor readings, we can leave the encoder architecture unchanged and directly concatenate the lowdimensional observations to the encoder output. For its simplicity, we chose to follow this simple approach in our experiments.</p>
<p>The overall architecture is represented in Figure 2.</p>
<p>Policy learning</p>
<p>Finally, the control policy π(a t |o t ) can be learned with any standard RL method. The RL algorithm only receives as input the state representation z, composed of the encoder output and, optionally, the low-dimensional observations. In practice we chose to use Soft Actor Critic (SAC) throughout our experiments because of the flexibility and generality of the method. We derived our implementation from the one provided by stable_baselines3 Raffin et al. (2021).</p>
<p>Bootstrap ensembles</p>
<p>To improve the reliability and repeatability of the method, and following evidence from Chua et al. (2018) and Nagabandi et al. (2019) we exploit bootstrap ensembles in the dynamics model, in the encoder and in the SAC actor network. The output of the single networks are aggregated performing a simple average. This in practice results in a more reliable training performance, converging faster to a correct solution and reducing the variability introduced by the network initialization and the environment randomness.</p>
<p>Training the agent</p>
<p>We train the DVAE-SAC agent online, by collecting experience via the current policy and alternating between training the DVAE latent extractor and the SAC control     Success rate progress for the sim-to-sim experiments. Figure (A) shows the minimal-gap scenario: It is possible to see the performance difference depending on the cube color. Figure (B) displays the results for all the scenarios in an aggregated form, the plots show the average performance across seeds on a 100 episode window. For the minimal, small, medium and large scenarios we used respectively 8, 12, 16, and 4 seeds. The shaded area represents a 95% confidence interval.</p>
<p>policy. The experience is collected in the form of (o t , a t , r t , o t+1 ) transitions and stored in one single replay buffer, which is used as the training set for both the policy and the latent extractor.</p>
<p>Algorithm 1 at Figure 3 shows the overall training procedure.</p>
<p>Transferring the agent</p>
<p>In performing the domain transfer the objective is to adapt the agent to the new environment while avoiding to lose the knowledge acquired during the source domain training. Such transfer could be attempted by simply finetuning the whole agent on target domain data, however in practice this does not perform well due to catastrophic forgetting. This is particularly problematic as policy training may potentially restart from scratch, as no experience for the later stages of successful episodes would be available until the environment is explored again.</p>
<p>To prevent these issues, we take advantage of the decoupled nature of the DVAE-SAC architecture: we freeze the SAC agent and transfer it as-is, while only finetuning the DVAE. Crucially, to prevent the latent representation from drifting and becoming incompatible with the SAC policy we also freeze the dynamics predictor section of the DVAE. In practice this means only the encoder and decoder sections of the architecture are adapted to the target domain.</p>
<p>FIGURE 7</p>
<p>Input image and predicted image in the simulated (A) and real (B) setups.</p>
<p>FIGURE 8</p>
<p>Success rate progress for the sim-to-sim VAE-SAC experiments. The plots show the average performance across seeds on a 100 episode window. The shaded area represents a 95% confidence interval.</p>
<p>By keeping the dynamics predictor frozen the DVAE is constrained to maintain a latent representation compatible with the dynamics predictor itself. We show experimentally that this is sufficient for the policy to keep operating correctly, as compatibility with the policy is also maintained.</p>
<p>Experiments</p>
<p>To demonstrate the effectiveness of the method we evaluate its performance on a robotic table-top object pushing task. In our scenario a 7-DOF Franka Emika Panda robotic arm is tasked with pushing a 6 cm cube to a target position. The robot arm is controlled in cartesian space, and the end-effector moves only horizontally within a 45 cm square workspace located in front of the robot itself. Each episode is initialized with a random cube position and a random end-effector position. The target cube position is kept constant across episodes. The agent controls the robot specifying a displacement in the bidimensional workspace of the end-effector, resulting in a continuous 2D action space. The environment is observed through a camera placed on the opposite side of the table with respect to the robot arm, which produces RGB images with 128 × 128 pixels resolution. In addition to the images the agent also has access to proprioceptive information from the robot, in the form of the 2D position of the end-effector tip. Table 1 summarizes the observation and action spaces. Figure 4 shows the simulated and real scenarios, Figure 5 displays an example of a successful episode.</p>
<p>FIGURE 9</p>
<p>Success rate progression in the sim-to-real experiment discussed in section 4.3. The solid line represents the success rate in the 100-episode window preceding the current episode, the background bands represent the corresponding 95% confidence interval.</p>
<p>Each episode lasts 40 steps. Once the cube reaches the target position, within a 5 cm tolerance, the episode is considered successful, but it is not interrupted until the 40 steps timeout is reached.</p>
<p>We define the reward function as a composition of three terms, one to encourage the end-effector tip to stay close to the cube, one for the cube to stay close to the goal, one for the cube to be moved in any direction. We define them as follows, where r(p c , p t ) is the overall reward, r c (p c , p g ) is the cube-goal term, r t (p t , p c ) the tip-cube term, r d (p c , p ′ c ) the cube displacement term, and r b (p c , p g ) is a further bonus given when the cube is within d m from the target. The constant α is a scaling factor, which we kept fixed at .1. The r c and r t functions are defined as linear ramps, with value 0 at 40 cm from the target and respectively 100 and 50 at the target.
r (p c , p t ) = α * (r c (p c , p t ) + r t (p t , p c ) + r d (p c , p ′ c )) With: r c (p c , p g ) = 100 0.4 * (0.4 − ‖p c − p g ‖) + r b (p c , p g ) r t (p t , p c ) = 50 0.4 * (0.4 − ‖p t − p c ‖) r d (p c , p ′ c ) = ‖p c − p ′ c ‖ * 100 * 20 r b (p c , p g ) = { { { { { { { 200 d * (d − ‖p c − p g ‖) if ‖p c − p g ‖ ≤ d 0 otherwise(2)
We implement this scenario both in the real world and in a Gazebo simulation [Koenig and Howard (2004)]. To evaluate transfer capability we perform both sim-to-sim and sim-to-real experiments, varying the characteristics of the simulation to enlarge or reduce the gap between source and target domains. Table 2 summarizes the characteristics of each scenario.</p>
<p>Sim-to-sim</p>
<p>We evaluate our proposed method on four sim-to-sim transfer scenarios of increasing difficulty. We do so by keeping a fixed source domain and defining four sets of target domains, in which we vary the width of the transfer gap by altering characteristics such as the cube color, the lighting, and the camera pose.</p>
<p>In this section we discuss the different setups and the respective results. The results are also reported in Table 3 and Figure 6. Figure 7 shows the performance achieved training from scratch in simulation.</p>
<p>Minimal gap</p>
<p>In this scenario we only change the color of the manipulated cube. While in the source domain the cube is black, we define eight target scenarios with eight different colors: red, green, blue, yellow, cyan, magenta and 2 grades of gray.</p>
<p>Depending on the selected color the policy has an initial performance that varies between 10 and 95 percent, however in just 110 episodes of finetuning the method consistently reaches a 90% success rate, and then continues maintaining a performance oscillating between 92 and 95 percent.</p>
<p>Small gap</p>
<p>In this setup we introduce variations also in the camera pose and the light direction. We vary the pose by translating it left, right, up or down of 5 cm. We vary the light direction from the vertical axis of the source domain to four possible axes with a 30°i nclination either toward the left, right front or back. We set the cube color to be red.</p>
<p>With these variations the agent performance is initially of about 5%, which is comparable to a random policy. Across our experiment the agent consistently reaches an 80% success rate in about 210 episodes and a 90% success rate in about 950. However, already at episode 500 the agent consistently reaches an 88% success rate.</p>
<p>Medium gap</p>
<p>To further widen the transfer gap in this scenario we increment the magnitude of the camera pose change. We move the camera of 20 cm instead of 5, and we alter its orientation to maintain the manipulation area in the field of view. We fix the cube color as red and vary light direction in the same way as in the previous scenario.</p>
<p>FIGURE 10</p>
<p>Success rate progression for the training from scratch performed in simulation. Two random seeds are being shown. The solid lines represents the success rate in the 100-episode window preceding the current episode, the background bands represent the corresponding 95% confidence interval.</p>
<p>The training performance is comparable to that of the smallgap scenario, reaching 80% success rate in 320 episodes and 90% in 1,250. Also in this case performance just under 90% is reached considerably sooner, reaching 85% at episode 750.</p>
<p>Large gap</p>
<p>In the hardest sim-to-sim scenario we completely change to camera point of view, while still altering cube color and light direction. The camera is moved so that it faces the manipulation area from the side instead of the front, with a 90°point of view change.</p>
<p>In this scenario, which goes beyond what is just a simto-real transfer problem, the agent training requires about as much time as is required to train the agent from scratch. About 1,200 episodes are required to reach an 80% success rate, and a maximum performance of 86% is reached by episode 2000, performing worse than the source training.</p>
<p>We hypotesize, this can be explained by the fact that the agent in this case encounters again an exploration problem, despite not using any kind of Reinforcement Learning method. The agent must again discover where the goal is located, and can only understand this from the training signal of the reward predictor present in the dynamics model.</p>
<p>Sim-to-sim with VAE-SAC</p>
<p>To explore the importance of the dynamics predictor presence for domain transfer we evaluated the performance of a VAE-SAC agent on the sim-to-sim scenarios. The VAE-SAC agent is a modified version of our architecture in which the dynamics predictor has been removed. Figure 8 shows the achieved transfer performance. As expected the domain transfer fails, as there is no constraint to keep the latent representation compatible with the control policy. Even in the minimal-gap scenarios, where the zero-shot performance is not zero, the success rate rapidly descends to performance comparable to that of a randmo policy. In the small-gap scenario we can see the performance initially rising, but then also decaying.</p>
<p>Sim-to-real</p>
<p>In addition to the sim-to-sim evaluation we also assess the performance of the method on a sim-to-real transfer scenario.</p>
<p>We only perform what we call a minimal-gap sim-to-real transfer, in which we minimize the differences by not intentionally introducing variations and trying to replicate the simulated scenario for what is possible. However, the transfer still presents small differences in the camera pose, and lighting and the texture of the environment is considerably different. Figure 9 shows the training performance in the real setup. Figure 10 shows the camera view and decoder reconstruction in the simulated and real scenarios.</p>
<p>The initial success rate achieved by the policy is about 10%, but the finetuning of the DVAE quickly brings it 80% in just 550 episodes, corresponding to 5 h of experience, a performance of 90% is achieved in 990 episodes, 10 h of experience data.</p>
<p>When compared to the training from scratch in simulation, the sim-to-real finetuning is considerably faster in the first stages of learning, achieving 80% success rate in about half the time, however reaching 90% requires almost as much time as the source training. It must be noted however that a training from scratch in the real requires considerably more time than in simulation, due to the higher complexity of the sensory inputs. Furthermore, the sim-to-real finetuning could be completed by unfreezing the control policy once the DVAE has reached good enough performance, allowing the agent to adapt to any further unaccounted difference in the target domain.</p>
<p>Conclusion</p>
<p>In this work we presented a method for efficiently learning visual manipulation policies and transferring trained agents from simulation to reality. The agent architecture uses a decoupled representation learning approach based on a predictive Variational Autoencoder, named DVAE, that can fully represent a system modeled as a Markov Decision Process. This formulation allows to learn the visual task with high sample efficiency, requiring far less data than traditional end-to-end Reinforcement Learning methods. This allows us to train a manipulation policy in simulation in less than 12 h. Furthermore, the decoupled nature of the method and the presence of the dynamics predictor give us the ability to transfer the agent effectively between simulation and reality. Differently from other sim-to-real adaptation works the method proposed in this work is completely unsupervised, is trained online and does not require any target domain knowledge while performing the source domain training. Consequently, it does not require any data collection outside of the experience collected by the RL agent, and as such, it can be applied to difficult exploration problems and tasks for which manual data collection is impractical.</p>
<p>We demonstrated the transfer capabilities of the method via sim-to-sim and sim-to-real experiments on an object-pushing robotic setup. The results show how the method can overcome considerable gaps between the characteristics of source and target domain. When the source-target domain gap is small the method can adapt extremely quickly, reducing training time by three to four times. If the reality gap is wider, the method naturally requires more time and data to adapt, but still brings considerable sample-efficiency improvements.</p>
<p>The sim-to-real transfer experiments show how the proposed method offers considerable efficiency improvements especially when looking at the first stages of training. The agent reaches an 80% success rate in just about 500 episodes, half of what is required by a from-scratch training in simulation. It must also be noted how training a policy from scratch in the real would not be as simple as in simulation, the complexity of the real environment would affect the agent performance also in this case. The improvement in real-world training efficiency may thus be greater than what shown.</p>
<p>When looking at the asymptotic performance the method lags when compared with the training from scratch. However this remaining gap in performance can be explained by the more complex nature of the real-world observations, and can potentially be bridged by performing a further finetuning, unfreezing the control policy network after finetuning.</p>
<p>In conclusion, our methodology shows how it is possible to train effectively a manipulation policy such as a robotic object pushing task example with very little real-world data. With just 6 h of real-world experience, the agent learns to solve our object pushing task, directly from visual input.</p>
<p>Data availability statement</p>
<p>The datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found below: Code will be made available at https://gitlab.com/crzz/dvae_s2r_pushing.</p>
<p>Author contributions</p>
<p>CR: Main contributor, designed methodology and experimental setup, implemented the method and conducted experiments FC: Supervision and support in the conception of the study, organizational support DC: Supervision and support in the conception of the study, organizational support, funding support.</p>
<p>Conflict of interest</p>
<p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
<p>Publisher's note</p>
<p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
<p>al. (2018);James et al. (2019)]. Other methods attempt to identify corresponding samples from source and target domain and then force the representations of these corresponding samples to be similar.Gupta et al. (2017) does so by assuming samples from corresponding timesteps in RL episodes should be similar,Tzeng et al. (2016) first identifies weakly paired samples and improves on this with an adversarial approach.</p>
<p>FIGURE 3
3Training procedure for DVAE-SAC. DVAE weights are updated with ADAMKingma and Ba (2015) and the policy is trained with SAC_UPDATE as defined inHaarnoja et al. (2018b).</p>
<p>FIGURE 4
4Simulated (A) and real (B) object pushing setups. The camera used to collect the input images is visible in the top left in both pictures.</p>
<p>FIGURE 5
5Example of one successful episode in the simulated setup. The image shows the observed images for steps0, 4, 8, 12, 16, 20, 24.    </p>
<p>FIGURE 6
6FIGURE 6 Success rate progress for the sim-to-sim experiments. Figure (A) shows the minimal-gap scenario: It is possible to see the performance difference depending on the cube color. Figure (B) displays the results for all the scenarios in an aggregated form, the plots show the average performance across seeds on a 100 episode window. For the minimal, small, medium and large scenarios we used respectively 8, 12, 16, and 4 seeds. The shaded area represents a 95% confidence interval.</p>
<p>TABLE 1
1Object pushing environment observation and action spaces.The action space is normalized to [−1, 1], but corresponds to a displacement of maximum 2.5 cm in x and y.Observation space 
[0, 1] 128 × 128 × [−1, 1] 2 </p>
<p>Action space 
[−1, 1] 2 </p>
<p>KL-divergence and the MSE reconstruction error: </p>
<p>TABLE 2
2Variations from the source domain across the different experimental scenarios.Scenario 
Cube color 
Light direction 
Camera position 
Camera orientation </p>
<p>Original (Sim) 
Black 
Vertical 
∖ 
∖ </p>
<p>S2S-Minimal Gap 
10 colors 
Vertical 
Unchanged 
Unchanged </p>
<p>S2S-Small Gap 
Red 
30°: Left, Right, Back, Front 
∼5 cm offset 
Unchanged </p>
<p>S2S-Medium Gap 
Red 
30°: Left, Right, Back, Front 
∼20 cm offset 
Toward Center </p>
<p>S2S-Large Gap 
Red 
30°Left 
∼70 cm offset 
90°Yaw </p>
<p>S2R-Minimal Gap 
Black 
Multiple sources, diffused 
∼5 cm offset 
Minimal </p>
<p>TABLE 3
3DVAE-SAC results on the four sim-to-sim (S2S) and the sim-to-real (S2R) scenarios. Columns indicate respectively: the best achieved success rate, the initial success rate in the target domain (i.e. zero-shot transfer performance), the number of episodes required to reach 80% success rate, the number of episodes required to reach 90% success rate.Scenario 
Success (%) 
Init. Succ (%) 
T.T. 80% 
T.T. 90% </p>
<p>Sim. From Scratch 
98 
5 
1,020 Eps 
1,180 Eps </p>
<p>S2S-Minimal Gap 
95 
50 
90 Eps 
110 Eps </p>
<p>S2S-Small Gap 
92 
5 
210 Eps 
950 Eps </p>
<p>S2S-Medium Gap 
92 
5 
320 Eps 
1,250 Eps </p>
<p>S2S-Large Gap 
85 
5 
1,200 Eps 
∖ </p>
<p>S2R-Minimal Gap 
92 
5 
550 Eps 
990 Eps </p>
<p>Frontiers in Robotics and AI 
07 
frontiersin.org </p>
<p>frontiersin.org
frontiersin.org Rizzardo et al.</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, 2018 IEEE international conference on robotics and automation (ICRA). Brisbane, AustraliaIEEEBousmalis, K., Irpan, A., Wohlhart, P., Bai, Y., Kelcey, M., Kalakrishnan, M., et al. (2018). "Using simulation and domain adaptation to improve efficiency of deep robotic grasping, " in 2018 IEEE international conference on robotics and automation (ICRA) (Brisbane, Australia: IEEE), 4243-4250.</p>
<p>Unsupervised pixel-level domain adaptation with generative adversarial networks. K Bousmalis, N Silberman, D Dohan, D Erhan, D Krishnan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHonolulu, HI, United StatesIEEEBousmalis, K., Silberman, N., Dohan, D., Erhan, D., and Krishnan, D. (2017). "Unsupervised pixel-level domain adaptation with generative adversarial networks, " in Proceedings of the IEEE conference on computer vision and pattern recognition (Honolulu, HI, United States: IEEE), 3722-3731.</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. K Chua, R Calandra, R Mcallister, S Levine, Advances in Neural Information Processing Systems. NeurIPSChua, K., Calandra, R., McAllister, R., and Levine, S. (2018). "Deep reinforcement learning in a handful of trials using probabilistic dynamics models, " in Advances in Neural Information Processing Systems, NeurIPS 2018. Editors S.</p>
<p>. H Bengio, H Wallach, K Larochelle, N Grauman, R Cesa-Bianchi, Garnett, Curran Associates, Inc31Montréal, CanadaBengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Montréal, Canada: Curran Associates, Inc.) 31.</p>
<p>Unsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, PMLRInternational conference on machine learning. Lille, FranceGanin, Y., and Lempitsky, V. (2015). "Unsupervised domain adaptation by backpropagation, " in International conference on machine learning (Lille, France: PMLR). 1180-1189.</p>
<p>From variational to deterministic autoencoders. P Ghosh, M S M Sajjadi, A Vergari, M J Black, B Schölkopf, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020: OpenReview.net)Ghosh, P., Sajjadi, M. S. M., Vergari, A., Black, M. J., and Schölkopf, B. (2020). "From variational to deterministic autoencoders, " in 8th International Conference on Learning Representations, ICLR 2020 (Addis Ababa, Ethiopia: OpenReview.net).</p>
<p>Learning invariant feature spaces to transfer skills with reinforcement learning. A Gupta, C Devin, Y Liu, P Abbeel, S Levine, International Conference on Learning Representations. Toulon, FranceICLRGupta, A., Devin, C., Liu, Y., Abbeel, P., and Levine, S. (2017). "Learning invariant feature spaces to transfer skills with reinforcement learning, " in International Conference on Learning Representations, ICLR 2017 (Toulon, France: ICLR).</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, PMLRInternational conference on machine learning, ICML 2018. Stockholm, SwedenHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018a). "Soft actor-critic: Off- policy maximum entropy deep reinforcement learning with a stochastic actor, " in International conference on machine learning, ICML 2018 (Stockholm, Sweden: PMLR), 1861-1870.</p>
<p>. T Haarnoja, A Zhou, K Hartikainen, G Tucker, S Ha, J Tan, Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., et al. (2018b).</p>
<p>Soft actor-critic algorithms and applications. arXiv:1812.05905arXiv preprintSoft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905.</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, PMLRInternational conference on machine learning, ICML 2019. Long Beach, CA, United StatesHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., et al. (2019). "Learning latent dynamics for planning from pixels, " in International conference on machine learning, ICML 2019 (Long Beach, CA, United States: PMLR), 2555-2565.</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T Lillicrap, J Ba, M Norouzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020: OpenReview.net)Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2020). "Dream to control: Learning behaviors by latent imagination, " in 8th International Conference on Learning Representations, ICLR 2020 (Addis Ababa, Ethiopia: OpenReview.net).</p>
<p>Mastering atari with discrete world models. D Hafner, T Lillicrap, M Norouzi, J Ba, 9th International Conference on Learning Representations, ICLR 2021 (Virtual Event, Austria: OpenReview.net). Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J. (2021). "Mastering atari with discrete world models, " in 9th International Conference on Learning Representations, ICLR 2021 (Virtual Event, Austria: OpenReview.net).</p>
<p>Neuralsim: Augmenting differentiable simulators with neural networks. E Heiden, D Millard, E Coumans, Y Sheng, G S Sukhatme, 2021 IEEE International Conference on Robotics and Automation, ICRA 2021. Xi'an, ChinaIEEEHeiden, E., Millard, D., Coumans, E., Sheng, Y., and Sukhatme, G. S. (2021). "Neuralsim: Augmenting differentiable simulators with neural networks, " in 2021 IEEE International Conference on Robotics and Automation, ICRA 2021 (Xi'an, China: IEEE), 9474-9481.</p>
<p>Cycada: Cycle-consistent adversarial domain adaptation. J Hoffman, E Tzeng, T Park, J.-Y Zhu, P Isola, K Saenko, PMLRInternational conference on machine learning, ICML 2018. Stockholm, SwedenHoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., et al. (2018). "Cycada: Cycle-consistent adversarial domain adaptation, " in International conference on machine learning, ICML 2018 (Stockholm, Sweden: PMLR), 1989-1998.</p>
<p>Searching for mobilenetv3. A Howard, M Sandler, G Chu, L.-C Chen, B Chen, M Tan, Proceedings of the IEEE/CVF international conference on computer vision, ICCV 2019. the IEEE/CVF international conference on computer vision, ICCV 2019Seoul, South KoreaHoward, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., et al. (2019). "Searching for mobilenetv3, " in Proceedings of the IEEE/CVF international conference on computer vision, ICCV 2019 (Seoul, South Korea), 1314-1324.</p>
<p>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. S James, P Wohlhart, M Kalakrishnan, D Kalashnikov, A Irpan, J Ibarz, Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, CVPR 2019. the IEEE CVF Conference on Computer Vision and Pattern Recognition, CVPR 2019James, S., Wohlhart, P., Kalakrishnan, M., Kalashnikov, D., Irpan, A., Ibarz, J., et al. (2019). "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks, " in Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, CVPR 2019, 12627-12637.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, 3rd International Conference on Learning Representations, ICLR 2015. Editors Y. Bengio, and Y. LeCun. San Diego, CA, United StatesKingma, D. P., and Ba, J. (2015). "Adam: A method for stochastic optimization, " in 3rd International Conference on Learning Representations, ICLR 2015. Editors Y. Bengio, and Y. LeCun (San Diego, CA, United States).</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, 2nd International Conference on Learning Representations. Y. LeCunBanff, AB, CanadaKingma, D. P., and Welling, M. (2014). "Auto-encoding variational bayes, " in 2nd International Conference on Learning Representations, ICLR 2014. Editors Y. Bengio, and Y. LeCun (Banff, AB, Canada).</p>
<p>Design and use paradigms for gazebo, an open-source multi-robot simulator. N Koenig, A Howard, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Sendai, JapanIEEE3Koenig, N., and Howard, A. (2004). "Design and use paradigms for gazebo, an open-source multi-robot simulator, " in 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No. 04CH37566) (Sendai, Japan: IEEE) 3, 2149-2154.</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. I Kostrikov, D Yarats, Fergus , R , arXiv:2004.13649arXiv preprintKostrikov, I., Yarats, D., and Fergus, R. (2020). Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649.</p>
<p>. M Laskin, K Lee, A Stooke, L Pinto, P Abbeel, Srinivas , A , Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. (2020a).</p>
<p>Reinforcement learning with augmented data. Adv. neural Inf. Process. Syst. 33Reinforcement learning with augmented data. Adv. neural Inf. Process. Syst. 33, 19884-19895.</p>
<p>CURL: contrastive unsupervised representations for reinforcement learning. M Laskin, A Srinivas, Abbeel , P , Proceedings of the 37th International Conference on Machine Learning, ICML 2020 (Virtual Event: PMLR). Proceed. the 37th International Conference on Machine Learning, ICML 2020 (Virtual Event: PMLR). Proceed119Laskin, M., Srinivas, A., and Abbeel, P. (2020b). "CURL: contrastive unsupervised representations for reinforcement learning, " in Proceedings of the 37th International Conference on Machine Learning, ICML 2020 (Virtual Event: PMLR). Proceed. Machine Learning Res. 119, 5639-5650.</p>
<p>Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. A X Lee, A Nagabandi, P Abbeel, S ; M Levine, R Ranzato, M Hadsell, H Balcan, Lin, Advances in Neural Information Processing Systems, NeurIPS 2020. Curran Associates, Inc33Lee, A. X., Nagabandi, A., Abbeel, P., and Levine, S. (2020). "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model, " in Advances in Neural Information Processing Systems, NeurIPS 2020. Editors H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Virtual-only: Curran Associates, Inc.) 33, 741-752.</p>
<p>Learning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, Jordan , M , PMLRInternational conference on machine learning, ICML 2015. Lille, FranceLong, M., Cao, Y., Wang, J., and Jordan, M. (2015). "Learning transferable features with deep adaptation networks, " in International conference on machine learning, ICML 2015 (Lille, France: PMLR), 97-105.</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, International conference on machine learning, ICML 2016 (PMLR). Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., et al. (2016). "Asynchronous methods for deep reinforcement learning, " in International conference on machine learning, ICML 2016 (PMLR), 1928-1937.</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, nature. 518Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. nature 518, 529-533.</p>
<p>. F Muratore, T Gruner, F Wiese, B Belousov, M Gienger, J Peters, Muratore, F., Gruner, T., Wiese, F., Belousov, B., Gienger, M., and Peters, J. (2021).</p>
<p>Neural posterior domain randomization. PMLR)Conference on Robot Learning, CoRL 2021. London, United Kingdom"Neural posterior domain randomization, " in Conference on Robot Learning, CoRL 2021 (London, United Kingdom: PMLR).</p>
<p>Deep dynamics models for learning dexterous manipulation. A Nagabandi, K Konolige, S Levine, V Kumar, PMLR3rd Annual Conference on Robot Learning. Osaka, Japan2019CoRLNagabandi, A., Konolige, K., Levine, S., and Kumar, V. (2019). "Deep dynamics models for learning dexterous manipulation, " in 3rd Annual Conference on Robot Learning, CoRL 2019, Proceedings (Osaka, Japan: PMLR), 100, 1101-1112.</p>
<p>A Nair, P Srinivasan, S Blackwell, C Alcicek, R Fearon, A De Maria, arXiv:1507.04296Massively parallel methods for deep reinforcement learning. arXiv preprintICML 2015 Deep Learning WorkshopNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., et al. (2015). Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296. ICML 2015 Deep Learning Workshop.</p>
<p>Nvidia isaac sim. Nvidia, Nvidia, (2020). Nvidia isaac sim. Available at: https://developer.nvidia.com/ isaac-sim.</p>
<p>Solving rubik's cube with a robot hand. Akkaya Openai, I Andrychowicz, M Chociej, M Litwin, M Mcgrew, B , ArXiv abs/1910.07113OpenAI, Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., et al. (2019). Solving rubik's cube with a robot hand. ArXiv abs/1910.07113.</p>
<p>Simto-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, Abbeel , P , 2018 IEEE International Conference on Robotics and Automation (ICRA). Peng, X. B., Andrychowicz, M., Zaremba, W., and Abbeel, P. (2017). "Sim- to-real transfer of robotic control with dynamics randomization, " in 2018 IEEE International Conference on Robotics and Automation (ICRA), 1-8.</p>
<p>Asymmetric actor critic for image-based robot learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, Abbeel , P , 10.15607/RSS.2018.XIV.008Proceedings of Robotics: Science and Systems, R:SS 2018. Robotics: Science and Systems, R:SS 2018Pittsburgh, PennsylvaniaPinto, L., Andrychowicz, M., Welinder, P., Zaremba, W., and Abbeel, P. (2018). "Asymmetric actor critic for image-based robot learning, " in Proceedings of Robotics: Science and Systems, R:SS 2018 (Pittsburgh, Pennsylvania). 10.15607/RSS.2018.XIV.008</p>
<p>Online bayessim for combined simulator parameter inference and policy improvement. R Possas, L Barcelos, R Oliveira, D Fox, F Ramos, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE2020Possas, R., Barcelos, L., Oliveira, R., Fox, D., and Ramos, F. (2020). Online bayessim for combined simulator parameter inference and policy improvement, " in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2020 (IEEE). IEEE, 5445-5452.</p>
<p>Stable-baselines3: Reliable reinforcement learning implementations. A Raffin, A Hill, A Gleave, A Kanervisto, M Ernestus, N Dormann, J. Mach. Learn. Res. 22Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N. (2021). Stable-baselines3: Reliable reinforcement learning implementations. J. Mach. Learn. Res. 22, 1-8.</p>
<p>Bayessim: Adaptive domain randomization via probabilistic inference for robotics simulators. F Ramos, R Possas, D Fox, &gt;Robotics: Science and Systems XV, R:SS 2019. Bicchi, H. Kress-Gazit, and SRamos, F., Possas, R., and Fox, D. (2019). "Bayessim: Adaptive domain randomization via probabilistic inference for robotics simulators, " in &gt;Robotics: Science and Systems XV, R:SS 2019. Editors A. Bicchi, H. Kress-Gazit, and S.</p>
<p>. Hutchinson, 10.15607/RSS.2019.XV.029Freiburg im Breisgau, GermanyHutchinson (Freiburg im Breisgau, Germany). doi:10.15607/RSS.2019.XV.029</p>
<p>Stochastic backpropagation and approximate inference in deep generative models. D J Rezende, S Mohamed, D Wierstra, Proceedings of the 31st International Conference on Machine Learning. the 31st International Conference on Machine LearningRezende, D. J., Mohamed, S., and Wierstra, D. (2014). "Stochastic backpropagation and approximate inference in deep generative models, " in Proceedings of the 31st International Conference on Machine Learning. Editors E.</p>
<p>P Xing, T Jebara, Proceedings of Machine Learning Research. Machine Learning ResearchBejing, China32P. Xing, and T. Jebara (Bejing, China: Proceedings of Machine Learning Research) 32, 1278-1286.</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. N Rudin, D Hoeller, P Reist, M Hutter, Proceedings of Machine Learning Research. Machine Learning ResearchLondon, United Kingdom915th Annual Conference on Robot LearningRudin, N., Hoeller, D., Reist, P., and Hutter, M. (2021). "Learning to walk in minutes using massively parallel deep reinforcement learning, " in 5th Annual Conference on Robot Learning, CoRL 2021 (London, United Kingdom: Proceedings of Machine Learning Research), 91.</p>
<p>Nonprehensile dynamic manipulation: A survey. F Ruggiero, V Lippiello, B Siciliano, 10.1109/lra.2018.2801939IEEE Robotics Automation Lett. 3Ruggiero, F., Lippiello, V., and Siciliano, B. (2018). Nonprehensile dynamic manipulation: A survey. IEEE Robotics Automation Lett. 3, 1711-1718. doi:10.1109/lra.2018.2801939</p>
<p>. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).</p>
<p>arXiv:1707.06347Frontiers in Robotics and AI 13 frontiersin.org. arXiv preprintProximal policy optimization algorithmsProximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Frontiers in Robotics and AI 13 frontiersin.org</p>
<p>A Srinivas, M Laskin, Abbeel , P , Curl: Contrastive unsupervised representations for reinforcement learning. arXiv preprint arXiv. Srinivas, A., Laskin, M., and Abbeel, P. (2020). Curl: Contrastive unsupervised representations for reinforcement learning. arXiv preprint arXiv:2004.</p>
<p>Deep coral: Correlation alignment for deep domain adaptation. B Sun, K Saenko, Computer Vision-ECCV. Sun, B., and Saenko, K. (2016). "Deep coral: Correlation alignment for deep domain adaptation, " in Computer Vision-ECCV 2016 Workshops. Editors G.</p>
<p>. H Hua, Jégou, Springer International PublishingAmsterdam, NetherlandsHua, and H. Jégou (Amsterdam, Netherlands: Springer International Publishing), 443-450.</p>
<p>. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, Abbeel , P , Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. (2017).</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). "Domain randomization for transferring deep neural networks from simulation to the real world, " in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 23-30.</p>
<p>Adapting deep visuomotor representations with weak pairwise constraints. E Tzeng, C Devin, J Hoffman, C Finn, P Abbeel, S Levine, 10.1007/978-3-030-43089-4_44Springer International PublishingSan Francisco, CA, United StatesTzeng, E., Devin, C., Hoffman, J., Finn, C., Abbeel, P., Levine, S., et al. (2016). Adapting deep visuomotor representations with weak pairwise constraints (San Francisco, CA, United States: Springer International Publishing), 688-703. doi:10.1007/978-3-030-43089-4_44</p>
<p>E Tzeng, C Devin, J Hoffman, C Finn, X Peng, S Levine, arXiv:1511.07111Towards adapting deep visuomotor representations from simulated to real environments. CoRR arXiv preprint. Tzeng, E., Devin, C., Hoffman, J., Finn, C., Peng, X., Levine, S., et al. (2015). Towards adapting deep visuomotor representations from simulated to real environments. CoRR arXiv preprint arXiv:1511.07111.</p>
<p>Adversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, Darrell , T , Proceedings of the IEEE conference on computer vision and pattern Recognition. the IEEE conference on computer vision and pattern RecognitionHonolulu, HI, United StatesTzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017). "Adversarial discriminative domain adaptation, " in Proceedings of the IEEE conference on computer vision and pattern Recognition, CVPR 2017 (Honolulu, HI, United States).</p>
<p>Deep domain confusion: Maximizing for domain invariance. E Tzeng, J Hoffman, N Zhang, K Saenko, Darrell , T , arXiv:1412.3474CoRR arXiv preprintTzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell, T. (2014). Deep domain confusion: Maximizing for domain invariance. CoRR arXiv preprint arXiv:1412.3474.</p>
<p>Unity. Unity, Unity (2020). Unity. Available at: https://unity.com/solutions/automotive- transportation-manufacturing/robotics.</p>
<p>Mastering visual continuous control: Improved data-augmented reinforcement learning. D Yarats, R Fergus, A Lazaric, L Pinto, The Tenth International Conference on Learning Representations, ICLR 2022 (Virtual Event: OpenReview.net). Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. (2022). "Mastering visual continuous control: Improved data-augmented reinforcement learning, " in The Tenth International Conference on Learning Representations, ICLR 2022 (Virtual Event: OpenReview.net).</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. D Yarats, I Kostrikov, Fergus , R , 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaOpenReview.netYarats, D., Kostrikov, I., and Fergus, R. (2021a). "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels, " in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 (OpenReview.net).</p>
<p>Improving sample efficiency in model-free reinforcement learning from images. D Yarats, A Zhang, I Kostrikov, B Amos, J Pineau, Fergus , R , Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021. the Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021AAAI PressYarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R. (2021b). "Improving sample efficiency in model-free reinforcement learning from images, " in Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021 (Held Virtually: AAAI Press), 10674- 10681.</p>            </div>
        </div>

    </div>
</body>
</html>
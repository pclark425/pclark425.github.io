<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1102 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1102</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1102</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-195767141</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1907.00824v1.pdf" target="_blank">Designing Deep Reinforcement Learning for Human Parameter Exploration</a></p>
                <p><strong>Paper Abstract:</strong> Software tools for generating digital sound often present users with high-dimensional, parametric interfaces, that may not facilitate exploration of diverse sound designs. In this article, we propose to investigate artificial agents using deep reinforcement learning to explore parameter spaces in partnership with users for sound design. We describe a series of user-centred studies to probe the creative benefits of these agents and adapting their design to exploration. Preliminary studies observing users’ exploration strategies with parametric interfaces and testing different agent exploration behaviours led to the design of a fully-functioning prototype, called Co-Explorer, that we evaluated in a workshop with professional sound designers. We found that the Co-Explorer enables a novel creative workflow centred on human–machine partnership, which has been positively received by practitioners. We also highlight varied user exploration behaviours throughout partnering with our system. Finally, we frame design guidelines for enabling such co-exploration workflow in creative digital applications.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1102.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1102.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sarsa-RL prototype</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sarsa-based Reinforcement Learning Prototype (pilot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Initial interactive RL agent implemented for the pilot study: a tabular Sarsa agent acting on discretized VST parameter spaces and learning from binary human feedback, using an ε-greedy policy for exploration/exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sarsa (tabular)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Tabular Sarsa algorithm defined on a discretized parameter-state space (parameters discretized to three levels per parameter in the pilot), mapping binary human feedback directly to the environment reward R; action set: increment/decrement a single parameter by one discretization step (with boundary handling). Exploration policy implemented via ε-greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>ε-greedy exploration (baseline) with discrete state-action Sarsa learning from human reward</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent updates state-action values using Sarsa from the human-provided reward signal; it selects actions according to ε-greedy (with ε values tested at 0, 0.5, and 1 in the pilot) so that it sometimes exploits accumulated feedback and sometimes takes random exploratory actions. Human binary feedback is mapped directly to R and used as the learning target.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>VST parameter space (pilot: 12-parameter VST)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>High-dimensional parametric control space represented as discrete states (pilot discretized each of 12 parameters to 3 levels); deterministic parameter transition when agent acts; the human feedback channel is non-stationary (user-driven), making the effective reward distribution non-stationary; perceptual mapping (parameters -> audio) is complex and effectively unknown to the agent a priori.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>12 parameters, each discretized to 3 levels (s_i ∈ {0, 0.5, 1}), yielding 3^12 (~531,441) possible discrete states; action set: change one parameter by ±0.5 (except at boundaries); sequential interactive loop with continuous audio output; no formal episode length specified (continuous interactive session).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No numeric RL learning curves reported; qualitative observation: subjectively slow/insufficient precision for users due to coarse discretization; participants reported that Sarsa required discretization which reduced perceived precision.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Controlled by ε in ε-greedy: ε = 0 (always explore/random in their definition), ε = 1 (always exploit) and ε = 0.5 tested; participants could not reliably distinguish the three ε conditions in pilot, indicating limited perceptual/behavioral impact in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared experimentally (pilot) across three ε-greedy configurations (ε=0, ε=0.5, ε=1) and qualitatively against manual parametric exploration (no agent).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Used in pilot to probe user reactions: participants appreciated focusing on sound but found the Sarsa-based agent lacking precision and occasionally unresponsive to feedback; discretization (3 levels) reduced precision and hindered perceived control; participants suggested richer feedback signals, ability to undo, and continuous/autonomous exploration modes. The pilot showed that vanilla tabular Sarsa + ε-greedy was insufficient for the intended creative exploratory workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires coarse discretization of continuous parameters (here 3 levels) which harmed precision and user satisfaction; participants reported the agent 'not reacting properly' to feedback and leading exploration at users' expense; inability to undo actions and restricted interaction design increased frustration; no quantitative RL performance metrics or convergence data in interactive setting were reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Designing Deep Reinforcement Learning for Human Parameter Exploration', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1102.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1102.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Co-Explorer (Deep TAMER + intrinsic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Co-Explorer: Deep TAMER agent with intrinsic motivation (density-based) exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Final prototype agent implemented for interactive human-guided exploration: a Deep TAMER architecture (deep neural network predicting human feedback) combined with an intrinsic-motivation exploration strategy based on a tile-coded density model to direct exploration toward novel/unvisited parameter states, integrated with multiple human interaction modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Deep TAMER + intrinsic (density) exploration (Co-Explorer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep neural network (2 hidden layers, 100 units each per Appendix B) trained to predict human reinforcement (Deep TAMER) from state-action inputs; uses a replay buffer to repeatedly sample human feedback examples; exploration is driven by an intrinsic motivation signal computed from a density model of visited states (tile coding) that biases action selection toward low-density (novel) states; the system augments human feedback with a density-based novelty bonus and supports interaction modalities (guiding feedback, zone feedback, state commands, direct manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Human-in-the-loop value learning (Deep TAMER) combined with curiosity/intrinsic motivation via count/density-based novelty (tile coding), with ε decay to modulate exploration probability.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent learns a model R_hat(s,a) predicting expected human feedback from observed state-action-feedback tuples stored in replay memory (Deep TAMER). It adapts its action-selection by (1) taking greedy actions according to learned predicted human reward, (2) adding an intrinsic novelty bonus based on a density estimate (tile-coded counts) to encourage actions leading to low-density states, and (3) using an ε parameter with exponential decay to stochastically choose exploratory actions; user inputs (guiding feedback distributed to recent p state-action pairs with gamma weighting, and zone feedback applied to all state-action pairs leading to a labeled state) are incorporated into the replay buffer and into reward updates. State commands (change-zone) sample the density model to jump to the lowest-density state and autonomous exploration steers actions toward nearest low-density states.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>VST parameter space (evaluation workshop: 10-parameter VST)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>High-dimensional continuous (normalized) parameter space (n=10 parameters, s_i ∈ [0,1]); deterministic parameter updates under agent actions; perceptual, complex mapping from parameters to audio output unknown to the agent prior to interaction; human feedback channel is sparse, noisy and non-stationary (creative subjective feedback); large effective state space requiring function approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>10 continuous parameters discretized conceptually at 100 levels for agent precision (s_i discretization a_i = 0.01), implying 100^10 theoretical discrete combinations (≈1e20) — treated via function approximation (deep net) rather than tabular methods; agent speed in continuous mode: one action per 0.1 s; sessions lasted minutes (users spent on average 13 minutes in autonomous exploration in a 25–30 minute task).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No standard RL learning curves reported; design choices (Deep TAMER + replay memory) aimed to improve sample-efficiency for rapid human teaching; qualitative evidence that users could teach meaningful behaviors within single workshop sessions (tens to hundreds of feedback events). Logged interaction quantities: guiding feedback counts per participant ranged from 54 to 1,489 events; zone feedback counts ranged 10–233; average guiding feedback positivity ~55% (σ=17%) during evaluation, suggesting non-trivial training signal within session lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced by a novelty-biased exploratory policy: actions are selected to greedily maximize predicted human feedback R_hat but with added intrinsic novelty bonus from the density model; stochastic exploration is modulated by an exponentially decaying ε so that exploration probability decreases slowly over time. Additionally, user commands can force exploration jumps (change-zone) or start/stop autonomous exploration, enabling manual control over the trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared qualitatively against: (a) the pilot Sarsa ε-greedy prototype, (b) manual parametric exploration, and (c) user-specified baseline behaviors (participants' own strategies). In pilot the Sarsa and ε-greedy variants were tested; in the final workshop the Co-Explorer was evaluated in use (no direct numeric algorithmic baseline reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Deep TAMER combined with density-based intrinsic motivation made the agent usable in real creative exploration: participants spent >50% of the discovery task time in autonomous mode (avg 13 min), used change-zone frequently (14–90 times per participant), and reported the intrinsic exploration behavior as preferable to simple random exploration. Interaction modalities (guiding feedback, zone feedback, state commands, direct manipulation) were effective and supported diverse co-exploration partnerships (user-as-leader vs agent-as-leader). Users rapidly appropriated the agent and reported that it promoted serendipity and discovery; however, quantitative RL performance metrics (cumulative reward, success rates) were not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No quantitative evaluation of RL convergence or numeric performance measures provided; some participants complained about agent reaction speed and wished for slower autonomous exploration to provide more precise guidance; zone-negative labels were at times counter-intuitive; Deep TAMER still assumes mostly-correct/stationary feedback in its formalism whereas actual creative users generate non-stationary, multi-objective, and sometimes socially-motivated signals — authors note this as a limitation and a direction for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Designing Deep Reinforcement Learning for Human Parameter Exploration', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces <em>(Rating: 2)</em></li>
                <li>Interactively shaping agents via human reinforcement: The TAMER framework <em>(Rating: 2)</em></li>
                <li>Unifying count-based exploration and intrinsic motivation <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning from human preferences <em>(Rating: 2)</em></li>
                <li>Human-level control through deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1102",
    "paper_id": "paper-195767141",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Sarsa-RL prototype",
            "name_full": "Sarsa-based Reinforcement Learning Prototype (pilot)",
            "brief_description": "Initial interactive RL agent implemented for the pilot study: a tabular Sarsa agent acting on discretized VST parameter spaces and learning from binary human feedback, using an ε-greedy policy for exploration/exploitation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Sarsa (tabular)",
            "agent_description": "Tabular Sarsa algorithm defined on a discretized parameter-state space (parameters discretized to three levels per parameter in the pilot), mapping binary human feedback directly to the environment reward R; action set: increment/decrement a single parameter by one discretization step (with boundary handling). Exploration policy implemented via ε-greedy.",
            "adaptive_design_method": "ε-greedy exploration (baseline) with discrete state-action Sarsa learning from human reward",
            "adaptation_strategy_description": "Agent updates state-action values using Sarsa from the human-provided reward signal; it selects actions according to ε-greedy (with ε values tested at 0, 0.5, and 1 in the pilot) so that it sometimes exploits accumulated feedback and sometimes takes random exploratory actions. Human binary feedback is mapped directly to R and used as the learning target.",
            "environment_name": "VST parameter space (pilot: 12-parameter VST)",
            "environment_characteristics": "High-dimensional parametric control space represented as discrete states (pilot discretized each of 12 parameters to 3 levels); deterministic parameter transition when agent acts; the human feedback channel is non-stationary (user-driven), making the effective reward distribution non-stationary; perceptual mapping (parameters -&gt; audio) is complex and effectively unknown to the agent a priori.",
            "environment_complexity": "12 parameters, each discretized to 3 levels (s_i ∈ {0, 0.5, 1}), yielding 3^12 (~531,441) possible discrete states; action set: change one parameter by ±0.5 (except at boundaries); sequential interactive loop with continuous audio output; no formal episode length specified (continuous interactive session).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": "No numeric RL learning curves reported; qualitative observation: subjectively slow/insufficient precision for users due to coarse discretization; participants reported that Sarsa required discretization which reduced perceived precision.",
            "exploration_exploitation_tradeoff": "Controlled by ε in ε-greedy: ε = 0 (always explore/random in their definition), ε = 1 (always exploit) and ε = 0.5 tested; participants could not reliably distinguish the three ε conditions in pilot, indicating limited perceptual/behavioral impact in this setup.",
            "comparison_methods": "Compared experimentally (pilot) across three ε-greedy configurations (ε=0, ε=0.5, ε=1) and qualitatively against manual parametric exploration (no agent).",
            "key_results": "Used in pilot to probe user reactions: participants appreciated focusing on sound but found the Sarsa-based agent lacking precision and occasionally unresponsive to feedback; discretization (3 levels) reduced precision and hindered perceived control; participants suggested richer feedback signals, ability to undo, and continuous/autonomous exploration modes. The pilot showed that vanilla tabular Sarsa + ε-greedy was insufficient for the intended creative exploratory workflow.",
            "limitations_or_failures": "Requires coarse discretization of continuous parameters (here 3 levels) which harmed precision and user satisfaction; participants reported the agent 'not reacting properly' to feedback and leading exploration at users' expense; inability to undo actions and restricted interaction design increased frustration; no quantitative RL performance metrics or convergence data in interactive setting were reported in this paper.",
            "uuid": "e1102.0",
            "source_info": {
                "paper_title": "Designing Deep Reinforcement Learning for Human Parameter Exploration",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Co-Explorer (Deep TAMER + intrinsic)",
            "name_full": "Co-Explorer: Deep TAMER agent with intrinsic motivation (density-based) exploration",
            "brief_description": "Final prototype agent implemented for interactive human-guided exploration: a Deep TAMER architecture (deep neural network predicting human feedback) combined with an intrinsic-motivation exploration strategy based on a tile-coded density model to direct exploration toward novel/unvisited parameter states, integrated with multiple human interaction modalities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Deep TAMER + intrinsic (density) exploration (Co-Explorer)",
            "agent_description": "A deep neural network (2 hidden layers, 100 units each per Appendix B) trained to predict human reinforcement (Deep TAMER) from state-action inputs; uses a replay buffer to repeatedly sample human feedback examples; exploration is driven by an intrinsic motivation signal computed from a density model of visited states (tile coding) that biases action selection toward low-density (novel) states; the system augments human feedback with a density-based novelty bonus and supports interaction modalities (guiding feedback, zone feedback, state commands, direct manipulation).",
            "adaptive_design_method": "Human-in-the-loop value learning (Deep TAMER) combined with curiosity/intrinsic motivation via count/density-based novelty (tile coding), with ε decay to modulate exploration probability.",
            "adaptation_strategy_description": "The agent learns a model R_hat(s,a) predicting expected human feedback from observed state-action-feedback tuples stored in replay memory (Deep TAMER). It adapts its action-selection by (1) taking greedy actions according to learned predicted human reward, (2) adding an intrinsic novelty bonus based on a density estimate (tile-coded counts) to encourage actions leading to low-density states, and (3) using an ε parameter with exponential decay to stochastically choose exploratory actions; user inputs (guiding feedback distributed to recent p state-action pairs with gamma weighting, and zone feedback applied to all state-action pairs leading to a labeled state) are incorporated into the replay buffer and into reward updates. State commands (change-zone) sample the density model to jump to the lowest-density state and autonomous exploration steers actions toward nearest low-density states.",
            "environment_name": "VST parameter space (evaluation workshop: 10-parameter VST)",
            "environment_characteristics": "High-dimensional continuous (normalized) parameter space (n=10 parameters, s_i ∈ [0,1]); deterministic parameter updates under agent actions; perceptual, complex mapping from parameters to audio output unknown to the agent prior to interaction; human feedback channel is sparse, noisy and non-stationary (creative subjective feedback); large effective state space requiring function approximation.",
            "environment_complexity": "10 continuous parameters discretized conceptually at 100 levels for agent precision (s_i discretization a_i = 0.01), implying 100^10 theoretical discrete combinations (≈1e20) — treated via function approximation (deep net) rather than tabular methods; agent speed in continuous mode: one action per 0.1 s; sessions lasted minutes (users spent on average 13 minutes in autonomous exploration in a 25–30 minute task).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": "No standard RL learning curves reported; design choices (Deep TAMER + replay memory) aimed to improve sample-efficiency for rapid human teaching; qualitative evidence that users could teach meaningful behaviors within single workshop sessions (tens to hundreds of feedback events). Logged interaction quantities: guiding feedback counts per participant ranged from 54 to 1,489 events; zone feedback counts ranged 10–233; average guiding feedback positivity ~55% (σ=17%) during evaluation, suggesting non-trivial training signal within session lengths.",
            "exploration_exploitation_tradeoff": "Balanced by a novelty-biased exploratory policy: actions are selected to greedily maximize predicted human feedback R_hat but with added intrinsic novelty bonus from the density model; stochastic exploration is modulated by an exponentially decaying ε so that exploration probability decreases slowly over time. Additionally, user commands can force exploration jumps (change-zone) or start/stop autonomous exploration, enabling manual control over the trade-off.",
            "comparison_methods": "Compared qualitatively against: (a) the pilot Sarsa ε-greedy prototype, (b) manual parametric exploration, and (c) user-specified baseline behaviors (participants' own strategies). In pilot the Sarsa and ε-greedy variants were tested; in the final workshop the Co-Explorer was evaluated in use (no direct numeric algorithmic baseline reported).",
            "key_results": "Deep TAMER combined with density-based intrinsic motivation made the agent usable in real creative exploration: participants spent &gt;50% of the discovery task time in autonomous mode (avg 13 min), used change-zone frequently (14–90 times per participant), and reported the intrinsic exploration behavior as preferable to simple random exploration. Interaction modalities (guiding feedback, zone feedback, state commands, direct manipulation) were effective and supported diverse co-exploration partnerships (user-as-leader vs agent-as-leader). Users rapidly appropriated the agent and reported that it promoted serendipity and discovery; however, quantitative RL performance metrics (cumulative reward, success rates) were not provided.",
            "limitations_or_failures": "No quantitative evaluation of RL convergence or numeric performance measures provided; some participants complained about agent reaction speed and wished for slower autonomous exploration to provide more precise guidance; zone-negative labels were at times counter-intuitive; Deep TAMER still assumes mostly-correct/stationary feedback in its formalism whereas actual creative users generate non-stationary, multi-objective, and sometimes socially-motivated signals — authors note this as a limitation and a direction for future work.",
            "uuid": "e1102.1",
            "source_info": {
                "paper_title": "Designing Deep Reinforcement Learning for Human Parameter Exploration",
                "publication_date_yy_mm": "2021-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces",
            "rating": 2,
            "sanitized_title": "deep_tamer_interactive_agent_shaping_in_highdimensional_state_spaces"
        },
        {
            "paper_title": "Interactively shaping agents via human reinforcement: The TAMER framework",
            "rating": 2,
            "sanitized_title": "interactively_shaping_agents_via_human_reinforcement_the_tamer_framework"
        },
        {
            "paper_title": "Unifying count-based exploration and intrinsic motivation",
            "rating": 2,
            "sanitized_title": "unifying_countbased_exploration_and_intrinsic_motivation"
        },
        {
            "paper_title": "Deep reinforcement learning from human preferences",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_from_human_preferences"
        },
        {
            "paper_title": "Human-level control through deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "humanlevel_control_through_deep_reinforcement_learning"
        }
    ],
    "cost": 0.0119785,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Designing Deep Reinforcement Learning For Human Parameter Exploration
1 Jul 2019</p>
<p>Hugo Scurto hugo.scurto@ircam.fr 
STMS Lab IRCAM-CNRS-Sorbonne Université</p>
<p>Bavo Van Kerrebroeck bavo.vankerrebroeck@ircam.fr 
STMS Lab IRCAM-CNRS-Sorbonne Université</p>
<p>Baptiste Caramiaux baptiste.caramiaux@lri.fr 
Situ CNRS -LRI
Université Paris-Sud</p>
<p>Frédéric Bevilacqua frederic.bevilacqua@ircam.fr 
STMS Lab IRCAM-CNRS-Sorbonne Université</p>
<p>Designing Deep Reinforcement Learning For Human Parameter Exploration
1 Jul 20197662CA06BF5633F1BABE000A74CB2BAFarXiv:1907.00824v1[cs.HC]
Software tools for generating digital sound often present users with high-dimensional, parametric interfaces, that may not facilitate exploration of diverse sound designs.In this paper, we propose to investigate artificial agents using deep reinforcement learning to explore parameter spaces in partnership with users for sound design.We describe a series of user-centred studies to probe the creative benefits of these agents and adapting their design to exploration.Preliminary studies observing users' exploration strategies with parametric interfaces and testing different agent exploration behaviours led to the design of a fully-functioning prototype, called Co-Explorer, that we evaluated in a workshop with professional sound designers.We found that the Co-Explorer enables a novel creative workflow centred on human-machine partnership, which has been positively received by practitioners.We also highlight varied user exploration behaviors throughout partnering with our system.Finally, we frame design guidelines for enabling such co-exploration workflow in creative digital applications.KeywordsInteraction Design • Machine Learning • Audio/Video Preprint.Under review for publication in ACM Transactions on Computer-Human Interaction (TOCHI).</p>
<p>Introduction</p>
<p>Reinforcement learning defines a computational framework for the interaction between a learning agent and its environment [61].The framework provides a basis for algorithms that learn an optimal behaviour in relation to the goal of a task [82].For example, reinforcement learning was recently used to learn to play the game of Go, simulating thousands of agent self-play games based on human expert games [78].The algorithm, called deep reinforcement learning, leveraged advances in deep neural networks to tackle learning of a behaviour in high-dimensional spaces [59].The autonomous abilities of deep reinforcement learning agents let machine learning researchers foresee prominent applications in several domains, such as transportation, healthcare, or finance [55].</p>
<p>Yet, one important current challenge for real-world applications is the ability for reinforcement learning agents to learn from interaction with human users.The so-called interactive reinforcement learning framework has been shown to hold great potential to build autonomous systems that are centered on human users [2], such as teachable and social robots [83], or assistive search engines [8].From a machine learning perspective, the main challenge lies in learning an optimal behaviour from small, non-stationary amounts of human data [50].From a human-computer interaction perspective, an important challenge consists in supporting human appropriation of algorithms' autonomous behaviours in relation to complex human tasks [80].</p>
<p>Our interest lies in investigating interactive reinforcement learning for human creative tasks, where a goal might not be well-defined by human users a priori [68].One such case of a human creative task is exploration [41].Exploration consists in trying different solutions to address a problem, encouraging the co-evolution of the solution and the problem itself [26].For example, designers may produce several sketches of a product to ideate the features of its final design, or test several parameter combinations of a software tool to create alternative designs in the case where the product has a digital form.The creative, human-centred, use case of exploration fundamentally differs from standard, machine-centred, reinforcement learning use cases, where a problem is implicitly defined as a goal behaviour, before the agent actually learns to find a solution as optimal behaviour [82].It thus stands as an exemplary use case to study human interaction with reinforcement learning agents.</p>
<p>In this paper, we aim at designing an interactive reinforcement learning system supporting human creative exploration.This question is addressed in the application domain of sound design, where practitioners typically face the challenge of exploring high-dimensional, parametric sound spaces.We propose a user-centred design approach with expert sound designers to steer the design of such a system and better conceptualize exploration within this context.We conducted two case studies to evaluate two prototypes that we developed.The last prototype implemented a deep reinforcement learning algorithm that we specifically designed to support human exploration tasks.</p>
<p>Our findings led to contributions at several levels.On the conceptual side, we were able to characterize different user approaches to exploration, and to what we have called co-exploration-exploration in cooperation with an interactive reinforcement learning agent.These range from analytical to spontaneous in the former case, and from user-to agent-as-leader in the latter.On the technical side, a user-centered approach let us adapt a deep reinforcement learning algorithm to the case of co-exploration in high-dimensional parameter spaces.This notably required creating additional interaction modalities to user reinforcement, jointly with autonomous learning implementations of reinforcement learning algorithms.Lastly, on the design side, we extracted a set of important challenges that we deem critical for joint HCI and machine learning design in creative applications.These include: (1) engaging users with machine learning, (2) foster diverse creative processes, and (3) steer users outside comfort zones.</p>
<p>Related Work</p>
<p>In this section, we review related work on machine learning in the field of Human-Computer Interaction, encompassing creativity support tools, interactive machine learning, and interactive reinforcement learning, with a focus on human exploration.</p>
<p>Creativity Support Tools</p>
<p>Creativity support tools have long focused on exploration as a central task to human creative work [77].Design guidelines for supporting exploration were developed, which include aiming at simple interfaces for appropriating the tool and getting into sophisticated interaction more easily [25].Flexible interaction modalities that can adapt to users' very own styles of thinking and creating may also be required [68].In particular, parameter space exploration remains a current challenge for HCI research [14].Recently, creativity-oriented HCI researchers underlined the need to move toward interdisciplinary research collaborations [64].</p>
<p>Machine learning was in this sense examined for its implications in design [51] and identified as an opportunity for user experience [27,88,89].Yet, a large body of work in the machine learning research community has so far focused on constructing autonomous algorithms learning creative behaviour from large amounts of impersonal data-falling under the name of computational creativity [85].While this have allowed the building of powerful tools and models for creation, one may be concerned in the question of how to include human users in the design of such models to support human-computer co-creation [46].Davis et al. proposed a model of creativity that explicitly considers the computer as an enactive entity [21].They notably stressed the potential of combining creativity support tools with computational creativity to enrich a collaborative process between the user and the computer [21].The Drawing Apprentice, a co-creative agent that improvizes in real-time with users as they draw, illustrates their approach [20].While their user study confirms the conceptual potential of building such artistic computer colleagues, its technical implementation remains specific to the use case at stake-e.g., drawing.We propose to jointly design a conceptual and technical framework that could be could easily be transferrable to other application domains-potentially realizing general mixed-initiative co-creativity [42,90].</p>
<p>Interactive Machine Learning</p>
<p>Interactive machine learning [28] allows human users to build customized models by providing their own data examplestypically a few of them.Not only users can customize training examples, but they are also allowed to directly manipulate algorithm parameters [47,86], as well as to receive information on the model's internal state [3,63].Applications in HCI cover a wide range of tasks, such as handwriting analysis [76], recommender systems [4], or prioritising notifications [5].Interactive machine learning mainly builds on supervised learning, which defines a computational framework for the learning of complex input-output models based on example input-output pairs.The "human-in-the-loop" approach to supervised learning critically differs from the computational creativity approach, which typically relies on huge, impersonal databases to learn models [39].</p>
<p>Interactive machine learning is one such example of a generic framework for human-computer co-creation [2].The technical framework was successfully applied across several creative domains, such as movement interaction design [91,35,38], web page design [54] or video games [49].Specifically, research studying users building customized gestural controllers for music brought insight on the creative benefits of interacting with machine learning [31].Not only were users able to accomplish their design goal-e.g., demonstrating a given gesture input for controlling a given sound parameter output-, but they also managed to explore and rapidly prototype alternative designs by structuring and changing training examples [32].These patterns were reproduced by novice users who gained accessibility using examples rather than raw parameters as input [48].The algorithms' sometimes surprising and unexpected outcomes favoured creative thinking and sense of partnership in human users [30].</p>
<p>Typical workflows in interactive machine learning tend to iterate on designing training examples that are built from a priori representative features of the input space to support exploration.Yet, in some creative tasks where a problem definition may be found only by arriving at a solution [26,69], it might be unfeasible for users to define, a priori, such representative features of the final design [48].Other approaches proposed methods to release such contraints, for example by exploring alternative machine learning designs by only defining the limits of some parameter space [73].We propose to further investigate machine learning frameworks able to iteratively learn from other user input modalities, and explicitly considering mixed-initiative workflows, where systems autonomously adapt to users [23].As reviewed in the next section, using interactive reinforcement learning offers such perspectives.</p>
<p>Interactive Reinforcement Learning</p>
<p>Interactive reinforcement learning defines a computational framework for the interaction between a learning agent, a human user, and an environment [2].Specifically, users can communicate positive or negative feedback to the agent, in the form of a numerical reward signal, to teach it which action to take when in a certain environment state.The agent is thus able to adapt its behaviour to users, while remaining capable of behaving autonomously in its environment.</p>
<p>Interactive reinforcement learning has been recently applied in HCI [70], with promising applications in exploratory search [40,9] and adaptive environments [36,66].Integrating user feedback in reinforcement learning algorithms is computationally feasible [79], helps agents learn better [50], can make data-driven design more accessible [56], and holds potential for rich human-computer collaboration [80].Applications in Human-Robot Interaction informed on how humans may give feedback to learning agents [83], and showed potential for enabling human-robot co-creativity [33].Recently, reinforcement learning has witnessed a rise in popularity thanks to advances in deep neural networks [59].Powerful models including user feedback have been developed for high-dimensional parameter spaces [17,84].Design researchers have identified reinforcement learning as a promising prospective technique to improve human-machine "joint cognitive and creative capacity" [53].</p>
<p>We believe that interactive reinforcement learning-especially deep reinforcement learning-holds great potential for supporting creative tasks-especially exploration of high-dimensional parameter spaces.First, its computational framework, constituted by environment states, agent actions, and user feedback, remains fully generic [82], and thus potentially allow the design of generic interaction modalities transferrable to different application domains.Second, the autonomous behaviour intrinsic to reinforcement learning algorithms may be exploited to build a novel creative mixed-initiative paradigm, where the user and the agent would cooperate by taking actions that are "neither fully aligned nor fully in conflict" [18].Finally, we consider that user feedback could be a relevant input modality in the case of exploration, notably for expressing on-the-fly, arbitrary preferences toward imminent modifications, as opposed to representative examples.As previously stated, this requires investigating a somewhat unconventional use of reinforcement learning: if previous works employed user feedback to teach agents a "correct" behavior in relation to a task's goal, it is less obvious whether such a correct behavior may be well-defined-or even exists-for human users performing exploration.</p>
<p>General Approach</p>
<p>In this section, we describe the general approach of our paper, applying interactive reinforcement learning for human parameter space exploration in the creative domain of sound design.</p>
<p>Application Domain</p>
<p>Sound design is an exemplary application domain for studying exploration-taking iterative actions and multiple steps to move from an ill-formed idea to a concrete realization [37].Sonic exploration tasks can take myriad of forms: for example, composers explore various sketches of their musical ideas to write a final score; musicians explore different playing modes to shape an instrument's tone; sound designers explore several digital audio parameters to create unheard-of sounds [60,22].</p>
<p>Most of today's digital commercial tools for sound synthesis, named Virtual Studio Technology (VST, see Fig. 1), still rely on complex interfaces using tens of technical parameters as inputs.These parameters often relate to the underlying algorithms that support sound synthesis, preventing users from establishing a direct perceptual relationship with the sound output.To that one may add the exponential number of parameter combinations, called presets, that eventually correspond to given sound designs.It is arguable that these interfaces may not be the best to support human exploration: as the perceptual outcome of acting on a given parameter may rapidly become unpredictable, they may hinder user appropriation [68,77].By formalizing human parameter space exploration as an interactive reinforcement learning problem, we seek to tackle both issues at once.First, human navigation in high-dimensional parameter spaces may be facilitated by the reinforcement learning computational framework, made of sequences of states, actions, and rewards.Second, human creativity may be stimulated by the autonomous behaviour of reinforcement learning algorithms, suggesting other directions or design solutions to users along exploration.</p>
<p>Method</p>
<p>We adopted a user-centered approach to lead joint conceptual and technical work on interactive reinforcement learning for parameter space exploration.Two design iterations-a pilot study and an evaluation workshop-were conducted over the course of our research.Two prototypes were designed and developed-one initial reinforcement learning prototype, and the Co-Explorer, our final deep reinforcement learning prototype.The process thus includes sequentially:</p>
<p>• Prototype 1: Implementing a reinforcement learning algorithm that learns to explore sound parameter spaces from binary human feedback • Pilot study: Observing and interviewing participants exploring sound spaces, first using standard parametric interfaces, then using our initial reinforcement learning prototype • Prototype 2: Designing deep reinforcement learning in response to design ideas suggested by our pilot study • Evaluation workshop: Observing and discussing with participants using and appropriating the Co-Explorer, our final prototype, in two creative tasks related to exploration</p>
<p>We worked with a total of 14 users (5 women, 9 men; all French) through the series of activities.From the 14 total, there were 2 who took part in all of the activities listed below, to testify of our prototype's improvements.Our users covered different areas of expertise in sound design and ranged from sound designers, composers, musicians, and artists to music researchers and teachers.Thus, they were not all constrained to one working methodology, one sonic practice or one application domain.Our motivation was to sample diverse approaches to exploration that sound design may provoke, in order to design a flexible reinforcement learning algorithm that may suit a variety of users' working styles [68].</p>
<p>Pilot Study</p>
<p>We organized a one-day pilot study with four of our expert participants.The aims of this pilot study were to: Observe approaches to exploration in standard parametric interfaces; Identify problems users experience; Introduce the reinforcement learning technology in the form of a prototype; Brainstorm ideas and possible breakdowns.</p>
<p>The study was divided in two parts: (1) parametric interface exploration, then (2) interactive reinforcement learningbased exploration.We conducted individual semi-structured interviews at the end of each part, having each participant do the study one by one.This structure was intended to bring each participant to become aware of their subjective experience of exploration [65].Our intention was to open up discussions and let participants suggest design ideas about interactive reinforcement learning, rather than testing different algorithmic conditions in a controlled, experimental setup.We spent an average of 2 hours with each of our four participants, who covered different expertise in sound design (composition, sound design, interaction design, research).In the first part of the study, participants were asked to find and create a sound preset of their choice using three different parametric interfaces with different number of parameters (respectively 2, 6, and 12, see Fig. 2).No reinforcement learning agent was used.We linked each interface to a different sound synthesis engine (respectively using FM synthesis1 , and one commercial VST from which we selected 6, then 12, parameters).Sound was synthesized continuously; participants' actions were limited to move the knobs using the mouse to explore the design space offered by all possible combinations.Knobs' technical names were hidden to test the generic effect of parameter dimensionality in interface exploration, and avoid any biases due to user knowledge of parameter function (which typically occur with labelled knobs).Interface order was randomized; we let participants spend as much time as they wanted on each interface to let them explore the spaces freely.</p>
<p>Analysis</p>
<p>We were interested in observing potential user strategies in parameter space exploration.We thus logged parameter temporal evolution during the task.It consists in an n-dimensional vector, with n being the number of parameters (respectively 2, 6, then 12).Sample rate was set to 100 ms, which is a standard value for interaction with sound and musical interfaces [45].We used Max/MSP2 and the MuBu3 library to track user actions on parameters and record their evolutions.We used structured observation to study participants' interviews.This method was meant to provide a thorough qualitative analysis on user exploration strategies.</p>
<p>Observations</p>
<p>Qualitative analysis of parameter temporal evolution let us observe a continuum of approaches to parametric interface exploration.We call the first extremity of this continuum analytical exploration: this involves actioning each of the knobs one after the other over their full range.The second is called spontaneous exploration: this involves making random actions on the knobs.Figure 3 shows examples for each of these two approaches.One participant was consistently analytical over the three interfaces; one was consistently spontaneous over the three.The two others combined both approaches over the three interfaces.Interview analysis let us map these approaches to different subgoals in exploration.The analytical approach concerns exploration of the interface at a parameter level: "The strategy is to test [each knob] one by one to try to grasp what they do", one participant said.The goal of exploration is then related to building a mental map of the parameters to learn how to navigate in the design space.The spontaneous approach concerns exploration of the design space at a creative level: "I moved the knobs more brutally and as a result of serendipity I came across into something different, that I preferred for other reasons...", another participant said.The goal of exploration is then related to discovering new parameter states leading to inspiring parts of the design space.</p>
<p>Discovery is critical to parameter space exploration."Once [the knobs] are isolated, you let yourself wander a bit more...", one participant analysed.Surprise is also important: "To explore is to be in a mental state in which you do not aim at something precise", one participant said.Interestingly, we observed that participants often used words related to perceptual aspects rather than technical parameters."I like when you can get a sound that is... um... Consistent, like, coherent.And at the same time, being able to twist in many different ways.This stimulates imagination, often", one participant said.Two participants mentioned that forgetting the parametric interface may be enjoyable in this sense: "I appreciate an interface that does not indicate [...], that has you go back into sound, so that you are not here reading things, looking at symbols...", one participant said.</p>
<p>All participants reported being hindered in their exploration by the parameter inputs of the three interfaces.As expected, the more parameters the interface contained, the larger the design space was, and the harder it was to learn the interface."For me, the most important difficulty is to manage to effectively organise all things to be able to re-use them.",one participant said.Time must be spent to first understand, then to memorize the role of parameters, taking into account that their role might change along the path of exploration.This hampers participants' motivation, often restraining themselves to a subspace of the whole design space offered by the tool: "after a while I was fed up, so I threw out some parameters", one participant said about the 12-knob interface.</p>
<p>Participants discussed the limitations encountered in the study in light of their real-world practice with commercial interfaces.Two participants mentioned using automation functions to support parameter space exploration.Such functions include randomizing parameter values, automating parameter modification over time, or creating new control parameters that "speak more to your sensibility, to your ears, than to what happens in the algorithm", to cite one of the participants.Two participants also use factory presets to start exploration: "I think that in some interfaces they are pretty well conceived for giving you the basis of a design space.Then it's up to you to find what parameters to move", one participant said.Two participants said that the graphical user interfaces, including parameter names, knob disposition, and visual feedback on sound, may help them manage to lead exploration of large parameter spaces.</p>
<p>Part 2: RL Agent Prototype</p>
<p>Results in first part let us identify different user approaches to parametric interface exploration, as well as different problems encountered in high-dimensional parameter spaces.In the second part, we were interested in having participants test the reinforcement learning technology in order to scope design ideas and possible breakthroughs in relation to exploration.</p>
<p>Implementation</p>
<p>We implemented an initial prototype for our pilot study, that we propose to call "RL agent" for concision purposes.</p>
<p>The prototype lets users navigate through different sounds by only communicating positive or negative feedback to a reinforcement learning agent.The agent learns from feedback how to act on the underlying synthesis parameters in lieu of users (see Fig. 4).Formally, the environment is constituted by the VST parameters, and the agent iteratively acts on them.Computationally, we considered the state space S = {S} constituted by all possible parameter configurations S = (s 1 , ..., s n ), with n being the number of parameters, and s i ∈ [s min , s max ] being the value of the i th parameter living in some bounded numerical range (for example, s i can control the level of noise normalized between 0 and 1).We defined the corresponding action space A(S) = {A} as moving up or down one of the n parameters by one step a i , except when the selected parameter equals one boundary value:
A(S) =    ±a i for s i ∈]s min , s max [ +a i for s i = s min −a i for s i = s max(1)
An ε-greedy method defines the autonomous exploration behaviour policy of the agent-how it may act by exploiting its accumulated feedback while still exploring new unvisited states [82].It consists in having the agent take an optimal action with probability ε, and reciprocally, take a random action with probability 1 − ε.For example, ε = 1 would configure an always exploiting agent-i.e., always taking the best actions based on accumulated feedback-, while ε = 0 would configure an always exploring agent-i.e., never taking into account the received feedback.Our purpose in this study was to examine whether different exploration-exploitation trade-offs could map to different user approaches to exploration.Finally, we propose that the user would be responsible for generating feedback.We directly mapped user feedback to the environmental reward signal R associated with a given state-action pair (S, A).The resulting formalization-where an agent takes actions that modifies the environment's state and learn from feedback received from a user-defines a generic interactive reinforcement learning problem.We implemented Sarsa, which is a standard algorithm to learn how to act in many different environment state, i.e., for each given parameter configuration [82].It differs from multi-armed bandits, which learns how to act in one unique environment state [56].Importantly, as evoked in Section 1, Sarsa was designed to learn an optimal behaviour in relation to the goal of a task.Our purpose in this study was to scope the pros and cons of such a standard reinforcement learning algorithm for human exploration tasks, judging how it may influence user experience, and framing how it may be engineered with regard to this.The convergence of the Sarsa algorithm in an interactive setup where users provide feedback was evaluated in a complementary work [72].</p>
<p>We used the largest VST-based 12-parameter space of the first part (n = 12) as the environment of our prototype.Because Sarsa is defined on discrete state spaces, each parameter range was discretized in three normalized levels (s i ∈ {0, 0.5, 1}, a i = 0.5; 0 ≤ i ≤ n).Although this would have been a design flaw in a perceptual experiment on typical VSTs, this allowed for obvious perceptual changes, which was required to investigate feedback-based interaction with a large variety of sounds.</p>
<p>Procedure</p>
<p>Our participants were asked to find and create a sound preset of their choice by communicating feedback to three different agents with different exploration behaviours (respectively ε = 0; ε = 1; and ε = 0.5).Sound was synthesized continuously, in a sequential workflow driven by the agents' algorithmic functioning.At step t, participants could listen to a synthesized sound, and give positive or negative feedback by clicking on a two-button interface (Fig. 5).This would have the agent take an action on hidden VST parameters, modify the environment's state, and synthesize a new sound at step t + 1. Participants were only told to give positive feedback when the agent gets closer to a sound that they enjoy, and negative feedback when it moves away from it.They were not explained the agent's internal functioning, nor the differences between the three agents.The starting state for t = 0 was randomly selected.Agent order was randomized; we asked participants to spend between 5 and 10 minutes with each.</p>
<p>Figure 5: One of our four participants using a two-button interface to communicate binary feedback to the RL agent prototype in the pilot study.</p>
<p>Analysis</p>
<p>We logged all participant actions in the graphical user interface.It consisted in timed onsets for positive feedback on the one hand, and negative feedback on the other hand.We also logged parameter temporal evolution to observe how the RL agent would act on parameters following user feedback.We used structured observation to study participants' interviews and discussions led at the end of the pilot study.</p>
<p>Reactions</p>
<p>All participants reported forgetting synthesis parameters to focus on the generated sound.The simplicity and straightforwardness of the new interface benefited their exploration."There's always this sensation that finally you are more focused on listening to the sound itself rather than trying to understand the technology that you have under your hands, which is really great, yeah, this is really great", one participant said.</p>
<p>The computational framework defined by reinforcement learning was well understood by all participants."There's somewhat a good exploration design [sic], because it does a bit what you do [with the parametric interface], you move a thing, you move another thing...", one participant said.All participants enjoyed following agents' exploration behaviours, mentioning a playful aspect that may be useful for serendipity.Three participants in turn adapted their exploration to that of the agent: "you convince yourself that the machine helps you, maybe you convince yourself that it is better... and after you go on exploring in relation to this", one participant said.Interestingly, one participant that was skeptical about partnering with a computer changed his mind interacting with the RL agent: "We are all different, so are they", he commented, not without a touch of humor.</p>
<p>Uses of Feedback</p>
<p>Descriptive statistics informed on how participants used the feedback channel.Three participants gave feedback every 2.6 seconds on average (σ = 0.4), globally balancing positive with negative (average of 44.8% positive, σ = 0.02).</p>
<p>The fourth participant gave feedback every 0.9 seconds on average (σ = 0.07) which was mostly negative (average of 17.2% positive, σ = 0.02).All participants reappropriated the feedback channel, quickly transgressing the task's instructions toward the two-button interface to fulfill their purposes.One participant used feedback to explore agents' possible behaviors: "Sometimes you click on the other button, like, to see if it will change something, [...] without any justification at all", he commented.Another used the '-' button to tell the agent to "change sound".Two participants also noticed the difference between feedback on sound itself, and feedback on the agent's behavior: "there's the 'I don't like' compared to the sound generated before, and the 'I don't like it at all', you see", one of them said.</p>
<p>Breakdowns</p>
<p>Rapidly, though, participants got frustrated interacting with the RL agent.All participants judged that agents did not always reacted properly to their feedback, and were leading exploration at the expense of them: "sometimes you tell 'I don't like', 'I don't like', 'I don't like', but it keeps straight into it!(laughs)", one participant said.Contrary to what we expected, participants did not expressed a strong preference for any of the three tested agents.Only one participant noticed the randomness of the exploring agent, while the three other participants could not distinguish the three agents.This may be caused by the fact that the Sarsa algorithm was not designed for the interactive task of human exploration.</p>
<p>Reciprocally, this may be induced by experiential factors due to the restricted interaction of our RL agent prototype, e.g., preventing users to undo their last actions.Finally, two participants also complained about the lack of precision of the agent toward the generated sounds.This was induced by the Sarsa algorithm, which required to discretize the VST parameter space.</p>
<p>Design Implications</p>
<p>Participants jointly expressed the wish to lead agent exploration.They suggested different improvements toward our RL agent prototype:</p>
<p>• Express richer feedback to the agent (e.g., differentiating "I like" from "I really like")</p>
<p>• Control agent path more directly (e.g., commanding the agent to go back to a previous state, or to some new unvisited state in the parameter space) • Improve agent algorithm (e.g., acting more precisely on parameters, reacting more accurately to feedback) • Integrate agent in standard workspace (e.g., directly manipulating knobs at times in lieu of the agent) Interestingly, one participant suggested moving from current sequential workflow (where the agent waits for user feedback to take an action on the environment's state) to an autonomous exploration workflow (where the agent would continuously take actions on the environment's state, based on both accumulated and instantaneous user feedback).Three participants envision that such an improved RL agent could be useful in their practice, potentially allowing for more creative partnerships between users and agents.</p>
<p>Co-Explorer</p>
<p>Our pilot study led us to the design of a final prototype, called Co-Explorer.We decided to first design new generic interaction modalities with RL agents, based on users' reactions with both parametric interfaces and our initial prototype.We then engineered these interaction modalities, developing a generic deep reinforcement learning algorithm for parameter space exploration along with a new specific interface for sound design.</p>
<p>Interaction Modalities</p>
<p>Our initial prototype only employed user feedback as its unique interaction modality.This limited our participants, who suggested a variety of new agent controls to support exploration.We translated these suggestions into new interaction modalities that we conceptualized under three generic categories: (1) user feedback, (2) state commands, and (2) direct manipulations (as shown in Fig. 6).</p>
<p>User Feedback</p>
<p>Our design intention is to support deeper user customization of the parameter space, while also allowing richer user contribution to agent learning.We thus propose to enhance user feedback as defined in our initial prototype, distinguishing between guiding and zone feedback.Guiding feedback corresponds to users giving binary guidance toward the agent's instantaneous trajectory in the parameter space.Users can give either positive-i.e., "keep going in that direction"-or negative guidance feedback-i.e., "avoid going in that direction".Zone feedback corresponds to users putting binary preference labels on given zones in the parameter space.It can either be positive-i.e., "this zone interests me"-or negative-i.e., "this zone does not interest me".Zone feedback would be used for making assertive customization choices in the design space, while guiding feedback would be used for communicating on-the-fly advice to the learning agent.</p>
<p>State Commands</p>
<p>Additionally, our design intention is to support an active user understanding of agent actions in the parameter space.We propose to define an additional type of interaction modality-we call them "state commands".State commands enable direct control of agent exploration in the parameter space, without contributing to its learning.We first allow users to command the agent to go backward to some previously-visited state.We also enable users to command the agent to change zone in the parameter space, which corresponds to the agent making an abrupt jump to an unexplored parameter configuration.Last but not least, we propose to let users start/stop an autonomous exploration mode.Starting autonomous exploration corresponds to letting the agent continuously act on parameters, possibly giving feedback throughout its course to influence its behaviour.Stopping autonomous exploration corresponds to going back to the sequential workflow implemented in our initial prototype, where the agent waits for user feedback before taking a new action on parameters.</p>
<p>Direct Manipulation</p>
<p>Lastly, our design intention is to augment, rather than replace, parametric interfaces with interactive reinforcement learning, leveraging users expertise with these interfaces and providing them with additional modalities that they could solicit when they may need it.We thus propose to add "direct manipulations" to support direct parameter modification through a standard parametric interface.It lets users explore the space on their own by only manipulating parameters without using the agent at all.It can also be used to take the agent to a given point in the parameter space-i.e., "start exploration from this state"-, or to define by hand certain zones of interest using a zone feedback-i.e., "this example preset interests me".Inversely, the parametric interface also allows to visualize agent exploration in real-time by observing how it acts on parameters.</p>
<p>A last, global interaction modality consists in resetting agent memory.This enables users to start exploration from scratch by having the agent forget accumulated feedback.Other modalities were considered, such as modifying the agent's speed and precision.Preliminary tests pushed us to decide not to integrate them in the Co-Explorer.</p>
<p>Deep Reinforcement Learning</p>
<p>Based on our observations in the pilot study, we developed our reinforcement learning agent at three intertwined technical levels: (1) feedback formalization, (2) learning algorithm, and (3) exploration behaviour.</p>
<p>Feedback Formalization</p>
<p>One challenge consisted in addressing the non-stationarity of user feedback data along their exploration.We implemented Deep TAMER, a reinforcement learning algorithm suited for human interaction [84].Deep TAMER leverages a feedback formalization that distinguishes between the environmental reward signal-i.e., named R in the Sarsa algorithm of our initial prototype-and the human reinforcement signal-e.g., feedback provided by a human user.This technique, implemented in the TAMER algorithm [50], was shown to reduce sample complexity over standard reinforcement learning agents, while also allowing human users to teach agents a variety of behaviours.We detail the differences between standard RL algorithms and (deep) TAMER in Appendix A.</p>
<p>Learning Algorithm</p>
<p>Another challenge was to tackle learning in high-dimensional parametric spaces that are typical of our use case.Deep TAMER employs function approximation [82] to generalize user feedback given on a subset of state-action pairs to unvisited state-action pairs.Specifically, a deep neural network is used to learn the best actions to take in a given environment state, by predicting the amount of user feedback it will receive [59,84].The resulting algorithm can learn in high-dimensional state spaces S = {S} and is robust to changes in discretization a i of the space.For our application in sound design, we engineered the algorithm for n = 10 parameters.We normalized all parameters and set the agent's precision by discretizing the space in one hundred levels (s i ∈ [0, 1], a i = 0.01; 0 ≤ i ≤ n).</p>
<p>A last challenge was to learn quickly from the small amounts of data provided by users during interaction.Deep TAMER uses a replay memory, which consists in storing the received human feedback in a buffer D, and sampling repeatedly from this buffer with replacement [84].This was shown to improve the learning of the deep neural network in high-dimensional parameter spaces in the relatively short amount of time devoted to human interaction.We set the parameters of the the deep neural network by performing a parameter sweep and leading sanity checks with the algorithm; we report them in Appendix B.</p>
<p>Exploration Behaviour</p>
<p>We developed a novel exploration method for autonomous exploration behaviour.It builds on an intrinsic motivation method, which pushes the agent to "explore what surprises it" [10].Specifically, it has the agent direct its exploratory actions toward uncharted parts of the space, rather than simply making random moves-as in the ε-greedy approach implemented in our initial prototype.It does so by building a density model of the parameter space based on all visited states.We used tile coding, a specific feature representation extensively used in the reinforcement learning literature to efficiently compute and update the density model in high-dimensional spaces [82].We parameterized ε with an exponential decay in such a way that its initial value would slowly decrease along user exploration.For our application in sound design, agent speed in continuous exploration mode was set to one action by tenths of a second.We report the parameters set for our exploration method after sanity checks in Appendix C.</p>
<p>Integrating Interaction Modalities In Reinforcement Learning</p>
<p>To fully realize our interaction design, we integrated the modalities defined in Section 5.1 within the reinforcement learning framework defined in Section 5.2.</p>
<p>User Feedback</p>
<p>We developed generic methods corresponding to user feedback modalities defined in Section 5.1.1 that we used in the feedback formalization of Section 5.2.1.For guiding feedback, we assigned user positive or negative feedback value over the p last state-action pairs taken by the agent (see Fig. 8, left), with a decreasing credit given by a Gamma distribution [50].For zone feedback, we computed all possible state-action pairs leading to the state being labelled and impacted them with positive or negative feedback received (see Fig. 8, right).This enables to build attractive and repulsive zones for the agent in the parameter space.Finally, we added a reward bonus to user feedback to enhance the agent's learning relatively to the novelty of a state.This reward bonus is computed using the density model described in Section 5.2.3.Figure 8: Schematic representations for feedback computation methods.Here, positive feedback is given in some state situated at the center of the square.Left: Guiding feedback is distributed over the p lastly-visited state-action pairs.Right: Zone feedback impacts all state-action pairs potentially leading to the labelled state.</p>
<p>State Commands</p>
<p>We developed generic methods corresponding to state commands defined in Section 5.1.2using the exploration behaviour defined in Section 5.2.3.Changing zone has the agent randomly sampling the density distribution and jump to the state with lowest density (see Fig. 7, left).Autonomous exploration mode has the agent take exploratory actions that lead to the nearest state with lowest density with probability ε (see Fig. 7, right).</p>
<p>Direct Manipulation</p>
<p>We integrated direct manipulations as defined in Section 5.1.3by leveraging the learning algorithm defined in Section 5.2.2.When parameters are modified by the user, the reinforcement learning agent converts all parameters' numerical values as a state representation, taking advantage of the algorithm's robustness in changes of discretization.Reseting agent memory has the reinforcement learning algorithm erase all stored user feedback and trajectory, and load a new model.</p>
<p>Implementation</p>
<p>Agent</p>
<p>We implemented the Co-Explorer as a Python library 4 .It allows to connect the deep reinforcement learning agent to any external input device and output software, using the OSC protocol for message communication [87].This was done to enable future applications outside the sound design domain.Each of the features described in Section 5.2 are implemented as parameterized functions, which supports experimentation of interactive reinforcement learning with various parameter values as well as order of function calls.The current version relies on TensorFlow for deep neural network computations.The complete algorithm implementation and all learning parameters are shown in the Appendix.</p>
<p>Interface</p>
<p>We implemented an interactive interface for our application in sound design (Fig. 9), which integrates all interaction modalities defined in Section 5.1.It builds on Max/MSP, a visual programming environment for real-time sound synthesis and processing.Standard parametric knobs enable users to directly manipulate parameters, as well as to see the agent act on it in real-time.An interactive history allows users to command the agent to go to a previously-visited state, be they affected by user feedback (red for negative, green for positive) or simply passed through (grey).Keyboard inputs support user feedback communication, as well as state commands that control agent exploration (changing zone, and start/stop continuous exploration mode).Lastly, a clickable button enables users to reset agent memory.</p>
<p>Evaluation Workshop</p>
<p>We evaluated the Co-Explorer in a workshop with a total of 12 professional users (5 female, 7 male).The aims of the workshop were to: Evaluate each interaction modality at stake in the Co-Explorer; understand how users may appropriate the agent to support parameter space exploration.</p>
<p>The workshop was divided in two tasks: (1) explore to discover, and (2) explore to create.This structure was intended to test the Co-Explorer in two different creative tasks (described in Section 6.1 and 6.2, respectively).Participants ranged from sound designers, composers, musicians, and artists to music researchers and teachers.They were introduced to the agent's interactive modalities and its internal functioning at the beginning of the workshop.In each part, they were asked to report their observations by filling a browser-based individual journal.Group discussion was carried on at the end of the workshop to let participants exchange views over parameter space exploration.The workshop lasted approximately three hours each.In the first part of the workshop, participants were presented with one parameter space (see Fig. 10).They were asked to use the Co-Explorer to explore and discover the sound space at stake.Specifically, we asked them to find and select five presets to constitute a representative sample of the space.We defined the parameter space by selecting ten parameters from a commercial VST.Participants were encouraged to explore the space thoroughly.The task took place after a 10-minute familiarizing session: individual exploration lasted 25 minutes, followed by 5 minutes of sample selection, and 20 minutes of group discussion.</p>
<p>Analysis</p>
<p>All participant's actions were logged into a file.These contained timed onsets for user feedback-i.e., binary guiding and zone feedback-, state commands-i.e., backward commands in the history, changing zone commands, and autonomous exploration starting/stopping-, and direct manipulations-i.e., parameter temporal evolutions.We also logged timed onsets for preset selection in relation to the task, but did not include the five presets themselves into our analysis.Our motivation was to focus on the process of exploration in cooperation with the Co-Explorer, rather than on the output of it.We used structured observation to extract information from individual journals and group discussion.</p>
<p>Results</p>
<p>We first looked at how users employed state commands.Specifically, the autonomous exploration mode, which consisted in letting the agent act cotinuously on parameters on its own, was an important new feature compared to our sequentiam initial RL agent prototype.Participants spent more than half of the task using the Co-Explorer in this mode (total of 13 minutes on average, σ = 4.7).Ten participants used autonomous exploration over several short time slices (average of 50 seconds, σ = 25s), while the two remaining participants used it over one single long period (respectively 9 and 21 minutes).P5 commented about the experience: "It created beautiful moments during which I really felt that I could anticipate what it was doing.That was when I really understood the collaborative side of artificial intelligence".</p>
<p>The changing zone command, which enabled to jump to an unexplored zone in the parameter space, was judged efficient by all participants to find diverse sounds within the design space.It was used between 14 and 90 times, either to start a new exploration (P1: "Every time I used it, I found myself in a zone that was sufficiently diametrically opposed to feel that I could explore something relatively new"), or to rapidly seize the design space in the context of the task (P12: "I felt it was easy to manage to touch the edges of all opposite textures").Interestingly, P2 noticed that the intrisic motivation method used for agent exploration behaviour "brought something more than a simple random function that is often very frustrating".</p>
<p>We then looked at how users employed feedback.Guiding feedback was effectively used in conjunction with autonomous exploration by all participants, balancing positive with negative (55% positive on average, σ = 17%).Participants gave various amounts of guiding feedback (between 54 and 1489 times).These strategies were reflected by different reactions toward the Co-Explorer.For example, one participant was uncertain in controlling the agent through feedback: "if the agent goes in the right direction, I feel like I should take time to see where it goes", he commented.On the contrary, P1 was radical in his controlling the agent, stating that he is "just looking for another direction", and that he uses feedback "without any value judgement".This reflects the results described in Section 4.2.4 using our initial RL agent prototype.</p>
<p>Zone feedback, enabling customization of the space with binary labels, was mostly given as positive by participants (72%, σ = 18%).Two participants found the concept of negative zones to be counter-intuitive."I was a bit afraid that if I label a zone as negative, I could not explore a certain part of the space", P8 coined.This goes in line with previous results on applying interactive reinforcement learning in the field of robotics [83].All participants agreed on the practicality of combining positive zone feedback with backward state commands in the history to complete the task."I labeled a whole bunch of presets that I found interesting [...] to after go back in the trajectory to compare how different the sounds were, and after continue going in other zones.I found it very practical", P8 reported.Overall, zone feedback was less times used than guiding feedback (between 10 and 233 times).</p>
<p>Finally, direct manipulation was deemed efficient by participants in certain zones of the design space."When I manage to hear that there is too much of something, it is quicker to parametrize sound by hand than to wait for the agent to find it itself, or to learn to detect it", P4 analyzed.P10 used them after giving a backward state command, saying she "found it great in cases where one is frustrated not to manage to guide the agent".P11 added that she directly manipulate parameters to "adjust the little sounds that [she] selected".P1 suggested that watching parameters move as the agent manipulates them could help learn the interface: "From a pedagogical point of view, [the agent] allows to access to the parameters' functioning and to the interaction between these parameters more easily [than without]".This supports the fact that machine learning visualizations may be primordial in human-centred applications to enable interpretability of models [2].</p>
<p>Relevance to Task</p>
<p>Three participants complained that the Co-Explorer did not react sufficiently quickly to feedback in relation to the task: "I would really like to feel the contribution of the agent, but I couldn't", P12 said.Also, P3 highlighted the difficulties to give evaluative feedback in the considered task: "without a context, I find it hard", he analysed.Despite this, all participants wished to spend more time teaching the Co-Explorer, by carefully customizing the parameter space with user feedback.For example, five participants wanted to slow the speed of the agent during autonomous exploration to be able to give more precise guidance feedback.Also, three participants wanted to express sound-related feedback: "There, I am going to guide you about the color of the spectrum.[...] There, I'm going to guide you about, I don't know, the harmonic richness of the sound, that kind of stuff...", P4 imagined.In the second part of the workshop, participants were presented with four pictures (Fig. 11).For each of these four pictures, they were asked to explore and create two sounds that subjectively depict the atmosphere of the picture.In this part, we encouraged participants to appropriate interaction with the Co-Explorer and feel free to work as they see fit.We used a new sound design space for this second part, which we designed by selecting another ten parameters from a commercial VST.Individual exploration and sound selection lasted 30 minutes, followed by 20 minutes of group discussion and 10 minutes of closing discussion.</p>
<p>Figure 11: The four pictures framing the creation task of the workshop.</p>
<p>Analysis</p>
<p>All participant actions were logged into a file, along with timed parameter presets selected for the four pictures.Again, we focused our analysis on the process of exploration rather than on the output of it.Specifically, for this open-ended, creative task, we did not aim at analysing how each agent interaction modality individually relates to a specific user intention.Rather, we were interested in observing how users may appropriate the mixed-initiative workflow at stake in the Co-Explorer.</p>
<p>We used Principal Component Analysis (PCA [44]), a dimensionality reduction method, to visualize how users switched parameter manipulation with agents.We first concatenated all participants' parameter evolution data as an n-dimensional vector to compute the two first principal components.We then projected each participant data onto these two components to support analysis of each user trajectory on a common basis.By doing this, relatively distant points would correspond to abrupt changes made in parameters (i.e., to moments when the user takes the lead on exploration).Continuous lines would correspond to step-by-step changes in parameters (i.e., to moments when the Co-Explorer explores autonomously).PCA had a stronger effect in the second part of our workshop.We interpret this as a support to the two-part structure that we designed for the workshop, and thus did not include analysis of the first part.Finally, we used structured observation to extract information from individual journals and group discussion.</p>
<p>Exploration Strategies</p>
<p>All participants globally expressed more ease interacting with the Co-Explorer in this second task."I felt that the agent was more adapted to such a creative, subjective... also more abstract task, where you have to illustrate.It's less quantitative than the first task", P9 analysed.User feedback was also reported to be more intuitive when related to a creative goal: "all parameters took their sense in a creative context.[...] I quickly found a way to work with it that was very efficient and enjoyable", P5 commented.Figure 12 illustrates the PCA for two different users interacting with the Co-Explorer.</p>
<p>Qualitative analysis of PCAs let us conceptualize a continuum of partnerships between our participants and the Co-Explorer.These could be placed anywhere between the two following endpoints: Figure 12: Two types of co-exploration partnerships shown in PCA visualizations of parameter evolution: User-as-leader (P9, left) and agent-as-leader (P7, right).Relatively distant points correspond to abrupt changes made in parameters (i.e., to moments when the user takes the lead).Continuous lines correspond to step-by-step changes in parameters (i.e., to moments when the Co-Explorer takes the lead).</p>
<p>• User-as-leader: This typically involves users first building a map of the design space (iteratively using changing zone and positive zone feedback), then generating variations of these presets (either through direct manipulation or short autonomous explorations).</p>
<p>• Agent-as-leader: This typically involves letting the Co-Explorer lead parameter manipulation (using autonomous exploration and guiding feedback), first setting some starting point in the design space (either using changing zone or direct manipulation).</p>
<p>Our interpretation is as follows.User-as-leader partnership may correspond to user profiles that approach creative work as a goal-oriented task, where efficacy and control are crucial (P10: "I am accustomed...Where I work, if you prefer, we have to get as quick as possible to the thing that works the best, say, and I cannot spend so much time listening to the agent wandering around").Reciprocally, agent-as-leader partnership may correspond to user profiles that approach creative work as an open-ended task, where serendipity is essential for inspiration (P5: "I did not try to look for the sound that would work the best.I rather let myself be pushed around, even a bit more than in my own practice").Some participants did not stabilize into one single partnership, but rather enjoyed the flexibility of the agent."It was quite fun to be able to let the agent explore, then stop, modulate a bit some parameters by hand, let it go and guide it again, changing zones too, then going back in the history... Globally, I have the impression of shaping, somewhat...I found it interesting", P11 coined.</p>
<p>Agent memory was handled with relevance to various creative processes toward the pictures.Seven participants disposed all four pictures in front of them (P7: "to always have them in mind.Then, depending on the agent's exploration, I told myself 'hey, this sound might correspond to this picture"').Three participants focused on one picture at a time, "without looking at the others".Four participants never reset the memory (P11: "my question was, rather, in this given sonic landscape, how can I handle these four pictures, and reciprocally"), and three participants reset agent memory for each of the different atmospheres shared by the pictures.Overall, participants benefited from partnering with the Co-Explorer in parameter space exploration: "It's a mix of both.I easily managed to project a sound on the picture at first glance, then depending on what was proposed, it gave birth to many ideas", one participant said.</p>
<p>Toward Real-World Usages</p>
<p>All participants were able to describe additional features for the Co-Explorer to be usable in their real-world professional work environments-examples are, among others, connection to other sound spaces, memory transfer from one space to another, multiple agent memory management, or data exportation.They also anticipated creative uses for which the Co-Explorer were not initially designed.Half of the participants were enthusiastic about exploiting the temporal trajectories as actual artifacts of their creation (P6: "What I would find super interesting is to be able to select the sequences corresponding to certain parameter evolution, or playing modes.[...] It would be super great to select and memorize this evolution, rather than just a small sonic fragment").Finally, two participants further imagined the Co-Explorer to be used as musical colleagues-either as improvisers with which one could "play with both hands" (P2), or as "piece generators" (P6) themselves.</p>
<p>Discussion</p>
<p>Our process of research, design, and development led to contributions at three different levels: (1) conceptual insight on human exploration; (2) technical insight on reinforcement learning; and (3) joint conceptual and technical design guidelines on machine learning for creative applications.Our work with interactive reinforcement learning allowed for observing and characterizing user approaches to parameter space exploration, and supported it.While manipulating unlabelled parametric knobs of sound synthesizers, participants alternated between an analytical approach-attempting to understand the individual role of each parameter-and a spontaneous approach that could lead to combinations in the parameter space that might not be guessed with the analytical approach.While interacting with a reinforcement learning agent, participants tended to alternate the lead in new types of mixed-initiative workflows [42] that we propose to call co-exploration workflows.User-as-leader workflow was used for gaining control over each parameter of the design space.Agent-as-leader workflow allowed to relax users' control and provoke discoveries through the specific paths autonomously taken by the agent in the parameter space.Importantly, the benefit of interactive reinforcement learning for co-exploring sound spaces was dependent on the task.We found that this co-exploration workflow were more relevant to human exploration tasks that have a focus on creativity, such as in our workshop's second task, rather than discovery.Therefore, we believe that this workflow is well-suited in cases where exploration is somehow holistic (as in the creative task) rather than analytic (as in the discovery task where the goal is to understand the sound space to find new sounds).</p>
<p>Methodology</p>
<p>Our user-centered approach to exploration with interactive reinforcement learning allowed us to rapidly evaluate flexible interaction designs without focusing on usability.This process let us discover innovative machine learning uses that we may not have anticipated if we had started our study with an engineering phase.The simple, flexible, and adaptable designs tested in our first pilot study (parametric vs. RL) could in this sense be thought as technology probes [43].</p>
<p>Working with professional users of different background and practices-from creative coders to artists less versed in technology-was crucial to include diverse user feedback in the design process.Our results support this, as many user styles were supported by the Co-Explorer.That said, user-driven design arguably conveys inherent biases of users.This is particularly true when promoting AI in interactive technology [6,13].As a matter of fact, alongside a general enthusiasm, we did observe a certain ease among our professional users for expressing tough critiques, at times being skeptical on using AI, especially when the perception of the algorithm choice would contradict their spontaneous choice.Yet, the two professional users that took part to both our pilot study and workshop found the use of AI as welcome, testifying of its improvement along the development process.</p>
<p>Evaluation</p>
<p>Lastly, evaluation of reinforcement learning tools for creativity remains to be investigated more deeply.While our qualitative approach allowed us to harvest thoughtful user feedback on our prototypes' interaction modalities, it is still hard to account for direct links between agent computations and user creative goals.Using questionnaire methods, such as the Creativity Support Index [15], may enable to measure different dimensions of human creativity in relation to different algorithm implementations.Also, focusing on a specific user category could also allow more precise evaluation in relationship to a situated set of creative practices and uses.Alternatively, one could aim at developing new reinforcement learning criteria that extends standard measures-such as convergence or learning time [82]-to the qualitative case of human exploration.Research on interactive supervised learning has shown that criteria usually employed in the field of Machine Learning may not be adapted to users leading creative work [31].We believe that both HCI and ML approaches may be required and combined to produce sound scientific knowledge on creativity support evaluation.</p>
<p>Technical Insight</p>
<p>Computational Framework</p>
<p>Our two working prototypes confirmed that interactive reinforcement learning may stand as a generic technical framework for parameter space exploration.The computational framework that we proposed in Section 4.2.1, leveraging states, actions, and rewards, strongly characterized the mixed-initiative co-exploration workflows observed in Section 6.2-e.g., making small steps and continuous trajectories in the parameter space.Other interactive behaviours could have been implemented-e.g., allowing the agent to act on many parameters in only one action, or using different a i values for different action sizes-to allow for more diverse mixed-initiative behaviours.Alternatively, we envision that domain-specific representations may be a promising approach for extending co-exploration.In the case of sound design, one could engineer high-level state features based on audio descriptors [71] instead of using raw parameters.This could allow RL agents to learn state-action representations that would be independent from the parameter space explored-potentially allowing memory transfer from one parameter state space to another.This could also enable agent adaptation of action speed and precision based on perceptual features of the parameter space-potentially avoiding abrupt jumps in sound spaces.</p>
<p>Learning Algorithm</p>
<p>Reinforcement learning algorithmic functioning, enabling agents to learn actions over states, was of interest for our users, who were enthusiastic in teaching an artificial agent by feedback.Our deep reinforcement learning agent is a novel contribution to HCI research compared to multi-armed bandits (which explore actions over one unique state [56]), contextual bandits (which explore in lower-dimensional state spaces [52]), and bayesian optimization (which explores at implicit scales [75]).We purposely implemented heterogeneous ways of teaching with feedback based on our observations of users' approaches to parameter space exploration, which extends previous implementations such as those in the Drawing Apprentice [20].Yet, rich computational models of user feedback for exploration tasks remain a challenge.Our observations indeed suggested that exploring users may not generate a goal-oriented feedback signal, but may rather have several sub-optimal goals.They may also make feedback mistakes, act socially toward agents, or even try to trigger surprising agent behaviours over time.Deep TAMER was adapted to the interactive of user feedback (as opposed to Sarsa); yet, it still made the assumption that users will generate a stationary and always correct feedback signal [84].Previous works investigating how users give feedback to machine learning [80] may need to be extended to include such creative use cases.</p>
<p>Exploration Behaviours</p>
<p>The exploration behaviours of reinforcement learning agents were shown promising for fostering creativity in our users.Both ε-greedy and intrisic method were adapted to the interactive case of a user leading exploration.One of our users felt that intrisic motivation had agents behave better than random.Yet, users' perception of agent exploration behaviours remains to be investigated more deeply.In a complementary work [72], we confirmed that users perceived the difference between a random parameter exploration and a RL agent exploration.Yet, they might not perceive the difference between various implementations of agent exploration; what they perceive may be more related to the agent's global effect in exploring the parameter space.Future work may study co-exploration partnerships over longer periods of time to inquire co-adaptation between users and agents [58].On the one hand, users could be expected to learn to provide better feedback to RL agents to fulfill their creative goals-as it was shown in interactive approaches to supervised learning [31].On the other hand, agents could be expected to act more in line with users by exploiting larger amounts of accumulated feedback data-as it is typical with interactive reinforcement learning agents [82].A more pragmatic option would be to give users full control over agent epsilon values-e.g., using an interactive slider [52]-to improve partnership in this sense.</p>
<p>Guidelines for Designing With Machine Learning in Creative Applications</p>
<p>Based on our work with reinforcement learning, we identified a set of design challenges for leading joint conceptual and technical development of other machine learning frameworks for creative HCI applications.We purposely put back quotes from our participants in this section to inspire readers with insights on AI from users outside our design team.</p>
<p>Engage Users with Machine Learning</p>
<p>The Co-Explorer enabled users to fully engage with reinforcement learning computational framework.Users could explore as many states, provide as much feedback, and generate as many agent actions as they wanted to.They also had access to agent memory, be it by navigating in the interactive history, or by reseting the learned behaviour.In this sense, they had full control over the algorithmic learning process of the agent.This is well articulated by a participant, whose quote can be reported here: "I did not feel as being an adversary to, or manipulated, by the system.A situation that can happen with certain audio software that currently use machine learning, where it is clear that one tries to put you on a given path, which I find frustrating-but this was not the case here".</p>
<p>These observations suggest that user engagement at different levels of machine learning processes may be essential to create partnering flows [62].That is, users should be provided with interactive controls and simple information on learning to actively direct co-creation.This goes in line with previous works studying user interaction with supervised learning in creative tasks [2], which showed how users can build better partnerships by spending time engaging with algorithms [31].Careful interaction design must be considered to balance full automation with full user control and aim at creating flow states among people [19].Aiming at such user engagement may also constitute a design opportunity to demystify AI systems, notably by having users learn from experience how algorithms work with data [29].</p>
<p>Foster Diverse Creative Processes</p>
<p>Our work showed that the Co-Explorer supported a wide diversity of creative user processes.Users could get involved in open-ended, agent-led exploration, or decide to focus on precise, user-led parameter modification.Importantly, none of these partnerships were clearly conceptualized at the beginning of our development process.Our main focus was to build a reinforcement learning agent able to learn from user feedback and to be easily controllable by users.In this sense, the Co-Explorer was jointly designed and engineered to ensure a dynamic human process rather than a static media outcome.As a matter of fact, we report one participant's own reflection, which we believe illustrate our point: "What am I actually sampling [from the parameter space]?Is is some kind of climate that is going to direct my creation afterwards?[...] Or am I already creating?".</p>
<p>This suggests that supporting the process of user appropriation may be crucial for building creative AI partnerships.Many creative tools based on machine learning often focus on engineering one model to ensure high performance for a given task.While these tools may be useful for creative tasks that have a focus on high productivity, it is arguable whether they may be suited to creative work that has a focus on exploration as a way to build expression.For the latter case, creative AI development should not focus on one given user task, but should rather focus on providing users with a dynamic space for expression allowing many styles of creation [68].The massive training datasets, which are usually employed in the Machine Learning community to build computational creativity tools, may also convey representational and historical biases among end users [81].Interactive approaches to machine learning directly address this issue by allowing users to intervene in real-time in the learning process [30].</p>
<p>Steer Users Outside Comfort Zones</p>
<p>The Co-Explorer actively exposed the exploration behaviour of reinforcement learning to users.This goes in opposition with standard uses of these algorithms [12], and may provoke moments where agents behaviours may not align with users creative drive [18].Yet, it managed to build "playful" and "funny" partnerships that led some users to reconsider their approach to creativity, as one participant confessed: "At times, the agent forced me to try and hear sounds that I liked less-but at least, this allowed me to visit unusual spaces and imagine new possibilities.This, as a process that I barely perform in my own creative practice, eventually appeared as appealing to me".</p>
<p>This suggests that AI may be used beyond customisation aspects to steer users outside their comfort zones in a positive way.That is, designers should exploit non-optimal algorithmic behaviours in machine learning methods to surprise, obstruct, or even challenge users inside their creative process.Data-driven user adaptation may be taken from an opposite side to inspire users from radical opposition and avoid hyper-personalization [7].Such an anti-solutionist [11] approach to machine learning may encourage innovative developments that fundamentally reconsider the underlying notion of universal performance commonly at stake in the field of Machine Learning and arguably not adapted to the human users studied in the field of Human-Computer Interaction.It may also allow the building of imperfect AI colleagues, in opposion to "heroic" AI colleagues [24]: being impressed by the creative qualities of an abstract artificial entity may not be the best alternative to help people develop as creative thinkers [67].The Co-Explorer fairly leans toward such an unconventional design approach, which, in default of fitting every user, surely forms one of its distinctive characteristics.</p>
<p>Several machine learning frameworks remains to be investigated under the light of these human-centred challenges.Evolutionary computation methods [34] may be fertile ground for supporting user exploration and automated refinement of example designs.Active learning methods [74] may enable communication flows between agents and users that go beyond positive or negative feedback.Dimensionality reduction methods for interactive visualization [57] may improve intelligibility of agent actions in large parameter spaces and allow for more trustable partnerships.Ultimately, combining reinforcement learning with supervised learning could offer users with the best of both worlds by supporting both example and feedback inputs.Inverse reinforcement learning [1] may stand as a technical framework supporting example input projection and transformation into reward functions in a parameter space.</p>
<p>Conclusion</p>
<p>In this paper we presented the design of a deep reinforcement learning agent for human parameter space exploration.We worked in close relationship with professional creatives in the field of sound design and led two design iterations during our research process.A first pilot study let us observe users interacting with standard parametric interfaces, as well as with an initial interactive reinforcement learning prototype.The gathered user feedback informed the design of the Co-Explorer, our fully-functioning prototype, for which we led joint design and engineering for the specific task of parameter space exploration.A final workshop allowed us to observe a wide range of partnerships between users and agents, in tasks requiring both quantitative, media-related sampling and qualitative, creative insight.</p>
<p>Our results raised contributions at different levels of research, development, and design.We defined properties of user approaches to parameter space exploration within standard parametric interfaces, as well as to what we called parameter space co-exploration-exploring in cooperation with a reinforcement learning agent.We adapted a deep reinforcement learning algorithm to the specific case of parameter space exploration, developing specific computational methods for user feedback input in high-dimensional spaces, as well as a new algorithm for agent exploration based on intrisic motivation.We raised general design challenges for guiding the building of new human-AI partnerships, encouraging interdisciplinary research collaborations [64] that value human creativity over machine learning performance.We look forward to collaborating with researchers, developers, designers, artists, and users from other domains to take up the societal challenge of designing partnering AI tools that nurture human creativity.</p>
<p>Figure 1 :
1
Figure 1: A typical VST interface for sound design, containing many technical parameters.</p>
<ol>
<li>1
1
Part 1: Parametric Interfaces 4.1.1Procedure</li>
</ol>
<p>Figure 2 :
2
Figure 2: Schematic view of the three parametric interfaces.</p>
<p>Figure 3 :
3
Figure 3: Two user exploration strategies with a 12-dimensional parametric interface: Analytical (top) vs. spontaneous (bottom).</p>
<p>Figure 4 :
4
Figure 4: Our RL agent prototype.Users can only provide feedback to the agent, which acts on hidden VST parameters.</p>
<p>Figure 6 :
6
Figure 6: Co-Explorer workflow.</p>
<p>Figure 7 :
7
Figure 7: Schematic representations for exploration methods.The color scale depicts the density model all states.Left: Changing zone has the agent jump to the state with lowest density.Right: Autonomous exploration has the agent take successive actions leading to the state with lowest density.</p>
<p>Figure 9 :
9
Figure 9: Co-Explorer interface.</p>
<ol>
<li>1
1
Part 1: Explore to Discover 6.1.1Procedure</li>
</ol>
<p>Figure 10 :
10
Figure 10: Our participants testing the Co-Explorer in the evaluation workshop.</p>
<ol>
<li>
<p>2
2
Part 2: Explore to Create 6.2.1 Procedure</p>
</li>
<li>
<p>1
1
Conceptual Insight 7.1.1From Exploration to Co-Exploration</p>
</li>
</ol>
<p>Frequency Modulation synthesis (a classic algorithmic method for sound synthesis[16]).
https://cycling74.com/products/max/
https://forum.ircam.fr/projects/detail/mubu/
https://github.com/Ircam-RnD/coexplorer
AcknowledgementsWe are grateful to our participants for their precious time and feedback.We thank Benjamin Matuszewski, Jean-Philippe Lambert, and Adèle Pécout for their support in designing the studies.Appendix AThe TAMER[50]and Deep TAMER[84]algorithms can be seen as value-based algorithms.They have been applied in settings that allow to quickly learn a policy on episodic tasks (small game environments or physical models) and aim to maximise direct human reward.This opposed to the traditional RL training objective to maximise the discounted sum of future rewards.These algorithms learn the human reward function R using an artificial neural network and construct a policy from R taking greedy actions.In addition, to accommodate sparse and delayed rewards from larger user response times, the algorithms include a weighting function u(t) to past state trajectories and a replay memory in the case of Deep TAMER.Specifically, while traditional RL algorithms aim to optimise the Mean-Square Error (MSE) losswith R t the reward at time t, γ the discount rate, and q(S t , A t , w t ) the computed state-action value function with parameters w, (Deep) TAMER aims to optimisewith r(t f ) and u t (t f ) respectively the user-provided feedback and weighting function at time t f , and R(S t , A t ) the average reward.Appendix BDeep neural network[84]Agent [84] number of hidden layers = 2 state dimension n = 10 number of units per layer = 100
Apprenticeship learning via inverse reinforcement learning. Pieter Abbeel, Andrew Y Ng, Proceedings of the twenty-first international conference on Machine learning. the twenty-first international conference on Machine learningACM20041</p>
<p>Power to the people: The role of humans in interactive machine learning. Saleema Amershi, Maya Cakmak, William Bradley Knox, Todd Kulesza, AI Magazine. 352014. 2014</p>
<p>Modeltracker: Redesigning performance analysis tools for machine learning. Saleema Amershi, Max Chickering, Steven M Drucker, Bongshin Lee, Patrice Simard, Jina Suh, Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. the 33rd Annual ACM Conference on Human Factors in Computing SystemsACM2015</p>
<p>Regroup: Interactive machine learning for on-demand group creation in social networks. Saleema Amershi, James Fogarty, Daniel Weld, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsACM2012</p>
<p>CueT: humanguided fast and accurate network alarm triage. Saleema Amershi, Bongshin Lee, Ashish Kapoor, Ratul Mahajan, Blaine Christian, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsACM2011</p>
<p>Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, Guidelines for Human-AI Interaction. 2019. 2019</p>
<p>Conversations with Expert Users in Music Retrieval and Research Challenges for Creative MIR. Kristina Andersen, Peter Knees, ISMIR. 122-1282016</p>
<p>Beyond relevance: Adapting exploration/exploitation in information retrieval. Kumaripaba Athukorala, Alan Medlar, Antti Oulasvirta, Giulio Jacucci, Dorota Glowacka, Proceedings of the 21st International Conference on Intelligent User Interfaces. the 21st International Conference on Intelligent User InterfacesACM2016a</p>
<p>Beyond Relevance: Adapting Exploration/Exploitation in Information Retrieval. Kumaripaba Athukorala, Alan Medlar, Antti Oulasvirta, Giulio Jacucci, Dorota Glowacka, 10.1145/2856767.2856786Proceedings of the 21st International Conference on Intelligent User Interfaces (IUI '16). the 21st International Conference on Intelligent User Interfaces (IUI '16)New York, NY, USAACM2016b</p>
<p>Unifying count-based exploration and intrinsic motivation. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos, Advances in Neural Information Processing Systems. 2016</p>
<p>Anti-Solutionist Strategies: Seriously Silly Design Fiction. Mark Blythe, Kristina Andersen, Rachel Clarke, Peter Wright, Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. the 2016 CHI Conference on Human Factors in Computing SystemsACM2016</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.01540Openai gym. 2016. 2016arXiv preprint</p>
<p>Baptiste Caramiaux, Fabien Lotte, Joost Geurts, Giuseppe Amato, Malte Behrmann, Frédéric Bimbot, Fabrizio Falchi, Ander Garcia, Jaume Gibert, Guillaume Gravier, AI in the media and creative industries. 2019. 2019</p>
<p>Mixploration: Rethinking the audio mixer interface. Mark Cartwright, Bryan Pardo, Josh Reiss, Proceedings of the 19th international conference on Intelligent User Interfaces. the 19th international conference on Intelligent User InterfacesACM2014</p>
<p>Quantifying the creativity support of digital tools through the creativity support index. Erin Cherry, Celine Latulipe, ACM Transactions on Computer-Human Interaction (TOCHI). 21212014. 2014</p>
<p>The synthesis of complex audio spectra by means of frequency modulation. Chowning John, Journal of the audio engineering society. 211973. 1973</p>
<p>Paul Christiano, Jan Leike, Miljan Tom B Brown, Shane Martic, Dario Legg, Amodei, arXiv:1706.03741Deep reinforcement learning from human preferences. 2017. 2017arXiv preprint</p>
<p>Cooperating with machines. Mayada Jacob W Crandall, Fatimah Oudah, Sherief Ishowo-Oloko, Jean-François Abdallah, Manuel Bonnefon, Azim Cebrian, Michael A Shariff, Iyad Goodrich, Rahwan, Nature communications. 92332018. 2018</p>
<p>Flow and the psychology of discovery and invention. Mihaly Csikszentmihalyi, 1997. 1997HarperPerennial39New York</p>
<p>Empirically studying participatory sense-making in abstract drawing with a co-creative cognitive agent. Nicholas Davis, Chih-Pin Hsiao, Yashraj Kunwar, Lisa Singh, Brian Li, Magerko, Proceedings of the 21st International Conference on Intelligent User Interfaces. the 21st International Conference on Intelligent User InterfacesACM2016</p>
<p>Yanna Nicholas M Davis, Ivan Popova, Chih-Pin Sysoev, Dingtian Hsiao, Brian Zhang, Magerko, Building Artistic Computer Colleagues with an Enactive Model of Creativity. </p>
<p>Embodied Sound Design. Stefano Delle Monache, Davide Rocchesso, Frédéric Bevilacqua, Guillaume Lemaitre, Stefano Baldan, Andrea Cera, International Journal of Human-Computer Studies. 2018. 2018</p>
<p>Mixed-Initiative Creative Interfaces. Christoph Sebastian Deterding, Jonathan David Hook, Rebecca Fiebrink, Jeremy Gow, Memo Akten, Gillian Smith, Antonios Liapis, Kate Compton, CHI EA'17: Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems. ACM2017</p>
<p>Jon Mark D'inverno, Mccormack, Heroic versus Collaborative AI for the Arts. 2015. 2015</p>
<p>Designing for appropriation. Alan Dix, Proceedings of the 21st British HCI Group Annual Conference on People and Computers: HCI... but not as we know it. the 21st British HCI Group Annual Conference on People and Computers: HCI... but not as we know itBritish Computer Society20072</p>
<p>Creativity in the design process: co-evolution of problem-solution. Kees Dorst, Nigel Cross, Design studies. 222001. 2001</p>
<p>UX Design Innovation: Challenges for Working with Machine Learning as a Design Material. Graham Dove, Kim Halskov, Jodi Forlizzi, John Zimmerman, Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. the 2017 CHI Conference on Human Factors in Computing SystemsACM2017</p>
<p>Interactive machine learning. Jerry , Alan Fails, Dan R OlsenJr, Proceedings of the 8th international conference on Intelligent user interfaces. the 8th international conference on Intelligent user interfacesACM2003</p>
<p>Machine Learning Education for Artists, Musicians, and Other Creative Practitioners. Rebecca Fiebrink, ACM Transactions on Computing Education. 2019. 2019</p>
<p>The machine learning algorithm as creative musical tool. Rebecca Fiebrink, Baptiste Caramiaux, 2016. 2016Handbook of Algorithmic Music</p>
<p>Human Model Evaluation in Interactive Supervised Learning. Rebecca Fiebrink, Perry R Cook, Dan Trueman, 10.1145/1978942.1978965Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). the SIGCHI Conference on Human Factors in Computing Systems (CHI '11)New York, NY, USAACM2011</p>
<p>Toward Understanding Human-Computer Interaction In Composing The Instrument. Rebecca Fiebrink, Daniel Trueman, Cameron Britt, Michelle Nagai, Konrad Kaczmarek, Michael Early, Anne Daniel, Perry R Hege, Cook, ICMC. 2010</p>
<p>. Tesca Fitzgerald, Ashok Goel, Andrea Thomaz, n. d.</p>
<p>Human-Robot Co-Creativity: Task Transfer on a Spectrum of Similarity. </p>
<p>Evolutionary computation: toward a new philosophy of machine intelligence. B David, Fogel, 2006John Wiley &amp; Sons1</p>
<p>Motion-Sound Mapping through Interaction: An Approach to User-Centered Design of Auditory Feedback Using Machine Learning. Jules Francoise, Frederic Bevilacqua, ACM Transactions on Interactive Intelligent Systems (TiiS). 8162018. 2018</p>
<p>Adaptive training environment without prior knowledge: Modeling feedback selection as a multi-armed bandit problem. Rémy Frenoy, Yann Soullard, Indira Thouvenin, Olivier Gapenne, Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization. the 2016 Conference on User Modeling Adaptation and PersonalizationACM2016</p>
<p>Interactive paper substrates to support musical creation. Jérémie Garcia, Theophanis Tsandilas, Carlos Agon, Wendy Mackay, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsACM2012</p>
<p>Understanding the Role of Interactive Machine Learning in Movement Interaction Design. Marco Gillies, ACM Transactions on Computer-Human Interaction (TOCHI). 2652019. 2019</p>
<p>Human-centred machine learning. Marco Gillies, Rebecca Fiebrink, Atau Tanaka, Jérémie Garcia, Frederic Bevilacqua, Alexis Heloir, Fabrizio Nunnari, Wendy Mackay, Saleema Amershi, Bongshin Lee, Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems. the 2016 CHI Conference Extended Abstracts on Human Factors in Computing SystemsACM2016</p>
<p>Directing Exploratory Search: Reinforcement Learning from User Interactions with Keywords. Dorota Glowacka, Tuukka Ruotsalo, Ksenia Konuyshkova, Samuel Athukorala, Giulio Kaski, Jacucci, 10.1145/2449396.2449413Proceedings of the 2013 International Conference on Intelligent User Interfaces (IUI '13). the 2013 International Conference on Intelligent User Interfaces (IUI '13)New York, NY, USAACM2013</p>
<p>Creative foraging: An experimental paradigm for studying exploration and discovery. Yuval Hart, Avraham E Mayo, Ruth Mayo, Liron Rozenkrantz, Avichai Tendler, Uri Alon, Lior Noy, PloS one. 12e01821332017. 2017</p>
<p>Principles of mixed-initiative user interfaces. Eric Horvitz, Proceedings of the SIGCHI conference on Human Factors in Computing Systems. the SIGCHI conference on Human Factors in Computing SystemsACM1999</p>
<p>Technology probes: inspiring design for and with families. Hilary Hutchinson, Wendy Mackay, Bo Westerlund, Allison Benjamin B Bederson, Catherine Druin, Michel Plaisant, Stéphane Beaudouin-Lafon, Helen Conversy, Heiko Evans, Hansen, Proceedings of the SIGCHI conference on Human factors in computing systems. the SIGCHI conference on Human factors in computing systemsACM2003</p>
<p>Principal component analysis. Ian Jolliffe, International encyclopedia of statistical science. Springer2011</p>
<p>Digital Lutherie Crafting musical computers for new musics' performance and improvisation. Sergi Jorda, 2005Universitat Pompeu FabraPh.D. Dissertation</p>
<p>From Isolation to Involvement: Adapting Machine Creativity Software to Support Human-Computer Co-Creation. Anna Kantosalo, Ping Jukka M Toivanen, Hannu Xiao, Toivonen, ICCC. 1-72014</p>
<p>Interactive optimization for steering machine classification. Ashish Kapoor, Bongshin Lee, Desney Tan, Eric Horvitz, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing Systems2010</p>
<p>Using interactive machine learning to support interface development through workshops with disabled people. Simon Katan, Mick Grierson, Rebecca Fiebrink, Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. the 33rd Annual ACM Conference on Human Factors in Computing SystemsACM2015</p>
<p>Customizing by doing for responsive video game characters. Andrea Kleinsmith, Marco Gillies, International Journal of Human-Computer Studies. 712013. 2013</p>
<p>Interactively shaping agents via human reinforcement: The TAMER framework. Knox Bradley, Peter Stone, Proceedings of the fifth international conference on Knowledge capture. the fifth international conference on Knowledge captureACM2009</p>
<p>Design implications for Designing with a Collaborative AI. Janin Koch, 2017. 2017</p>
<p>May AI?: Design Ideation with Cooperative Contextual Bandits. Janin Koch, Andrés Lucero, Lena Hegemann, Antti Oulasvirta, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. the 2019 CHI Conference on Human Factors in Computing SystemsACM2019633</p>
<p>Group Cognition and Collaborative AI. Janin Koch, Antti Oulasvirta, Human and Machine Learning. Springer2018</p>
<p>Bricolage: example-based retargeting for web design. Ranjitha Kumar, Jerry O Talton, Salman Ahmad, Scott R Klemmer, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsACM2011</p>
<p>Yuxi Li, arXiv:1810.06339Deep reinforcement learning. 2018. 2018arXiv preprint</p>
<p>Interface design optimization as a multi-armed bandit problem. Derek Lomas, Jodi Forlizzi, Nikhil Poonwala, Nirmal Patel, Sharan Shodhan, Kishan Patel, Ken Koedinger, Emma Brunskill, Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. the 2016 CHI Conference on Human Factors in Computing SystemsACM2016</p>
<p>Visualizing data using t-SNE. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 92008. 2008</p>
<p>Users and customizable software: A co-adaptive phenomenon. Wendy E Mackay, 1990Ph.D. Dissertation. Citeseer</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Nature. 5185292015. 2015</p>
<p>A toolkit for explorations in sonic interaction design. Stefano Delle Monache, Pietro Polotti, Davide Rocchesso, Proceedings of the 5th audio mostly conference: a conference on interaction with sound. the 5th audio mostly conference: a conference on interaction with soundACM20101</p>
<p>Reinforcement learning in the brain. Yael Niv, Journal of Mathematical Psychology. 532009. 2009</p>
<p>Reflexive loopers for solo musical improvisation. François Pachet, Pierre Roy, Julian Moreira, Mark D' Inverno, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsACM2013</p>
<p>Using multiple models to understand data. Kayur Patel, Steven M Drucker, James Fogarty, Ashish Kapoor, Desney S Tan, IJCAI Proceedings-International Joint Conference on Artificial Intelligence. 2011. 172322</p>
<p>Twenty Years of Creativity Research in Human-Computer Interaction: Current State and Future Directions. Jonas Frich Pedersen, Michael Mose Biskjaer, Peter Dalsgaard, Designing Interactive Systems. ACM2018</p>
<p>Describing one's subjective experience in the second person: An interview method for the science of consciousness. Claire Petitmengin, Phenomenology and the Cognitive sciences. 52006. 2006</p>
<p>Inline Co-Evolution between Users and Information Presentation for Data Exploration. Landy Rajaonarivo, Matthieu Courgeon, Eric Maisel, Pierre De, Loor , Proceedings of the 22nd International Conference on Intelligent User Interfaces. the 22nd International Conference on Intelligent User InterfacesACM2017</p>
<p>All I really need to know (about creative thinking) I learned (by studying how children learn) in kindergarten. Mitchel Resnick, Proceedings of the 6th ACM SIGCHI conference on Creativity &amp; cognition. the 6th ACM SIGCHI conference on Creativity &amp; cognitionACM2007</p>
<p>Design principles for tools to support creative thinking. Mitchel Resnick, Brad Myers, Kumiyo Nakakoji, Ben Shneiderman, Randy Pausch, Ted Selker, Mike Eisenberg, 2005. 2005Working Paper</p>
<p>On the Planning Crisis: Systems Analysis of the "First and Second Generations. W J Horst, Rittel, 1972Institute of Urban and Regional Development</p>
<p>Interactive Intent Modeling: Information Discovery Beyond Search. Tuukka Ruotsalo, Giulio Jacucci, Petri Myllymäki, Samuel Kaski, 10.1145/2656334Commun. ACM. 582014. Dec. 2014</p>
<p>Sound search by content-based navigation in large databases. Diemo Schwarz, Norbert Schnell, Sound and Music Computing (SMC). 2009</p>
<p>Perceiving Agent Collaborative Sonic Exploration In Interactive Reinforcement Learning. Hugo Scurto, Frédéric Bevilacqua, Baptiste Caramiaux, Proceedings of the 15th Sound and Music Computing Conference. the 15th Sound and Music Computing Conference2018. 2018</p>
<p>Grab-and-play mapping: Creative machine learning approaches for musical inclusion and exploration. Hugo Scurto, Rebecca Fiebrink, Proceedings of the 2016 International Computer Music Conference. the 2016 International Computer Music Conference2016</p>
<p>Active learning literature survey. Burr Settles, Madison. 52112010. 2010University of Wisconsin</p>
<p>Taking the human out of the loop: A review of bayesian optimization. Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, Nando De Freitas, Proc. IEEE. 1042016. 2016</p>
<p>CueTIP: a mixed-initiative interface for correcting handwriting errors. Michael Shilman, Patrice Desney S Tan, Simard, Proceedings of the 19th annual ACM symposium on User interface software and technology. the 19th annual ACM symposium on User interface software and technology2006</p>
<p>Creativity support tools: Accelerating discovery and innovation. Ben Shneiderman, Commun. ACM. 502007. 2007</p>
<p>Mastering the game of Go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 5294842016. 2016</p>
<p>Toward harnessing user feedback for machine learning. Simone Stumpf, Vidya Rajaram, Lida Li, Margaret Burnett, Thomas Dietterich, Erin Sullivan, Russell Drummond, Jonathan Herlocker, Proceedings of the 12th international conference on Intelligent user interfaces. the 12th international conference on Intelligent user interfacesACM2007</p>
<p>Interacting meaningfully with machine learning systems: Three experiments. Simone Stumpf, Vidya Rajaram, Lida Li, Weng-Keen Wong, Margaret Burnett, Thomas Dietterich, Erin Sullivan, Jonathan Herlocker, International Journal of Human-Computer Studies. 672009. 2009</p>
<p>Harini Suresh, John V Guttag, arXiv:1901.10002A Framework for Understanding Unintended Consequences of Machine Learning. 2019. 2019arXiv preprint</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2011MIT PressCambridge, MA</p>
<p>Teachable robots: Understanding human teaching behavior to build more effective robot learners. L Andrea, Cynthia Thomaz, Breazeal, Artificial Intelligence. 1722008. 2008</p>
<p>Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, Peter Stone, arXiv:1709.10163Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces. 2017. 2017arXiv preprint</p>
<p>A preliminary framework for description, analysis and comparison of creative systems. A Geraint, Wiggins, Knowledge-Based Systems. 192006. 2006</p>
<p>End-user feature labeling: A locally-weighted regression approach. Weng-Keen Wong, Ian Oberst, Shubhomoy Das, Travis Moore, Simone Stumpf, Kevin Mcintosh, Margaret Burnett, Proceedings of the 16th international conference on Intelligent user interfaces. the 16th international conference on Intelligent user interfaces2011</p>
<p>Open Sound Control: an enabling technology for musical networking. Matthew Wright, Organised Sound. 102005. 2005</p>
<p>Mapping Machine Learning Advances from HCI Research to Reveal Starting Places for Design Innovation. Qian Yang, Nikola Banovic, John Zimmerman, Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. the 2018 CHI Conference on Human Factors in Computing SystemsACM2018a130</p>
<p>Investigating How Experienced UX Designers Effectively Work with Machine Learning. Qian Yang, Alex Scuito, John Zimmerman, Jodi Forlizzi, Aaron Steinfeld, Proceedings of the 2018 Designing Interactive Systems Conference. the 2018 Designing Interactive Systems ConferenceACM2018b</p>
<p>Antonios Liapis, and Constantine Alexopoulos. N Georgios, Yannakakis, 2014Mixed-initiative co-creativity.. In FDG</p>
<p>Fluid gesture interaction design: Applications of continuous recognition for the design of modern gestural interfaces. Frederic Bruno Zamborlin, Marco Bevilacqua, Mark D' Gillies, Inverno, ACM Transactions on Interactive Intelligent Systems (TiiS). 3222014. 2014</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2454 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2454</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2454</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-248386306</p>
                <p><strong>Paper Title:</strong> Accelerating materials discovery using artificial intelligence, high performance computing and robotics</p>
                <p><strong>Paper Abstract:</strong> New tools enable new ways of working, and materials science is no exception. In materials discovery, traditional manual, serial, and human-intensive work is being augmented by automated, parallel, and iterative processes driven by Artificial Intelligence (AI), simulation and experimental automation. In this perspective, we describe how these new capabilities enable the acceleration and enrichment of each stage of the discovery cycle. We show, using the example of the development of a novel chemically amplified photoresist, how these technologies’ impacts are amplified when they are used in concert with each other as powerful, heterogeneous workflows.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2454.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2454.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IBM Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cloud-exposed Bayesian optimization API suite used to prioritize computational simulations and experiments by optimizing acquisition functions and collecting batched evaluations to spend computational budget efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>IBM Bayesian Optimization (IBO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A cloud-deployed Bayesian optimization system exposed via simple APIs that orchestrates surrogate models and acquisition functions to prioritize which candidates to simulate or test next. IBO supports batch selection (parallel evaluation) and can be configured with different batch acquisition strategies (e.g., PDTS sampling). In the exemplarb PAG workflow IBO was used with ECFP molecular descriptors to select batches of 10 TD-DFT simulations in parallel, optimizing a scoring metric combining target excitation energy distance and oscillator strength.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials discovery / molecular photoacid generator (PAG) design (general: materials and molecular discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select candidates by optimizing a Bayesian acquisition function; allocate computational budget to the highest-acquisition candidates in batches (configured here as batches of 10). Uses surrogate (probabilistic) models over candidate space and ranks candidates by expected utility for simulation allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Implicitly measured as number of simulations run and parallel batch size (wall-clock time through parallel batches); in practice measured by number of TD-DFT simulations executed and parallel core/time usage.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition function from Bayesian optimization (implemented via sampling/Thompson-sampling based batch selection here) estimating expected utility of acquiring each data point (not expressed as an explicit mutual information formula in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Bayesian optimization acquisition function mechanism balances exploration and exploitation; in the applied configuration PDTS (posterior sampling) introduces stochastic exploration while focusing on high-posterior regions (exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Batch selection and descriptor-based representation (ECFP) produce chemical diversity implicitly; when combined with specific batch algorithms (e.g., K-means batch BO) explicit partitioning can promote diverse batch picks (K-means is supported by IBO but PDTS was used in the reported experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed computational budget expressed as number of simulations and available parallel cores (batch size/time constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Optimize acquisition function under the available batch-parallel resources; use batch selection algorithms (PDTS, K-means batch) to maximize expected utility per batch and spend higher-fidelity simulations on fewer, higher-value candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Problem-specific performance score S = f_obs * |E_obs - E_target|^{-1} (paper uses S = f_obs / |E_obs - E_target| equivalent formulation) combining closeness to target excitation energy and oscillator strength; high S indicates candidate likely to meet optical requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>For a library of >400 PAG candidates IBO configured with PDTS (batch size 10) found the highest-performing 193 nm candidate after testing ~90 molecules (best IBO run) versus the non-accelerated workflow which typically required testing ~half the library (~200 molecules) on average; ECFP descriptors were used to represent molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Non-accelerated virtual screening (traditional/naïve screening) / exhaustive or uniform screening baseline shown in the paper (also plotted against an ideal upper bound).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>IBO (PDTS batch) located the best candidate after ~90 simulations vs ~200 simulations for the non-accelerated workflow (for >400 candidates), i.e., found the optimum in ~45% of the evaluations required by the baseline on average in the presented example.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported practical speedup: in the PAG example the best-performing candidate was found after testing 90 molecules with IBO vs ~200 for the non-accelerated baseline (approx. 2.2x fewer simulations to find the optimum). Separately, the overall workflow reported a ~100-fold reduction in human expert review burden through generative + filtering steps (not purely IBO).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper qualitatively discusses the tradeoff: use Bayesian optimization to concentrate computational budget on higher-value regions so that more accurate (more expensive) models can be used on fewer data points; explicit quantitative tradeoff curves are shown for the PAG example comparing accelerated vs non-accelerated workflows, but no general closed-form cost-information tradeoff analysis is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: use Bayesian optimization (including batch strategies like PDTS and K-means batch) to allocate simulations under limited computational budgets; combine descriptor choices (fast-to-compute descriptors like ECFP) with acquisition-based selection to efficiently identify high-performing candidates while reducing total simulation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating materials discovery using artificial intelligence, high performance computing and robotics', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2454.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2454.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel and Distributed Thompson Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch Bayesian optimization algorithm that parallelizes exploration by drawing posterior samples (Thompson samples) from the surrogate model and selecting batches of candidates for simultaneous evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Parallel Distributed Thompson Sampling (PDTS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PDTS parallelizes Thompson sampling by drawing multiple posterior function samples from a probabilistic surrogate (e.g., GP or Bayesian neural net) and selecting high-ranked candidates from each sample to form a batch; this enables scalable batch acquisition for expensive evaluations while preserving the stochastic exploration properties of Thompson sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Large-scale chemical and materials discovery (batch selection for computationally expensive simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate batch evaluation slots by sampling from the posterior and selecting candidates that maximize sampled objectives—allocates parallel compute to points suggested by different posterior realizations to diversify search while targeting high-value regions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of parallel evaluations (batch size), wall-clock time under parallel execution, and number of expensive simulations performed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicitly uses posterior sampling as a surrogate for expected utility (Thompson sampling approximates probability of being optimal); not cast as explicit mutual information in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Stochastic posterior sampling yields exploration (different posterior samples may select diverse candidates) while naturally exploiting regions with high posterior mean/high reward; batch diversity comes from independent samples.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity arises from independent posterior samples selecting different modal candidates; no explicit clustering but tends to provide spread in the batch due to sampling variability.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed parallel batch resources and total number of evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Uses batch size and number of iterations as knobs; selects the most valuable batch members per iteration under the parallel evaluation constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Algorithm-agnostic: optimized against the task-specific objective (e.g., the S score used in the PAG experiment); breakthroughs correspond to candidates with top objective values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In the PAG example PDTS with batch size 10 was the configured acquisition method in IBO and led to discovering the top candidate after ~90 simulations (for >400 candidates).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared operationally to non-accelerated workflows; conceptually compared to other batch BO schemes (e.g., K-means batch BO).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>PDTS-equipped IBO outperformed non-accelerated screening in the PAG example by locating the top candidate after fewer simulations (~90 vs ~200).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Provides multi-fold reductions in required expensive evaluations in demonstrated chemical discovery tasks (example: roughly 2x reduction in evaluated candidates to find optimum in the PAG example).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses that PDTS parallelizes acquisition and reduces wall-clock time while maintaining a balance between exploration and exploitation; explicit quantitative tradeoff curves are provided for specific experiments but no general optimality proof in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>PDTS is effective for allocating parallel compute in batch active search and should be used where parallel expensive evaluations are available to accelerate exploration while preserving probabilistic exploration behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating materials discovery using artificial intelligence, high performance computing and robotics', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2454.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2454.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KMeans-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-means Batch Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch Bayesian optimization technique that partitions acquisition function space via K-means clustering to select diverse batches of candidates for expensive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient and Scalable Batch Bayesian Optimization Using K-Means.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>K-means Batch Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This method partitions candidate space or the acquisition function landscape using K-means clustering and then selects representative, high-acquisition candidates from each cluster to form a batch; the clustering step promotes diversity in each batch while retaining high acquisition values within clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch selection for computationally expensive optimization problems in chemistry and materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate batch evaluation slots by clustering the acquisition landscape and selecting top candidates per cluster, thereby allocating evaluations across diverse regions of candidate space rather than concentrating all resources in one mode.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of selected batch evaluations and associated simulation wall-clock time; additional small clustering overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition function values used to rank points within clusters; implicit expected utility through the acquisition function but not an explicit mutual information calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration is promoted by cluster-based spread across the acquisition landscape; exploitation is achieved by selecting high-acquisition points within each cluster.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: K-means clustering enforces spatial diversity across candidate feature/descriptor space so the batch covers multiple regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of parallel evaluations (batch size) and overall evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Divides batch budget across clusters to ensure sampling of multiple candidate regions per batch, improving chance of discovering diverse promising candidates under a tight batch budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Dependent on task-specific acquisition/objective; method improves chances of sampling novel high-performing regions that might contain breakthroughs by ensuring batch diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited as an effective scalable batch BO strategy; not directly run in the PAG experiment (PDTS was used there).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to naive batch selection (top-k acquisition without diversity) and other batch BO strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported in cited literature to produce more diverse and effective batches under large-scale settings; specific numeric gains not presented in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Aims to increase the probability of finding high-performing candidates per batch under fixed budget by ensuring coverage; efficiency gains depend on problem structure (cited as scalable and effective).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Method explicitly addresses the exploration-exploitation/diversity tradeoff by dividing budget across clusters; the perspective notes the approach as a deployed option for chemical discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Partitioning acquisition space with clustering is a practical strategy to allocate limited parallel evaluation budget to improve diversity and discovery probability in batch acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating materials discovery using artificial intelligence, high performance computing and robotics', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2454.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2454.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PairwiseTaskSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise Task Similarity Recommender</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recommender approach that uses pairwise task similarity measures from joint training effects to suggest simulation or modeling methodologies for low-data tasks using knowledge from high-data tasks, balancing cost and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Powerful, transferable representations for molecules through intelligent task selection in deep multitask networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pairwise Task Similarity Recommender</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A method that computes similarity between tasks by observing the sign and magnitude of transfer when training tasks jointly; similar tasks produce additive positive transfer, enabling recommendation of methodologies (e.g., cheaper ML models) for low-data tasks by leveraging related high-data tasks. Used as a recommender for selecting simulation/analysis protocols based on anticipated performance and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Selection of simulation and modeling methodologies in materials and molecular property prediction (generalizable across computational tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate modeling effort and choose simulation protocols by recommending methods that historically transfer well from related tasks—thereby reducing unnecessary expensive computations for low-data tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Modeling/simulation cost (training time, need for data, computational expense of protocols); paper frames cost qualitatively rather than as a precise numeric metric.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit—expected improvement in predictive accuracy via transfer learning; similarity measure derived from joint-training outcomes approximates expected utility of method transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploit known related tasks to recommend low-cost methods for new tasks (exploitation of transfer), while exploration of method space can be guided by uncertainty in transfer estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not primarily a diversity mechanism; focuses on transferability and cost-accuracy tradeoffs rather than enforcing hypothesis diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited computational/data budget for new tasks (low-data regimes).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Recommend lower-cost methods for low-data tasks using similarity with high-data tasks so that computational resources are used where they are most effective.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not explicitly defined for breakthroughs; objective is to maximize predictive utility given resource constraints which indirectly supports discovery by improving model accuracy where it matters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports that pairwise similarity provides chemically plausible similarity measures and improves recommendation quality across tasks (quantitative metrics in cited works rather than in this perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to naive selection of methods without transfer-aware recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Described as improving method selection in low-data tasks by leveraging transfer; no specific numeric comparisons in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reduces unnecessary high-cost simulations or models on low-data tasks by suggesting lower-cost effective alternatives via transfer; exact gains depend on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Positions the approach as balancing cost vs accuracy using transfer signals; recommends intelligent task selection to allocate limited computational budget efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Insight: when data is sparse for a new task, use pairwise task similarity (transferability) to recommend cheaper methods trained on related high-data tasks so that limited resources produce maximal predictive value.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating materials discovery using artificial intelligence, high performance computing and robotics', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2454.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2454.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Δ-ML GP Calibration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delta-machine learning with Gaussian process calibration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A delta-machine learning approach that learns the systematic difference between simulation outputs and experimental results using a Gaussian process with uncertainty estimates to calibrate simulation predictions per candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Big data meets quantum chemistry approximations: the Δ-machine learning approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Δ-machine learning (delta-ML) with Gaussian Process calibration</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A methodology where a machine-learned model (the delta model) is trained to predict the discrepancy between simulation outputs and experimental measurements; the authors use a Gaussian process trained on circular fingerprints (ECFP-style) to provide per-candidate corrections together with Bayesian uncertainty estimates, informing trust in simulation outputs and guiding allocation of experimental/simulation resources.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bridging simulation outputs and experimental observations across materials and molecular discovery tasks (calibration of virtual screening results).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Use uncertainty-aware calibration to decide which simulation outputs are reliable and where to allocate experimental verification resources; prioritize candidates with high corrected predicted performance and acceptable uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Training and inference costs for GP models (scales with training set size), plus cost of simulations and experiments that the calibration aims to reduce.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Model posterior uncertainty from the Gaussian process used as an information/uncertainty measure; expected reduction in predictive uncertainty guides allocation implicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploit simulation outputs where calibrated corrections are confident; explore candidates where uncertainty is high to improve calibration (active selection of calibration experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not an explicit diversity-promoting algorithm; diversity can be induced via active selection of high-uncertainty candidates across feature space.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited experimental verification budget and computational budget for high-fidelity simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Prioritize experiments/simulations where calibration uncertainty is high or where potential utility (after correction) is large; use GP uncertainty to avoid overcommitting resources to low-confidence corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Candidates with high corrected predicted performance and sufficiently low uncertainty are prioritized as likely breakthroughs; no universal novelty metric is specified.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited as enabling more robust selection of simulation methods and improved trust when making design decisions; quantitative results are in cited calibration literature rather than this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to uncalibrated simulation outputs which can mislead selection decisions; GP calibration reduces risk of overfitting and misprioritization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Calibration improves alignment of simulation predictions with experiment and reduces risk in decision-making; specific numeric improvements are provided in referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>By improving per-candidate trust estimates, calibration reduces wasted experimental effort on poorly predicted candidates; exact gains depend on application.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes that uncertainty-aware calibration allows using cheaper simulations more aggressively while keeping trust via calibrated corrections—this is a practical tradeoff between computational cost and predictive fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use delta-ML with Bayesian uncertainty (GP) to decide when to trust simulations and when to spend resources on experiments or higher-fidelity simulations; integrate uncertainty into allocation decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating materials discovery using artificial intelligence, high performance computing and robotics', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2454.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2454.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EITL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert-in-the-loop (EITL) discriminative filtering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human–AI hybrid approach where generative model outputs are constrained by expert-defined rules and filtered using discriminative AI models with human oversight to reduce candidate sets and focus resources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expert-in-the-loop (EITL) constrained generative downselection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generative models produce large numbers of candidate molecules; EITL applies domain expert rules and discriminative (predictive) AI models to constrain and rank candidates before expensive simulations or experiments. This reduces the human and computational burden by narrowing candidates to a manageable subset that meets application-specific constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular generative design and materials ideation (demonstrated for sulfonium PAG cation design).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate human review and expensive simulation/experimental effort only to candidates that pass expert rules and discriminative model filters, thereby conserving computational and human resources for the most promising hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Count of candidates requiring simulation/experimental evaluation and human expert review time; cost of running discriminative filters (typically cheap compared to high-fidelity simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not formalized as an information-theoretic metric; selection is driven by rule satisfaction and discriminative model scores indicating predicted feasibility/safety/performance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploit domain knowledge to focus on plausible candidates (exploitation); exploration is limited by the constraints but generative models still explore structural diversity prior to filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity primarily comes from the generative model; EITL applies constraints that may reduce diversity — the human expert tunes rules to balance novelty vs practicality.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Human time and simulation/experimental budget (practical limits on number of candidates reviewed/tested).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Massively reduce candidate pool via rule-based filters and discriminative models to fit the available human and computational budget; result in ~100-fold reduction in human review burden in the PAG use case.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Final human expert selection combined with discriminative model scores and downstream simulation/experimental validation; breakthroughs identified by meeting application-specific optical and sustainability constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Workflow reduced ~3000 generated candidates to a few hundred by rule- and model-based filtering, producing a ~100-fold reduction in number of candidates the expert needed to analyze before final selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to raw generative model outputs with manual inspection of large sets (no constrained filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>EITL reduced the human review workload by approximately two orders of magnitude compared with inspecting all generative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Approximately 100x reduction in candidate set size requiring expert analysis in the demonstrated PAG workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Acknowledges tradeoff: aggressive constraints simplify downstream resource needs but risk excluding novel candidates; recommends human-in-the-loop tuning to balance novelty and practicality.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Pragmatic recommendation: use expert-defined constraints plus discriminative models to pare generative outputs to a budget-compatible set for simulation and experimental validation, accepting a controlled reduction in exploratory breadth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating materials discovery using artificial intelligence, high performance computing and robotics', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2454.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2454.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayes-AL Closed-Loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian active learning closed-loop materials discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An on-the-fly closed-loop active learning framework applying Bayesian methods to select next experiments/measurements to maximize learning per experimental cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On-the-fly closed-loop materials discovery via Bayesian active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian active learning closed-loop</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A closed-loop framework where a Bayesian surrogate model is updated with each new experimental/simulation result and an acquisition function selects the next experiments to perform to maximize information gain or objective improvement under cost constraints; designed for on-the-fly operation with experimental hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials discovery, experimental design, and autonomous laboratories.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Iteratively allocate experimental runs by maximizing an acquisition function (e.g., expected improvement, information gain, or uncertainty reduction) per unit experimental cost, integrating experimental feedback in real time.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of experiments (or simulations), real-time experimental wall-clock, and lab resource usage; cost-per-experiment can be incorporated into acquisition function.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition functions based on expected improvement, uncertainty reduction, or other Bayesian utility measures; design aims to directly maximize information gain per cost.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition-function-driven balance (e.g., EI/UCB/Thompson sampling) that trades off sampling in high-uncertainty regions (exploration) vs high-mean/expected-improvement regions (exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Batch acquisition strategies and clustering (e.g., K-means BO) can be used within the closed-loop framework to ensure diverse experimental selections per iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Time and experimental resource constraints (on-the-fly lab operation and limited experiment count).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition functions can be weighted by experiment cost or constrained to fixed budgets/number of runs; closed-loop updates enable reallocation of remaining budget adaptively based on observed outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Typically measured as highest objective observed (e.g., material performance), novelty relative to existing candidates, or significant improvements over state-of-the-art within budgeted experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited literature demonstrates faster convergence to high-performing materials with fewer experiments; the perspective cites on-the-fly Bayesian active learning as an enabling methodology but does not report new numeric experiments for this method beyond referencing the Kusne et al. work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to static experimental design or exhaustive grid/random search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Cited works show closed-loop Bayesian active learning can substantially reduce number of experiments needed, but specific numeric comparisons are in referenced papers rather than this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported in referenced studies as significant reductions in experimental count to reach comparable discoveries; magnitude problem-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Framework explicitly designed to manage tradeoffs between information gain and experimental cost; the perspective emphasizes the need to integrate cost-awareness into acquisition selection and the HPC scheduling model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: adopt Bayesian active learning closed-loop strategies with cost-aware acquisition functions and batch/diversity-aware selection to maximize discovery probability under constrained experimental budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating materials discovery using artificial intelligence, high performance computing and robotics', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. <em>(Rating: 2)</em></li>
                <li>Efficient and Scalable Batch Bayesian Optimization Using K-Means. <em>(Rating: 2)</em></li>
                <li>A Tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. <em>(Rating: 2)</em></li>
                <li>On-the-fly closed-loop materials discovery via Bayesian active learning. <em>(Rating: 2)</em></li>
                <li>Big data meets quantum chemistry approximations: the Δ-machine learning approach. <em>(Rating: 2)</em></li>
                <li>Powerful, transferable representations for molecules through intelligent task selection in deep multitask networks. <em>(Rating: 1)</em></li>
                <li>A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials. <em>(Rating: 1)</em></li>
                <li>Expert-in-the-loop AI for polymer discovery. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2454",
    "paper_id": "paper-248386306",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "IBO",
            "name_full": "IBM Bayesian Optimization",
            "brief_description": "A cloud-exposed Bayesian optimization API suite used to prioritize computational simulations and experiments by optimizing acquisition functions and collecting batched evaluations to spend computational budget efficiently.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "IBM Bayesian Optimization (IBO)",
            "system_description": "A cloud-deployed Bayesian optimization system exposed via simple APIs that orchestrates surrogate models and acquisition functions to prioritize which candidates to simulate or test next. IBO supports batch selection (parallel evaluation) and can be configured with different batch acquisition strategies (e.g., PDTS sampling). In the exemplarb PAG workflow IBO was used with ECFP molecular descriptors to select batches of 10 TD-DFT simulations in parallel, optimizing a scoring metric combining target excitation energy distance and oscillator strength.",
            "application_domain": "Materials discovery / molecular photoacid generator (PAG) design (general: materials and molecular discovery)",
            "resource_allocation_strategy": "Select candidates by optimizing a Bayesian acquisition function; allocate computational budget to the highest-acquisition candidates in batches (configured here as batches of 10). Uses surrogate (probabilistic) models over candidate space and ranks candidates by expected utility for simulation allocation.",
            "computational_cost_metric": "Implicitly measured as number of simulations run and parallel batch size (wall-clock time through parallel batches); in practice measured by number of TD-DFT simulations executed and parallel core/time usage.",
            "information_gain_metric": "Acquisition function from Bayesian optimization (implemented via sampling/Thompson-sampling based batch selection here) estimating expected utility of acquiring each data point (not expressed as an explicit mutual information formula in the paper).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Bayesian optimization acquisition function mechanism balances exploration and exploitation; in the applied configuration PDTS (posterior sampling) introduces stochastic exploration while focusing on high-posterior regions (exploitation).",
            "diversity_mechanism": "Batch selection and descriptor-based representation (ECFP) produce chemical diversity implicitly; when combined with specific batch algorithms (e.g., K-means batch BO) explicit partitioning can promote diverse batch picks (K-means is supported by IBO but PDTS was used in the reported experiment).",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed computational budget expressed as number of simulations and available parallel cores (batch size/time constraints).",
            "budget_constraint_handling": "Optimize acquisition function under the available batch-parallel resources; use batch selection algorithms (PDTS, K-means batch) to maximize expected utility per batch and spend higher-fidelity simulations on fewer, higher-value candidates.",
            "breakthrough_discovery_metric": "Problem-specific performance score S = f_obs * |E_obs - E_target|^{-1} (paper uses S = f_obs / |E_obs - E_target| equivalent formulation) combining closeness to target excitation energy and oscillator strength; high S indicates candidate likely to meet optical requirements.",
            "performance_metrics": "For a library of &gt;400 PAG candidates IBO configured with PDTS (batch size 10) found the highest-performing 193 nm candidate after testing ~90 molecules (best IBO run) versus the non-accelerated workflow which typically required testing ~half the library (~200 molecules) on average; ECFP descriptors were used to represent molecules.",
            "comparison_baseline": "Non-accelerated virtual screening (traditional/naïve screening) / exhaustive or uniform screening baseline shown in the paper (also plotted against an ideal upper bound).",
            "performance_vs_baseline": "IBO (PDTS batch) located the best candidate after ~90 simulations vs ~200 simulations for the non-accelerated workflow (for &gt;400 candidates), i.e., found the optimum in ~45% of the evaluations required by the baseline on average in the presented example.",
            "efficiency_gain": "Reported practical speedup: in the PAG example the best-performing candidate was found after testing 90 molecules with IBO vs ~200 for the non-accelerated baseline (approx. 2.2x fewer simulations to find the optimum). Separately, the overall workflow reported a ~100-fold reduction in human expert review burden through generative + filtering steps (not purely IBO).",
            "tradeoff_analysis": "The paper qualitatively discusses the tradeoff: use Bayesian optimization to concentrate computational budget on higher-value regions so that more accurate (more expensive) models can be used on fewer data points; explicit quantitative tradeoff curves are shown for the PAG example comparing accelerated vs non-accelerated workflows, but no general closed-form cost-information tradeoff analysis is provided.",
            "optimal_allocation_findings": "Recommendation: use Bayesian optimization (including batch strategies like PDTS and K-means batch) to allocate simulations under limited computational budgets; combine descriptor choices (fast-to-compute descriptors like ECFP) with acquisition-based selection to efficiently identify high-performing candidates while reducing total simulation cost.",
            "uuid": "e2454.0",
            "source_info": {
                "paper_title": "Accelerating materials discovery using artificial intelligence, high performance computing and robotics",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "PDTS",
            "name_full": "Parallel and Distributed Thompson Sampling",
            "brief_description": "A batch Bayesian optimization algorithm that parallelizes exploration by drawing posterior samples (Thompson samples) from the surrogate model and selecting batches of candidates for simultaneous evaluation.",
            "citation_title": "Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space.",
            "mention_or_use": "use",
            "system_name": "Parallel Distributed Thompson Sampling (PDTS)",
            "system_description": "PDTS parallelizes Thompson sampling by drawing multiple posterior function samples from a probabilistic surrogate (e.g., GP or Bayesian neural net) and selecting high-ranked candidates from each sample to form a batch; this enables scalable batch acquisition for expensive evaluations while preserving the stochastic exploration properties of Thompson sampling.",
            "application_domain": "Large-scale chemical and materials discovery (batch selection for computationally expensive simulations).",
            "resource_allocation_strategy": "Allocate batch evaluation slots by sampling from the posterior and selecting candidates that maximize sampled objectives—allocates parallel compute to points suggested by different posterior realizations to diversify search while targeting high-value regions.",
            "computational_cost_metric": "Number of parallel evaluations (batch size), wall-clock time under parallel execution, and number of expensive simulations performed.",
            "information_gain_metric": "Implicitly uses posterior sampling as a surrogate for expected utility (Thompson sampling approximates probability of being optimal); not cast as explicit mutual information in the paper.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Stochastic posterior sampling yields exploration (different posterior samples may select diverse candidates) while naturally exploiting regions with high posterior mean/high reward; batch diversity comes from independent samples.",
            "diversity_mechanism": "Diversity arises from independent posterior samples selecting different modal candidates; no explicit clustering but tends to provide spread in the batch due to sampling variability.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed parallel batch resources and total number of evaluations.",
            "budget_constraint_handling": "Uses batch size and number of iterations as knobs; selects the most valuable batch members per iteration under the parallel evaluation constraint.",
            "breakthrough_discovery_metric": "Algorithm-agnostic: optimized against the task-specific objective (e.g., the S score used in the PAG experiment); breakthroughs correspond to candidates with top objective values.",
            "performance_metrics": "In the PAG example PDTS with batch size 10 was the configured acquisition method in IBO and led to discovering the top candidate after ~90 simulations (for &gt;400 candidates).",
            "comparison_baseline": "Compared operationally to non-accelerated workflows; conceptually compared to other batch BO schemes (e.g., K-means batch BO).",
            "performance_vs_baseline": "PDTS-equipped IBO outperformed non-accelerated screening in the PAG example by locating the top candidate after fewer simulations (~90 vs ~200).",
            "efficiency_gain": "Provides multi-fold reductions in required expensive evaluations in demonstrated chemical discovery tasks (example: roughly 2x reduction in evaluated candidates to find optimum in the PAG example).",
            "tradeoff_analysis": "Paper discusses that PDTS parallelizes acquisition and reduces wall-clock time while maintaining a balance between exploration and exploitation; explicit quantitative tradeoff curves are provided for specific experiments but no general optimality proof in this perspective.",
            "optimal_allocation_findings": "PDTS is effective for allocating parallel compute in batch active search and should be used where parallel expensive evaluations are available to accelerate exploration while preserving probabilistic exploration behavior.",
            "uuid": "e2454.1",
            "source_info": {
                "paper_title": "Accelerating materials discovery using artificial intelligence, high performance computing and robotics",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "KMeans-BO",
            "name_full": "K-means Batch Bayesian Optimization",
            "brief_description": "A batch Bayesian optimization technique that partitions acquisition function space via K-means clustering to select diverse batches of candidates for expensive evaluation.",
            "citation_title": "Efficient and Scalable Batch Bayesian Optimization Using K-Means.",
            "mention_or_use": "mention",
            "system_name": "K-means Batch Bayesian Optimization",
            "system_description": "This method partitions candidate space or the acquisition function landscape using K-means clustering and then selects representative, high-acquisition candidates from each cluster to form a batch; the clustering step promotes diversity in each batch while retaining high acquisition values within clusters.",
            "application_domain": "Batch selection for computationally expensive optimization problems in chemistry and materials discovery.",
            "resource_allocation_strategy": "Allocate batch evaluation slots by clustering the acquisition landscape and selecting top candidates per cluster, thereby allocating evaluations across diverse regions of candidate space rather than concentrating all resources in one mode.",
            "computational_cost_metric": "Number of selected batch evaluations and associated simulation wall-clock time; additional small clustering overhead.",
            "information_gain_metric": "Acquisition function values used to rank points within clusters; implicit expected utility through the acquisition function but not an explicit mutual information calculation.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration is promoted by cluster-based spread across the acquisition landscape; exploitation is achieved by selecting high-acquisition points within each cluster.",
            "diversity_mechanism": "Explicit: K-means clustering enforces spatial diversity across candidate feature/descriptor space so the batch covers multiple regions.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of parallel evaluations (batch size) and overall evaluation budget.",
            "budget_constraint_handling": "Divides batch budget across clusters to ensure sampling of multiple candidate regions per batch, improving chance of discovering diverse promising candidates under a tight batch budget.",
            "breakthrough_discovery_metric": "Dependent on task-specific acquisition/objective; method improves chances of sampling novel high-performing regions that might contain breakthroughs by ensuring batch diversity.",
            "performance_metrics": "Cited as an effective scalable batch BO strategy; not directly run in the PAG experiment (PDTS was used there).",
            "comparison_baseline": "Compared to naive batch selection (top-k acquisition without diversity) and other batch BO strategies.",
            "performance_vs_baseline": "Reported in cited literature to produce more diverse and effective batches under large-scale settings; specific numeric gains not presented in this perspective.",
            "efficiency_gain": "Aims to increase the probability of finding high-performing candidates per batch under fixed budget by ensuring coverage; efficiency gains depend on problem structure (cited as scalable and effective).",
            "tradeoff_analysis": "Method explicitly addresses the exploration-exploitation/diversity tradeoff by dividing budget across clusters; the perspective notes the approach as a deployed option for chemical discovery.",
            "optimal_allocation_findings": "Partitioning acquisition space with clustering is a practical strategy to allocate limited parallel evaluation budget to improve diversity and discovery probability in batch acquisition.",
            "uuid": "e2454.2",
            "source_info": {
                "paper_title": "Accelerating materials discovery using artificial intelligence, high performance computing and robotics",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "PairwiseTaskSim",
            "name_full": "Pairwise Task Similarity Recommender",
            "brief_description": "A recommender approach that uses pairwise task similarity measures from joint training effects to suggest simulation or modeling methodologies for low-data tasks using knowledge from high-data tasks, balancing cost and accuracy.",
            "citation_title": "Powerful, transferable representations for molecules through intelligent task selection in deep multitask networks.",
            "mention_or_use": "mention",
            "system_name": "Pairwise Task Similarity Recommender",
            "system_description": "A method that computes similarity between tasks by observing the sign and magnitude of transfer when training tasks jointly; similar tasks produce additive positive transfer, enabling recommendation of methodologies (e.g., cheaper ML models) for low-data tasks by leveraging related high-data tasks. Used as a recommender for selecting simulation/analysis protocols based on anticipated performance and cost.",
            "application_domain": "Selection of simulation and modeling methodologies in materials and molecular property prediction (generalizable across computational tasks).",
            "resource_allocation_strategy": "Allocate modeling effort and choose simulation protocols by recommending methods that historically transfer well from related tasks—thereby reducing unnecessary expensive computations for low-data tasks.",
            "computational_cost_metric": "Modeling/simulation cost (training time, need for data, computational expense of protocols); paper frames cost qualitatively rather than as a precise numeric metric.",
            "information_gain_metric": "Implicit—expected improvement in predictive accuracy via transfer learning; similarity measure derived from joint-training outcomes approximates expected utility of method transfer.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploit known related tasks to recommend low-cost methods for new tasks (exploitation of transfer), while exploration of method space can be guided by uncertainty in transfer estimates.",
            "diversity_mechanism": "Not primarily a diversity mechanism; focuses on transferability and cost-accuracy tradeoffs rather than enforcing hypothesis diversity.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Limited computational/data budget for new tasks (low-data regimes).",
            "budget_constraint_handling": "Recommend lower-cost methods for low-data tasks using similarity with high-data tasks so that computational resources are used where they are most effective.",
            "breakthrough_discovery_metric": "Not explicitly defined for breakthroughs; objective is to maximize predictive utility given resource constraints which indirectly supports discovery by improving model accuracy where it matters.",
            "performance_metrics": "Paper reports that pairwise similarity provides chemically plausible similarity measures and improves recommendation quality across tasks (quantitative metrics in cited works rather than in this perspective).",
            "comparison_baseline": "Compared conceptually to naive selection of methods without transfer-aware recommendations.",
            "performance_vs_baseline": "Described as improving method selection in low-data tasks by leveraging transfer; no specific numeric comparisons in this perspective.",
            "efficiency_gain": "Reduces unnecessary high-cost simulations or models on low-data tasks by suggesting lower-cost effective alternatives via transfer; exact gains depend on downstream tasks.",
            "tradeoff_analysis": "Positions the approach as balancing cost vs accuracy using transfer signals; recommends intelligent task selection to allocate limited computational budget efficiently.",
            "optimal_allocation_findings": "Insight: when data is sparse for a new task, use pairwise task similarity (transferability) to recommend cheaper methods trained on related high-data tasks so that limited resources produce maximal predictive value.",
            "uuid": "e2454.3",
            "source_info": {
                "paper_title": "Accelerating materials discovery using artificial intelligence, high performance computing and robotics",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Δ-ML GP Calibration",
            "name_full": "Delta-machine learning with Gaussian process calibration",
            "brief_description": "A delta-machine learning approach that learns the systematic difference between simulation outputs and experimental results using a Gaussian process with uncertainty estimates to calibrate simulation predictions per candidate.",
            "citation_title": "Big data meets quantum chemistry approximations: the Δ-machine learning approach.",
            "mention_or_use": "mention",
            "system_name": "Δ-machine learning (delta-ML) with Gaussian Process calibration",
            "system_description": "A methodology where a machine-learned model (the delta model) is trained to predict the discrepancy between simulation outputs and experimental measurements; the authors use a Gaussian process trained on circular fingerprints (ECFP-style) to provide per-candidate corrections together with Bayesian uncertainty estimates, informing trust in simulation outputs and guiding allocation of experimental/simulation resources.",
            "application_domain": "Bridging simulation outputs and experimental observations across materials and molecular discovery tasks (calibration of virtual screening results).",
            "resource_allocation_strategy": "Use uncertainty-aware calibration to decide which simulation outputs are reliable and where to allocate experimental verification resources; prioritize candidates with high corrected predicted performance and acceptable uncertainty.",
            "computational_cost_metric": "Training and inference costs for GP models (scales with training set size), plus cost of simulations and experiments that the calibration aims to reduce.",
            "information_gain_metric": "Model posterior uncertainty from the Gaussian process used as an information/uncertainty measure; expected reduction in predictive uncertainty guides allocation implicitly.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploit simulation outputs where calibrated corrections are confident; explore candidates where uncertainty is high to improve calibration (active selection of calibration experiments).",
            "diversity_mechanism": "Not an explicit diversity-promoting algorithm; diversity can be induced via active selection of high-uncertainty candidates across feature space.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Limited experimental verification budget and computational budget for high-fidelity simulations.",
            "budget_constraint_handling": "Prioritize experiments/simulations where calibration uncertainty is high or where potential utility (after correction) is large; use GP uncertainty to avoid overcommitting resources to low-confidence corrections.",
            "breakthrough_discovery_metric": "Candidates with high corrected predicted performance and sufficiently low uncertainty are prioritized as likely breakthroughs; no universal novelty metric is specified.",
            "performance_metrics": "Cited as enabling more robust selection of simulation methods and improved trust when making design decisions; quantitative results are in cited calibration literature rather than this perspective.",
            "comparison_baseline": "Compared conceptually to uncalibrated simulation outputs which can mislead selection decisions; GP calibration reduces risk of overfitting and misprioritization.",
            "performance_vs_baseline": "Calibration improves alignment of simulation predictions with experiment and reduces risk in decision-making; specific numeric improvements are provided in referenced works.",
            "efficiency_gain": "By improving per-candidate trust estimates, calibration reduces wasted experimental effort on poorly predicted candidates; exact gains depend on application.",
            "tradeoff_analysis": "Paper notes that uncertainty-aware calibration allows using cheaper simulations more aggressively while keeping trust via calibrated corrections—this is a practical tradeoff between computational cost and predictive fidelity.",
            "optimal_allocation_findings": "Use delta-ML with Bayesian uncertainty (GP) to decide when to trust simulations and when to spend resources on experiments or higher-fidelity simulations; integrate uncertainty into allocation decisions.",
            "uuid": "e2454.4",
            "source_info": {
                "paper_title": "Accelerating materials discovery using artificial intelligence, high performance computing and robotics",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "EITL",
            "name_full": "Expert-in-the-loop (EITL) discriminative filtering",
            "brief_description": "A human–AI hybrid approach where generative model outputs are constrained by expert-defined rules and filtered using discriminative AI models with human oversight to reduce candidate sets and focus resources.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Expert-in-the-loop (EITL) constrained generative downselection",
            "system_description": "Generative models produce large numbers of candidate molecules; EITL applies domain expert rules and discriminative (predictive) AI models to constrain and rank candidates before expensive simulations or experiments. This reduces the human and computational burden by narrowing candidates to a manageable subset that meets application-specific constraints.",
            "application_domain": "Molecular generative design and materials ideation (demonstrated for sulfonium PAG cation design).",
            "resource_allocation_strategy": "Allocate human review and expensive simulation/experimental effort only to candidates that pass expert rules and discriminative model filters, thereby conserving computational and human resources for the most promising hypotheses.",
            "computational_cost_metric": "Count of candidates requiring simulation/experimental evaluation and human expert review time; cost of running discriminative filters (typically cheap compared to high-fidelity simulations).",
            "information_gain_metric": "Not formalized as an information-theoretic metric; selection is driven by rule satisfaction and discriminative model scores indicating predicted feasibility/safety/performance.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Exploit domain knowledge to focus on plausible candidates (exploitation); exploration is limited by the constraints but generative models still explore structural diversity prior to filtering.",
            "diversity_mechanism": "Diversity primarily comes from the generative model; EITL applies constraints that may reduce diversity — the human expert tunes rules to balance novelty vs practicality.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Human time and simulation/experimental budget (practical limits on number of candidates reviewed/tested).",
            "budget_constraint_handling": "Massively reduce candidate pool via rule-based filters and discriminative models to fit the available human and computational budget; result in ~100-fold reduction in human review burden in the PAG use case.",
            "breakthrough_discovery_metric": "Final human expert selection combined with discriminative model scores and downstream simulation/experimental validation; breakthroughs identified by meeting application-specific optical and sustainability constraints.",
            "performance_metrics": "Workflow reduced ~3000 generated candidates to a few hundred by rule- and model-based filtering, producing a ~100-fold reduction in number of candidates the expert needed to analyze before final selection.",
            "comparison_baseline": "Compared to raw generative model outputs with manual inspection of large sets (no constrained filtering).",
            "performance_vs_baseline": "EITL reduced the human review workload by approximately two orders of magnitude compared with inspecting all generative outputs.",
            "efficiency_gain": "Approximately 100x reduction in candidate set size requiring expert analysis in the demonstrated PAG workflow.",
            "tradeoff_analysis": "Acknowledges tradeoff: aggressive constraints simplify downstream resource needs but risk excluding novel candidates; recommends human-in-the-loop tuning to balance novelty and practicality.",
            "optimal_allocation_findings": "Pragmatic recommendation: use expert-defined constraints plus discriminative models to pare generative outputs to a budget-compatible set for simulation and experimental validation, accepting a controlled reduction in exploratory breadth.",
            "uuid": "e2454.5",
            "source_info": {
                "paper_title": "Accelerating materials discovery using artificial intelligence, high performance computing and robotics",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Bayes-AL Closed-Loop",
            "name_full": "Bayesian active learning closed-loop materials discovery",
            "brief_description": "An on-the-fly closed-loop active learning framework applying Bayesian methods to select next experiments/measurements to maximize learning per experimental cost.",
            "citation_title": "On-the-fly closed-loop materials discovery via Bayesian active learning.",
            "mention_or_use": "mention",
            "system_name": "Bayesian active learning closed-loop",
            "system_description": "A closed-loop framework where a Bayesian surrogate model is updated with each new experimental/simulation result and an acquisition function selects the next experiments to perform to maximize information gain or objective improvement under cost constraints; designed for on-the-fly operation with experimental hardware.",
            "application_domain": "Materials discovery, experimental design, and autonomous laboratories.",
            "resource_allocation_strategy": "Iteratively allocate experimental runs by maximizing an acquisition function (e.g., expected improvement, information gain, or uncertainty reduction) per unit experimental cost, integrating experimental feedback in real time.",
            "computational_cost_metric": "Number of experiments (or simulations), real-time experimental wall-clock, and lab resource usage; cost-per-experiment can be incorporated into acquisition function.",
            "information_gain_metric": "Acquisition functions based on expected improvement, uncertainty reduction, or other Bayesian utility measures; design aims to directly maximize information gain per cost.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition-function-driven balance (e.g., EI/UCB/Thompson sampling) that trades off sampling in high-uncertainty regions (exploration) vs high-mean/expected-improvement regions (exploitation).",
            "diversity_mechanism": "Batch acquisition strategies and clustering (e.g., K-means BO) can be used within the closed-loop framework to ensure diverse experimental selections per iteration.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Time and experimental resource constraints (on-the-fly lab operation and limited experiment count).",
            "budget_constraint_handling": "Acquisition functions can be weighted by experiment cost or constrained to fixed budgets/number of runs; closed-loop updates enable reallocation of remaining budget adaptively based on observed outcomes.",
            "breakthrough_discovery_metric": "Typically measured as highest objective observed (e.g., material performance), novelty relative to existing candidates, or significant improvements over state-of-the-art within budgeted experiments.",
            "performance_metrics": "Cited literature demonstrates faster convergence to high-performing materials with fewer experiments; the perspective cites on-the-fly Bayesian active learning as an enabling methodology but does not report new numeric experiments for this method beyond referencing the Kusne et al. work.",
            "comparison_baseline": "Compared to static experimental design or exhaustive grid/random search.",
            "performance_vs_baseline": "Cited works show closed-loop Bayesian active learning can substantially reduce number of experiments needed, but specific numeric comparisons are in referenced papers rather than this perspective.",
            "efficiency_gain": "Reported in referenced studies as significant reductions in experimental count to reach comparable discoveries; magnitude problem-dependent.",
            "tradeoff_analysis": "Framework explicitly designed to manage tradeoffs between information gain and experimental cost; the perspective emphasizes the need to integrate cost-awareness into acquisition selection and the HPC scheduling model.",
            "optimal_allocation_findings": "Recommendation: adopt Bayesian active learning closed-loop strategies with cost-aware acquisition functions and batch/diversity-aware selection to maximize discovery probability under constrained experimental budgets.",
            "uuid": "e2454.6",
            "source_info": {
                "paper_title": "Accelerating materials discovery using artificial intelligence, high performance computing and robotics",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space.",
            "rating": 2,
            "sanitized_title": "parallel_and_distributed_thompson_sampling_for_largescale_accelerated_exploration_of_chemical_space"
        },
        {
            "paper_title": "Efficient and Scalable Batch Bayesian Optimization Using K-Means.",
            "rating": 2,
            "sanitized_title": "efficient_and_scalable_batch_bayesian_optimization_using_kmeans"
        },
        {
            "paper_title": "A Tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning.",
            "rating": 2,
            "sanitized_title": "a_tutorial_on_bayesian_optimization_of_expensive_cost_functions_with_application_to_active_user_modeling_and_hierarchical_reinforcement_learning"
        },
        {
            "paper_title": "On-the-fly closed-loop materials discovery via Bayesian active learning.",
            "rating": 2,
            "sanitized_title": "onthefly_closedloop_materials_discovery_via_bayesian_active_learning"
        },
        {
            "paper_title": "Big data meets quantum chemistry approximations: the Δ-machine learning approach.",
            "rating": 2,
            "sanitized_title": "big_data_meets_quantum_chemistry_approximations_the_δmachine_learning_approach"
        },
        {
            "paper_title": "Powerful, transferable representations for molecules through intelligent task selection in deep multitask networks.",
            "rating": 1,
            "sanitized_title": "powerful_transferable_representations_for_molecules_through_intelligent_task_selection_in_deep_multitask_networks"
        },
        {
            "paper_title": "A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials.",
            "rating": 1,
            "sanitized_title": "a_bayesian_approach_to_calibrating_highthroughput_virtual_screening_results_and_application_to_organic_photovoltaic_materials"
        },
        {
            "paper_title": "Expert-in-the-loop AI for polymer discovery.",
            "rating": 1,
            "sanitized_title": "expertintheloop_ai_for_polymer_discovery"
        }
    ],
    "cost": 0.019971749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PERSPECTIVE Accelerating materials discovery using artificial intelligence, high performance computing and robotics</p>
<p>Edward O Pyzer-Knapp 
✉ 
Jed W Pitera 
Peter W J Staar 
Seiji Takeda 
Teodoro Laino 
Daniel P Sanders 
James Sexton 
John R Smith 
Alessandro Curioni 
PERSPECTIVE Accelerating materials discovery using artificial intelligence, high performance computing and robotics
10.1038/s41524-022-00765-zOPEN
New tools enable new ways of working, and materials science is no exception. In materials discovery, traditional manual, serial, and human-intensive work is being augmented by automated, parallel, and iterative processes driven by Artificial Intelligence (AI), simulation and experimental automation. In this perspective, we describe how these new capabilities enable the acceleration and enrichment of each stage of the discovery cycle. We show, using the example of the development of a novel chemically amplified photoresist, how these technologies' impacts are amplified when they are used in concert with each other as powerful, heterogeneous workflows.npj Computational Materials (2022) 8:84 ; https://doi.</p>
<p>INTRODUCTION</p>
<p>Events such as the COVID-19 global pandemic have starkly illustrated the need for ever accelerating cycles of scientific discovery. This challenge has instigated one of the greatest races in the history of scientific discovery-one that has demanded unprecedented agility and speed. This requirement is not localized to the healthcare domain, however; with significant pressure being exerted on the speed of materials discovery by challenges such as the climate emergency, which arguably are of an even greater magnitude. Fortunately, our tools for performing such discovery cycles are transforming-with data, artificial intelligence and hybrid cloud being used in new ways to break through longstanding bottlenecks 1,2 .</p>
<p>Historically, science has seen a number of major paradigm shifts, as depicted in Fig. 1, which have been driven by the advent and advancement of core underlying technology 3 . Moving from empirical studies, the collection and sharing of studies allowed a more global view of scientific problems, and led to the development of key underpinning theory. As the advent of computational systems allowed ever more complex calculations to be achieved, our understanding grew, with technology driving scale to new heights. The last two decades have seen the emergence of the Fourth Paradigm of big-data-driven science, dominated by an exa-flood of data 4 and the associated systems and analytics to process it. The Fourth Paradigm has definitively made science a big-data problem 5 . For example, today virtual chemical databases contain billions of identified and characterized compounds 6 . Now, with the maturation of AI and robotic technology, alongside the further scaling of high-performance computing and hybrid cloud technologies, we are entering a new paradigm where the key is not any one individual technology, but instead how heterogeneous capabilities work together to achieve results greater than the sum of their parts.</p>
<p>A typical materials discovery effort can be decomposed into a series of phases: (1) specification of a research question, (2) collection of relevant existing data, then (3) formation of a hypothesis and finally (4) experimentation and testing of this hypothesis, which may in turn lead to knowledge generation and the creation of a new hypothesis. This process, whilst conceptually simple, has many significant bottlenecks which can hinder its successful execution 7 . For example, there are challenges in determining impactful research challenges, which requires increasingly deep and broad expertise. This is in part due to the known difficulties in keeping up with the rapidly expanding base of domain knowledge; for example, more than 28,000 articles were published on the subject of 'photovoltaics' since 2020-and this is just one area of research in our drive for renewable energy. Even when the materials space is constrained to be molecular, there are significant challenges in developing hypotheses relating structure to function, in part due to the sheer size of chemical space. Estimates say there are 10 108 potential organic molecules 8 which implies an intelligent navigation is necessary for any kind of accelerated discovery beyond serendipity. Similarly, there are gaps in experimentation, bridging digital models and physical testing, and ensuring reproducibility-it has been reported that 70% of scientists have at least once tried and failed to replicate the results of another 9 . Figure. 2 shows how the inclusion of AI, automation and improvements to deployment technologies can move towards a community-driven, closed loop process. This includes advances at each step, for example, to extract, integrate, and reason with knowledge at scale to better respond to question 10 , to the use of deep generative models to automatically propose new hypotheses, to automating testing and experimentation using robotic labs 11 . Important advances in the machine representation of knowledge also enable new results to lead to new questions and hypotheses 12 .</p>
<p>In this perspective, we describe technologies we have been exploring for this aim, and concretize our view with a real example where we have applied these technologies to a problem of commercial importance, the development of more sustainable photoacid generators (PAGs) for chemically amplified photoresists 13 .</p>
<p>ENABLING TECHNOLOGIES Capturing unstructured technical data</p>
<p>Historical material science data is embedded in unstructured patents, papers, reports, and datasheets. Automated platforms are needed to ingest these documents, extract the data and ultimately present it to users for query and downstream use. For each individual component of the process, there are both proprietary and non-proprietary solutions available. For example, Semantic Scholar 14 provides search access to over 190M scholarly articles. They also provide trained AI models for document conversion. Furthermore, solutions such as ChemDataExtractor 15 and tmChem 16 allow users to extract materials related entities from these documents, either in the generic case or for very specific use-cases-for example, zeolites 17 . Each of these components solve one aspect of the bigger problem, i.e. extracting unstructured materials related data from documents. We have devised the IBM DeepSearch platform to provide a holistic solution to the overall challenge of extracting unstructured data from documents, by having different tight-coupled services. These services allow users to upload documents and apply NLP algorithms on them in order to create KGs for deep queries (see Fig. 3).</p>
<p>Specifically, the IBM DeepSearch platform consists of the Corpus Conversion Service (CCS) 18 and Corpus Processing Services (CPS) 19 . The CCS leverages state-of-the-art AI models 20 to convert documents from PDF to the structured file format JSON. In this ingestion stage, there are a number of technical challenges-the segmentation of pages of the document into their component structure, the assignment of labels to each of these segments, and the identification and extraction of data from tables embedded in the document. To achieve sufficient accuracy across these tasks, a range of different models are required [20][21][22] . All these models run concurrently on a cloud-deployed cluster, enabling a conversion rate of 0.25 page/sec/core. This enables the conversion of the entire ArXiv repository 23 in less than 24 h on 640 cores. Using the converted documents, the CPS service builds document-centric Knowledge Graphs (KGs) and supports rich queries and data extraction for downstream use. Common queries consist of searching for previously patented materials or associating reported properties with known materials. To this end, we pretrained natural language processing (NLP) models for Named Fig. 1 The progression of the scientific method. Science has seen a number of major paradigm shifts, which have been driven by the advent and advancement of core underlying technology. Key open challenges in the use of unstructured data in materials discovery include data access, entity resolution, and complex ad hoc queries. Data access is an issue as much of the content of interest, particularly technical papers and domain-specific databases, are not yet open access, particularly for large-scale machine ingestion. Navigating the appropriate copyright and usage agreements is often the most complex phase of an unstructured data project. The entity resolution problem in materials is often also complex. For example, the text of a paper might describe a material sample, which is then sub-divided and processed according to parameters shown in a table. Subsequent graphs of the properties for each individual sample may be labeled with symbolic references that require combining the information from both the text and the table to accurately identify the material and processing conditions which yield the graphed property. In effect, the materials entity is specified in a diffuse fashion across multiple modalities in the document. Finally, as capabilities to collect and organize materials data improve, there is a natural expectation that more complex queries should be supported, progressing from existence ('Has this material been made?') through performance ('What's the highest recorded T c ?') to hypothesis ('Could a Heusler compound be useful in this spintronic device?').</p>
<p>Using AI to make simulation workflows more efficient and effective The materials literature is overwhelmingly vast, but also incomplete 24 . Property data on existing materials is sparse, and data on hypothetical materials are necessarily absent. Simulation gives us the means to generate this data, but this switch from physical to digital experimentation provides some challenges. For example, the choice of simulation protocol can present complexity, and a poor choice can doom a discovery campaign before it is begun 25 . Even if an accurate protocol exists, the computational expense of executing it may severely limit the size of the design space being searched 26 . The area of AI or ML-assisted simulations can address some of these issues, and has been gaining some significant traction in recent years 1,27 . Emerging from the use of neural networks to bypass expensive physics-based routines 28,29 , AI has been used to predict ever more complex properties, such as energetic materials 30 , solid-state materials properties 31,32 and even the structure of proteins 33 . In addition, we have seen the emergence of machine-learned potentials which enable access to quantum-chemical-like accuracies at a fraction of the cost 34 .</p>
<p>In our accelerated discovery paradigm, we consider that modeling and simulation workflows have the following structure. We believe that AI has the ability to enrich this structure in a number of ways. First, we perceive a valuable application of mature AI methodologies such as recommender systems to suggest which particular methodologies to use for particular tasks based on cost and accuracy. Where there is existing experimental data, this is trivial, but for new tasks, where data is sparse, this can become problematic. To circumvent this, we use a pairwise task similarity approach to guide the recommendation of low-data tasks from what we already know about high-data tasks. This method exploits the fact that the joint training of similar tasks will be broadly additive and positive, whilst the effect of joint training broadly dissimilar tasks will be net negative 35 . This has been shown to provide chemically plausible similarity measures for a range of tasks 36 .</p>
<p>Secondly, we can dynamically improve candidate prioritization using Bayesian optimization 37,38 , allowing us to selectively spend our computational budget, and thus use more accurate models on a smaller amount of data, thus improving the traditional 'virtual high throughput screening' 39 model shown in Fig. 4. This methodology is similar to other active learning approaches 40 and allows us to balance the exploitation of trends from data we already have with the acquisition of new knowledge in unexplored areas 41,42 . Bayesian optimization is a general methodology often utilized when each data point is expensive (in time, cost, or effort) to acquire 43 . At each stage of screening, candidates are selected by optimizing an acquisition function which estimates the value for acquiring each data point. Improved Bayesian optimization algorithms allow the selection of batches of data points. Parallel Distributed Thompson Sampling 37 parallelizes through sampling of the Bayesian model, while K-means Batch Bayesian optimization 38 parallelizes through unsupervised partitioning of the acquisition function, and both have been deployed successfully to chemical discovery problems. In order to maximize the usability of this system, we present it to users through a simple set of cloud APIs known as IBM Bayesian Optimization (IBO).</p>
<p>The final part of the workflow where we believe AI can add value is to improve the relatedness of simulation outputs to realworld data. We achieve this through the calibration of the output of a simulation to better reflect an experimental outcome. This calibration is a type of delta machine learning 44 with the addition of a concept of Bayesian uncertainty. Uncertainty aware models of how simulations systematically differ from experiment can provide highly accurate calibrations on a per-candidate basis, which avoid the pitfall of overfitting through the communication of a notion of the certainty of the correction (i.e. how much it can be trusted) 25 . The methodology we have chosen to achieve this goal is a Gaussian process model 45 built on molecules described by their circular fingerprint 46 . This enables more robustness in the selection of simulation methods, and corresponding trust when making critical design decisions.</p>
<p>For these complex, AI-driven workflows to deliver on the promise of faster, more efficient simulation, several challenges need to be addressed. First, the practice of 'virtual experiments' that capture all aspects of a given computational task needs to become generally accepted and used in the community. Second, the traditional high performance computing model of manual or semi-manual long-running batch calculations will have to adapt to the dynamic active learning model described here. Finally, there is a need to eventually integrate the complex heterogeneous computing systems of the future, whether they are quantum computers, AI hardware accelerators, or classes of computer we cannot yet imagine.</p>
<p>Applying computational creativity to the molecular design problem For molecular materials, across the wide range of structural scales, the molecular structure is dominant in determining properties of interest, and therefore, as we have previously noted, the materials design space can be intractably vast. In conventional molecular design processes, human experts explore this vast parameter space guided with their knowledge, experience, and intuition in a trial-and-error approach, which can yield a long development period and potentially limited variety. To counter this, we adopt an AI-driven generative modeling approach to collaborate with human experts and augment their creativity. Deep generative modeling (DGM) is one important example of such a class of technologies. Recent developments in AI technology based on pre-trained language models 47 and Generative Adversarial Networks (GANs) 48,49 , have been used to automatically generate images, speech, and natural language, and have recently been applied to materials discovery problems 50,51 . In addition to DGM, other AI approaches have been effectively used for this purpose, including Monte Carlo tree search 52 , genetic algorithms 53 , and the junction tree algorithm 54 . Generative AI models can generate new candidate chemicals, molecules 24,55,56 , and materials 57 , and expand both the discovery space and the creativity of scientists. Our experience is that generative models can accelerate early materials ideation processes by 100x 58 .</p>
<p>Since there are a large number of approaches to generating materials candidates, it is important that the overall workflows are kept consistent. This can be distilled into the following common stages (see Fig. 5): after an initial training step, input molecular structures are encoded in a space that is used to predict associated properties. Next, the feature space (or latent space) is Fig. 4 How computational funnels are commonly used to accelerate the discovery process. a The 'traditional' computational funnel of high-throughput virtual screening. b Each level in the funnel is affected by the accuracy of screening, and the computational cost to perform the screen. Machine learning has the ability to markedly improve both of these, if the right training data is available. Finally, the sampled feature vectors are decoded to molecular structures. In deep generative models, our approach is to leverage Wasserstein Auto Encoders (WAE) and Conditional Latent Space Sampling (CLaSS) for this purpose which we have demonstrated successfully in peptide sequence generation to design antimicrobial materials 59,60 . Another approach is a combination of VAE and reinforcement learning (RL) 61,62 , where drug molecules' SMILES and target proteins are both encoded on a common latent space. Reinforcement learning explores this space, guided by a model to predict the efficacy of the generated drug to target cancer proteins 61 . Another powerful scheme is the Molecular Generation Experience (MolGX), which leverages an explicit graph enumeration algorithm 58,63 . In MolGX, the encoder/decoder is preconfigured by the graph algorithms to generate valid chemical structures, therefore pre-training by a huge dataset is unnecessary. In addition, a user can fine tune the molecular generation process in atomistic detail (e.g. to control the number of (un)desired functional groups). The base MolGX functions are provided as a public web application at https://molgx.draco.res.ibm.com. The set of those generative models work under review and control by human experts, who tune and reinforce the models with domain knowledges.</p>
<p>Looking forward, generative models will need to evolve in their coverage of materials classes, extend beyond materials composition to processing and form, and effectively capture and encode application constraints based on human knowledge. The first of these, coverage of materials classes, is obvious, and will happen gradually as data becomes available. An open question is whether there will be a single unified generative model for all materials classes, or the gradual coverage of materials categories with independent models specialized for organic materials, crystalline inorganic materials, polymers, metal-organic frameworks, and so on. Regardless of the material category, these models will eventually need to capture the full complexity of materials manufacture and use. This will involve training the models on not just materials structures but also the un-and semi-structured data that describe materials synthesis and processing. Finally, we have found that in practice generative models are most useful when their outputs are either informed or filtered by the deep expertise of human subject matter experts. Tools and technologies to capture that expertise efficiently and encode it in the model will maximize the chances of generating not just a possible material, but a useful material.</p>
<p>Owing to its data-driven nature, generative model is highly compatible with open-innovative contribution by multiple users. For the sake of scalable offering of state-of-the-art pre-trained generative models and algorithms, we've been underway to integrate our algorithms onto a hybrid cloud platform, whereby users can contribute to the development and reuse them. We believe open science is a key concept to accelerate the evolution of generative modeling.</p>
<p>Materials evaluation using AI and cloud-powered automated labs</p>
<p>At the end of the design cycle, we face the need to accelerate the synthesis and testing of the large number of materials hypotheses. Recent advances in AI enabled digitization of common tasks in chemical synthesis, including forward reaction prediction 64 , retrosynthetic analysis 65 , and inference of experimental protocols to execute novel chemical synthesis 66 . Concurrently, there is an explosion of automation and AI in chemical synthesis, with the important contribution in the use of commodity hardware 67 , fluidic reactors 68 , or the use of robots able to execute the same tasks as human chemists 69 .</p>
<p>The construction of autonomous synthesis platforms is still a work in progress. One of the most recent efforts is RoboRXN, relying on an integration of three technologies: cloud, AI, and commercial automation to assist chemists all the way from the selection of synthetic routes to the actual synthesis of the molecule. A graphical overview of the architecture of RoboRXN is shown in Fig. 6. The use of cloud technologies enables a remote chemical laboratory as an embodiment of the cloud infrastructure, thus providing chemical services wherever an internet connection is available. AI is the core technology fueling the entire execution of domain experts' tasks.</p>
<p>A core component of RoboRXN is a pipeline of multiple machine learning models that enables a complete automation of the synthesis plan, starting from a target molecule or a paragraph of a chemical recipe and ending with the process steps executed by the robot. Reaction prediction tasks are cast as translation tasks 70 , and trained on &gt;2.5 million chemical reactions. These models, based on the Molecular Transformer 64 , allow the design of the synthesis starting from commercially available materials and provide an essential requirement of autonomous synthesis: the Fig. 6 The architectural design of the AI-powered, Cloud-based autonomous chemical laboratory. The prototype is made up of two parts. The first one which includes AI, Frontend and Backend components can live either in the Cloud or Premise thanks to the OpenShift technology that allows a seamless portability across different infrastructures. The second one comprising automation hardware physically located on the edge behind a firewall. inference of precise sequence of operations that are executed by the synthesis hardware. All the models are freely available through a cloud platform called IBM RXN for Chemistry [https://rxn.res.ibm. com]. In the IBM lab, the integration of analytical chemistry technologies (LCMS and NMR) with the commercial synthesis hardware provides real-time monitoring to monitor results and generate feedback for improvement.</p>
<p>Future challenges in this space of automated chemistry include the generation and integration of in silico chemical data, the further integration of analytical chemistry and application-specific testing, and the expansion and adaptation of these technologies to other materials classes. The space of known chemical reactions, while vast, is still finite. In contrast, computational chemistry techniques could in principle allow the automated exploration of the vast space of hypothetical reactions. Intelligent exploration of this space could yield a wealth of training data for reaction prediction, provided it can be generated accurately and integrated with existing data-derived knowledge. These robotic systems also offer an opportunity for exploration of a different kind, specifically the prospect of independent active learning systems that on their own explore the chemical space searching for materials that suit application objectives. To do this effectively, the simulation and analytical chemistry systems need to be integrated with automated in-line application-specific testing. Finally, to fully realize the dream of a general-purpose materials synthesis robot, these capabilities need to be extended across materials classes. As with generative models, the question of whether there would ever be a single generalized robotic system or instead specialized robotic systems for specific materials and applications remains open.</p>
<p>EXEMPLAR USE CASE</p>
<p>As an exemplar use case, we carried out a project to address a key sustainability challenge focused on photoacid generators (PAGs), a critical photosensitive complex employed in chemically amplified photoresists used in semiconductor manufacturing 71 . Of the several classes of known PAGs 72-74 , sulfonium ([SR 3 ]+) or iodonium ([IR 2 ]+) -based complexes are the most widely used in semiconductor lithography [75][76][77] . Recently, onium-based photoacid generators have come under heightened regulatory scrutiny for potential persistence, bioaccumulation, and toxicity (PBT) risks 78 . While studies have helped clarify the potential PBT risks associated with representative PAGs 79 as well as identify relevant photodecomposition products 80,81 , it remains extremely challenging for industry to design, synthesize, test, and bring to market new PAGs with improved sustainability profiles in a timely manner.</p>
<p>While both the cation and anion of prototypical onium PAGs could benefit from improved sustainability profiles, we initially focused our work on developing an accelerated discovery workflow for the discovery of sulfonium-based PAG cations with improved environmental health and safety profiles. The workflow is summarized in Fig. 7. In the first stage of the workflow, approximately 6000 patents, papers, and data sources were ingested by Deep Search to form a knowledge graph from which the structures of approximately 5000 sulfonium PAGs were obtained. To overcome the limited availability of key property values for most of the identified cations, AI-enriched simulation was used to compute both UV absorption using TD-DFT 82 using the GAMESS-US framework 83 and selected sustainability properties for several hundred sulfoniums using OPERA 84 . The predicted sustainability parameter set included basic physicochemical properties (octanol-water partition coefficient (LogP) and water solubility (LogWS)), an environmental persistence parameter (biodegradability-LogHalfLife), and a toxicity endpoint (CATMoS-LD 50 ). The resulting structure-property dataset was then used to train a generative model, which was able to produce 3000 candidate sulfonium cations over the course of a 6 h run. However, many application-specific constraints that determine PAG utility in the context of semiconductor lithography were either partially or completely unaccounted for in the limited property dataset used to train the generative model. To overcome this, a combination of expertdefined rules and discriminative expert-in-the-loop (EITL) AI models 85 were used to first constrain the generative model output and then aid in the candidate downselection process, respectively. In this manner, the 3000 PAG candidates output from the generative model were filtered down to only a few hundred candidates, a more manageable number for which property data could be simulated.</p>
<p>Simulation on these candidates was prioritized using the aforementioned IBM Bayesian optimization (IBO) functionality, using a simple scoring metric, S, which combined the distance from the target excitation energy (6.46 eV and 5.00 eV for the common 193 nm and 248 nm targets) and the computed oscillator strength:
S ¼ f obs jE obs À E target j(1)
where E obs is the computed excitation energy, E target is the target excitation energy and f obs is the computed oscillator strength. An example of the speedup possible through this method can be seen in Fig. 8, which shows a comparison between an IBO accelerated workflow, and a non-accelerated workflow. For this example, IBO was configured to use PDTS (parallel, distributed Thompson sampling 37 ), collecting batches of 10 simulations in parallel, with molecules described using ECFP descriptors 46 . ECFP fingerprints were chosen due to their previous successes at this task 37,41 , their speed of calculation relative to other 'learned' representations, and their ability to be generated from 2D information. For a library of over 400 candidate PAGs, the highest performing molecule targeting the 193 nm wavelength was found on average after only testing half the library, with the best performing accelerated workflow locating this candidate after only testing 90 molecules.</p>
<p>This multi-step refinement methodology enabled a 100-fold reduction in the number of generated candidates that a human environmental toxicology expert was required to analyze in order to perform a final selection of a few top candidates for the next stage of automated retrosynthetic analysis. With several top candidates consisting of substituted variants of a dialkylphenylsulfonium core, (4-methylphenyl)dimethylsulfonium triflate was employed as a model candidate surrogate to simplify the final experimental validation. Application of the AI retrosynthetic model identified a one-step reaction involving the S-alkylation of 4-(methylthio)toluene by methyl trifluoromethanesulfonate 86,87 as the most promising pathway (shown in Fig. 9). The reaction instruction set was generated and transferred via the cloud to the RoboRXN system, which successfully carried out the reaction to afford the expected product. This initial demonstration of the applicability and utility of the discovery workflows for PAGs has inspired us to begin expanding the simulation portfolio and diversifying the types of generative AI models used in the workflow for future discovery cycles.</p>
<p>OUTLOOK</p>
<p>The work described above illustrates a prototype for the future of accelerated materials discovery. There are certainly examples of larger-scale computational screening efforts 88 , as well as more complex laboratory automation 69 , but in contrast, the workflow we describe here is much more irregular and heterogeneous, requiring the linking together of multiple distinct capabilities over multiple geographies. Of note, the complexity of this irregular and heterogeneous discovery workflow was enabled by the use of the OpenShift hybrid cloud computing framework 89 , enabling a single researcher to orchestrate the available resources across 3 datacenters on 3 continents to execute the necessary stepsa model which we believe will become more essential as the task of materials discovery continues to globalize and new technologies such as quantum computing continue to challenge what is possible in each stage of the discovery cycle. In this prototype, a series of sophisticated applications, algorithms, and computational systems are seamlessly orchestrated to accelerate cycles of learning and support human scientists in their quest for knowledge. In our research, we have seen tangible examples of this acceleration across all stages of the discovery process, and we strongly believe that the commoditization and democratization of such diverse workflows will fundamentally alter the way we respond to emerging discovery challenges. Fig. 9 Using IBM RXN to generate a reterosynthetic pathway for a target molecule. RXN determined that a one-step reaction involving the S-alkylation of 4-(methylthio)toluene by methyl trifluoromethanesulfonate would be the most promising pathway. Fig. 8 A comparison between workflows accelerated by Bayesian optimization (IBO accelerated) and those without this acceleration. Solid lines represent bootstrap estimates of the mean run from 5 replicate workflows, with shaded areas representing 95% confidence intervals for that estimate. Ideal behavior (i.e. the best possible score) is shown as a gray dashed line, and the best IBOaccelerated workflow is shown as a blue dashed line.</p>
<p>Fig. 2
2Technology-driven acceleration of the discovery cycle. AI, HPC and robotic automation are helping to accelerate and enrich all stages of the discovery cycle through the ability to further scale efforts through improved generation of, access to and reasoning on a wide variety of data.E.O. Pyzer-Knapp et al. Entity Recognition (NER) of materials, properties, materials classes and unit-and-values. These entities become nodes in the graph linked by edges corresponding to detected relationships. Currently, the creation of KGs from corpora of hundreds of thousands of documents can be completed in approximately 6 h on 640 cores.</p>
<p>•
Intent-the 'translation' of the property of interest into a corresponding computational workflow• Decision-the specific methodological choices used • Execution-scheduling, prioritization, and monitoring • Analysis-mapping output of the workflow to the real-world property</p>
<p>Fig. 3
3IBM deep search. Knowledge generation from unstructured data (PDF) is achieved through the use of a platform called IBM DeepSearch, which consists of two systems; the Corpus Conversion Service (CCS) and Corpus Processing Service (CPS). E.O. Pyzer-Knapp et al.</p>
<p>Fig. 5
5Using generative models to explore chemical space. A conceptual framework of molecular generative model. Each component; data formatting, encoding, prediction, sampling, and decoding are dependent on an approach (e.g. deep generative model, graph theory approach, etc.).E.O. Pyzer-Knapp et al. explored to sample feature vectors satisfying target properties.</p>
<p>Fig. 7
7Accelerating the discovery of novel photoacid generators. An example of how the technologies connect together to accelerate the discovery of novel materials.E.O. Pyzer-Knapp et al.
Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences npj Computational Materials (2022) 84
npj Computational Materials (2022)84 Published in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences
© The Author(s) 2022
ACKNOWLEDGEMENTSThe authors would like to acknowledge their IBM colleagues for their continued contributions to this exciting area of research. In particular, we would like to thank Dmitry Zubarev and Brooke Tvermoes for helpful conversations during the preparation of this perspective.Received: 13 August 2021; Accepted: 26 March 2022;AUTHOR CONTRIBUTIONSAll authors contributed to the conception, structuring, and writing of this perspective.COMPETING INTERESTSThe authors declare no Competing Non-Financial Interests but declare the following Competing Financial Interests: All authors are employed by IBM Research, which has an active research program in accelerated discovery. The IBM DeepSearch, IBM Bayesian Optimization, IBM Molecule Generation Experience and IBM RoboRXN technologies described in this Perspective were developed as part of that effort.ADDITIONAL INFORMATIONCorrespondence and requests for materials should be addressed to Edward O. Pyzer-Knapp.Reprints and permission information is available at http://www.nature.com/ reprintsPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons. org/licenses/by/4.0/.
Evolving the materials genome: how machine learning is fueling the next generation of materials discovery. C Suh, C Fare, J A Warren, E O Pyzer-Knapp, Annu. Rev. Mater. Res. 50Suh, C., Fare, C., Warren, J. A. &amp; Pyzer-Knapp, E. O. Evolving the materials genome: how machine learning is fueling the next generation of materials discovery. Annu. Rev. Mater. Res. 50, 1-25 (2020).</p>
<p>Accelerating the discovery of materials for clean energy in the era of smart automation. D P Tabor, Nat. Rev. Mater. 3Tabor, D. P. et al. Accelerating the discovery of materials for clean energy in the era of smart automation. Nat. Rev. Mater. 3, 5-20 (2018).</p>
<p>The Structure of Scientific Revolutions 2nd edn. T Kuhn, The University of Chicago PressKuhn, T. The Structure of Scientific Revolutions 2nd edn (The University of Chicago Press, 1970).</p>
<p>Scientific Research and Big Data. S Leonelli, Stanford Encyclopedia of PhilosophyLeonelli, S. Scientific Research and Big Data (Stanford Encyclopedia of Philosophy, 2020).</p>
<p>A J Hey, S Tansley, K M Tolle, The Fourth Paradigm: Data-intensive Scientific Discovery. Microsoft Research Redmond1Hey, A. J., Tansley, S., Tolle, K. M. The Fourth Paradigm: Data-intensive Scientific Discovery 1 (Microsoft Research Redmond, 2009).</p>
<p>Enumeration of 166 Billion Organic Small Molecules in the Chemical Universe Database GDB-17. L Ruddigkeit, R Van Deursen, L C Blum, J.-L Reymond, J. Chem. Inf. Model. 52Ruddigkeit, L., van Deursen, R., Blum, L. C. &amp; Reymond, J.-L. Enumeration of 166 Billion Organic Small Molecules in the Chemical Universe Database GDB-17. J. Chem. Inf. Model. 52, 2864-2875 (2012).</p>
<p>D Y Zubarev, J W Pitera, Machine Learning in Chemistry: Data-Driven Algorithms, Learning Systems, and Predictions. Pyzer-Knapp, E. O. &amp; Laino, TACS PublicationsZubarev, D. Y. &amp; Pitera, J. W. In Machine Learning in Chemistry: Data-Driven Algorithms, Learning Systems, and Predictions (eds Pyzer-Knapp, E. O. &amp; Laino, T.) 103-120 (ACS Publications, 2019).</p>
<p>The enumeration of chemical space. J.-L Reymond, L Ruddigkeit, L Blum, R Van Deursen, WIREs Comput. Mol. Sci. 2Reymond, J.-L., Ruddigkeit, L., Blum, L. &amp; van Deursen, R. The enumeration of chemical space. WIREs Comput. Mol. Sci. 2, 717-733 (2012).</p>
<p>Reproducibility crisis. M Baker, Nature. 533Baker, M. Reproducibility crisis. Nature 533, 353-66 (2016).</p>
<p>Automated hypothesis generation based on mining scientific literature. S Spangler, Proc. 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 20th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningSpangler, S. et al. Automated hypothesis generation based on mining scientific literature. In: Proc. 20th ACM SIGKDD International Conference on Knowledge Dis- covery and Data Mining 1877-1886 (2014).</p>
<p>Automated extraction of chemical synthesis actions from experimental procedures. A C Vaucher, Nat. Commun. 11Vaucher, A. C. et al. Automated extraction of chemical synthesis actions from experimental procedures. Nat. Commun. 11, 1-11 (2020).</p>
<p>Help Scientists Ask More Powerful Questions. P Kohli, Will, Scientific American Blog Network. Kohli, P. AI Will Help Scientists Ask More Powerful Questions. Scientific American Blog Network https://blogs.scientificamerican.com/observations/ai-will-help- scientists-ask-more-powerful-questions/.</p>
<p>Approaches to the design of radiation-sensitive polymeric imaging systems with improved sensitivity and resolution. C G Willson, H Ito, J M Fréchet, T G Tessier, F M Houlihan, J. Electrochem. Soc. 133181Willson, C. G., Ito, H., Fréchet, J. M., Tessier, T. G. &amp; Houlihan, F. M. Approaches to the design of radiation-sensitive polymeric imaging systems with improved sensitivity and resolution. J. Electrochem. Soc. 133, 181 (1986).</p>
<p>S2ORC: The Semantic Scholar Open Research Corpus. K Lo, L L Wang, M Neumann, R Kinney, D Weld, Proc. 58th Annual Meeting of the Association for Computational Linguistics 4969-4983. 58th Annual Meeting of the Association for Computational Linguistics 4969-4983Association for Computational LinguisticsLo, K., Wang, L. L., Neumann, M., Kinney, R. &amp; Weld, D. S2ORC: The Semantic Scholar Open Research Corpus. In: Proc. 58th Annual Meeting of the Association for Computational Linguistics 4969-4983 (Association for Computational Linguistics, 2020).</p>
<p>ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature. M C Swain, J M Cole, J. Chem. Inf. Model. 56Swain, M. C. &amp; Cole, J. M. ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature. J. Chem. Inf. Model. 56, 1894-1904 (2016).</p>
<p>tmChem: a high performance approach for chemical named entity recognition and normalization. R Leaman, C.-H Wei, Z Lu, J. Cheminformatics. 7S3Leaman, R., Wei, C.-H. &amp; Lu, Z. tmChem: a high performance approach for che- mical named entity recognition and normalization. J. Cheminformatics 7, S3 (2015).</p>
<p>A machine learning approach to zeolite synthesis enabled by automatic literature data extraction. Z Jensen, ACS Cent. Sci. 5Jensen, Z. et al. A machine learning approach to zeolite synthesis enabled by automatic literature data extraction. ACS Cent. Sci. 5, 892-899 (2019).</p>
<p>Corpus conversion service: a machine learning platform to ingest documents at scale. P W J Staar, M Dolfi, C Auer, C Bekas, Proc. 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningACMStaar, P. W. J., Dolfi, M., Auer, C. &amp; Bekas, C. Corpus conversion service: a machine learning platform to ingest documents at scale. In: Proc. 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining 774-782 (ACM, 2018).</p>
<p>Corpus processing service: a knowledge graph platform to perform deep data exploration on corpora. P W J Staar, M Dolfi, C Auer, Appl AI Lett. 120Staar, P. W. J., Dolfi, M. &amp; Auer, C. Corpus processing service: a knowledge graph platform to perform deep data exploration on corpora. Appl AI Lett. 1, e20 (2020).</p>
<p>Robust PDF document conversion using recurrent neural networks. N Livathinos, Proc. AAAI Conf. AAAI Conf35Livathinos, N. et al. Robust PDF document conversion using recurrent neural networks. Proc. AAAI Conf. Artif. Intell. 35, 15137-15145 (2021).</p>
<p>. Y Wu, A Kirillov, F Massa, W.-Y Lo, R Girshick, Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y. &amp; Girshick, R. https://github.com/ facebookresearch/detectron2. (2019).</p>
<p>X Zhong, E Shafieibavani, A Jimeno Yepes, Computer Vision-ECCV 2020. Vedaldi, A., Bischof, H., Brox, T. &amp; Frahm, J.-MSpringer International PublishingZhong, X., ShafieiBavani, E. &amp; Jimeno Yepes, A. In Computer Vision-ECCV 2020 (eds. Vedaldi, A., Bischof, H., Brox, T. &amp; Frahm, J.-M.) 564-580 (Springer Interna- tional Publishing, 2020).</p>
<p>Data-driven materials science: status, challenges, and perspectives. L Himanen, A Geurts, A S Foster, P Rinke, Adv. Sci. 61900808Himanen, L., Geurts, A., Foster, A. S. &amp; Rinke, P. Data-driven materials science: status, challenges, and perspectives. Adv. Sci. 6, 1900808 (2019).</p>
<p>A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials. E O Pyzer-Knapp, G N Simm, A A Guzik, Mater. Horiz. 3Pyzer-Knapp, E. O., Simm, G. N. &amp; Guzik, A. A. A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials. Mater. Horiz. 3, 226-233 (2016).</p>
<p>Accelerating computational discovery of porous solids through improved navigation of energy-structurefunction maps. E O Pyzer-Knapp, L Chen, G M Day, A I Cooper, Sci. Adv. 7334763Pyzer-Knapp, E. O., Chen, L., Day, G. M. &amp; Cooper, A. I. Accelerating computational discovery of porous solids through improved navigation of energy-structure- function maps. Sci. Adv. 7(33), eabi4763 (2021).</p>
<p>. H M Cartwright, Machine Learning in Chemistry. Royal Society of ChemistryCartwright, H. M. Machine Learning in Chemistry. (Royal Society of Chemistry, 2020).</p>
<p>Machine learning of molecular electronic properties in chemical compound space. G Montavon, N. J. Phys. 1595003Montavon, G. et al. Machine learning of molecular electronic properties in che- mical compound space. N. J. Phys. 15, 095003 (2013).</p>
<p>Learning from the Harvard clean energy project: the use of neural networks to accelerate materials discovery. E O Pyzer-Knapp, K Li, A Aspuru-Guzik, Adv. Func. Mater. 25. 41Pyzer-Knapp, E. O., Li, K. &amp; Aspuru-Guzik, A. Learning from the Harvard clean energy project: the use of neural networks to accelerate materials discovery. Adv. Func. Mater. 25.41, 6495-6502 (2015).</p>
<p>Applying machine learning techniques to predict the properties of energetic materials. D C Elton, Z Boukouvalas, M S Butrico, M D Fuge, P W Chung, Sci. Rep. 89059Elton, D. C., Boukouvalas, Z., Butrico, M. S., Fuge, M. D. &amp; Chung, P. W. Applying machine learning techniques to predict the properties of energetic materials. Sci. Rep. 8, 9059 (2018).</p>
<p>Predicting materials properties without crystal structure: deep representation learning from stoichiometry. R E A Goodall, A A Lee, Nat. Commun. 116280Goodall, R. E. A. &amp; Lee, A. A. Predicting materials properties without crystal structure: deep representation learning from stoichiometry. Nat. Commun. 11, 6280 (2020).</p>
<p>Recent advances and applications of machine learning in solid-state materials science. J Schmidt, M R G Marques, S Botti, M A L Marques, npj Comput Mater. 5Schmidt, J., Marques, M. R. G., Botti, S. &amp; Marques, M. A. L. Recent advances and applications of machine learning in solid-state materials science. npj Comput Mater. 5, 1-36 (2019).</p>
<p>Improved protein structure prediction using potentials from deep learning. A W Senior, Nature. 577Senior, A. W. et al. Improved protein structure prediction using potentials from deep learning. Nature 577, 706-710 (2020).</p>
<p>ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. J S Smith, O Isayev, A E Roitberg, Chem. Sci. 8Smith, J. S., Isayev, O. &amp; Roitberg, A. E. ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. Chem. Sci. 8, 3192-3203 (2017).</p>
<p>Demystifying multitask deep neural networks for quantitative structure-activity relationships. Y Xu, J Ma, A Liaw, R P Sheridan, V Svetnik, J. Chem. Inf. Model. 57Xu, Y., Ma, J., Liaw, A., Sheridan, R. P. &amp; Svetnik, V. Demystifying multitask deep neural networks for quantitative structure-activity relationships. J. Chem. Inf. Model. 57, 2490-2504 (2017).</p>
<p>Powerful, transferable representations for molecules through intelligent task selection in deep multitask networks. C Fare, L Turcani, E O Pyzer-Knapp, Phys. Chem. Chem. Phys. 22. 23Fare, C., Turcani, L. &amp; Pyzer-Knapp, E. O. Powerful, transferable representations for molecules through intelligent task selection in deep multitask networks. Phys. Chem. Chem. Phys. 22.23, 13041-13048 (2020).</p>
<p>Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. J M Hernández-Lobato, J Requeima, E O Pyzer-Knapp, A Aspuru-Guzik, Proc. 34th International Conference on Machine Learning. 34th International Conference on Machine Learning70Hernández-Lobato, J. M., Requeima, J., Pyzer-Knapp, E. O. &amp; Aspuru-Guzik, A. Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. In: Proc. 34th International Conference on Machine Learning-Volume 70 1470-1479 (JMLR. org, 2017).</p>
<p>Efficient and Scalable Batch Bayesian Optimization Using K-Means. M Groves, E O Pyzer-Knapp, Preprint atGroves, M. &amp; Pyzer-Knapp, E. O. Efficient and Scalable Batch Bayesian Optimization Using K-Means. Preprint at https://arxiv.org/abs/1806.01159 (2018).</p>
<p>What Is high-throughput virtual screening? a perspective from organic materials discovery. E O Pyzer-Knapp, C Suh, R Gómez-Bombarelli, J Aguilera-Iparraguirre, A Aspuru-Guzik, Annu. Rev. Mater. Res. 45Pyzer-Knapp, E. O., Suh, C., Gómez-Bombarelli, R., Aguilera-Iparraguirre, J. &amp; Aspuru-Guzik, A. What Is high-throughput virtual screening? a perspective from organic materials discovery. Annu. Rev. Mater. Res. 45, 195-216 (2015).</p>
<p>On-the-fly closed-loop materials discovery via Bayesian active learning. A G Kusne, Nat. Commun. 115966Kusne, A. G. et al. On-the-fly closed-loop materials discovery via Bayesian active learning. Nat. Commun. 11, 5966 (2020).</p>
<p>Bayesian optimization for accelerated drug discovery. E Pyzer-Knapp, IBM J. Res Dev. 62Pyzer-Knapp, E. Bayesian optimization for accelerated drug discovery. IBM J. Res Dev. 62, 2-1 (2018).</p>
<p>Using Bayesian optimization to accelerate virtual screening for the discovery of therapeutics appropriate for repurposing for COVID-19. E O Pyzer-Knapp, Preprint atPyzer-Knapp, E. O. Using Bayesian optimization to accelerate virtual screening for the discovery of therapeutics appropriate for repurposing for COVID-19. Preprint at https://arxiv.org/abs/2005.07121 (2020).</p>
<p>A Tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. E Brochu, V M Cora, N De Freitas, Preprint atBrochu, E., Cora, V. M. &amp; de Freitas, N. A Tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hier- archical reinforcement learning. Preprint at https://arxiv.org/abs/10122599 (2010).</p>
<p>Big data meets quantum chemistry approximations: the Δ-machine learning approach. R Ramakrishnan, P O Dral, M Rupp, O A Von Lilienfeld, J. Chem. Theory Comput. 11Ramakrishnan, R., Dral, P. O., Rupp, M. &amp; von Lilienfeld, O. A. Big data meets quantum chemistry approximations: the Δ-machine learning approach. J. Chem. Theory Comput. 11, 2087-2096 (2015).</p>
<p>Gaussian Processes for Machine Learning. C E Rasmussen, MIT PressRasmussen, C. E. Gaussian Processes for Machine Learning. (MIT Press, 2006).</p>
<p>Extended-connectivity fingerprints. D Rogers, M Hahn, J. Chem. Inf. Model. 50Rogers, D. &amp; Hahn, M. Extended-connectivity fingerprints. J. Chem. Inf. Model. 50, 742-754 (2010).</p>
<p>Language models are few-shot learners. T B Brown, Preprint atBrown, T. B. et al. Language models are few-shot learners. Preprint at https://arxiv. org/abs/2005.14165 (2020).</p>
<p>Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models. G L Guimaraes, B Sanchez-Lengeling, C Outeiral, P L C Farias, A Aspuru-Guzik, Preprint atGuimaraes, G. L., Sanchez-Lengeling, B., Outeiral, C., Farias, P. L. C. &amp; Aspuru-Guzik, A. Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models. Preprint at https://arxiv.org/abs/1705.10843 (2018).</p>
<p>Mol-CycleGAN: a generative model for molecular optimization. Ł Maziarka, J. Cheminformatics. 12Maziarka, Ł. et al. Mol-CycleGAN: a generative model for molecular optimization. J. Cheminformatics 12, 2 1-18 (2020).</p>
<p>Automatic chemical design using a data-driven continuous representation of molecules. R Gómez-Bombarelli, ACS Cent. Sci. 4Gómez-Bombarelli, R. et al. Automatic chemical design using a data-driven continuous representation of molecules. ACS Cent. Sci. 4, 268-276 (2018).</p>
<p>Grammar variational autoencoder. M J Kusner, B Paige, J M Hernández-Lobato, International Conference on Machine Learning. PMLRKusner, M. J., Paige, B. &amp; Hernández-Lobato, J. M. Grammar variational auto- encoder. In: International Conference on Machine Learning 1945-1954 (PMLR, 2017).</p>
<p>ChemTS: an efficient python library for de novo molecular generation. X Yang, J Zhang, K Yoshizoe, K Terayama, K Tsuda, Sci. Technol. Adv. Mater. 18Yang, X., Zhang, J., Yoshizoe, K., Terayama, K. &amp; Tsuda, K. ChemTS: an efficient python library for de novo molecular generation. Sci. Technol. Adv. Mater. 18, 972-976 (2017).</p>
<p>A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space. J H Jensen, Chem. Sci. 10Jensen, J. H. A graph-based genetic algorithm and generative model/Monte Carlo tree search for the exploration of chemical space. Chem. Sci. 10, 3567-3572 (2019).</p>
<p>Junction tree variational autoencoder for molecular graph generation. W Jin, R Barzilay, T Jaakkola, International Conference on Machine Learning. PMLRJin, W., Barzilay, R. &amp; Jaakkola, T. Junction tree variational autoencoder for molecular graph generation. In: International Conference on Machine Learning 2323-2332 (PMLR, 2018).</p>
<p>Deep generative models for molecular science. P B Jørgensen, M N Schmidt, O Winther, Mol. Inf. 371700133Jørgensen, P. B., Schmidt, M. N. &amp; Winther, O. Deep generative models for molecular science. Mol. Inf. 37, 1700133 (2018).</p>
<p>Generative models for automatic chemical design. D Schwalbe-Koda, R Gómez-Bombarelli, Machine Learning Meets Quantum Physics. ChamSpringerSchwalbe-Koda, D. &amp; Gómez-Bombarelli, R. Generative models for automatic chemical design. In Machine Learning Meets Quantum Physics. pp. 445-467 (Springer, Cham, 2020).</p>
<p>Molecule attention transformer. Ł Maziarka, arXiv:2002.08264Preprint atMaziarka, Ł. et al. Molecule attention transformer. Preprint at https://arxiv.org/ abs/arXiv:2002.08264 (2020).</p>
<p>Molecular inverse-design platform for material industries. S Takeda, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningTakeda, S. et al. Molecular inverse-design platform for material industries. Pro- ceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (2020).</p>
<p>PepCVAE: semi-supervised targeted design of antimicrobial peptide sequences. P Das, Preprint atDas, P. et al. PepCVAE: semi-supervised targeted design of antimicrobial peptide sequences. Preprint at https://arxiv.org/abs/1810.07743 (2018).</p>
<p>Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations. P Das, Nat. Biomed. Eng. 5Das, P. et al. Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations. Nat. Biomed. Eng. 5, 613-623 (2021).</p>
<p>Toward explainable anticancer compound sensitivity prediction via multimodal attention-based convolutional encoders. M Manica, Mol. Pharmaceutics. 16Manica, M. et al. Toward explainable anticancer compound sensitivity prediction via multimodal attention-based convolutional encoders. Mol. Pharmaceutics 16, 4797-4806 (2019).</p>
<p>PaccMann: a web service for interpretable anticancer compound sensitivity prediction. J Cadow, J Born, M Manica, A Oskooei, M Rodríguez Martínez, Nucleic Acids Res. 48Cadow, J., Born, J., Manica, M., Oskooei, A. &amp; Rodríguez Martínez, M. PaccMann: a web service for interpretable anticancer compound sensitivity prediction. Nucleic Acids Res. 48, W502-W508 (2020).</p>
<p>Molecule generation experience: an open platform of material design for public users. S Takeda, Preprint atTakeda, S. et al. Molecule generation experience: an open platform of material design for public users. Preprint at https://arxiv.org/abs/2108.03044 (2021).</p>
<p>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. P Schwaller, ACS Cent. Sci. 59Schwaller, P. et al. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS Cent. Sci. 5(9), 1572-1583 (2019).</p>
<p>Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. P Schwaller, Chem. Sci. 11Schwaller, P. et al. Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. Chem. Sci. 11, 3316-3325 (2020).</p>
<p>Inferring experimental procedures from text-based representations of chemical reactions. A C Vaucher, Nat. Commun. 12Vaucher, A. C. et al. Inferring experimental procedures from text-based repre- sentations of chemical reactions. Nat. Commun. 12, 2573-11 (2021).</p>
<p>Convergence of multiple synthetic paradigms in a universally programmable chemical synthesis machine. D Angelone, Nat. Chem. 13Angelone, D. et al. Convergence of multiple synthetic paradigms in a universally programmable chemical synthesis machine. Nat. Chem. 13, 63-69 (2021).</p>
<p>A robotic platform for flow synthesis of organic compounds informed by AI planning. C W Coley, Science. 3651566Coley, C. W. et al. A robotic platform for flow synthesis of organic compounds informed by AI planning. Science 365, 6453 (2019): eaax1566 (2019).</p>
<p>A mobile robotic chemist. B Burger, Nature. 583Burger, B. et al. A mobile robotic chemist. Nature 583, 237-241 (2020).</p>
<p>Found in Translation': predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models. P Schwaller, T Gaudin, D Lányi, C Bekas, T Laino, Chem. Sci. 9Schwaller, P., Gaudin, T., Lányi, D., Bekas, C. &amp; Laino, T. 'Found in Translation': predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models. Chem. Sci. 9, 6091-6098 (2018).</p>
<p>H Ito, Microlithography. Molecular Imprinting. Springer Berlin HeidelbergIto, H. In Microlithography. Molecular Imprinting 37-245 (Springer Berlin Heidel- berg, 2005).</p>
<p>Recent progress in photo-acid generators for advanced photopolymer materials. T Tsuchimura, J. Photopolym. Sci. Technol. 33Tsuchimura, T. Recent progress in photo-acid generators for advanced photo- polymer materials. J. Photopolym. Sci. Technol. 33, 15-26 (2020).</p>
<p>Photoacid generators. Application and current state of development. N A Kuznetsova, G V Malkov, B G Gribov, Russ. Chem. Rev. 89Kuznetsova, N. A., Malkov, G. V. &amp; Gribov, B. G. Photoacid generators. Application and current state of development. Russ. Chem. Rev. 89, 173-190 (2020).</p>
<p>Recent advances and challenges in the design of organic photoacid and photobase generators for polymerizations. N Zivic, Angew. Chem. Int. Ed. 58Zivic, N. et al. Recent advances and challenges in the design of organic photoacid and photobase generators for polymerizations. Angew. Chem. Int. Ed. 58, 10410-10422 (2019).</p>
<p>The discovery and development of onium salt cationic photoinitiators. J V Crivello, J. Polym. Sci. A Polym. Chem. 37Crivello, J. V. The discovery and development of onium salt cationic photo- initiators. J. Polym. Sci. A Polym. Chem. 37, 4241-4254 (1999).</p>
<p>Photoinitiated cationic polymerization with triarylsulfonium salts. J V Crivello, J H W Lam, J. Polym. Sci. A: Polym. Chem. 17Crivello, J. V. &amp; Lam, J. H. W. Photoinitiated cationic polymerization with triar- ylsulfonium salts. J. Polym. Sci. A: Polym. Chem. 17, 977-999 (1979).</p>
<p>Diaryliodonium salts. a new class of photoinitiators for cationic polymerization. J V Crivello, J H W Lam, Macromolecules. 10Crivello, J. V. &amp; Lam, J. H. W. Diaryliodonium salts. a new class of photoinitiators for cationic polymerization. Macromolecules 10, 1307-1315 (1977).</p>
<p>Increased regulatory scrutiny of photolithography chemistries: the need for science and innovation (Conference Presentation). B Tvermoes, D Speed, 10.1117/12.2516159Advances in Patterning Materials and Processes XXXVI. Gronheid, R. &amp; Sanders, D. P.) (SPIETvermoes, B. &amp; Speed, D. Increased regulatory scrutiny of photolithography chemistries: the need for science and innovation (Conference Presentation). In: Advances in Patterning Materials and Processes XXXVI (eds. Gronheid, R. &amp; Sanders, D. P.) (SPIE, 2019). https://doi.org/10.1117/12.2516159.</p>
<p>Bioconcentration potential and microbial toxicity of onium cations in photoacid generators. X.-Z Niu, Environ. 28Niu, X.-Z. et al. Bioconcentration potential and microbial toxicity of onium cations in photoacid generators. Environ 28, 8915-8921 (2021).</p>
<p>Photochemical fate of sulfonium photoacid generator cations under photolithography relevant UV irradiation. X.-Z Niu, J. Photochem. Photobiol. A. 416113324Niu, X.-Z. et al. Photochemical fate of sulfonium photoacid generator cations under photolithography relevant UV irradiation. J. Photochem. Photobiol. A 416, 113324 (2021).</p>
<p>Triphenylsulfonium topophotochemistry. E Despagnet-Ayoub, Photochem. Photobiol. Sci. 17Despagnet-Ayoub, E. et al. Triphenylsulfonium topophotochemistry. Photochem. Photobiol. Sci. 17, 27-34 (2018).</p>
<p>Density-functional theory for time-dependent systems. E Runge, E K Gross, Phys. Rev. Lett. 52997Runge, E. &amp; Gross, E. K. Density-functional theory for time-dependent systems. Phys. Rev. Lett. 52, 997 (1984).</p>
<p>Recent developments in the general atomic and molecular electronic structure system. G M J Barca, J. Chem. Phys. 152154102Barca, G. M. J. et al. Recent developments in the general atomic and molecular electronic structure system. J. Chem. Phys. 152, 154102 (2020).</p>
<p>OPERA models for predicting physicochemical properties and environmental fate endpoints. K Mansouri, C M Grulke, R S Judson, A J Williams, J. Cheminformatics. 1010Mansouri, K., Grulke, C. M., Judson, R. S. &amp; Williams, A. J. OPERA models for predicting physicochemical properties and environmental fate endpoints. J. Cheminformatics 10, 10 (2018).</p>
<p>Expert-in-the-loop AI for polymer discovery. P Ristoski, 10.1145/3340531.3416020Proc. 29th ACM International Conference on Information &amp; Knowledge Management. 29th ACM International Conference on Information &amp; Knowledge ManagementACMRistoski, P. et al. Expert-in-the-loop AI for polymer discovery. In: Proc. 29th ACM International Conference on Information &amp; Knowledge Management (ACM, 2020). https://doi.org/10.1145/3340531.3416020.</p>
<p>Palladium-catalyzed borylation of aryl sulfoniums with diborons. H Minami, S Otsuka, K Nogi, H Yorimitsu, ACS Catal. 8Minami, H., Otsuka, S., Nogi, K. &amp; Yorimitsu, H. Palladium-catalyzed borylation of aryl sulfoniums with diborons. ACS Catal. 8, 579-583 (2017).</p>
<p>Redox-neutral borylation of aryl sulfonium salts via C-S activation enabled by light. C Huang, Org. Lett. 21Huang, C. et al. Redox-neutral borylation of aryl sulfonium salts via C-S activation enabled by light. Org. Lett. 21, 9688-9692 (2019).</p>
<p>Finding unprecedentedly low-thermal-conductivity half-Heusler semiconductors via high-throughput materials modeling. J Carrete, W Li, N Mingo, S Wang, S Curtarolo, Phys. Rev. X. 411019Carrete, J., Li, W., Mingo, N., Wang, S. &amp; Curtarolo, S. Finding unprecedentedly low-thermal-conductivity half-Heusler semiconductors via high-throughput materials modeling. Phys. Rev. X 4, 011019 (2014).</p>
<p>OpenShift for Developers: A Guide for Impatient Beginners. G Shipley, G Dumpleton, O'Reilly Media, IncShipley, G. &amp; Dumpleton, G. OpenShift for Developers: A Guide for Impatient Beginners (O'Reilly Media, Inc., 2016).</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3346 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3346</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3346</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-2b333a51c829681c41f0879a3c0241ea8ff559a7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2b333a51c829681c41f0879a3c0241ea8ff559a7" target="_blank">Unstructured and structured data: Can we have the best of both worlds with large language models?</a></p>
                <p><strong>Paper Venue:</strong> IEEE Data Engineering Bulletin</p>
                <p><strong>Paper TL;DR:</strong> An opinion on the potential of using large language models to query on both unstructured and structured data and some research challenges related to the topic of building question-answering systems for both types of data are presented.</p>
                <p><strong>Paper Abstract:</strong> This paper presents an opinion on the potential of using large language models to query on both unstructured and structured data. It also outlines some research challenges related to the topic of building question-answering systems for both types of data.</p>
                <p><strong>Cost:</strong> 0.002</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3346",
    "paper_id": "paper-2b333a51c829681c41f0879a3c0241ea8ff559a7",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00190025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Unstructured and structured data: Can we have the best of both worlds with large language models?</h1>
<p>Wang-Chiew Tan*<br>Reality Labs Research, Meta<br>wangchiew@meta.com</p>
<h2>1 Introduction</h2>
<p>We are witnessing rapid advancements in the area of large language models (LLMs). A search on Google Scholar shows there are about 3,910 papers with "large language models" in the titles of the papers in 2022. As of April 12, 2023, there are already 1700 articles with LLMs in their titles. In addition to Google Scholar, we are also witnessing huge volumes of blog posts, news articles, twitter feeds, and open-source repositories around LLMs that have sprung up in recent months.</p>
<p>Perhaps ChatGPT (released on November 30, 2022) is the epitome of this LLM revolution that truly unleashed and showcased, to the masses, the power of what has been brewing in the natural language and machine learning communities in recent years. There are many things ChatGPT ${ }^{1}$ can do and does so impressively by generating intelligent human-like responses to your questions. Through natural language as input and output ${ }^{2}$, it can solve non-trival mathematical problems, to a certain extent $[4,21]$, translate your specification into code in a programming language of your choice (e.g., $[11,21]$ ), help you write proses in different styles and the list of accolades goes on [18].</p>
<p>In addition to its ability to answer questions, one can also prompt it with tables, such as CSV files, and ask questions over the tables in natural language. More interestingly, it is also possible to prompt ChatGPT with both text and tables and ask questions over the two types of information seamlessly. A little more perseverance</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>in this exercise quickly reveals that ChatGPT has a limit on how much one can input with each prompt and how much information it will retain, at least based on my experience when I tried this at the end of March 2023.</p>
<p>It is natural to wonder whether we can use ChatGPT, with some extensions, as a system for storing and querying data, with natural language as the primary medium of input and output, which it excels at. What are the challenges of doing so, and how can we, as database practitioners and theoreticians, make progress in this context?</p>
<h1>2 Unstructured and structured data</h1>
<p>A lot of data, including text, images, audio, and videos, sits "outside the box" today. Often, such unstructured data contain multiple modalities simultaneously. For example, we often find text in images [12], and we may also find text associated with videos and/or images on the web. Unstructured data is prevalent to a large extent because it is easily authored and shared by users [8] through a variety of apps and authoring tools that are widely available. Such data is often queried with keyword search and today, they can also be queried in natural language with LLMs. Typically the cost of devising a schema and setting up a database for querying the data inhibits the use of a database management system (DBMS) upfront. However, as data scales, the need for structure and semantics becomes more critical, so as to enable faster and more accurate retrieval of content. For example, organizing photos by year, trips, or entity types (such as people or pets) adds some structure, which makes answering certain types of queries much faster and more accurate. However, answering complex queries such as "when was the last time I went to the coffee shop beside restaurant Italio?" or "how many times did I celebrate Anna's birthday with a mango cake?" requires non-trivial reasoning and computation over the data that goes beyond the capabilities of LLMs today [1, 22].</p>
<p>Enterprise data sits on the other end of the spectrum. It is, for the most part, not authored by everyday users and is highly structured, often sitting "in a box" in some DBMS. Enterprise data comes with a well-devised schema and is typically highly optimized to serve a sizable query workload with great efficiency. Such data is often queried directly with SQL which is adequately expressive for specifying complex queries such as those with aggregates and recursion. Significant research has also been carried out to enable querying a database with natural language (e.g., $[17,20]$ ), where questions are posed in natural language and translated into SQL, which can then be executed over the DBMS. A DBMS is highly optimized to handle large amounts of data and can also perform transactions with ACID guarantees [6]. However, the core dbms does not understand natural lan-</p>
<p>guage and it tends to fall short in its ability to query unstructured data, which are often stored as blobs and adding semantics to the blobs require additional effort. For example, "show me the sales numbers over Black Friday and Cyber Monday last year" requires commonsense knowledge on what "sales numbers" mean and how that maps to relevant table attribute(s), which may be stored under different names in different databases, and when Black Friday and Cyber Monday occurred. Another example is "find all items with good reviews that are similar to these images," which requires matching images (semantically) and interpreting what "good reviews" mean based on the data.</p>
<p>Despite the divide in how unstructured and structured data are managed today, the desire to query in natural language is common to both. At the same time, it is unreasonable and unnatural to expect all unstructured data to fit in some structure, or vice versa to leverage one system for querying. So, how can we effectively query both types of data with the help of LLMs, which possess tremendous knowledge and language understanding?</p>
<h1>3 The best of both worlds with LLMs?</h1>
<p>LLMs contain tremendous parametric knowledge in their model parameters but lack the ability to incorporate external data (i.e., data outside their model). Hence, if a model is trained based on data up to, say, 2021, it will not provide correct answers about events or facts that require knowledge after 2021. For example, if someone asks the question "How were the midterm election results of 2022?" on the webpage with text-davinci-003 [15], the answer returned is "The midterm election results of 2022 are not yet available, as the election has not yet taken place." This is because text-davinci-003 is trained with data up to June 2021.</p>
<p>Retrieval-augmented language models (e.g., [5, 10, 25]) overcome this limitation by adopting a semi-parametric approach to answering queries. They use external data, by first retrieving relevant data from an external data store, and then attempts to answer a question conditioned on the retrieved data and with their parametric knowledge. However, as demonstrated by (Table 5, [1]), such systems can still perform poorly on complex queries involving aggregates and certain types of temporal queries. This is because, for such queries, oftentimes, it is impossible to fit all necessary data for answering the question into the finite-length token input imposed by language models. However, as LLMs get even larger or as more advances are made to increase the token limits imposed by language models, one can anticipate that LLMs will take larger and larger inputs in future and the finitelength token limit may no longer be an issue soon. At the same time, it is likely</p>
<p>that there will be even larger datasets to manage and an even larger set of data is required for computing the right answers. So this problem will persist, at least for a while.</p>
<p>A proposal to overcome the above limitation for some types of queries is described in [2]. The paper presents a vision of using views (e.g., as tables) to structure portions of the underlying data sources. The data sources may be of different modalities, such as text, images, videos, or even tables. Views are used to surface important properties about data or associations between data of different modalities and LLMs are used to translate natural language queries into queries (e.g., SQL in this case) that can be executed over the views whenever possible. With this proposal, a key question is to understand when a natural language query can be answered with views. If a query cannot be answered using views, the system falls back to retrieval-augmented language models to answer the query to its best effort. Alternatively, the two components (view-based and retrieval-based query answering components) can also collaborate to produce a final answer. There are several other questions raised in the paper, such as what views should be materialized? How can one automatically select the "right" views to materialize given an anticipated query workload? And how can one decide when a question is better answered with views, the retrieval system, or even both?</p>
<p>In addition to the above, I will highlight below what I believe are some of the more pertinent questions that may be of immediate interest to the database community:
Query answering with different resources and budget constraints The topic of finding a good query plan to answer an given query was already discussed in [2]. The core of that system relies on two components - views and a retrievalaugmented language model for answering queries. It is conceivable to augment the system with additional components, such as one that generates images given a natural language prompt, which may sometimes be useful for answering certain queries ${ }^{3}$. A key question then becomes how do we understand when to leverage which component for answering a query or have the components collaborate to derive an answer? Furthermore, LLMs are compute-intensive and can be slow in generating an answer. Some LLMs are also not free. In addition to deciding how best to answer a query with all the available resources, how can one account for the strengths and limitations of each component to enumerate and compare plans for computing an answer to a given query and/or under a given budget? Can we also use a language model to generate a query plan or some parts of it, similar to how language models have been used to self-reason a sequence of steps to derive answers from questions (e.g., [23, 24])?</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Provenance Provenance is well-studied for certain classes of SQL queries, and is roughly defined as the source tuples that explains why a tuple is in the result of the query. As mentioned in [2], one should attempt to answer queries using table views whenever possible by translating the natural language query into a SQL query that can be executed over the views. This way, it is possible that provenance can be obtained "for free". However, SQL queries that are generated by LLMs can be complex, for example, with nested SQL queries and/or aggregates in the FROM or WHERE clauses. For such cases, can we decompose the generated SQL query into a sequence of one or more "simpler queries" instead, where the provenance for simpler queries is well-understood and can be derived easily? If this is not possible, can we strategize a plan for answering the query in a different way so that provenance can be derived? The problem of finding an alternative query plan is related to the discussion in the earlier paragraph, but here the focus is on deriving a plan with sufficiently simple steps to enable provenance.</p>
<p>Retrieval-augmented language models also provide more guarantees for providing evidence for their answers. We are also beginning to witness implementations where sources of answers presented by retrieval-augmented language models are returned as part of the answers [13]. However, more research needs to be carried out to attribute provenance to training data (e.g., $[3,9,16]$ ) to form a more comprehensive picture of provenance for the provided answer.
Prompt Engineering Analysis LLMs have limits on the number of tokens they can take as input. Even if one takes advantage of all the tokens one can use, prompting them with more information does not always translate to better answers as sometimes, presenting the language model with more information confuses the language model. This means one needs to be judicious in what we send to a language model for it to derive a correct answer of high quality ${ }^{4}$.</p>
<p>The answers returned by language models are also sensitive to how they are prompted. Sometimes the same question phrased slightly differently will result in completely different answers. In prompt engineering, the goal is to find the best prompt for the task at hand.</p>
<p>Given the maximum token limit of language models, can we optimize the answers returned by a language model by strategically summarizing relevant data and/or removing irrelevant data? For example, the entity matching system of [14], which uses a language model, immediately performs entity matching more accurately when text descriptions of data are strategically shortened using a simple trick; by keeping only tokens of value, with high TD/IDF. On a more theoretical side, given a suitable definition of what is a prompt and assumptions about language models, can we characterize what types of queries that require access to</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>external data can be answered with one prompt, a finite number of prompts, or an asymptotic number of prompts under a budget of tokens?</p>
<h1>4 Conclusion</h1>
<p>The field of LLMs is moving fast, both in research and industry. In addition to [2], [7] have also described the challenges of answering queries, in the context of augmented language models and data integration, with a single source or by chaining multiple sources. In [19], the authors described how a DBMS can be extended to leverage LLMs to improve query answering and also pointed to the direction of a hybrid query answering system involving both a DBMS and LLMs. I believe this is only the beginning and we will see many more visions, research, and implementations soon in this area of query answering systems that embrace both unstructured and structured data through the use of large language models.</p>
<p>Acknowledgements Many of the ideas above are inspired from discussions with Alon Halevy and Yuliang Li. I also thank many of my colleagues at Meta Lambert Mathias, Richard Newcombe, and Luna Dong - for active discussions around large language models from which I have learnt lots and also to Lucian Popa for his feedback on this article.</p>
<h2>References</h2>
<p>[1] To be released.
[2] To be released.
[3] E. Akyurek, T. Bolukbasi, F. Liu, B. Xiong, I. Tenney, J. Andreas, and K. Guu. Towards tracing knowledge in language models back to the training data. In EMNLP, pages 2429-2446, Dec. 2022.
[4] S. Frieder, L. Pinchetti, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz, P. C. Petersen, A. Chevalier, and J. Berner. Mathematical Capabilities of ChatGPT, 2023.
[5] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. Realm: Retrieval-augmented language model pre-training. In ICML, 2020.
[6] T. Haerder and A. Reuter. Principles of transaction-oriented database recovery. ACM Comput. Surv., 15(4):287-317, dec 1983.
[7] A. Y. Halevy and J. Dwivedi-Yu. Learnings from data integration for augmented language models. CoRR, abs/2304.04576, 2023.
[8] A. Y. Halevy, O. Etzioni, A. Doan, Z. G. Ives, J. Madhavan, L. K. McDowell, and I. Tatarinov. Crossing the Structure Chasm. In CIDR, 2003.</p>
<p>[9] X. Han, B. C. Wallace, and Y. Tsvetkov. Explaining black box predictions and unveiling data artifacts through influence functions. In ACL, pages 5553-5563, July 2020.
[10] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave. Few-shot Learning with Retrieval Augmented Language Models. 2022.
[11] A. Kashefi and T. Mukerji. ChatGPT for Programming Numerical Methods, 2023.
[12] D. Kiela, H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, and D. Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In NeurIPS, 2020.
[13] LangChain. Retrieval Question Answering with Sources. https://python.langchain. com/en/latest/modules/chains/index_examples/vector_db_qa_with_sources.html.
[14] Y. Li, J. Li, Y. Suhara, A. Doan, and W. Tan. Deep entity matching with pre-trained language models. Proc. VLDB Endow., 14(1):50-60, 2020.
[15] OpenAI. text-davinci-003. cited on April 12, 2023, Playground at https://platform. openai.com/playground, info on training data at https://help.openai.com/en/articles/ 6643408-how-do-davinci-and-text-davinci-003-differ.
[16] G. Pruthi, F. Liu, S. Kale, and M. Sundararajan. Estimating training data influence by tracing gradient descent. In NeurIPS, volume 33, pages 19920-19930, 2020.
[17] J. Qi, J. Tang, Z. He, X. Wan, Y. Cheng, C. Zhou, X. Wang, Q. Zhang, and Z. Lin. RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL. In EMNLP, 2022.
[18] K. Rose. How Should I Use A.I. Chatbots Like ChatGPT? New York Times (Mar 30, 2023) https://www.nytimes.com/2023/03/30/technology/ ai-chatbot-chatgpt-uses-work-life.html.
[19] M. Saeed, N. D. Cao, and P. Papotti. Querying Large Language Models with SQL, 2023.
[20] T. Scholak, N. Schucher, and D. Bahdanau. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In EMNLP, pages 98959901, Nov. 2021.
[21] P. Shakarian, A. Koyyalamudi, N. Ngu, and L. Mareedu. An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP), 2023.
[22] J. Thorne, M. Yazdani, M. Saeidi, F. Silvestri, S. Riedel, and A. Y. Levy. From Natural Language Processing to Neural Databases. VLDB Endow., 14(6):1033-1039, 2021.
[23] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. In NeurIPS, 2022.</p>
<p>[24] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In ICLR, 2023.
[25] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang, M. Lewis, L. Zettlemoyer, and W. tau Yih. Retrieval-augmented multimodal language modeling, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ There are varied ways to answer a question correctly. Some are better than others.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
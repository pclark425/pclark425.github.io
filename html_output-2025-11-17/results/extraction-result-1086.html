<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1086 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1086</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1086</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-6999fd72868c4044e852c43a040a87a43d03ab3a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6999fd72868c4044e852c43a040a87a43d03ab3a" target="_blank">Prioritized Level Replay</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work introduces Prioritized Level Replay, a general framework for estimating the future learning potential of a level given the current state of the agent's policy, and finds that temporal-difference errors, while previously used to selectively sample past transitions, also prove effective for scoring a level's futurelearning potential in generating entire episodes that an agent would experience when replaying it.</p>
                <p><strong>Paper Abstract:</strong> Simulated environments with procedurally generated content have become popular benchmarks for testing systematic generalization of reinforcement learning agents. Every level in such an environment is algorithmically created, thereby exhibiting a unique configuration of underlying factors of variation, such as layout, positions of entities, asset appearances, or even the rules governing environment transitions. Fixed sets of training levels can be determined to aid comparison and reproducibility, and test levels can be held out to evaluate the generalization and robustness of agents. While prior work samples training levels in a direct way (e.g.~uniformly) for the agent to learn from, we investigate the hypothesis that different levels provide different learning progress for an agent at specific times during training. We introduce Prioritized Level Replay, a general framework for estimating the future learning potential of a level given the current state of the agent's policy. We find that temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level's future learning potential in generating entire episodes that an agent would experience when replaying it. We report significantly improved sample-efficiency and generalization on the majority of Procgen Benchmark environments as well as two challenging MiniGrid environments. Lastly, we present a qualitative analysis showing that Prioritized Level Replay induces an implicit curriculum, taking the agent gradually from easier to harder levels.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1086.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1086.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO+PLR (Procgen)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal Policy Optimization agent trained with Prioritized Level Replay on Procgen Benchmark (easy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated RL agent using PPO with GAE whose environment-sampling is controlled by Prioritized Level Replay (PLR), adaptively prioritizing previously-seen levels with high estimated learning potential (L1 value loss / GAE magnitude). Evaluated on Procgen Benchmark (16 procedurally-generated games) with 25M training steps on 200 training levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO+PLR agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy trained with Proximal Policy Optimization (PPO) using Generalized Advantage Estimation (GAE); level sampling replaced by PLR which scores levels by average absolute GAE (L1 value loss) and samples via rank-prioritized distribution mixed with a staleness distribution (staleness coefficient rho).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Procgen Benchmark (16 procedurally generated games; easy setting)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A suite of 16 procedurally-generated 2D game environments. Each episode level is generated from a seed and varies in layout, assets/visuals, and sometimes dynamics. Observations are 64x64x3 RGB images; discrete 15-dimensional action space. In the easy evaluation: training on 200 fixed training levels, testing on unseen levels sampled from the full level distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Environment complexity characterized per-game by varied layout and dynamics; concrete metrics reported include observation size (64x64x3), action-space dimensionality (15 discrete actions), number of distinct games (16). Training protocol uses 25M environment interaction steps on 200 training levels. Complexity also operationalized via per-level 'difficulty' as reflected in episodic returns and value errors.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high (varies by game; visual + control complexity across 16 games)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural variation measured by number of distinct levels: training set size N_train = 200 (easy); test levels drawn from the full procedural distribution (effectively very large). PLR also experimented with buffered/top-M tracking to handle unbounded level pools. Variation arises from random seeds changing layout, assets, initial states, and dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (wide procedural variation across levels; large/unbounded test distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized mean test return (per Cobbe et al. normalization), unnormalized episodic test returns, and generalization gap (train minus test returns). Statistical significance assessed by Welch t-test (p=0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Mean normalized test returns (averaged over all games): PLR (rank prioritization, beta=0.1, rho=0.1) = 128.3% ± 5.8 relative to uniform baseline set to 100.0% ± 4.5. Combined method UCB-DrAC + PLR = 176.4% ± 6.1. PLR produced statistically significant test improvements on 10 of 16 games (easy). Training used 25M steps on 200 levels.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly links environment complexity and variation via the Value Correction Hypothesis: in sparse or threshold difficulty levels (complex but solvable), replaying levels with high value-error (L1 value loss / large GAE magnitude) yields transferable learning that expands the agent's capability to harder levels, producing an implicit curriculum (sampling shifts from easier to harder levels). PLR's prioritized sampling trades off focusing on levels with high learning potential (improves generalization and sample efficiency) against risk of score 'staleness' (off-policy drift), which is mitigated by mixing in a staleness distribution (rho); alternative scoring choices and extreme settings (rho=0 or rho=1) degrade results. The method therefore ties variation (which levels exist and are sampled) to effective exploitation of complexity (learning on threshold-complex levels) to improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Adaptive curriculum via Prioritized Level Replay (PLR) — selective replay of previously-seen levels by estimated learning potential (L1 value loss / GAE magnitude) with staleness-aware mixing; training algorithm otherwise PPO with GAE. Baseline comparison: uniform level sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>PLR significantly improved generalization relative to uniform sampling: on Procgen easy, mean normalized test returns improved from 100% (uniform) to 128.3% (PLR); PLR reduced the mean generalization gap across games and achieved statistically significant test gains on 10/16 games. Combining PLR with image-augmentation (UCB-DrAC) gave further gains (+76% over PPO uniform).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained for 25 million environment steps on 200 training levels (easy). PLR improved sample efficiency in many games (faster achievement of higher test returns) and often matched or improved training returns, though on some games PLR reduced training sample efficiency when it did not discover a useful curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adaptive level-sampling (PLR) based on L1 value loss (GAE magnitude) yields emergent curricula that prioritize increasingly-difficult but learnable levels, improving both sample efficiency and generalization on Procgen Benchmark; rank-based prioritization with a small temperature (beta=0.1) and nonzero staleness mixing (rho≈0.1) worked best. Alternative score functions and extreme staleness settings performed worse or inconsistently across games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prioritized Level Replay', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1086.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1086.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO (Uniform baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal Policy Optimization agent with uniform level sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard PPO agent with GAE trained by uniformly sampling training levels (direct level sampling). Used as the primary baseline for PLR experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Proximal policy optimization algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO (uniform-sampling) agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PPO with GAE, standard experience collection via uniform sampling over the fixed training level set (direct level sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Procgen Benchmark (easy) and MiniGrid (various)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same procedurally-generated environments as used with PLR: Procgen (16 games, visual + dynamics variation) and MiniGrid (gridworld tasks with discrete difficulty bins). Under uniform sampling, training episodes uniformly sample from the finite training set (e.g., 200 levels for Procgen easy).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>As above: observation dimensionality (64x64x3 for Procgen), action-space 15, per-level difficulty reflected in episodic returns; training on N_train = 200 (Procgen easy) or thousands of levels for MiniGrid.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high (varies by environment)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distinct training levels (e.g., 200 for Procgen easy; thousands for MiniGrid experiments); full test distribution for Procgen is effectively much larger than training set.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (procedural generation across many possible seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized mean test return, unnormalized episodic test returns, generalization gap.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline normalized mean test returns set to 100.0% ± 4.5 (Procgen easy). Per-environment absolute returns listed in Table 2 of the paper (e.g., BigFish uniform mean 3.7 ± 1.2).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Implicitly present: uniform sampling does not adapt to level difficulty/complexity or to the agent's evolving capabilities, which can reduce sample efficiency and lead to larger generalization gaps compared to adaptive strategies like PLR.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Direct level sampling (uniform over training set).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Serves as reference: uniform sampling produced lower normalized mean test returns and larger mean generalization gaps than PLR in the reported experiments (Procgen easy baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained for the same budget as PLR (e.g., 25M steps on Procgen easy).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Uniform sampling is a common default but can be suboptimal for generalization in procedurally-generated environments; PLR's adaptive sampling provided consistent improvements over uniform in average test returns and generalization gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prioritized Level Replay', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1086.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1086.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO+PLR (MiniGrid)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal Policy Optimization agent trained with Prioritized Level Replay on MiniGrid (MultiRoom and ObstructedMazeGamut)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PPO agent with PLR applied to MiniGrid gridworlds that have discrete difficulty levels; PLR induced an emergent curriculum that shifted sampling from easier to harder difficulty bins and improved generalization and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO+PLR agent (MiniGrid)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PPO with GAE; PLR scores full-episode trajectories by average absolute GAE (L1 value loss) and samples levels from a rank-prioritized distribution mixed with a staleness distribution. CNN policy architecture (3-layer CNN) used for MiniGrid.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MiniGrid (MultiRoom-N4-Random, ObstructedMazeGamut-Easy / Medium)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Symbolic grid-world tasks with clear, discrete difficulty bins (e.g., number of rooms to traverse, additional steps like moving obstacles or uncovering keys). Observations are full-grid integer encodings (modified to full-grid N x M x 3). Levels are procedurally generated by seed; training sets used in experiments: MultiRoom-N4-Random: 4000 training levels; ObstructedMazeGamut-Easy: 3000 levels; ObstructedMazeGamut-Medium: 6000 levels.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity captured by discrete difficulty bins (e.g., MultiRoom requires traversing 1–4 rooms; ObstructedMaze adds objectives like finding keys, moving obstacles). Quantitative training-level counts used: 3000–6000 training levels per environment; PPO rollout length 256, training steps sufficient to observe curriculum emergence (updates on the order of thousands).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>ranging from low (1-room) to high (multi-room with obstacles); ObstructedMazeGamut-Medium = high within this suite</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation measured by number of training levels (3k–6k) and by discrete difficulty bins (levels uniformly distributed across difficulty settings). Also considered full distribution experiments (very large number of possible levels, e.g., ~4 billion for some MiniGrid full distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (experimentally: thousands of training levels; full distribution extremely large)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Unnormalized episodic test return; normalized test return (per Procgen normalization for comparability); tracking of probability mass over difficulty bins (to show curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Normalized test returns (MiniGrid aggregate): PLR = 124.3% ± 4.7 relative to uniform baseline 100.0% ± 2.5. Per-environment: MultiRoom-N4-Random final test return: uniform 0.80 ± 0.04 vs PLR 0.81 ± 0.01; ObstructedMazeGamut-Easy: uniform 0.53 ± 0.04 vs PLR 0.85 ± 0.04 (substantial improvement). PLR trained with rho=0.3 worked best on MiniGrid.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — because MiniGrid has explicit discrete difficulty bins, PLR demonstrates how prioritizing levels with large value errors yields an implicit curriculum: early training concentrates on easier bins and progressively shifts probability mass toward harder bins as the agent improves, supporting the Value Correction Hypothesis. The paper shows that PLR trained on a small fraction of the full distribution (e.g., 6000 levels vs ~4 billion total) can outperform uniform sampling on the full distribution when it leverages the emergent curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Adaptive sampling / implicit curriculum via PLR with rank prioritization (beta=0.1) and staleness mixing (rho=0.3); PPO with 3-layer CNN policy for MiniGrid.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>PLR significantly improved generalization in the two MiniGrid environments tested: large improvement in ObstructedMazeGamut-Easy (0.53 → 0.85 test return) and modest improvement in MultiRoom-N4-Random. PLR induced a curriculum that prioritized progressively harder levels and in some cases achieved better performance than uniform sampling on the full (very large) level distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>MiniGrid experiments used training sets of ~3k–6k levels and training until convergence (PPO settings with rollout length 256; number of steps not identically reported per environment but sufficient to observe curriculum emergence). PLR improved sample efficiency on these discrete difficulty environments, discovering useful curricula within a few thousand updates (PLR struggled for ~4k updates on one environment before discovering a useful curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In discrete-difficulty gridworlds, PLR using L1 value loss as score produces an emergent curriculum that shifts sampling to harder levels as agent competence increases, substantially improving generalization and sample efficiency (notably on ObstructedMazeGamut). A nonzero staleness mixing coefficient (rho) is important to prevent score staleness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prioritized Level Replay', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1086.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1086.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robotic manipulation (PCG example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robotic arm object-manipulation tasks conceptualized as procedural content generation (PCG) levels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper mentions robotic object-manipulation tasks as an illustrative, real-world example of problems that fit the PCG framework: each 'level' corresponds to a unique combination of block positions, arm state, and target formation; PLR would be applicable to prioritize training instances in such domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>robotic arm (conceptual example)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Physical or simulated robotic manipulator tasked with object manipulation (e.g., stacking), learning via reinforcement learning; not empirically evaluated in this paper but described as a domain that conforms to the PCG abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent (physical or simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Robotic object-manipulation (conceptual PCG mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Each level is defined by a unique combination of initial block positions, initial arm state, and target formation; complexity arises from combinatorial object configurations and contact dynamics; variation arises from randomizing initial states and goal specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Conceptual metrics: number of objects, combinatorial initial states, degrees of freedom of the manipulator, contact dynamics complexity; no experimental quantitative measures provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>potentially high (real-world manipulation is high-dimensional and contact-rich), but not empirically quantified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Level variation described as different initial block positions, arm initial states, and target formation permutations; degree of domain randomization not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (conceptual claim)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Mentioned qualitatively: PCG framing (many varied level instances) is applicable to robotic tasks, implying that methods like PLR that adaptively select instances by learning potential could improve sample efficiency and generalization in such high-complexity, high-variation domains; no empirical evaluation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Conceptual mapping: apply PLR-style prioritized replay over procedural instances / domain randomization; no empirical protocol reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper claims the PCG abstraction (and thus PLR) generalizes beyond games to real problems like robotic manipulation, where levels correspond to randomized task instances; this is a conceptual extension rather than an experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prioritized Level Replay', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Leveraging Procedural Generation to Benchmark Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Minimalistic gridworld environment for OpenAI Gym <em>(Rating: 1)</em></li>
                <li>Teacher-student curriculum learning <em>(Rating: 2)</em></li>
                <li>Emergent complexity and zero-shot transfer via unsupervised environment design <em>(Rating: 2)</em></li>
                <li>POET: open-ended coevolution of environments and their optimized solutions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1086",
    "paper_id": "paper-6999fd72868c4044e852c43a040a87a43d03ab3a",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "PPO+PLR (Procgen)",
            "name_full": "Proximal Policy Optimization agent trained with Prioritized Level Replay on Procgen Benchmark (easy)",
            "brief_description": "A simulated RL agent using PPO with GAE whose environment-sampling is controlled by Prioritized Level Replay (PLR), adaptively prioritizing previously-seen levels with high estimated learning potential (L1 value loss / GAE magnitude). Evaluated on Procgen Benchmark (16 procedurally-generated games) with 25M training steps on 200 training levels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO+PLR agent",
            "agent_description": "Policy trained with Proximal Policy Optimization (PPO) using Generalized Advantage Estimation (GAE); level sampling replaced by PLR which scores levels by average absolute GAE (L1 value loss) and samples via rank-prioritized distribution mixed with a staleness distribution (staleness coefficient rho).",
            "agent_type": "simulated agent",
            "environment_name": "Procgen Benchmark (16 procedurally generated games; easy setting)",
            "environment_description": "A suite of 16 procedurally-generated 2D game environments. Each episode level is generated from a seed and varies in layout, assets/visuals, and sometimes dynamics. Observations are 64x64x3 RGB images; discrete 15-dimensional action space. In the easy evaluation: training on 200 fixed training levels, testing on unseen levels sampled from the full level distribution.",
            "complexity_measure": "Environment complexity characterized per-game by varied layout and dynamics; concrete metrics reported include observation size (64x64x3), action-space dimensionality (15 discrete actions), number of distinct games (16). Training protocol uses 25M environment interaction steps on 200 training levels. Complexity also operationalized via per-level 'difficulty' as reflected in episodic returns and value errors.",
            "complexity_level": "medium-high (varies by game; visual + control complexity across 16 games)",
            "variation_measure": "Procedural variation measured by number of distinct levels: training set size N_train = 200 (easy); test levels drawn from the full procedural distribution (effectively very large). PLR also experimented with buffered/top-M tracking to handle unbounded level pools. Variation arises from random seeds changing layout, assets, initial states, and dynamics.",
            "variation_level": "high (wide procedural variation across levels; large/unbounded test distribution)",
            "performance_metric": "Normalized mean test return (per Cobbe et al. normalization), unnormalized episodic test returns, and generalization gap (train minus test returns). Statistical significance assessed by Welch t-test (p=0.05).",
            "performance_value": "Mean normalized test returns (averaged over all games): PLR (rank prioritization, beta=0.1, rho=0.1) = 128.3% ± 5.8 relative to uniform baseline set to 100.0% ± 4.5. Combined method UCB-DrAC + PLR = 176.4% ± 6.1. PLR produced statistically significant test improvements on 10 of 16 games (easy). Training used 25M steps on 200 levels.",
            "complexity_variation_relationship": "Yes — the paper explicitly links environment complexity and variation via the Value Correction Hypothesis: in sparse or threshold difficulty levels (complex but solvable), replaying levels with high value-error (L1 value loss / large GAE magnitude) yields transferable learning that expands the agent's capability to harder levels, producing an implicit curriculum (sampling shifts from easier to harder levels). PLR's prioritized sampling trades off focusing on levels with high learning potential (improves generalization and sample efficiency) against risk of score 'staleness' (off-policy drift), which is mitigated by mixing in a staleness distribution (rho); alternative scoring choices and extreme settings (rho=0 or rho=1) degrade results. The method therefore ties variation (which levels exist and are sampled) to effective exploitation of complexity (learning on threshold-complex levels) to improve generalization.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Adaptive curriculum via Prioritized Level Replay (PLR) — selective replay of previously-seen levels by estimated learning potential (L1 value loss / GAE magnitude) with staleness-aware mixing; training algorithm otherwise PPO with GAE. Baseline comparison: uniform level sampling.",
            "generalization_tested": true,
            "generalization_results": "PLR significantly improved generalization relative to uniform sampling: on Procgen easy, mean normalized test returns improved from 100% (uniform) to 128.3% (PLR); PLR reduced the mean generalization gap across games and achieved statistically significant test gains on 10/16 games. Combining PLR with image-augmentation (UCB-DrAC) gave further gains (+76% over PPO uniform).",
            "sample_efficiency": "Trained for 25 million environment steps on 200 training levels (easy). PLR improved sample efficiency in many games (faster achievement of higher test returns) and often matched or improved training returns, though on some games PLR reduced training sample efficiency when it did not discover a useful curriculum.",
            "key_findings": "Adaptive level-sampling (PLR) based on L1 value loss (GAE magnitude) yields emergent curricula that prioritize increasingly-difficult but learnable levels, improving both sample efficiency and generalization on Procgen Benchmark; rank-based prioritization with a small temperature (beta=0.1) and nonzero staleness mixing (rho≈0.1) worked best. Alternative score functions and extreme staleness settings performed worse or inconsistently across games.",
            "uuid": "e1086.0",
            "source_info": {
                "paper_title": "Prioritized Level Replay",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "PPO (Uniform baseline)",
            "name_full": "Proximal Policy Optimization agent with uniform level sampling",
            "brief_description": "Standard PPO agent with GAE trained by uniformly sampling training levels (direct level sampling). Used as the primary baseline for PLR experiments.",
            "citation_title": "Proximal policy optimization algorithms",
            "mention_or_use": "use",
            "agent_name": "PPO (uniform-sampling) agent",
            "agent_description": "PPO with GAE, standard experience collection via uniform sampling over the fixed training level set (direct level sampling).",
            "agent_type": "simulated agent",
            "environment_name": "Procgen Benchmark (easy) and MiniGrid (various)",
            "environment_description": "Same procedurally-generated environments as used with PLR: Procgen (16 games, visual + dynamics variation) and MiniGrid (gridworld tasks with discrete difficulty bins). Under uniform sampling, training episodes uniformly sample from the finite training set (e.g., 200 levels for Procgen easy).",
            "complexity_measure": "As above: observation dimensionality (64x64x3 for Procgen), action-space 15, per-level difficulty reflected in episodic returns; training on N_train = 200 (Procgen easy) or thousands of levels for MiniGrid.",
            "complexity_level": "medium-high (varies by environment)",
            "variation_measure": "Number of distinct training levels (e.g., 200 for Procgen easy; thousands for MiniGrid experiments); full test distribution for Procgen is effectively much larger than training set.",
            "variation_level": "high (procedural generation across many possible seeds)",
            "performance_metric": "Normalized mean test return, unnormalized episodic test returns, generalization gap.",
            "performance_value": "Baseline normalized mean test returns set to 100.0% ± 4.5 (Procgen easy). Per-environment absolute returns listed in Table 2 of the paper (e.g., BigFish uniform mean 3.7 ± 1.2).",
            "complexity_variation_relationship": "Implicitly present: uniform sampling does not adapt to level difficulty/complexity or to the agent's evolving capabilities, which can reduce sample efficiency and lead to larger generalization gaps compared to adaptive strategies like PLR.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Direct level sampling (uniform over training set).",
            "generalization_tested": true,
            "generalization_results": "Serves as reference: uniform sampling produced lower normalized mean test returns and larger mean generalization gaps than PLR in the reported experiments (Procgen easy baseline).",
            "sample_efficiency": "Trained for the same budget as PLR (e.g., 25M steps on Procgen easy).",
            "key_findings": "Uniform sampling is a common default but can be suboptimal for generalization in procedurally-generated environments; PLR's adaptive sampling provided consistent improvements over uniform in average test returns and generalization gap.",
            "uuid": "e1086.1",
            "source_info": {
                "paper_title": "Prioritized Level Replay",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "PPO+PLR (MiniGrid)",
            "name_full": "Proximal Policy Optimization agent trained with Prioritized Level Replay on MiniGrid (MultiRoom and ObstructedMazeGamut)",
            "brief_description": "PPO agent with PLR applied to MiniGrid gridworlds that have discrete difficulty levels; PLR induced an emergent curriculum that shifted sampling from easier to harder difficulty bins and improved generalization and sample efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO+PLR agent (MiniGrid)",
            "agent_description": "PPO with GAE; PLR scores full-episode trajectories by average absolute GAE (L1 value loss) and samples levels from a rank-prioritized distribution mixed with a staleness distribution. CNN policy architecture (3-layer CNN) used for MiniGrid.",
            "agent_type": "simulated agent",
            "environment_name": "MiniGrid (MultiRoom-N4-Random, ObstructedMazeGamut-Easy / Medium)",
            "environment_description": "Symbolic grid-world tasks with clear, discrete difficulty bins (e.g., number of rooms to traverse, additional steps like moving obstacles or uncovering keys). Observations are full-grid integer encodings (modified to full-grid N x M x 3). Levels are procedurally generated by seed; training sets used in experiments: MultiRoom-N4-Random: 4000 training levels; ObstructedMazeGamut-Easy: 3000 levels; ObstructedMazeGamut-Medium: 6000 levels.",
            "complexity_measure": "Complexity captured by discrete difficulty bins (e.g., MultiRoom requires traversing 1–4 rooms; ObstructedMaze adds objectives like finding keys, moving obstacles). Quantitative training-level counts used: 3000–6000 training levels per environment; PPO rollout length 256, training steps sufficient to observe curriculum emergence (updates on the order of thousands).",
            "complexity_level": "ranging from low (1-room) to high (multi-room with obstacles); ObstructedMazeGamut-Medium = high within this suite",
            "variation_measure": "Variation measured by number of training levels (3k–6k) and by discrete difficulty bins (levels uniformly distributed across difficulty settings). Also considered full distribution experiments (very large number of possible levels, e.g., ~4 billion for some MiniGrid full distributions).",
            "variation_level": "medium-high (experimentally: thousands of training levels; full distribution extremely large)",
            "performance_metric": "Unnormalized episodic test return; normalized test return (per Procgen normalization for comparability); tracking of probability mass over difficulty bins (to show curriculum).",
            "performance_value": "Normalized test returns (MiniGrid aggregate): PLR = 124.3% ± 4.7 relative to uniform baseline 100.0% ± 2.5. Per-environment: MultiRoom-N4-Random final test return: uniform 0.80 ± 0.04 vs PLR 0.81 ± 0.01; ObstructedMazeGamut-Easy: uniform 0.53 ± 0.04 vs PLR 0.85 ± 0.04 (substantial improvement). PLR trained with rho=0.3 worked best on MiniGrid.",
            "complexity_variation_relationship": "Yes — because MiniGrid has explicit discrete difficulty bins, PLR demonstrates how prioritizing levels with large value errors yields an implicit curriculum: early training concentrates on easier bins and progressively shifts probability mass toward harder bins as the agent improves, supporting the Value Correction Hypothesis. The paper shows that PLR trained on a small fraction of the full distribution (e.g., 6000 levels vs ~4 billion total) can outperform uniform sampling on the full distribution when it leverages the emergent curriculum.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Adaptive sampling / implicit curriculum via PLR with rank prioritization (beta=0.1) and staleness mixing (rho=0.3); PPO with 3-layer CNN policy for MiniGrid.",
            "generalization_tested": true,
            "generalization_results": "PLR significantly improved generalization in the two MiniGrid environments tested: large improvement in ObstructedMazeGamut-Easy (0.53 → 0.85 test return) and modest improvement in MultiRoom-N4-Random. PLR induced a curriculum that prioritized progressively harder levels and in some cases achieved better performance than uniform sampling on the full (very large) level distribution.",
            "sample_efficiency": "MiniGrid experiments used training sets of ~3k–6k levels and training until convergence (PPO settings with rollout length 256; number of steps not identically reported per environment but sufficient to observe curriculum emergence). PLR improved sample efficiency on these discrete difficulty environments, discovering useful curricula within a few thousand updates (PLR struggled for ~4k updates on one environment before discovering a useful curriculum).",
            "key_findings": "In discrete-difficulty gridworlds, PLR using L1 value loss as score produces an emergent curriculum that shifts sampling to harder levels as agent competence increases, substantially improving generalization and sample efficiency (notably on ObstructedMazeGamut). A nonzero staleness mixing coefficient (rho) is important to prevent score staleness.",
            "uuid": "e1086.2",
            "source_info": {
                "paper_title": "Prioritized Level Replay",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Robotic manipulation (PCG example)",
            "name_full": "Robotic arm object-manipulation tasks conceptualized as procedural content generation (PCG) levels",
            "brief_description": "Paper mentions robotic object-manipulation tasks as an illustrative, real-world example of problems that fit the PCG framework: each 'level' corresponds to a unique combination of block positions, arm state, and target formation; PLR would be applicable to prioritize training instances in such domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "robotic arm (conceptual example)",
            "agent_description": "Physical or simulated robotic manipulator tasked with object manipulation (e.g., stacking), learning via reinforcement learning; not empirically evaluated in this paper but described as a domain that conforms to the PCG abstraction.",
            "agent_type": "robotic agent (physical or simulated)",
            "environment_name": "Robotic object-manipulation (conceptual PCG mapping)",
            "environment_description": "Each level is defined by a unique combination of initial block positions, initial arm state, and target formation; complexity arises from combinatorial object configurations and contact dynamics; variation arises from randomizing initial states and goal specifications.",
            "complexity_measure": "Conceptual metrics: number of objects, combinatorial initial states, degrees of freedom of the manipulator, contact dynamics complexity; no experimental quantitative measures provided in the paper.",
            "complexity_level": "potentially high (real-world manipulation is high-dimensional and contact-rich), but not empirically quantified in this paper",
            "variation_measure": "Level variation described as different initial block positions, arm initial states, and target formation permutations; degree of domain randomization not quantified here.",
            "variation_level": "high (conceptual claim)",
            "performance_metric": null,
            "performance_value": null,
            "complexity_variation_relationship": "Mentioned qualitatively: PCG framing (many varied level instances) is applicable to robotic tasks, implying that methods like PLR that adaptively select instances by learning potential could improve sample efficiency and generalization in such high-complexity, high-variation domains; no empirical evaluation provided.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Conceptual mapping: apply PLR-style prioritized replay over procedural instances / domain randomization; no empirical protocol reported.",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Paper claims the PCG abstraction (and thus PLR) generalizes beyond games to real problems like robotic manipulation, where levels correspond to randomized task instances; this is a conceptual extension rather than an experiment.",
            "uuid": "e1086.3",
            "source_info": {
                "paper_title": "Prioritized Level Replay",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Minimalistic gridworld environment for OpenAI Gym",
            "rating": 1
        },
        {
            "paper_title": "Teacher-student curriculum learning",
            "rating": 2
        },
        {
            "paper_title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "rating": 2
        },
        {
            "paper_title": "POET: open-ended coevolution of environments and their optimized solutions",
            "rating": 2
        }
    ],
    "cost": 0.019948,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Prioritized Level Replay</h1>
<h2>Minqi Jiang ${ }^{12}$ Edward Grefenstette ${ }^{12}$ Tim Rocktäschel ${ }^{12}$</h2>
<h4>Abstract</h4>
<p>Environments with procedurally generated content serve as important benchmarks for testing systematic generalization in deep reinforcement learning. In this setting, each level is an algorithmically created environment instance with a unique configuration of its factors of variation. Training on a prespecified subset of levels allows for testing generalization to unseen levels. What can be learned from a level depends on the current policy, yet prior work defaults to uniform sampling of training levels independently of the policy. We introduce Prioritized Level Replay (PLR), a general framework for selectively sampling the next training level by prioritizing those with higher estimated learning potential when revisited in the future. We show TD-errors effectively estimate a level's future learning potential and, when used to guide the sampling procedure, induce an emergent curriculum of increasingly difficult levels. By adapting the sampling of training levels, PLR significantly improves sample efficiency and generalization on Procgen Benchmark—matching the previous state-of-the-art in test return-and readily combines with other methods. Combined with the previous leading method, PLR raises the state-of-the-art to over $76 \%$ improvement in test return relative to standard RL baselines.</p>
<h2>1. Introduction</h2>
<p>Deep reinforcement learning (RL) easily overfits to training experiences, making generalization a key open challenge to widespread deployment of these methods. Procedural content generation (PCG) environments have emerged as a promising class of problems with which to probe and address this core weakness (Risi \&amp; Togelius, 2020; ChevalierBoisvert et al., 2018; Cobbe et al., 2020a; Juliani et al., 2019;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Zhong et al., 2020; Küttler et al., 2020). Unlike singleton environments, like the Arcade Learning Environment games (Bellemare et al., 2013), PCG environments take on algorithmically created configurations at the start of each training episode, potentially varying the layout, asset appearances, or even game dynamics. Each environment instance generated this way is called a level. In mapping an identifier, such as a random seed, to each level, PCG environments allow us to measure a policy's generalization to held-out test levels. In this paper, we assume only a blackbox generation process that returns a level given an identifier. We avoid the strong assumption of control over the generation procedure itself, explored by several prior works (see Section 6). Further, we assume levels follow common latent dynamics, so that in aggregate, experiences collected in individual levels reveal general rules governing the entire set of levels.</p>
<p>Despite its humble origin in games, the PCG abstraction proves far-reaching: Many control problems, such as teaching a robotic arm to stack blocks in a specific formation, easily conform to PCG. Here, each level may consist of a unique combination of initial block positions, arm state, and target formation. In a vastly different domain, Hanabi (Bard et al., 2020) also conforms to PCG, where levels map to initial deck orderings. These examples illustrate the generality of the PCG abstraction: Most challenging RL problems entail generalizing across instances (or levels) differing along some underlying factors of variation and thereby can be aptly framed as PCG. This highlights the importance of effective deep RL methods for PCG environments.</p>
<p>Many techniques have been proposed to improve generalization in the PCG setting (see Section 6), requiring changes to the model architecture, learning algorithm, observation space, or environment structure. Notably, these approaches default to uniform sampling of training levels. We instead hypothesize that the variation across levels implies that at each point of training, each level likely holds different potential for an agent to learn about the structure shared across levels to improve generalization. Inspired by this insight and selective sampling methods from active learning, we investigate whether sampling the next training level weighed by its learning potential can improve generalization.</p>
<p>We introduce Prioritized Level Replay (PLR), illustrated in Figure 1, a method for sampling training levels that exploits</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of Prioritized Level Replay. The next level is either sampled from a distribution with support over unseen levels (top), which could be the environment's (perhaps implicit) full training-level distribution, or alternatively, sampled from the replay distribution, which prioritizes levels based on future learning potential (bottom). In either case, a trajectory $\tau$ is sampled from the next level and used to update the replay distribution. This update depends on the lists of previously seen levels $\Lambda_{\text {seen }}$, their latest estimated learning potentials $S$, and last sampled timestamps $C$.
the differences in learning potential among levels to improve both sample efficiency and generalization. PLR selectively samples the next training level based on an estimated learning potential of replaying each level anew. During training, our method updates scores estimating each level's learning potential as a function of the agent's policy and temporaldifference (TD) errors collected along the last trajectory sampled on that level. Our method then samples the next training level from a distribution derived from a normalization procedure over these level scores. PLR makes no assumptions about how the policy is updated, so it can be used in tandem with any RL algorithm and combined with many other methods such as data augmentation. Our method also does not assume any external, predefined ordering of levels by difficulty or other criteria, but instead derives level scores dynamically during training based on how the policy interacts with the environment. The only requirements are as follows-satisfied by almost any problem that can be framed as PCG, including RL environments implemented as seeded simulators: (i) Some notion of "level" exists, such that levels follow common latent dynamics; (ii) such levels can be sampled from the environment in an identifiable way; and (iii) given a level identifier, the environment can be set to that level to collect new experiences from it.</p>
<p>While previous works in off-policy RL devised effective methods to directly reuse past experiences for training (Schaul et al., 2016; Andrychowicz et al., 2017), PLR uses past experiences to inform the collection of future experiences by estimating how much replaying each level anew will benefit learning. Our method can thus be seen as a forward-view variation of prioritized experience replay, and an online counterpart to this off-policy method.</p>
<p>This paper makes the following contributions ${ }^{1}$ : (i) We in-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>troduce a computationally-efficient algorithm for sampling levels during training based on an estimate of the future learning potential of collecting new experiences from each level; (ii) we show our method significantly improves generalization on 10 of 16 environments in Procgen Benchmark and two challenging MiniGrid environments; (iii) we demonstrate our method combines with the previous leading method to set a new state-of-the-art on Procgen Benchmark; and (iv) we show our method induces an implicit curriculum over training levels in sparse reward settings.</p>
<h2>2. Background</h2>
<p>In this paper, we refer to a PCG environment as any computational process that, given a level identifier (e.g. an index or a random seed), generates a level, defined as an environment instance exhibiting a unique configuration of its underlying factors of variation, such as layout, asset appearances, or specific environment dynamics (Risi \&amp; Togelius, 2020). For example, MiniGrid's MultiRoom environment instantiates mazes with varying numbers of rooms based on the seed (Chevalier-Boisvert et al., 2018). We refer to sampling a new trajectory generated from the agent's latest policy acting on a given level $l$ as replaying that level $l$.</p>
<p>The level diversity of PCG environments makes them useful testbeds for studying the robustness and generalization ability of RL agents, measured by agent performance on unseen test levels. The standard test evaluation protocol for PCG environments consists of training the agent on a finite number of training levels $\Lambda_{\text {train }}$, and evaluating performance on unseen test levels $\Lambda_{\text {test }}$, drawn from the set of all levels. Training levels are sampled from an arbitrary distribution $P_{\text {train }}\left(l \mid \Lambda_{\text {train }}\right)$. We call this training process direct level sampling. A common variation of this protocol sets $\Lambda_{\text {train }}$ to the set of all levels, though in practice, the agent will still only effectively see a finite set of levels after training for a finite</p>
<p>number of steps. In the case of a finite training set, typically $P_{\text {train }}\left(l \mid \Lambda_{\text {train }}\right)=\operatorname{Uniform}\left(l ; \Lambda_{\text {train }}\right)$. See Appendix C for the pseudocode outlining this procedure.</p>
<p>PCG environments naturally lend themselves to curriculum learning. Prior works have shown that directly altering levels to match their difficulty to the agent's abilities can improve generalization (Justesen et al., 2018; Dennis et al., 2020; Chevalier-Boisvert et al., 2018; Zhong et al., 2020). These findings further suggest the levels most useful for improving an agent's policy vary throughout the course of training. In this work, we consider how to automatically discover a curriculum that improves generalization for a general blackbox PCG environment-crucially, without assuming any knowledge or control of how levels are generated (beyond providing the random seed or other indicial level identifier).</p>
<h2>3. Prioritized Level Replay</h2>
<p>In this section, we present Prioritized Level Replay (PLR), an algorithm for selectively sampling the next training level given the current policy, by prioritizing levels with higher estimated learning potential when replayed (that is, revisited). PLR is a drop-in replacement for the experience-collection process used in a wide range of RL algorithms. Algorithm 1 shows how it is straightforward to incorporate PLR into a generic policy-gradient training loop. For clarity, we focus on training on batches of complete trajectories (see Appendix C for pseudocode of PLR with $T$-step rollouts).</p>
<p>Our method, illustrated in Figure 1 and fully specified in Algorithm 2, induces a dynamic, nonparametric sampling distribution $P_{\text {replay }}\left(l \mid \Lambda_{\text {seen }}\right)$ over previously visited training levels $\Lambda_{\text {seen }}$ that prioritizes visited levels with higher learning potential based on properties of the agent's past trajectories. We refer to $P_{\text {replay }}\left(l \mid \Lambda_{\text {seen }}\right)$ as the replay distribution. Throughout training, our method updates this replay distribution according to a heuristic score, assigning greater weight to visited levels with higher future learning potential. Using dynamic arrays $S$ and $C$ of equal length to $\Lambda_{\text {seen }}$, PLR tracks level scores $S_{i} \in S$ for each visited training level $l_{i}$ based on the latest episode trajectory on $l_{i}$, as well as the episode count $C_{i} \in C$ at which each level $l_{i} \in \Lambda_{\text {seen }}$ was last sampled. Our method updates $P_{\text {replay }}$ after each terminated episode by computing a mixture of two distributions, $P_{S}$, based on the level scores, and $P_{C}$, based on how long ago each level was last sampled:</p>
<p>$$
P_{\text {replay }}=(1-\rho) \cdot P_{S}+\rho \cdot P_{C}
$$</p>
<p>where the staleness coefficient $\rho \in[0,1]$ is a hyperparameter. We discuss how we compute level scores $S_{i}$, parameterizing the scoring distribution $P_{S}$, and the staleness distribution $P_{C}$, in Sections 3.1 and 3.2, respectively.</p>
<p>PLR chooses the next level at the start of every training episode by first sampling a replay-decision from a</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Policy</span><span class="o">-</span><span class="n">gradient</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">loop</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">PLR</span>
<span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">levels</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">Lambda_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">train</span><span class="w"> </span><span class="p">}}</span>\<span class="p">),</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">pi_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="p">),</span>
<span class="w">    </span><span class="n">policy</span><span class="w"> </span><span class="n">update</span><span class="w"> </span><span class="n">function</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">U</span><span class="p">}(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">},</span><span class="w"> </span>\<span class="n">theta</span><span class="p">)</span><span class="w"> </span>\<span class="n">rightarrow</span><span class="w"> </span>\<span class="n">theta</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">size</span><span class="w"> </span>\<span class="p">(</span><span class="n">N_</span><span class="p">{</span><span class="n">b</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">scores</span><span class="w"> </span>\<span class="p">(</span><span class="n">S</span>\<span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">timestamps</span><span class="w"> </span>\<span class="p">(</span><span class="n">C</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">global</span><span class="w"> </span><span class="n">episode</span><span class="w"> </span><span class="n">counter</span><span class="w"> </span>\<span class="p">(</span><span class="n">c</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ordered</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">visited</span><span class="w"> </span><span class="n">levels</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">Lambda_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">seen</span><span class="w"> </span><span class="p">}}</span><span class="o">=</span>\<span class="n">varnothing</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">experience</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">}</span><span class="o">=</span>\<span class="n">varnothing</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="n">collecting</span><span class="w"> </span><span class="n">experiences</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">}</span><span class="w"> </span>\<span class="n">cup</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">collect</span><span class="p">}</span><span class="w"> </span><span class="o">.</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">experiences</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">Lambda_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">train</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span>\<span class="n">Lambda_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">seen</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span>\<span class="n">pi_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">},</span><span class="w"> </span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">c</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">                                    </span><span class="n">Using</span><span class="w"> </span><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span>
<span class="w">        </span><span class="n">end</span><span class="w"> </span><span class="k">while</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">theta</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">U</span><span class="p">}(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">},</span><span class="w"> </span>\<span class="n">theta</span><span class="p">)</span><span class="w"> </span>\<span class="n">quad</span>\<span class="p">)</span><span class="w"> </span><span class="n">Update</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">collected</span><span class="w"> </span><span class="n">experiences</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">while</span>
</code></pre></div>

<p>Algorithm 2 Experience collection with PLR
Input: Training levels $\Lambda_{\text {train }}$, visited levels $\Lambda_{\text {seen }}$, policy $\pi$, global level scores $S$, global level timestamps $C$, and global episode counter $c$.
Output: A sampled trajectory $\tau$
$c \leftarrow c+1$
Sample replay decision $d \sim P_{D}(d)$
if $d=0$ and $\left|\Lambda_{\text {train }} \backslash \Lambda_{\text {seen }}\right|&gt;0$ then
Define new index $i \leftarrow|S|+1$
Sample $l_{i} \sim P_{\text {new }}\left(l \mid \Lambda_{\text {train }}, \Lambda_{\text {seen }}\right) \quad$ Sample an unseen level Add $l_{i}$ to $\Lambda_{\text {seen }}$
Add initial value $S_{i}=0$ to $S$ and $C_{i}=0$ to $C$
else
Sample $l_{i} \sim(1-\rho) \cdot P_{S}\left(l \mid \Lambda_{\text {seen }}, S\right)+\rho \cdot P_{C}\left(l \mid \Lambda_{\text {seen }}, C, c\right)$
Sample a level for replay
end if
Sample $\tau \sim P_{\pi}\left(\tau \mid l_{i}\right)$
Update score $S_{i} \leftarrow \operatorname{score}(\tau, \pi)$ and timestamp $C_{i} \leftarrow c$
Bernoulli (or similar) distribution $P_{D}(d)$ to determine whether to replay a level sampled from the replay distribution $P_{\text {replay }}\left(l \mid \Lambda_{\text {seen }}\right)$ or to experience a new, unseen level from $\Lambda_{\text {train }}$, according to some distribution $P_{\text {new }}\left(l \mid \Lambda_{\text {train }}\right.$. $\Lambda_{\text {seen }}$ ). In practice, for the case of a finite number of training levels, we implement $P_{\text {new }}$ as a uniform distribution over the remaining unseen levels. For the case of a countably infinite number of training levels, we simulate $P_{\text {new }}$ by sampling levels from $P_{\text {train }}$ until encountering an unseen level. In our experiments based on a finite number of training levels, we opt to naturally anneal $P_{D}(d=1)$ as $\left|\Lambda_{\text {seen }}\right| /\left|\Lambda_{\text {train }}\right|$, so replay occurs more often as more training levels are visited.</p>
<p>The following sections describes how Prioritized Level Replay updates the replay distribution $P_{\text {replay }}\left(l \mid \Lambda_{\text {seen }}\right)$, namely through level scoring and staleness-aware prioritization.</p>
<h3>3.1. Scoring Levels for Learning Potential</h3>
<p>After collecting each complete episode trajectory $\tau$ on level $l_{i}$ using policy $\pi$, our method assigns $l_{i}$ a score $S_{i}=\operatorname{score}(\tau, \pi)$ measuring the learning potential of replaying $l_{i}$ in the future. We employ a function of the TDerror at timestep $t, \delta_{t}=r_{t}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)$, as a proxy</p>
<p>for this learning potential. The expectation of the TD-error over next states is equivalent to the advantage estimate, and therefore higher-magnitude TD-errors imply greater discrepancy between expected and actual returns, making $\delta_{t}$ a useful measure of the learning potential in revisiting a particular state transition. To prioritize the learning potential of future experiences resulting from replaying a level, we use a scoring function based on the average magnitude of the Generalized Advantage Estimate (GAE; Schulman et al., 2016) over each of $T$ time steps in the latest trajectory $\tau$ from that level:</p>
<p>$$
S_{i}=\operatorname{score}(\tau, \pi)=\frac{1}{T} \sum_{t=0}^{T}\left|\sum_{k=t}^{T}(\gamma \lambda)^{k-t} \delta_{k}\right|
$$</p>
<p>While the GAE at time $t$ is most commonly expressed as the discounted sum of all 1-step TD-errors starting at $t$ as in Equation 2, it is equivalent to an exponentially-discounted sum of all $k$-step TD-errors from $t$, with discount factor $\lambda$. By considering all $k$-step TD-errors, the GAE mitigates the bias introduced by the bootstrap term in 1-step TD-errors. The discount factor $\lambda$ then controls the trade-off between bias and variance. Our scoring function considers the absolute value of the GAE, as we assume the learning potential grows with the magnitude of the TD-error irrespective of its sign. This also avoids opposite signed errors canceling out.</p>
<p>Another useful interpretation of Equation 2 comes from observing that the GAE magnitude at $t$ is equivalent to the L1 value loss $\left|\hat{V}<em t="t">{t}-V</em>$ ), as done in state-of-the-art implementations of PPO (Schulman et al., 2017) used in our experiments. Unless otherwise indicated, PLR refers to the instantiation of our algorithm with L1 value loss as the scoring function.}\right|$ under a policy-gradient algorithm that uses GAE for its own advantage estimates (and therefore value targets $\hat{V}_{t</p>
<p>We further formulate the Value Correction Hypothesis to motivate our approach: In sparse reward settings, prioritizing the sampling of training levels with greatest average absolute value loss leads to a curriculum that improves both sample efficiency and generalization. We reason that on threshold levels (i.e. those at the limit of the agent's current abilities) the agent will see non-stationary returns (or value targets) - and therefore incur relatively high value errorsuntil it learns to solve them consistently. In contrast, levels beyond the agent's current abilities tend to result in stationary value targets signaling failure and therefore low value errors, until the agent learns useful, transferable behaviors from threshold levels. Prioritizing levels by value loss then naturally guides the agent along the expanding threshold of its ability-without the need for any externally provided measure of difficulty. We believe that learning behaviors systematically aligned with the inherent complexities of the environment in this way may lead to better generalization, and will seek to verify this empirically in Section 5.2.</p>
<p>While we provide principled motivations for our specific choice of scoring function, we emphasize that in general, the scoring function can be any approximation of learning potential based on trajectory values. Note that candidate scoring functions should asymptotically decrease with frequency of level visitation to avoid mode collapse of $P_{\text {replay }}$ to a limited set of levels and possible overfitting. In Section 4, we compare our choice of the GAE magnitude, or equivalently, the L1 value loss, to alternative TD-error-based and uncertainty-based scoring approaches, listed in Table 1.</p>
<p>Given level scores, we use normalized outputs of a prioritization function $h$ evaluated over these scores and tuned using a temperature parameter $\beta$ to define the score-prioritized distribution $P_{S}\left(\Lambda_{\text {train }}\right)$ over the training levels, under which</p>
<p>$$
P_{S}\left(l_{i} \mid \Lambda_{\text {seen }}, S\right)=\frac{h\left(S_{i}\right)^{1 / \beta}}{\sum_{j} h\left(S_{j}\right)^{1 / \beta}}
$$</p>
<p>The function $h$ defines how differences in level scores translate into differences in prioritization. The temperature parameter $\beta$ allows us to tune how much $h(S)$ ultimately determines the resulting distribution. We make the design choice of using rank prioritization, for which $h\left(S_{i}\right)=1 / \operatorname{rank}\left(S_{i}\right)$, where $\operatorname{rank}\left(S_{i}\right)$ is the rank of level score $S_{i}$ among all scores sorted in descending order. We also experimented with proportional prioritization $\left(h\left(S_{i}\right)=S_{i}\right)$ as well as greedy prioritization (the level with the highest score receives probability 1 ), both of which tend to perform worse.</p>
<h3>3.2. Staleness-Aware Prioritization</h3>
<p>As the scores used to parameterize $P_{S}$ are a function of the state of the policy at the time the associated level was last played, they come to reflect a gradually more off-policy measure the longer they remain without an update through replay. We mitigate this drift towards "off-policy-ness" by explicitly mixing the sampling distribution with a stalenessprioritized distribution $P_{C}$ :</p>
<p>$$
P_{C}\left(l_{i} \mid \Lambda_{\text {seen }}, C, c\right)=\frac{c-C_{i}}{\sum_{C_{j} \in C} c-C_{j}}
$$</p>
<p>which assigns probability mass to each level $l_{i}$ in proportion to the level's staleness $c-C_{i}$. Here, $c$ is the count of total episodes sampled so far in training and $C_{i}$ (referred to as the level's timestamp) is the episode count at which $l_{i}$ was last sampled. By pushing support to levels with staler scores, $P_{C}$ ensures no score drifts too far off-policy.</p>
<p>Plugging Equations 3 and 4 into Equation 1 gives us a replay distribution that is calculated as
$P_{\text {replay }}\left(l_{i}\right)=(1-\rho) \cdot P_{S}\left(l_{i} \mid \Lambda_{\text {seen }}, S\right)+\rho \cdot P_{C}\left(l_{i} \mid \Lambda_{\text {seen }}, C, c\right)$.
Thus, a level has a greater chance of being sampled when its score is high or it has not been sampled for a long time.</p>
<h2>4. Experimental Setting</h2>
<p>We evaluate PLR on several PCG environments with various combinations of scoring functions and prioritization schemes, and compare to the most common direct level sampling baseline of $P_{\text {train }}\left(l \mid \Lambda_{\text {train }}\right)=\operatorname{Uniform}\left(l ; \Lambda_{\text {train }}\right)$. We train and test on all 16 environments in the Procgen Benchmark on easy and hard difficulties, but focus discussion on the easy results, which allow direct comparison to several prior studies. We compare to UCB-DrAC (Raileanu et al., 2021), the state-of-the-art image augmentation method on this benchmark, and mixreg (Wang et al., 2020a), a recently introduced data augmentation method. We also compare to TSCL Window (Matiisen et al., 2020), which resembles PLR with an alternative scoring function using the slope of recent returns and no staleness sampling. For fair comparison, we also evaluate a custom TSCL Window variant that mixes in the staleness distribution $P_{C}$ weighted by $\rho&gt;0$. Further, to demonstrate the ease of combining PLR with other methods, we evaluate UCB-DrAC using PLR for sampling training levels. Finally, we test the Value Correction Hypothesis on two challenging MiniGrid environments.</p>
<p>We measure episodic test returns per game throughout training, as well as the performance of the final policy over 100 unseen test levels of each game relative to PPO with uniform sampling. We also evaluate the mean normalized episodic test return and mean generalization gap, averaged over all games ( 10 runs each). We normalize returns according to Cobbe et al. (2019) and compute the generalization gap as train returns minus test returns. Thus, a larger gap indicates more overfitting, making it an apt measure of generalization. We assess statistical significance at $p=0.05$, using the Welch t-test.</p>
<p>In line with the standard baseline for these environments, all experiments use PPO with GAE for training. For Procgen, we use the same ResBlock architecture as Cobbe et al. (2020a) and train for 25M total steps on 200 levels on the easy setting as in the original baselines. For MiniGrid, we use a 3-layer CNN architecture based on Igl et al. (2019), and provide approximately 1000 levels of each difficulty per environment during training. Detailed descriptions of the environments, architectures, and hyperparameters used in our experiments (and how they were set or obtained) can be found in Appendix A. See Table 1 for the full set of scoring functions investigated in our experiments.</p>
<p>Additionally, we extend PLR to support training on an unbounded number of levels by tracking a rolling, finite buffer of the top levels so far encountered by learning potential. Appendix B. 3 reports the results of this extended PLR algorithm when training on the full level distribution of the MiniGrid environments featured in our main experiments.</p>
<p>Table 1. Scoring functions investigated in this work.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Scoring function</th>
<th style="text-align: left;">$\operatorname{score}(\tau, \pi)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Policy entropy</td>
<td style="text-align: left;">$\frac{1}{T} \sum_{t=0}^{T} \sum_{a} \pi\left(a, s_{t}\right) \log \pi\left(a, s_{t}\right)$</td>
</tr>
<tr>
<td style="text-align: left;">Policy min-margin</td>
<td style="text-align: left;">$\frac{1}{T} \sum_{t=0}^{T}\left(\max <em t="t">{a} \pi\left(a, s</em>\right)-\right.$</td>
</tr>
<tr>
<td style="text-align: left;">Policy least-confidence</td>
<td style="text-align: left;">$\left.\frac{1}{T} \sum_{t=0}^{T}\left(1-\max <em t="t">{a} \pi\left(a, s</em>\right)\right)\right)$</td>
</tr>
<tr>
<td style="text-align: left;">1-step TD error</td>
<td style="text-align: left;">$\frac{1}{T} \sum_{t=0}^{T}\left</td>
</tr>
<tr>
<td style="text-align: left;">GAE</td>
<td style="text-align: left;">$\frac{1}{T} \sum_{t=0}^{T} \sum_{k=t}^{T}(\gamma \lambda)^{k-t} \delta_{k}$</td>
</tr>
<tr>
<td style="text-align: left;">GAE magnitude</td>
<td style="text-align: left;">$\frac{1}{T} \sum_{t=0}^{T}\left</td>
</tr>
<tr>
<td style="text-align: left;">(L1 value loss)</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h2>5. Results and Discussion</h2>
<p>Our main findings are that (i) PLR significantly improves both sample efficiency and generalization, attaining the highest normalized mean test and train returns and mean reduction in generalization gap on Procgen out of all individual methods evaluated, while matching UCB-DrAC in test improvement relative to PPO; (ii) alternative scoring functions lead to inconsistent improvements across environments; (iii) PLR combined with UCB-DrAC sets a new state-of-the-art on Procgen; and (iv) PLR induces an implicit curriculum over training levels, which substantially aids training in two challenging MiniGrid environments.</p>
<h3>5.1. Procgen Benchmark</h3>
<p>Our results, summarized in Figure 2, show PLR with rank prioritization $(\beta=0.1, \rho=0.1)$ leads to the largest statistically significant gains in mean normalized test and train returns and reduction in generalization gap compared to uniform sampling, outperforming all other methods besides UCB-DrAC + PLR. PLR combined with UCB-DrAC sees the most drastic improvements in these metrics. As reported in Table 2, UCB-DrAC + PLR yields a $76 \%$ improvement in mean test return relative to PPO with uniform sampling, and a $28 \%$ improvement relative to the previous state-of-the-art set by UCB-DrAC. While PLR with rank prioritization leads to statistically significant gains in test return on 10 of 16 environments and proportional prioritization, on 11 of 16 games, we prefer rank prioritization: While we find the two comparable in mean normalized returns, Figure 3 shows rank prioritization results in higher mean unnormalized test returns and a significantly lower mean generalization gap, averaged over all environments.</p>
<p>Further, Figure 3 shows that gains only occur when $P_{\text {replay }}$ considers both level scores and staleness $(0&lt;\rho&lt;1)$, highlighting the importance of staleness-based sampling in keeping scores from drifting off-policy. Lastly, we also</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Left: Mean episodic test returns (10 runs) of each method. Each colored $\star$ indicates statistically significant ( $p&lt;0.05$ ) gains in final test performance or sample complexity along the curve, relative to uniform sampling, for the PLR-based method of the same color. Center: Mean normalized train and test returns averaged across all games. Right: Mean generalization gaps averaged across all games.
benchmarked PLR on the hard setting against the same set of methods, where it again leads with $35 \%$ greater test returns relative to uniform sampling and $83 \%$ greater test returns when combined with UCB-DrAC. Figures 10-18 and Table 4 in Appendix B report additional details on these results.</p>
<p>The alternative scoring metrics based on TD-error and classifier uncertainty perform inconsistently across games. While certain games, such as BigFish, see improved sampleefficiency and generalization under various scoring functions, others, such as Ninja, see no improvement or worse, degraded performance. See Figure 3 for an example of this inconsistent effect across games. We find the bestperforming variant of TSCL Window does not incorporate staleness information $(\rho=0)$ and similarly leads to inconsistent outcomes across games at test time, notably significantly worsening performance on StarPilot, as seen in Figure 2, and increasing the generalization gap on some environments as revealed in Figure 15 of Appendix B.</p>
<h3>5.2. MiniGrid</h3>
<p>We provide empirical support for the Value Correction Hypothesis (defined in Section 3) on two challenging MiniGrid environments, whose levels fall into discrete difficulties (e.g. by number of rooms to be traversed). In both, PLR with rank prioritization significantly improves sample efficiency and generalization over uniform sampling, demonstrating our method also works well in discrete state spaces. We find a staleness coefficient of $\rho=0.3$ leads to the best test performance on MiniGrid. The top row of Figure 4 summarizes these results.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Top: Two example Procgen environments, between which all scoring functions except L1 value loss show inconsistent improvements to test performance (rank prioritization, $\beta=0.1$, $\rho=0.3$ ). This inconsistency holds across settings in our grid search. Bottom: Mean unnormalized episodic test returns (left) and mean generalization gap (right) for various PLR settings.</p>
<p>To test our hypothesis, we bin each level into its corresponding difficulty, expressed as ascending, discrete values (note that PLR does not have access to this privileged information). In the bottom row of Figure 4, we see how the expected difficulty of levels sampled using PLR changes during training for each environment. We observe that as $P_{\text {replay }}$ is updated, levels become sampled according to an implicit curriculum over the training levels that prioritizes progressively harder levels. Of particular note, PLR seems to struggle to discover a useful curriculum for around the first 4,000 updates on ObstructedMazeGamut-Medium, at which point it discovers</p>
<p>Table 2. Test returns of policies trained using each method with its best hyperparameters. Following Raileanu et al. (2021), the reported mean and standard deviations per environment are computed by evaluating the final policy's average return on 100 test episodes, aggregated across multiple training runs ( 10 runs for Procgen Benchmark and 3 for MiniGrid, each initialized with a different training seed). Normalized test returns per run are computed by dividing the average test return per run for each environment by the corresponding average test return of the uniform-sampling baseline over all runs. We then report the means and standard deviations of normalized test returns aggregated across runs. We report the normalized return statistics for Procgen and MiniGrid environments separately. Bolded methods are not significantly different from the method with highest mean, unless all are, in which case none are bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Environment</th>
<th style="text-align: center;">Uniform</th>
<th style="text-align: right;">TSCL</th>
<th style="text-align: right;">mixreg</th>
<th style="text-align: right;">UCB-DrAC</th>
<th style="text-align: right;">PLR</th>
<th style="text-align: right;">UCB-DrAC + PLR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BigFish</td>
<td style="text-align: center;">$3.7 \pm 1.2$</td>
<td style="text-align: right;">$4.3 \pm 1.3$</td>
<td style="text-align: right;">$6.9 \pm 1.6$</td>
<td style="text-align: right;">$8.7 \pm 1.1$</td>
<td style="text-align: right;">$10.9 \pm 2.8$</td>
<td style="text-align: right;">$\mathbf{1 4 . 3} \pm \mathbf{2 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">BossFight</td>
<td style="text-align: center;">$7.7 \pm 0.4$</td>
<td style="text-align: right;">$7.4 \pm 0.8$</td>
<td style="text-align: right;">$8.1 \pm 0.7$</td>
<td style="text-align: right;">$7.7 \pm 0.7$</td>
<td style="text-align: right;">$\mathbf{8 . 9} \pm \mathbf{0 . 4}$</td>
<td style="text-align: right;">$\mathbf{8 . 8} \pm \mathbf{0 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">CaveFlyer</td>
<td style="text-align: center;">$5.4 \pm 0.8$</td>
<td style="text-align: right;">$\mathbf{6 . 3} \pm \mathbf{0 . 6}$</td>
<td style="text-align: right;">$6.0 \pm 0.6$</td>
<td style="text-align: right;">$4.6 \pm 0.9$</td>
<td style="text-align: right;">$\mathbf{6 . 3} \pm \mathbf{0 . 5}$</td>
<td style="text-align: right;">$\mathbf{6 . 8} \pm \mathbf{0 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Chaser</td>
<td style="text-align: center;">$5.2 \pm 0.7$</td>
<td style="text-align: right;">$4.9 \pm 1.0$</td>
<td style="text-align: right;">$5.7 \pm 1.1$</td>
<td style="text-align: right;">$6.8 \pm 0.9$</td>
<td style="text-align: right;">$6.9 \pm 1.2$</td>
<td style="text-align: right;">$\mathbf{8 . 0} \pm \mathbf{0 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Climber</td>
<td style="text-align: center;">$5.9 \pm 0.6$</td>
<td style="text-align: right;">$6.0 \pm 0.8$</td>
<td style="text-align: right;">$\mathbf{6 . 6} \pm \mathbf{0 . 7}$</td>
<td style="text-align: right;">$\mathbf{6 . 4} \pm \mathbf{0 . 9}$</td>
<td style="text-align: right;">$\mathbf{6 . 3} \pm \mathbf{0 . 8}$</td>
<td style="text-align: right;">$\mathbf{6 . 8} \pm \mathbf{0 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">CoinRun</td>
<td style="text-align: center;">$8.6 \pm 0.4$</td>
<td style="text-align: right;">$\mathbf{9 . 2} \pm \mathbf{0 . 2}$</td>
<td style="text-align: right;">$8.6 \pm 0.3$</td>
<td style="text-align: right;">$8.6 \pm 0.4$</td>
<td style="text-align: right;">$8.8 \pm 0.5$</td>
<td style="text-align: right;">$\mathbf{9 . 0} \pm \mathbf{0 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Dodgeball</td>
<td style="text-align: center;">$1.7 \pm 0.2$</td>
<td style="text-align: right;">$1.2 \pm 0.4$</td>
<td style="text-align: right;">$1.8 \pm 0.4$</td>
<td style="text-align: right;">$5.1 \pm 1.6$</td>
<td style="text-align: right;">$1.8 \pm 0.5$</td>
<td style="text-align: right;">$\mathbf{1 0 . 3} \pm \mathbf{1 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">FruitBot</td>
<td style="text-align: center;">$27.3 \pm 0.9$</td>
<td style="text-align: right;">$27.1 \pm 1.6$</td>
<td style="text-align: right;">$27.7 \pm 0.8$</td>
<td style="text-align: right;">$27.0 \pm 1.3$</td>
<td style="text-align: right;">$28.0 \pm 1.3$</td>
<td style="text-align: right;">$27.6 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: left;">Heist</td>
<td style="text-align: center;">$2.8 \pm 0.9$</td>
<td style="text-align: right;">$2.5 \pm 0.6$</td>
<td style="text-align: right;">$2.7 \pm 0.4$</td>
<td style="text-align: right;">$3.2 \pm 0.7$</td>
<td style="text-align: right;">$2.9 \pm 0.5$</td>
<td style="text-align: right;">$\mathbf{4 . 9} \pm \mathbf{1 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Jumper</td>
<td style="text-align: center;">$5.7 \pm 0.4$</td>
<td style="text-align: right;">$6.1 \pm 0.6$</td>
<td style="text-align: right;">$6.1 \pm 0.3$</td>
<td style="text-align: right;">$5.6 \pm 0.5$</td>
<td style="text-align: right;">$5.8 \pm 0.5$</td>
<td style="text-align: right;">$5.9 \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: left;">Leaper</td>
<td style="text-align: center;">$4.2 \pm 1.3$</td>
<td style="text-align: right;">$6.4 \pm 1.2$</td>
<td style="text-align: right;">$5.2 \pm 1.1$</td>
<td style="text-align: right;">$4.4 \pm 1.4$</td>
<td style="text-align: right;">$6.8 \pm 1.2$</td>
<td style="text-align: right;">$\mathbf{8 . 7} \pm \mathbf{1 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Maze</td>
<td style="text-align: center;">$5.5 \pm 0.4$</td>
<td style="text-align: right;">$5.0 \pm 0.3$</td>
<td style="text-align: right;">$5.4 \pm 0.5$</td>
<td style="text-align: right;">$6.2 \pm 0.5$</td>
<td style="text-align: right;">$5.5 \pm 0.8$</td>
<td style="text-align: right;">$\mathbf{7 . 2} \pm \mathbf{0 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Miner</td>
<td style="text-align: center;">$8.7 \pm 0.7$</td>
<td style="text-align: right;">$8.9 \pm 0.6$</td>
<td style="text-align: right;">$9.5 \pm 0.4$</td>
<td style="text-align: right;">$\mathbf{1 0 . 1} \pm \mathbf{0 . 6}$</td>
<td style="text-align: right;">$\mathbf{9 . 6} \pm \mathbf{0 . 6}$</td>
<td style="text-align: right;">$\mathbf{1 0 . 0} \pm \mathbf{0 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Ninja</td>
<td style="text-align: center;">$6.0 \pm 0.4$</td>
<td style="text-align: right;">$\mathbf{6 . 8} \pm \mathbf{0 . 5}$</td>
<td style="text-align: right;">$\mathbf{6 . 9} \pm \mathbf{0 . 5}$</td>
<td style="text-align: right;">$5.8 \pm 0.8$</td>
<td style="text-align: right;">$\mathbf{7 . 2} \pm \mathbf{0 . 4}$</td>
<td style="text-align: right;">$\mathbf{7 . 0} \pm \mathbf{0 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Plunder</td>
<td style="text-align: center;">$5.1 \pm 0.6$</td>
<td style="text-align: right;">$5.9 \pm 1.1$</td>
<td style="text-align: right;">$5.7 \pm 0.5$</td>
<td style="text-align: right;">$\mathbf{7 . 8} \pm \mathbf{0 . 9}$</td>
<td style="text-align: right;">$\mathbf{8 . 7} \pm \mathbf{2 . 2}$</td>
<td style="text-align: right;">$\mathbf{7 . 7} \pm \mathbf{0 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">StarPilot</td>
<td style="text-align: center;">$26.8 \pm 1.5$</td>
<td style="text-align: right;">$19.8 \pm 3.4$</td>
<td style="text-align: right;">$\mathbf{3 2 . 7} \pm \mathbf{1 . 5}$</td>
<td style="text-align: right;">$\mathbf{3 1 . 7} \pm \mathbf{2 . 4}$</td>
<td style="text-align: right;">$27.9 \pm 4.4$</td>
<td style="text-align: right;">$29.6 \pm 2.2$</td>
</tr>
<tr>
<td style="text-align: left;">Normalized test returns (\%)</td>
<td style="text-align: center;">$100.0 \pm 4.5$</td>
<td style="text-align: right;">$103.0 \pm 3.6$</td>
<td style="text-align: right;">$113.8 \pm 2.8$</td>
<td style="text-align: right;">$129.8 \pm 8.2$</td>
<td style="text-align: right;">$128.3 \pm 5.8$</td>
<td style="text-align: right;">$\mathbf{1 7 6 . 4} \pm \mathbf{6 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">MultiRoom-N4-Random</td>
<td style="text-align: center;">$0.80 \pm 0.04$</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$\mathbf{0 . 8 1} \pm \mathbf{0 . 0 1}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">ObstructedMazeGamut-Easy</td>
<td style="text-align: center;">$0.53 \pm 0.04$</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$\mathbf{0 . 8 5} \pm \mathbf{0 . 0 4}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">ObstructedMazeGamut-Med</td>
<td style="text-align: center;">$0.65 \pm 0.01$</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$\mathbf{0 . 7 3} \pm \mathbf{0 . 0 7}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">Normalized test returns (\%)</td>
<td style="text-align: center;">$100.0 \pm 2.5$</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$\mathbf{1 2 4 . 3} \pm \mathbf{4 . 7}$</td>
<td style="text-align: right;">-</td>
</tr>
</tbody>
</table>
<p>a curriculum that gradually assigns more weight to harder levels. This curriculum enables PLR with access to only 6,000 training levels to attain even higher mean test returns than the uniform-sampling baseline with access to the full set of training levels, of which there are roughly 4 billion (so our training levels constitute $0.00015 \%$ of the total number).</p>
<p>We further tested an extended version of PLR that trains on the full level distribution on these two environments by tracking a buffer of levels with the highest estimated learning potential. We find it outperforms uniform sampling with access to the full level distribution. These additional results are presented in Appendix B.3.</p>
<h2>6. Related Work</h2>
<p>Several methods for improving generalization in deep RL adapt techniques from supervised learning, including stochastic regularization (Igl et al., 2019; Cobbe et al., 2020a), data augmentation (Kostrikov et al., 2020; Raileanu et al., 2021; Wang et al., 2020a), and feature distillation (Igl
et al., 2020; Cobbe et al., 2020b). In contrast, PLR modifies only how the next training level is sampled, thereby easily combining with any model or RL algorithm.</p>
<p>The selective-sampling performed by PLR makes it a form of active learning (Cohn et al., 1994; Settles, 2009). Our work also echoes ideas from Graves et al. (2017), who train a multi-armed bandit to choose the next task in multi-task supervised learning, so to maximize gradient-based progress signals. Sharma et al. (2018) extend these ideas to multitask RL, but add the additional requirement of knowing a maximum target return for each task a priori. Zhang et al. (2020b) use an ensemble of value functions for selective goal sampling in the off-policy continuous control setting, requiring prior knowledge of the environment structure to generate candidate goals. Unlike PLR, these methods assume the ability to sample tasks or levels based on their structural properties, an assumption that does not typically hold for PCG simulators. Instead, our method automatically uncovers similarly difficult levels, giving rise to a curriculum without prior knowledge of the environment.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Top: Mean episodic test returns of PLR and the uniform-sampling baseline on MultiRoom-N4-Random (4 runs), ObstructedMazeGamut-Easy (3 runs), and ObstructedMazeGamut-Medium (3 runs). Bottom: The probability mass assigned to levels of varying difficulty over the course of training in a single, randomly selected run for the respective environment.</p>
<p>A recent theme in the PCG setting explores adaptively generating levels to facilitate learning (Sukhbaatar et al., 2017; Wang et al., 2019; 2020b; Khalifa et al., 2020; Akkaya et al., 2019; Campero et al., 2020; Dennis et al., 2020). Unlike these approaches, our method does not assume control over level generation, requiring only the ability to replay previously visited levels. These methods also require parameterizing level generation with additional learning modules. In contrast, our approach does not require such extensions of the environment, for example including teacher-specific action spaces in the case of Campero et al. (2020). Most similar to our method, Matiisen et al. (2020) proposes a teacher-student curriculum learning (TSCL) algorithm that samples training levels by considering the change in episodic returns per level, though they neither design nor test the method for generalization. As shown in Section 5.1, it provides inconsistent benefits at test time. Further, unlike TSCL, PLR does not assume access to all levels at the start of training, and as we show in Appendix B.3, PLR can be extended to improve sample efficiency and generalization by training on an unbounded number of training levels.</p>
<p>Like our method, Schaul et al. (2016) and Kapturowski et al. (2019) use TD-errors to estimate learning potential. While these methods make use of TD-errors to prioritize learning from past experiences, our method uses such estimates to prioritize revisiting levels for generating entirely new future experiences for learning.</p>
<p>Generalization requires sufficient exploration of environment states and dynamics. Thus, recent exploration strategies (e.g. Raileanu &amp; Rocktäschel, 2020; Campero et al., 2020; Zhang et al., 2020a; Zha et al., 2021) shown to benefit simple PCG settings are complementary to the aims of this work. However, as these studies focus on PCG environments with low-dimensional state spaces, whether such methods can be successfully applied to more complex PCG environments like Procgen Benchmark remains to be seen. If so, they may potentially combine with PLR to yield additive improvements. We believe the interplay between such exploration methods and PLR to be a promising direction for future research.</p>
<h2>7. Conclusion and Future Work</h2>
<p>We introduced Prioritized Level Replay (PLR), an algorithm for selectively sampling the next training level in PCG environments based on the estimated learning potential of revisiting each level for the current policy. We showed that our method remarkably improves both the sample efficiency and generalization of deep RL agents in PCG environments, including the majority of environments in Procgen Benchmark and two challenging MiniGrid environments. We further combined PLR with the prior leading method to set a new state-of-the-art on Procgen Benchmark. Further, on MiniGrid environments, we showed PLR induces an emergent curriculum of increasingly more difficult levels.</p>
<p>The flexibility of the PCG abstraction makes PLR applicable to many problems of practical importance, for example, robotic object manipulation tasks, where domain randomized environment instances map to the notion of levels. We believe PLR may even be applicable to singleton environments, given a procedure for generating variations of the underlying MDP as a function of a level identifier, for ex-</p>
<p>ample, by varying the starting positions of entities. Another natural extension of PLR is to adapt the method to operate in the goal-conditioned setting, by incorporating goals into the level parameterization.</p>
<p>Despite the wide applicability of PCG and consequently PLR, not all problem domains can be effectively represented in seed-based simulation. Many real world problems require transfer into domains too complex to be adequately captured by simulation, such as car driving, where realizing a completely faithful simulation would entail solving the very same control problem of interest, creating a chicken-and-egg dilemma. Further, environment resets are not universally available, such as in the continual learning setting, where the agent interacts with the environment without explicit episode boundaries-arguably, a more realistic interaction model for a learning agent deployed in the wild.</p>
<p>Still, pre-training in simulation with resets can nevertheless benefit such settings, where the target domain is rife with open-ended complexity and where resets are unavailable, especially as training through real-world interactions can be slow, expensive, and precarious. In fact, in practice, we almost exclusively train deep RL policies in simulation for these reasons. As PLR provides a simple method to more fully exploit the simulator for improved test-time performance, we believe PLR can also be adapted to improve learning in these settings.</p>
<p>We further note that while we empirically demonstrated that the L1 value loss acts as a highly effective scoring function, there likely exist even more potent choices. Directly learning such functions may reveal even better alternatives. Lastly, combining PLR with various exploration strategies may further improve test performance in hard exploration environments. We look forward to investigating each of these promising directions in future work, prioritized accordingly, by learning potential.</p>
<h2>Acknowledgements</h2>
<p>We thank Roberta Raileanu, Heinrich Küttler, and Jakob Foerster for useful discussions and feedback on this work, and our anonymous reviewers, for their recommendations on improving this paper.</p>
<h2>References</h2>
<p>Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., Schneider, J., Tezak, N., Tworek, J., Welinder, P., Weng, L., Yuan, Q., Zaremba, W., and Zhang, L. Solving rubik's cube with a robot hand. CoRR, abs/1910.07113, 2019. URL http://arxiv.org/ abs/1910.07113.</p>
<p>Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, O. P., and Zaremba, W. Hindsight experience replay. In Advances in neural information processing systems, pp. 5048-5058, 2017.</p>
<p>Bard, N., Foerster, J. N., Chandar, S., Burch, N., Lanctot, M., Song, H. F., Parisotto, E., Dumoulin, V., Moitra, S., Hughes, E., Dunning, I., Mourad, S., Larochelle, H., Bellemare, M. G., and Bowling, M. The hanabi challenge: A new frontier for AI research. $A r$ tif. Intell., 280:103216, 2020. doi: 10.1016/j.artint. 2019.103216. URL https://doi.org/10.1016/ j.artint.2019.103216.</p>
<p>Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, Jun 2013. ISSN 1076-9757. doi: 10.1613/jair.3912. URL http://dx.doi.org/10. 1613/jair. 3912.</p>
<p>Campero, A., Raileanu, R., Küttler, H., Tenenbaum, J. B., Rocktäschel, T., and Grefenstette, E. Learning with AMIGo: Adversarially Motivated Intrinsic Goals. CoRR, abs/2006.12122, 2020. URL https://arxiv.org/ abs/2006.12122.</p>
<p>Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. BabyAI: First steps towards grounded language learning with a human in the loop. CoRR, abs/1810.08272, 2018. URL http://arxiv.org/abs/1810.08272.</p>
<p>Chevalier-Boisvert, M., Willems, L., and Pal, S. Minimalistic gridworld environment for OpenAI Gym. https:// github.com/maximecb/gym-minigrid, 2018.</p>
<p>Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J. Quantifying generalization in reinforcement learning. In International Conference on Machine Learning, pp. 1282-1289. PMLR, 2019.</p>
<p>Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. Leveraging Procedural Generation to Benchmark Reinforcement Learning. In International Conference on Machine Learning, pp. 2048-2056. PMLR, November 2020a. URL http://proceedings.mlr.press/v119/ cobbe20a.html. ISSN: 2640-3498.</p>
<p>Cobbe, K., Hilton, J., Klimov, O., and Schulman, J. Phasic Policy Gradient. CoRR, abs/2009.04416, 2020b. URL https://arxiv.org/abs/2009.04416.</p>
<p>Cohn, D., Atlas, L., and Ladner, R. Improving generalization with active learning. Machine learning, 15(2): 201-221, 1994.</p>
<p>Dennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-shot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings. neurips.cc/paper/2020/hash/ 985e9a46e10005356bbaf194249f6856Abstract.html.</p>
<p>Graves, A., Bellemare, M. G., Menick, J., Munos, R., and Kavukcuoglu, K. Automated curriculum learning for neural networks. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 1311-1320. PMLR, 2017. URL http://proceedings.mlr.press/ v70/graves17a.html.</p>
<p>Igl, M., Ciosek, K., Li, Y., Tschiatschek, S., Zhang, C., Devlin, S., and Hofmann, K. Generalization in reinforcement learning with selective noise injection and information bottleneck. In Advances in Neural Information Processing Systems, pp. 13978-13990, 2019.</p>
<p>Igl, M., Farquhar, G., Luketina, J., Boehmer, W., and Whiteson, S. The impact of non-stationarity on generalisation in deep reinforcement learning. CoRR, abs/2006.05826, 2020. URL https://arxiv.org/ abs/2006.05826.</p>
<p>Juliani, A., Khalifa, A., Berges, V.-P., Harper, J., Teng, E., Henry, H., Crespi, A., Togelius, J., and Lange, D. Obstacle tower: A generalization challenge in vision, control, and planning. Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, Aug 2019. doi: 10.24963/ijcai.2019/373. URL http: //dx.doi.org/10.24963/ijcai.2019/373.</p>
<p>Justesen, N., Torrado, R. R., Bontrager, P., Khalifa, A., Togelius, J., and Risi, S. Procedural level generation improves generality of deep reinforcement learning. CoRR, abs/1806.10729, 2018. URL http://arxiv.org/ abs/1806.10729.</p>
<p>Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W. Recurrent experience replay in distributed reinforcement learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum? id=rl1yTjAqYX.</p>
<p>Khalifa, A., Bontrager, P., Earle, S., and Togelius, J. PCGRL: procedural content generation via reinforcement learning. CoRR, abs/2001.09212, 2020. URL https: //arxiv.org/abs/2001.09212.</p>
<p>Kostrikov, I., Yarats, D., and Fergus, R. Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels. CoRR, abs/2004.13649, 2020. URL https://arxiv.org/abs/2004.13649.</p>
<p>Küttler, H., Nardelli, N., Miller, A. H., Raileanu, R., Selvatici, M., Grefenstette, E., and Rocktäschel, T. The nethack learning environment. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings. neurips.cc/paper/2020/hash/ 569ff987c643b4bedf504efda8f786c2Abstract.html.</p>
<p>Matiisen, T., Oliver, A., Cohen, T., and Schulman, J. Teacher-student curriculum learning. IEEE Transactions on Neural Networks and Learning Systems, 31 (9):3732-3740, Sep 2020. ISSN 2162-2388. doi: 10.1109/tnnls.2019.2934906. URL http://dx.doi. org/10.1109/TNNLS.2019.2934906.</p>
<p>Raileanu, R. and Rocktäschel, T. RIDE: rewarding impactdriven exploration for procedurally-generated environments. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=rkg-TJBFPB.</p>
<p>Raileanu, R., Goldstein, M., Yarats, D., Kostrikov, I., and Fergus, R. Automatic data augmentation for generalization in reinforcement learning, 2021. URL https: //openreview.net/forum?id=919WD4ahJgs.</p>
<p>Risi, S. and Togelius, J. Increasing generality in machine learning through procedural content generation. Nature Machine Intelligence, 2(8):428-436, Aug 2020. ISSN 2522-5839. doi: 10.1038/s42256-020-0208-z. URL http://dx.doi.org/10.1038/ s42256-020-0208-z.</p>
<p>Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. In Bengio, Y. and LeCun, Y. (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.05952.</p>
<p>Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and Abbeel, P. High-dimensional continuous control using</p>
<p>generalized advantage estimation. In Bengio, Y. and LeCun, Y. (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1506.02438.</p>
<p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv. org/abs/1707.06347.</p>
<p>Settles, B. Active learning literature survey. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 2009.</p>
<p>Sharma, S., Jha, A. K., Hegde, P., and Ravindran, B. Learning to multi-task by active sampling. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum? id=BlnZlweCZ.</p>
<p>Sukhbaatar, S., Kostrikov, I., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via asymmetric self-play. CoRR, abs/1703.05407, 2017. URL http://arxiv.org/abs/1703.05407.</p>
<p>Wang, K., Kang, B., Shao, J., and Feng, J. Improving generalization in reinforcement learning with mixture regularization. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020a. URL https://proceedings. neurips.cc/paper/2020/hash/ 5a751d6a0b6ef05cfe51b86e5d1458e6Abstract.html.</p>
<p>Wang, R., Lehman, J., Clune, J., and Stanley, K. O. POET: open-ended coevolution of environments and their optimized solutions. In Auger, A. and Stützle, T. (eds.), Proceedings of the Genetic and Evolutionary Computation Conference, GECCO 2019, Prague, Czech Republic, July 13-17, 2019, pp. 142-151. ACM, 2019. doi: 10.1145/3321707.3321799. URL https://doi. org/10.1145/3321707.3321799.</p>
<p>Wang, R., Lehman, J., Rawal, A., Zhi, J., Li, Y., Clune, J., and Stanley, K. Enhanced POET: Open-ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions. In International Conference on Machine Learning, pp. 9940-9951. PMLR, November 2020b. URL http://proceedings. mlr.press/v119/wang201.html. ISSN: 26403498.</p>
<p>Zha, D., Ma, W., Yuan, L., Hu, X., and Liu, J. Rank the episodes: A simple approach for exploration in procedurally-generated environments. CoRR, abs/2101.08152, 2021. URL https://arxiv.org/ abs/2101.08152.</p>
<p>Zhang, T., Xu, H., Wang, X., Wu, Y., Keutzer, K., Gonzalez, J. E., and Tian, Y. Bebold: Exploration beyond the boundary of explored regions. CoRR, abs/2012.08621, 2020a. URL https://arxiv.org/abs/2012.08621.</p>
<p>Zhang, Y., Abbeel, P., and Pinto, L. Automatic curriculum learning through value disagreement. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020b. URL https://proceedings. neurips.cc/paper/2020/hash/ 566f0ea4f6c2e947f36795c8f58ba901Abstract.html.</p>
<p>Zhong, V., Rocktäschel, T., and Grefenstette, E. RTFM: generalising to new environment dynamics via reading. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=SJgob6NKvH.</p>
<h2>A. Experiment Details and Hyperparameters</h2>
<h2>A.1. Procgen Benchmark</h2>
<p>Procgen Benchmark consists of 16 PCG environments of varying styles, exhibiting a diversity of gameplay similar to that of the ALE benchmark. Game levels are determined by a random seed and can vary in navigational layout, visual appearance, and starting positions of entities. All Procgen environments share the same discrete 15-dimensional action space and produce $64 \times 64 \times 3$ RGB observations. (Cobbe et al., 2020a) provides a comprehensive description of each of the 16 environments. State-of-the-art RL algorithms, like PPO, lead to significant generalization gaps between test and train performance in all games, making Procgen a useful benchmark for assessing generalization performance.</p>
<p>We follow the standard protocol for testing generalization performance on Procgen: We train an agent for each game on a finite number of levels, $N_{\text {train }}$, and sample test levels from the full distribution of levels. Normalized test returns are computed as $\left(R-R_{\min }\right) /\left(R_{\max }-R_{\min }\right)$, where $R$ is the unnormalized return and each game's minimum return, $R_{\min }$, and maximum return, $R_{\max }$, are provided in Cobbe et al. (2020a), which uses this same normalization.</p>
<p>To make the most efficient use of our computational resources, we perform hyperparameter sweeps on the easy setting. This also makes our results directly comparable to most prior works benchmarked on Procgen, which have likewise focused on the easy setting. In Procgen easy, our experiments use the recommended settings of $N_{\text {train }}=200$ and 25 M steps of training, as well as the same ResNet policy architecture and PPO hyperparameters shared across all games as in Cobbe et al. (2020a) and Raileanu et al. (2021). We find 25 M steps to be sufficient for uncovering differences in generalization performance among our methods and standard baselines. Moreover, under this setup, we find Procgen training runs require much less wall-clock time than training runs on the two MiniGrid environments of interest over an equivalent number of steps needed to uncover differences in generalization performance. Therefore we survey the empirical differences across various settings of PLR on Procgen easy rather than MiniGrid.</p>
<p>To find the best hyperparameters for PLR, we evaluate each combination of the scoring function choices in Table 1 with both rank and proportional prioritization, performing a coarse grid search for each pair over different settings of the temperature parameter $\beta$ in ${0.1,0.5,1.0,1.4,2.0}$ and the staleness coefficient $\rho$ in ${0.1,0.3,1.0}$. For each setting, we run 4 trials across all 16 of games of the Procgen Benchmark, evaluating based on mean unnormalized test return across games. In our TD-error-based scoring functions, we set $\gamma$ and $\lambda$ equal to the same respective values used by the GAE in PPO during training. We found PLR offered the most pronounced gains at $\beta=0.1$ and $\rho=0.1$ on Procgen, but these benefits also held for higher values ( $\beta=0.5$ and $\rho=0.3$ ), though to a lesser degree.</p>
<p>For UCB-DrAC, we make use of the best-reported hyperparameters on the easy setting of Procgen in Raileanu et al. (2021), listed in Table 3.</p>
<p>We found the default setting of mixreg's $\alpha=0.2$, as used by Wang et al. (2020a) in the hard setting, performs poorly on the easy setting. Instead, we conducted a grid search over $\alpha$ in ${0.001,0.005,0.01,0.05,0.1,0.2,0.8,0.2,0.5,0.8,1}$.</p>
<p>Since the TSCL Window algorithm was not previously evaluated on Procgen Benchmark, we perform a grid search over different settings for both Boltzmann and $\epsilon$-greedy variants of the algorithm to determine the best hyperparameter settings for Procgen easy. We searched over window size $K$ in ${10,100,1000,10000}$, bandit learning rate $\alpha$ in ${0.01,0.1,0.5,1.0}$, random exploration probability $\epsilon$ in ${0.0,0.01,0.1,0.5}$ for the $\epsilon$-greedy variant, and temperature $\tau$ in ${0.1,0.5,1.0}$ for the Boltzmann variant. Additionally, for a fairer comparison to PLR we further evaluated a variant of TSCL Window that, like PLR, incorporates the staleness distribution, by additionally searching over values of the staleness coefficient $\rho$ in ${0.0,0.1,0.3,0.5}$, though we ultimately found that TSCL Window performed best without staleness sampling $(\rho=0)$.</p>
<p>See Table 3 for a comprehensive overview of the hyperparameters used for PPO, UCB-DrAC, mixreg, and TSCL Window, shared across all Procgen environments to generate our reported results on Procgen easy.</p>
<p>The evaluation protocol on the hard setting entails training on 500 levels over 200M steps (Cobbe et al., 2020a), making it more compute-intensive than the easy setting. To save on computational resources, we make use of the same hyperparameters found in the easy setting for each method on Procgen hard, with one exception: As our PPO implementation does not use multi-GPU training, we were unable to quadruple our GPU actors as done in Cobbe et al. (2020a) and Wang et al. (2020a). Instead, we resorted to doubling the number of environments in our single actor to 128, resulting in mini-batch sizes half as large as used in these two prior works. As such, our baseline results on hard are not directly comparable to theirs. We found setting mixreg's $\alpha=0.2$ as done in Wang et al. (2020a) led to poor performance under this reduced batch size. We conducted an additional grid search, finding $\alpha=0.01$ to perform best, as on Procgen easy.</p>
<h2>A.2. MiniGrid</h2>
<p>The MiniGrid suite (Chevalier-Boisvert et al., 2018) features a series of highly structured environments of increasing difficulty. Each environment features a task in a grid world</p>
<p>Table 3. Hyperparameters used for training on Procgen Benchmark and MiniGrid environments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: right;">Procgen</th>
<th style="text-align: right;">MiniGrid</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PPO</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">$\gamma$</td>
<td style="text-align: right;">0.999</td>
<td style="text-align: right;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">$\lambda_{\text {GAE }}$</td>
<td style="text-align: right;">0.95</td>
<td style="text-align: right;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">PPO rollout length</td>
<td style="text-align: right;">256</td>
<td style="text-align: right;">256</td>
</tr>
<tr>
<td style="text-align: left;">PPO epochs</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">PPO minibatches per epoch</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">PPO clip range</td>
<td style="text-align: right;">0.2</td>
<td style="text-align: right;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">PPO number of workers</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">64</td>
</tr>
<tr>
<td style="text-align: left;">Adam learning rate</td>
<td style="text-align: right;">$5 \mathrm{e}-4$</td>
<td style="text-align: right;">$7 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: left;">Adam $\epsilon$</td>
<td style="text-align: right;">$1 \mathrm{e}-5$</td>
<td style="text-align: right;">$1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">return normalization</td>
<td style="text-align: right;">yes</td>
<td style="text-align: right;">yes</td>
</tr>
<tr>
<td style="text-align: left;">entropy bonus coefficient</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">value loss coefficient</td>
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">0.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">PLR</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Prioritization</td>
<td style="text-align: right;">rank</td>
<td style="text-align: right;">rank</td>
</tr>
<tr>
<td style="text-align: left;">Temperature, $\beta, 0.1$</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Staleness coefficient, $\rho$</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">UCB-DrAC</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Window size, $K$</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">Regularization coefficient, $\alpha_{r}$</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">UCB exploration coefficient, $c$</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">mixreg</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Beta shape, $\alpha$</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">TSCL Window</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Bandit exploration strategy</td>
<td style="text-align: right;">$\epsilon$-greedy</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">Window size, $K$</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">Bandit learning rate, $\alpha$</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">Exploration probability, $\epsilon$</td>
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">-</td>
</tr>
</tbody>
</table>
<p>setting, and as in Procgen, environment levels are determined by a seed. Harder levels require the agent to perform longer action sequences over a combinatorially-rich set of game entities, on increasingly larger grids. The clear ordering of difficulty over subsets of MiniGrid environments allows us to track the relative difficulty of levels sampled by PLR over the course of training.</p>
<p>MiniGrid environments share a discrete 7-dimensional action space and produce a 3-channel integer state encoding of the $7 \times 7$ grid immediately including and in front of the agent. However, following the training setup in Igl et al. (2019), we modify the environment to produce an $N \times M \times 3$ encoding of the full grid, where $N$ and $M$ vary according to the maximum grid dimensions of each environment. Full observability makes generalization harder, requiring the agent to generalize across different level layouts in their entirety.</p>
<p>We evaluate PLR with rank prioritization on two MiniGrid environments whose levels are uniformly distributed across several difficulty settings. Training on levels of varying difficulties helps agents make use of the easier levels as stepping stones to learn useful behaviors that help the agent make progress on harder levels. However, under the uniformsampling baseline, learning may be inefficient, as the training process does not selectively train the agent on levels of increasing difficulty, leading to wasted training steps when a difficult level is sampled early in training. On the contrary, if PLR scores levels according to the time-averaged L1 value loss of recently experienced level trajectories, the average difficulty of the sampled levels should adapt to the agent's current abilities, following the reasoning outlined in the Value Correction Hypothesis, stated in Section 3.</p>
<p>As in Igl et al. (2019), we parameterize the agent policy as a 3-layer CNN with 16, 32, and 32 channels, with a final hidden layer of size 64 . All kernels are $2 \times 2$ and use a stride of 1 . For the ObstructedMazeGamut environments, we increase the number of channels of the final CNN layer to 64 . We follow the same high-level generalization evaluation protocol used for Procgen, training the agent on a fixed set of 4000 levels for MultiRoom-N4-Random, 3000 levels for ObstructedMazeGamut-Easy, and 6000 levels for ObstructedMazeGamut-Medium, and testing on the full level distribution. We chose these values for $\left|\lambda_{\text {train }}\right|$ to ensure roughly 1000 training levels of each difficulty setting of each environment. We model our PPO parameters on those used by Igl et al. (2019) in their MiniGrid experiments. We performed a grid search to find that PLR with rank prioritization, $\beta=0.1$, and $\rho=0.3$ learned most quickly on the MultiRoom environment, and used this setting for all our MiniGrid experiments. Table 3 summarizes these hyperparameter choices.</p>
<p>The remainder of this section provides more details about the various MiniGrid environments used in this work.</p>
<p>MultiRoom-N4-Random This environment requires the agent to navigate through 1, 2, 3, or 4 rooms respectively to reach a goal object, resulting in a natural ordering of levels over four levels of difficulty. The agent always starts at a random position in the furthest room from the goal object, facing a random direction. The goal object is also initialized to a random position within its room. See Figure 5 for screenshots of example levels.</p>
<p>ObstructedMazeGamut-Easy This environment consists of levels uniformly distributed across the first three difficulty settings of the ObstructedMaze environment, in which the agent must locate and pick up the key in order to unlock the door to pick up a goal object in a second room. The agent and goal object are always initialized in random positions in different rooms separated by the locked door.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Example levels of each of the four difficulty levels of MultiRoom-N4-Random, in order of increasing difficulty from left to right. The agent (red triangle) must reach the goal (green square).</p>
<p>The second difficulty setting further requires the agent to first uncover the key from under a box before picking up the key. The third difficulty level further requires the agent to first move a ball blocking the door before entering the door. See Figure 6 for screenshots of example levels.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Example levels of each of the three difficulty levels of ObstructedMazeGamut-Easy, in order of increasing difficulty from left to right. The agent must find the key, which may be hidden under a box, to unlock a door, which may be blocked by an obstacle, to reach the goal object (blue circle).</p>
<p>ObstructedMazeGamut-Hard This environment consists of levels uniformly distributed across the first six difficulty levels of the ObstructedMaze environment. Harder levels corresponding to the fourth, fifth, and sixth difficulty settings include two additional rooms with no goal object to distract the agent. Each instance of these harder levels also contain two pairs of keys of different colors, each opening a door of the same color. The agent always starts one room away from the randomly positioned goal object. Each of the two keys is visible in the fourth difficulty setting and doors are unobstructed. The fifth difficulty setting hides the keys under boxes, and the sixth again places obstacles that must be removed before entering two of the doors, one of which is always the door to the goal-containing room. See Figure 7 for example screenshots.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Example levels in increasing difficulty from left to right of each additional difficulty setting introduced by ObstructedMazeGamut-Hard in addition to those in ObstructedMazeGamut-Easy.</p>
<h2>B. Additional Experimental Results</h2>
<h2>B.1. Extended Results on Procgen Benchmark</h2>
<p>We present an overview of the improvements in test performance of each method across all 16 Procgen Benchmark games over 10 runs in Figure 14. For each game, Figure 15 further shows how the generalization gap changes over the course of training under each method tested. We show in Figures 16 and 17, the mean test episodic returns on the Procgen Benchmark (easy) for PLR with rank and proportional prioritization, respectively. In both of these plots, we can see that using only staleness ( $\rho=1$ ) or only L1 value loss scores $(\rho=0)$ is considerably worse than direct level sampling. Thus, we only observe gains compared to the baseline when both level scores and staleness are used for the sampling distribution. Comparing Figures 16 with 17 we find that PLR with proportional instead of rank prioritization provides statistically significant gains over uniform level sampling on an additional game (CoinRun), but rank prioritization leads to slightly larger mean improvements on several games.</p>
<p>Figure 18 shows that when PLR improves generalization performance, it also either matches or improves training sample efficiency, suggesting that when beneficial to test performance, the representations learned via the auto-curriculum induced by PLR prove similarly useful on training levels. However we see that our method reduces training sample efficiency on two games on which our method does not improve generalization performance. Since our method does not discover useful auto-curricula for these games, it is likely that uniformly sampling levels at training time allows the agent to better memorize useful behaviors on each of the training levels compared to the selective sampling performed by our method.</p>
<p>Finally, we also benchmarked PLR and UCB-DrAC + PLR against uniform sampling, TSCL Window, mixreg, and UCB-DrAC on Procgen hard across 5 runs per environment. Due to the high computational cost of the evaluation protocol for Procgen hard, which entails 200M training steps, we directly use the best hyperparameters found in the easy setting for each method. The results in Figure 10 show the two PLR-based methods significantly outperform all other methods in terms of normalized mean train and test episodic return, as well as reduction in mean generalization gap, attaining even greater margins of improvement than in the easy setting. As summarized by Table 4, the gains of PLR and UCB + PLR in mean normalized test return relative to uniform sampling in the hard setting are comparable to those in the easy setting. We provide plots of episodic test return over training for each individual environment in Figure 12.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. PLR consistently induces emergent curricula from easier to harder levels during training. Left and right correspond to two additional training runs independent of that in Figure 4.</p>
<h2>B.2. Extended Results on Minigrid</h2>
<p>To demonstrate that PLR consistently induces emergent curriculum, we present plots showing the change in probability mass over different difficulty bins for additional training runs in Figure 8. Like in Figure 4, we see the probability mass assigned by $P_{\text {replay }}$ gradually shifts from easier to harder levels over the course of training.</p>
<h2>B.3. Training on the Full Level Distribution</h2>
<p>While assessing generalization performance calls for using a fixed set of training levels, ideally our method can also make use of the full level distribution if given access to it. We take advantage of an unbounded number of training levels by modifying the list structures for storing scores and timestamps (see Algorithm 1 and 2) to track the top $M$ levels by learning potential in our finite level buffer. When the lists are full, we set the next level for replacement to be</p>
<p>$$
l_{\min }=\underset{l}{\arg \min } P_{\text {replay }}(l)
$$</p>
<p>When the outcome of the Bernoulli $P_{D}$ entails sampling a new level $l$, the score and timestamps of $l$ replace those of $l_{\min }$ only if the score of $l_{\min }$ is lower than that of $l$. In this way, PLR keeps a running buffer throughout training of the top $M$ levels appraised to have the highest learning potential for replaying anew.</p>
<p>Figure 9 shows that with access to the full level distribution at training, PLR improves sample efficiency and generalization performance in both environments compared to
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Mean test episodic returns on MultiRoom-N4-Random (top) and ObstructedMazeGamut-Easy (bottom) with access to the full level distribution at training. Plots are averaged over 3 runs. We set $P_{D}$ to a Bernoulli parameterized as $p=0.5$ for MultiRoom-N4-Random and $p=0.95$ for ObstructedMazeGamut-Easy (found via grid search). As with all MiniGrid experiments using PLR, we use rank prioritization, $\beta=0.1$, and $\rho=0.3$.
uniform sampling on the full distribution. In MultiRoom-N4-Random, the value $M$ makes little difference to test performance, and training with PLR on the full level distribution leads to a policy outperforming one trained with PLR on a fixed set of training levels. However, on ObstructedMazeGamut-Easy, a smaller $M$ leads to worse test performance. Nevertheless, for all but $M=500$, including the case of a fixed set of 3000 training levels, PLR leads to better mean test performance than uniform sampling on the full level distribution.</p>
<h2>C. Algorithms</h2>
<p>In this section, we provide detailed pseudocode for how PLR can be used for experience collection when using $T$-step rollouts. Algorithm 3 presents the extension of the generic policy-gradient training loop presented in Algorithm 1 to the case of $T$-step rollouts, and Algorithm 4 presents an implementation of experience collection in this setting (extending Algorithm 2). Note that when using $T$-step rollouts in the training loop, rollouts may start and end between episode boundaries. To compute level scores on full trajectories segmented across rollouts, we compute scores of partial episodes according to Equation 2, and record these partial scores alongside the partial episode step count in a separate buffer $\tilde{S}$. The function score then technically, optionally takes the additional input $\tilde{S}$ (through an abuse of notation) as an additional argument to stitch together this partial information into scores of full episodic trajectories.</p>
<p>Table 4. Comparison of test scores of PPO with PLR against PPO with uniform-sampling on the hard setting of Procgen Benchmark. Following (Raileanu et al., 2021), reported figures represent the mean and standard deviation of average test scores over 100 episodes aggregated across 5 runs, each initialized with a unique training seed. For each run, a normalized average return is computed by dividing the average test return for each game by the corresponding average test return of the uniform-sampling baseline over all 500 test episodes of that game, followed by averaging these normalized returns over all 16 games. The final row reports the mean and standard deviation of the normalized returns aggregated across runs. Bolded methods are not significantly different from the method with highest mean, unless all are, in which case none are bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Environment</th>
<th style="text-align: right;">Uniform</th>
<th style="text-align: right;">TSCL</th>
<th style="text-align: right;">mixreg</th>
<th style="text-align: right;">UCB-DrAC</th>
<th style="text-align: right;">PLR</th>
<th style="text-align: right;">UCB-DrAC + PLR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BigFish</td>
<td style="text-align: right;">$9.7 \pm 1.8$</td>
<td style="text-align: right;">$\mathbf{1 1 . 9} \pm \mathbf{2 . 5}$</td>
<td style="text-align: right;">$\mathbf{1 2 . 0} \pm \mathbf{2 . 5}$</td>
<td style="text-align: right;">$10.9 \pm 1.6$</td>
<td style="text-align: right;">$\mathbf{1 5 . 3} \pm \mathbf{3 . 6}$</td>
<td style="text-align: right;">$\mathbf{1 5 . 5} \pm \mathbf{2 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">BossFight</td>
<td style="text-align: right;">$\mathbf{9 . 6} \pm \mathbf{0 . 2}$</td>
<td style="text-align: right;">$8.4 \pm 0.7$</td>
<td style="text-align: right;">$\mathbf{9 . 3} \pm \mathbf{0 . 9}$</td>
<td style="text-align: right;">$9.0 \pm 0.2$</td>
<td style="text-align: right;">$\mathbf{9 . 7} \pm \mathbf{0 . 4}$</td>
<td style="text-align: right;">$\mathbf{9 . 5} \pm \mathbf{1 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">CaveFlyer</td>
<td style="text-align: right;">$3.5 \pm 0.8$</td>
<td style="text-align: right;">$6.3 \pm 0.6$</td>
<td style="text-align: right;">$4.0 \pm 1.0$</td>
<td style="text-align: right;">$2.6 \pm 0.8$</td>
<td style="text-align: right;">$6.4 \pm 0.6$</td>
<td style="text-align: right;">$\mathbf{8 . 0} \pm \mathbf{0 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Chaser</td>
<td style="text-align: right;">$5.9 \pm 0.5$</td>
<td style="text-align: right;">$6.2 \pm 1.0$</td>
<td style="text-align: right;">$\mathbf{6 . 5} \pm \mathbf{0 . 8}$</td>
<td style="text-align: right;">$\mathbf{7 . 0} \pm \mathbf{0 . 6}$</td>
<td style="text-align: right;">$\mathbf{6 . 8} \pm \mathbf{2 . 2}$</td>
<td style="text-align: right;">$\mathbf{7 . 6} \pm \mathbf{0 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Climber</td>
<td style="text-align: right;">$5.3 \pm 1.1$</td>
<td style="text-align: right;">$5.2 \pm 0.7$</td>
<td style="text-align: right;">$5.7 \pm 0.7$</td>
<td style="text-align: right;">$6.1 \pm 1.0$</td>
<td style="text-align: right;">$7.4 \pm 0.6$</td>
<td style="text-align: right;">$7.6 \pm 1.8$</td>
</tr>
<tr>
<td style="text-align: left;">CoinRun</td>
<td style="text-align: right;">$4.5 \pm 0.4$</td>
<td style="text-align: right;">$5.8 \pm 0.8$</td>
<td style="text-align: right;">$\mathbf{6 . 2} \pm \mathbf{1 . 0}$</td>
<td style="text-align: right;">$5.2 \pm 1.0$</td>
<td style="text-align: right;">$\mathbf{6 . 8} \pm \mathbf{0 . 6}$</td>
<td style="text-align: right;">$\mathbf{7 . 1} \pm \mathbf{0 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Dodgeball</td>
<td style="text-align: right;">$3.9 \pm 0.6$</td>
<td style="text-align: right;">$1.9 \pm 0.9$</td>
<td style="text-align: right;">$4.7 \pm 1.0$</td>
<td style="text-align: right;">$9.9 \pm 1.2$</td>
<td style="text-align: right;">$7.4 \pm 1.3$</td>
<td style="text-align: right;">$\mathbf{1 2 . 4} \pm \mathbf{0 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">FruitBot</td>
<td style="text-align: right;">$\mathbf{1 1 . 9} \pm \mathbf{4 . 2}$</td>
<td style="text-align: right;">$13.1 \pm 2.3$</td>
<td style="text-align: right;">$\mathbf{1 4 . 7} \pm \mathbf{2 . 2}$</td>
<td style="text-align: right;">$\mathbf{1 5 . 6} \pm \mathbf{3 . 7}$</td>
<td style="text-align: right;">$\mathbf{1 6 . 7} \pm \mathbf{1 . 0}$</td>
<td style="text-align: right;">$\mathbf{1 2 . 9} \pm \mathbf{5 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Heist</td>
<td style="text-align: right;">$1.5 \pm 0.4$</td>
<td style="text-align: right;">$0.9 \pm 0.3$</td>
<td style="text-align: right;">$1.2 \pm 0.4$</td>
<td style="text-align: right;">$1.1 \pm 0.3$</td>
<td style="text-align: right;">$1.3 \pm 0.4$</td>
<td style="text-align: right;">$2.6 \pm 2.2$</td>
</tr>
<tr>
<td style="text-align: left;">Jumper</td>
<td style="text-align: right;">$3.2 \pm 0.3$</td>
<td style="text-align: right;">$3.2 \pm 0.3$</td>
<td style="text-align: right;">$3.3 \pm 0.4$</td>
<td style="text-align: right;">$2.9 \pm 0.9$</td>
<td style="text-align: right;">$3.5 \pm 0.5$</td>
<td style="text-align: right;">$3.3 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: left;">Leaper</td>
<td style="text-align: right;">$7.1 \pm 0.3$</td>
<td style="text-align: right;">$\mathbf{7 . 5} \pm \mathbf{0 . 5}$</td>
<td style="text-align: right;">$\mathbf{7 . 5} \pm \mathbf{0 . 5}$</td>
<td style="text-align: right;">$3.8 \pm 1.6$</td>
<td style="text-align: right;">$\mathbf{7 . 4} \pm \mathbf{0 . 2}$</td>
<td style="text-align: right;">$\mathbf{8 . 2} \pm \mathbf{0 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Maze</td>
<td style="text-align: right;">$3.6 \pm 0.7$</td>
<td style="text-align: right;">$3.8 \pm 0.6$</td>
<td style="text-align: right;">$3.9 \pm 0.5$</td>
<td style="text-align: right;">$4.4 \pm 0.2$</td>
<td style="text-align: right;">$4.0 \pm 0.4$</td>
<td style="text-align: right;">$\mathbf{6 . 2} \pm \mathbf{0 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Miner</td>
<td style="text-align: right;">$12.8 \pm 1.4$</td>
<td style="text-align: right;">$11.7 \pm 0.9$</td>
<td style="text-align: right;">$13.3 \pm 1.6$</td>
<td style="text-align: right;">$\mathbf{1 6 . 1} \pm \mathbf{0 . 6}$</td>
<td style="text-align: right;">$11.3 \pm 0.7$</td>
<td style="text-align: right;">$\mathbf{1 5 . 3} \pm \mathbf{0 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Ninja</td>
<td style="text-align: right;">$5.2 \pm 0.1$</td>
<td style="text-align: right;">$5.9 \pm 0.8$</td>
<td style="text-align: right;">$5.0 \pm 1.0$</td>
<td style="text-align: right;">$5.2 \pm 1.0$</td>
<td style="text-align: right;">$\mathbf{6 . 1} \pm \mathbf{0 . 6}$</td>
<td style="text-align: right;">$\mathbf{6 . 9} \pm \mathbf{0 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Plunder</td>
<td style="text-align: right;">$3.2 \pm 0.1$</td>
<td style="text-align: right;">$5.4 \pm 1.1$</td>
<td style="text-align: right;">$3.7 \pm 0.4$</td>
<td style="text-align: right;">$7.8 \pm 1.1$</td>
<td style="text-align: right;">$8.6 \pm 2.7$</td>
<td style="text-align: right;">$\mathbf{1 7 . 5} \pm \mathbf{1 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">StarPilot</td>
<td style="text-align: right;">$5.5 \pm 0.6$</td>
<td style="text-align: right;">$2.1 \pm 0.4$</td>
<td style="text-align: right;">$6.9 \pm 0.6$</td>
<td style="text-align: right;">$\mathbf{1 1 . 2} \pm \mathbf{1 . 7}$</td>
<td style="text-align: right;">$5.4 \pm 0.8$</td>
<td style="text-align: right;">$\mathbf{1 2 . 3} \pm \mathbf{1 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Normalized test returns (\%)</td>
<td style="text-align: right;">$100.0 \pm 2.0$</td>
<td style="text-align: right;">$103.9 \pm 3.5$</td>
<td style="text-align: right;">$110.6 \pm 3.9$</td>
<td style="text-align: right;">$126.6 \pm 3.0$</td>
<td style="text-align: right;">$135.0 \pm 6.1$</td>
<td style="text-align: right;">$\mathbf{1 8 2 . 9} \pm \mathbf{8 . 2}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Left: Mean normalized train and test episode returns on Procgen Benchmark (hard). Right: Corresponding generalization gaps during training. All curves are averaged across all environments over 5 runs. The shaded area indicates one standard deviation around the mean. PLR-based methods statistically significantly outperform all others in both train and test returns. Only the PLR-based methods statistically significantly reduce the generalization gap $(p&lt;0.05)$.</p>
<p>Training performance on Procgen Benchmark (hard)
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Mean train episode returns ( 5 runs) on Procgen Benchmark (hard), using the best hyperparameters found on the easy setting. The shaded area indicates one standard deviation around the mean. A $\star$ indicates statistically significant improvement over the uniformsampling baseline by the PLR-based method of the matching color $(p&lt;0.05)$. Note that while PLR reduces training performance on StarPilot, it performs comparably to the uniform-sampling baseline at test time, indicating less overfitting to training levels.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12. Mean test episode returns (5 runs) on Procgen Benchmark (hard), using best hyperparameters found on the easy setting. The shaded area indicates one standard deviation around the mean. A $\star$ indicates statistically significant improvement over the uniform-sampling baseline by the PLR-based method of the matching color $(p&lt;0.05)$.</p>
<p>Training peformance on Procgen Benchmark (easy)
Uniform TSCL Window mixreg UCB-DrAC PLR UCB-DrAC + PLR
bigfish $\star \star$ bossfight $\star \star$ caveflyer
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13. Mean train episode returns (5 runs) on Procgen Benchmark (easy). The shaded area indicates one standard deviation around the mean. A $\star$ indicates statistically significant improvement over the uniform-sampling baseline by the PLR-based method of the matching color $(p&lt;0.05)$. PLR tends to improve or match training sample efficiency. Note that while PLR reduces training performance on StarPilot, it performs comparably to the uniform-sampling baseline at test time, indicating less overfitting to training levels.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14. Mean test episode return (10 runs) on each Procgen Benchmark game (easy). The shaded area indicates one standard deviation around the mean. PLR-based methods consistently match or outperform uniform sampling with statistically significance ( $p&lt;0.05$ ), indicated by a $\star$ of the corresponding color. We see that TSCL results in inconsistent outcomes across games, notably drastically lower test returns on StarPilot.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15. Mean generalization gaps throughout training ( 10 runs) on each Procgen Benchmark game (easy). The shaded area indicates one standard deviation around the mean. A $\star$ indicates the method of matching color results in a statistically significant ( $p&lt;0.05$ ) reduction in generalization gap compared to the uniform-sampling baseline. By itself, PLR significantly reduces the generalization gap on 7 games, and UCB-DrAC, on 5 games. This number jumps to 10 of 16 games when these two methods are combined. TSCL only significantly reduces generalization gap on 2 of 16 games relative to uniform sampling, while increasing it on others, most notably on Dodgeball.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Our code is available at https://github.com/ facebookresearch/level-replay.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
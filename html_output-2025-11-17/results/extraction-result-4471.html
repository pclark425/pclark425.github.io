<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4471 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4471</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4471</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-278782622</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.15050v3.pdf" target="_blank">Improving the fact-checking performance of language models by relying on their entailment ability</a></p>
                <p><strong>Paper Abstract:</strong> Automated fact-checking has been a challenging task for the research community. Past works tried various strategies, such as end-to-end training, retrieval-augmented generation, and prompt engineering, to build robust fact-checking systems. However, their accuracy has not been very high for real-world deployment. We, on the other hand, propose a simple yet effective strategy, where entailed justifications generated by LLMs are used to train encoder-only language models (ELMs) for fact-checking. We conducted a rigorous set of experiments, comparing our approach with recent works and various prompting and fine-tuning strategies to demonstrate the superiority of our approach. Additionally, we did quality analysis of model explanations, ablation studies, and error analysis to provide a comprehensive understanding of our approach.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4471.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4471.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Macro-F1 veracity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Macro-F1 score for claim veracity prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard multi-class classification metric (macro-averaged F1) used to evaluate veracity prediction performance across balanced classes; reported for training-based and inference-based experiments and compared to prior baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Macro-F1 (macro-averaged F1) for veracity prediction</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute F1 per class and average across classes (macro-average) to measure overall veracity classification performance across labels (e.g., true/false/half-true or 6-way labels). Applied to outputs of ELMs (fine-tuned) and prompted GLMs in both training-based experiments (TBE-1..TBE-3) and inference-based experiments (IBE-1..IBE-4). Used to compare models (ELMs/GLMs), input formats (raw evidence, generated understandings, entailed justifications), and adapters (LoRA/LoRA+).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Classification accuracy across classes summarized as macro-F1 (sensitivity to class balance and per-class F1), used to judge empirical adequacy and predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral, Llama, Gemma, Qwen, Falcon; RoBERTa, XLNet (as ELMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational linguistics / automated fact-checking (applied NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanatory justifications for factual claims / veracity classification (predictive classification models)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Training with entailed justifications (TBE-3) produced the largest gains: up to +28.57% macro-F1 on LIAR-RAW and +44.26% on RAW-FC relative to best baselines; baseline macro-F1 cited ~0.42 (LIAR-RAW) and 0.61 (RAW-FC). Many TBE-3 models (RoBERTa/XLNet fine-tuned on Llama entailed justifications) reached macro-F1 >0.5 on LIAR-RAW and up to ~0.94 on RAW-FC (RoBERTa-L Llama TBE-3 reported 0.88 MF1 in tables). Inference-based prompting (IBEs) performed worse overall (IBEs typically below baselines for LIAR-RAW; Llama IBE-2 surpassed baseline on RAW-FC).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: purely automated classification metric (macro-F1) computed on held-out test splits; compared across models and experimental conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standard train/validation/test splits from LIAR-RAW and RAW-FC; averaged over three random seeds; comparisons to prior reported baselines (HiSS, FactLLaMa, RAFTS, L-Defence).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Macro-F1 reflects classification performance but does not measure explanation quality or causal fidelity; sensitive to dataset label distributions and to differing label taxonomies across datasets (3-way vs 6-way). Large gains on RAW-FC may reflect dataset-specific artifacts; inference prompting methods struggled to leverage generated justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>LIAR-RAW and RAW-FC (train/val/test splits from Yang et al. [39])</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the fact-checking performance of language models by relying on their entailment ability', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4471.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4471.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lexical/Semantic Overlap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lexical-overlap and semantic-matching metrics (ROUGE, BLEU, BERTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic metrics to evaluate the similarity between generated entailed justifications (concatenated support+refute) and gold reference explanations, combining n-gram overlap and embedding-based semantic similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE-1, ROUGE-2, ROUGE-L, BLEU, BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>ROUGE-1 and ROUGE-2 measure unigram and bigram overlap between generated and gold explanations; ROUGE-L measures longest common subsequence; BLEU measures n-gram precision-based overlap; BERTScore computes contextual embedding cosine similarities (semantic matching) between generated and reference explanations. Metrics applied to concatenated supporting+refuting justifications produced by GLMs and compared across generator models (Mistral, Llama, Gemma, Qwen, Falcon).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Lexical fidelity (ROUGE/BLEU) and semantic closeness (BERTScore) of generated explanations to gold explanations; higher scores indicate closer match to reference rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral, Llama, Gemma, Qwen, Falcon (decoder-only GLMs used to generate explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational linguistics / explanation generation for fact-checking</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Generated textual justifications/explanations (rationales) for claim veracity</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Automatic overlaps were generally low-to-moderate: ROUGE-1 values ~0.17–0.23, ROUGE-2 ~0.04–0.07, ROUGE-L ~0.10–0.18 across models and datasets; BLEU very low (~0.02–0.08); BERTScore moderate (range reported ~0.24–0.40), indicating limited lexical overlap but non-trivial semantic similarity. Falcon attained highest ROUGE-1 for both LIAR-RAW (0.23) and RAW-FC (0.40); model rankings varied across metrics and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric suite (lexical and embedding-based) used as objective evaluation of explanation fidelity; complemented elsewhere by subjective human/LLM ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct comparison to gold concatenated justifications provided in datasets (gold explanations); reported per-model metric tables; results averaged over seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Low lexical overlap can understate explanation quality when phrasing differs; BLEU/ROUGE sensitive to exact wording and reference variability. BERTScore mitigates some semantic mismatch but can still produce inflated similarity for generic, plausible-sounding text. Metrics do not assess logical correctness or factual grounding beyond surface/embedding similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Gold concatenated supporting+refuting justifications from LIAR-RAW and RAW-FC (used as references)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the fact-checking performance of language models by relying on their entailment ability', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4471.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4471.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Subjective Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subjective evaluation of generated explanations by decoder-only LLMs across five dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompted decoder-only LLMs were used as automatic raters to score generated explanations on informativeness, logicality, objectivity, readability, and accuracy, producing per-dimension Likert-scale assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Decoder-only LLM subjective scoring on five dimensions (informativeness, logicality, objectivity, readability, accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generated justifications are evaluated by prompting decoder-only GLMs to rate explanations on five criteria (informativeness, logicality, objectivity, readability, accuracy) using a Likert scale (1–5). The same class of models (Mistral, Llama, Gemma, Qwen, Falcon) served both as explanation generators and as evaluators in different combinations; prompts are standardized (see Table 14 ID 14). Scores are aggregated to compare generator quality and correlated with downstream veracity performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Five subjective dimensions: informativeness, logicality (coherence of reasoning), objectivity (neutrality), readability (clarity/grammar), accuracy (factual correctness relative to reference).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral, Llama, Gemma, Qwen, Falcon (used as evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational linguistics / explainability evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Textual explanations/justifications (evaluative scoring of generated rationales)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLM evaluators rated Llama-generated explanations highest in most subjective dimensions: informativeness ~4.48–4.78, readability ~4.43–4.91, objectivity ~4.17–4.60, logicality ~4.26–4.68 (ranges across dataset/evaluator combinations). Falcon was often judged most accurate by some evaluators (accuracy ~3.55–3.96). Higher subjective ratings for Llama explanations correlated with better downstream veracity macro-F1 for ELMs trained on those explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated subjective evaluation (LLMs as raters) — an automated surrogate for human judgment, enabling large-scale subjective scoring; reported alongside human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation observed between LLM subjective scores (per-generator) and veracity prediction performance (ELM macro-F1) when trained on those explanations; no external gold-standard for scores other than cross-model consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Using the same model family as both generator and evaluator risks bias (evaluator may prefer text in its own style); LLM evaluators can be overconfident and may lack true external objectivity. Prompting design and evaluator selection drive variability.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Generated entailed justifications on LIAR-RAW and RAW-FC (evaluated across generator/evaluator combinations)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the fact-checking performance of language models by relying on their entailment ability', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4471.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4471.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Likert Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Likert-scale evaluation with inter-annotator agreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotators rated a sample of generated explanations on five criteria using a 1–5 Likert scale; inter-annotator agreement measured with Fleiss' Kappa and Krippendorff's Alpha to quantify reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human Likert-scale ratings + IAA (Fleiss' Kappa, Krippendorff's Alpha)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Three independent human annotators scored 40 randomly sampled explanations per dataset (LIAR-RAW, RAW-FC) on informativeness, accuracy, readability, objectivity, and logicality (1=poor to 5=excellent). Inter-annotator agreement across criteria was computed using Fleiss' Kappa and Krippendorff's Alpha to assess consistency and subjectivity of judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same five dimensions as LLM subjective eval: informativeness, accuracy, readability, objectivity, logicality; aggregated Likert scores used to characterize human-perceived explanation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-generated explanations were human-evaluated (primary example); other generators included for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational linguistics / human evaluation of explanations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Textual justifications/explanations for claims</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human ratings showed Llama explanations generally high on readability and logicality; inter-annotator agreement ranged from poor to moderate: Fleiss' Kappa/Krippendorff's Alpha reported ranges approx 0.12–0.54 for LIAR-RAW (better agreement on readability/logicality) and lower for RAW-FC (≈0.06–0.21), indicating substantial subjectivity in human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based evaluation (expert/annotator Likert ratings) used to validate explanation quality; compared with LLM subjective ratings to triangulate.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Inter-annotator agreement metrics (Fleiss' Kappa, Krippendorff's Alpha) used to validate reliability of human annotations; small sample (40 samples) used so statistical power limited.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Low IAA on some criteria (near-chance agreement) indicates subjectivity and limited reliability; small human sample size (40) reduces generalizability. Human ratings assess perceived quality, not objective factual grounding or causal correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Human-evaluated subset of LIAR-RAW and RAW-FC generated explanations</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the fact-checking performance of language models by relying on their entailment ability', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4471.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4471.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TBE/IBE Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training-Based Experiments (TBE) and Inference-Based Experiments (IBE) entailment pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-stage experimental framework that (1) classifies evidence sentences as supporting/refuting (entailment), (2) generates consolidated supporting and refuting justifications, and (3) uses those justifications to train an encoder-only model (ELM) for veracity prediction; includes both training-based (use justifications as training inputs) and inference-based (prompting chains at test time) variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Three-stage entailment-driven evaluation pipeline (TBE vs IBE experimental framework)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Stage 1: GLM classifies each evidence sentence as SUPPORT or REFUTE relative to a claim (entailment classification). Stage 2: GLM consolidates classified evidences into concise supporting and refuting justifications (generation). Stage 3: ELMs (RoBERTa/XLNet) are trained (TBE) or GLMs prompted at inference (IBE) to predict veracity using claim + generated justifications. TBE variants: TBE-1 (raw evidences), TBE-2 (overall understanding), TBE-3 (entailed justifications). IBE variants: IBE-1..IBE-4 correspond to zero-shot, understanding-based, CoT, and entailment-based prompting pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Effectiveness measured by downstream macro-F1 for veracity, explanation quality (lexical/semantic metrics), and ablation sensitivity; differentiates training-time incorporation of justifications vs. inference-time prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLMs as generators/classifiers: Mistral, Llama, Gemma, Qwen, Falcon; ELMs as veracity predictors: RoBERTa, XLNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational linguistics / automated fact-checking</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Procedural evaluation framework for generated explanations/justifications (applied evaluation of generated scientific/hypothesis-like explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>TBE-3 (training on entailed justifications) consistently produced the best veracity performance (largest macro-F1 gains over baselines). TBE-2 (training on GLM-generated 'understandings') produced moderate gains (e.g., +16.39% on RAW-FC). IBEs (inference-only) generally underperformed compared to TBEs and often failed to leverage generated justifications effectively; some IBE variants (IBE-2 using Llama) matched or exceeded baselines on RAW-FC but not LIAR-RAW.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Framework evaluation is empirical/automated (downstream macro-F1 and automatic explanation metrics) with supplementary human/LLM subjective evaluations for explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ablation studies (removing supporting/refuting justifications), segmentation by evidence-count, and comparisons across adapter/fine-tuning settings (LoRA, LoRA+); reported averages over three seeds and comparisons to prior baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Pipeline assumes closed-domain with gold evidence; performance may degrade in open-domain retrieval settings. IBEs struggled to make use of generated justifications at inference. The approach depends on quality of GLM entailment/classification and may propagate hallucinations. Resource constraints precluded fine-tuning some GLMs without adapters.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>LIAR-RAW and RAW-FC; experimental prompt templates and ablation splits documented in appendix (Table 14, Tables 9-11)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the fact-checking performance of language models by relying on their entailment ability', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4471.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4471.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablation + Attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation studies and attention-based linguistic analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diagnostic evaluation approach that removes supporting/refuting justifications to measure causal impact on veracity performance and inspects model attention heatmaps to analyze which words/facts models rely on for predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Justification ablation & attention heatmap analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Ablation: train/evaluate best-performing ELM models with (a) both supporting and refuting justifications, (b) without supporting justifications, (c) without refuting justifications, and (d) with neither (claim-only) to quantify the contribution of each justification component to macro-F1. Attention analysis: visualize top-k attention weights (top 25% most-attended words) in ELMs when processing support/refute justifications to interpret which tokens drive decisions, and compare patterns across true/half-true/false labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Causal contribution to predictive performance (percent drop in macro-F1 when components removed), interpretability via attention focus on salient factual tokens/phrases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLNet-L + Llama justifications (LIAR-RAW) and RoBERTa-L + Llama justifications (RAW-FC) used as case studies</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational linguistics / model interpretability for fact-checking</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Ablative analysis of explanatory inputs and attention-based interpretability of generated justifications</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Removing supporting justifications caused large drops in macro-F1: LIAR-RAW −31.48%, RAW-FC −12.50%. Removing refuting justifications produced milder but noticeable drops: LIAR-RAW −9.26%, RAW-FC −9.09%. Removing both justifications led to drastic degradations: LIAR-RAW −55.56%, RAW-FC −47.73%. Attention heatmaps showed concentrated attention on meaningful phrases for true/false labels, while attention was scattered for borderline labels (e.g., half-true).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (ablation quantified by macro-F1) combined with qualitative human-readable attention visualizations; no formal human evaluation of attention maps beyond examples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Controlled ablation experiments on held-out test sets; segmentation by number of evidences to test robustness; qualitative inspection of attention heatmaps with illustrative examples (Tables 12, 15).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Attention is an imperfect proxy for model reasoning and may not indicate causal importance; ablation isolates components but may interact with model training dynamics (e.g., models can compensate). Findings are dataset- and model-specific and may not generalize to open-domain settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving the fact-checking performance of language models by relying on their entailment ability', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>L-Defence <em>(Rating: 2)</em></li>
                <li>FactLLaMa <em>(Rating: 2)</em></li>
                <li>HiSS <em>(Rating: 2)</em></li>
                <li>Explainable Automated Fact-Checking for Public Health Claims <em>(Rating: 2)</em></li>
                <li>FEVER: a Large-scale Dataset for Fact Extraction and VERification <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4471",
    "paper_id": "paper-278782622",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Macro-F1 veracity",
            "name_full": "Macro-F1 score for claim veracity prediction",
            "brief_description": "Standard multi-class classification metric (macro-averaged F1) used to evaluate veracity prediction performance across balanced classes; reported for training-based and inference-based experiments and compared to prior baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Macro-F1 (macro-averaged F1) for veracity prediction",
            "evaluation_method_description": "Compute F1 per class and average across classes (macro-average) to measure overall veracity classification performance across labels (e.g., true/false/half-true or 6-way labels). Applied to outputs of ELMs (fine-tuned) and prompted GLMs in both training-based experiments (TBE-1..TBE-3) and inference-based experiments (IBE-1..IBE-4). Used to compare models (ELMs/GLMs), input formats (raw evidence, generated understandings, entailed justifications), and adapters (LoRA/LoRA+).",
            "evaluation_criteria": "Classification accuracy across classes summarized as macro-F1 (sensitivity to class balance and per-class F1), used to judge empirical adequacy and predictive performance.",
            "model_name": "Mistral, Llama, Gemma, Qwen, Falcon; RoBERTa, XLNet (as ELMs)",
            "model_size": null,
            "scientific_domain": "Computational linguistics / automated fact-checking (applied NLP)",
            "theory_type": "Explanatory justifications for factual claims / veracity classification (predictive classification models)",
            "human_comparison": false,
            "evaluation_results": "Training with entailed justifications (TBE-3) produced the largest gains: up to +28.57% macro-F1 on LIAR-RAW and +44.26% on RAW-FC relative to best baselines; baseline macro-F1 cited ~0.42 (LIAR-RAW) and 0.61 (RAW-FC). Many TBE-3 models (RoBERTa/XLNet fine-tuned on Llama entailed justifications) reached macro-F1 &gt;0.5 on LIAR-RAW and up to ~0.94 on RAW-FC (RoBERTa-L Llama TBE-3 reported 0.88 MF1 in tables). Inference-based prompting (IBEs) performed worse overall (IBEs typically below baselines for LIAR-RAW; Llama IBE-2 surpassed baseline on RAW-FC).",
            "automated_vs_human_evaluation": "Automated: purely automated classification metric (macro-F1) computed on held-out test splits; compared across models and experimental conditions.",
            "validation_method": "Standard train/validation/test splits from LIAR-RAW and RAW-FC; averaged over three random seeds; comparisons to prior reported baselines (HiSS, FactLLaMa, RAFTS, L-Defence).",
            "limitations_challenges": "Macro-F1 reflects classification performance but does not measure explanation quality or causal fidelity; sensitive to dataset label distributions and to differing label taxonomies across datasets (3-way vs 6-way). Large gains on RAW-FC may reflect dataset-specific artifacts; inference prompting methods struggled to leverage generated justifications.",
            "benchmark_dataset": "LIAR-RAW and RAW-FC (train/val/test splits from Yang et al. [39])",
            "uuid": "e4471.0",
            "source_info": {
                "paper_title": "Improving the fact-checking performance of language models by relying on their entailment ability",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Lexical/Semantic Overlap",
            "name_full": "Lexical-overlap and semantic-matching metrics (ROUGE, BLEU, BERTScore)",
            "brief_description": "Automatic metrics to evaluate the similarity between generated entailed justifications (concatenated support+refute) and gold reference explanations, combining n-gram overlap and embedding-based semantic similarity.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "ROUGE-1, ROUGE-2, ROUGE-L, BLEU, BERTScore",
            "evaluation_method_description": "ROUGE-1 and ROUGE-2 measure unigram and bigram overlap between generated and gold explanations; ROUGE-L measures longest common subsequence; BLEU measures n-gram precision-based overlap; BERTScore computes contextual embedding cosine similarities (semantic matching) between generated and reference explanations. Metrics applied to concatenated supporting+refuting justifications produced by GLMs and compared across generator models (Mistral, Llama, Gemma, Qwen, Falcon).",
            "evaluation_criteria": "Lexical fidelity (ROUGE/BLEU) and semantic closeness (BERTScore) of generated explanations to gold explanations; higher scores indicate closer match to reference rationales.",
            "model_name": "Mistral, Llama, Gemma, Qwen, Falcon (decoder-only GLMs used to generate explanations)",
            "model_size": null,
            "scientific_domain": "Computational linguistics / explanation generation for fact-checking",
            "theory_type": "Generated textual justifications/explanations (rationales) for claim veracity",
            "human_comparison": false,
            "evaluation_results": "Automatic overlaps were generally low-to-moderate: ROUGE-1 values ~0.17–0.23, ROUGE-2 ~0.04–0.07, ROUGE-L ~0.10–0.18 across models and datasets; BLEU very low (~0.02–0.08); BERTScore moderate (range reported ~0.24–0.40), indicating limited lexical overlap but non-trivial semantic similarity. Falcon attained highest ROUGE-1 for both LIAR-RAW (0.23) and RAW-FC (0.40); model rankings varied across metrics and datasets.",
            "automated_vs_human_evaluation": "Automated metric suite (lexical and embedding-based) used as objective evaluation of explanation fidelity; complemented elsewhere by subjective human/LLM ratings.",
            "validation_method": "Direct comparison to gold concatenated justifications provided in datasets (gold explanations); reported per-model metric tables; results averaged over seeds.",
            "limitations_challenges": "Low lexical overlap can understate explanation quality when phrasing differs; BLEU/ROUGE sensitive to exact wording and reference variability. BERTScore mitigates some semantic mismatch but can still produce inflated similarity for generic, plausible-sounding text. Metrics do not assess logical correctness or factual grounding beyond surface/embedding similarity.",
            "benchmark_dataset": "Gold concatenated supporting+refuting justifications from LIAR-RAW and RAW-FC (used as references)",
            "uuid": "e4471.1",
            "source_info": {
                "paper_title": "Improving the fact-checking performance of language models by relying on their entailment ability",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM Subjective Eval",
            "name_full": "Subjective evaluation of generated explanations by decoder-only LLMs across five dimensions",
            "brief_description": "Prompted decoder-only LLMs were used as automatic raters to score generated explanations on informativeness, logicality, objectivity, readability, and accuracy, producing per-dimension Likert-scale assessments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Decoder-only LLM subjective scoring on five dimensions (informativeness, logicality, objectivity, readability, accuracy)",
            "evaluation_method_description": "Generated justifications are evaluated by prompting decoder-only GLMs to rate explanations on five criteria (informativeness, logicality, objectivity, readability, accuracy) using a Likert scale (1–5). The same class of models (Mistral, Llama, Gemma, Qwen, Falcon) served both as explanation generators and as evaluators in different combinations; prompts are standardized (see Table 14 ID 14). Scores are aggregated to compare generator quality and correlated with downstream veracity performance.",
            "evaluation_criteria": "Five subjective dimensions: informativeness, logicality (coherence of reasoning), objectivity (neutrality), readability (clarity/grammar), accuracy (factual correctness relative to reference).",
            "model_name": "Mistral, Llama, Gemma, Qwen, Falcon (used as evaluators)",
            "model_size": null,
            "scientific_domain": "Computational linguistics / explainability evaluation",
            "theory_type": "Textual explanations/justifications (evaluative scoring of generated rationales)",
            "human_comparison": false,
            "evaluation_results": "LLM evaluators rated Llama-generated explanations highest in most subjective dimensions: informativeness ~4.48–4.78, readability ~4.43–4.91, objectivity ~4.17–4.60, logicality ~4.26–4.68 (ranges across dataset/evaluator combinations). Falcon was often judged most accurate by some evaluators (accuracy ~3.55–3.96). Higher subjective ratings for Llama explanations correlated with better downstream veracity macro-F1 for ELMs trained on those explanations.",
            "automated_vs_human_evaluation": "Automated subjective evaluation (LLMs as raters) — an automated surrogate for human judgment, enabling large-scale subjective scoring; reported alongside human annotation.",
            "validation_method": "Correlation observed between LLM subjective scores (per-generator) and veracity prediction performance (ELM macro-F1) when trained on those explanations; no external gold-standard for scores other than cross-model consistency.",
            "limitations_challenges": "Using the same model family as both generator and evaluator risks bias (evaluator may prefer text in its own style); LLM evaluators can be overconfident and may lack true external objectivity. Prompting design and evaluator selection drive variability.",
            "benchmark_dataset": "Generated entailed justifications on LIAR-RAW and RAW-FC (evaluated across generator/evaluator combinations)",
            "uuid": "e4471.2",
            "source_info": {
                "paper_title": "Improving the fact-checking performance of language models by relying on their entailment ability",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Human Likert Eval",
            "name_full": "Human Likert-scale evaluation with inter-annotator agreement",
            "brief_description": "Human annotators rated a sample of generated explanations on five criteria using a 1–5 Likert scale; inter-annotator agreement measured with Fleiss' Kappa and Krippendorff's Alpha to quantify reliability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Human Likert-scale ratings + IAA (Fleiss' Kappa, Krippendorff's Alpha)",
            "evaluation_method_description": "Three independent human annotators scored 40 randomly sampled explanations per dataset (LIAR-RAW, RAW-FC) on informativeness, accuracy, readability, objectivity, and logicality (1=poor to 5=excellent). Inter-annotator agreement across criteria was computed using Fleiss' Kappa and Krippendorff's Alpha to assess consistency and subjectivity of judgments.",
            "evaluation_criteria": "Same five dimensions as LLM subjective eval: informativeness, accuracy, readability, objectivity, logicality; aggregated Likert scores used to characterize human-perceived explanation quality.",
            "model_name": "Llama-generated explanations were human-evaluated (primary example); other generators included for comparison",
            "model_size": null,
            "scientific_domain": "Computational linguistics / human evaluation of explanations",
            "theory_type": "Textual justifications/explanations for claims",
            "human_comparison": true,
            "evaluation_results": "Human ratings showed Llama explanations generally high on readability and logicality; inter-annotator agreement ranged from poor to moderate: Fleiss' Kappa/Krippendorff's Alpha reported ranges approx 0.12–0.54 for LIAR-RAW (better agreement on readability/logicality) and lower for RAW-FC (≈0.06–0.21), indicating substantial subjectivity in human judgments.",
            "automated_vs_human_evaluation": "Human-based evaluation (expert/annotator Likert ratings) used to validate explanation quality; compared with LLM subjective ratings to triangulate.",
            "validation_method": "Inter-annotator agreement metrics (Fleiss' Kappa, Krippendorff's Alpha) used to validate reliability of human annotations; small sample (40 samples) used so statistical power limited.",
            "limitations_challenges": "Low IAA on some criteria (near-chance agreement) indicates subjectivity and limited reliability; small human sample size (40) reduces generalizability. Human ratings assess perceived quality, not objective factual grounding or causal correctness.",
            "benchmark_dataset": "Human-evaluated subset of LIAR-RAW and RAW-FC generated explanations",
            "uuid": "e4471.3",
            "source_info": {
                "paper_title": "Improving the fact-checking performance of language models by relying on their entailment ability",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "TBE/IBE Pipeline",
            "name_full": "Training-Based Experiments (TBE) and Inference-Based Experiments (IBE) entailment pipeline",
            "brief_description": "A three-stage experimental framework that (1) classifies evidence sentences as supporting/refuting (entailment), (2) generates consolidated supporting and refuting justifications, and (3) uses those justifications to train an encoder-only model (ELM) for veracity prediction; includes both training-based (use justifications as training inputs) and inference-based (prompting chains at test time) variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Three-stage entailment-driven evaluation pipeline (TBE vs IBE experimental framework)",
            "evaluation_method_description": "Stage 1: GLM classifies each evidence sentence as SUPPORT or REFUTE relative to a claim (entailment classification). Stage 2: GLM consolidates classified evidences into concise supporting and refuting justifications (generation). Stage 3: ELMs (RoBERTa/XLNet) are trained (TBE) or GLMs prompted at inference (IBE) to predict veracity using claim + generated justifications. TBE variants: TBE-1 (raw evidences), TBE-2 (overall understanding), TBE-3 (entailed justifications). IBE variants: IBE-1..IBE-4 correspond to zero-shot, understanding-based, CoT, and entailment-based prompting pipelines.",
            "evaluation_criteria": "Effectiveness measured by downstream macro-F1 for veracity, explanation quality (lexical/semantic metrics), and ablation sensitivity; differentiates training-time incorporation of justifications vs. inference-time prompting.",
            "model_name": "GLMs as generators/classifiers: Mistral, Llama, Gemma, Qwen, Falcon; ELMs as veracity predictors: RoBERTa, XLNet",
            "model_size": null,
            "scientific_domain": "Computational linguistics / automated fact-checking",
            "theory_type": "Procedural evaluation framework for generated explanations/justifications (applied evaluation of generated scientific/hypothesis-like explanations)",
            "human_comparison": false,
            "evaluation_results": "TBE-3 (training on entailed justifications) consistently produced the best veracity performance (largest macro-F1 gains over baselines). TBE-2 (training on GLM-generated 'understandings') produced moderate gains (e.g., +16.39% on RAW-FC). IBEs (inference-only) generally underperformed compared to TBEs and often failed to leverage generated justifications effectively; some IBE variants (IBE-2 using Llama) matched or exceeded baselines on RAW-FC but not LIAR-RAW.",
            "automated_vs_human_evaluation": "Framework evaluation is empirical/automated (downstream macro-F1 and automatic explanation metrics) with supplementary human/LLM subjective evaluations for explanations.",
            "validation_method": "Ablation studies (removing supporting/refuting justifications), segmentation by evidence-count, and comparisons across adapter/fine-tuning settings (LoRA, LoRA+); reported averages over three seeds and comparisons to prior baselines.",
            "limitations_challenges": "Pipeline assumes closed-domain with gold evidence; performance may degrade in open-domain retrieval settings. IBEs struggled to make use of generated justifications at inference. The approach depends on quality of GLM entailment/classification and may propagate hallucinations. Resource constraints precluded fine-tuning some GLMs without adapters.",
            "benchmark_dataset": "LIAR-RAW and RAW-FC; experimental prompt templates and ablation splits documented in appendix (Table 14, Tables 9-11)",
            "uuid": "e4471.4",
            "source_info": {
                "paper_title": "Improving the fact-checking performance of language models by relying on their entailment ability",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Ablation + Attention",
            "name_full": "Ablation studies and attention-based linguistic analysis",
            "brief_description": "A diagnostic evaluation approach that removes supporting/refuting justifications to measure causal impact on veracity performance and inspects model attention heatmaps to analyze which words/facts models rely on for predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Justification ablation & attention heatmap analysis",
            "evaluation_method_description": "Ablation: train/evaluate best-performing ELM models with (a) both supporting and refuting justifications, (b) without supporting justifications, (c) without refuting justifications, and (d) with neither (claim-only) to quantify the contribution of each justification component to macro-F1. Attention analysis: visualize top-k attention weights (top 25% most-attended words) in ELMs when processing support/refute justifications to interpret which tokens drive decisions, and compare patterns across true/half-true/false labels.",
            "evaluation_criteria": "Causal contribution to predictive performance (percent drop in macro-F1 when components removed), interpretability via attention focus on salient factual tokens/phrases.",
            "model_name": "XLNet-L + Llama justifications (LIAR-RAW) and RoBERTa-L + Llama justifications (RAW-FC) used as case studies",
            "model_size": null,
            "scientific_domain": "Computational linguistics / model interpretability for fact-checking",
            "theory_type": "Ablative analysis of explanatory inputs and attention-based interpretability of generated justifications",
            "human_comparison": false,
            "evaluation_results": "Removing supporting justifications caused large drops in macro-F1: LIAR-RAW −31.48%, RAW-FC −12.50%. Removing refuting justifications produced milder but noticeable drops: LIAR-RAW −9.26%, RAW-FC −9.09%. Removing both justifications led to drastic degradations: LIAR-RAW −55.56%, RAW-FC −47.73%. Attention heatmaps showed concentrated attention on meaningful phrases for true/false labels, while attention was scattered for borderline labels (e.g., half-true).",
            "automated_vs_human_evaluation": "Automated (ablation quantified by macro-F1) combined with qualitative human-readable attention visualizations; no formal human evaluation of attention maps beyond examples.",
            "validation_method": "Controlled ablation experiments on held-out test sets; segmentation by number of evidences to test robustness; qualitative inspection of attention heatmaps with illustrative examples (Tables 12, 15).",
            "limitations_challenges": "Attention is an imperfect proxy for model reasoning and may not indicate causal importance; ablation isolates components but may interact with model training dynamics (e.g., models can compensate). Findings are dataset- and model-specific and may not generalize to open-domain settings.",
            "uuid": "e4471.5",
            "source_info": {
                "paper_title": "Improving the fact-checking performance of language models by relying on their entailment ability",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "L-Defence",
            "rating": 2
        },
        {
            "paper_title": "FactLLaMa",
            "rating": 2
        },
        {
            "paper_title": "HiSS",
            "rating": 2
        },
        {
            "paper_title": "Explainable Automated Fact-Checking for Public Health Claims",
            "rating": 2,
            "sanitized_title": "explainable_automated_factchecking_for_public_health_claims"
        },
        {
            "paper_title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
            "rating": 1,
            "sanitized_title": "fever_a_largescale_dataset_for_fact_extraction_and_verification"
        }
    ],
    "cost": 0.017440249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving the fact-checking performance of language models by relying on their entailment ability
21 Oct 2025</p>
<p>Gaurav Kumar gaurav22@iiserb.ac.in 
Debajyoti Mazumder debajyoti22@iiserb.ac.in 
Jasabanta Patro jpatro@iiserb.ac.in </p>
<p>Indian Institute of Science Education and Research Department of Data Science and Engineering Bhopal
India</p>
<p>Indian Institute of Science Education and Research Department of Data Science and Engineering Bhopal
India</p>
<p>Indian Institute of Science Education and Research Department of Data Science and Engineering Bhopal
India</p>
<p>Indian Institute of Science Education and Research Department of Data Science and Engineering Bhopal
India</p>
<p>Improving the fact-checking performance of language models by relying on their entailment ability
21 Oct 2025F4B77FA57163F5B511485197F94121F7arXiv:2505.15050v3[cs.CL]Frozen parameters:
Automated fact-checking has been a challenging task for the research community.Past works tried various strategies, such as end-to-end training, retrieval-augmented generation, and prompt engineering, to build robust fact-checking systems.However, their accuracy has not been very high for real-world deployment.We, on the other hand, propose a simple yet effective strategy, where entailed justifications generated by LLMs are used to train encoderonly language models (ELMs) for fact-checking.We conducted a rigorous set of experiments, comparing our approach with recent works and various prompting and fine-tuning strategies to demonstrate the superiority of our approach.Additionally, we did quality analysis of model explanations, ablation studies, and error analysis to provide a comprehensive understanding of our approach.CCS Concepts• Computing methodologies → Natural language processing; Neural networks; • Applied computing → Computational linguistics; • Social and professional topics → Fact-checking.</p>
<p>not high enough to deploy in the real world.The primary reason behind this is that fact verification is a complex process.For a given claim, language models have to parse through multiple pieces of evidence, which often contradict each other.The contradictory pieces of evidence confuse the language models, leading to poor performance.</p>
<p>Prompt-engineering approaches, on the other hand, have achieved human-level performance when used in conjunction with large generative language model (with over a trillion parameters) [25].However, their deployment is costly, as large GLMs usually remain behind a paywall.Using prompting approaches on smaller GLMs such as Mistral-7B [16], Llama-7B [1], etc, may seem to be a solution, but (i) their performance is lower than training-based approaches, and (ii) the model outcome often suffers from bias and hallucination.To overcome this, we propose a simple yet effective strategy, where we rely on (i) the entailing and the generative ability of GLMs, and (ii) the task-specific learning ability of ELMs.Particularly, we follow a three-step process as illustrated in Figure 1.In the first step, given a claim and its associated evidence sentences, we ask the GLM to classify each evidence sentence as either "supporting" or "refuting" the claim.Based on the classification, in the second step, we ask the GLM to generate "supporting" and "refuting" justification advocating the claim to be true or false.Finally, in the last step, we train the ELM to take the entailed justifications as input and predict the veracity.Our approach was inspired by Wang et al. [32], where authors used an attention-based module to capture claim-evidence entailment (top-k evidences).Replacing attention module with a GLM makes the new approach simpler, scalable, sustainable and easy to deploy.Furthermore, unlike Wang et al. [32], we experimented with a variety of GLMs and ELMs to demonstrate the efficacy of our approach with respect to the baselines.Additionally, we conducted a variety of training (TBE) and inference (IBE) based experiments for different input scenarios (taking raw claims-evidence pairs and their understanding).All of our experiments are done around three research questions.They are,</p>
<p>• R1: "How well do the language models perform when only claim and raw evidence sentences are available?"• R2: "Does prompted claim-evidence understanding improve the performance of the language models?• R3: "Can prompted entailed justifications improve the claim veracity prediction?"</p>
<p>Our key contributions and observations in this work can be summarised as follows,</p>
<p>• We conducted three training-based (TBEs) and four inferencebased (IBEs) experiments along the line of R1, R2 and R3.</p>
<p>For the RAW-FC dataset, training with raw evidence sentences (TBE-1) and overall claim-evidence understanding (TBE-2) registered an improvement up to 8.20% and 16.39% in macro-F1 ( 1) over the baselines.Training with the entailed justifications (TBE-3), on the other hand, outperformed the baselines by a huge margin (up to 28.57% and 44.26% for LIAR-RAW and RAW-FC, respectively).• We considered the entailed justifications generated by GLMs as model explanations, and evaluated them using two strategies: (i) checking lexical overlap and semantic matching, and (ii) doing subjective evaluation by GLMs itself.Notably, explanations generated by Llama received the highest subjective scores, correlating with the highest veracity prediction performance.• We conducted (i) an ablation study by removing individual supporting and refuting entailed justifications and (ii) a thorough linguistic analysis of our models.We found that both of the entailed justifications are important, as their removal leads to a decline in  performance.In linguistic analysis, we found that LLM could give high attention scores to appropriate keywords and factual pieces for samples with 'true' and 'false' labels.However, for samples with borderline (e.g.half-true) labels, attention scores seem to be scattered.</p>
<p>Related works:</p>
<p>Researchers have focused on the core and peripheral tasks of automated fact-checking for a long time.Several datasets and methodologies have been proposed, spanning different domains, languages, and task frameworks [12].In the following, we have reported the existing literature on monolingual, multilingual, and multimodal fact-checking [2,12].We found several datasets focusing on monolingual fact-checking.Some of the popular ones are FEVER [31], FEVEROUS [6], VITAMIN-C [26], LIAR-RAW and RAW-FC [39].Datasets produced as part of the FEVER series were synthetic, large (more than 185k samples), and primarily sourced from Wikipedia.The LIAR series datasets [39], on the other hand, are medium-sized (2k-12k samples) and expert-annotated.Both series are domain-agnostic and mainly used to train general-domain fact-checking models.We have domainspecific datasets as well.For example, the PUBHEALTH (11.8K claims) dataset by Kotonya and Toni [17], and the SciClaimHunt dataset by [12] focused on fact-checking in the healthcare and science domains.A complete list of datasets focusing on automated fact-checking is reported in [12].In multilingual settings, datasets such as FakeCovid [28] (40 languages), X-Fact [13] (25 languages) are popular.Prior surveys [34] provide a complete list of datasets and methods for multi-lingual fact-checking.The research community have proposed several datasets for multi-modal fact-checking as well.Datasets such as FactDrill [12] combine video, audio, image, text, and metadata, while r/Fakeddit [22], MuMiN [23], MOCHEG [40], and Factify-2 [29] datasets focus only on image-text pairs.Past studies [2] have reported the list of datasets and methods proposed for multimodal fact checking.</p>
<p>Approach-wise, we see an evolution of proposed methods in parallel with the evolution of the AI-ML field.Traditional fact-checking [12,42] approaches have focused on manual feature engineering and getting signals from external sources.With the emergence of deep learning, researchers have shifted their focus to learn latent features using neural architectures.The wide popularity of LLMs directed the research on fact-checking as well.Many modern approaches have fine-tuned the LLMs or used neural architectures on top of the LLMs for fact-checking [27].Recently, with the emergence of large generative language models, researchers have been exploring prompt engineering approaches [25] for fact-checking.Some of the methods they have tried are zero-shot, few-shot, and chain-of-thought prompting [25], among others.The accuracy, however, is not high enough to deploy them in the real world.3 Dataset details:</p>
<p>In this section, we have reported the details of the datasets considered in our study.We considered the LAIR-RAW and RAW-FC datasets provided by Yang et al. [39].We have chosen them as they are widely used in past work such as HiSS [44], FactLLaMa [7], RAFTS [41], L-Defence [32], and others [33,36].While each sample of LAIR-RAW is annotated with one of six veracity labels, samples of RAW-FC follows a three-way labeling scheme.The datasets are open-sourced under the Apache 2.0 license.We have used the training, validation, and test splits originally provided by Yang et al. [39].</p>
<p>The label distribution across the training splits were reported in Table 1.  4 Experiments:</p>
<p>GLMs</p>
<p>In this section, we have reported the details of the experiments that we conducted in this study.As said earlier, we conducted three   2.In TBEs, we finetuned (i) ELMs like RoBERTa [20] and XLNet [38], and (ii) GLMs like Mistral [16], Llama [1], Gemma [30], Qwen [37] and Falcon [4] with LoRA [15] and LoRA+ [14] adapters.</p>
<p>The maximum supported input lenght and the diffrent version of ELMs and GLMs are reported in Table 2.Note that, unlike ELMs, we can not directly fine-tune GLMs (without adapters) due to their large parameter size and computational constraints.Similarly, in IBEs, we prompted the considered GLMs to predict the veracity.</p>
<p>The details of individual experiments and the current state-of-theart baselines are reported in the subsequent subsections.</p>
<p>Training</p>
<p>Based Experiments (TBEs): , where, we fine-tuned the GLMs (using LoRA and LoRA+) by giving claims and raw evidence sentences.If any sample doesn't have any associated evidence sentence, then we gave only claim as the input.We restricted ourselves from using ELMs like RoBERTa [20] and XLNet [38], as the length of many input instances exceeded their maximum supported input length (Table 2).Past work [7] demonstrated the effectiveness of fine-tuning Llama with LoRA adapters using evidence pieces from web search, whereas in our case, we used the gold evidence sentences given in the dataset.To the best of our knowledge, we believe we are the first to fine-tune the GLMs using raw evidence sentences.Our approach is illustrated in sub-figure (a) of Figure 2.</p>
<p>TBE-2:</p>
<p>Training based on overall understanding: Our second experiment in this line was based on R2.To conduct it, we first prompted the five considered GLMs to generate their understanding of a given claim and its evidence pieces.For samples which don't have an associated evidence sentence, GLMs generated their understandings based on only the claim.With that, we fine-tuned the ELMs and GLMs (with adapters) to produce the claim veracity.The detailed experimental process is illustrated in sub-figure (b) of Figure 2. The prompts used are reported at ID #3 in Table 14 in the appendix.To conduct it, we followed a three-step approach.In the first step, we prompted the considered GLMs to classify if the associated evidence sentences are "supporting" or "refuting" a given claim.In the second step, we prompted the language models to generate supporting and refuting justifications based on the classified evidence sentences.For the cases where claims don't have any supporting or refuting evidence  sentences, GLMs generated justifications based on their embedded knowledge.Finally, based on the claims and the generated justifications, we fine-tuned the (i) considered ELMs and (ii) adapters with the GLMs, to generate the veracity.The detailed approach is illustrated in sub-figure (c) of Figure 2. Some prompt samples we used at each step are reported in Table 14 (ID 9,10,11) in the appendix.</p>
<p>Inference Based Experiments (IBEs):</p>
<p>We have conducted four inference-based experiments (IBEs) based on R1, R2, and R3.These include: (i) zero-shot prompting (IBE-1), where we prompted GLMs to predict claim veracity directly from claims and associated evidence sentences, unlike prior studies [7,44] that either used only the claims or relied on web-retrieved evidence; (ii) zero-shot prompting with overall understanding (IBE-2), where we first prompted GLMs to generate claim-evidence understanding and then used it to predict veracity, a two-step prompting strategy, which was not explored previously; (iii) CoT prompting with overall understanding (IBE-3), which extended IBE-2 by explicitly instructing GLMs to produce step-by-step reasoning, differing from prior CoT approaches [44] that depended on external evidence retrieval.In contrast, our method leverages GLMs generated claimevidence understanding, thereby avoiding dependency on external retrieval, and (iv) prompting based on entailment (IBE-4), where we prompted GLMs to predict veracity using entailed justifications marking the first attempt to classify each evidence sentence as supporting or refuting a claim and then generate entailment-based justifications for fact verification, unlike prior entailment works [8,9].For claims without associated evidence, only the claim text was used as input.Representative prompt samples for each step are listed in Table 14 (IDs 1-12) in the appendix.</p>
<p>Baselines:</p>
<p>In this section, we have reported the previously proposed bestperforming models as the baselines.Particularly, we compared our models with the performances of HiSS [44], FactLLaMa [7], RAFTS [41], and L-Defence [32] models.Out of them, HiSS [44] and FactL-LaMa [7] retrieved evidence from external sources, while RAFTS [41] employed a coarse-to-fine retrieval technique to extract evidence directly from the dataset.In contrast, L-Defense [32] used relevant evidence without additional retrieval.The previously reported performance of these models on LIAR-RAW and RAW-FC datasets are presented in Table 3.</p>
<p>Parameters Values</p>
<p>Learning rate {2e-6, 2e-</p>
<p>Experimental set-up:</p>
<p>4.4.1 Evaluation metrics: We used standard metrics such as macro-f1 ( 1) to evaluate our models.To assess explanation quality (based on concatenated supporting and refuting entailed justifications from TBE-3), we employed two types of strategies: (i) checking lexical overlap and semantic matching, and (ii) doing subjective evaluation by decoder-only LLMs.To check lexical overlap, we used several standard evaluation metrics such as ROUGE-1 ( 1 ), ROUGE-2 ( 2 ), ROUGE-L (  ) [19] and BLEU [24].While  1 and  2 measure the overlap of unigrams and bigrams between predicted and gold explanations,   measures the longest common subsequence.BLEU, on the other hand, measures the precision of matching n-grams between predicted and gold explanations.To measure the semantic matching between predicted and gold explanations, we deployed BERT score [43].It generates contextual embeddings of predicted and gold explanations and calculates cosine similarity between them.To do the subjective evaluation, we prompted the considered decoder-only LLMs to assess the model explanations generated by each decoder-only LLMs.The decoder-only LLMs were asked to assess across five dimensions (i) informativeness, (ii) logicality, (iii) objectivity, (iv) readability and (v) accuracy [45].The prompt template used for the assessment is presented at ID 14 in Table 14.</p>
<p>Hyperparameter details:</p>
<p>We did an extensive hyperparameter search that led to the optimal performance of our models.The list of hyperparameters for which we trained our models is presented in Table 4.For the VLLMs, we kept the temperature constant at '0.001' for consistency.We conducted all our experiments on a NVIDIA A100 80GB GPU cards.</p>
<p>Results and Discussion:</p>
<p>In this section, we have reported the results of our experiments.</p>
<p>Observations for veracity prediction:</p>
<p>We reported the  1 of various models under TBE-1, TBE-2 -TBE-3 and IBEs in Table 5, Table 6, and Table 7 respectively.Our observations are as follows • For LIAR-RAW, none of the macro-F1 reported by IBE models could surpass the best baseline performance.In contrast, for RAW-FC, Llama achieved the highest performance in IBE-2, surpassing the highest baseline performance.</p>
<p>• In TBE-1, for LIAR-RAW, the highest reported  1 does not surpass that of baselines.However, for RAWFC, we found that many models, such as (i) Mistral (∼ 6.56% ↑), Llama (∼ 6.56% ↑) and Qwen (∼ 8.20% ↑) trained with LoRA, and (ii) Llama (∼ 6.56% ↑), Qwen (∼ 6.56% ↑) and Falcon (∼ 3.28% ↑) trained with LoRA+, outperform the best baseline performance ( 1: 0.61).</p>
<p>Observations from model explainability:</p>
<p>In this section, we have reported our observations from evaluating model explanations.The results of lexical-overlapping and semantic-matching based evaluations are reported in Table 8.Similarly, the results of subjective evaluations are reported in Table 10.Some of the key findings we got are,</p>
<p>Observations from ablation study:</p>
<p>In this section, we have reported our observations from the ablation study.We took the best-performing models (XLNet-L Llama for LIAR-RAW and RoBERTa-L Llama for RAW-FC) for both datasets and removed individual justification (supporting and refuting) components during training to gauge their impact.The results we got for different training scenarios are reported in Table 9. Apart from that, we also segmented the test set into six parts based on the number of evidence pieces each claim has, and calculated their performances.The segment-wise performance scores are reported in Table 11a and Table 11b, respectively.We observed the following,</p>
<p>• When supporting justifications were removed, macro-F1 ( 1) scores dropped sharply by 31.48% for LIAR-RAW and 12.50% for RAW-FC.In contrast, the removal of refuting justifications had a milder but still noticeable impact, where  1 decrease of 9.26% and 9.09% for LIAR-RAW and RAW-FC, respectively.When both supporting and refuting justifications were excluded, model performance degraded drastically, with  1 decreasing by 55.56% for LIAR-RAW and 47.73% for RAW-FC.These findings emphasise that removing the supporting justifications had a more adversarial impact than removing refuting justifications, and removing both had the highest adversarial impact.• While investigating the behaviour with a varying number of evidence, we observed the following.For LIAR-RAW, performance of veracity predictor peaked with '6-20' evidences ( 1: 0.61).It showed significant sensitivity to increasing number of evidences with  1 dropping 29.50% from '6-20' evidences ( 1: 0.61) to '&gt;50' evidences ( 1: 0.43).However, for RAW-FC, the highest  1 were observed with '11-20' evidences ( 1: 0.94).It showed more robustness with  1 decreasing slightly 7.45% from '11-20' evidences ( 1: 0.94) to '&gt;50' evidences ( 1: 0.87).</p>
<p>Linguistic insights:</p>
<p>In this section, we analyzed how the best-performing models (XLNet-L Llama for LIAR-RAW and RoBERTa-L Llama for RAW-FC) assign attention to words and phrases when predicting veracity.We presented some of the samples, their gold labels, predicted labels, support justifications, and refute justifications in Table 15 and 12. Attention heatmaps highlighted the top 25% most-attended words, showing that models focus on meaningful phrases in support and refute justifications depending on the label.Here, the higher intensity of blue color indicates a higher attention score, while the red color indicates low attention.We outlined some of the notable findings from both datasets in the paragraph below.</p>
<p>For the LIAR-RAW dataset, where both the gold and predicted labels are 'true', high attention was focused on keywords such as 'supported by multiple sources', 'evidence', 'supports' in the support justification part.In contrast, attention scores were more scattered in the refute justification, which lacks strong phrases directly refuting the claim with confidence.However, for the label 'half-true', attention was split between conflicting phrases.We observed more attention on expressions like 'appears that the claim is accurate' in support justification and 'inconsistencies undermine the validity of the claim' in refute justification.Finally, for the label 'false', the ELM assigned higher attention scores to economic indicators, due to the absence of reasons to support the claim.Meanwhile, the refuting justification contains statements that disprove the claim, with ELM showed greater attention toward terms such as 'layoffs', 'assertion', and 'deaths' which offer counter-evidence against the claim.The illustrative examples of these insights are shown in Table 15.</p>
<p>In the Raw-FC dataset, when the label was 'true', strong attention was observed to credible sources like Fintan O's (well-known columnist) writing and reputation of 'Irish Times'.The ELM also assigned higher attention scores to these details, which supported the claim with authority.In contrast, the refute justification seem to be weaker in disproving the claim due to the lack of refuting evidence.For the label 'half-true', the ELM put high attention on keywords like 'evidence', 'statement is true', 'credibility' and 'support' around factual components justifying truthfulness in the support justification.While in the refute justification, we observed higher attention scores on keywords like 'false', and phrases like 'disputes the claim' and 'In conclusion, the evidence suggests that the claim is false'.This distribution reflects how the ELM processed the conflicting narratives and ultimately assigned a 'half-true' label.Finally, for the label 'false', the refute justification generated by GLM supported its reasoning using facts like 'event being from 2011, not during COVID', and the horses were rehomed well before the claim was made.Thus, the ELM could extend higher attention scores to phrases like 'claim is not supported by facts' and 'outdated story' leading to 'false' label.In contrast, the support justification the  GLM could not find any solid fact to put more attention to it.The examples that illustrate these insights are presented in Table 12.</p>
<p>Human Evaluation</p>
<p>We conducted a human evaluation on 40 randomly selected samples from LIAR-RAW and RAW-FC, where three independent annotators assessed explanations generated by the Llama model across five dimensions: Informativeness, Accuracy, Readability, Objectivity, and Logicality.Ratings were given on a Likert scale (1: Poor to 5: Excellent) consistent with prior GLMs evaluations, and inter-annotator agreement was measured using Fleiss' Kappa () [11], Krippendorff's Alpha () [18].For LIAR-RAW, agreement ranged from poor to moderate ( ≈ 0.12-0.46, up to 0.54), indicating reasonable consistency particularly for readability and logicality.In contrast, RAW-FC exhibited lower agreement overall ( ≈ 0.076-0.21, from 0.06 to 0.20), reflecting greater subjectivity in judgments.Detailed results are provided in Table 13.</p>
<p>Conclusion:</p>
<p>We drew the following conclusions from our experiments:</p>
<p>• Our study showed that incorporating entailment-based justifications makes fact-checking systems work better.Training language models with GLM entailed justifications surpassed the baseline macro-F1 scores substantially with an improvement of 28.57% and 44.26% for LIAR-RAW and RAW-FC, respectively.The approach of training with claim-evidence understanding (TBE-2) secured the second spot, with an increment of 16.39% in the RAW-FC dataset compared to the best baseline macro-F1.In contrast, the inference-based methods (IBEs) were unable to understand the justifications generated from GLM and performed consistently poor.• The subjective evaluation found that Llama-generated explanations were more informative, readable, and logically structured.This aligns with the superior performance of models such as XLNet and RoBERTa (in TBE-3), which leveraged these explanations as inputs.• The role of GLM entailed justification as a second step in TBE-3 is justified in the ablation study, where we observed that removing supporting and refuting justification adversarially affected the scores.• Linguistic analysis also showed that ELMs focused more accurately on important factual words and signals that indicate whether a statement supports or contradicts a claim.However, for samples with other veracity labels, attention seem to be scattered.</p>
<p>ID Content with Attention Scores 1</p>
<p>Claim: " Police pepper-sprayed a double amputee and removed his prosthetic legs during a June 21, 2020, protest against police brutality in Columbus, Ohio."Gold Label: "half" Predicted Label: "half" Support Justifications: "Based on the provided evidence, I conclude that the statement is TRUE.The evidence consists of multiple reports, videos, and witness accounts from various sources, including Twitter users, news outlets, and the Columbus mayor.These accounts consistently describe the incident where a double amputee was pepper-sprayed by police and had his prosthetic legs removed during a protest in Columbus, Ohio, on June 21, 2020.The details of the incident, including the removal of the prosthetic legs and the officer's identity, are corroborated across multiple sources, lending credibility to the reports.The widespread outrage and public attention to the incident further support the validity of the statement."Refute Justifications: " The claim that police pepper-sprayed a double amputee and removed his prosthetic legs during a June 21, 2020, protest against police brutality in Columbus, Ohio, is false.This conclusion is supported by multiple lines of evidence.Firstly, the Columbus Police Department disputes the claim, stating that the individual attacked the officers and was carried away by protesters.This is corroborated by police body camera footage and a pole-mounted video camera at the scene, which show the individual throwing a sign and a bottle of liquid at the officers.Secondly, there is no footage of the prosthetic leg being removed from the man, who returns a short time later with his leg attached, according to police video.This suggests that the prosthetic leg was not taken by the police.Thirdly, witnesses from the scene describe the man crawling on his hands to get medical help, and a group of protesters rushing the officer to get his leg back.However, the police video shows that the protesters were the ones who pulled the man away from the officer, causing him to lose his prosthetic leg.Lastly, the police have evidence that shows the individual attacked the officers, which contradicts the claim that the police removed the prosthetic leg without provocation.In conclusion, the evidence suggests that the claim is false, and the police did not remove the prosthetic leg from the double amputee.The incident was a result of a violent clash between the police and protesters, and the man's prosthetic leg was lost due to the actions of the protesters, not the police." 2</p>
<p>Claim: "52 thoroughbred horses whose owner died from COVID-19 are destined for slaughter if adopters are not found for them soon."Gold Label: "false" Predicted Label: "false" Support Justifications: "The claim is likely true.The provided evidence suggests that there is an urgent need to find new homes for 52 thoroughbred horses due to the death of their owner.This implies that the horses are indeed at risk of being sent to slaughter if adopters are not found soon.The fact that a circulating message is asking recipients to help find new homes for the horses further supports the urgency of the situation.Additionally, the specific number of horses mentioned (52) and the breed (thoroughbred) provide context and credibility to the claim.Overall, the evidence suggests that the claim is based on a genuine concern for the welfare of the horses and is not a fabricated or exaggerated story."Refute Justifications: "The claim that 52 thoroughbred horses are destined for slaughter if adopters are not found for them soon is challenged by several key factors.Firstly, the claim has been circulating for several years, with the original post dating back to 2011.According to credible reports on horse-related forums, all 52 horses were actually rehomed, with most going to family friends of the deceased owner.This suggests that the claim is not based on current events, but rather a recycled and outdated story.Furthermore, the claim has undergone subtle changes over the years, including the reason for the horses being in danger.Initially, the post stated that the horses would be sent to a glue factory, but more recently, the reason cited is the owner's passing due to COVID-19.However, there is no evidence to suggest that the horses are currently in danger or that they are being considered for slaughter.In fact, according to TheHorse.com,all 52 horses were able to find homes within a week of the initial post in 2011.This contradicts the claim that the horses are in imminent danger of being slaughtered.Therefore, based on the available evidence, it appears that the claim is not supported by facts and is likely a recycled and outdated story." 3</p>
<p>Claim: "The Irish Times 'newspaper' published a column by Fintan 'O' 'Toole expressing 'pity' for the United States over U.S. President Donald Trump' 's leadership during the COVID-19 pandemic " Gold Label: "true" Predicted Label: "true" Support Justifications: " The claim suggests that Fintan O' 'Toole wrote a column in The Irish Times expressing pity for the United States due to President Trump' 's handling of the pandemic.This can be verified through multiple sources.Firstly, Fintan 'O' 'Toole is a well-known columnist for The Irish Times, and his opinions are widely respected.Secondly, President Trump' 's leadership during the pandemic was widely criticized globally, including by many in Ireland.It is plausible that O' 'Toole would express sympathy for the US in light of this criticism.Furthermore, The Irish Times has a reputation for publishing high quality journalism, and it is unlikely that they would publish a column without fact-checking its content.Therefore, it is reasonable to conclude that the claim is accurate.The combination of O' 'Toole' 's credibility as a columnist, the global criticism of Trump' 's leadership, and The Irish Times' reputation for quality journalism all support the validity of the claim" Refute Justifications: " The claim that 'The Irish Times 'newspaper' published a column by Fintan O' 'Toole expressing "pity" for the United States over U.S. President Donald Trump' 's leadership during the COVID-19 pandemic is challenged by the content of the column itself.While O' 'Toole does express sympathy for the majority of Americans who did not vote for Trump, he does not express a sense of pity for the country as a whole.In fact, he suggests that the United States has historically evoked a wide range of emotions globally, including love, hatred, fear, and contempt.Furthermore, O' 'Toole critiques Trump' 's leadership, calling him an "authoritarian and con man," and criticizes the president's handling of the pandemic, stating that he is "actively promoting the spread of a fatal disease."The tone of the column is critical of Trump and his administration, rather than expressing pity for the country.Therefore, the claim appears to be inaccurate."System Prompt: "You are a fact-checking assistant.You are given a claim along with evidence sentences." "Your task is to determine the veracity of the claim based on the provided evidence sentences.\n\n"."Output your answer in exactly the following format:\n" "Claim Veracity: [label]\n \n" "Answer with one word: true, false, half-true, mostly-true, barely-true, or pants-fire.\n\n" "Label Definitions:\n" " true: The claim is completely supported by the evidence and is verified as accurate.\n"." false: The claim is clearly refuted by the evidence and is demonstrably incorrect.\n"." half-true: The claim contains both accurate and inaccurate details, making it partially true.\n"." mostly-true: The claim is mostly supported by the evidence, but contains some inaccuracies.\n"." barely-true: The claim contains some truth but is mostly misleading or inaccurate.\n"." pants-fire: The claim is completely false and outrageously ridiculous." User Prompt: f"Claim: claim\n".f"Evidence: evidence\n \n"."Based on the above evidence sentences, determine the veracity of the claim." "Answer using one word (true, false, half-true, mostly-true, barely-true, pants-fire) using the specified format." 2 IBE-1 (RAW-FC): Prompt used to predict the veracity of the claim using zero-shot prompting based on the given evidence sentences and the claim.</p>
<p>System Prompt: "You are a fact-checking assistant.You are given a claim along with evidences sentences.". "Your task is to determine the overall veracity of the claim based on the provided evidence sentences.\n\n"."Output your answer in exactly the following format:\n"."Claim Veracity: [label]\n \n"."Answer with one word: true, false, or half.\n\n"."Label Definitions for RAWFC:\n"."true: The claim is completely supported by the evidence and is verified as accurate.\n"."false: The claim is clearly refuted by the evidence and is demonstrably in incorrect.\n"."half: The claim contains both accurate and inaccurate details, making it partially true." User Prompt: "Claim: claim\n".f"Evidence: evidence\n \n"."Based on the above evidence, determine the veracity of the claim.". "Answer using one word (true, false, or half) using the specified format." 3 IBE-2 and TBE-2: First step, prompt used to generate an overall understanding of the claim based on the provided evidence sentences.</p>
<p>System Prompt: You are a helpful assistant.Your job is to read a claim and the raw evidence provided for it, and then explain your overall understanding of the claim based on that evidence.Focus on the key points and how the evidence helps in understanding the claim.Be objective, neutral, and avoid repetition.User Prompt: f"Here is a claim: \"claim\" \n \n" f"Here is the evidence: \"evidences\" \n \n"."Based on the provided claim and evidence, what is your overall understanding of the claim?""Provide a brief overall understanding (not more then 150 words) that captures the key reasoning between them." 4 IBE-2 (LIAR-RAW): Second step, prompt using zero-shot prompting to directly predict the claim's veracity based on the overall understanding.</p>
<p>System Prompt: "You are a fact-checking assistant.You are given a claim along with an 'Understanding' of the evidence sentences of the claim." "Do not consider any other evidence.Your task is to determine the veracity of the claim based on the understanding.\n\n""Your task is to determine the claim's veracity using one of the following six categories:\n\n"."1) pants-fire : The claim is ridiculously or outrageously untrue.\n"."2) false : The claim is entirely incorrect and unsupported by evidence.\n"."3) barely-true : The claim has a small element of truth but is mostly misleading.\n"."4) half-true : The claim mixes true and false information in significant amounts.\n"."5) mostly-true : The claim is generally correct but contains minor inaccuracies.\n"."6) true : The claim is completely accurate and fully supported by evidence.\n\n"."Output your answer in exactly the following format:\n"."Claim Veracity: [label]\n\n".</p>
<p>"where [label] must be one of (pants-fire, false, barely-true, half-true, mostly-true, or true)."User Prompt: f"Claim: claim\n" f"Understanding: understanding\n\n"."Based on the above understanding, determine the veracity of the claim.Answer using one word label from the following options:\n"."pants-fire\n" "false\n" "barely-true\n" "half-true\n" "mostly-true\n" "true\n\n" "Answer only with: Claim Veracity: [label]." 5 IBE-2 (RAW-FC): Second step, using zero-shot prompting to predict the claim's veracity from the overall understanding.</p>
<p>System Prompt: "You are a fact-checking assistant.You are given a claim along with an 'Understanding' of the evidence sentences." "Do not consider any other evidence.Your task is to determine the overall veracity of the claim based on the understanding.\n\n""Output your answer in exactly the following format:\n" "Claim Veracity: [label]\n\n" "Answer with one word: true, false, or half."User Prompt: f"Claim: claim\n" f"Understanding: understanding\n\n" "Label Definitions for RAWFC:\n" "true: The claim is completely supported by the evidence and is verified as accurate.\n""false: The claim is clearly refuted by the evidence and is demonstrably incorrect.\n""half: The claim contains both accurate and inaccurate details, making it partially true.\n\n" "Based solely on the above understanding, determine the overall veracity of the claim.""Answer using one word (true, false, or half) using the specified format." 6 IBE-3: First step, prompt used to generate an overall understanding of the claim based on the provided evidence sentences.</p>
<p>System Prompt: You are a helpful assistant.Your job is to read a claim and the raw evidence provided for it, and then explain your overall understanding of the claim based on that evidence.Focus on the key points and how the evidence helps in understanding the claim.Be objective, neutral, and avoid repetition.User Prompt: f"Here is a claim: \"claim\"\n\n" f"Here is the evidence: \"evidences\"\n\n" "Based on the provided claim and evidence, what is your overall understanding of the claim?""Provide a brief overall understanding (not more then 150 words) that captures the key reasoning between them." 7 IBE-3 (LIAR-RAW): Second step, prompt using Chain-of-Thought reasoning to predict the veracity of the claim from the overall understanding.</p>
<p>System Prompt: "You are a fact-checking assistant.You are given a claim along with an 'Understanding' of the evidence sentences for the claim." "Do not consider any other evidence.Your task is to determine the veracity of the claim based on the understanding.\n\n""First, explain your reasoning step-by-step using only the understanding provided.\n""Then, determine the claim's veracity using one of the following six categories:\n\n" "1) pants-fire : The claim is ridiculously or outrageously untrue.\n""2) false : The claim is entirely incorrect and unsupported by evidence.\n""3) barely-true : The claim has a small element of truth but is mostly misleading.\n""4) half-true : The claim mixes true and false information in significant amounts.\n""5) mostly-true : The claim is generally correct but contains minor inaccuracies.\n""6) true : The claim is completely accurate and fully supported by evidence.\n\n""Finally, output your answer in exactly the following format:\n" "Claim Veracity: [label]\n\n" "where [label] must be one of (pants-fire, false, barely-true, half-true, mostly-true, or true)." User Prompt: f"Claim: claim\n" f"Understanding: understanding\n\n" "First, explain your reasoning based only on the understanding provided above.\n""Then, determine the overall veracity of the claim using one of the following labels:\n" "-pants-fire\n" "-false\n" "-barely-true\n" "-half-true\n" "-mostly-true\n" "-true\n\n" "Finally, answer in the following format:\n" "Claim Veracity: [label]" 8 IBE-3 (RAW-FC): Second step, prompt using Chain-of-Thought reasoning to predict the veracity of the claim from the overall understanding.</p>
<p>System Prompt: "You are a fact-checking assistant.You are given a claim along with an 'Understanding' of the claim." "Do not consider any other evidence.Your task is to determine the overall veracity of the claim based on the understanding.\n\n""First, explain your reasoning in detail based only on the understanding.\n""Then output the claim veracity label in <strong>exactly</strong> the following format:\n" "Claim Veracity: [label]\n\n" "Answer with one word: true, false, or half."User Prompt: f"Claim: claim\n" f"Understanding:understanding\n\n" "Label Definitions:\n" "true: The claim is completely supported by the evidence and accurate.\n""false: The claim is clearly refuted by the evidence and incorrect.\n""half: The claim contains both accurate and inaccurate details, making it partially true.\n\n" "First, explain your reasoning step-by-step using only the understanding above.\n""Then, based on your reasoning, provide the final claim veracity in the format:\n" "Claim Veracity: [true/false/half] 9 IBE-4 and TBE-3: First step prompt used to classify each evidence sentence as supporting or refuting the claim.</p>
<p>System Prompt: You are an evidence classifier that determines whether a sentence supports or refutes a given claim.User Prompt:</p>
<p>Task: Analyze the sentence below and classify its relationship to the given claim.Classification Rules: 1. SUPPORT -The sentence provides evidence that supports the claim.System Prompt: "You are a fact-checking assistant.You are given a claim along with justifications: a 'Support Justification' and a 'Refute Justification'." "Do not consider any other evidence.Your task is to compare these justifications and determine the veracity of the claim based on justification.\n\n""Your task is to determine the claim's veracity using one of the following six categories:\n\n" "1) pants-fire : The claim is ridiculously or outrageously untrue.\n""2) false : The claim is entirely incorrect and unsupported by evidence.\n""3) barely-true : The claim has a small element of truth but is mostly misleading.\n""4) half-true : The claim mixes true and false information in significant amounts.\n""5) mostly-true : The claim is generally correct but contains minor inaccuracies.\n""6) true : The claim is completely accurate and fully supported by evidence.\n\n""Output your answer in exactly the following format:\n" "Claim Veracity: [label]\n\n" "where [label] must be one of (pants-fire, false, barely-true, half-true, mostly-true, or true)." User Prompt: f"Claim: claim\n" f"Support Justification 1: true_just\n" f"refute Justification 2: false_just\n\n" "Based on the justifications above, determine the best label from the following options:\n" "pants-fire\n" " false\n" " barely-true\n" " half-true\n" " mostly-true\n" " true\n\n" "Answer only with: Claim Veracity: [label]." 13 IBE-4 (RAW-FC): Third step prompt used to predict the veracity of the claim using the supporting and refuting justifications.</p>
<p>System Prompt: "You are a fact-checking assistant.You are given a claim along with two generated justifications: a 'Support Justification' and a 'Refute Justification'." "Do not consider any other evidence.Your task is to compare these justifications and determine the overall veracity of the claim based on justification.\n\n""Output your answer in exactly the following format:\n" "Claim Veracity: [label]\n\n" "Answer with one word: true, false, or half." User Prompt: f"Claim: claim\n" f"Support Justification: support_just\n" f"Refute Justification: refute_just\n\n" "Label Definitions for RAWFC:\n" "true: The claim is completely supported by the evidence and is verified as accurate.\n""false: The claim is clearly refuted by the evidence and is demonstrably incorrect.\n""half:</p>
<p>The claim contains both accurate and inaccurate details, making it partially true.\n\n" "Based on the above justifications, determine the overall veracity of the claim." "Answer using one word (true, false, or half) using the specified format." 14 Prompt sample for subjective evaluation of generated explanation.</p>
<p>System Prompt: "" Your task is to evaluate a model-generated explanation ("generated") against a reference explanation ("gold") across five specific criteria.For each criterion, assign an integer score from 1 to 5 (inclusive): Accuracy -Does it reflect the correct meaning based on the reference?3. Readability -Is it grammatically correct and easy to understand? 4. Objectivity -Is it neutral and free of emotional language?5. Logicality -Does the reasoning follow a coherent and sound process?<strong>Return JSON (strict format):</strong> "Informativeness": int, "Accuracy": int, "Readability": int, "Objectivity": int, "Logicality": int Expected explanation: \"\"\"expected\"\"\" Actual explanation: \"\"\"actual\"\"\" """ ID Content with Attention Scores 1 Claim: "Foreign aid is less than 1 percent of our federal budget." Gold Label: "true" Predicted Label: "true" Support Justifications: "The statement that foreign aid is less than 1 percent of our federal budget is supported by multiple sources.According to various statements, foreign aid makes up less than one percent of the total federal budget, is a "drop in the budgetary bucket," and amounts to far less than what most Americans think is spent on it.Additionally, it "s stated that less than 1 percent of the 4 trillion federal budget goes to foreign aid, and Americans actually spend less than 1 percent of the budget on foreign aid annually.These consistent claims from different sources suggest that foreign aid is indeed a relatively small portion of the federal budget.The evidence collectively supports this assertion." Refute Justifications: "The claim that foreign aid is less than 1 percent of our federal budget appears to be misleading.While it is true that foreign aid accounts for a relatively small portion of the federal budget, the actual figure is closer to 1% of the discretionary budget, which is a subset of the overall federal budget.Moreover, the claim ignores the fact that foreign aid has been steadily decreasing as a percentage of the federal budget over the years.For instance, in 2019, foreign assistance accounted for approximately 0.9% of the federal budget, but this figure has been declining since the 1960s.Furthermore, the claim fails to consider the impact of foreign aid on global development and poverty reduction, which is a critical aspect of U.S. foreign policy.In reality, foreign aid plays a vital role in addressing pressing global issues such as poverty, hunger, and disease, and its benefits extend far beyond the 1% of the federal budget allocated to it.Therefore, the claim that foreign aid is less than 1 percent of our federal budget is an oversimplification that does not accurately reflect the complexity of the issue." 2 Claim: "Among Hispanics, support for immigration reform is close to universal." Gold Label: "mostly-true" Predicted Label: "mostly-true" Support Justifications: "The statement "Among Hispanics, support for immigration reform is close to universal" is true.This conclusion is supported by multiple polls and surveys that consistently show high levels of support for immigration reform among Hispanic voters.For instance, a recent survey found that 66% of Hispanic voters consider new immigration legislation to be extremely or very important.Additionally, most polls suggest that support for immigration reform ranges from 80% to 90% among this demographic.Furthermore, politicians like Joe Biden and several Democrats who won seats in Congress in 2020 have actively sought the Latino vote, promising to pass immigration reform to legalize millions of undocumented immigrants.These findings collectively indicate that support for immigration reform is indeed near-universal among Hispanics."Refute Justifications: "The statement "Among Hispanics, support for immigration reform is close to universal" is challenged by several factors.Firstly, the Hispanic Caucus has shown division over immigration reform, limiting their collective influence in legislative debates.Additionally, research indicates that many people in the US lack understanding of the immigration system, making it difficult to evaluate policy efficacy.Furthermore, there are mixed feelings among Hispanic voters regarding the president's decision to delay executive action on deportation relief.Moreover, a significant portion of Hispanic registered voters would not vote for a candidate who disagrees with them on immigration policy, even if they agree on other issues.Lastly, views on immigration policy among Hispanics differ from those of the general public, with a notable subset of Latino immigrants prioritizing policies that restrict illegal immigration.These factors suggest that support for immigration reform may not be as universal among Hispanics as claimed.Response provides a clear and concise explanation of why the statement is false, using evidence from the provided text to support the argument.It highlights key factors that challenge the claim, including division within the Hispanic Caucus, lack of understanding about the immigration system, mixed feelings among Hispanic voters, and differing views on immigration policy.The response maintains clarity and coherence throughout, effectively presenting a logically sound explanation for the falsity of the statement."</p>
<p>Continued on next page</p>
<p>Figure 1 :
1
Figure 1: A schematic diagram explaining fact-checking based on entailment, which breaks down into three stages.(a) Entailment: Given a claim and its corresponding evidences, a GLM first distinguishes evidences into supporting/ refuting evidence via entailment.(b) Evidence consolidation: Then the same GLM consolidates these two groups into concise supporting and refuting justification.(c) Veracity prediction: Using the claim and both justifications, an ELM is trained to predict veracity.</p>
<p>Figure 2 :
2
Figure 2: Illustration of steps we followed in different experiments.Sub-figure (a) presents the case where only raw evidence sentences and claims are given as input (R1).This approach is used in TBE-1 and IBE-1.Sub-figure (b) shows the overall process of TBE-2, IBE-2 and IBE-3 based on R2.Sub-figure (c) illustrates the overall experimental process of TBE-3 and IBE-4 based on R3.</p>
<ol>
<li>
<p>1 . 1 TBE- 1 :
111
Training based on raw-evidences: Our first trainingbased experiment was based on R1</p>
</li>
<li>
<p>1 . 3 TBE- 3 :
133
Training based on entailment understanding: Our final training-based experiment was based on R3.</p>
</li>
</ol>
<p>1 = very poor, 2 = poor, 3 = average, 4 = good, 5 = excellent"".User Prompt: """<strong>Instructions:</strong> -Only return a <strong>strict JSON object</strong> in the exact format shown below.-Do NOT include any explanation, comments, or additional text.-Do NOT use floating point numbers or non-integer values.-If you are unsure, always pick the conservative score.<strong>Criteria:</strong> 1. Informativeness -Does it add meaningful context or background? 2.</p>
<p>Table 1 :
1
Distribution of samples in the LIAR-RAW and RAW-FC datasets including train, validation, and test splits.
DatasetClassesCountTrainValTestTrue (T)2,0211,647169205Mostly-true (MT)2,4391,950251238Half-true (HT)2,5941,950251238LAIR-RAWBarely-true (BT)2,0571,611236210[39]False (F)2,4661,958259249Pants-fire (PF)1,01381211586Total12,590True (T)6955616767RAW-FCHalf-true (HT)6715376767[39]False (F)6465146666Total2,012</p>
<p>Table 2 :
2
Language model details.Here, 'Max.length' denotes the maximum supported sequence length allowed by the particular model.training-based (TBEs) and four inference-based (IBEs) experiments.The schematic diagram of individual TBEs and IBEs are illustrated in Figure</p>
<p>Table 3 :
3
Performance of the considered baselines for LIAR-RAW and RAWFC datasets.Notation: our reproduced results are marked as ' †'.</p>
<p>Table 4 :
4
Hyperparameters explored during model training and evaluation.
5, 1e-5}OptimizerAdamW, AdamBatch size8, 16Patience (Early stop) 2, 3lora_rank8Learning rate{1e-5, 1e-4}lr_scheduler_typecosinebf16trueDataset Method Mistral Llama Gemma Qwen FalconLIAR-RAWLoRA0.270.300.230.29 0.26LoRA+0.250.290.220.29 0.29RAW-FCLoRA0.650.650.570.66 0.54LoRA+0.550.650.570.65 0.63</p>
<p>Table 5 :
5
Performance of models under TBE-1 for LIAR-RAW and RAW-FC datasets in terms of  1.Here, we have reported the average values of three random seeds.Green and Blue indicate best and second-best scores, respectively.</p>
<p>• In TBE-2, for LIAR-RAW, no model surpassed the best  1</p>
<p>reported by the baselines.However, for RAW-FC, we observed that many models, such as (i) XLNet fine-tuned on Llama understandings (∼ 1.64% ↑), and (ii) Llama trained with LoRA+ based on Llama understandings (∼ 16.39% ↑) outperformed the best reported baseline  1. • Many models in TBE-3, such as (i) RoBERTa fine-tuned with Mistral (∼ 16.39% ↑), Llama (∼ 23.81% ↑), Gemma (∼ 14.29% ↑), Qwen (∼ 9.52% ↑), and Falcon (∼ 4.76% ↑) based entailed justifications, (ii) XLNet fine-tuned with Mistral (∼ 16.39% ↑), Llama (∼ 28.57% ↑), Qwen (∼ 14.29% ↑), and Falcon (∼ 4.76% ↑) based entailed justifications, and (iii) Llama trained with Llama justifications and LoRA+ adapter (∼ 16.67% ↑) surpassed the best reported macro-F1 score of baselines ( 1 : 0.42).Similarly, for RAW-FC, we observed that (i) RoBERTa fine-tuned with Mistral (∼ 36.07%↑), Llama (∼ 44.26% ↑), Qwen (∼ 16.39% ↑), and Falcon (∼ 4.91% ↑) based entailed justifications, (ii) XLNet fine-tuned with Mistral (∼ 34.42% ↑), Llama (∼ 42.62% ↑), Qwen (∼ 14.75% ↑), and Falcon (∼ 21.31% ↑) based entailed justifications, and (iii) Llama trained (with Llama justifications) with LoRA+ adapter (∼ 36.07%↑) outperformed the best reported macro-F1 score by the baselines ( 1 : 0.61).Models from TBE-3, exhibit highest overall performance.</p>
<p>Table 6 :
6Dataset (− →)LIAR-RAWRAW-FCMethod (↓)TBE-2 TBE-3 TBE-2 TBE-3Fine-tuning-RoBERTa-L Mistral0.260.470.500.83-RoBERTa-L Llama0.250.520.490.88-RoBERTa-L Gemma0.270.480.500.49-RoBERTa-L Qwen0.280.460.480.71-RoBERTa-L Falcon0.270.440.480.64-XLNet-L Mistral0.280.470.610.82-XLNet-L Llama0.290.540.620.87-XLNet-L Gemma0.250.420.500.46-XLNet-L Qwen0.280.480.580.70-XLNet-L Falcon0.240.440.600.74LoRA-Mistral0.320.290.600.58-Llama0.250.300.460.61-Gemma0.180.200.400.30-Qwen0.210.320.530.46-Falcon0.230.210.530.41LoRA+-Mistral0.300.270.600.46-Llama0.230.490.710.82-Gemma0.200.290.510.46-Qwen0.240.290.430.47-Falcon0.280.360.530.50Dataset (− →)LIAR-RAWRAW-FCMethod (↓)IBE-1 IBE-2 IBE-3 IBE-4 IBE-1 IBE-2 IBE-3 IBE-4Prompting-Mistral0.220.200.210.140.530.580.450.43-Llama0.200.220.210.130.540.620.490.35-Gemma0.190.130.160.110.400.380.400.24-Qwen0.200.200.210.130.590.570.520.43-Falcon0.200.200.210.140.540.570.480.37
• Falcon explanations got highest  1 score for both LIAR-RAW (0.23) and RAW-FC (0.40).It means they show maximum unigram overlap with the gold explanations.Similarly, Mistral and Falcon explanations got the highest   score for LIAR-RAW (0.14).Mistral explanations got highest   for RAW-FC (0.18) as well.It indicates that these explanations show a maximum overlap of longest common subsequences with the gold explanations.Interestingly, we see a small deviation for  2 scores.While explanations generated by Mistral, Llama and Falcon scored high  2 for LIAR-RAW (0.06-0.07)Performance of claim veracity prediction using gold evidences in terms of macro F1 (MF) score.Here, we have reported the average values of three random seeds.Green and Blue indicate best and second-best performance, respectively.</p>
<p>Table 7 :
7LIAR-RAWRAW-FC𝑅 1𝑅 2𝑅 𝐿 𝐵𝐿𝐸𝑈 𝐵𝐸𝑅𝑇 𝑅 1𝑅 2𝑅 𝐿 𝐵𝐿𝐸𝑈 𝐵𝐸𝑅𝑇Mistral 0.17 0.06 0.14 0.030.030.39 0.12 0.180.040.02Llama0.19 0.07 0.120.030.040.39 0.11 0.170.040.04Gemma 0.20 0.04 0.110.020.08 0.19 0.20 0.110.060.24Qwen0.17 0.06 0.100.020.030.28 0.08 0.150.020.07Falcon0.23 0.06 0.14 0.030.05 0.40 0.12 0.170.050.02
Performance of Prompting methods across IBE-1 to IBE-4 settings in terms of macro F1 (MF) score.Green and Blue indicate best and second-best performance, respectively.</p>
<p>Table 8 :
8
Performance of explanation generation.Bold indicates best results and underline indicates second-best.
Dataset (− →)LIAR-RAWRAW-FCMethod (↓)MPMRMF1MPMRMF1TBE-30.550.540.540.880.880.88-w/o Supp. just.0.380.380.370.770.780.77-w/o Ref. just.0.490.500.490.800.800.80-w/o Both just.0.260.260.240.460.460.46
dataset, Gemma scored highest (0.20) for RAW-FC dataset.It means their explanations show maximum bigram overlap with the gold explanations.</p>
<p>Table 9 :
9
Ablation study showing classification performance on the LIAR-RAW (XLNet-L Llama ) and RAW-FC (RoBERTa-L Llama ) datasets."w/o Supp.just."indicates that only refuting justifications were passed to the model; "w/o Ref. just." passes only supporting justifications; and "w/o Both just."uses the claim alone without any justification.Here, we have reported the average values of three random seeds.Green and Blue indicate best and second-best performance, respectively.</p>
<p>• As evaluating GLMs, four out of five models found that Llama-generated explanations are better in four out of five subjective dimensions i.e. (informativeness: 4.73 -4.78, readability: 4.51 -4.91, objectivity: 4.17 -4.60 and Logicality: 4.26 -4.68) for the LIAR-RAW dataset.However, most of them found Falcon-generated explanations to be more accurate (3.55 -3.96).Similarly, three out of five evaluating GLMs found that Llama-generated explanations are better for all five dimensions (informativeness: 4.48 -4.92, accuracy: 4.10 -4.14, readability: 4.43 -4.82, objectivity: 4.28 -4.47 and Logicality: 4.33 -4.52).These observations correlate with the veracity prediction results, as RoBERTa and XLNet gave the highest  1 when trained with Llama-generated explanations.</p>
<p>Table 10 :
10
Scores of subjective evaluation by GLMs.Notations: Info.for Informativeness, Acc. for Accuracy, Read.for Readability, Obj. for Objectivity and Logi.for Logicality.
LIAR-RAWRAW-FCEvaluator GLMsGenerator LMsInfo.Acc.Read.Obj.Logi.Info.Acc.Read.Obj.Logi.Mistral4.303.474.074.184.143.603.113.913.913.81Llama4.783.934.794.554.624.754.124.764.474.44MistralGemma3.672.944.203.723.403.402.744.233.773.29Qwen3.122.874.043.753.572.932.853.883.323.34Falcon4.153.833.394.124.163.153.213.663.793.26Mistral4.133.444.054.144.133.573.503.964.064.03Llama4.523.714.514.174.264.484.144.434.284.33LlamaGemma3.512.944.063.793.513.352.854.123.813.55Qwen2.972.823.953.633.493.143.364.153.613.76Falcon3.883.803.484.003.993.193.363.613.853.47Mistral3.703.483.403.913.992.993.023.123.333.55Llama3.532.822.482.952.972.201.481.891.521.71GemmaGemma2.872.603.413.563.172.882.503.603.432.84Qwen2.492.523.313.223.032.132.042.882.592.49Falcon3.553.552.793.623.392.142.082.382.251.89Mistral4.313.434.104.124.214.243.644.113.944.15Llama4.733.824.774.454.544.924.104.824.324.52QwenGemma3.813.114.333.893.623.722.894.413.793.51Qwen3.182.824.093.653.563.303.304.073.573.76Falcon4.223.843.504.094.183.723.613.624.053.71Mistral4.363.494.034.134.073.053.083.613.733.60Llama4.733.924.914.604.683.742.653.603.003.02FalconGemma3.712.894.213.703.413.002.563.973.803.15Qwen3.132.904.103.883.692.051.913.452.572.59Falcon4.293.963.564.184.182.102.433.393.062.45Mistral4.163.463.934.104.113.493.273.743.793.83Llama4.463.644.294.144.224.023.303.903.523.60AverageGemma3.522.904.043.733.423.272.714.063.723.27Qwen2.972.783.903.633.472.712.693.693.133.19Falcon4.023.803.344.003.982.862.943.333.402.96No. Evid.MPMRMF1No. Evid.MPMRMF100.470.510.484-50.830.910.8410.530.490.496-100.870.850.862-5 6-200.58 0.620.59 0.600.58 0.6111-20 21-500.93 0.850.95 0.860.94 0.8521-500.540.500.48&gt;500.890.880.87&gt;500.450.480.43(a) LIAR-RAW(b) RAW-FC</p>
<p>Table 11 :
11
Claim veracity performance by evidence count (No.</p>
<p>Evid.) on LIAR-RAW (XLNet fine-tuned on Llama-based entailed justifications) and RAW-FC (RoBERTa fine-tuned on Llama-based entailed justifications).Metrics: MP, MR, MF1.</p>
<p>Table 12 :
12
Attention-Based Explanation Samples for Fact-Checking Predictions on RAWFC Datasets.
CriterionLIAR-RAWRAW-FCFKKAFKKAInformativeness0.250.480.210.20Accuracy0.300.330.130.15Readability0.460.540.070.07Objectivity0.120.220.100.19Logicality0.370.510.180.06</p>
<p>Table 13 :
13
Inter-annotator agreement (IAA) using Fleiss' Kappa (FK) and Krippendorff's Alpha (KA) for LIAR-RAW and RAW-FC.Low  (e.g., 0.073 for RAW-FC Readability) indicates agreement near chance level.
ID CaptionPrompt1IBE-1 (LIAR-RAW): Prompt used topredict the veracity of the claim us-ing zero-shot prompting based onthe given evidence sentences and theclaim.</p>
<p>You have been specially designed to perform abductive reasoning for the fake news detection task and justification generation.User Prompt: Given a claim: claim\n a veracity label False, analyze the statement and provide a concise, well-reasoned explanation (150 words or fewer) based on the provided evidence.Do not mention the label 'label' in your response.Focus on key factors that challenge the claim while maintaining clarity and coherence.Evidence: entry['refuting_evidence'] 12 IBE-4 (LIAR-RAW): Third step prompt used to predict the veracity of the claim using the supporting and refuting justifications.
ID CaptionPrompt11 IBE-4 and TBE-3, Second stepSystem Prompt:prompt used to generate a refutingjustification based on the refutingevidence sentences.2. REFUTE-The sentence provides evidence that contradicts or refutes the claim. Claim:claim Label for Claim: label Sentence to classify: sent Carefully analyze the sentence and respond with: 1. A classification (SUPPORTor REFUTE).10 IBE-4 and TBE-3, Second stepprompt used to generate a support-ing justification based on the sup-porting evidence sentences.
System Prompt: You have been specially designed to perform abductive reasoning for the fake news detection task and justification generation.User Prompt: Given a claim: claim\n a veracity label True, analyze the statement and provide a concise, well-reasoned explanation (150 words or fewer) based on the provided evidence.Do not mention the label 'label' in your response.Focus only on key supporting factors and logical reasoning.Evidence: entry['supporting_evidence'] Continued on next page</p>
<p>Table 14 :
14
Collection of prompt templates used in TBEs and IBEs experiments, covering various stages such as claim understanding, evidence classification, justification generation, and veracity prediction.The final prompt is used for subjective evaluation of generated explanations.</p>
<p>Limitations:Despite these promising results, our work has certain limitations.We limited our experiments to open-source language models for reproducibility and resource efficiency.Also, we restricted ourselves to two English-language datasets (LIAR-RAW and RAW-FC) due to our limited linguistic expertise.This approach can be extended to commercial language models and, multilingual setup for better generalization.One can modify our approach to check its performance on a variety of fact-checking datasets like FEVER[31]series, factcheck-bench[35], etc.In our work, we assumed a closed-domain fact-checking setup for reproducibility.In the future, one can consider open-domain fact-checking, i.e., retrieve evidence from an external source and test our hypothesis for generalization., ,ID Content with Attention Scores 3Claim: "Says Gov. Chris Christie cut spending (by) $1 billion and provided $850 million in new education funding."Gold Label: "half-true" Predicted Label: "half-true" Support Justifications: "The claim states that Gov. Chris Christie cut spending by $1 billion and provided $850 million in new education funding.To assess this claim, we need to consider the context of Gov. Christie's tenure as Governor of New Jersey.During his time in office, Gov. Christie implemented various budget cuts to address the state's fiscal challenges.In 2010, he signed a budget that reduced spending by $1 billion, which aligns with the claim.Additionally, Gov. Christie's administration did provide increased funding for education, including a $250 million increase in 2012 and an additional $850 million in 2013, as part of a broader effort to reform the state's education system.Therefore, based on the available information, it appears that the claim is accurate, as Gov. Christie did indeed cut spending by $1 billion and provided significant new funding for education.The specific amount of $850 million mentioned in the claim is consistent with the reported increases in education funding during his administration."Refute Justifications: "The claim that Gov. Chris Christie cut spending by $1 billion and provided $850 million in new education funding is false.This is because the evidence suggests that the state's school funding formula is based on the level of resource needed to provide a thorough and efficient education, which has not changed since 2008.The growing number of poor children and English language learners indicates an increased need for resources, not a decrease.Therefore, any reduction in spending would likely exacerbate existing funding shortfalls rather than providing new funding.Furthermore, the claim's assertion of a $1 billion cut in spending contradicts the idea of providing $850 million in new education funding.These inconsistencies undermine the validity of the claim." 4Claim: "Because of the federal health care law, 300,000 health plans canceled in Florida." Gold Label: "barely-true" Predicted Label: "barely-true" Support Justifications: "The claim states that due to the federal health care law, 300,000 health plans were canceled in Florida.To assess like the Veracity of this claim, let's consider the underlying facts.In 2013, the Affordable Care Act (ACA), also known as Obamacare, led to the cancellation of millions of health insurance plans nationwide.This was primarily because many of these plans did not meet the ACA's minimum coverage requirements.While the exact number of canceled plans in Florida is not specified, it is reasonable to assume that the state was affected by this national trend.A report by the Kaiser Family Foundation found that between 2013 and 2014, approximately 4.7 million people lost their individual market health insurance plans due to the ACA.Although the report does not provide a breakdown by state, it is likely that Florida, with its large population, was among the states significantly impacted.Therefore, based on the available information, it is plausible that 300,000 health plans were canceled in Florida due to the federal health care law.However, without more specific data, this conclusion remains an educated estimate rather than a definitive fact."Refute Justifications: "The claim that 300,000 health plans were canceled in Florida due to the federal health care law is false.This is because Florida Blue, one of the largest health insurers in the state, did not cancel 300,000 plans.Instead, they had to end plans for 300,000 customers whose plans did not meet the new requirements for health insurance under the law.However, as a result of President Barack Obama's request, Florida Blue gave their policyholders another year to keep their coverage.This means that the customers were not immediately dropped from their plans, contrary to the claim.Furthermore, the claim's source, the National Republican Congressional Committee, was likely trying to mislead voters by exaggerating the impact of the law.Therefore, the claim is false and lacks credibility."5Claim: "Wisconsin is on pace to double the number of layoffs this year." Gold Label: "false" Predicted Label: "false" Support Justifications: "To assess the veracity of the claim "Wisconsin is on pace to double the number of layoffs this year, " let's consider the following: 1. <strong>Economic trends</strong>: Wisconsin's economy has experienced fluctuations in recent years, with some sectors experiencing growth while others face decline.This variability could contribute to an increase in layoffs.Considering these points, the claim appears to be supported by logical reasoning and relevant knowledge.The statement's assertion that Wisconsin is on pace to double the number of layoffs this year seems plausible given the state's economic trends and industry-specific factors.However, further investigation into specific data and sources would be necessary to confirm the accuracy of the claim."Refute Justifications: "I'd be happy to help you analyze the claim!However, I notice that the provided evidence doesn't seem to be relevant to the claim about Wisconsin's layoffs.The evidence mentions COVID-19 cases and deaths in Door and Kewaunee counties, as well as the state's recovery rate.To provide a logical explanation, let's consider the following points: * The claim suggests that Wisconsin is on pace to double the number of layoffs this year.However, there is no evidence provided to support this assertion.<em> The evidence focuses on COVID-19 cases and deaths, which may have an impact on the economy, but it does not directly relate to layoffs.</em> Without any data or information about layoffs in Wisconsin, it's challenging to assess the validity of the claim.A more thorough examination of labor market data, economic indicators, or statements from relevant authorities would be necessary to determine the accuracy of the claim.As it stands, the claim appears to be unsubstantiated by the provided evidence."6Claim: " 'Florida' Democrats just voted to impose Sharia law on women."Gold Label: "pants-fire" Predicted Label: "pants-fire" Support Justifications: discuss, feel free to ask!I'm here to help" Refute Justifications: "The claim 'Florida Democrats just voted to impose Sharia law on women" is false.This conclusion can be drawn based on the evidence provided.Firstly, the Democrats voted against a bill that would have limited the use of foreign law in certain contexts, which is a far more limited measure than imposing Sharia law.Secondly, the bill in question only addressed contracts that select foreign law, and even then, a judge could still reject any foreign law that violates FL Const.such as child labor rules.Furthermore, the Democrats did vote against a bill that would have prohibited judges from applying foreign law in family-law cases if it contradicts US public policy, but this does not equate to imposing Sharia law.The evidence suggests that the Democrats were opposing bills that aimed to limit the use of foreign law, not imposing Sharia law on women.Therefore, the claim is unfounded and lacks merit."
. A I , Meta , 2024</p>
<p>Multimodal automated fact-checking: A survey. Mubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo, Oana Cocarascu, Elena Simperl, Andreas Vlachos, arXiv:2305.135072023. 2023arXiv preprint</p>
<p>Social media and fake news in the 2016 election. Hunt Allcott, Matthew Gentzkow, Journal of economic perspectives. 312017. 2017</p>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, arXiv:2311.16867[cs.CLThe Falcon Series of Open Language Models. 2023</p>
<p>Fake news detection using deep learning: A systematic literature review. Paula Mohammad Q Alnabhan, Branco, IEEE Access. 2024. 2024</p>
<p>Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) Shared Task. Rami Aly, Zhijiang Guo, Sejr Michael, James Schlichtkrull, Thorne, 10.18653/v1/2021.fever-1.1Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER), Rami Aly, Christos Christodoulopoulos, Oana Cocarascu, Zhijiang Guo. Arpit Mittal, Michael Schlichtkrull, James Thorne, Andreas Vlachos, the Fourth Workshop on Fact Extraction and VERification (FEVER), Rami Aly, Christos Christodoulopoulos, Oana Cocarascu, Zhijiang GuoDominican RepublicAssociation for Computational Linguistics</p>
<p>Factllama: Optimizing instructionfollowing language models with external knowledge for automated fact-checking. Tsun-Hin Cheung, Kin-Man Lam, 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE2023</p>
<p>FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. Eun Cheol, Choi , Emilio Ferrara, 10.1145/3589335.3651504Companion Proceedings of the ACM Web Conference 2024. Singapore, Singapore; New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Ask to Know More: Generating Counterfactual Explanations for Fake Claims. Shih-Chieh Dai, Yi-Li Hsu, Aiping Xiong, Lun-Wei Ku, 10.1145/3534678.3539205Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data MiningWashington DC, USA; New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Pizzagate: From rumor, to hashtag, to gunfire in DC. Marc Fisher, John Woodrow Cox, Peter Hermann, Washington Post. 62016. 2016</p>
<p>Measuring Nominal Scale Agreement Among Many Raters. L Joseph, Fleiss, Psychological Bulletin. 761971. 1971</p>
<p>A survey on automated fact-checking. Zhijiang Guo, Michael Schlichtkrull, Andreas Vlachos, Transactions of the Association for Computational Linguistics. 102022. 2022</p>
<p>X-Fact: A New Benchmark Dataset for Multilingual Fact Checking. Ashim Gupta, Vivek Srikumar, 10.18653/v1/2021.acl-short.86Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Short Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20212</p>
<p>LoRA+: Efficient Low Rank Adaptation of Large Models. Soufiane Hayou, Nikhil Ghosh, Bin Yu, International Conference on Machine Learning. PMLR2024</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, ICLR 20222022</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825[cs.CLMistral 7B. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Explainable Automated Fact-Checking for Public Health Claims. Neema Kotonya, Francesca Toni, 10.18653/v1/2020.emnlp-main.623Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingOnline2020</p>
<p>Klaus Krippendorff, Content Analysis: An Introduction to Its Methodology. Thousand Oaks, CASage Publications20133rd ed.</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019. 2019arXiv preprint</p>
<p>A genocide incited on Facebook, with posts from Myanmar's military. Paul Mozur, The New York Times. 152018. 2018. 2018</p>
<p>Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection. Kai Nakamura, Sharon Levy, William Yang, Wang ; Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Proceedings of the Twelfth Language Resources and Evaluation Conference. Asuncion Moreno, Jan Odijk, Stelios Piperidis, the Twelfth Language Resources and Evaluation ConferenceMarseille, France2020European Language Resources Association</p>
<p>MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset. Dan S Nielsen, Ryan Mcconville, 10.1145/3477495.3531744Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information RetrievalMadrid, Spain; New York, NY, USAAssociation for Computing Machinery2022SIGIR '22)</p>
<p>Bleu: a Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Eugene Charniak, Dekang Lin, the 40th Annual Meeting of the Association for Computational LinguisticsPierre Isabelle; Philadelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Reasoning with Language Model Prompting: A Survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence. Tal Schuster, Adam Fisch, Regina Barzilay, 10.18653/v1/2021.naacl-main.52Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, Yichao Zhou, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline2021</p>
<p>Surprising efficacy of fine-tuned transformers for fact-checking over larger language models. Vinay Setty, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>FakeCovid-A Multilingual Cross-domain Fact Check News Dataset for COVID-19. Kishore Gautam, Durgesh Shahi, Nandini, ICWSM 2020Workshop Proceedings of the 14th International AAAI Conference on Web and Social Media. 2020</p>
<p>. Atlanta Workshops, Georgia, Usa, 10.36190/2020.14Stevie Chancellor, Kiran Garimella, and Katrin WellerJune 8, 2020</p>
<p>Factify 2: A multimodal fake news and satire news dataset. S Suryavardan, Shreyash Mishra, Parth Patwa, Megha Chakraborty, Anku Rani, Aishwarya Naresh Reganti, Aman Chadha, Amitava Das, Amit P Sheth, Manoj Chinnakotla, Asif Ekbal, Srijan Kumar, Proceedings of De-Factify 2: 2nd Workshop on Multimodal Fact Checking and Hate Speech Detection, co-located with AAAI 2023. CEUR Workshop Proceedings. Amitava Das, Amit P Sheth, Asif Ekbal, De-Factify 2: 2nd Workshop on Multimodal Fact Checking and Hate Speech Detection, co-located with AAAI 2023Washington DC, USA2023. February 14, 20233555CEUR-WS.org</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024. 2024arXiv preprint</p>
<p>FEVER: a Large-scale Dataset for Fact Extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, 10.18653/v1/N18-1074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Marilyn Walker, Ji Heng, Amanda Stent, the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Explainable fake news detection with large language model via defense among competing wisdom. Bo Wang, Jing Ma, Hongzhan Lin, Zhiwei Yang, Ruichao Yang, Yuan Tian, Yi Chang, Proceedings of the ACM Web Conference 2024. the ACM Web Conference 20242024</p>
<p>End-to-End Explainable Fake News Detection Via Evidence-Claim Variational Causal Inference. Jinguang Wang, Shengsheng Qian, Jun Hu, Wenxiang Dong, Xudong Huang, Richang Hong, ACM Transactions on Information Systems. 432025. 2025</p>
<p>Xinyu Wang, Wenbo Zhang, Sarah Rajtmajer, arXiv:2410.18390Monolingual and Multilingual Misinformation Detection for Low-Resource Languages: A Comprehensive Survey. 2024. 2024arXiv preprint</p>
<p>Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers. Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, 10.18653/v1/2024.findings-emnlp.830Findings of the Association for Computational Linguistics: EMNLP 2024. Mohit Al-Onaizan, Yun-Nung Bansal, Chen, Miami, Florida, USAAssociation for Computational Linguistics2024Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein</p>
<p>DelphiAgent: A trustworthy multi-agent verification framework for automated fact verification. Cheng Xiong, Gengfeng Zheng, Xiao Ma, Chunlin Li, Jiangfeng Zeng, Information Processing &amp; Management. 621042412025. 2025</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, arXiv:2412.151152024arXiv preprintand Zihan Qiu. 2024. Qwen2.5 Technical Report</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, Advances in neural information processing systems. 322019. 2019</p>
<p>A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for Explainable Fake News Detection. Zhiwei Yang, Jing Ma, Hechang Chen, Hongzhan Lin, Ziyang Luo, Yi Chang, ; Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun, Pum-Mo Choi, Hsin-Hsi Ryu, Lucia Chen, Heng Donatelli, Sadao Ji, Patrizia Kurohashi, Nianwen Paggio, Seokhwan Xue, Younggyun Kim, Hahm, Proceedings of the 29th International Conference on Computational Linguistics. Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, Seung-Hoon Na, the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022International Committee on Computational Linguistics</p>
<p>End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models. Menglong Barry, Aditya Yao, Lichao Shah, Jin-Hee Sun, Lifu Cho, Huang, 10.1145/3539618.3591879Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information RetrievalTaipei, Taiwan; New York, NY, USAAssociation for Computing Machinery2023SIGIR '23)</p>
<p>Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments. Zhenrui Yue, Huimin Zeng, Lanyu Shang, Yifan Liu, Yang Zhang, Dong Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Automated factchecking: A survey. Xia Zeng, Amani S Abumansour, Arkaitz Zubiaga, Language and Linguistics Compass. 15e124382021. 2021</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019. 2019arXiv preprint</p>
<p>Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method. Xuan Zhang, Wei Gao, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapterthe Association for Computational Linguistics20231</p>
<p>From Predictions to Analyses: Rationale-Augmented Fake News Detection with Large Vision-Language Models. Xiaofan Zheng, Zinan Zeng, Heng Wang, Yuyang Bai, Yuhan Liu, Minnan Luo, 10.1145/3696410.3714532Proceedings of the ACM on Web Conference 2025. the ACM on Web Conference 2025Sydney NSW, Australia; New York, NY, USAAssociation for Computing Machinery2025</p>
<p>. Appendix, </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4843 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4843</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4843</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-ae22f7c57916562e2729a1a7f34298e4220b77a7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ae22f7c57916562e2729a1a7f34298e4220b77a7" target="_blank">Learning to Retrieve In-Context Examples for Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference of the European Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs and shows the generalization ability of the framework to unseen tasks during training.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4843.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4843.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Retriever (LLM-R)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative framework that trains a dense bi-encoder retriever (via knowledge distillation from a cross-encoder reward model) to retrieve high-quality k-shot in-context examples for a frozen LLM; it uses LLM log-likelihoods to rank candidates and a reward model to provide soft labels for distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-R (retriever + frozen LLM, e.g., LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A system combining a frozen autoregressive LLM (used for candidate ranking and final generation/evaluation) with a learned dense bi-encoder retriever. Training pipeline: (1) retrieve top-n candidates with BM25, (2) rank candidates by LLM conditional log-probabilities of ground-truth, (3) train a cross-encoder reward model on top/bottom ranked positives/negatives, (4) distill reward model into a bi-encoder dense retriever; iterate retrieval + training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (example pool of training examples)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory is implemented as an external pool (~6.3M training examples union across tasks). At test time the bi-encoder retrieves the top-k (8 in main experiments) examples from that pool; retrieved examples are concatenated to the input prompt as k-shot in-context examples. Training uses BM25 initial candidates, LLM ranking scores, a cross-encoder reward model, and distillation into a bi-encoder vector store.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot in-context learning across diverse NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>The agent solves many NLP tasks by conditioning a frozen LLM on k retrieved input-output exemplars (k-shot) drawn from the external pool; tasks include classification, multiple-choice, closed-book QA, reading comprehension, summarization, data-to-text, paraphrase, NLI, commonsense reasoning, coreference, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>A suite of 30 public datasets (examples: AGNews, ARC Easy/Challenge, BoolQ, CommonGen, COPA, DART, E2E NLG, Gigaword, HellaSwag, MNLI, MRPC, MultiRC, NQ, OpenBookQA, PAWS, PIQA, QNLI, QQP, RTE, Sentiment140, SNLI, SQuAD v1, SST2, Winogrande, WSC, WSC273, Yelp, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>For main setting with LLaMA-7B as the evaluation LLM and 8 retrieved examples: average metric for the 30-task suite: LLM-R (2 iterations) 66.5 (absolute points, Table 1). On held-out 4 tasks average = 81.7 (Table 3). Per-task examples in Table 10 (e.g., QNLI 69.6, PIQA 81.6). For other eval LLMs: with GPT-Neo-2.7B avg 61.8, LLaMA-13B avg 68.8, GPT-35-Turbo avg 75.1 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Zero-shot (no in-context examples) average = 44.9; random selection of k-shot examples average = 57.9; BM25 baseline average = 61.3; other dense retrievers (E5_base, SBERT, EPR) range ~61.4–63.5 (Table 1). Ablations: 'w/o reward model' (still uses retrieval but no distillation from reward model) average = 64.9; 'LLM score as reward' average = 65.2; 'reward model w/o label y+' average = 63.8 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Retrieval (any non-zero-shot strategy) improves performance substantially over zero-shot (44.9 -> 57.9 for random; BM25 61.3). The learned retriever LLM-R further improves over strong retrieval baselines (BM25, off-the-shelf dense retrievers) to 66.5 average. Iterative training gives additional gains (first->second iteration +0.8 avg), after which gains plateau. Using a reward model for distillation produces small but consistent gains: removing it drops average by ~0.8 points; using raw LLM log-likelihoods as distillation targets is worse than using the reward model (drop ~0.5 points vs full pipeline). Benefits are larger for smaller LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limitations reported: (1) the framework treats in-context examples independently and retrieves top-k without modeling interactions among them (may be suboptimal); (2) tasks that are knowledge-intensive (commonsense, complex reasoning, memorized factual knowledge) benefit less from retrieval — gains are smaller; (3) retrieval can reduce diversity (e.g., SQuAD where many examples share the same passage), hurting performance; (4) reward model requires access to ground-truth labels during training (not available at test time); (5) LLM log-likelihood scores have high variance and are suboptimal as direct distillation targets; (6) computational cost: ranking with LLaMA-7B is expensive (reported ~12 hours for 200k examples on 8 V100 GPUs); (7) iterative training shows diminishing returns after ~2 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Key conclusions: (1) Retrieval-augmented in-context exemplars substantially improve LLM few-shot performance compared to zero-shot and random exemplars; (2) a reward-model + distillation pipeline (cross-encoder teacher -> bi-encoder student) yields better retrievers than direct use of LLM log-probabilities; (3) BM25 is a strong and inexpensive initial retriever to warm-start training; (4) top retrieved examples tend to share input patterns or labels with test examples, which is a main source of gains; (5) improvements are consistent across LLM sizes but are larger for smaller LLMs; (6) increasing k (number of in-context examples) helps up to a point (diminishing returns after ~4), and larger retriever models yield better results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges_compact</strong></td>
                            <td>Inter-example interactions not modeled; weaker gains on knowledge-intensive tasks; possible retrieval-induced low diversity; training requires ground-truth labels and expensive LLM ranking; LLM log-scores noisy for distillation; diminishing returns with more iterations/k.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Retrieve In-Context Examples for Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4843.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4843.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Large Language Models (RAG-style systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of systems that augment LLM generation by retrieving relevant external information (documents, passages, exemplars) from an external store and conditioning the model on retrieved items to improve factuality, up-to-dateness, or task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-augmented LLM (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>General paradigm combining a retriever (sparse or dense) and a generative LLM; retrieved items are incorporated into the model input, attention, or decoding to influence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (documents / exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retrieve relevant documents/examples from an external store (BM25, dense vector store, etc.) and incorporate them into LLM input (concatenation, attention fusion, interpolation) to provide grounded context or demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-intensive generation, question answering, and in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to improve factual generation, provide up-to-date knowledge, or supply exemplars for few-shot prompting across tasks such as open-domain QA, summarization, and in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Paper cites retrieval-augmented LLMs in related work as motivation and situates LLM-R in this family, but provides no per-se performance numbers for other RAG systems; LLM-R is an example of retrieval-augmentation specifically targeted to retrieving in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Mentioned generally: choice of retrieval mechanism and how retrieved content is fused into the model affects performance; retrieval quality and integration method matters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>The paper reinforces that retrieval augmentation (here: retrieving in-context exemplars) is an effective way to improve LLM few-shot performance and that training retrievers specifically for the in-context selection problem (using LLM feedback and distillation) yields further gains over off-the-shelf retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Retrieve In-Context Examples for Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>Replug: Retrieval-augmented black-box language models <em>(Rating: 2)</em></li>
                <li>Learning to retrieve prompts for in-context learning <em>(Rating: 2)</em></li>
                <li>Unified demonstration retriever for in-context learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4843",
    "paper_id": "paper-ae22f7c57916562e2729a1a7f34298e4220b77a7",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "LLM-R",
            "name_full": "LLM Retriever (LLM-R)",
            "brief_description": "An iterative framework that trains a dense bi-encoder retriever (via knowledge distillation from a cross-encoder reward model) to retrieve high-quality k-shot in-context examples for a frozen LLM; it uses LLM log-likelihoods to rank candidates and a reward model to provide soft labels for distillation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM-R (retriever + frozen LLM, e.g., LLaMA-7B)",
            "agent_description": "A system combining a frozen autoregressive LLM (used for candidate ranking and final generation/evaluation) with a learned dense bi-encoder retriever. Training pipeline: (1) retrieve top-n candidates with BM25, (2) rank candidates by LLM conditional log-probabilities of ground-truth, (3) train a cross-encoder reward model on top/bottom ranked positives/negatives, (4) distill reward model into a bi-encoder dense retriever; iterate retrieval + training.",
            "memory_type": "retrieval-augmented external memory (example pool of training examples)",
            "memory_description": "Memory is implemented as an external pool (~6.3M training examples union across tasks). At test time the bi-encoder retrieves the top-k (8 in main experiments) examples from that pool; retrieved examples are concatenated to the input prompt as k-shot in-context examples. Training uses BM25 initial candidates, LLM ranking scores, a cross-encoder reward model, and distillation into a bi-encoder vector store.",
            "task_name": "Few-shot in-context learning across diverse NLP tasks",
            "task_description": "The agent solves many NLP tasks by conditioning a frozen LLM on k retrieved input-output exemplars (k-shot) drawn from the external pool; tasks include classification, multiple-choice, closed-book QA, reading comprehension, summarization, data-to-text, paraphrase, NLI, commonsense reasoning, coreference, etc.",
            "benchmark_name": "A suite of 30 public datasets (examples: AGNews, ARC Easy/Challenge, BoolQ, CommonGen, COPA, DART, E2E NLG, Gigaword, HellaSwag, MNLI, MRPC, MultiRC, NQ, OpenBookQA, PAWS, PIQA, QNLI, QQP, RTE, Sentiment140, SNLI, SQuAD v1, SST2, Winogrande, WSC, WSC273, Yelp, etc.)",
            "performance_with_memory": "For main setting with LLaMA-7B as the evaluation LLM and 8 retrieved examples: average metric for the 30-task suite: LLM-R (2 iterations) 66.5 (absolute points, Table 1). On held-out 4 tasks average = 81.7 (Table 3). Per-task examples in Table 10 (e.g., QNLI 69.6, PIQA 81.6). For other eval LLMs: with GPT-Neo-2.7B avg 61.8, LLaMA-13B avg 68.8, GPT-35-Turbo avg 75.1 (Table 4).",
            "performance_without_memory": "Zero-shot (no in-context examples) average = 44.9; random selection of k-shot examples average = 57.9; BM25 baseline average = 61.3; other dense retrievers (E5_base, SBERT, EPR) range ~61.4–63.5 (Table 1). Ablations: 'w/o reward model' (still uses retrieval but no distillation from reward model) average = 64.9; 'LLM score as reward' average = 65.2; 'reward model w/o label y+' average = 63.8 (Table 2).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Retrieval (any non-zero-shot strategy) improves performance substantially over zero-shot (44.9 -&gt; 57.9 for random; BM25 61.3). The learned retriever LLM-R further improves over strong retrieval baselines (BM25, off-the-shelf dense retrievers) to 66.5 average. Iterative training gives additional gains (first-&gt;second iteration +0.8 avg), after which gains plateau. Using a reward model for distillation produces small but consistent gains: removing it drops average by ~0.8 points; using raw LLM log-likelihoods as distillation targets is worse than using the reward model (drop ~0.5 points vs full pipeline). Benefits are larger for smaller LLMs.",
            "limitations_or_challenges": "Limitations reported: (1) the framework treats in-context examples independently and retrieves top-k without modeling interactions among them (may be suboptimal); (2) tasks that are knowledge-intensive (commonsense, complex reasoning, memorized factual knowledge) benefit less from retrieval — gains are smaller; (3) retrieval can reduce diversity (e.g., SQuAD where many examples share the same passage), hurting performance; (4) reward model requires access to ground-truth labels during training (not available at test time); (5) LLM log-likelihood scores have high variance and are suboptimal as direct distillation targets; (6) computational cost: ranking with LLaMA-7B is expensive (reported ~12 hours for 200k examples on 8 V100 GPUs); (7) iterative training shows diminishing returns after ~2 iterations.",
            "key_insights": "Key conclusions: (1) Retrieval-augmented in-context exemplars substantially improve LLM few-shot performance compared to zero-shot and random exemplars; (2) a reward-model + distillation pipeline (cross-encoder teacher -&gt; bi-encoder student) yields better retrievers than direct use of LLM log-probabilities; (3) BM25 is a strong and inexpensive initial retriever to warm-start training; (4) top retrieved examples tend to share input patterns or labels with test examples, which is a main source of gains; (5) improvements are consistent across LLM sizes but are larger for smaller LLMs; (6) increasing k (number of in-context examples) helps up to a point (diminishing returns after ~4), and larger retriever models yield better results.",
            "limitations_or_challenges_compact": "Inter-example interactions not modeled; weaker gains on knowledge-intensive tasks; possible retrieval-induced low diversity; training requires ground-truth labels and expensive LLM ranking; LLM log-scores noisy for distillation; diminishing returns with more iterations/k.",
            "uuid": "e4843.0",
            "source_info": {
                "paper_title": "Learning to Retrieve In-Context Examples for Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Retrieval-Augmented LLMs",
            "name_full": "Retrieval-Augmented Large Language Models (RAG-style systems)",
            "brief_description": "A class of systems that augment LLM generation by retrieving relevant external information (documents, passages, exemplars) from an external store and conditioning the model on retrieved items to improve factuality, up-to-dateness, or task performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Retrieval-augmented LLM (general concept)",
            "agent_description": "General paradigm combining a retriever (sparse or dense) and a generative LLM; retrieved items are incorporated into the model input, attention, or decoding to influence outputs.",
            "memory_type": "retrieval-augmented external memory (documents / exemplars)",
            "memory_description": "Retrieve relevant documents/examples from an external store (BM25, dense vector store, etc.) and incorporate them into LLM input (concatenation, attention fusion, interpolation) to provide grounded context or demonstrations.",
            "task_name": "Knowledge-intensive generation, question answering, and in-context learning",
            "task_description": "Used to improve factual generation, provide up-to-date knowledge, or supply exemplars for few-shot prompting across tasks such as open-domain QA, summarization, and in-context learning.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Paper cites retrieval-augmented LLMs in related work as motivation and situates LLM-R in this family, but provides no per-se performance numbers for other RAG systems; LLM-R is an example of retrieval-augmentation specifically targeted to retrieving in-context examples.",
            "limitations_or_challenges": "Mentioned generally: choice of retrieval mechanism and how retrieved content is fused into the model affects performance; retrieval quality and integration method matters.",
            "key_insights": "The paper reinforces that retrieval augmentation (here: retrieving in-context exemplars) is an effective way to improve LLM few-shot performance and that training retrievers specifically for the in-context selection problem (using LLM feedback and distillation) yields further gains over off-the-shelf retrieval.",
            "uuid": "e4843.1",
            "source_info": {
                "paper_title": "Learning to Retrieve In-Context Examples for Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2
        },
        {
            "paper_title": "Replug: Retrieval-augmented black-box language models",
            "rating": 2
        },
        {
            "paper_title": "Learning to retrieve prompts for in-context learning",
            "rating": 2
        },
        {
            "paper_title": "Unified demonstration retriever for in-context learning",
            "rating": 2
        }
    ],
    "cost": 0.014456499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning to Retrieve In-Context Examples for Large Language Models</h1>
<p>Liang Wang and Nan Yang and Furu Wei<br>Microsoft Research<br>{wangliang,nanya,fuwei}@microsoft.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes. The code and data are available at https://github.com/microsoft/LMOps/ tree/main/llm_retriever.</p>
<h2>1 Introduction</h2>
<p>In-context learning (ICL) (Brown et al., 2020) is an emerging learning paradigm that allows LLMs to perform tasks with few-shot examples, without requiring any updates to the model parameters. This approach stands in stark contrast to traditional machine learning, where models are typically trained on large datasets of labeled examples (Devlin et al., 2019). In-context learning offers a significant advantage in domains where labeled data is scarce or expensive to obtain, as it greatly reduces the amount of required labeled data.</p>
<p>There are several challenges associated with understanding and enhancing the effectiveness of incontext learning. One such challenge is that LLMs
can be highly sensitive to the quality of the incontext examples provided (Liu et al., 2022; Min et al., 2022). If the examples are not representative of the target task, then the model may not be able to learn effectively. Empirical studies (Liu et al., 2022; Luo et al., 2023) have demonstrated that using BM25 algorithm or off-the-shelf sentence embeddings (Reimers and Gurevych, 2019) to retrieve examples from the training set can substantially enhance the performance of in-context learning over random selection. Another approach involves training dense retrievers based on the feedback signals from LLMs, which has shown promising results in semantic parsing (Rubin et al., 2022), cross-task prompt retrieval (Cheng et al., 2023), and unified multi-task retrieval (Li et al., 2023). However, existing methods either focus on a relatively small language model (Rubin et al., 2022), or fail to exploit the fine-grained feedback information from LLMs in a principled manner (Li et al., 2023).</p>
<p>In this paper, we propose a novel framework, LLM-R (LLM Retriever), which aims to retrieve high-quality in-context examples for large language models. Given an initial set of retrieved candidates, our framework ranks them based on the conditional LLM log probabilities of the groundtruth outputs. Subsequently, a cross-encoder based reward model is trained to capture the fine-grained ranking signals from LLMs. Finally, a bi-encoder based dense retriever is trained using knowledge distillation. The reward model plays a crucial role in providing more informative soft-labels that are suitable for distillation, instead of using heuristically constructed one-hot labels. This pipeline can be iterated multiple times by retrieving a new set of candidates based on the latest dense retriever.</p>
<p>For evaluation purposes, we assemble a diverse set of 30 NLP tasks, which span 9 categories, including question answering, natural language inference, commonsense reasoning, and summarization, among others. Experimental results obtained using</p>
<p>LLaMA-7B (Touvron et al., 2023) demonstrate that our model improves the in-context learning performance by an average of $7.8 \%$ compared to random selection. Similar improvements are also observed on held-out tasks and LLMs of varying sizes. Further analysis reveals that the top-retrieved examples share similar input patterns or the same labels as the testing example. Our model is particularly effective for classification tasks with ample training examples. In contrast, tasks such as closed-book question answering and commonsense reasoning rely more on the inherent capabilities of LLMs and are less sensitive to the quality of in-context examples.</p>
<h2>2 Related Work</h2>
<p>In-Context Learning is an emergent property of large language models (LLMs) that enables them to perform various tasks conditioned on a few inputoutput examples, without any parameter updates or fine-tuning. This property has been demonstrated in LLMs such as GPT-3 (Brown et al., 2020), GPTNeo (Black et al., 2021), and LLaMA (Touvron et al., 2023), and attracts considerable attention from the research community. One area of research is focused on understanding the underlying mechanism and principles of in-context learning. For instance, Xie et al. view in-context learning as implicit Bayesian inference, while Dai et al. interpret it as meta optimization.</p>
<p>Another area of research is to explore different strategies for selecting and designing in-context examples for LLMs. Recent studies (Liu et al., 2022; Rubin et al., 2022; Li et al., 2023; Luo et al., 2023) have shown that using BM25 algorithm or fine-tuning dense retrievers based on LLM feedback to retrieve from the training set can improve the performance of in-context learning. Our work also falls into this area by proposing a novel training method. To model the interaction between in-context examples, determinantal point process (Ye et al., 2023) and sequential decision-making (Zhang et al., 2022) are introduced as preliminary explorations. In contrast, Structured Prompting (Hao et al., 2022) breaks the limitation of input context length and scales the number of in-context examples to thousands.</p>
<p>Dense Retrieval is a widely used information retrieval approach that utilizes dense vectors to perform semantic matching between queries and
documents in the latent space (Reimers and Gurevych, 2019; Wang et al., 2022). Compared to sparse retrieval methods such as BM25, dense retrieval exploits the powerful modeling capacity of pre-trained language models (PLMs) (Devlin et al., 2019) to learn relevance functions and has the potential to overcome the vocabulary mismatch problem. Various techniques such as hard negative mining (Karpukhin et al., 2020), knowledge distillation (Ren et al., 2021), and continual pre-training (Wang et al., 2022) have been proposed to enhance the performance of dense retrieval.</p>
<p>Retrieval Augmented LLMs combine the generative power of LLMs with the ability to retrieve relevant information from external sources (Ram et al., 2023; Lewis et al., 2020; Shi et al., 2023). This paradigm has the potential to enhance the factual consistency of generated texts, make LLMs aware of the up-to-date knowledge, as well as provide a natural way for source attribution (Nakano et al., 2021). The retrieved information can be incorporated into LLMs through various mechanisms, such as input concatenation (Shi et al., 2023), intermediate attention fusion (Borgeaud et al., 2022), and output interpolation (Khandelwal et al., 2020). For in-context learning, the goal of retrieval augmentation is to improve the performance of LLMs on downstream tasks by retrieving informative examples (Li et al., 2023; Luo et al., 2023).</p>
<h2>3 Preliminaries</h2>
<p>In this section, we provide a brief introduction to the problem setting of in-context example retrieval. Given a test example $x_{\text {test }}$ from a target task and $k$ in-context examples $\left{\left(x_{i}, y_{i}\right)\right}<em _test="{test" _text="\text">{i=1}^{k}$ from a pre-defined pool $\mathbb{P}$, a frozen language model $M$ is employed to predict an output $y</em>$ is the union of the training set for all the tasks in our evaluation.}}^{\prime}$ through autoregressive decoding. The primary objective of in-context example retrieval is to retrieve $k$ examples from $\mathbb{P}$ such that the predicted output $y_{\text {test }}^{\prime}$ is as close as possible to the ground-truth output $y_{\text {test }}$ based on some task-specific metrics. In this paper, the example pool $\mathbb{P</p>
<p>Straightforward solutions include utilizing the BM25 algorithm or readily available text embedding models (Wang et al., 2022; Liu et al., 2022) to retrieve examples from $\mathbb{P}$ by treating $x_{\text {test }}$ as a query. Despite their simplicity, these methods</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overall architecture of our proposed framework LLM-R. The training process comprises three stages: generating training data based on an initial retriever and LLM feedback, reward modeling, and training dense retrievers by distilling the knowledge from the reward model. At inference time, the trained dense retriever is employed to retrieve in-context examples from the pool $\mathbb{P}$ and the retrieved examples are fed to the LLM to generate the output.
have been shown to be more effective empirically when compared to the random selection baseline. In contrast, our framework aims to learn a dense retriever customized for in-context example retrieval by leveraging the feedback from LLMs.</p>
<h2>4 Methodology</h2>
<p>Our proposed framework is depicted in Figure 1. It includes four main components: training data generation, reward modeling, dense retriever training, and inference, which are described in detail in the following subsections.</p>
<h3>4.1 Training Data Generation</h3>
<p>Initial Candidates Retrieval Given an example $(x, y)$ from the training set, where $x$ is the input and $y$ is the groundtruth output, we retrieve the top- $n$ candidates $\left{\left(x_{i}, y_{i}\right)\right}_{i=1}^{n}$ from the example pool $\mathbb{P}$ using an initial retriever. The pool $\mathbb{P}$ contains the training examples from a mixture of tasks. Since $(x, y) \in \mathbb{P}$ holds during training, we exclude itself from the retrieval results.</p>
<p>In this paper, we employ the unsupervised BM25 algorithm as the initial retriever. The query only consists of the input $x$, while each retrieval candidate is the string concatenation of the input $x_{i}$ and the output $y_{i}$. This setting aligns with the test-time scenario, where the groundtruth output is unavailable. With a reasonably effective
initial retriever, the top- $n$ candidates would likely contain some positive examples and hard negative examples.</p>
<p>Ranking Candidates using LLMs To assess the quality of the retrieved candidates, we utilize feedback signals from a frozen LLM. Specifically, we rank the candidates in descending order based on the log-likelihood of the groundtruth output $y$, as given by the following equation:</p>
<p>$$
\log p\left(y \mid x, x_{i}, y_{i}\right), \forall i \in{1,2, \ldots, n}
$$</p>
<p>Here, $p\left(y \mid x, x_{i}, y_{i}\right)$ is the conditional probability of $y$ given the input $x$ and the $i$-th candidate $\left(x_{i}, y_{i}\right)$. It is noteworthy that computing $p\left(y \mid x, x_{i}, y_{i}\right)$ requires only one forward pass, and does not rely on any task-specific metrics, despite the autoregressive nature of language models. In practical applications, this helps reduce the inference cost of LLMs.</p>
<h3>4.2 Reward Modeling</h3>
<p>In order to capture the preferences of LLMs over the retrieved candidates and provide fine-grained supervision for dense retrievers, we propose to train a cross-encoder based reward model. For a training example $(x, y)$, we first sample one positive example $\left(x^{+}, y^{+}\right)$from the top-ranked candidates and $N_{\text {neg }}$ hard negative examples $\left{\left(x_{i}^{-}, y_{i}^{-}\right)\right}<em _neg="{neg" _text="\text">{i=1}^{N</em>$ from}}</p>
<p>the bottom-ranked candidates. The reward model takes as input the concatenation of $\left(x, y, x^{+}, y^{+}\right)$ and produces a real-valued score $s\left(x, y, x^{+}, y^{+}\right)$, similarly for the hard negatives. It is trained to minimize the following cross-entropy loss:</p>
<p>$$
\mathcal{L}<em i="1">{\text {reward }}=-\log \frac{e^{s\left(x, y, x^{+}, y^{+}\right)}}{e^{s\left(x, y, x^{+}, y^{+}\right)}+\sum</em>
$$}^{N_{\text {test }}} e^{s\left(x, y, x_{i}^{-}, y_{i}^{-}\right)}</p>
<p>It is important to note that the reward model is only used to provide supervision for the dense retriever and has access to the groundtruth label $y$, which is not available at test time. This is a key difference from the re-ranker in the ad-hoc retrieval setting (Ren et al., 2021). Compared to the bi-encoder based dense retrievers, the reward model enables full interaction between the inputs and can therefore serve as a teacher model. The log-likelihood scores from LLMs display high variance across different examples. In contrast, the reward model scores are more suitable for knowledge distillation.</p>
<h3>4.3 Training LLM Retrievers with Knowledge Distillation</h3>
<p>To facilitate efficient inference, the dense retriever is based on the bi-encoder architecture. Given a query $x$, we compute its low-dimensional embedding $\mathbf{h}<em _left_x__i="\left(x_{i">{x}$ by performing average pooling over the last-layer hidden states. Similarly, we obtain the embedding $\mathbf{h}</em>}, y_{i}\right)}$ for the candidate $\left(x_{i}, y_{i}\right)$ by taking the concatenation of $x_{i}$ and $y_{i}$ as input. The matching score $f\left(x, x_{i}, y_{i}\right)$ is computed as the temperature-scaled cosine similarity $\cos \left(\mathbf{h<em _left_x__i="\left(x_{i">{x}, \mathbf{h}</em>\right) / \tau$, where $\tau$ is a temperature hyperparameter. In this paper, we use a shared encoder for both the query and the retrieval candidates.}, y_{i}\right)</p>
<p>The dense retriever is trained to distill the knowledge from the reward model. We use the KL divergence loss $\mathcal{L}<em _reward="{reward" _text="\text">{\text {distill }}=\operatorname{KL}\left(p</em>}} | p_{\text {retriever }}\right)$ to measure the mismatch between the reward model distribution $p_{\text {reward }}$ and the retriever distribution $p_{\text {retriever } .} \mathcal{L<em _cont="{cont" _text="\text">{\text {distill }}$ is only computed over the hard negatives for efficiency reasons. To incorporate the in-batch negatives, we also include an InfoNCEbased contrastive loss $\mathcal{L}</em>$ is a weighted sum of the contrastive loss and the knowledge distillation loss:}}$ (Chen et al., 2020) by treating the candidate with the highest reward as the positive example. The final loss function $\mathcal{L}_{\text {retriever }</p>
<p>$$
\mathcal{L}<em _cont="{cont" _text="\text">{\text {retriever }}=\alpha \mathcal{L}</em>
$$}}+\mathcal{L}_{\text {distill }</p>
<p>Here, $\alpha$ is a constant that controls the relative importance of the two losses.</p>
<p>Iterative Training As illustrated in Figure 1, the retriever trained in iteration $i$ can be employed to retrieve candidates for the subsequent iteration $i+1$. In the first iteration, the candidates are retrieved using BM25. Such an iterative training approach (Xiong et al., 2021; Li et al., 2023) allows improving retriever quality by mining better positive and hard negative examples.</p>
<h3>4.4 Evaluation of LLM Retrievers</h3>
<p>Given a test example $x_{\text {test }}$, we compute its embedding $\mathbf{h}<em _test="{test" _text="\text">{\text {test }}$ using the trained retriever and retrieve the top $k$ candidates from the pool $\mathbb{P}$ as the $k$-shot in-context examples. The input to the LLM is the concatenation of the $k$-shot examples and $x</em>$. The overall procedure is illustrated in Figure 1.}</p>
<p>Depending on the task type of $x_{\text {test }}$, different decoding strategies are employed to generate the final prediction. For classification tasks, we use greedy search with constrained decoding to make sure the prediction is a valid class label. For multiple choice tasks, all the choices are ranked based on the average token-level log-likelihood score, and the one with the highest score is selected as the model's prediction. Generation tasks use greedy search without any constraints. For quantitative evaluation, the prediction is compared with the groundtruth $y_{\text {test }}$ using task-specific metrics.</p>
<h2>5 Experiments</h2>
<h3>5.1 Evaluation Setup</h3>
<p>We utilize a total of 30 publicly available datasets ${ }^{1}$ from 9 distinct categories for training and evaluation, as shown in Figure 2. This collection is based on FLAN (Wei et al., 2022) and UPRISE (Cheng et al., 2023). Different from our work, FLAN is focused on fine-tuning language models to follow instructions, while UPRISE is designed for cross-task retrieval. To test the generalization ability of the models to unseen tasks, we held out four datasets, namely QNLI, PIQA, WSC273, and Yelp, from the training process. The retrieval pool is created by taking the union of all the training examples, which results in a total of approximately 6.3 M examples. For each dataset, we sample a maximum of 30 k examples for training and 10 k examples for evaluation to reduce the cost of LLM</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The collection of datasets used in our experiments. The yellow-colored datasets are held out and excluded from training. For further information, please refer to Table 8 in the Appendix.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"># of datasets $\rightarrow$</th>
<th style="text-align: center;">CQA <br> 3</th>
<th style="text-align: center;">Comm. <br> 3</th>
<th style="text-align: center;">Coref. <br> 3</th>
<th style="text-align: center;">NLI <br> 5</th>
<th style="text-align: center;">Para. <br> 3</th>
<th style="text-align: center;">RC <br> 4</th>
<th style="text-align: center;">Sent. <br> 3</th>
<th style="text-align: center;">D2T <br> 3</th>
<th style="text-align: center;">Summ. <br> 3</th>
<th style="text-align: center;">Avg <br> 30</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">57.9</td>
</tr>
<tr>
<td style="text-align: center;">K-means</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">61.3</td>
</tr>
<tr>
<td style="text-align: center;">E5base</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">61.4</td>
</tr>
<tr>
<td style="text-align: center;">SBERT</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">62.1</td>
</tr>
<tr>
<td style="text-align: center;">EPR $^{1}$</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">63.5</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R (1 iter)</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">65.7</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R (2 iter)</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">66.5</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R (3 iter)</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: center;">Std dev.</td>
<td style="text-align: center;">$\pm 0.2$</td>
<td style="text-align: center;">$\pm 0.8$</td>
<td style="text-align: center;">$\pm 0.7$</td>
<td style="text-align: center;">$\pm 0.1$</td>
<td style="text-align: center;">$\pm 1.1$</td>
<td style="text-align: center;">$\pm 0.0$</td>
<td style="text-align: center;">$\pm 0.4$</td>
<td style="text-align: center;">$\pm 0.0$</td>
<td style="text-align: center;">$\pm 0.1$</td>
<td style="text-align: center;">$\pm 0.2$</td>
</tr>
</tbody>
</table>
<p>Table 1: Our main results. We report the average metrics for Close QA (CQA), Commonsense Reasoning (Comm.), Coreference (Coref.), NLI, Paraphrase (Para.), Reading Comprehension (RC), Sentiment (Sent.), Data-to-text (D2T), Summarize (Summ.). The standard deviation is computed over 3 runs with the "Random" baseline. Dense retriever baselines include E5 (Wang et al., 2022), SBERT (Reimers and Gurevych, 2019), and EPR (Rubin et al., 2022). ${ }^{1}$ : Our re-implementation for fair comparison.
inference. For evaluation, we report the average metrics in each task category. Please check Table 8 for the specific metrics used for each dataset.</p>
<p>In the main experiments, we use LLaMA-7B (Touvron et al., 2023) as the default LLM for candidate ranking and task evaluation unless otherwise specified. The reward model is initialized with ELECTRA ${ }<em _base="{base" _text="\text">{\text {base }}$ (Clark et al., 2020) and the retriever is initialized with E5 ${ }</em>$. Except for zero-shot evaluation, we retrieve 8 in-context examples for each test input. More implementation details and training hyperparameters are in Appendix A.}}$ (Wang et al., 2022). The baselines include zero-shot prompting, k-means clustering, random selection, BM25 (Lin et al., 2021), and two off-the-shelf dense retrievers, namely SBERT (all-mpnet-base-v2) (Reimers and Gurevych, 2019) and E5 ${ }_{\text {base }</p>
<h3>5.2 Main Results</h3>
<p>Table 1 presents the main results of our experiments. We observe that the simple BM25 algorithm serves as a strong baseline, exhibiting consistent improvements over the random selection strategy. This conclusion aligns with the findings of Luo et al.. Such effectiveness of BM25 can help warm up the first training iteration by providing a set of high-quality candidates. We also tried to use E5 ${ }_{\text {base }}$ as the initial retriever, but the benefits compared to BM25 are marginal. Therefore, we stick to BM25 for its simplicity.</p>
<p>After the first iteration, our proposed model LLM-R outperforms all the baselines ( $63.5 \rightarrow$ 65.7) by training on the BM25 retrieved candidates. The second iteration includes the mined positive and hard negative examples from "LLM-R (1 iter)",</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">CQA</th>
<th style="text-align: center;">Comm.</th>
<th style="text-align: center;">Coref.</th>
<th style="text-align: center;">NLI</th>
<th style="text-align: center;">Para.</th>
<th style="text-align: center;">RC</th>
<th style="text-align: center;">Sent.</th>
<th style="text-align: center;">D2T</th>
<th style="text-align: center;">Summ.</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLM-R (1 iter)</td>
<td style="text-align: center;">$\mathbf{4 8 . 8}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 1}$</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">$\mathbf{7 1 . 9}$</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">$\mathbf{9 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 1}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">model variants</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w/o reward model</td>
<td style="text-align: center;">$\mathbf{4 8 . 8}$</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">$64.9^{\downarrow 0.8}$</td>
</tr>
<tr>
<td style="text-align: left;">reward model w/o label $y^{+}$</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">$\mathbf{6 0 . 8}$</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">$63.8^{\downarrow 1.9}$</td>
</tr>
<tr>
<td style="text-align: left;">LLM score as reward</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">$\mathbf{7 4 . 0}$</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">$65.2^{\downarrow 0.5}$</td>
</tr>
<tr>
<td style="text-align: left;">retriever initialization</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">initialize w/ BERT ${ }_{\text {base }}$</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">$\mathbf{6 9 . 4}$</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">$65.2^{\downarrow 0.5}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Different training variants of LLM-R. "w/o reward model" is trained solely with contrastive loss on LLM ranked candidates. "LLM score as reward" uses the log-likelihood score from LLMs as the distillation target. Neither of these two variants utilizes the reward model. "reward model w/o label $y^{+}$" denotes that the reward model is trained without access to the groundtruth label $y^{+}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Zero-shot</th>
<th style="text-align: center;">Random</th>
<th style="text-align: center;">K-means</th>
<th style="text-align: center;">BM25</th>
<th style="text-align: center;">E5 $5_{\text {base }}$</th>
<th style="text-align: center;">SBERT</th>
<th style="text-align: center;">LLM-R</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">QNLI</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">$\mathbf{6 9 . 6}^{\uparrow 7.7}$</td>
</tr>
<tr>
<td style="text-align: left;">PIQA</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">$\mathbf{8 1 . 6}^{\uparrow 0.3}$</td>
</tr>
<tr>
<td style="text-align: left;">WSC273</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">$\mathbf{7 9 . 5}^{\uparrow 4.8}$</td>
</tr>
<tr>
<td style="text-align: left;">Yelp</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">$\mathbf{9 7 . 3}$</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">$95.9^{\downarrow 1.4}$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">$\mathbf{8 1 . 7}^{\uparrow 5.4}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Generalization to four held-out tasks.
raising the average score to $66.5(+0.8)$. Further iterations do not yield substantial improvements, indicating that the model has converged.</p>
<h2>6 Analysis</h2>
<p>In this section, we examine the performance of LLM-R across various tasks, LLMs, and model variants. Unless explicitly specified, "LLM-R" refers to the model with 2 training iterations.</p>
<h3>6.1 Training Pipeline of LLM-R</h3>
<p>We investigate several LLM-R variants LLM-R in Table 2 to understand the contribution of each component. The "w/o reward model" variant removes the knowledge distillation loss and sees 0.8 points drop in average score. This indicates that the reward model is crucial for the performance of LLM-R. Inspired by REPLUG (Shi et al., 2023), we experiment with a variant that uses the loglikelihood from LLMs as the reward for distillation. Although it outperforms the "w/o reward model" variant, it still lags behind our method by 0.5 points. The average token-level log-likelihood from LLMs is not a probability distribution by nature. We empirically observe that feedback scores for some training examples are concentrated in a very narrow range, while other scores are more dispersed. This makes it suboptimal to serve as target distribution within KL-divergence framework. Changing the retriever initialization from E5 (Wang et al.,
2022) to BERT (Devlin et al., 2019) results in a performance drop, but not as significant as in the ad-hoc retrieval setting.</p>
<h3>6.2 Generalization Ability of LLM-R</h3>
<p>We evaluate the generalization ability of LLM-R from two dimensions. In the first scenario, we test whether the trained retriever can retrieve good incontext examples for tasks that are not seen during training. In the second scenario, we test whether a model trained with one LLM can generalize to other LLMs that vary in size and quality.</p>
<p>In Table 3, we report the performance of LLM-R on four held-out tasks. The results demonstrate that LLM-R surpasses the second-best model E5 $5_{\text {base }}$ by an average of 5.4 points, indicating its ability to generalize to previously unseen tasks. Under the current evaluation protocol, there are training datasets that share the same task category as the held-out ones (e.g., QNLI and SNLI are both for natural language inference). A more challenging setting is to test on non-overlapping task categories, which we leave for future work.</p>
<p>The LLM-R model is trained with LLaMA-7B. To evaluate its generalization ability across different LLMs, we test on three other models, namely GPT-Neo-2.7B (Black et al., 2021), LLaMA-13B, and GPT-35-Turbo. Results in Table 4 show that LLM-R consistently outperforms the BM25 baseline for LLMs with parameter ranges from 2.7B</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">CQA</th>
<th style="text-align: left;">Comm.</th>
<th style="text-align: left;">Coref.</th>
<th style="text-align: left;">NLI</th>
<th style="text-align: left;">Para.</th>
<th style="text-align: left;">RC</th>
<th style="text-align: left;">Sent.</th>
<th style="text-align: left;">D2T</th>
<th style="text-align: left;">Summ.</th>
<th style="text-align: left;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">gpt-neo-2.7b</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BM25</td>
<td style="text-align: left;">41.1</td>
<td style="text-align: left;">67.0</td>
<td style="text-align: left;">53.2</td>
<td style="text-align: left;">47.6</td>
<td style="text-align: left;">64.5</td>
<td style="text-align: left;">51.2</td>
<td style="text-align: left;">78.3</td>
<td style="text-align: left;">45.4</td>
<td style="text-align: left;">47.3</td>
<td style="text-align: left;">54.4</td>
</tr>
<tr>
<td style="text-align: left;">LLM-R</td>
<td style="text-align: left;">42.2</td>
<td style="text-align: left;">68.0</td>
<td style="text-align: left;">59.7</td>
<td style="text-align: left;">71.5</td>
<td style="text-align: left;">73.0</td>
<td style="text-align: left;">51.6</td>
<td style="text-align: left;">91.6</td>
<td style="text-align: left;">46.9</td>
<td style="text-align: left;">48.8</td>
<td style="text-align: left;">$\mathbf{6 1 . 8}^{77.4}$</td>
</tr>
<tr>
<td style="text-align: left;">llama-13b</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BM25</td>
<td style="text-align: left;">49.6</td>
<td style="text-align: left;">80.1</td>
<td style="text-align: left;">61.1</td>
<td style="text-align: left;">67.0</td>
<td style="text-align: left;">69.9</td>
<td style="text-align: left;">60.5</td>
<td style="text-align: left;">92.5</td>
<td style="text-align: left;">49.9</td>
<td style="text-align: left;">50.9</td>
<td style="text-align: left;">64.6</td>
</tr>
<tr>
<td style="text-align: left;">LLM-R</td>
<td style="text-align: left;">52.0</td>
<td style="text-align: left;">83.7</td>
<td style="text-align: left;">71.2</td>
<td style="text-align: left;">76.8</td>
<td style="text-align: left;">73.3</td>
<td style="text-align: left;">62.2</td>
<td style="text-align: left;">94.2</td>
<td style="text-align: left;">50.7</td>
<td style="text-align: left;">52.0</td>
<td style="text-align: left;">$\mathbf{6 8 . 8}^{74.2}$</td>
</tr>
<tr>
<td style="text-align: left;">gpt-35-turbo ${ }^{\dagger}$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">BM25</td>
<td style="text-align: left;">75.3</td>
<td style="text-align: left;">85.2</td>
<td style="text-align: left;">65.0</td>
<td style="text-align: left;">78.1</td>
<td style="text-align: left;">78.0</td>
<td style="text-align: left;">84.4</td>
<td style="text-align: left;">95.7</td>
<td style="text-align: left;">51.9</td>
<td style="text-align: left;">52.8</td>
<td style="text-align: left;">74.7</td>
</tr>
<tr>
<td style="text-align: left;">LLM-R</td>
<td style="text-align: left;">79.3</td>
<td style="text-align: left;">86.7</td>
<td style="text-align: left;">63.8</td>
<td style="text-align: left;">79.6</td>
<td style="text-align: left;">76.0</td>
<td style="text-align: left;">84.0</td>
<td style="text-align: left;">95.4</td>
<td style="text-align: left;">52.2</td>
<td style="text-align: left;">53.0</td>
<td style="text-align: left;">$\mathbf{7 5 . 1}^{70.4}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Generalization to LLMs that are not used for training. $\dagger$ : Since the official API of gpt-35-turbo does not return the log-probabilities, we use different input-output templates to formulate all tasks as text generation. Consequently, the scores of gpt-35-turbo cannot be directly compared with those of other LLMs. More details are in Appendix B.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance gains of LLM-R over the random selection baseline. The selected knowledge-intensive tasks are NQ, ARC (easy and challenge), PIQA, HellaSwag, COPA, Paws, OpenBook QA, WSC273, WSC, Winogrande, and MultiRC.
to tens of billions. Notably, the gains are particularly significant for small-size language models, possibly because they are less powerful and thus require higher-quality examples to perform in-context learning.</p>
<h3>6.3 When does LLM-R Work and When Does it Not?</h3>
<p>Reporting a single aggregate score for all tasks facilitates comparison across different model variants. However, this approach hides the fact that LLM-R performs better on certain tasks than others, and may even lead to performance degradation in some cases. In Figure 3, we partition the tasks into two groups. A task is considered to be knowledge-intensive if solving this task requires commonsense, complex reasoning, or memorized factual knowledge.</p>
<p>For tasks in the knowledge-intensive set, the absolute improvements are substantially smaller than the average, with NQ being the only exception. This is not surprising, as these tasks rely more
heavily on the underlying foundation model's capability to perform reasoning and knowledge memorization. For the NQ dataset, we empirically find that there is some overlap between the training and test sets, where test questions are paraphrases of some training questions. Despite this, we decide to keep the NQ dataset in our evaluation, as it is a widely used benchmark and the remaining nonoverlapping questions are still valuable.</p>
<p>Another noticeable case is the SQuAD v1 dataset (Rajpurkar et al., 2016), where LLM-R performs worse than the random selection baseline. Upon manual inspection, we find that many questions in SQuAD share the same passage as the context. This frequently results in LLM-R retrieving examples with limited diversity, which may account for the observed decline in performance.</p>
<p>In Table 5, for the Sentiment140 and MNLI datasets, our model helps by retrieving examples that share similar input patterns with the test example. In contrast, the PIQA dataset requires commonsense knowledge and may not benefit much</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task name</th>
<th style="text-align: left;">Sentiment140</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Test Input</td>
<td style="text-align: left;">Math review. Im going to fail the exam. What is the sentiment of this tweet?</td>
</tr>
<tr>
<td style="text-align: left;">Test Answer</td>
<td style="text-align: left;">Negative</td>
</tr>
<tr>
<td style="text-align: left;">LLM-R</td>
<td style="text-align: left;">revising for maths exam on tuesday which im gonna fail badly What is the sentiment of this tweet? Negative</td>
</tr>
<tr>
<td style="text-align: left;">Task name</td>
<td style="text-align: left;">MNLI-m</td>
</tr>
<tr>
<td style="text-align: left;">Test Input</td>
<td style="text-align: left;">Premise: "Part 2), Confidentiality of Alcohol and Drug Abuse Patient Records." Hypothesis: "Drug and alcohol patient <br> records should be confidential" Does the premise entail the hypothesis? Yes, No, or Maybe?</td>
</tr>
<tr>
<td style="text-align: left;">Test Answer</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">LLM-R</td>
<td style="text-align: left;">Premise: "Eligible Clients unable to attain needed legal assistance" Hypothesis: "Clients that should have received legal <br> assistance but didn't" Does the premise entail the hypothesis? Yes, No, or Maybe? Yes</td>
</tr>
<tr>
<td style="text-align: left;">Task name</td>
<td style="text-align: left;">PIQA</td>
</tr>
<tr>
<td style="text-align: left;">Test Input</td>
<td style="text-align: left;">Here is a goal: "How can I keep a bathroom mirror from fogging up?" How would you accomplish this goal?</td>
</tr>
<tr>
<td style="text-align: left;">Test Answer</td>
<td style="text-align: left;">Wipe down with shaving cream.</td>
</tr>
<tr>
<td style="text-align: left;">LLM-R</td>
<td style="text-align: left;">Here is a goal: "how do you 'clean up' an eyebrow you've filled in?" How would you accomplish this goal? use concealer <br> to cover up any mistakes made.</td>
</tr>
</tbody>
</table>
<p>Table 5: Retrieved examples by LLM-R. The bold texts are the groundtruth answers for the test inputs and retrieved candidates. More examples are available in Table 11.
from the retrieved examples.</p>
<h3>6.4 Using Different LLMs for Data Generation and Task Evaluation</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Rank LLM $\rightarrow$ <br> Eval LLM $\downarrow$</th>
<th style="text-align: center;">GPT-Neo-2.7B</th>
<th style="text-align: center;">LLaMA-7B</th>
<th style="text-align: center;">Both</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-Neo-2.7B</td>
<td style="text-align: center;">$\mathbf{6 1 . 7}$</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">61.6</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-7B</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">$\mathbf{6 6 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 6: On the impacts of using different LLMs for candidate ranking and task evaluation. The "Both" setting merges the training data from two LLMs.</p>
<p>One crucial aspect of our framework is the selection of the LLM for training data generation and task evaluation. During the training phase, the LLM plays a pivotal role in ranking the retrieved candidates and providing supervision signals for the reward model. In the task evaluation phase, the LLM is used to generate the final predictions.</p>
<p>We experiment with GPT-Neo-2.7B and LLaMA-7B. Table 6 shows the results under different combinations of LLMs for training and evaluation. We observe that the quality of the evaluation LLM is the primary determinant for the final performance, while the choice of ranking LLM has a relatively minor impact. Although merging the training data from two LLMs yields the best overall performance, we do not employ this technique in our main experiments for the sake of simplicity.</p>
<h3>6.5 Scaling the Number of In-Context Examples and Retriever Size</h3>
<p>In Figure 4, we investigate the scaling effect of LLM-R from two aspects: the number of in-context
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The scaling effect with respect to the number of in-context examples and retriever size. Our main experiments use 8 in-context examples and base-size retriever. We vary the retriever model size by initializing with the released E5-{small, base, large} checkpoints from Wang et al..
examples and the retriever model size. The overall performance improves as we increase the number of retrieved examples, but the gains diminish after 4 examples. Including more examples usually leads to longer prompts and higher inference cost.</p>
<p>With regard to the retriever size, we observe that the small-size model produces comparable results with the base-size one, whereas the large-size retriever exhibits a more substantial performance boost. The trends are consistent for the two examined language models. Practitioners can select the</p>
<p>appropriate configurations based on the trade-off between performance and computational cost.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we introduce an iterative training framework named $L L M-R$ to retrieve high-quality in-context examples for large language models. This framework generates training data by utilizing a frozen LLM to rank the top retrieved candidates, and then learns a cross-encoder based reward model to capture the ranking preference. Biencoder based dense retrievers are trained to distill the knowledge from the reward model. We conduct a comprehensive evaluation of LLM-R on a diverse set of tasks and demonstrate that it consistently outperforms various strong baselines. Our model also generalizes well to held-out tasks and LLMs of varying sizes.</p>
<h2>Limitations</h2>
<p>In our framework, we treat each candidate example independently and retrieve the top- $k$ results for each test example. This may be suboptimal as the in-context examples can influence each other. Incorporating the techniques from the field of combinatorial optimization and sequential decision making can be a promising direction to explore.</p>
<p>Another limitation of our study is related to the automatic evaluation protocol. To compare the performance of different methods, we report the arithmetic mean of the metrics over all tasks. However, this may put generation tasks at a disadvantage since metrics like ROUGE and BLEU typically have a narrower range of variation compared to classification accuracy. Moreover, the simple arithmetic mean does not account for the quality of each dataset.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank anonymous reviewers for their valuable comments, and EACL 2024 and ACL Rolling Review organizers for their efforts.</p>
<h2>References</h2>
<p>Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing textual entailment challenge.</p>
<p>Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind</p>
<p>Tafjord, and Peter Clark. 2021. Think you have solved direct-answer question answering? try arcda, the direct-answer ai2 reasoning challenge. ArXiv preprint, abs/2102.03315.</p>
<p>Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 74327439. AAAI Press.</p>
<p>Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 2206-2240. PMLR.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on</p>
<p>Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1597-1607. PMLR.</p>
<p>Daixuan Cheng, Shaohan Huang, Junyu Bi, Yu-Wei Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, and Qi Zhang. 2023. Uprise: Universal prompt retrieval for improving zero-shot evaluation. ArXiv preprint, abs/2303.08518.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2022. Why can gpt learn incontext? language models secretly perform gradient descent as meta optimizers. ArXiv preprint, abs/2212.10559.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</p>
<p>Ondfej Dušek, David M. Howcroft, and Verena Rieser. 2019. Semantic noise matters for neural natural language generation. In Proceedings of the 12th International Conference on Natural Language Generation, pages 421-426, Tokyo, Japan. Association for Computational Linguistics.</p>
<p>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N project report, Stanford, 1(12):2009.</p>
<p>Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. 2022. Structured prompting: Scaling in-context learning to 1,000 examples. ArXiv preprint, abs/2212.06713.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252-262, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified demonstration retriever for incontext learning. ArXiv preprint, abs/2305.04320.</p>
<p>Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823-1840, Online. Association for Computational Linguistics.</p>
<p>Jimmy J. Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, Rodrigo Nogueira, and David R. Cheriton. 2021. Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.</p>
<p>Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Y Zhao. 2023. Dr. icl: Demonstrationretrieved in-context learning. ArXiv preprint, abs/2305.14128.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381-2391, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1104811064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. ArXiv preprint, abs/2112.09332.</p>
<p>Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2021. DART: Open-domain structured data record to text generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 432-447, Online. Association for Computational Linguistics.</p>
<p>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated Gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge</p>
<p>Extraction (AKBC-WEKEX), pages 95-100, Montréal, Canada. Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. ArXiv preprint, abs/2302.00083.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2825-2835, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655-2671, Seattle, United States. Association for Computational Linguistics.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 87328740. AAAI Press.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrievalaugmented black-box language models. ArXiv preprint, abs/2301.12652.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. ArXiv preprint, abs/2212.03533.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and</p>
<p>Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023. Compositional exemplars for in-context learning. ArXiv preprint, abs/2302.05698.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics.</p>
<p>Rui Zhang and Joel Tetreault. 2019. This email could save your life: Introducing the task of email subject line generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446-456, Florence, Italy. Association for Computational Linguistics.</p>
<p>Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 712, 2015, Montreal, Quebec, Canada, pages 649657.</p>
<p>Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Active example selection for in-context learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9134-9148, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1298-1308, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<h2>A Implementation Details</h2>
<p>The hyperparameters for the retriever model and reward model are summarized in Table 7. The E5 ${ }_{\text {base }}$ checkpoint is available at https://huggingface. co/intfloat/e5-base-v2. This checkpoint is also employed for the k-means clustering baseline, where we select 8 examples closest to each cluster center as the in-context examples. For each iteration, we employ LLaMA-7B to rank the top-100 retrieved candidates. As we retrieve from a unified pool of examples, it is possible that a candidate</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Retriever</th>
<th style="text-align: center;">Reward Model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">initialization</td>
<td style="text-align: center;">E5 $5_{\text {base }}$</td>
<td style="text-align: center;">ELECTRA $_{\text {base }}$</td>
</tr>
<tr>
<td style="text-align: left;">learning rate</td>
<td style="text-align: center;">$3 \times 10^{-5}$</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;"># of GPUs</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">train steps</td>
<td style="text-align: center;">6 k</td>
<td style="text-align: center;">3 k</td>
</tr>
<tr>
<td style="text-align: left;">$\tau$</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">n.a.</td>
</tr>
<tr>
<td style="text-align: left;">$\alpha$</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">n.a.</td>
</tr>
<tr>
<td style="text-align: left;">positive examples</td>
<td style="text-align: center;">top 3</td>
<td style="text-align: center;">bottom 16</td>
</tr>
<tr>
<td style="text-align: left;">negative examples</td>
<td style="text-align: center;">top 3</td>
<td style="text-align: center;">bottom 16</td>
</tr>
<tr>
<td style="text-align: left;"># of negatives</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;">ranking depth</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">input length</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">384</td>
</tr>
</tbody>
</table>
<p>Table 7: Hyperparameters for training the bi-encoder retriever and reward model. We use the same hyperparameters for every iteration.
comes from a different task than the query. In this case, we assign a low score to it.</p>
<p>During the evaluation, we retrieve top-8 candidates and use them as in-context examples. The maximum input length for LLaMA-7B is set to 1024. Longer inputs are truncated from the left side. The maximum output length is set to 64 . The most time-consuming part in our pipeline is ranking candidates with LLaMA-7B, which takes about 12 hours for 200 k examples with 8 V100 GPUs. Training the retriever model and reward model takes less than 10 hours in total.</p>
<h1>B Evaluation with GPT-35-Turbo</h1>
<p>Due to quota limits, we sample at most 1 k examples for each dataset. As GPT-35-Turbo does not return token-level log-probabilities, we cannot evaluate the multiple-choice datasets by computing the loglikelihood of each option. Instead, we append all the options to the end of the input, and let the model generate the option index. An example is shown in Table 9. We also tried using this format to LLaMA7B, but the performance is significantly worse than comparing the log-likelihood of each option.</p>
<p>For a small number of test examples, GPT-35Turbo fails to follow the patterns of in-context examples and generates outputs that are not valid class labels. We add some simple heuristics based on string matching to determine the model prediction.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset name</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;"># train</th>
<th style="text-align: center;"># test</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Held-out?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AESLC (Zhang and Tetreault, 2019)</td>
<td style="text-align: center;">Summarize</td>
<td style="text-align: center;">13,181</td>
<td style="text-align: center;">1,750</td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">AGNews (Zhang et al., 2015)</td>
<td style="text-align: center;">Summarize</td>
<td style="text-align: center;">120,000</td>
<td style="text-align: center;">7,600</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">ARC Challenge (Bhakthavatsalam et al., 2021)</td>
<td style="text-align: center;">Close QA</td>
<td style="text-align: center;">1,117</td>
<td style="text-align: center;">1,165</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">ARC Easy (Bhakthavatsalam et al., 2021)</td>
<td style="text-align: center;">Close QA</td>
<td style="text-align: center;">2,241</td>
<td style="text-align: center;">2,365</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">BoolQ (Clark et al., 2019)</td>
<td style="text-align: center;">Reading Comp.</td>
<td style="text-align: center;">9,427</td>
<td style="text-align: center;">3,270</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">CommonGen (Lin et al., 2020)</td>
<td style="text-align: center;">Data-to-text</td>
<td style="text-align: center;">67,389</td>
<td style="text-align: center;">4,018</td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">COPA (Roemmele et al., 2011)</td>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">DART (Nan et al., 2021)</td>
<td style="text-align: center;">Data-to-text</td>
<td style="text-align: center;">62,659</td>
<td style="text-align: center;">2,768</td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">E2E NLG (Dušek et al., 2019)</td>
<td style="text-align: center;">Data-to-text</td>
<td style="text-align: center;">33,525</td>
<td style="text-align: center;">1,847</td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Gigaword (Napoles et al., 2012)</td>
<td style="text-align: center;">Summarize</td>
<td style="text-align: center;">2,044,465</td>
<td style="text-align: center;">730</td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">HellaSwag (Zellers et al., 2019)</td>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">39,905</td>
<td style="text-align: center;">10,042</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">MNLI (m) (Williams et al., 2018)</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">392,702</td>
<td style="text-align: center;">9,815</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">MNLI (mm) (Williams et al., 2018)</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">392,702</td>
<td style="text-align: center;">9,832</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">MRPC (Dolan and Brockett, 2005)</td>
<td style="text-align: center;">Paraphrase</td>
<td style="text-align: center;">3,668</td>
<td style="text-align: center;">408</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">MultiRC (Khashabi et al., 2018)</td>
<td style="text-align: center;">Reading Comp.</td>
<td style="text-align: center;">27,243</td>
<td style="text-align: center;">4,848</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">NQ (Kwiatkowski et al., 2019)</td>
<td style="text-align: center;">Close QA</td>
<td style="text-align: center;">87,925</td>
<td style="text-align: center;">3,610</td>
<td style="text-align: center;">Exact Match</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">OpenBook QA (Mihaylov et al., 2018)</td>
<td style="text-align: center;">Reading Comp.</td>
<td style="text-align: center;">4,957</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">PAWS (Zhang et al., 2019)</td>
<td style="text-align: center;">Paraphrase</td>
<td style="text-align: center;">49,401</td>
<td style="text-align: center;">8,000</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">PIQA (Bisk et al., 2020)</td>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">16,113</td>
<td style="text-align: center;">1,838</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">QNLI (Rajpurkar et al., 2018)</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">104,743</td>
<td style="text-align: center;">5,463</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">QQP (Wang et al., 2019)</td>
<td style="text-align: center;">Paraphrase</td>
<td style="text-align: center;">363,846</td>
<td style="text-align: center;">40,430</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">RTE (Bentivogli et al.)</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">2,490</td>
<td style="text-align: center;">277</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Sentiment140 (Go et al., 2009)</td>
<td style="text-align: center;">Sentiment</td>
<td style="text-align: center;">1,600,000</td>
<td style="text-align: center;">359</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">SNLI (Bowman et al., 2015)</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">549,367</td>
<td style="text-align: center;">9,824</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD v1 (Rajpurkar et al., 2016)</td>
<td style="text-align: center;">Reading Comp.</td>
<td style="text-align: center;">87,599</td>
<td style="text-align: center;">10,570</td>
<td style="text-align: center;">Exact Match</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">SST2 (Socher et al., 2013)</td>
<td style="text-align: center;">Sentiment</td>
<td style="text-align: center;">67,349</td>
<td style="text-align: center;">872</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Winogrande (Sakaguchi et al., 2020)</td>
<td style="text-align: center;">Coreference</td>
<td style="text-align: center;">40,398</td>
<td style="text-align: center;">1,267</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">WSC (Levesque et al., 2012)</td>
<td style="text-align: center;">Coreference</td>
<td style="text-align: center;">554</td>
<td style="text-align: center;">104</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">WSC273 (Levesque et al., 2012)</td>
<td style="text-align: center;">Coreference</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">273</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Yelp (Zhang et al., 2015)</td>
<td style="text-align: center;">Sentiment</td>
<td style="text-align: center;">490,456</td>
<td style="text-align: center;">33,285</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">n.a.</td>
<td style="text-align: center;">6.3 M</td>
<td style="text-align: center;">177 k</td>
<td style="text-align: center;">n.a.</td>
<td style="text-align: center;">n.a.</td>
</tr>
<tr>
<td style="text-align: center;">Total (sampled)</td>
<td style="text-align: center;">n.a.</td>
<td style="text-align: center;">591 k</td>
<td style="text-align: center;">123 k</td>
<td style="text-align: center;">n.a.</td>
<td style="text-align: center;">n.a.</td>
</tr>
</tbody>
</table>
<p>Table 8: Statistics for the datasets used in this paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">What happens next in this paragraph? How to survive remedial classes Look at the course as an opportunity. Many students are discouraged when they are assigned to a remedial class. Some assume this placement means they aren't ready for college. OPTIONS: <br> A) However, people who are not unable to do what they're given on campus, or those who are cut out from college academies, are likely to have some little snitches. You want to be prepared for a negative outcome if possible. <br> B) In this case, you should consider what you will do if your subject consists of a certain term or number of subject areas. You could set up a study study program yourself or tutor a student who is struggling to thoroughly comprehend where they sat for homework. <br> C) If you take the course, you might find you feel highly motivated after passing the test. Try to develop a positive attitude towards the course so that you are not discouraged when you take your homework at the end of the day. <br> D) However, being assigned a remedial class doesn't mean that you are behind, just that you have an opportunity to receive better instruction and improve your skills in a subject that you have struggled with in the past. There is nothing unusual about being asked to attend a remedial course: two thirds of community college students take at least one remedial course.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 9: Input-output format for GPT-35-Turbo. This example is from the HellaSwag dataset. We add some line breaks for better readability.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Zero-shot</th>
<th style="text-align: center;">Random</th>
<th style="text-align: center;">Kmeans</th>
<th style="text-align: center;">BM25</th>
<th style="text-align: center;">$\mathrm{E} 5_{\text {base }}$</th>
<th style="text-align: center;">SBERT</th>
<th style="text-align: center;">EPR</th>
<th style="text-align: center;">LLM-R</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1 iter</td>
<td style="text-align: center;">2 iter</td>
<td style="text-align: center;">3 iter</td>
</tr>
<tr>
<td style="text-align: center;">AESLC</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">27.1</td>
</tr>
<tr>
<td style="text-align: center;">AGNews</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">93.5</td>
</tr>
<tr>
<td style="text-align: center;">ARC Chall.</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: center;">ARC Easy</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">63.6</td>
</tr>
<tr>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">74.1</td>
</tr>
<tr>
<td style="text-align: center;">CommonGen</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">37.3</td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">84.0</td>
</tr>
<tr>
<td style="text-align: center;">DART</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">57.3</td>
</tr>
<tr>
<td style="text-align: center;">E2E NLG</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">54.9</td>
</tr>
<tr>
<td style="text-align: center;">Gigaword</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">31.8</td>
</tr>
<tr>
<td style="text-align: center;">HellaSwag</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">75.4</td>
</tr>
<tr>
<td style="text-align: center;">MNLI (m)</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">69.8</td>
</tr>
<tr>
<td style="text-align: center;">MNLI (mm)</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">71.3</td>
</tr>
<tr>
<td style="text-align: center;">MRPC</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">78.2</td>
</tr>
<tr>
<td style="text-align: center;">MultiRC</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">52.1</td>
</tr>
<tr>
<td style="text-align: center;">NQ</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">39.2</td>
</tr>
<tr>
<td style="text-align: center;">OpenBook QA</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">53.4</td>
</tr>
<tr>
<td style="text-align: center;">PAWS</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: center;">PIQA</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">80.6</td>
</tr>
<tr>
<td style="text-align: center;">QNLI</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">69.4</td>
</tr>
<tr>
<td style="text-align: center;">QQP</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">83.3</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">70.4</td>
</tr>
<tr>
<td style="text-align: center;">Sentiment140</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">90.3</td>
</tr>
<tr>
<td style="text-align: center;">SNLI</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">82.2</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD v1</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">52.5</td>
</tr>
<tr>
<td style="text-align: center;">SST2</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">93.1</td>
</tr>
<tr>
<td style="text-align: center;">Winogrande</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">67.2</td>
</tr>
<tr>
<td style="text-align: center;">WSC</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: center;">WSC273</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">78.8</td>
</tr>
<tr>
<td style="text-align: center;">Yelp</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">95.5</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">66.4</td>
</tr>
</tbody>
</table>
<p>Table 10: Detailed results for each dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Name</th>
<th style="text-align: center;">AG News</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test Input</td>
<td style="text-align: center;">"Holiday Shoppers Off to a Fast Start Holiday shoppers spent 10 percent more Friday than they did a year ago, according to early reports, but Wal-Mart Stores Inc. dampened hopes for a strong start to the key retail season by " What is this text about? World, Sports, Business, or Technology?</td>
</tr>
<tr>
<td style="text-align: center;">Test Answer</td>
<td style="text-align: center;">Business</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R Top 1</td>
<td style="text-align: center;">"Disappointing holiday news hurts retail shares Shares in a range of area retailers dipped Monday on disappointing Thanksgiving sales data from Wal-Mart Stores Inc. In addition, ShopperTrak, which tallies sales results from 30,000 stores nationwide, said " What is this text about? World, Sports, Business, or Technology? Business</td>
</tr>
<tr>
<td style="text-align: center;">Task name</td>
<td style="text-align: center;">ARC Challenge</td>
</tr>
<tr>
<td style="text-align: center;">Test Input</td>
<td style="text-align: center;">In the 17th century, to estimate the distance to other planets, scientists first used the technique of viewing the planet from two different locations on Earth's surface. Which characteristic of the planet were the scientists using to calculate the distance from Earth?</td>
</tr>
<tr>
<td style="text-align: center;">Test Answer</td>
<td style="text-align: center;">location</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R Top 1</td>
<td style="text-align: center;">Which physical characteristic of Earth is similar to a physical characteristic of the Moon? its mountain ranges</td>
</tr>
<tr>
<td style="text-align: center;">Task name</td>
<td style="text-align: center;">ARC Easy</td>
</tr>
<tr>
<td style="text-align: center;">Test Input</td>
<td style="text-align: center;">What is the major cause of seasonal changes?</td>
</tr>
<tr>
<td style="text-align: center;">Test Answer</td>
<td style="text-align: center;">tilt of the Earth's axis</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R Top 1</td>
<td style="text-align: center;">Which occurs as a result of Earth's tilt on its rotating axis? seasonal changes in the climate</td>
</tr>
<tr>
<td style="text-align: center;">Task name</td>
<td style="text-align: center;">CommonGen</td>
</tr>
<tr>
<td style="text-align: center;">Test Input</td>
<td style="text-align: center;">Concepts: field, throw, kid, bunch, ball. Write a sentence that includes all these words.</td>
</tr>
<tr>
<td style="text-align: center;">Test Answer</td>
<td style="text-align: center;">A bunch of kids are running around and throwing a ball on a field.</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R Top 1</td>
<td style="text-align: center;">Concepts: look, ball, lot. Write a sentence that includes all these words. Two babies look up while they are playing in a playpen with a lot of balls.</td>
</tr>
<tr>
<td style="text-align: center;">Task name</td>
<td style="text-align: center;">COPA</td>
</tr>
<tr>
<td style="text-align: center;">Test Input</td>
<td style="text-align: center;">"The boy skipped dinner." What is the cause?</td>
</tr>
<tr>
<td style="text-align: center;">Test Answer</td>
<td style="text-align: center;">He ate a big lunch.</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R Top 1</td>
<td style="text-align: center;">"The parents left their children with a babysitter." What is the cause? They made plans to celebrate their anniversary.</td>
</tr>
<tr>
<td style="text-align: center;">Task name</td>
<td style="text-align: center;">DART</td>
</tr>
<tr>
<td style="text-align: center;">Test Input</td>
<td style="text-align: center;">Triple: The Mill, eatType, coffee shop; The Mill, food, Chinese; The Mill, priceRange, moderate; The Mill, area, city centre; The Mill, near, The Sorrento What is a sentence that describes this triple?</td>
</tr>
<tr>
<td style="text-align: center;">Test Answer</td>
<td style="text-align: center;">There is a coffee shop serving Chinese food called The Mill. It has a moderate price range is is find in the city centre near The Sorrento.</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R Top 1</td>
<td style="text-align: center;">Triple: The Mill, eatType, coffee shop; The Mill, food, Indian; The Mill, priceRange, cheap; The Mill, area, riverside; The Mill, near, The Sorrento What is a sentence that describes this triple? The Mill coffee shop is located in the riverside area near The Sorrento. They serve Indian food at a cheap price.</td>
</tr>
<tr>
<td style="text-align: center;">Task name</td>
<td style="text-align: center;">Gigaword</td>
</tr>
<tr>
<td style="text-align: center;">Test Input</td>
<td style="text-align: center;">Write a short summary for this text: the dollar and major european currencies traded within narrow ranges on tuesday on the london forex market, which was waiting for the easter holiday weekend and for us employment figures to be announced on friday, traders said in late afternoon .</td>
</tr>
<tr>
<td style="text-align: center;">Test Answer</td>
<td style="text-align: center;">london forex market stable as market waits for easter us data <br> Write a short summary for this text: the dollar was stable over-all early monday afternoon by comparison with morning levels on the london forex market, which was waiting for publication at the end of the week of us inflation figures, traders said . dollar stable in london as market waits for us inflation data</td>
</tr>
<tr>
<td style="text-align: center;">Task name</td>
<td style="text-align: center;">MRPC</td>
</tr>
<tr>
<td style="text-align: center;">Test Input</td>
<td style="text-align: center;">Here are two sentences: An episode is declared when the ozone reaches .20 parts per million parts of air for one hour. A Stage 1 episode is declared when ozone levels reach 0.20 parts per million. Do they have the same meaning?</td>
</tr>
<tr>
<td style="text-align: center;">Test Answer</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R Top 1</td>
<td style="text-align: center;">Here are two sentences: A Stage One alert is declared when ozone readings exceed 0.20 parts per million during a one-hour period. A Stage 1 episode is declared when ozone levels reach 0.20 parts per million . Do they have the same meaning? Yes</td>
</tr>
<tr>
<td style="text-align: center;">Task name</td>
<td style="text-align: center;">NQ</td>
</tr>
<tr>
<td style="text-align: center;">Test Input</td>
<td style="text-align: center;">Question: legislation regarding data protection and security in uk? Answer:</td>
</tr>
<tr>
<td style="text-align: center;">Test Answer</td>
<td style="text-align: center;">The Data Protection Act 1998</td>
</tr>
<tr>
<td style="text-align: center;">LLM-R Top 1</td>
<td style="text-align: center;">Question: which law relates to the protection of personal information? Answer: Data Protection Act 1998</td>
</tr>
</tbody>
</table>
<p>Table 11: More retrieved examples. The format is the same as Table 5.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ We use "datasets" and "tasks" interchangeably.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1288 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1288</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1288</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-e43618a367ee44666c3241d6ab8c0eb70e745997</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e43618a367ee44666c3241d6ab8c0eb70e745997" target="_blank">Sequential Bayesian optimal experimental design via approximate dynamic programming</a></p>
                <p><strong>Paper TL;DR:</strong> This paper rigorously formulate the general sequential optimal experimental design (sOED) problem as a dynamic program, adopting a Bayesian formulation with an information theoretic design objective, and develops new numerical approaches for nonlinear design with continuous parameter, design, and observation spaces.</p>
                <p><strong>Paper Abstract:</strong> The design of multiple experiments is commonly undertaken via suboptimal strategies, such as batch (open-loop) design that omits feedback or greedy (myopic) design that does not account for future effects. This paper introduces new strategies for the optimal design of sequential experiments. First, we rigorously formulate the general sequential optimal experimental design (sOED) problem as a dynamic program. Batch and greedy designs are shown to result from special cases of this formulation. We then focus on sOED for parameter inference, adopting a Bayesian formulation with an information theoretic design objective. To make the problem tractable, we develop new numerical approaches for nonlinear design with continuous parameter, design, and observation spaces. We approximate the optimal policy by using backward induction with regression to construct and refine value function approximations in the dynamic program. The proposed algorithm iteratively generates trajectories via exploration and exploitation to improve approximation accuracy in frequently visited regions of the state space. Numerical results are verified against analytical solutions in a linear-Gaussian setting. Advantages over batch and greedy design are then demonstrated on a nonlinear source inversion problem where we seek an optimal policy for sequential sensing.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1288.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1288.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>sOED-ADP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Optimal Experimental Design via Approximate Dynamic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential (closed-loop) Bayesian experimental design algorithm that casts multi-experiment design as a dynamic program and approximates the optimal policy with approximate dynamic programming (ADP), using one-step lookahead policies parameterized by linear value-function approximations and iterative exploration/exploitation policy updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>sOED-ADP (one-step lookahead policy with value-function regression)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Implements a DP formulation of sequential Bayesian OED. Key components: belief-state (posterior) representation (adaptive grid in nonlinear cases or analytic parametric form in conjugate cases); one-step lookahead policy mu_k(x_k)=argmax_d E[g_k + ~J_{k+1}(F_k(...))]; value functions approximated with a linear architecture r_k^T phi_k(x_k) (features are polynomials of posterior mean and log-variance and of physical state); backward induction with regression to fit ~J_k; iterative policy updates mixing exploration (random design sampling) and exploitation (current policy) to generate regression points; Monte Carlo inner expectation estimation and stochastic approximation for noisy optimization over continuous design spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information-gain maximization (expected KL divergence / mutual information) implemented within a sequential Bayesian optimal experimental design framework solved approximately via ADP (one-step lookahead with value-function regression).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts designs sequentially using the current belief (posterior) about model parameters and physical state. At each stage the agent selects the next experimental design by maximizing the expected immediate reward plus the approximated value-to-go (~J_{k+1}) under the predicted posterior update. Policies are improved offline by iterating: generate trajectories via exploration (random designs) and exploitation (current policy), compute training targets via Monte Carlo expectation and inner optimization, regress to update value-function weights, then repeat (policy update iterations). Exploration/exploitation mixture (e.g., 30% exploration / 70% exploitation in examples) controls coverage vs. refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear-Gaussian parameter inference and Contaminant source inversion (nonlinear advection-diffusion, 1D)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (true parameter θ hidden; agent maintains belief/posterior), stochastic observation noise, continuous parameter and observation spaces, continuous multi-dimensional design spaces; in contaminant case includes an additional physical state (vehicle position) and nonlinear forward model producing non-Gaussian posteriors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Linear-Gaussian: very low complexity (scalar parameter θ, horizon N=2, continuous design d in [0.1,3], belief state reducible to mean and variance). Contaminant inversion: higher complexity — scalar spatial source in 1D, physical state is vehicle position z, design is displacement d_k in [-3,3], horizon N variable (multi-stage), belief represented via adaptive grid (100 nodes in example), observation noise possibly state-dependent; continuous action and state spaces with movement constraints and movement costs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Linear-Gaussian: numerical sOED policies match analytical optimum closely. Exact optimal expected reward U(π*) ≈ 0.7833; numerical sOED (analytical belief representation) achieved mean rewards 0.77 (ℓ=1), 0.78 (ℓ=2), 0.78 (ℓ=3) over 1000 simulated trajectories (MC SE ±0.02). Grid belief representation achieved 0.74, 0.76, 0.75 (ℓ=1..3). Exploration-only baseline: U ≈ -8.5. Contaminant source inversion: paper reports that sOED outperforms batch (open-loop) and greedy (myopic) designs in the nonlinear experiments (qualitative advantage demonstrated), but specific numeric metrics for those cases are not provided in the supplied text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Linear-Gaussian: batch and sOED coincide for this deterministic variance problem (batch not worse); exploration-only baseline much worse (U≈-8.5). Contaminant case: greedy and batch are used as baselines and are reported to be outperformed by sOED in the nonlinear experiments (no numeric values in provided excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirical settings: evaluation of policies used 1000 simulated trajectories to estimate expected reward; regression used 1000 training points (analytical belief) or 500 (grid belief) per policy-update iteration in the linear-Gaussian example; L=3 policy updates were used. No formal sample-complexity bound is provided; sample requirements depend on regression points, Monte Carlo samples for inner expectation, and number of exploration/exploitation trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced by explicit mixture during policy-update iterations: initial iterations use pure exploration to generate regression points; subsequent iterations sample both exploration (random designs from a design measure, e.g., a Gaussian) and exploitation (trajectories produced by current policy) — example mixture 30% exploration / 70% exploitation. This balances coverage of the design space (to avoid missing promising regions) with refinement around high-probability states under the current policy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Benchmarked against: analytical optimal solution (when available), batch (open-loop) design, greedy (myopic) sequential design, and an exploration-only baseline. Also compared two belief representations: analytic parametric (conjugate Gaussian) and adaptive-grid discretization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Rigorous DP formulation of sOED with information-theoretic objective (expected KL divergence). 2) Practical ADP solution: one-step lookahead policy parameterized via linear value-function approximator, with backward induction/regression and iterative policy updates using exploration/exploitation trajectories. 3) In linear-Gaussian test, ADP-based sOED recovers nearly exact optimal designs and expected reward; grid belief representation agrees well with analytic belief representation. 4) In a nonlinear contaminant source inversion problem, sOED (ADP) yields demonstrable advantages over batch and greedy designs, showing the value of coordinating experiments with feedback under nonlinear/non-Gaussian uncertainty. 5) Method is flexible to continuous design and observation spaces and scalable through refinable approximations (feature selection, number of regression points, adaptive grids), but requires careful choice of features and exploration measures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported limitations include: (a) curse of dimensionality — adaptive grid belief representation impractical for high-dimensional θ; companion transport-map approaches are referenced for higher dimensions. (b) Need for good feature selection in linear value-function approximation; poor features degrade policy quality. (c) Approximation errors in backward induction can accumulate (possibly exponentially). (d) Exploration policy must be chosen sensibly; poor exploration can harm the learned policy. (e) No formal convergence guarantees for the iterative exploration/exploitation policy-update scheme are provided in the paper; theoretical analysis deferred to future work. (f) Computational cost: Monte Carlo inner expectations and stochastic optimization over continuous designs are computationally intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_notes_on_how_agent_handles_partial_observability</strong></td>
                            <td>Agent encodes uncertainty explicitly via a belief state (posterior f(θ|I_k)); decision (design) is a function of belief and physical state. Bayesian updates (exact analytic in conjugate case, adaptive-grid inference in nonlinear case) propagate partial observability into the DP state for sequential decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sequential Bayesian optimal experimental design via approximate dynamic programming', 'publication_date_yy_mm': '2016-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1288.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1288.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy OED (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy / Myopic Optimal Experimental Design (baseline defined in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common sequential design baseline that chooses each experiment to maximize only the immediate expected stage reward (e.g., immediate expected information gain) without accounting for future experiments or terminal objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Greedy (myopic) OED</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At each stage k selects d_k = argmax_d E[g_k(x_k,y_k,d)] using the current belief, ignoring the value-to-go J_{k+1}; typically implemented via sampling-based evaluation of expected stage utility. In paper used as a baseline for comparison to sOED-ADP.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Myopic information gain maximization (stagewise expected information gain) — not planning across future stages.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts designs using current posterior only; updates posterior after each observation and recomputes the myopic optimum for the next stage, but does not consider terminal KL reward or coordination across stages.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used as baseline on same environments: Linear-Gaussian inference and nonlinear contaminant source inversion (1D).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same partially observable, continuous-state environments as sOED-ADP; by definition ignores longer-horizon coupling.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as the environment where tested; greedy simpler computationally because no value-to-go approximation is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>In the linear-Gaussian deterministic-variance example greedy yields same designs as sOED (batch and optimal coincide there). In nonlinear contaminant inversion, the paper reports greedy is outperformed by sOED (no numeric value given in supplied text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Generally higher per-decision computational simplicity but can be sample-inefficient overall because it ignores long-term planning; paper provides qualitative evidence in nonlinear case.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicit: greedy focuses on immediate exploitation of current belief; exploration arises only insofar as the immediate utility favors informative designs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against sOED-ADP and batch designs in nonlinear experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Greedy (myopic) design is in general suboptimal relative to full sOED; suboptimality is most evident when future effects and coordination between experiments matter (demonstrated in nonlinear source inversion).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails to plan for future-stage objectives, can miss coordinated multi-stage strategies; performance gap with sOED grows in nonlinear/non-Gaussian problems where the terminal objective depends on future belief evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sequential Bayesian optimal experimental design via approximate dynamic programming', 'publication_date_yy_mm': '2016-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1288",
    "paper_id": "paper-e43618a367ee44666c3241d6ab8c0eb70e745997",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "sOED-ADP",
            "name_full": "Sequential Optimal Experimental Design via Approximate Dynamic Programming",
            "brief_description": "A sequential (closed-loop) Bayesian experimental design algorithm that casts multi-experiment design as a dynamic program and approximates the optimal policy with approximate dynamic programming (ADP), using one-step lookahead policies parameterized by linear value-function approximations and iterative exploration/exploitation policy updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "sOED-ADP (one-step lookahead policy with value-function regression)",
            "agent_description": "Implements a DP formulation of sequential Bayesian OED. Key components: belief-state (posterior) representation (adaptive grid in nonlinear cases or analytic parametric form in conjugate cases); one-step lookahead policy mu_k(x_k)=argmax_d E[g_k + ~J_{k+1}(F_k(...))]; value functions approximated with a linear architecture r_k^T phi_k(x_k) (features are polynomials of posterior mean and log-variance and of physical state); backward induction with regression to fit ~J_k; iterative policy updates mixing exploration (random design sampling) and exploitation (current policy) to generate regression points; Monte Carlo inner expectation estimation and stochastic approximation for noisy optimization over continuous design spaces.",
            "adaptive_design_method": "Information-gain maximization (expected KL divergence / mutual information) implemented within a sequential Bayesian optimal experimental design framework solved approximately via ADP (one-step lookahead with value-function regression).",
            "adaptation_strategy_description": "Adapts designs sequentially using the current belief (posterior) about model parameters and physical state. At each stage the agent selects the next experimental design by maximizing the expected immediate reward plus the approximated value-to-go (~J_{k+1}) under the predicted posterior update. Policies are improved offline by iterating: generate trajectories via exploration (random designs) and exploitation (current policy), compute training targets via Monte Carlo expectation and inner optimization, regress to update value-function weights, then repeat (policy update iterations). Exploration/exploitation mixture (e.g., 30% exploration / 70% exploitation in examples) controls coverage vs. refinement.",
            "environment_name": "Linear-Gaussian parameter inference and Contaminant source inversion (nonlinear advection-diffusion, 1D)",
            "environment_characteristics": "Partially observable (true parameter θ hidden; agent maintains belief/posterior), stochastic observation noise, continuous parameter and observation spaces, continuous multi-dimensional design spaces; in contaminant case includes an additional physical state (vehicle position) and nonlinear forward model producing non-Gaussian posteriors.",
            "environment_complexity": "Linear-Gaussian: very low complexity (scalar parameter θ, horizon N=2, continuous design d in [0.1,3], belief state reducible to mean and variance). Contaminant inversion: higher complexity — scalar spatial source in 1D, physical state is vehicle position z, design is displacement d_k in [-3,3], horizon N variable (multi-stage), belief represented via adaptive grid (100 nodes in example), observation noise possibly state-dependent; continuous action and state spaces with movement constraints and movement costs.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Linear-Gaussian: numerical sOED policies match analytical optimum closely. Exact optimal expected reward U(π*) ≈ 0.7833; numerical sOED (analytical belief representation) achieved mean rewards 0.77 (ℓ=1), 0.78 (ℓ=2), 0.78 (ℓ=3) over 1000 simulated trajectories (MC SE ±0.02). Grid belief representation achieved 0.74, 0.76, 0.75 (ℓ=1..3). Exploration-only baseline: U ≈ -8.5. Contaminant source inversion: paper reports that sOED outperforms batch (open-loop) and greedy (myopic) designs in the nonlinear experiments (qualitative advantage demonstrated), but specific numeric metrics for those cases are not provided in the supplied text.",
            "performance_without_adaptation": "Linear-Gaussian: batch and sOED coincide for this deterministic variance problem (batch not worse); exploration-only baseline much worse (U≈-8.5). Contaminant case: greedy and batch are used as baselines and are reported to be outperformed by sOED in the nonlinear experiments (no numeric values in provided excerpt).",
            "sample_efficiency": "Empirical settings: evaluation of policies used 1000 simulated trajectories to estimate expected reward; regression used 1000 training points (analytical belief) or 500 (grid belief) per policy-update iteration in the linear-Gaussian example; L=3 policy updates were used. No formal sample-complexity bound is provided; sample requirements depend on regression points, Monte Carlo samples for inner expectation, and number of exploration/exploitation trajectories.",
            "exploration_exploitation_tradeoff": "Balanced by explicit mixture during policy-update iterations: initial iterations use pure exploration to generate regression points; subsequent iterations sample both exploration (random designs from a design measure, e.g., a Gaussian) and exploitation (trajectories produced by current policy) — example mixture 30% exploration / 70% exploitation. This balances coverage of the design space (to avoid missing promising regions) with refinement around high-probability states under the current policy.",
            "comparison_methods": "Benchmarked against: analytical optimal solution (when available), batch (open-loop) design, greedy (myopic) sequential design, and an exploration-only baseline. Also compared two belief representations: analytic parametric (conjugate Gaussian) and adaptive-grid discretization.",
            "key_results": "1) Rigorous DP formulation of sOED with information-theoretic objective (expected KL divergence). 2) Practical ADP solution: one-step lookahead policy parameterized via linear value-function approximator, with backward induction/regression and iterative policy updates using exploration/exploitation trajectories. 3) In linear-Gaussian test, ADP-based sOED recovers nearly exact optimal designs and expected reward; grid belief representation agrees well with analytic belief representation. 4) In a nonlinear contaminant source inversion problem, sOED (ADP) yields demonstrable advantages over batch and greedy designs, showing the value of coordinating experiments with feedback under nonlinear/non-Gaussian uncertainty. 5) Method is flexible to continuous design and observation spaces and scalable through refinable approximations (feature selection, number of regression points, adaptive grids), but requires careful choice of features and exploration measures.",
            "limitations_or_failures": "Reported limitations include: (a) curse of dimensionality — adaptive grid belief representation impractical for high-dimensional θ; companion transport-map approaches are referenced for higher dimensions. (b) Need for good feature selection in linear value-function approximation; poor features degrade policy quality. (c) Approximation errors in backward induction can accumulate (possibly exponentially). (d) Exploration policy must be chosen sensibly; poor exploration can harm the learned policy. (e) No formal convergence guarantees for the iterative exploration/exploitation policy-update scheme are provided in the paper; theoretical analysis deferred to future work. (f) Computational cost: Monte Carlo inner expectations and stochastic optimization over continuous designs are computationally intensive.",
            "brief_notes_on_how_agent_handles_partial_observability": "Agent encodes uncertainty explicitly via a belief state (posterior f(θ|I_k)); decision (design) is a function of belief and physical state. Bayesian updates (exact analytic in conjugate case, adaptive-grid inference in nonlinear case) propagate partial observability into the DP state for sequential decision-making.",
            "uuid": "e1288.0",
            "source_info": {
                "paper_title": "Sequential Bayesian optimal experimental design via approximate dynamic programming",
                "publication_date_yy_mm": "2016-04"
            }
        },
        {
            "name_short": "Greedy OED (baseline)",
            "name_full": "Greedy / Myopic Optimal Experimental Design (baseline defined in paper)",
            "brief_description": "A common sequential design baseline that chooses each experiment to maximize only the immediate expected stage reward (e.g., immediate expected information gain) without accounting for future experiments or terminal objectives.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Greedy (myopic) OED",
            "agent_description": "At each stage k selects d_k = argmax_d E[g_k(x_k,y_k,d)] using the current belief, ignoring the value-to-go J_{k+1}; typically implemented via sampling-based evaluation of expected stage utility. In paper used as a baseline for comparison to sOED-ADP.",
            "adaptive_design_method": "Myopic information gain maximization (stagewise expected information gain) — not planning across future stages.",
            "adaptation_strategy_description": "Adapts designs using current posterior only; updates posterior after each observation and recomputes the myopic optimum for the next stage, but does not consider terminal KL reward or coordination across stages.",
            "environment_name": "Used as baseline on same environments: Linear-Gaussian inference and nonlinear contaminant source inversion (1D).",
            "environment_characteristics": "Same partially observable, continuous-state environments as sOED-ADP; by definition ignores longer-horizon coupling.",
            "environment_complexity": "Same as the environment where tested; greedy simpler computationally because no value-to-go approximation is needed.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "In the linear-Gaussian deterministic-variance example greedy yields same designs as sOED (batch and optimal coincide there). In nonlinear contaminant inversion, the paper reports greedy is outperformed by sOED (no numeric value given in supplied text).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Generally higher per-decision computational simplicity but can be sample-inefficient overall because it ignores long-term planning; paper provides qualitative evidence in nonlinear case.",
            "exploration_exploitation_tradeoff": "Implicit: greedy focuses on immediate exploitation of current belief; exploration arises only insofar as the immediate utility favors informative designs.",
            "comparison_methods": "Compared against sOED-ADP and batch designs in nonlinear experiments.",
            "key_results": "Greedy (myopic) design is in general suboptimal relative to full sOED; suboptimality is most evident when future effects and coordination between experiments matter (demonstrated in nonlinear source inversion).",
            "limitations_or_failures": "Fails to plan for future-stage objectives, can miss coordinated multi-stage strategies; performance gap with sOED grows in nonlinear/non-Gaussian problems where the terminal objective depends on future belief evolution.",
            "uuid": "e1288.1",
            "source_info": {
                "paper_title": "Sequential Bayesian optimal experimental design via approximate dynamic programming",
                "publication_date_yy_mm": "2016-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.013830499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sequential Bayesian optimal experimental design via approximate dynamic programming</h1>
<p>Xun Huan<em> and Youssef M. Marzouk</em><br>April 29, 2016</p>
<h4>Abstract</h4>
<p>The design of multiple experiments is commonly undertaken via suboptimal strategies, such as batch (open-loop) design that omits feedback or greedy (myopic) design that does not account for future effects. This paper introduces new strategies for the optimal design of sequential experiments. First, we rigorously formulate the general sequential optimal experimental design (sOED) problem as a dynamic program. Batch and greedy designs are shown to result from special cases of this formulation. We then focus on sOED for parameter inference, adopting a Bayesian formulation with an information theoretic design objective. To make the problem tractable, we develop new numerical approaches for nonlinear design with continuous parameter, design, and observation spaces. We approximate the optimal policy by using backward induction with regression to construct and refine value function approximations in the dynamic program. The proposed algorithm iteratively generates trajectories via exploration and exploitation to improve approximation accuracy in frequently visited regions of the state space. Numerical results are verified against analytical solutions in a linear-Gaussian setting. Advantages over batch and greedy design are then demonstrated on a nonlinear source inversion problem where we seek an optimal policy for sequential sensing.</p>
<h2>1 Introduction</h2>
<p>Experiments are essential to learning about the physical world. Whether obtained through field observations or controlled laboratory experiments, however, experimental data may be time-consuming or expensive to acquire. Also, experiments are not equally useful: some can provide valuable information while others may prove irrelevant to the goals of an investigation. It is thus important to navigate the tradeoff between experimental costs and benefits, and to maximize the ultimate value of experimental data - i.e., to design experiments that are optimal by some appropriate measure. Experimental design thus addresses questions such as where and when to take measurements, which variables to probe, and what experimental conditions to employ.</p>
<p>The systematic design of experiments has received much attention in the statistics community and in many science and engineering applications. Basic design approaches include</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>factorial, composite, and Latin hypercube designs, based on notions of space filling and blocking $[27,13,21,14]$. While these methods can produce useful designs in relatively simple situations involving a few design variables, they generally do not take into account-or exploitknowledge of the underlying physical process. Model-based experimental design uses the relationship between observables, parameters, and design variables to guide the choice of experiments, and optimal experimental design (OED) further incorporates specific and relevant metrics to design experiments for a particular purpose, such as parameter inference, prediction, or model discrimination $[26,2,18]$.</p>
<p>The design of multiple experiments can be pursued via two broad classes of approaches:</p>
<ul>
<li>Batch or open-loop design involves the design of all experiments concurrently, such that the outcome of any experiment cannot affect the design of the others.</li>
<li>Sequential or closed-loop design allows experiments to be chosen and conducted in sequence, thus permitting newly acquired data to guide the design of future experiments. In other words, sequential design involves feedback.</li>
</ul>
<p>Batch OED for linear models is well established (see, e.g., [26, 2]), and recent years have seen many advances in OED methodology for nonlinear models and large-scale applications [35, $34,1,12,32,42,43,56,64]$. In the context of Bayesian design with nonlinear models and non-Gaussian posteriors, rigorous information-theoretic criteria have been proposed [40, 29]; these criteria lead to design strategies that maximize the expected information gain due to the experiments, or equivalently, maximize the mutual information between the experimental observables and the quantities of interest $[52,34,38]$.</p>
<p>In contrast, sequential optimal experimental design (sOED) has seen much less development and use. Many approaches for sequential design rely directly on batch OED, simply by repeating it in a greedy manner for each next experiment; this strategy is known as greedy or myopic design. Since many physically realistic models involve output quantities that depend nonlinearly on model parameters, these models yield non-Gaussian posteriors in a Bayesian setting. The key challenge for greedy design is then to represent and propagate these posteriors beyond the first experiment. Various inference methodologies and representations have been employed within the greedy design framework, with a large body of research based on sample representations of the posterior. For example, posterior importance sampling has been used to evaluate variance-based design utilities [54] and in greedy augmentations of generalized linear models [22]. Sequential Monte Carlo methods have also been used in experimental design for parameter inference [23] and for model discrimination [17, 24]. Even grid-based discretizations/representations of posterior probability density functions have shown success in adaptive design using hierarchical models [37]. While these developments provide a convenient and intuitive avenue for extending existing batch OED tools, greedy design is ultimately suboptimal. An optimal sequential design framework must account for all relevant future effects in making each design decision.
sOED is essentially a problem of sequential decision-making under uncertainty, and thus it can rigorously be cast in a dynamic programming (DP) framework. While DP approaches are widely used in control theory [11, 8, 9], operations research [51, 50], and machine learning [36, 55], their application to sOED raises several distinctive challenges. In the Bayesian sOED context, the state of the dynamic program must incorporate the current posterior distribution or "belief state." In many physical applications, this distribution is continuous, non-Gaussian,</p>
<p>and multi-dimensional. The design variables and observations are typically continuous and multi-dimensional as well. These features of the DP problem lead to enormous computational demands. Thus, while the DP description of sOED has received some attention in recent years $[46,60]$, implementations and applications of this framework remain limited.</p>
<p>Existing attempts have focused mostly on optimal stopping problems [6], motivated by the design of clinical trials. For example, direct backward induction with tabular storage has been used in $[15,61]$, but is only practical for discrete variables that can take on a few possible outcomes. More sophisticated numerical techniques have been used for sOED problems with other special structure. For instance, [16] proposes a forward sampling method that directly optimizes a Monte Carlo estimate of the objective, but targets monotonic loss functions and certain conjugate priors that result in threshold policies based on the posterior mean. Computationally feasible implementations of backward induction have also been demonstrated in situations where policies depend only on low-dimensional sufficient statistics, such as the posterior mean and standard deviation $[7,19]$. Other DP approaches introduce alternative approximations: for instance, [47] solves a dynamic treatment problem over a countable decision space using $Q$-factors approximated by regret functions of quadratic form. Furthermore, most of these efforts employ relatively simple design objectives. Maximizing information gain leads to design objectives that are much more challenging to compute, and thus has been pursued for sOED only in simple situations. For instance, [5] finds near-optimal stopping policies in multidimensional design spaces by exploiting submodularity $[38,28]$ of the expected incremental information gain. However, this is possible only for linear-Gaussian problems, where mutual information does not depend on the realized values of the observations.</p>
<p>Overall, most current efforts in sOED focus on problems with specialized structure and consider settings that are partially or completely discrete (i.e., with experimental outcomes, design variables, or parameters of interest taking only a few values). This paper will develop a mathematical and computational framework for a much broader class of sOED problems. We will do so by developing refinable numerical approximations of the solution to the exact optimal sequential design problem. In particular, we will:</p>
<ul>
<li>Develop a rigorous formulation of the sOED problem for finite numbers of experiments, accommodating nonlinear models (i.e., nonlinear parameter-observable relationships); continuous parameter, design, and observation spaces; a Bayesian treatment of uncertainty encompassing non-Gaussian distributions; and design objectives that quantify information gain.</li>
<li>Develop numerical methodologies for solving such sOED problems in a computationally tractable manner, using approximate dynamic programming (ADP) techniques to find principled approximations of the optimal policy.</li>
</ul>
<p>We will demonstrate our approaches first on a linear-Gaussian problem where an exact solution to the optimal design problem is available, and then on a contaminant source inversion problem involving a nonlinear model of advection and diffusion. In the latter examples, we will explicitly contrast the sOED approach with batch and greedy design methods.</p>
<p>This paper focuses on the formulation of the optimal design problem and on the associated ADP methodologies. The sequential design setting also requires repeated applications of Bayesian inference, using data realized from their prior predictive distributions. A companion</p>
<p>paper will describe efficient strategies for performing the latter; our approach will use transport map representations $[59,25,49,45]$ of the prior and posterior distributions, constructed in a way that allows for fast Bayesian inference tailored to the optimal design problem. A full exploration of such methods is deferred to that paper. To keep the present focus on DP issues, here we will simply discretize the prior and posterior density functions on a grid and perform Bayesian inference via direct evaluations of the posterior density, coupled with a grid adaptation procedure.</p>
<p>The remainder of this paper is organized as follows. Section 2 formulates the sOED problem as a dynamic program, and then shows how batch and greedy design strategies result from simplifications of this general formulation. Section 3 describes ADP techniques for solving the sOED problem in dynamic programming form. Section 4 provides numerical demonstrations of our methodology, and Section 5 includes concluding remarks and a summary of future work.</p>
<h1>2 Formulation</h1>
<p>An optimal approach for designing a collection of experiments conducted in sequence should account for all sources of uncertainty occurring during the experimental campaign, along with a full description of the system state and its evolution. We begin by formulating an optimization problem that encompasses these goals, then cast it as a dynamic program. We next discuss how to choose certain elements of the formulation in order to perform Bayesian OED for parameter inference.</p>
<h3>2.1 Problem definition</h3>
<p>The core components of a general sOED formulation are as follows:</p>
<ul>
<li>Experiment index: $k=0, \ldots, N-1$. The experiments are assumed to occur at discrete times, ordered by the integer index $k$, for a total of $N&lt;\infty$ experiments.</li>
<li>State: $x_{k}=\left[x_{k, b}, x_{k, p}\right] \in \mathcal{X}<em b="b" k_="k,">{k}$. The state contains information necessary to make optimal decisions about the design of future experiments. Generally, it comprises the belief state $x</em>$, which describes deterministic decision-relevant variables. We consider continuous and possibly unbounded state variables. Specific state choices will be discussed later.}$, which reflects the current state of uncertainty, and the physical state $x_{k, p</li>
<li>
<p>Design: $d_{k} \in \mathcal{D}<em k="k">{k}$. The design $d</em>$. We consider continuous real-valued design variables.
Design approaches that produce a policy are sequential (closed-loop) designs because the outcomes of the previous experiments are necessary to determine the current state, which in turn is needed to apply the policy. These approaches contrast with batch (open-loop) designs, where the designs are determined only from the initial state and do not depend on subsequent observations (hence, no feedback). Figure 1 illustrates these two different strategies.}$ represents the conditions under which the $k$ th experiment is to be performed. We seek a policy $\pi \equiv\left{\mu_{0}, \mu_{1}, \ldots, \mu_{N-1}\right}$ consisting of a set of policy functions, one for each experiment, that specify the design as a function of the current state: i.e., $\mu_{k}\left(x_{k}\right)=d_{k</p>
</li>
<li>
<p>Observations: $y_{k} \in \mathcal{Y}_{k}$. The observations from each experiment are endowed with uncertainties representing both measurement noise and modeling error. Along with prior uncertainty on the model parameters, these are assumed to be the only sources of uncertainty in the experimental campaign. Some models might also have internal stochastic dynamics, but we do not study such cases here. We consider continuous real-valued observations.</p>
</li>
<li>Stage reward: $g_{k}\left(x_{k}, y_{k}, d_{k}\right)$. The stage reward reflects the immediate reward associated with performing a particular experiment. This quantity could depend on the state, observations, or design. Typically, it reflects the cost of performing the experiment (e.g., money and/or time), as well as any additional benefits or penalties.</li>
<li>Terminal reward: $g_{N}\left(x_{N}\right)$. The terminal reward reflects the value of the final state $x_{N}$ that is reached after all experiments have been completed.</li>
<li>System dynamics: $x_{k+1}=\mathcal{F}<em k="k">{k}\left(x</em>\right)$. The system dynamics describes the evolution of the system state from one experiment to the next, and includes dependence on both the current design and the observations resulting from the current experiment. This evolution includes the propagation of the belief state (e.g., statistical inference) and of the physical state. The specific form of the dynamics depends on the choice of state variable, and will be discussed later.}, y_{k}, d_{k</li>
</ul>
<p>Taking a decision-theoretic approach, we seek a design policy that maximizes the following expected utility (also called an expected reward) functional:</p>
<p>$$
U(\pi):=\mathbb{E}<em 0="0">{y</em>\right)\right]
$$}, \ldots, y_{N-1} \mid \pi}\left[\sum_{k=0}^{N-1} g_{k}\left(x_{k}, y_{k}, \mu_{k}\left(x_{k}\right)\right)+g_{N}\left(x_{N</p>
<p>where the states must adhere to the system dynamics $x_{k+1}=\mathcal{F}<em k="k">{k}\left(x</em>\right)$. The optimal policy is then}, y_{k}, d_{k</p>
<p>$$
\begin{aligned}
&amp; \pi^{<em>}:=\left{\mu_{0}^{</em>}, \ldots, \mu_{N-1}^{*}\right}=\arg \max <em 0="0">{\pi=\left{\mu</em> \quad U(\pi), \
&amp; \text { s.t. } \quad x_{k+1}=\mathcal{F}}, \ldots, \mu_{N-1}\right}<em k="k">{k}\left(x</em>\right) \
&amp; \mu_{k}\left(\mathcal{X}}, y_{k}, d_{k<em k="k">{k}\right) \subseteq \mathcal{D}</em>, \quad k=0, \ldots, N-1 .
\end{aligned}
$$</p>
<p>For simplicity, we will refer to (2) as "the sOED problem."</p>
<h1>2.2 Dynamic programming form</h1>
<p>The sOED problem involves the optimization of the expected reward functional (1) over a set of policy functions, which is a challenging problem to solve directly. Instead, we can express the problem in an equivalent form using Bellman's principle of optimality [3, 4], leading to a finite-horizon dynamic programming formulation (e.g., $[8,9]$ ):</p>
<p>$$
\begin{aligned}
J_{k}\left(x_{k}\right) &amp; =\max <em k="k">{d</em>} \in \mathcal{D<em y__k="y_{k">{k}} \mathbb{E}</em>} \mid x_{k}, d_{k}}\left[g_{k}\left(x_{k}, y_{k}, d_{k}\right)+J_{k+1}\left(\mathcal{F<em k="k">{k}\left(x</em>\right)\right)\right] \
J_{N}\left(x_{N}\right) &amp; =g_{N}\left(x_{N}\right)
\end{aligned}
$$}, y_{k}, d_{k</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Batch design is an open-loop strategy with no feedback of information, in that the observations $y_{k}$ from any experiment do not affect the design of any other experiments. Sequential design encodes a closed-loop strategy, where feedback of information takes place, and the data $y_{k}$ from an experiment are used to guide the design of subsequent experiments.
for $k=0, \ldots, N-1$. The $J_{k}\left(x_{k}\right)$ functions are known as the "cost-to-go" or "value" functions. Collectively, these expressions are known as Bellman equations. The optimal policies are now implicitly represented by the arguments of each maximization: if $d_{k}^{<em>}=\mu_{k}^{</em>}\left(x_{k}\right)$ maximizes the right-hand side of (3), then the policy $\pi^{<em>}=\left{\mu_{0}^{</em>}, \mu_{1}^{<em>}, \ldots, \mu_{N-1}^{</em>}\right}$ is optimal (under mild assumptions; see [10] for more detail on these verification theorems).</p>
<p>The DP problem is well known to exhibit the "curse of dimensionality," where the number of possible scenarios (i.e., sequences of design and observation realizations) grows exponentially with the number of stages $N$. It often can only be solved numerically and approximately. We will develop numerical methods for finding an approximate solution in Section 3.</p>
<h1>2.3 Information-based Bayesian experimental design</h1>
<p>Our description of the sOED problem thus far has been somewhat general and abstract. We now make it more specific, for the particular goal of inferring uncertain model parameters $\theta$ from noisy and indirect observations $y_{k}$. Given this goal, we can choose appropriate state variables and reward functions.</p>
<p>We use a Bayesian perspective to describe uncertainty and inference. Our state of knowledge about the parameters $\theta$ is represented using a probability distribution. Moreover, if the $k$ th experiment is performed under design $d_{k}$ and yields the outcome $y_{k}$, then our state of knowledge is updated via an application of Bayes' rule:</p>
<p>$$
f\left(\theta \mid y_{k}, d_{k}, I_{k}\right)=\frac{f\left(y_{k} \mid \theta, d_{k}, I_{k}\right) f\left(\theta \mid I_{k}\right)}{f\left(y_{k} \mid d_{k}, I_{k}\right)}
$$</p>
<p>Here, $I_{k}=\left{d_{0}, y_{0}, \ldots, d_{k-1}, y_{k-1}\right}$ is the information vector representing the "history" of previous experiments, i.e., their designs and observations; the probability density function $f\left(\theta \mid I_{k}\right)$ represents the prior for the $k$ th experiment; $f\left(y_{k} \mid \theta, d_{k}, I_{k}\right)$ is the likelihood function; $f\left(y_{k} \mid d_{k}, I_{k}\right)$ is the evidence; and $f\left(\theta \mid y_{k}, d_{k}, I_{k}\right)$ is the posterior probability density following the $k$ th experiment. ${ }^{1}$ Note that $f\left(\theta \mid y_{k}, d_{k}, I_{k}\right)=f\left(\theta \mid I_{k+1}\right)$. To keep notation consistent, we define $I_{0}=\emptyset$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In this Bayesian setting, the "belief state" that describes the state of uncertainty after $k$ experiments is simply the posterior distribution. How to represent this distribution, and thus how to define $x_{k, b}$ in a computation, is an important question. Options include: (i) series representations (e.g., polynomial chaos expansions) of the posterior random variable $\theta \mid I_{k}$ itself; (ii) numerical discretizations of the posterior probability density function $f\left(\theta \mid I_{k}\right)$ or cumulative distribution function $F\left(\theta \mid I_{k}\right)$; (iii) parameters of these distributions, if the priors and posteriors all belong to a simple parametric family; or (iv) the prior $f\left(\theta \mid I_{0}\right)$ at $k=0$ plus the entire history of designs and observations from all previous experiments. For example, if $\theta$ is a discrete random variable that can take on only a finite number of distinct values, then it is natural to define $x_{k, b}$ as the finite-dimensional vector specifying the probability mass function of $\theta$. This is the approach most often taken in constructing partially observable Markov decision processes (POMDP) [53, 51]. Since we are interested in continuous and possibly unbounded $\theta$, an analogous perspective would yield in principle an infinite-dimensional belief state-unless, again, the posteriors belonged to a parametric family (for instance, in the case of conjugate priors and likelihoods). In this paper, we will not restrict our attention to standard parametric families of distributions, however, and thus we will employ finite-dimensional discretizations of infinite-dimensional belief states. The level of discretization is a refinable numerical parameter; details are deferred to Section 3.3. We will also use the shorthand $x_{k, b}=\theta \mid I_{k}$ to convey the underlying notion that the belief state is just the current posterior distribution.</p>
<p>Following the information-theoretic approach suggested by Lindley [40], we choose the terminal reward to be the Kullback-Leibler (KL) divergence from the final posterior (after all $N$ experiments have been performed) to the prior (before any experiment has been performed):</p>
<p>$$
g_{N}\left(x_{N}\right)=D_{\mathrm{KL}}\left(f_{\theta \mid I_{N}} | f_{\theta \mid I_{0}}\right)=\int f_{\theta \mid I_{N}}(\theta) \ln \left[\frac{f_{\theta \mid I_{N}}(\theta)}{f_{\theta \mid I_{0}}(\theta)}\right] d \theta
$$</p>
<p>The stage rewards $\left{g_{k}\right}_{k&lt;N}$ can then be chosen to reflect all other immediate rewards or costs associated with performing particular experiments.</p>
<p>We use the KL divergence in our design objective (1) for several reasons. First, as shown in [29], the expected KL divergence belongs to a broad class of useful divergence measures of the information in a statistical experiment; this class of divergences is defined by a minimal set of requirements that must be satisfied to induce a total information ordering on the space of possible experiments. ${ }^{2}$ Interestingly, these requirements do not rely on a Bayesian perspective or a decision-theoretic formulation, though they can be interpreted quite naturally in these settings. Second, and perhaps more immediately, the KL divergence quantifies information gain in the sense of Shannon information [20, 44]. A large KL divergence from posterior to prior implies that the observations $y_{k}$ decrease entropy in $\theta$ by a large amount, and hence that the observations are informative for parameter inference. Indeed, the expected KL divergence is also equivalent to the mutual information between the parameters $\theta$ and the observations $y_{k}$ (treating both as random variables), given the design $d_{k}$. Third, the KL divergence satisfies useful consistency conditions. It is invariant under one-to-one reparameterizations of $\theta$. And</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>while it is directly applicable to non-Gaussian distributions and to forward models that are nonlinear in the parameters $\theta$, maximizing KL divergence in the linear-Gaussian case reduces to Bayesian $D$-optimal design from linear optimal design theory [18] (i.e., maximizing the determinant of the posterior precision matrix, and hence of the Fisher information matrix plus the prior precision). Finally we should note that, as an alternative to KL divergence, it is entirely reasonable to construct a terminal reward from some other loss function tied to an alternative goal (e.g., squared error loss if the goal is point estimation). But in the absence of such a goal, the KL divergence is a general-purpose objective that seeks to maximize learning about the uncertain environment represented by $\theta$, and should lead to good performance for a broad range of estimation tasks.</p>
<h1>2.4 Notable suboptimal sequential design methods</h1>
<p>Two design approaches frequently encountered in the OED literature are batch design and greedy/myopic sequential design. Both can be seen as special cases or restrictions of the sOED problem formulated here, and are thus in general suboptimal. We illustrate these relationships below.</p>
<p>Batch OED involves the concurrent design of all experiments, and hence the outcome of any experiment cannot affect the design of the others. Mathematically, the policy functions $\mu_{k}$ for batch design do not depend on the states $x_{k}$, since no feedback is involved. (2) thus reduces to an optimization problem over the joint design space $\mathcal{D}:=\mathcal{D}<em 1="1">{0} \times \mathcal{D}</em>$ rather than over a space of policy functions, i.e.,} \times \cdots \mathcal{D}_{N-1</p>
<p>$$
\left(d_{0}^{<em>}, \ldots, d_{N-1}^{</em>}\right)=\underset{\left(d_{0}, \ldots, d_{N-1}\right) \in \mathcal{D}}{\arg \max } \mathbb{E}<em 0="0">{y</em>\right)\right]
$$}, \ldots, y_{N-1} \mid d_{0}, \ldots, d_{N-1}}\left[\sum_{k=0}^{N-1} g_{k}\left(x_{k}, y_{k}, d_{k}\right)+g_{N}\left(x_{N</p>
<p>subject to the system dynamics $x_{k+1}=\mathcal{F}<em k="k">{k}\left(x</em>\right)$, for $k=0, \ldots, N-1$. Since batch OED involves the application of stricter constraints to the sOED problem than (2), it generally yields suboptimal designs.}, y_{k}, d_{k</p>
<p>Greedy design is a particular sequential and closed-loop formulation where only the next experiment is considered at each stage, without taking into account the entire horizon of future experiments and system dynamics. ${ }^{3}$ Mathematically, the greedy policy results from solving</p>
<p>$$
J_{k}\left(x_{k}\right)=\max <em k="k">{d</em>} \in \mathcal{D<em y__k="y_{k">{k}} \mathbb{E}</em>\right)\right]
$$} \mid x_{k}, d_{k}}\left[g_{k}\left(x_{k}, y_{k}, d_{k</p>
<p>where the states must obey the system dynamics $x_{k+1}=\mathcal{F}<em k="k">{k}\left(x</em>$ Since greedy design involves truncating the DP form of the sOED problem, it again yields suboptimal designs.}, y_{k}, d_{k}\right), k=0, \ldots, N-1$. If $d_{k}^{\mathrm{gr}}=\mu_{k}^{\mathrm{gr}}\left(x_{k}\right)$ maximizes the right-hand side of (8) for all $k=0, \ldots, N-1$, then the policy $\pi^{\mathrm{gr}}=\left{\mu_{0}^{\mathrm{gr}}, \mu_{1}^{\mathrm{gr}}, \ldots, \mu_{N-1}^{\mathrm{gr}}\right}$ is the greedy policy. Note that the terminal reward in (4) no longer plays a role in greedy design. ${ }^{4</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3 Numerical approaches</h1>
<p>Approximate dynamic programming (ADP) broadly refers to numerical methods for finding an approximate solution to a DP problem. The development of such techniques has been the target of substantial research efforts across a number of communities (e.g., control theory, operations research, machine learning), targeting different variations of the DP problem. While a variety of terminologies are used in these fields, there is often a large overlap among the fundamental spirits of their solution approaches. We thus take a perspective that groups many ADP techniques into two broad categories:</p>
<ol>
<li>Problem approximation: These are ADP techniques that do not provide a natural way to refine the approximation, or where refinement does not lead to the solution of the original problem. Such techniques typically lead to suboptimal strategies (e.g., batch and greedy designs, certainty-equivalent control, Gaussian approximations).</li>
<li>Solution approximation: Here there is some natural way to refine the approximation, such that the effects of approximation diminish with refinement. These methods have some notion of "convergence" and may be refined towards the solution of the original problem. Methods used in solution approximation include policy iteration, value function and $Q$-factor approximations, numerical optimization, Monte Carlo sampling, regression, quadrature and numerical integration, discretization and aggregation, and rolling horizon procedures.</li>
</ol>
<p>In practice, techniques from both categories are often combined in order to find an approximate solution to a DP problem. The approach in this paper will to try to preserve the original problem as much as possible, relying more heavily on solution approximation techniques to approximately solve the exact problem.</p>
<p>Subsequent sections (Sections 3.1-3.3) will describe successive building blocks of our ADP approach, and the entire algorithm will be summarized in Section 3.4.</p>
<h3>3.1 Policy representation</h3>
<p>In seeking the optimal policy, we first must be able to represent a (generally suboptimal) policy $\pi=\left{\mu_{0}, \mu_{1}, \ldots, \mu_{N-1}\right}$. One option is to represent a candidate policy function $\mu_{k}\left(x_{k}\right)$ directly (and approximately)-for example, by brute-force tabulation over a finite collection of $x_{k}$ values representing a discretization of the state space, or by using standard function approximation techniques. On the other hand, one can preserve the recursive structure of the Bellman equations and "parameterize" the policy via approximations of the value functions appearing in (3). We take this approach here. In particular, we represent the policy using one step of lookahead [8], thus retaining some structure from the original DP problem while keeping the method computationally feasible. By looking ahead only one step, the recursion between value functions is broken and the exponential growth of computational cost with respect to the</p>
<p>horizon $N$ is reduced to linear growth. ${ }^{5}$ The one-step lookahead policy representation ${ }^{6}$ is:</p>
<p>$$
\mu_{k}\left(x_{k}\right)=\underset{d_{k} \in \mathcal{D}<em y__k="y_{k">{k}}{\arg \max } \mathbb{E}</em>} \mid x_{k}, d_{k}}\left[g_{k}\left(x_{k}, y_{k}, d_{k}\right)+\widetilde{J<em k="k">{k+1}\left(\mathcal{F}</em>\right)\right)\right]
$$}\left(x_{k}, y_{k}, d_{k</p>
<p>for $k=0, \ldots, N-1$, and $\widetilde{J}<em N="N">{N}\left(x</em>}\right) \equiv g_{N}\left(x_{N}\right)$. The policy function $\mu_{k}$ is therefore indirectly represented via the approximate value function $\widetilde{J<em 1="1">{k+1}$, and one can view the policy $\pi$ as implicitly parameterized by the set of value functions $\widetilde{J}</em>}, \ldots, \widetilde{J<em k_1="k+1">{N} .{ }^{7}$ If $\widetilde{J}</em>}\left(x_{k+1}\right)=J_{k+1}\left(x_{k+1}\right)$, we recover the Bellman equations (3) and (4), and hence we have $\mu_{k}=\mu_{k}^{*}$. Therefore we would like to find a collection of $\left{\widetilde{J<em k="k">{k+1}\right}</em>$.}$ that is close to $\left{J_{k+1}\right}_{k</p>
<p>We employ a simple parametric "linear architecture" for these value function approximations:</p>
<p>$$
\widetilde{J}<em k="k">{k}\left(x</em>\right)
$$}\right)=r_{k}^{\top} \phi_{k}\left(x_{k}\right)=\sum_{i=1}^{m} r_{k, i} \phi_{k, i}\left(x_{k</p>
<p>where $r_{k, i}$ is the coefficient (weight) corresponding to the $i$ th feature (basis function) $\phi_{k, i}\left(x_{k}\right)$. While more sophisticated nonlinear or even nonparametric function approximations are possible (e.g., $k$-nearest-neighbor [30], kernel regression [48], neural networks [11]), the linear approximator is easy to use and intuitive to understand [39], and is often required for many analysis and convergence results [9]. It follows that the construction of $\widetilde{J}<em k="k">{k}\left(x</em>\right)$ involves the selection of features and the training of coefficients.</p>
<p>The choice of features is an important but often difficult task. A concise set of features that is relevant to the actual dependence of the value function on the state can substantially improve the accuracy and efficiency of (10) and, in turn, of the overall algorithm. Identifying helpful features, however, is non-trivial. In the machine learning and statistics communities, substantial research has been dedicated to the development of systematic procedures for both extracting and selecting features [31, 41]. Nonetheless, finding good features in practice often relies on experience, trial and error, and expert knowledge of the particular problem at hand. We acknowledge the difficulty of this process, but do not pursue a detailed discussion of general and systematic feature construction here. Instead, we employ a reasonable heuristic by choosing features that are polynomial functions of the mean and log-variance of the belief state, as well as of the physical state. The main motivation for this choice stems from the KL divergence term in the terminal reward. The impact of this terminal reward is propagated to earlier stages via the value functions, and hence the value functions must represent the state-dependence of future information gain. While the belief state is generally not Gaussian and the optimal policy is expected to depend on higher moments, the analytic expression for the KL divergence</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>between two univariate Gaussian distributions, which involves their mean and log-variance terms, provides a starting point for promising features. Polynomials then generalize this initial set. We will provide more detail about our feature choices in Section 4. For the present purpose of developing our ADP method, we assume that the features are fixed. We now focus on developing an efficient procedure for training the coefficients.</p>
<h1>3.2 Policy construction via approximate value iteration</h1>
<p>Having decided on a way to represent candidate policies, we now aim to construct policies within this representation class that are close to the optimal policy. We achieve this goal by constructing and iteratively refining value function approximations via regression over targeted relevant states.</p>
<p>Note that the procedure for policy construction described in this section can be performed entirely offline. Once this process is terminated and the resulting value function approximations are available, applying the policy as experimental data are acquired is an online process, which involves evaluating (9). The computational costs of these online evaluations are generally much smaller than those of offline policy construction.</p>
<h3>3.2.1 Backward induction with regression</h3>
<p>Our goal is to find value function approximations (implicitly, policy parameterizations) $\widetilde{J}<em k="k">{k}$ that are close to the value functions $J</em>$ of the optimal policy, i.e., the value functions that satisfy (3) and (4). We take a direct approach, and would in principle like to solve the following "ideal" regression problem: minimize the squared error of the approximation under the state measure induced by the optimal policy,</p>
<p>$$
\min <em k="k">{r</em>}, \forall k} \int_{\mathcal{X<em N-1="N-1">{1} \times \cdots \times \mathcal{X}</em>
$$}}\left[\sum_{k=1}^{N-1}\left(J_{k}\left(x_{k}\right)-r_{k}^{\top} \phi_{k}\left(x_{k}\right)\right)^{2}\right] f_{\pi^{*}}\left(x_{1}, \ldots, x_{N-1}\right) d x_{1} \ldots d x_{N-1</p>
<p>The weighted $L^{2}$ norm above is also known as the $D$-norm in other work [57]; its associated density function is denoted by $f_{\pi^{*}}\left(x_{1}, \ldots, x_{N-1}\right)$. Here we have imposed the linear architecture $\widetilde{J}<em k="k">{k}\left(x</em>}\right)=r_{k}^{\top} \phi_{k}\left(x_{k}\right)(10) . \mathcal{X<em k="k">{k}$ is the support of $x</em>$.</p>
<p>In practice, the integral above must be replaced by a sum over discrete regression points, and the distribution of these points reflects where we place more emphasis on the approximation being accurate. Intuitively, we would like more accurate approximations in regions of the state space that are more frequently visited under the optimal policy, e.g., as captured by sampling from $f_{\pi^{<em>}}$. But we should actually consider a further desideratum: accuracy over the state measure induced by the optimal policy and by the numerical methods used to evaluate this policy (whatever they may be). Numerical optimization methods used to solve (9), for instance, may visit many intermediate values of $d_{k}$ and hence of $x_{k+1}=\mathcal{F}<em k="k">{k}\left(x</em>\right)$. The accuracy of the value function approximation at these intermediate states can be crucial; poor approximations can potentially mislead the optimizer to arrive at completely different designs, and in turn change the outcomes of regression and policy evaluation. We thus include the states visited within our numerical methods (such as iterations of stochastic approximation for solving (9)) as regression points too. For simplicity of notation, we henceforth let $f_{\pi^{}, y_{k}, d_{k</em>}}$ represent the state measure induced by the optimal policy and the associated numerical methods.</p>
<p>In any case, as we have neither $J_{k}\left(x_{k}\right)$ nor $f_{\pi^{*}}\left(x_{1}, \ldots, x_{N-1}\right)$, we must solve (11) approximately. First, to sidestep the need for $J_{k}\left(x_{k}\right)$, we will construct the value function approximations via an approximate value iteration, specifically using backward induction with regression. Starting with $\widetilde{J}<em N="N">{N}\left(x</em>\right)$, we proceed backwards from $k=N-1$ to $k=1$ and form}\right) \equiv g_{N}\left(x_{N</p>
<p>$$
\begin{aligned}
\widetilde{J}<em k="k">{k}\left(x</em>\right) \
&amp; =\mathcal{P}\left{\max }\right) &amp; =r_{k}^{\top} \phi_{k}\left(x_{k<em k="k">{d</em>} \in \mathcal{D<em y__k="y_{k">{k}} \mathbb{E}</em>} \mid x_{k}, d_{k}}\left[g_{k}\left(x_{k}, y_{k}, d_{k}\right)+\widetilde{J<em k="k">{k+1}\left(\mathcal{F}</em>\right)\right)\right]\right} \
&amp; =\mathcal{P} \widetilde{J}}\left(x_{k}, y_{k}, d_{k<em k="k">{k}\left(x</em>\right)
\end{aligned}
$$</p>
<p>where $\mathcal{P}$ is an approximation operator that here represents a regression procedure. This approach leads to a sequence of ideal regression problems to be solved at each stage $k$ :</p>
<p>$$
\min <em k="k">{r</em>}} \int_{\mathcal{X<em k="k">{k}}\left(\widehat{J}</em>
$$}\left(x_{k}\right)-r_{k}^{\top} \phi_{k}\left(x_{k}\right)\right)^{2} f_{\pi^{*}}\left(x_{k}\right) d x_{k</p>
<p>where $\widehat{J}<em k="k">{k}\left(x</em>\right) \equiv \max <em k="k">{d</em>} \in \mathcal{D<em y__k="y_{k">{k}} \mathbb{E}</em>} \mid x_{k}, d_{k}}\left[g_{k}\left(x_{k}, y_{k}, d_{k}\right)+\widetilde{J<em k="k">{k+1}\left(\mathcal{F}</em>\right)\right)\right]$ and $f_{\pi^{}\left(x_{k}, y_{k}, d_{k<em>}}\left(x_{k}\right)$ is the marginal of $f_{\pi^{</em>}}\left(x_{1}, \ldots, x_{N-1}\right)$.</p>
<p>First, we note that since $\widetilde{J}<em k="k">{k}\left(x</em>}\right)$ is built from $\widetilde{J<em k_1="k+1">{k+1}\left(x</em>}\right)$ through backward induction and regression, the effects of approximation error can accumulate, potentially at an exponential rate [58]. The accuracy of all $\widetilde{J<em k="k">{k}\left(x</em>}\right)$ approximations (i.e., for all $k$ ) is thus important. Second, while we no longer need $J_{k}\left(x_{k}\right)$ to construct $\widetilde{J<em k="k">{k}\left(x</em>\right)$. This issue is addressed next.}\right)$, we remain unable to select regression points according to $f_{\pi^{*}}\left(x_{k</p>
<h1>3.2.2 Exploration and exploitation</h1>
<p>Although we cannot a priori generate regression points from the state measure induced by the optimal policy, it is possible to generate them according to a given (suboptimal) policy. We thus generate regression points via two main processes: exploration and exploitation. Exploration is conducted simply by randomly selecting designs (i.e., applying a random policy). For example, if the feasible design space is bounded, the random policy could simply be uniform sampling. In general, however, and certainly when the design spaces $\left{\mathcal{D}<em k="0">{k}\right}</em>\right}}^{N-1}$ are unbounded, a design measure for exploration needs to be prescribed, often selected from experience and an understanding of the problem. The purpose of exploration is to allow a positive probability of probing regions that can potentially lead to good reward. Exploration states are generated from a design measure as follows: we sample $\theta$ from the prior, sample designs $\left{d_{k<em k="k">{k=0}^{N-1}$ from the design measure, generate a $y</em>$.}$ from the likelihood $p\left(y_{k} \mid \theta, d_{k}, I_{k}\right)$ for each design, and then perform inference to obtain states $x_{k}=\theta \mid y_{k}, d_{k}, I_{k</p>
<p>Exploitation, on the other hand, involves using the current understanding of a good policy to visit regions that are also likely to be visited under the optimal policy. Specifically, we will perform exploitation by exercising the one-step lookahead policy based on the currently available approximate value functions $\widetilde{J}_{k}$. In practice, a mixture of both exploration and exploitation is used to achieve good results, and various strategies have been developed and studied for this purpose (see, e.g., [50]). In our algorithm, the states visited from both exploration and exploitation are used as regression points for the least-squares problems in (13). Next, we describe exactly how these points are obtained.</p>
<h1>3.2.3 Iteratively updating approximations of the optimal policy</h1>
<p>Exploitation in the present context involves a dilemma of sorts: generating exploitation points for regression requires the availability of an approximate optimal policy, but the construction of such a policy requires regression points. To address this issue, we introduce an iterative approach to update the approximation of the optimal policy and the state measure induced by it. We refer to this mechanism as "policy update" in this paper, to avoid confusion with approximate value iteration introduced previously.</p>
<p>At a high level, our algorithm alternates between generating regression points via exploitation and then constructing an approximate optimal policy using those regression points. The algorithm is initialized with only an exploration heuristic, denoted by $\pi^{\text {explore }}$. States visited by exploration trajectories generated from $\pi^{\text {explore }}$ are then used as initial regression points to discretize (13), producing a collection of value functions $\left{\widehat{J}<em k="1">{k}^{1}\right}</em>}^{N-1}$ that parameterize the policy $\pi^{1}$. The new policy $\pi^{1}$ is then used to generate exploitation trajectories via (9). These states are mixed with a random selection of exploration states from $\pi^{\text {explore }}$, and this new combined set of states is used as regression points to again discretize and solve (13), yielding value functions $\left{\widehat{J<em k="1">{k}^{2}\right}</em>$ becomes available; smaller changes typically occur in subsequent iterations. A schematic of the procedure is shown in Figure 2.}^{N-1}$ that parameterize an updated policy $\pi^{2}$. The process is repeated. As these iterations continue, we expect a cyclical improvement: regression points should move closer to the state measure induced by the optimal policy, and with more accurate regression, the policies themselves can further improve. The largest change is expected to occur after the first iteration, when the first exploitation policy $\pi^{1</p>
<p>In this paper, we will focus on empirical numerical investigations of these iterations and their convergence. A theoretical analysis of this iterative procedure presents additional challenges, given the mixture of exploration and exploitation points, along with the generally unpredictable state measure induced by the numerical methods used to evaluate the policy. We defer such an analysis to future work.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Iterative procedure for policy updates.
Combining the regression problems (13) from all stages $k=1 \ldots N-1$, the overall problem that is solved approximates the original "ideal" regression problem of (11):
$\min <em k="k">{r</em>}, \forall k} \int_{\mathcal{X<em N-1="N-1">{1} \times \cdots \times \mathcal{X}</em>}}\left[\sum_{k=1}^{N-1}\left(\widehat{J<em k="k">{k}^{\ell+1}\left(x</em>$
where $f_{\pi^{\text {explore }}+\pi^{\ell}}\left(x_{1}, \ldots, x_{N-1}\right)$ is the joint density corresponding to the mixture of exploration and exploitation from the $\ell$ th iteration, and $\left(r_{k}^{\ell+1}\right)^{\top} \phi_{k}\left(x_{k}\right)$ is $\widehat{J}}\right)-\left(r_{k}^{\ell+1}\right)^{\top} \phi_{k}\left(x_{k}\right)\right)^{2}\right] f_{\pi^{\text {explore }}+\pi^{\ell}}\left(x_{1}, \ldots, x_{N-1}\right) d x_{1} \ldots d x_{N-1<em k="k">{k}^{\ell+1}\left(x</em>\right)$. Note that</p>
<p>$f_{\pi^{\text {explore }+\pi^{\ell}}}\left(x_{1}, \ldots, x_{N-1}\right)$ lags one iteration behind $\widehat{J}<em k="k">{k}^{\ell+1}\left(x</em>}\right)$ and $\widehat{J<em k="k">{k}^{\ell+1}\left(x</em>\right)$, since we need to have constructed the policy before we can sample trajectories from it.</p>
<p>Simulating exploitation trajectories, evaluating policies, and computing the values of $\widehat{J}$ for the purpose of linear regression all involve maximizing an expectation over a continuous design space (see both (9) and the definition of $\widehat{J}$ following (13)). While the expected value generally cannot be found analytically, a robust and natural approximation may be obtained via Monte Carlo estimation. As a result, the optimization objective is effectively noisy. We use RobbinsMonro (or Kiefer-Wolfowitz if gradients are not available analytically) stochastic approximation algorithms to solve these stochastic optimization problems; more details can be found in [35].</p>
<h1>3.3 Belief state representation</h1>
<p>As discussed in Section 2.3, a natural choice of the belief state $x_{k, b}$ in the present context is the posterior $\theta \mid I_{k}$. An important question is how to represent this belief state numerically. There are two major considerations. First, since we seek to accommodate general nonlinear forward models with continuous parameters, the posterior distributions are continuous, non-Gaussian, and not from any particular parametric family; such distributions are difficult to represent in a finite- or low-dimensional manner. Second, sequential Bayesian inference, as part of the system dynamics $\mathcal{F}<em k="k">{k}$, needs to be performed repeatedly under different realizations of $d</em>$.}, y_{k}$, and $x_{k</p>
<p>In this paper, we represent belief states numerically by discretizing their probability density functions on a dynamically evolving grid. To perform Bayesian inference, the grid needs to be adapted in order to ensure reasonable coverage and resolution of the posterior density. Our scheme first computes values of the unnormalized posterior density on the current grid, and then decides whether grid expansion is needed on either side, based on a threshold for the ratio of the density value at the grid endpoints to the value of the density at the mode. Second, a uniform grid is laid over the expanded regions, and new unnormalized posterior density values are computed. Finally, a new grid encompassing the original and expanded regions is constructed such that the probability masses between neighboring grid points are equal; this provides a mechanism for coarsening the grid in regions of low probability density.</p>
<p>While this adaptive gridding approach is suitable for one- or perhaps two-dimensional $\theta$, it becomes impractical in higher dimensions. In a companion paper, we will introduce a more flexible technique based on transport maps (e.g., $[59,45]$ ) that can represent multi-dimensional non-Gaussian posteriors in a scalable way, and that immediately enables fast Bayesian inference from multiple realizations of the data.</p>
<h3>3.4 Algorithm pseudocode</h3>
<p>The complete approximate dynamic programming approach developed over the preceding sections is outlined in Algorithm 1.</p>
<h2>4 Numerical examples</h2>
<p>We present two examples to highlight different aspects of the approximate dynamic programming methods developed in this paper. First is a linear-Gaussian problem (Section 4.1). This example establishes (a) the ability of our numerical methods to solve an sOED problem, in</p>
<p>Algorithm 1 Pseudocode for solving the sOED problem.
1: Set parameters: Select number of experiments $N$, features $\left{\phi_{k}\right}<em k="k">{k=1}^{N-1}$, exploration policy $\pi^{\text {explore }}$, number of policy updates $L$, number of exploration trajectories $R$, number of exploitation trajectories $T$
2: for $\ell=1, \ldots, L$ do
3: Exploration: Simulate $R$ exploration trajectories by sampling $\theta$ from the prior, $d</em>\right)$, for $k=0, \ldots, N-1$
4: Store all states visited: $\mathcal{X}}$ from exploration policy $\pi^{\text {explore }}$, and $y_{k}$ from the likelihood $p\left(y_{k} \mid \theta, d_{k}, I_{k<em k="k">{k, \text { explore }}^{\ell}=\left{x</em>\right}}^{r<em k="k">{r=1}^{R}, k=1, \ldots, N-1$
5: Exploitation: If $\ell&gt;1$, simulate $T$ exploitation trajectories by sampling $\theta$ from the prior, $d</em>$ from the one-step lookahead policy</p>
<p>$$
\mu_{k}^{\ell-1}\left(x_{k}\right)=\underset{d_{k}^{\prime}}{\arg \max } \mathbb{E}<em k="k">{y</em>} \mid x_{k}, d_{k}^{\prime}}\left[g_{k}\left(x_{k}, y_{k}, d_{k}^{\prime}\right)+\widetilde{J<em k="k">{k+1}^{\ell-1}\left(\mathcal{F}</em>\right)\right)\right]
$$}\left(x_{k}, y_{k}, d_{k}^{\prime</p>
<p>and $y_{k}$ from the likelihood $p\left(y_{k} \mid \theta, d_{k}, I_{k}\right), k=0, \ldots, N-1$
6: Store all states visited: $\mathcal{X}<em k="k">{k, \text { exploit }}^{\ell}=\left{x</em>\right}}^{t<em k="k">{t=1}^{T}, k=1, \ldots, N-1$
7: Approximate value iteration: Construct functions $\widetilde{J}</em>}^{\ell}$ via backward induction using new regression points $\left{\mathcal{X<em _="{" _text="\text" exploit="exploit" k_="k,">{k, \text { explore }}^{\ell} \cup \mathcal{X}</em>\right}, k=1, \ldots, N-1$, as described in the loops below
8: for $k=N-1, \ldots, 1$ do
9: $\quad$ for $\alpha=1, \ldots, R+T$ where $x_{k}^{(\alpha)}$ are members of $\left{\mathcal{X}}}^{\ell<em _="{" _text="\text" exploit="exploit" k_="k,">{k, \text { explore }}^{\ell} \cup \mathcal{X}</em>\right}$ do
10: Compute training values:}}^{\ell</p>
<p>$$
\widetilde{J}<em k="k">{k}^{\ell}\left(x</em>\right)=\max }^{(\alpha)<em k="k">{d</em>}^{\prime}} \mathbb{E<em k="k">{y</em>} \mid x_{k}^{(\alpha)}, d_{k}^{\prime}}\left[g_{k}\left(x_{k}^{(\alpha)}, y_{k}, d_{k}^{\prime}\right)+\widetilde{J<em k="k">{k+1}^{\ell}\left(\mathcal{F}</em>\right)\right)\right]
$$}\left(x_{k}^{(\alpha)}, y_{k}, d_{k}^{\prime</p>
<p>Construct $\widetilde{J}<em k="k">{k}^{\ell}=\mathcal{P} \widetilde{J}</em>$ by regression on training values
11: end for
12: end for
13: end for
14: Extract final policy parameterization: $\widetilde{J}_{k}^{L}, k=1, \ldots, N-1$
a setting where we can make direct comparisons to an exact solution obtained analytically; and (b) agreement between results generated using grid-based or analytical representations of the belief state, along with their associated inference methods. Second is a nonlinear source inversion problem (Section 4.2). This problem has three cases: Case 1 illustrates the advantage of sOED over batch (open-loop) design; Case 2 illustrates the advantage of sOED over greedy (myopic) design; and Case 3 demonstrates the ability to accommodate longer sequences of experiments, as well as the effects of policy updates.}^{\ell</p>
<h1>4.1 Linear-Gaussian problem</h1>
<h3>4.1.1 Problem setup</h3>
<p>Consider a forward model that is linear in its parameters $\theta$, with a scalar output corrupted by additive Gaussian noise $\epsilon \sim \mathcal{N}\left(0, \sigma_{\epsilon}^{2}\right)$ :</p>
<p>$$
y_{k}=G\left(\theta, d_{k}\right)+\epsilon=\theta d_{k}+\epsilon
$$</p>
<p>The prior on $\theta$ is $\mathcal{N}\left(s_{0}, \sigma_{0}^{2}\right)$ and the design parameter is $d \in\left[d_{L}, d_{R}\right]$. The resulting inference problem on $\theta$ has a conjugate Gaussian structure, such that all subsequent posteriors are Gaussian with mean and variance given by</p>
<p>$$
\left(s_{k+1}, \sigma_{k+1}^{2}\right)=\left(\frac{\frac{y_{k} / d_{k}}{\sigma_{\epsilon}^{2} / d_{k}^{2}}+\frac{s_{k}}{\sigma_{k}^{2}}}{\frac{1}{\sigma_{\epsilon}^{2} / d_{k}^{2}}+\frac{1}{\sigma_{k}^{2}}}, \frac{1}{\frac{1}{\sigma_{\epsilon}^{2} / d_{k}^{2}}+\frac{1}{\sigma_{k}^{2}}}\right)
$$</p>
<p>Let us consider the design of $N=2$ experiments, with prior parameters $s_{0}=0$ and $\sigma_{0}^{2}=9$, noise variance $\sigma_{\epsilon}^{2}=1$, and design limits $d_{L}=0.1$ and $d_{R}=3$. The Gaussian posteriors in this problem-i.e., the belief states-are completely specified by values of the mean and variance; hence we may designate $x_{k, b}=\left(s_{k}, \sigma_{k}^{2}\right)$. We call this parametric representation the "analytical method," as it also allows inference to be performed exactly using (15). The analytical method will be compared to the adaptive-grid representation of the belief state (along with its associated inference procedure) described in Section 3.3. In this example, the adaptive grids use 50 nodes. There is no physical state $x_{k, p}$; we simply have $x_{k}=x_{k, b}$.</p>
<p>Our goal is to infer $\theta$. The stage and terminal reward functions are:</p>
<p>$$
\begin{aligned}
g_{k}\left(x_{k}, y_{k}, d_{k}\right) &amp; =0, \quad k \in{0,1} \
g_{N}\left(x_{N}\right) &amp; =D_{\mathrm{KL}}\left(f_{\theta \mid I_{N}} | f_{\theta \mid I_{0}}\right)-2\left(\ln \sigma_{N}^{2}-\ln 2\right)^{2}
\end{aligned}
$$</p>
<p>The terminal reward is thus a combination of information gain in $\theta$ and a penalty for deviation from a particular log-variance target. The latter term increases the difficulty of this problem by moving the outputs of the optimal policy away from the design space boundary; doing so helps avoid the fortuitous construction of successful policies. ${ }^{8}$ Following the discussion in Section 3.1, we approximate the value functions using features $\phi_{k, i}$ (in (10)) that are polynomials of degree two or less in the posterior mean and log-variance: $1, s_{k}, \ln \left(\sigma_{k}^{2}\right), s_{k}^{2}, \ln ^{2}\left(\sigma_{k}^{2}\right)$, and $s_{k} \ln \left(\sigma_{k}^{2}\right)$. When using the grid representation of the belief state, the values of the features are evaluated by trapezoidal integration rule. The terminal KL divergence is approximated by first estimating the mean and variance, and then applying the analytical formula for KL divergence between Gaussians. The ADP approach uses $L=3$ policy updates (described in Section 3.2.3); these updates are conducted using regression points that, for the first iteration $(\ell=1)$, are generated entirely via exploration. The design measure for exploration is chosen to be $d_{k} \sim \mathcal{N}\left(1.25,0.5^{2}\right)$ in order to have a wide coverage of the design space. ${ }^{9}$ Subsequent iterations use regression with</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>a mix of $30 \%$ exploration samples and $70 \%$ exploitation samples. At each iteration, we use 1000 regression points for the analytical method and 500 regression points for the grid method.</p>
<p>To compare the policies generated via different numerical methods, we apply each policy to generate 1000 simulated trajectories. Producing a trajectory of the system involves first sampling $\theta$ from its prior, applying the policy on the inital belief state $x_{0}$ to obtain the first design $d_{0}$, drawing a sample $y_{0}$ to simulate the outcome of the first experiment, updating the belief state to $x_{1}=\theta \mid y_{0}, d_{0}, I_{0}$, applying the policy to $x_{1}$ in order to obtain $d_{1}$, and so on. The mean reward over all these trajectories is an estimate of the expected total reward (1). But evaluating the reward requires some care. Our procedure for doing so is summarized in Algorithm 2. Each policy is first applied to belief state trajectories computed using the same state representation (i.e., analytical or grid) originally used to construct that policy; this yields sequences of designs and observations. Then, to assess the reward from each trajectory, inference is performed on the associated sequence of designs and observations using a common assessment framework - the analytical method in this case-regardless of how the trajectory was produced. This process ensures a fair comparison between policies, where the designs are produced using the "native" belief state representation for which the policy was originally created, but all final trajectories are assessed using a common method.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">Procedure</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">assessing</span><span class="w"> </span><span class="n">policies</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">simulating</span><span class="w"> </span><span class="n">trajectories</span><span class="o">.</span>
<span class="w">    </span><span class="n">Select</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="s2">&quot;native&quot;</span><span class="w"> </span><span class="n">belief</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">representation</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">generate</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="p">,</span><span class="w"> </span><span class="n">analytical</span><span class="w"> </span><span class="ow">or</span>
<span class="w">    </span><span class="n">grid</span><span class="p">)</span>
<span class="w">    </span><span class="n">Construct</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">pi</span>\<span class="p">)</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">belief</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">representation</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">n_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">trajectories</span><span class="w"> </span><span class="p">}}</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="n">Apply</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">native</span><span class="w"> </span><span class="n">belief</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">representation</span><span class="p">:</span><span class="w"> </span><span class="n">Sample</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">theta</span>\<span class="p">)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prior</span><span class="o">.</span>
<span class="w">        </span><span class="n">Then</span><span class="w"> </span><span class="n">evaluate</span><span class="w"> </span>\<span class="p">(</span><span class="n">d_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">applying</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">constructed</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mu_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">),</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span>\<span class="p">(</span><span class="n">y_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">likelihood</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">p</span>\<span class="n">left</span><span class="p">(</span><span class="n">y_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span>\<span class="n">theta</span><span class="p">,</span><span class="w"> </span><span class="n">d_</span><span class="p">{</span><span class="n">k</span><span class="p">},</span><span class="w"> </span><span class="n">I_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">simulate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">experimental</span><span class="w"> </span><span class="n">outcome</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">evaluate</span><span class="w"> </span>\<span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">=</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">F</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">k</span><span class="p">},</span><span class="w"> </span><span class="n">y_</span><span class="p">{</span><span class="n">k</span><span class="p">},</span><span class="w"> </span><span class="n">d_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">1</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">        </span><span class="n">Evaluate</span><span class="w"> </span><span class="n">rewards</span><span class="w"> </span><span class="n">via</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">common</span><span class="w"> </span><span class="n">assessment</span><span class="w"> </span><span class="n">framework</span><span class="p">:</span><span class="w"> </span><span class="n">Discard</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">belief</span><span class="w"> </span><span class="n">state</span>
<span class="w">        </span><span class="n">sequence</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="w"> </span><span class="n">Perform</span><span class="w"> </span><span class="n">inference</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">d_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">y_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">sequences</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">current</span>
<span class="w">        </span><span class="n">trajectory</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">evaluate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">total</span><span class="w"> </span><span class="n">reward</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">chosen</span><span class="w"> </span><span class="n">common</span><span class="w"> </span><span class="n">assessment</span><span class="w"> </span><span class="n">framework</span><span class="o">.</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<h1>4.1.2 Results</h1>
<p>Since this example has a horizon of $N=2$, we only need to construct the value function approximation $\widehat{J}<em 2="2">{1} . J</em>}=g_{2}$ is the terminal reward and can be evaluated directly. Figure 3 shows contours of $\widehat{J<em k="k">{1}$, along with the regression points used to build the approximation, as a function of the belief state (mean and variance). We observe excellent agreement between the analytical and grid-based methods. We also note a significant change in the distribution of regression points from $\ell=1$ (regression points obtained only from exploration) to $\ell=2$ (regression points obtained from a mixture of exploration and exploitation), leading to a better approximation of the state measure induced by the optimal policy. The regression points appear to be grouped more closely together for $\ell=1$ even though they result from exploration; this is because exploration in fact covers a large region of $d</em>$. In this simple example, our choice of design measure for exploration did not have a particularly}$ space that leads to small values of $\sigma_{k}^{2</p>
<p>negative impact on the expected reward for $\ell=1$ (see Figure 5). However, this situation can easily change for problems with more complicated value functions and less suitable choices of the exploration policy.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Linear-Gaussian problem: contours represent $\widetilde{J}<em 1="1">{1}\left(x</em>\right)$ and blue points are the regression points used to build these value function approximations. The left, middle, and right columns correspond to $\ell=1,2$, and 3 , respectively.</p>
<p>Note that the optimal policy is not unique for this problem, as there is a natural notion of exchangeability between the two designs $d_{0}$ and $d_{1}$. With no stage cost, the overall objective of this problem is the sum of the expected KL divergence and the expected distance of the final log-variance to the target log-variance. Both terms only depend on the final variance, which is determined exactly by the chosen values of $d_{k}$ through (15)-i.e., independently of the observations $y_{k}$. This phenomenon is particular to the linear-Gaussian problem with constant $\sigma_{\epsilon}^{2}$ : the design problem is in fact deterministic, as the final variance is independent of the realized values of $y_{k}$. The optimal policy is then reducible to a choice of scalar-valued optimal designs $d_{0}^{<em>}$ and $d_{1}^{</em>}$. In other words, batch design will produce the same optimal designs as sOED for such deterministic problems since feedback does not add any design-relevant information. Moreover, we can find the exact optimal designs and expected reward function analytically in this problem; a derivation is given in Appendix B of [33]. Figure 4 plots the exact expected reward function over the design space, and overlays pairwise scatter plots of $\left(d_{0}, d_{1}\right)$ from 1000 trajectories simulated with the numerically-obtained sOED policy. The dotted black line represents the locus of exact optimal designs. The symmetry between the two experiments is evident, and the optimizer may hover around different parts of the optimal locus in different cases. The expected reward is relatively flat around the optimal design contour, however, and thus all these methods perform well. Histograms of total reward and their means from the</p>
<p>1000 trajectories are presented in Figure 5 and Table 1. The exact optimal reward value is $U\left(\pi^{*}\right) \approx 0.7833$; the numerical sOED approaches agree with this value very closely, considering the Monte Carlo standard error. In contrast, the exploration policy produces a much lower expected reward of $U\left(\pi^{\text {explore }}\right)=-8.5$.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Linear-Gaussian problem: scatter plots of design pairs $\left(d_{0}, d_{1}\right)$ from 1000 simulated trajectories, superimposed on contours of the exact expected reward function. The black dotted line is the locus of exact optimal designs, obtained analytically. The left, middle, and right columns correspond to $\ell=1,2$, and 3 , respectively.</p>
<p>Table 1: Linear-Gaussian problem: expected reward values (mean values of histograms in Figure 5) from 1000 simulated trajectories. Monte Carlo standard errors are all $\pm 0.02$. The true optimal expected reward is $\approx 0.7833$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\ell=1$</th>
<th style="text-align: center;">$\ell=2$</th>
<th style="text-align: center;">$\ell=3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Analytical</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">Grid</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.75</td>
</tr>
</tbody>
</table>
<p>In summary, we have shown agreement between numerical sOED results and the true optimal design. Furthermore, we have demonstrated agreement between the analytical and grid-based methods of representing the belief state and performing inference, thus establishing credibility of the grid-based method for the subsequent nonlinear and non-Gaussian example, where an exact representation of the belief state is no longer possible.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Linear-Gaussian problem: total reward histograms from 1000 simulated trajectories. The left, middle, and right columns correspond to $\ell=1,2$, and 3 , respectively.</p>
<h1>4.2 Contaminant source inversion problem</h1>
<p>Consider a situation where a chemical contaminant is accidentally released into the air. The contaminant plume diffuses and is advected by the wind. It is crucial to infer the location of the contaminant source so that an appropriate response can be undertaken. Suppose that an aerial or ground-based vehicle is dispatched to measure contaminant concentrations at a sequence of different locations, under a fixed time schedule. We seek the optimal policy for deciding where the vehicle should take measurements in order to maximize expected information gain in the source location. Our sOED problem will also account for hard constraints on possible vehicle movements, as well movement costs incorporated into the stage rewards.</p>
<p>We use the following simple plume model of the contaminant concentration at location $z$ and time $t$, given source location $\theta$ :</p>
<p>$$
G(\theta, z, t)=\frac{s}{\sqrt{2 \pi}(\sqrt{1.2+4 D t})} \exp \left(-\frac{\left|\theta+d_{w}(t)-z\right|^{2}}{2(1.2+4 D t)}\right)
$$</p>
<p>where $s, D$, and $d_{w}(t)$ are the known source intensity, diffusion coefficient, and cumulative net displacement due to wind up to time $t$, respectively. The displacement $d_{w}(t)$ depends on the time history of the wind velocity. (Values of these coefficients will be specified later.) A total of $N$ measurements are performed, at uniformly spaced times given by $t=k+1$. (While $t$ is a continuous variable, we assume it to be suitably scaled so that it corresponds to the experiment index in this fashion; hence, observation $y_{0}$ is taken at $t=1, y_{1}$ at $t=2$, etc.) The state $x_{k}$ is a</p>
<p>combination of a belief state and a physical state. Because an exact parametric representation of the posterior is not available in this nonlinear problem, the belief state is represented by an adaptive discretization of the posterior probability density function, using 100 nodes. The relevant physical state is the current location of the vehicle, i.e., $x_{k, p}=z$. Inclusion of physical state is necessary since the optimal design is expected to depend on the vehicle position as well as the belief state. Here we will consider the source inversion problem in one spatial dimension, where $\theta, d_{k}$, and $x_{k, p}$ are scalars (i.e., the plume and vehicle are confined to movements in a line). The design variables themselves correspond to the spatial displacement of the vehicle from one measurement time to the next. To introduce limits on the range of the vehicle, we use the box constraint $d_{k} \in\left[-d_{L}, d_{R}\right]$, where $d_{L}$ and $d_{R}$ are bounds on the leftwards and rightwards displacement. The physical state dynamics then simply describe position and displacement: $x_{k+1, p}=x_{k, p}+d_{k}$.</p>
<p>The concentration measurements are corrupted by additive Gaussian noise:</p>
<p>$$
y_{k}=G\left(\theta, x_{k+1, p}, k+1\right)+\epsilon_{k}\left(x_{k}, d_{k}\right)
$$</p>
<p>where the noise $\epsilon_{k} \sim \mathcal{N}\left(0, \sigma_{\epsilon_{k}}^{2}\left(x_{k}, d_{k}\right)\right)$ may depend on the state and the design. When simulating a trajectory, the physical state must first be propagated before an observation $y_{k}$ can be generated, since the latter requires the evaluation of $G$ at $x_{k+1, p}$. Once $y_{k}$ is obtained, the belief state can then be propagated forward via Bayesian inference.</p>
<p>The reward functions used in this problem are</p>
<p>$$
\begin{aligned}
g_{k}\left(x_{k}, y_{k}, d_{k}\right) &amp; =-c_{b}-c_{q}\left|d_{k}\right|^{2}, \text { and } \
g_{N}\left(x_{N}\right) &amp; =D_{\mathrm{KL}}\left(f_{\theta \mid I_{N}} | f_{\theta \mid I_{0}}\right)
\end{aligned}
$$</p>
<p>for $k=0, \ldots, N-1$. The terminal reward is simply the KL divergence, and the stage reward consists of a base cost of operation plus a penalty that is quadratic in the vehicle displacement.</p>
<p>We study three different cases of the problem, as described at the start of Section 4. Problem and algorithm settings common to all cases can be found in Tables 2 and 3, and additional variations will be described separately.</p>
<p>Table 2: Contaminant source inversion problem: problem settings.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prior on $\theta$</th>
<th style="text-align: center;">$\mathcal{N}\left(0,2^{2}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Design constraints</td>
<td style="text-align: center;">$d_{k} \in[-3,3]$</td>
</tr>
<tr>
<td style="text-align: center;">Initial physical state</td>
<td style="text-align: center;">$x_{0, p}=5.5$</td>
</tr>
<tr>
<td style="text-align: center;">Concentration strength</td>
<td style="text-align: center;">$s=30$</td>
</tr>
<tr>
<td style="text-align: center;">Diffusion coefficient</td>
<td style="text-align: center;">$D=0.1$</td>
</tr>
<tr>
<td style="text-align: center;">Base operation cost</td>
<td style="text-align: center;">$c_{b}=0.1$</td>
</tr>
<tr>
<td style="text-align: center;">Quadratic movement cost coefficient</td>
<td style="text-align: center;">$c_{q}=0.1$</td>
</tr>
</tbody>
</table>
<h1>4.2.1 Case 1: comparison with greedy (myopic) design</h1>
<p>This case highlights the advantage of sOED over greedy design, which is accentuated when future factors are important to the design of the current experiments. sOED will allow for coordination between subsequent experiments in a way that greedy design does not. We illustrate</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ Without the second term in the terminal reward, the optimal policies will always be those that lead to the highest achievable signal, which occurs at the $d_{k}=3$ boundary. Policies that produce such boundary designs can be realized even when the overall value function approximation is poor. Nothing is wrong with this situation per se, but adding the second term leads to a more challenging test of the numerical approach.
${ }^{9}$ Designs proposed outside the design constraints are simply projected back to the nearest feasible design; thus the actual design measure is not exactly Gaussian.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
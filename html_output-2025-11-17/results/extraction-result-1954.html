<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1954 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1954</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1954</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-41.html">extraction-schema-41</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <p><strong>Paper ID:</strong> paper-273994720</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.08643v2.pdf" target="_blank">A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards</a></p>
                <p><strong>Paper Abstract:</strong> Task specification for robotic manipulation in open-world environments is challenging, requiring flexible and adaptive objectives that align with human intentions and can evolve through iterative feedback. We introduce Iterative Keypoint Reward (IKER), a visually grounded, Python-based reward function that serves as a dynamic task specification. Our framework leverages VLMs to generate and refine these reward functions for multi-step manipulation tasks. Given RGB-D observations and free-form language instructions, we sample keypoints in the scene and generate a reward function conditioned on these keypoints. IKER operates on the spatial relationships between keypoints, leveraging commonsense priors about the desired behaviors, and enabling precise SE(3) control. We reconstruct real-world scenes in simulation and use the generated rewards to train reinforcement learning (RL) policies, which are then deployed into the real world-forming a real-to-sim-to-real loop. Our approach demonstrates notable capabilities across diverse scenarios, including both prehensile and non-prehensile tasks, showcasing multi-step task execution, spontaneous error recovery, and on-the-fly strategy adjustments. The results highlight IKER's effectiveness in enabling robots to perform multi-step tasks in dynamic environments through iterative reward shaping. Project Page: https://iker-robot.github.io/</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1954.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1954.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shoe Place (IKER)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shoe placement task using Iterative Keypoint Reward (IKER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pick-and-place style task: robot picks a shoe from the ground and places it on a rack using policies trained in simulation with IKER rewards and deployed on a real XArm7; sim-to-real transfer evaluated with/without domain randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>pick-and-place (shoe pick and place onto rack)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td>5 cm tolerance (success criterion: average keypoint distance to target ≤ 5 cm)</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Only end-effector kinematics were used: policy outputs ∆p_e and ∆r_e (end-effector pose deltas). Joint-level actuator dynamics (torque limits, bandwidth, motor damping, motor friction) were not explicitly modeled; inverse kinematics maps end-effector poses to joint commands at deployment. Gripper behavior is modeled heuristically (gripper closed by default, opens in grasp mode).</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Joint-level dynamics ignored (ideal inverse kinematics assumed); gripper grasp modeled by a heuristic in simulation; no explicit actuator bandwidth, latency, or torque-limit modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Simplified kinematic-level control in simulation (IsaacGym) with full rigid-body physics for objects but without joint-actuator dynamics fidelity; domain randomization applied to object physical parameters (mass, friction, restitution, compliance) and initial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate (binary per-trial success by keypoint distance ≤ 5 cm). Reported: simulation success 94.5% (0.945) and real-world success 80% (0.8) for the IKER / annotated setup.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Real-world success rate was 14.5 percentage points lower than simulation (0.945 → 0.80).</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training reported as ~5 minutes per task in IsaacGym (single configuration described); authors note this can be reduced with more parallel environments and stronger GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared policies trained with and without domain randomization: simulation performance slightly higher without DR (0.964 no-DR vs 0.945 with-DR), but real-world performance much better with DR (0.8 with-DR vs 0.6 no-DR). Also compared IKER (keypoint-based rewards) vs pose-based rewards: when VLMs auto-generate targets, IKER outperformed pose-based rewards (annotated pose-based ~0.938 sim vs IKER ~0.945 sim; automatic IKER 0.7 vs automatic pose 0.3 in a different experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Domain randomization applied to object properties including friction, mass, restitution, compliance, geometry, and to initial conditions such as object position, gripper location, and grasp pose. Exact numeric ranges are provided in the paper's Appendix B (table truncated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>XArm7 6-DOF manipulator with wrist-mounted camera and a parallel gripper (heuristic grasp in sim; AnyGrasp used in real world for grasp detection).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Failures attributed to: discrepancies between heuristic grasps in simulation vs AnyGrasp grasps in real world, incorrect VLM predictions (wrong keypoint selection or incomplete use of object keypoints), inaccuracies in environment reconstruction and perception (pose estimation errors), and unmodeled extreme object dynamics during pushing/placement.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Actuator-level dynamics were not modeled (kinematic end-effector control + idealized inverse kinematics used), yet domain randomization over object and environment parameters significantly improved sim-to-real transfer; major transfer failures were primarily due to grasp-model mismatch and perception/reconstruction errors rather than explicit joint-actuator modeling, indicating that for these tabletop pick-and-place tasks, accurate grasp modeling and object-dynamics randomization are critical first-order factors while joint-actuator dynamics were less emphasized in this work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1954.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1954.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shoe Push (IKER)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shoe pushing alignment task using Iterative Keypoint Reward (IKER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-prehensile pushing task: robot pushes a shoe to align it with another shoe (pair formation) using policies trained in simulation with IKER rewards and transferred to the XArm7 robot; domain randomization effects studied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>non-prehensile push alignment (shoe push)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td>5 cm tolerance (success criterion: average keypoint distance to target ≤ 5 cm)</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Same as other tasks: only end-effector pose commands (∆p_e, ∆r_e) were modeled in policy; no explicit actuator dynamics (bandwidth, actuator compliance, joint friction) modeled. Physics simulation handles object contact dynamics but actuator dynamics are simplified.</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>No joint-level actuator dynamics or control bandwidth limitations modeled; gripper and contact interactions simplified by physics engine; pushes rely on simulated contact parameters rather than actuator-level impedance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Simplified actuator model (kinematic end-effector control) with physics-based contact simulation for objects; domain randomization over object physical properties to improve robustness to contact/dynamics mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate by keypoint distance threshold. Reported: sim success 87.1% (0.871) and real-world success 85.0% (0.850) for one reported condition. With/without domain randomization: push success 20% without DR vs 70% with DR (real-world evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>For the reported condition the gap was small (0.871 sim → 0.850 real, a 2.1 percentage-point drop); however, without domain randomization real-world performance collapsed (0.2 without DR → 0.7 with DR), showing strong sensitivity to DR.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training time ~5 minutes per task in IsaacGym (authors note possibility of speedup with more parallelism).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared policies trained with and without domain randomization: pushing tasks are highly sensitive — DR greatly improved real-world transfer (0.2 no-DR → 0.7 with-DR). Also compared keypoint-based IKER vs pose-based reward representations: IKER outperformed pose-based automatic generation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized object mass, friction, restitution, compliance, geometry, object position, gripper location, and grasp parameters. The authors emphasize these randomizations as crucial for non-prehensile tasks like pushing.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>XArm7 6-DOF manipulator with wrist-mounted camera and parallel gripper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Without DR the policy often crushed the shoe or caused it to slip or rotate unpredictably during pushing (failure modes attributed to mismatch in contact dynamics and insufficient variability in simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Non-prehensile contact-rich tasks are particularly sensitive to object/contact dynamics; while actuator dynamics were not explicitly modeled, domain randomization over object contact properties (mass, friction, restitution, compliance) substantially improved sim-to-real performance, implying that accurate modeling or robustification of contact/dynamics parameters is critical for transfer of pushing behaviors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1954.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1954.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Book Push / Book Reorient (IKER)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Book pushing and reorientation tasks using Iterative Keypoint Reward (IKER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Push-to-align and reorientation tasks where robot pushes or reorients books (sometimes large/partially graspable) using policies trained in simulation with IKER; used to test robustness, re-grasp/re-plan behaviors, and non-prehensile/prehensile transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>push-to-align / reorient book (stow/reorient)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td>5 cm tolerance (success criterion: average keypoint distance to target ≤ 5 cm)</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Policy issues end-effector pose deltas; no explicit modeling of joint actuator dynamics; grasping uses heuristic in sim and AnyGrasp in real world; large-book cases often require pushing rather than grasping.</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Idealized inverse kinematics & heuristic gripper behavior; no explicit modeling of actuator bandwidth, delay, or torque limits; contact interactions handled by simulation physics with randomized object parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Kinematic-level control with physics-based object contact simulation and domain randomization on object properties; actuator fidelity low (no joint-actuator dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate by keypoint distance threshold (5 cm). Specific per-task numeric success rates for book tasks are reported in aggregate in Table I but explicit per-task sim vs real percentages are not enumerated in text snippets beyond qualitative statements that annotated (human-labeled) rewards performed similarly between keypoints and pose representations.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training time ~5 minutes per task in IsaacGym (per authors).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Authors compared keypoint-based IKER vs pose-based rewards and found IKER substantially better when VLMs auto-generate targets (pose-based automatic targets struggled with rotations in SO(3)). Domain randomization improved real-world robustness though exact numeric comparisons per book task are not fully enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Same set of object and scene randomizations (mass, friction, restitution, compliance, geometry, positions, grasp perturbations).</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>XArm7 6-DOF manipulator with wrist-mounted camera and parallel gripper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Failures arise from incorrect VLM predictions (wrong keypoints, insufficient use of keypoints for alignment), pose-estimation inaccuracies, and grasping limitations for large books (leading to re-planning to push strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>When tasks require orientation reasoning (SO(3)) and regrasping, keypoint-based goal specification reduces VLM difficulty and improves transfer; however, unmodeled actuator-level dynamics and inaccurate grasp models still produce failures, indicating that accurate grasp/contact modeling and robust perception matter more than joint-actuator fidelity for these single-object manipulation scenarios in this work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1954.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1954.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-step chaining (box then shoes)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-step task chaining: push shoe box to create space then sequentially place shoes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage real-world experiment where IKER iteratively replans: first push a shoe box to clear space, then place two shoes on a rack; policies trained in sim and applied to real XArm7, compared against VoxPoser baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multi-step sequential manipulation (push box then place multiple objects)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td>5 cm tolerance (success criterion: average keypoint distance to target ≤ 5 cm)</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>End-effector pose control; no explicit actuator dynamics modeling; closed-loop replanning used to adapt to environment changes; physics simulation used for object interactions but actuator bandwidth/latency not modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Joint-actuator dynamics omitted; heuristic grasping in simulation; actuator control assumed ideal at end-effector level.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Closed-loop, kinematic-level control with physics-based object simulation and domain randomization of object properties; actuator fidelity low (no joint-level dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Per-stage success counts over 10 start/end configurations (real-world evaluation). Reported results (IKER vs VoxPoser): Stage 1 (push box) IKER 8/10 vs VoxPoser 5/10; Stage 2 IKER 5/10 vs VoxPoser 1/10; Stage 3 IKER 4/10 vs VoxPoser 0/10.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training time reported ~5 minutes per task in IsaacGym; multi-step tasks require sequential policy training phases per stage.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared closed-loop IKER-based replanning against VoxPoser open-loop baseline; IKER substantially outperformed VoxPoser in chained multi-step execution in real-world trials. Domain randomization and closed-loop feedback identified as important for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>As above: object mass, friction, restitution, compliance, geometry and initial conditions randomized; randomization highlighted as critical for non-prehensile and multi-step robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>XArm7 6-DOF manipulator with wrist-mounted camera and parallel gripper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Failures attributed to poor grasps (heuristic vs real-world grasp detection mismatch), VLM mispredictions (wrong keypoints or incomplete keypoint usage), and contact/dynamics discrepancies during pushes (e.g., pushing box too far or not far enough).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Closed-loop iterative reward specification (IKER) combined with domain randomization and object-mesh reconstruction enables multi-step sim-to-real transfer better than open-loop planning baselines; however, omission of actuator-level dynamics modeling and mismatch in simulated vs real grasp implementation remain practical failure modes, implying that accurate grasp/contact modeling and closed-loop perceptual feedback are crucial for chained tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reconciling reality through simulation: A realto-sim-to-real approach for robust manipulation <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Bayessimig: Scalable parameter inference for adaptive domain randomization with isaacgym <em>(Rating: 1)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1954",
    "paper_id": "paper-273994720",
    "extraction_schema_id": "extraction-schema-41",
    "extracted_data": [
        {
            "name_short": "Shoe Place (IKER)",
            "name_full": "Shoe placement task using Iterative Keypoint Reward (IKER)",
            "brief_description": "Pick-and-place style task: robot picks a shoe from the ground and places it on a rack using policies trained in simulation with IKER rewards and deployed on a real XArm7; sim-to-real transfer evaluated with/without domain randomization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "pick-and-place (shoe pick and place onto rack)",
            "task_timescale": null,
            "task_contact_ratio": null,
            "task_precision_requirement": "5 cm tolerance (success criterion: average keypoint distance to target ≤ 5 cm)",
            "actuator_parameters_modeled": "Only end-effector kinematics were used: policy outputs ∆p_e and ∆r_e (end-effector pose deltas). Joint-level actuator dynamics (torque limits, bandwidth, motor damping, motor friction) were not explicitly modeled; inverse kinematics maps end-effector poses to joint commands at deployment. Gripper behavior is modeled heuristically (gripper closed by default, opens in grasp mode).",
            "actuator_parameters_simplified": "Joint-level dynamics ignored (ideal inverse kinematics assumed); gripper grasp modeled by a heuristic in simulation; no explicit actuator bandwidth, latency, or torque-limit modeling.",
            "fidelity_level_description": "Simplified kinematic-level control in simulation (IsaacGym) with full rigid-body physics for objects but without joint-actuator dynamics fidelity; domain randomization applied to object physical parameters (mass, friction, restitution, compliance) and initial conditions.",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Task success rate (binary per-trial success by keypoint distance ≤ 5 cm). Reported: simulation success 94.5% (0.945) and real-world success 80% (0.8) for the IKER / annotated setup.",
            "sim_vs_real_performance": "Real-world success rate was 14.5 percentage points lower than simulation (0.945 → 0.80).",
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": true,
            "computational_cost_details": "Training reported as ~5 minutes per task in IsaacGym (single configuration described); authors note this can be reduced with more parallel environments and stronger GPUs.",
            "fidelity_comparison": "Compared policies trained with and without domain randomization: simulation performance slightly higher without DR (0.964 no-DR vs 0.945 with-DR), but real-world performance much better with DR (0.8 with-DR vs 0.6 no-DR). Also compared IKER (keypoint-based rewards) vs pose-based rewards: when VLMs auto-generate targets, IKER outperformed pose-based rewards (annotated pose-based ~0.938 sim vs IKER ~0.945 sim; automatic IKER 0.7 vs automatic pose 0.3 in a different experiment).",
            "domain_randomization_used": true,
            "domain_randomization_details": "Domain randomization applied to object properties including friction, mass, restitution, compliance, geometry, and to initial conditions such as object position, gripper location, and grasp pose. Exact numeric ranges are provided in the paper's Appendix B (table truncated in main text).",
            "robot_type": "XArm7 6-DOF manipulator with wrist-mounted camera and a parallel gripper (heuristic grasp in sim; AnyGrasp used in real world for grasp detection).",
            "transfer_failure_analysis": "Failures attributed to: discrepancies between heuristic grasps in simulation vs AnyGrasp grasps in real world, incorrect VLM predictions (wrong keypoint selection or incomplete use of object keypoints), inaccuracies in environment reconstruction and perception (pose estimation errors), and unmodeled extreme object dynamics during pushing/placement.",
            "key_finding_for_theory": "Actuator-level dynamics were not modeled (kinematic end-effector control + idealized inverse kinematics used), yet domain randomization over object and environment parameters significantly improved sim-to-real transfer; major transfer failures were primarily due to grasp-model mismatch and perception/reconstruction errors rather than explicit joint-actuator modeling, indicating that for these tabletop pick-and-place tasks, accurate grasp modeling and object-dynamics randomization are critical first-order factors while joint-actuator dynamics were less emphasized in this work.",
            "uuid": "e1954.0"
        },
        {
            "name_short": "Shoe Push (IKER)",
            "name_full": "Shoe pushing alignment task using Iterative Keypoint Reward (IKER)",
            "brief_description": "Non-prehensile pushing task: robot pushes a shoe to align it with another shoe (pair formation) using policies trained in simulation with IKER rewards and transferred to the XArm7 robot; domain randomization effects studied.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "non-prehensile push alignment (shoe push)",
            "task_timescale": null,
            "task_contact_ratio": null,
            "task_precision_requirement": "5 cm tolerance (success criterion: average keypoint distance to target ≤ 5 cm)",
            "actuator_parameters_modeled": "Same as other tasks: only end-effector pose commands (∆p_e, ∆r_e) were modeled in policy; no explicit actuator dynamics (bandwidth, actuator compliance, joint friction) modeled. Physics simulation handles object contact dynamics but actuator dynamics are simplified.",
            "actuator_parameters_simplified": "No joint-level actuator dynamics or control bandwidth limitations modeled; gripper and contact interactions simplified by physics engine; pushes rely on simulated contact parameters rather than actuator-level impedance.",
            "fidelity_level_description": "Simplified actuator model (kinematic end-effector control) with physics-based contact simulation for objects; domain randomization over object physical properties to improve robustness to contact/dynamics mismatch.",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Task success rate by keypoint distance threshold. Reported: sim success 87.1% (0.871) and real-world success 85.0% (0.850) for one reported condition. With/without domain randomization: push success 20% without DR vs 70% with DR (real-world evaluation).",
            "sim_vs_real_performance": "For the reported condition the gap was small (0.871 sim → 0.850 real, a 2.1 percentage-point drop); however, without domain randomization real-world performance collapsed (0.2 without DR → 0.7 with DR), showing strong sensitivity to DR.",
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": true,
            "computational_cost_details": "Training time ~5 minutes per task in IsaacGym (authors note possibility of speedup with more parallelism).",
            "fidelity_comparison": "Compared policies trained with and without domain randomization: pushing tasks are highly sensitive — DR greatly improved real-world transfer (0.2 no-DR → 0.7 with-DR). Also compared keypoint-based IKER vs pose-based reward representations: IKER outperformed pose-based automatic generation.",
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized object mass, friction, restitution, compliance, geometry, object position, gripper location, and grasp parameters. The authors emphasize these randomizations as crucial for non-prehensile tasks like pushing.",
            "robot_type": "XArm7 6-DOF manipulator with wrist-mounted camera and parallel gripper.",
            "transfer_failure_analysis": "Without DR the policy often crushed the shoe or caused it to slip or rotate unpredictably during pushing (failure modes attributed to mismatch in contact dynamics and insufficient variability in simulation).",
            "key_finding_for_theory": "Non-prehensile contact-rich tasks are particularly sensitive to object/contact dynamics; while actuator dynamics were not explicitly modeled, domain randomization over object contact properties (mass, friction, restitution, compliance) substantially improved sim-to-real performance, implying that accurate modeling or robustification of contact/dynamics parameters is critical for transfer of pushing behaviors.",
            "uuid": "e1954.1"
        },
        {
            "name_short": "Book Push / Book Reorient (IKER)",
            "name_full": "Book pushing and reorientation tasks using Iterative Keypoint Reward (IKER)",
            "brief_description": "Push-to-align and reorientation tasks where robot pushes or reorients books (sometimes large/partially graspable) using policies trained in simulation with IKER; used to test robustness, re-grasp/re-plan behaviors, and non-prehensile/prehensile transitions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "push-to-align / reorient book (stow/reorient)",
            "task_timescale": null,
            "task_contact_ratio": null,
            "task_precision_requirement": "5 cm tolerance (success criterion: average keypoint distance to target ≤ 5 cm)",
            "actuator_parameters_modeled": "Policy issues end-effector pose deltas; no explicit modeling of joint actuator dynamics; grasping uses heuristic in sim and AnyGrasp in real world; large-book cases often require pushing rather than grasping.",
            "actuator_parameters_simplified": "Idealized inverse kinematics & heuristic gripper behavior; no explicit modeling of actuator bandwidth, delay, or torque limits; contact interactions handled by simulation physics with randomized object parameters.",
            "fidelity_level_description": "Kinematic-level control with physics-based object contact simulation and domain randomization on object properties; actuator fidelity low (no joint-actuator dynamics).",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Task success rate by keypoint distance threshold (5 cm). Specific per-task numeric success rates for book tasks are reported in aggregate in Table I but explicit per-task sim vs real percentages are not enumerated in text snippets beyond qualitative statements that annotated (human-labeled) rewards performed similarly between keypoints and pose representations.",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": true,
            "computational_cost_details": "Training time ~5 minutes per task in IsaacGym (per authors).",
            "fidelity_comparison": "Authors compared keypoint-based IKER vs pose-based rewards and found IKER substantially better when VLMs auto-generate targets (pose-based automatic targets struggled with rotations in SO(3)). Domain randomization improved real-world robustness though exact numeric comparisons per book task are not fully enumerated in text.",
            "domain_randomization_used": true,
            "domain_randomization_details": "Same set of object and scene randomizations (mass, friction, restitution, compliance, geometry, positions, grasp perturbations).",
            "robot_type": "XArm7 6-DOF manipulator with wrist-mounted camera and parallel gripper.",
            "transfer_failure_analysis": "Failures arise from incorrect VLM predictions (wrong keypoints, insufficient use of keypoints for alignment), pose-estimation inaccuracies, and grasping limitations for large books (leading to re-planning to push strategies).",
            "key_finding_for_theory": "When tasks require orientation reasoning (SO(3)) and regrasping, keypoint-based goal specification reduces VLM difficulty and improves transfer; however, unmodeled actuator-level dynamics and inaccurate grasp models still produce failures, indicating that accurate grasp/contact modeling and robust perception matter more than joint-actuator fidelity for these single-object manipulation scenarios in this work.",
            "uuid": "e1954.2"
        },
        {
            "name_short": "Multi-step chaining (box then shoes)",
            "name_full": "Multi-step task chaining: push shoe box to create space then sequentially place shoes",
            "brief_description": "A multi-stage real-world experiment where IKER iteratively replans: first push a shoe box to clear space, then place two shoes on a rack; policies trained in sim and applied to real XArm7, compared against VoxPoser baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "multi-step sequential manipulation (push box then place multiple objects)",
            "task_timescale": null,
            "task_contact_ratio": null,
            "task_precision_requirement": "5 cm tolerance (success criterion: average keypoint distance to target ≤ 5 cm)",
            "actuator_parameters_modeled": "End-effector pose control; no explicit actuator dynamics modeling; closed-loop replanning used to adapt to environment changes; physics simulation used for object interactions but actuator bandwidth/latency not modeled.",
            "actuator_parameters_simplified": "Joint-actuator dynamics omitted; heuristic grasping in simulation; actuator control assumed ideal at end-effector level.",
            "fidelity_level_description": "Closed-loop, kinematic-level control with physics-based object simulation and domain randomization of object properties; actuator fidelity low (no joint-level dynamics).",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Per-stage success counts over 10 start/end configurations (real-world evaluation). Reported results (IKER vs VoxPoser): Stage 1 (push box) IKER 8/10 vs VoxPoser 5/10; Stage 2 IKER 5/10 vs VoxPoser 1/10; Stage 3 IKER 4/10 vs VoxPoser 0/10.",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": true,
            "computational_cost_details": "Training time reported ~5 minutes per task in IsaacGym; multi-step tasks require sequential policy training phases per stage.",
            "fidelity_comparison": "Compared closed-loop IKER-based replanning against VoxPoser open-loop baseline; IKER substantially outperformed VoxPoser in chained multi-step execution in real-world trials. Domain randomization and closed-loop feedback identified as important for robustness.",
            "domain_randomization_used": true,
            "domain_randomization_details": "As above: object mass, friction, restitution, compliance, geometry and initial conditions randomized; randomization highlighted as critical for non-prehensile and multi-step robustness.",
            "robot_type": "XArm7 6-DOF manipulator with wrist-mounted camera and parallel gripper.",
            "transfer_failure_analysis": "Failures attributed to poor grasps (heuristic vs real-world grasp detection mismatch), VLM mispredictions (wrong keypoints or incomplete keypoint usage), and contact/dynamics discrepancies during pushes (e.g., pushing box too far or not far enough).",
            "key_finding_for_theory": "Closed-loop iterative reward specification (IKER) combined with domain randomization and object-mesh reconstruction enables multi-step sim-to-real transfer better than open-loop planning baselines; however, omission of actuator-level dynamics modeling and mismatch in simulated vs real grasp implementation remain practical failure modes, implying that accurate grasp/contact modeling and closed-loop perceptual feedback are crucial for chained tasks.",
            "uuid": "e1954.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reconciling reality through simulation: A realto-sim-to-real approach for robust manipulation",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Bayessimig: Scalable parameter inference for adaptive domain randomization with isaacgym",
            "rating": 1
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 1
        }
    ],
    "cost": 0.015891999999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards
18 Feb 2025</p>
<p>Shivansh Patel 
Xinchen Yin 
Wenlong Huang 
Shubham Garg 
Hooshang Nayyeri 
Li Fei-Fei 
Svetlana Lazebnik 
Yunzhu Li 
A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards
18 Feb 20256E13CFB72120D237447AAA74061540C2arXiv:2502.08643v2[cs.RO]
Task specification for robotic manipulation in open-world environments is challenging, requiring flexible and adaptive objectives that align with human intentions and can evolve through iterative feedback.We introduce Iterative Keypoint Reward (IKER), a visually grounded, Python-based reward function that serves as a dynamic task specification.Our framework leverages VLMs to generate and refine these reward functions for multi-step manipulation tasks.Given RGB-D observations and free-form language instructions, we sample keypoints in the scene and generate a reward function conditioned on these keypoints.IKER operates on the spatial relationships between keypoints, leveraging commonsense priors about the desired behaviors, and enabling precise SE(3) control.We reconstruct real-world scenes in simulation and use the generated rewards to train reinforcement learning (RL) policies, which are then deployed into the real world-forming a real-to-sim-to-real loop.Our approach demonstrates notable capabilities across diverse scenarios, including both prehensile and non-prehensile tasks, showcasing multi-step task execution, spontaneous error recovery, and on-the-fly strategy adjustments.The results highlight IKER's effectiveness in enabling robots to perform multi-step tasks in dynamic environments through iterative reward shaping.Project</p>
<p>I. INTRODUCTION</p>
<p>Suppose that a robot is tasked with placing a pair of shoes on a rack, but a shoe box is occupying the rack, leaving insufficient space for both shoes (Figure 1, top right).The robot must first push the box aside to create space and then proceed to place the shoes.This example highlights the importance of task specification for robots in unstructured, realworld environments, where tasks can often involve multiple implicit steps.In such cases, rigid predefined instructions fail to capture the complexities of interaction required to accomplish the goal.To be effective, task specifications must incorporate commonsense priors-expectations about how the robot should behave.For instance, rather than attempting to squeeze the shoes in awkwardly, the robot should realize that it must first clear space.</p>
<p>Recent vision-language models (VLMs) show promise for freeform robotic task specification due to their rapidly advancing ability to encode rich world knowledge by pretraining on vast and diverse datasets [1][2][3][4][5][6][7][8].VLMs excel in interpreting natural language descriptions and complex instructions.Their broad knowledge bridges human expectations and robot behavior, capturing human-like priors and problem-solving strategies.However, previous works that *indicates equal contribution. 1University of Illinois at Urbana-Champaign, 2 Stanford University, 3 Amazon, 4   leverage VLMs in robotics face two major limitations: (1) they lack the capability to specify precise target locations in 3D, and (2) they are often unable to adapt to the environment changes as the task progresses.</p>
<p>In this work, we introduce Iterative Keypoint Reward (IKER), a visually grounded reward function for robotic manipulation that addresses these limitations.Inspired by recent work [9], we draw the observation that both object positions and orientations can be encoded using keypoints.Hence, IKER allows for fine-grained manipulation in 3D, facilitating complex tasks that require accurate location and orientation control.Additionally, IKER incorporates an iterative refinement mechanism, where the VLM updates the task specification based on feedback from the robot's interactions with the environment.This mechanism enables dynamically-adjusting strategies and intermediate steps, such as repositioning objects for a better grasp.</p>
<p>While VLMs excel in processing real-world visual data, training policies directly in the real world is often infeasible due to safety, scalability, and efficiency constraints.</p>
<p>To address this, we first generate IKER using real-world observations, then transfer the scene and the reward to simulation for training, and finally, deploy the optimized policy back into the real world.Thus, our system operates in a real-to-sim-to-real loop.</p>
<p>We demonstrate the efficacy of our real-to-sim-to-real framework with IKER across diverse scenarios involving VLMs in Robotics.VLMs have become a prominent tool in robotics .Existing works utilizing VLMs in robotics primarily focus on two areas: task specification [10-12, 15, 16, 21] and low-level control [11,13,14,39].Our work aligns with the former, with an emphasis on flexibility and adaptability in complex, real-world environments.</p>
<p>For task specification, many works employ VLMs to break down complex tasks into manageable subtasks, demonstrating their utility in bridging high-level instructions and robotic actions.Huang et al. [40] demonstrate the use of LLMs as zero-shot planners, enabling task decomposition into actionable steps.Similarly, Ahn et al. [10] leverage VLMs to parse long-horizon tasks and sequence them into executable steps for robots.Belkhale et al. [41] introduce "language motions" that serve as intermediaries between high-level instructions and specific robotic actions, allowing policies to capture reusable, low-level behaviors.Unlike these works, our approach focuses on flexible interpretation of tasks in the context of a dynamically changing environment.</p>
<p>Beyond task decomposition, VLMs have been used to generate affordances and value maps that guide robotic actions.Huang et al. [12] employs VLMs to generate 3D affordance maps, providing robots with spatial knowledge of which parts of the environment are suitable for interaction.Liu et al. [15] use VLMs to predict point-based affordances, enabling zeroshot manipulation tasks.Zhao et al. [42] incorporate VLMs into model predictive control, where the models predict the outcomes of candidate actions to guide optimal decisionmaking.These works demonstrate the potential of VLMs to bridge high-level task understanding with spatial and functional knowledge needed for robotic control.Similar to our work, Huang et al. [9] use keypoints and define relations and constraints between them to execute manipulation tasks, but their approach follows an open-loop strategy.In contrast, we employ a closed-loop approach, enabling dynamic plan adjustments.Additionally, our approach also supports nonprehensile manipulations, such as pushing.</p>
<p>Some works have also explored VLMs for reward function generation [43][44][45][46].However, most of these approaches have limited real-world applicability.Some lack demonstrations on real robots [44], are restricted to a single realworld scenario [46], or focus on highly constrained tasks like a robot dog walking on a ball [45].In contrast, our work demonstrates the versatility and robustness of VLMgenerated rewards on multiple real-world manipulation tasks.</p>
<p>Algorithm 1 IKER Execution Framework 1: Given: Language instruction I 2: Initialize: done ← false, execution history ← [ ], i ← 1 3: Generate 3D models of objects 4: while true do 5:</p>
<p>({k
(i) j } K i j=1 , Oi) ← GetKeypoints(3D models) 6:
code ← QueryVLM(Oi, execution history)</p>
<p>7:</p>
<p>(done, {k
target(i) j } K i j=1 ) ← Execute(code)</p>
<p>8:</p>
<p>if done is true then 9: πi ← LearnPolicy(si, {k
target(i) j } K i j=1 )</p>
<p>13:</p>
<p>ExecutePolicyInRealWorld(πi)</p>
<p>14:</p>
<p>Append (Oi, code) to execution history 15:
i ← i + 1
16: end while Real-to-Sim and Sim-to-Real.Real-to-sim has gained significant attention for its ability to facilitate agent training.Once a scene is transferred to simulation, it can be used for a wide range of tasks, including RL.Several approaches focus on reconstructing rigid bodies for use in simulation [47][48][49][50][51][52][53].For instance, Kappler et al. [47] introduce a method for reconstructing rigid objects to facilitate grasping.Some works rather focus on reconstructing articulated objects [54][55][56][57][58][59][60][61].Huang et al. [62] present methods for reconstructing the occluded shapes of articulated objects.Jiang et al. [55] introduce a framework, DITTO, to generate digital twins of articulated objects from real-world interactions.In our work, we utilize the fast state-of-the-art BundleSDF method [63] to generate object meshes that are transferred to the simulation.</p>
<p>Sim-to-real transfer has shown great performance in a variety of skills, including tabletop manipulation [64,65], mobile manipulation [66,67], dynamic manipulation [68], dexterous manipulation [69][70][71][72], and locomotion [73,74].However, directly deploying learned policies to physical robots cannot guarantee successful performance due to the sim-to-real gap.To bridge this gap, researchers have developed many techniques, such as system identification [75][76][77], domain adaptation [78][79][80][81][82], and domain randomization [73,83,84,[84][85][86][87][88].In our work, we use domain randomization as it does not require any interaction data from the real world during training.It relies entirely on simulation and makes policies robust by exposing them to a wide variety of randomized conditions.Recently, Torne et al. [89] proposed RialTo, a complete real-to-sim-to-real loop system that focuses on leveraging simulation to robustify imitation learning policies trained using real-world collected demonstrations.In contrast, we focus on executing long-horizon tasks by training only in simulation, bypassing the need for demonstrations.</p>
<p>III. METHOD</p>
<p>In this section we formally define Iterative Keypoint Reward (IKER) and discuss how it is automatically synthesized and refined by VLMs by continuously taking in environmental feedback.Then, we discuss our overall framework, which uses IKER in a real-to-sim-to-real loop.Our method overview is illustrated in Figure 2, with detailed steps provided in Algorithm 1.</p>
<p>Keypoints &amp; execution history</p>
<p>VLM Generated Reward Function</p>
<p>Fig. 3: Iterative Keypoint Reward Generation.This corresponds to the first step in Figure 2. We first obtain keypoints in the scene.These keypoints, combined with a human command and execution history, are processed by a VLM to generate code that maps keypoints to the reward function.A more detailed illustration of the keypoints and generated code is provided in Figure 7.</p>
<p>A. Iterative Keypoint Reward (IKER)</p>
<p>Given an RGB-D observation of the environment and an open-vocabulary language instruction I for a multi-step task, our goal is to obtain a sequence of policies, π N i=1 , that complete the task.Crucially, the number of policies N is not predefined, allowing for flexibility in how the robot approaches the task.For example, in the scenario of Fig. 3, the first policy, π 1 , moves the shoe box to create space, while subsequent policies handle the placement of each shoe.</p>
<p>For each step i, we denote the RGB observation as O i .We assume a set of K i keypoints {k (i) j } Ki j=1 is given (discussed later in Sec.III-B), each specifying a 3D position in the task space.Using these keypoints, our objective is to automatically generate a reward function, termed IKER, that maps the keypoint positions to a scalar reward f (i) : R Ki×3 → R.</p>
<p>To generate the reward function f (i) , we use a VLM (GPT-4o [1] in our case), which is provided with the context comprising (1) the human instruction I describing the task, (2) the current RGB observation O i with keypoints overlaid with numerical markers, and (3) the sequence of previous observations and reward functions up to step i − 1, i.e. {O 1 , f (1) , . . ., O i−1 , f (i−1) }.</p>
<p>Additionally, the VLM is guided by a prompt that instructs it to generate a Python function for the reward f (i) .The prompt directs the VLM to break down the task into executable steps, predict which object to interact with, specify the movement of objects by indicating where their keypoints should be placed relative to other keypoints, and perform arithmetic calculations on these keypoints to predict their final locations.We do not explicitly specify which keypoint belongs to which object, allowing the VLM to infer this information.The prompt also instructs the VLM to present all outputs in a prescribed code format and set the flag done = True if the task is completed.By predicting code, the VLM can perform arbitrary and precise calculations using the current keypoint locations, which would not be possible if limited to raw text.Please refer to Appendix C for the complete prompt, and Figure 7 for a step-by-step walkthrough of RGB observations and generated reward functions for the example of Figure 3.</p>
<p>Upon receiving the final keypoint locations by executing the generated code, we compute a scalar reward to evaluate the policy's performance.The reward function, f (i) , facilitates learning by combining the following terms:</p>
<p>• Gripper-to-object Distance Reward (r dist ): Encourages the robot to approach the object of interest by penalizing large distances between them.• Direction Reward (r dir ): Guides the robot to move the keypoints in the direction of the target locations.• Alignment Reward (r align ): Drives the robot to position the keypoints close to their target locations.• Success Bonus (r bonus ): Provides an additional reward when the average distance between the keypoints and their target positions remains within a specified threshold for a certain number of timesteps, indicating successful task completion.• Penalty Term (r penalty ): Applies penalties for undesirable actions such as excessive movements, dropping the object, or applying excessive force.
f (i) = α dist r dist + α dir r dir + α align r align + α bonus r bonus + α penalty r penalty</p>
<p>B. Transferring real-world scene to simulation</p>
<p>To transfer the real-world scene within the workspace boundary to simulation, we first generate 3D meshes of manipulable objects, such as the shoe box and shoes shown in Figure 3, by capturing video footage of each object as it is moved to ensure the camera captures all sides.These videos allow for accurate 3D mesh reconstruction using BundleSDF [63], and multiple objects can be processed in parallel to speed up the scanning phase.Once a mesh is created for an object, it can be reused in different settings, eliminating the need to recreate it for each new scenario.With the meshes prepared, we use FoundationPose [90] to estimate the objects' poses, enabling precise placement of the corresponding meshes in the simulated environment.For static elements, like the workspace table and shoe rack in Figure 3, we capture a point cloud to create their meshes for use in the simulation.</p>
<p>The generated meshes are further used to identify candidate keypoints.For manipulable objects like shoes or books, keypoints are placed at the object's extremities along its axes, defined with respect to the object's center, independent of the human instruction.For static objects like shoe racks, which are part of the environment, keypoints are uniformly distributed across their surfaces.Numerical labels assigned to keypoints are grouped by objects.For example, as shown in Figure 7, keypoints 1-4 correspond to the box, 5-8 to the left shoe, 9-12 to the right shoe, and the remaining keypoints to the rack.Keypoints that are too close together in the image projection are removed.Specifically, background keypoints (e.g., like rack) near object keypoints are removed first.Among overlapping object keypoints, only the one with the lower numerical label is retained.Note, however, that the VLM is not explicitly told this information but has to infer the association between the keypoints and the objects based on the input image.</p>
<p>C. Policy Training in Simulation</p>
<p>We control the robot in the end-effector space, which has six degrees of freedom: three prismatic joints for movement along the x, y, and z axes, and three revolute joints for rotation.The gripper fingers remain closed by default, opening only when grasping objects.Refer to Appendix A for a detailed discussion on grasping.</p>
<p>State Space: The state space for our policy captures the essential information to execute the task.The input is a vector s t consisting of the gripper's end-effector pose (p e , q e ) ∈ R 7 , the pose of object currently being manipulated (p o , q o ) ∈ R 7 , a set of object keypoints
K o = {k (i) j } Ki j=1 ∈ R Ki×3
, and their corresponding target positions
K t = {k (i) tj } Ki j=1 ∈ R Ki×3 .
K o is calculated by applying rigid body transformations to keypoints defined in the object's local coordinate frame, mapping them to their corresponding positions in the world frame.K t is derived from the reward function f (i) generated by the VLM.Rotations q e and q o are represented as quaternions.This state space s t = (p e , q e , p o , q o , K o , K t ) captures essential information on objects of interest as well as the goal of the policy.Instead of incorporating raw RGBD data directly into the state space, object poses and keypoints are extracted from RGBD inputs using a vision-based pose estimation method, as detailed in Section III-D.This preprocessing step removes the necessity of including raw RGBD data in the policy.</p>
<p>Action Space: The action space is defined relative to the gripper's current position and orientation.The policy outputs actions a t = (∆p e , ∆r e ), where ∆p e ∈ R 3 and ∆r e ∈ R 3 specifies the changes in translation and rotation respectively.</p>
<p>Training Algorithm &amp; Architecture: We train our policies using IsaacGym [91] simulator with the PPO [92] algorithm.We use an actor-critic architecture [93] with a shared backbone.The network is a multi-layer perceptron (MLP) consisting of hidden layers with 256, 128, and 64 units, each followed by ELU [94] activation.Currently, it takes about 5 minutes to train per task, which can be prohibitive for certain applications.However, this training time can be reduced by increasing the number of parallel environments and utilizing more powerful GPUs.</p>
<p>Domain Randomization (DR): Recognizing the challenges inherent in transferring policies between the simulation and the real world, we employ DR to bridge the real-tosim-to-real gaps.DR is applied to object properties like friction, mass, restitution, compliance, and geometry.We further randomize the object position, the gripper location, and the grasp within a range.We found these to be especially crucial for non-prehensile tasks like pushing.The specific parameter ranges are detailed in Appendix B, and the effectiveness of DR is evaluated in Section IV-E.</p>
<p>D. Deployment of Trained Policy</p>
<p>The trained RL policy π i is deployed directly in the real world.Since the policy outputs the end-effector pose, we employ inverse kinematics to compute the joint angles at each timestep.The RL policy operates at 10Hz, producing action commands that are then clipped to ensure the end effector remains within the workspace limits.For keypoint tracking, we utilize FoundationPose [90] to estimate the object's pose.These pose estimates are subsequently used to compute the keypoint locations that are defined relative to the objects.When VLM predicts to grasp objects, we use AnyGrasp [95] to detect grasps in the real-world.</p>
<p>IV. EXPERIMENTS AND ANALYSIS</p>
<p>We aim to investigate whether Iterative Keypoint Reward can effectively represent reward functions for diverse manipulation skills within our IKER for real-to-sim-to-real pipeline.We also want to see whether our pipeline can perform multi-step tasks in dynamic environments by leveraging Iterative Keypoint Reward as feedback for replanning.</p>
<p>A. Experimental Setup, Metrics and Baselines</p>
<p>Fig. 4: Setup and experiment objects.We use XArm7 to conduct all our experiments.Our setup includes 4 stationary and 1 wristmounted camera.We experiment with 5 shoe pairs and 2 shoe racks for tasks involving shoe scenarios.Additionally, we experiment with 9 different books for stowing tasks.</p>
<p>We conduct experiments on XArm7 with four stationary RealSense cameras.Figure 4 shows the setup, along with  the objects used.These cameras capture the point clouds, which are used to construct the simulation environment and to provide data for AnyGrasp to predict grasp.Additionally, a wrist-mounted camera is used to capture images that are used to query the VLM.</p>
<p>As a baseline, we use an annotated variant of IKER with human-labeled reward functions, allowing evaluation without VLM influence.We also compare our keypointbased method with another baseline that uses object pose to construct reward function, which is more conventional in RL training [71,[96][97][98][99].In this pose-based method, the VLM generates a function f that maps the initial object poses (represented by xyz coordinates for position and RPY angles for orientation) to their final poses.The prompt for this baseline is discussed in Sec. C.</p>
<p>We evaluate our approach across four scenarios, illustrated in Figure 1 (left): Shoe Place, Shoe Push, Book Push, and Book Reorient.In Shoe Place, the robot picks up a shoe from the ground and places it on a rack.In Shoe Push, it pushes a shoe towards other shoe to form a matching pair.In Book Push, it pushes a book to align with other book, or push the book towards table edge, and in Book Reorient, it repositions a book on a shelf.Each scenario has 10 start/end configurations.In simulation, success rates are averaged over 128 randomized environments generated for 10 start/end configuration, making a total of 1280 trials per scenario.In the real-world, success is evaluated directly on the 10 start/end configurations.A trial is considered successful in both cases if the average keypoint distance to the target is within 5 cm.</p>
<p>B. Policy Training with IKER for Single-Step Tasks</p>
<p>We conduct experiments comparing RL training with keypoints and object pose in reward functions.Our experiments span four representative tasks, and are summarized in Table I.</p>
<p>In the annotated method, success rates for shoe placement using IKER and object pose are 0.945 and 0.938, respectively.A similar trend is observed in the shoe push, stowing push, and reorient tasks, where performance differences are minimal.These results demonstrate that, when targets are specified through human annotations, both keypoints and object poses effectively capture the target locations and serve as viable approaches for RL policy training.</p>
<p>In the automatic method, IKER significantly outperforms object pose representations.For example, in shoe placement, IKER achieves a 0.7 success rate, while object poses reach only 0.3.Similar results are seen across other tasks.Object pose success is limited to simpler scenarios with no orientation changes, as VLMs struggle with rotations in SO(3) space.In contrast, keypoints simplify the challenge by requiring VLMs to reason only in Cartesian space, eliminating the need to handle object poses in SE(3) space.</p>
<p>As shown in Table I, there is a slight reduction in success rate from simulation to the real world.For shoe placement, IKER achieves success of 0.945 in simulation and 0.8 in the real world.For shoe push, the success rate drops from 0.871 to 0.850.These results suggest that domain randomization described in Section III-C helps the model generalize to realworld conditions, but factors like inaccuracies in environment reconstruction, real-world perception errors, and the inability to simulate extreme object dynamics still affect performance.</p>
<p>Most of the failures in our framework stem from discrepancies between the heuristic grasps used in simulation and the grasps generated by AnyGrasp in the real world, as well as incorrect VLM predictions.For incorrect VLM predictions, the model sometimes selects the wrong keypoints or fails to use all available keypoints on an object when determining its relationship to another object.For instance, if an object has four keypoints, the VLM may only use one of them, leading to suboptimal alignment and placement.These issues can be mitigated by providing more in-context examples while querying the VLMs.These challenges may become less pronounced with the incorporation of advancements such as [100], which enhance the spatial reasoning capabilities of VLMs.Additionally, some failures are caused by physical dynamics when pushing objects.These issues can be partially mitigated by explicitly estimating dynamic parameters during real-to-sim transfer.</p>
<p>C. Iterative Replanning for Multi-Step Tasks</p>
<p>We demonstrate the robot's iterative chaining ability with a task of three sequential actions: first pushing a shoe box to create space, then placing a pair of shoes on a rack.Failure in one task leads to failure in the next.We evaluate this process using 10 different start and end configurations, iterating through each to assess overall performance.</p>
<p>We compare our method with VoxPoser [12], which employs LLMs to generate code that produces potential fields for motion planning.VoxPoser serves as an ideal baseline because it synthesizes motion plans for diverse manipulation tasks from free-form language instructions.Notably, VoxPoser plans are open-loop and lack feedback to refine specifications at each step.To adapt it to our tasks, we enhanced VoxPoser with two major modifications: (1) Vox-  Our proposed framework consistently demonstrates superior performance compared to VoxPoser at every step of the task sequence.</p>
<p>Poser used OWL-ViT [101] to find object bounding boxes, but it struggled to distinguish between left and right shoes, so we provided ground-truth object locations.(2) We gave VoxPoser the entire plan, as the original planner struggled with multi-step tasks.This gave VoxPoser an advantage over our method due to access to privileged information.</p>
<p>Figure 6 shows the iterative chaining results.Across the three tasks, our method consistently outperformed VoxPoser.In the first task, we succeeded 8 out of 10 times compared to VoxPoser's 5 successes.For the second task, we had 5 successes while VoxPoser had 1.In the final task, our method succeeded 4 times, whereas VoxPoser failed in all attempts.VoxPoser's failures can be attributed to several factors, such as pushing the shoe box either too far or not far enough.Additionally, its grasping strategy relies on a simple heuristic that positions the robot's end effector around the object center before closing the gripper, often resulting in failed grasps.It also struggles with collisions during object manipulation, as it does not account for the environment to avoid obstacles.Furthermore, improper placement of shoes-such as stacking both shoes on top of each other, causing them to fall-further highlights its limitations.</p>
<p>D. Robustness, Adjusting Plans, and Re-Planning</p>
<p>Unlike previous works that rely on open-loop plans, our approach leverages closed-loop plans, enabling adjustments during execution.This feature gives rise to several capabilities, as demonstrated in Figure 5.</p>
<p>In the first scenario, a human interrupts the robot while it is in the process of placing shoes on the ground.The framework demonstrates resilience by recovering from the interruption.The robot re-grasps the shoe and successfully completes the task by placing both shoes on the rack.</p>
<p>In the second scenario, when the robot attempts to place the left shoe, it detects that the shoe is not positioned close enough to the right shoe.To address this, the VLM predicts a corrective action, suggesting that the robot push the left shoe closer to the right shoe to form a proper pair.</p>
<p>In the third scenario, the robot is tasked with stowing a book on a shelf.However, the initial grasp attempt fails because the book is too large to be grasped.In response, the VLM predicts an alternative strategy to complete the task, adjusting the approach to ensure successful placement.We present the results of our framework with and without DR for shoe place and push.The performance is averaged over 10 runs.In the simulation, the performance without DR is 0.964, while with DR, it is slightly lower at 0.945, suggesting that without DR, the policy performs better in a single, controlled setting.However, in the real world, the performance without DR drops to 0.6, whereas with DR, it is 0.8, highlighting the effectiveness of DR.For the place task without DR, we observe that the policy is less robust to simto-real gap, frequently colliding with the shoe rack during transport.Additionally, immediately after picking up the object, the policy sometimes fails, likely due to discrepancies in the pose estimation.</p>
<p>E. Effect of Domain Randomization</p>
<p>For push, these issues are more pronounced: success is 0.2 without DR but improves to 0.7 with DR.Without DR, the policy often crushes the shoe or causes it to slip out of alignment during pushing.These findings demonstrate the importance of DR for reliable real-world performance.</p>
<p>V. CONCLUSION AND LIMITATIONS</p>
<p>In this work, we introduced Iterative Keypoint Reward (IKER), a framework that leverages VLMs to generate visually grounded reward functions for robotic manipulation in open-world environments.By using keypoints from RGB-D observations, our approach enables precise SE(3) control and integrates priors from VLMs without relying on rigid instructions.IKER bridges simulation and real-world execution through a real-to-sim-to-real loop, training policies in simulation and deploying them in physical environments.Experiments across diverse tasks demonstrate the framework's ability to handle complex, long-horizon challenges with adaptive strategies and error recovery.This work represents a step toward more intelligent and flexible robots capable of operating effectively in dynamic, real-world settings.</p>
<p>Despite these advancements, our approach has certain limitations.We need to capture objects from all views to obtain object meshes.In the future, this may be simplified by using methods [52] that can generate meshes from a single image.Additionally, our real-to-sim transfer does not account for dynamics parameters, which could be modeled more accurately through system identification techniques.Also, while our framework reconstructs multiple objects in the environment, we do not account for tasks involving complicated multi-object interactions, limiting our evaluation primarily to single-object manipulation at each stage.</p>
<p>APPENDIX</p>
<p>A. Grasping Subroutine</p>
<p>During training, the gripper fingers open only in the grasp mode, where the end-effector approaches the object with open fingers and then closes them to grasp the object.We employ a heuristic-based grasp for faster training.In realworld, the gripper fingers remain closed until the grasp mode is triggered.AnyGrasp predicts an appropriate grasp pose and the fingers close at the predicted position.To address the simto-real gap, we add randomization to the heuristic grasp pose during simulation.This allows the policy to generalize more effectively, resulting in more robust and reliable policies in the real-world.</p>
<p>B. Domain Randomization Parameters</p>
<p>To enhance the robustness of our policies for effective real-to-sim-to-real transfer, we apply domain randomization to various object properties and initial conditions.Table III details the key randomized parameters and their respective ranges.These variations ensure that our learned policies generalize effectively to real-world conditions, mitigating the discrepancies between simulation and real-world.</p>
<p>Parameter Range</p>
<p>Object Scale [0.8,</p>
<p>C. VLM Prompts</p>
<p>The VLM receives the image overlaid with keypoints 1, . . ., K, along with the task description as text.These are given to the VLM, along with the prompt.We do not provide any in-context examples with the prompt.Our prompt for single-step tasks is as follows:</p>
<h2>Instructions Your job is to help with moving rigid objects in realworld by writing code in python.The task is given as an image of the environment, overlayed with keypoints marked with their indices, along with a text instruction.These keypoints are in 3D space, and are projected onto the 2D image.They are attached with the objects, and move along with them.So to determine where a specific point should go, you should specify where its corresponding keypoint should go.The coordinate system is marked at the bottom right in the image, with a vertical arrow pointing forward in the positive x direction and the horizontal arrow pointing to the left in the positive y direction.The code should predict the final keypoint locations relative to their matching keypoints.Use all matching keypoints to determine the final position of the moving object, not just a single reference point.Note: -You should determine if you need to grasp or push the object.You should output a boolean grasp_mode for that.</h2>
<p>-Some objects should not be moved.Hence, the final location of key points on them should be the same as the initial locations.</p>
<p>-If you need to interact with an object, the final location of only the keypoints marked on it should change.Hence, you should first try to understand which object should move.</p>
<p>-You should try to understand where the moving object should go relative to other stationary objects and use all the matching keypoints for alignment.Then you can give the final locations of keypoints of moving objects relative to keypoints on stationary objects.</p>
<p>-Positive x direction points towards up and positive y direction points towards left.</p>
<p>-The input to the function is a dictionary of keypoint coordinates.So keys will be strings like "1", "2", ... and their values will be numpy arrays ([x, y,  The prompt for multi-step tasks is as follows:</p>
<h2>Instructions Your job is to help with moving rigid objects in realworld by writing code in python.The task is given as an image of the environment, overlayed with keypoints marked with their indices, along with a text instruction.These keypoints are in 3D space, and are projected onto the 2D image.They are attached with the objects, and move along with them.So to determine where a specific point should go, you should specify where its corresponding keypoint should go.The coordinate system is marked at the bottom right in the image, with a vertical arrow pointing forward in the positive x direction and the horizontal arrow pointing to the left in the positive y direction.The code should predict the final keypoint locations relative to their matching keypoints.Use all matching keypoints to determine the final position of the moving object, not just a single reference point.Note:</h2>
<p>-You should determine if you need to grasp or push the object.You should output a boolean grasp_mode for that.Generally, big objects can only be pushed as they are too big to be grasped.</p>
<p>-Some objects should not be moved.Hence, the final location of key points on them should be the same as the initial locations.The units here are in meters and left direction corresponds to + y-axis.-Make use of semantics.Some objects should be placed in a certain way, like a left shoe should be placed on the left of the right shoe.</p>
<p>-Some tasks involve multiple stages, so you will also predict the overall plan.Then you will write the description and code for the current stage.You will interact with only one object in a stage.Placing or pushing a single object will be considered a single stage.Grasping is not considered as a separate stage.</p>
<p>-You are free to make minor changes to the plan, or change the plan altogether if you think is necessary.-We will keep adding the previous states of the environment as images and the corresponding code to the description.This will show how the task progressed.At the start, you will only see the task description.</p>
<p>-You should predict done=True when the task is complete, otherwise False.Only predict done=True when you see that the task is completed.</p>
<p>Structure your output in a single python code block as The prompt for baseline that uses pose input for singlestep tasks is as follows: The robot uses the environment to regrasp and stow the book.Then, the human updates the instructions to place it on the other shelf.</p>
<p>We present results on a complex 3D understanding task.The task involves stowing a book on a shelf, where the book is initially positioned with only its shorter edge graspable.The instruction is to place the book on the shelf.However, the robot cannot place the book directly with the shorter edge grasped, as this would result in a collision between the book and the table due to the position of its arm.To complete this task, the robot must perform multiple steps: first, it needs to regrasp the book along its longer edge using some part of the environment, and only then can it stow the book on the shelf.After the robot places the book on the initial shelf, a human intervenes by adding an instruction to move the book to a different shelf.</p>
<p>Given the complexity of this long-horizon task, we employ in-context examples to guide the VLM.With this change, our system is able to successfully perform the task.Figure 8 illustrates the progression of the task.</p>
<p>Fig. 1 :
1
Fig. 1: Capabilities of Our Framework.IKER is designed to handle a wide range of real-world tasks.It can be seamlessly chained to execute multi-step tasks.It exhibits robustness to disturbances and demonstrates the ability to solve problems flexibly.</p>
<p>Fig. 2 :
2
Fig. 2: Framework Overview.Iterative Keypoint Reward (IKER) is a visually grounded reward generated by Vision-Language Models (VLMs) as task specification.The framework reconstructs the real-world scene in simulation, and the generated reward is used to train RL policies, which are subsequently deployed in the real-world.</p>
<p>Fig. 5 :
5
Fig. 5: Scenarios demonstrating capabilities of our framework.The framework is robust to disturbances and can adapt in response to unexpected events.Additionally, it can propose new plans when the original ones become infeasible.</p>
<p>Fig. 6 :
6
Fig. 6: Multi-Step Task Chaining Comparison with VoxPoser.</p>
<p>Fig. 7 :
7
Fig. 7: Examples of keypoint-marked images with corresponding predicted codes.The top row represents the starting point, with subsequent rows illustrating the progression step by step.The VLM first predicts to push the box to create space, followed by sequential placement of the shoes.</p>
<p>keypoint calculation for each keypoint in keypoint_indices_to_interact keypoint_coordinates['keypoint_indices_to_interact [0]'] = ?# Write calculation here.You may use multiple lines # Repeat for other keypoints return object_to_interact, keypoint_indices_to_interact, grasp_mode, keypoint_coordinates ## Query Query Task: '[TASK]' Query Image: [IMAGE_WITH_KEYPOINTS]</p>
<p>Fig. 8 :
8
Fig. 8: Case study of a complex task with in-context examples.</p>
<p>Diverse Task Specification Multi-Step With Environment Feedback
Shoe PlaceBook PlaceMulti-Step Skill ChainingShoe PushBook PushDisturbance RecoveryShoe ReorientBook ReorientRegraspColumbia University</p>
<p>TABLE I :
I
Performance of IKER in simulation and real-world.IKER, which makes use of visual keypoints, significantly outperforms the conventional pose-based approach, especially when using VLMs to automatically generate reward functions.</p>
<p>TABLE II :
II
Performance</p>
<p>of Shoe Place and Push with and without Domain Randomization (DR).DR slightly reduces simulation performance but significantly improves real-world task performance across different scenarios.</p>
<p>TABLE III :
III
Domain randomization ranges for key object properties and initial conditions in simulation.</p>
<p>VI. ACKNOWLEDGEMENTSWe thank Aditya Prakash, Arjun Gupta, Binghao Huang, Hanxiao Jiang, Kaifeng Zhang, and Unnat Jain for fruitful discussions.This work is partially supported by the Amazon AICE Award, the Sony Group Corporation, and the DARPA TIAMAT program (HR0011-24-9-0430).This work does not relate to the positions of Shubham Garg and Hooshang Nayyeri at Amazon.This article solely reflects the opinions and conclusions of its authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors.VLMdef interaction_data(keypoint_coordinates):""" Previous Plan Description: None""" """ Current Plan Description: The task is to place the shoes onto the rack.This will involve moving the shoes to the surface where the box is located.Since there is not enough space for both the shoes, an appropriate plan is to first make space by pushing the box.Then the shoes should be placed on the rack."""""" Current stage description: Push the box (keypoints 1, 2, 3, 4) towards left.
Openai, Gpt-4 technical report. 2023arXiv</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. A Zeng, A Wong, S Welker, K Choromanski, F Tombari, A Purohit, M Ryoo, V Sindhwani, J Lee, V Vanhoucke, arXiv:2204.005982022arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, 2021</p>
<p>Scaling up visual and vision-language representation learning with noisy text supervision. C Jia, Y Yang, Y Xia, Y.-T Chen, Z Parekh, H Pham, Q V Le, Y Sung, Z Li, T Duerig, 2021</p>
<p>Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. J Li, D Li, C Xiong, S Hoi, International conference on machine learning. PMLR202212900</p>
<p>Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, 2023</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, R Ring, E Rutherford, S Cabi, T Han, Z Gong, S Samangooei, M Monteiro, J Menick, S Borgeaud, A Brock, A Nematzadeh, S Sharifzadeh, M Binkowski, R Barreira, O Vinyals, A Zisserman, K Simonyan, 2022</p>
<p>Coca: Contrastive captioners are image-text foundation models. J Yu, Z Wang, V Vasudevan, L Yeung, M Seyedhosseini, Y Wu, 2022</p>
<p>W Huang, C Wang, Y Li, R Zhang, L Fei-Fei, arXiv:2409.01652Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation. 2024arXiv preprint</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, arXiv:2209.077532022in arXiv preprint</p>
<p>Voxposer: Composable 3d value maps for robotic manipulation with language models. W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, arXiv:2307.059732023arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Visionlanguage-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. F Liu, K Fang, P Abbeel, S Levine, arXiv:2403.031742024arXiv preprint</p>
<p>Copa: General robotic manipulation through spatial constraints of parts with foundation models. H Huang, F Lin, Y Hu, S Wang, Y Gao, arXiv:2403.082482024arXiv preprint</p>
<p>Octo: An open-source generalist robot policy. O M Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, T Kreiman, C Xu, arXiv:2405.122132024arXiv preprint</p>
<p>In-struct2act: Mapping multi-modality instructions to robotic actions with large language model. S Huang, Z Jiang, H Dong, Y Qiao, P Gao, H Li, arXiv:2305.111762023arXiv preprint</p>
<p>Creative robot tool use with large language models. M Xu, P Huang, W Yu, S Liu, X Zhang, Y Niu, T Zhang, F Xia, J Tan, D Zhao, arXiv:2310.130652023arXiv preprint</p>
<p>Generalizable long-horizon manipulations with large language models. H Zhou, M Ding, W Peng, M Tomizuka, L Shao, C Gan, arXiv:2310.022642023arXiv preprint</p>
<p>Pivot: Iterative visual prompting elicits actionable knowledge for vlms. S Nasiriany, F Xia, W Yu, T Xiao, J Liang, I Dasgupta, A Xie, D Driess, A Wahid, Z Xu, arXiv:2402.078722024arXiv preprint</p>
<p>Keypoint action tokens enable in-context imitation learning in robotics. N , Di Palo, E Johns, arXiv:2403.195782024arXiv preprint</p>
<p>Large language models for robotics: A survey. F Zeng, W Gan, Y Wang, N Liu, P S Yu, arXiv:2311.072262023arXiv preprint</p>
<p>Distilling and retrieving generalizable knowledge for robot manipulation via language corrections. L Zha, Y Cui, L.-H Lin, M Kwon, M G Arenas, A Zeng, F Xia, D Sadigh, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024179</p>
<p>How to prompt your robot: A promptbook for manipulation skills with code as policies. M G Arenas, T Xiao, S Singh, V Jain, A Ren, Q Vuong, J Varley, A Herzog, I Leal, S Kirmani, 2024 IEEE International Conference on Robotics and Automation (ICRA). </p>
<p>Generative expressive robot behaviors using large language models. K Mahadevan, J Chien, N Brown, Z Xu, C Parada, F Xia, A Zeng, L Takayama, D Sadigh, Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction. the 2024 ACM/IEEE International Conference on Human-Robot Interaction2024</p>
<p>Learning to learn faster from human feedback with language model predictive control. J Liang, F Xia, W Yu, A Zeng, M G Arenas, M Attarian, M Bauza, M Bennice, A Bewley, A Dostmohamed, arXiv:2402.114502024arXiv preprint</p>
<p>Grounded decoding: Guiding text generation with grounded models for embodied agents. W Huang, F Xia, D Shah, D Driess, A Zeng, Y Lu, P Florence, I Mordatch, S Levine, K Hausman, Advances in Neural Information Processing Systems. 202436</p>
<p>A Z Ren, A Dixit, A Bodrova, S Singh, S Tu, N Brown, P Xu, L Takayama, F Xia, J Varley, arXiv:2307.01928Robots that ask for help: Uncertainty alignment for large language model planners. 2023arXiv preprint</p>
<p>Vima: General robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, arXiv:2210.03094202226arXiv preprint</p>
<p>Guiding long-horizon task and motion planning with vision language models. Z Yang, C Garrett, D Fox, T Lozano-Pérez, L P Kaelbling, arXiv:2410.021932024arXiv preprint</p>
<p>Aha: A visionlanguage-model for detecting and reasoning over failures in robotic manipulation. J Duan, W Pumacay, N Kumar, Y R Wang, S Tian, W Yuan, R Krishna, D Fox, A Mandlekar, Y Guo, arXiv:2410.003712024arXiv preprint</p>
<p>Manipulate-anything: Automating real-world robots using vision-language models. J Duan, W Yuan, W Pumacay, Y R Wang, K Ehsani, D Fox, R Krishna, arXiv:2406.189152024arXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. W Yuan, J Duan, V Blukis, W Pumacay, R Krishna, A Murali, A Mousavian, D Fox, arXiv:2406.107212024arXiv preprint</p>
<p>Progprompt: program generation for situated robot task planning using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, Autonomous Robots. 4782023</p>
<p>Kalie: Fine-tuning vision-language models for open-world manipulation without robot data. G Tang, S Rajkumar, Y Zhou, H R Walke, S Levine, K Fang, arXiv:2409.140662024arXiv preprint</p>
<p>Eurekaverse: Environment curriculum generation via large language models. W Liang, S Wang, H.-J Wang, O Bastani, D Jayaraman, Y J Ma, arXiv:2411.017752024arXiv preprint</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, W Chen, K Pertsch, O Mees, C Finn, S Levine, arXiv:2407.086932024arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. A O'neill, A Rehman, A Gupta, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, arXiv:2310.088642023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>Rt-h: Action hierarchies using language. S Belkhale, T Ding, T Xiao, P Sermanet, Q Vuong, J Tompson, Y Chebotar, D Dwibedi, D Sadigh, 2024</p>
<p>Vlmpc: Vision-language model predictive control for robotic manipulation. W Zhao, J Chen, Z Meng, D Mao, R Song, W Zhang, Robotics: Science and Systems. 2024</p>
<p>Language to rewards for robotic skill synthesis. W Yu, N Gileadi, C Fu, S Kirmani, K.-H Lee, M G Arenas, H.-T L Chiang, T Erez, L Hasenclever, J Humplik, arXiv:2306.086472023arXiv preprint</p>
<p>Eureka: Human-level reward design via coding large language models. Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, arXiv: Arxiv-2310.129312023arXiv preprint</p>
<p>Dreureka: Language model guided sim-to-real transfer. Y J Ma, W Liang, H.-J Wang, S Wang, Y Zhu, L Fan, O Bastani, D Jayaraman, 2024</p>
<p>Text2reward: Automated dense reward function generation for reinforcement learning. T Xie, S Zhao, C H Wu, Y Liu, Q Luo, V Zhong, Y Yang, T Yu, arXiv:2309.114892023arXiv preprint</p>
<p>Realtime perception meets reactive motion generation. D Kappler, F Meier, J Issac, J Mainprice, C G Cifuentes, M Wüthrich, V Berenz, S Schaal, N Ratliff, J Bohg, IEEE Robotics and Automation Letters. 332018</p>
<p>You only demonstrate once: Category-level manipulation from single visual demonstration. B Wen, W Lian, K Bekris, S Schaal, arXiv:2201.127162022arXiv preprint</p>
<p>One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. M Liu, R Shi, L Chen, Z Zhang, C Xu, X Wei, H Chen, C Zeng, J Gu, H Su, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20241083</p>
<p>Sparp: Fast 3d object reconstruction and pose estimation from sparse views. C Xu, A Li, L Chen, Y Liu, R Shi, H Su, M Liu, European Conference on Computer Vision. Springer2025</p>
<p>Zero123++: a single image to consistent multi-view diffusion base model. R Shi, H Chen, Z Zhang, M Liu, C Xu, X Wei, L Chen, C Zeng, H Su, arXiv:2310.151102023arXiv preprint</p>
<p>Zero-1-to-3: Zero-shot one image to 3d object. R Liu, R Wu, B Van Hoorick, P Tokmakov, S Zakharov, C Vondrick, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Get3d: A generative model of high quality 3d textured shapes learned from images. J Gao, T Shen, Z Wang, W Chen, K Yin, D Li, O Litany, Z Gojcic, S Fidler, Advances In Neural Information Processing Systems. 352022</p>
<p>A-sdf: Learning disentangled signed distance functions for articulated shape representation. J Mu, W Qiu, A Kortylewski, A Yuille, N Vasconcelos, X Wang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision20211311</p>
<p>Ditto: Building digital twins of articulated objects from interaction. Z Jiang, C.-C Hsu, Y Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Structure from action: Learning interactions for articulated object 3d structure discovery. N Nie, S Y Gadre, K Ehsani, S Song, arXiv:2207.089972022arXiv preprint</p>
<p>Urdformer: A pipeline for constructing articulated simulation environments from real-world images. Z Chen, A Walsman, M Memmel, K Mo, A Fang, K Vemuri, A Wu, D Fox, A Gupta, arXiv:2405.116562024arXiv preprint</p>
<p>Real2code: Reconstruct articulated objects via code generation. Z Mandi, Y Weng, D Bauer, S Song, arXiv:2406.084742024arXiv preprint</p>
<p>Paris: Part-level reconstruction and motion analysis for articulated objects. J Liu, A Mahdavi-Amiri, M Savva, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Cage: Controllable articulation generation. J Liu, H I I Tam, A Mahdavi-Amiri, M Savva, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202417889</p>
<p>Singapo: Single image controlled generation of articulated parts in object. J Liu, D Iliash, A X Chang, M Savva, A Mahdavi-Amiri, arXiv:2410.164992024arXiv preprint</p>
<p>Occlusion-aware reconstruction and manipulation of 3d articulated objects. X Huang, I Walker, S Birchfield, 2012 IEEE international conference on robotics and automation. IEEE2012</p>
<p>Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects. B Wen, J Tremblay, V Blukis, S Tyree, T Muller, A Evans, D Fox, J Kautz, S Birchfield, 2023CVPR</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, arXiv: Arxiv- 2109.120982021arXiv preprint</p>
<p>Transic: Sim-to-real policy transfer by learning from online correction. Y Jiang, C Wang, R Zhang, J Wu, L Fei-Fei, 2024</p>
<p>Multi-skill mobile manipulation for object rearrangement. J Gu, D S Chaplot, H Su, J Malik, arXiv: Arxiv- 2209.027782022arXiv preprint</p>
<p>Homerobot: Open-vocabulary mobile manipulation. S Yenamandra, A Ramachandran, K Yadav, A Wang, M Khanna, T Gervet, T.-Y Yang, V Jain, A W Clegg, J Turner, Z Kira, M Savva, A Chang, D S Chaplot, D Batra, R Mottaghi, Y Bisk, C Paxton, arXiv: Arxiv-2306.115652023arXiv preprint</p>
<p>Dynamic handover: Throw and catch with bimanual hands. B Huang, Y Chen, T Wang, Y Qin, Y Yang, N Atanasov, X Wang, arXiv:2309.056552023arXiv preprint</p>
<p>Sequential dexterity: Chaining dexterous policies for long-horizon manipulation. Y Chen, C Wang, L Fei-Fei, C K Liu, arXiv:2309.009872023arXiv preprint</p>
<p>Dexpoint: Generalizable point cloud reinforcement learning for sim-toreal dexterous manipulation. Y Qin, B Huang, Z.-H Yin, H Su, X Wang, 2022</p>
<p>In-hand object rotation via rapid motor adaptation. H Qi, A Kumar, R Calandra, Y Ma, J Malik, Conference on Robot Learning. PMLR2023</p>
<p>Rotating without seeing: Towards in-hand dexterity through touch. Z.-H Yin, B Huang, Y Qin, Q Chen, X Wang, arXiv:2303.108802023arXiv preprint</p>
<p>RMA: rapid motor adaptation for legged robots. A Kumar, Z Fu, D Pathak, J Malik, 10.15607/RSS.2021.XVII.011Robotics: Science and Systems XVII, Virtual Event. D A Shell, M Toussaint, M A Hsieh, July 12-16, 2021. 2021</p>
<p>Agile but safe: Learning collision-free high-speed legged locomotion. T He, C Zhang, W Xiao, G He, C Liu, G Shi, 2024</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv: Arxiv-1804.103322018arXiv preprint</p>
<p>Sim2real2sim: Bridging the gap between simulation and real-world in flexible object manipulation. P Chang, T Padir, arXiv: Arxiv-2002.025382020arXiv preprint</p>
<p>Planar robot casting with real2sim2real self-supervised learning. V Lim, H Huang, L Y Chen, J Wang, J Ichnowski, D Seita, M Laskey, K Goldberg, arXiv: Arxiv-2111.048142021arXiv preprint</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, 2018 IEEE international conference on robotics and automation (ICRA). IEEE2018</p>
<p>Meta reinforcement learning for sim-to-real domain adaptation. K Arndt, M Hazara, A Ghadirzadeh, V Kyrki, 2019</p>
<p>Rl-cyclegan: Reinforcement learning aware simulation-to-real. K Rao, C Harris, A Irpan, S Levine, J Ibarz, M Khansari, 2020</p>
<p>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-tocanonical adaptation networks. S James, P Wohlhart, M Kalakrishnan, D Kalashnikov, A Irpan, J Ibarz, S Levine, R Hadsell, K Bousmalis, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019637</p>
<p>Bayesian imitation learning for end-to-end mobile manipulation. Y Du, D Ho, A Alemi, E Jang, M Khansari, International Conference on Machine Learning. PMLR2022</p>
<p>Solving rubik's cube with a robot hand. I Openai, M Akkaya, M Andrychowicz, M Chociej, B Litwin, A Mcgrew, A Petron, M Paino, G Plappert, R Powell, J Ribas, N Schneider, J Tezak, P Tworek, L Welinder, Q Weng, W Yuan, L Zaremba, Zhang, arXiv: Arxiv-1910.071132019arXiv preprint</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Bayessimig: Scalable parameter inference for adaptive domain randomization with isaacgym. R Antonova, F Ramos, R Possas, D Fox, arXiv:2107.045272021arXiv preprint</p>
<p>Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science robotics. 54759862020</p>
<p>Sim-toreal transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). </p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Reconciling reality through simulation: A realto-sim-to-real approach for robust manipulation. M Torne, A Simeonov, Z Li, A Chan, T Chen, A Gupta, P , arXiv:2403.039492024arXiv preprint</p>
<p>Foundationpose: Unified 6d pose estimation and tracking of novel objects. B Wen, W Yang, J Kautz, S Birchfield, arXiv:2312.083442023arXiv preprint</p>
<p>Isaac gym: High performance gpu-based physics simulation for robot learning. V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, G State, 2021</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Actor-critic algorithms. V Konda, J Tsitsiklis, Advances in neural information processing systems. 199912</p>
<p>Fast and accurate deep network learning by exponential linear units (elus). D.-A Clevert, arXiv:1511.072892015arXiv preprint</p>
<p>Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. H.-S Fang, C Wang, H Fang, M Gou, J Liu, H Yan, W Liu, Y Xie, C Lu, IEEE Transactions on Robotics. 2023</p>
<p>Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. S Gu, E Holly, T Lillicrap, S Levine, 2017 IEEE international conference on robotics and automation (ICRA). IEEE2017</p>
<p>Dataefficient deep reinforcement learning for dexterous manipulation. I Popov, N Heess, T Lillicrap, R Hafner, G Barth-Maron, M Vecerik, T Lampe, Y Tassa, T Erez, M Riedmiller, arXiv:1704.030732017arXiv preprint</p>
<p>Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. M Vecerik, T Hester, J Scholz, F Wang, O Pietquin, B Piot, N Heess, T Rothörl, T Lampe, M Riedmiller, arXiv:1707.088172017arXiv preprint</p>
<p>Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. A Rajeswaran, V Kumar, A Gupta, G Vezzani, J Schulman, E Todorov, S Levine, arXiv:1709.100872017arXiv preprint</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, Z Xu, S Kirmani, B Ichter, D Sadigh, L Guibas, F Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202414465</p>
<p>Simple open-vocabulary object detection with vision transformers. M Minderer, A Gritsenko, A Stone, M Neumann, D Weissenborn, A Dosovitskiy, A Mahendran, A Arnab, M Dehghani, Z Shen, arXiv:2205.062302022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
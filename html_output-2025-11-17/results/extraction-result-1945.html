<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1945 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1945</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1945</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-282139614</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.14836v1.pdf" target="_blank">QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models</a></p>
                <p><strong>Paper Abstract:</strong> Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1945.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1945.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QDepth-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quantized Depth Prediction for Vision-Language-Action models (QDepth-VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA model that augments a pretrained vision-language backbone with a dedicated depth expert that predicts quantized VQ-VAE depth tokens as an auxiliary task to improve 3D spatial reasoning for robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QDepth-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified VLA built on a pretrained PaLi-Gemma-3B vision-language model (SigLIP vision encoder + Gemma language decoder), an action expert (transformer-based action head), and a dedicated depth expert that predicts discrete depth tokens (VQ-VAE codebook indices). Uses a hybrid attention mask that isolates text/image intra-attention, allows depth to attend to image+text, and enables action tokens to attend to all preceding modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (pretrained VLM backbone) with additional task-specific pretraining/finetuning on robot datasets (Fractal, LIBERO, Bridge) and a separately pretrained VQ-VAE on depth frames</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained VLM (PaLi-Gemma-3B) provides image-text alignment; datasets used for downstream pretraining/finetuning include Fractal, LIBERO-90 and Bridge (robotic/embodied datasets). Depth targets are generated via a monocular Video-Depth-Anything (ViDA) estimator and tokenized via a VQ-VAE (256-codebook entries). The pretraining data thus contains images paired with language instructions and robot demonstrations; explicit descriptions of verbs/affordances in the VLM pretraining corpus are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation / Vision-Language-Action control (pick-and-place, long-horizon manipulation, stacking, object placement)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Language-conditioned robotic manipulation evaluated on simulation benchmarks (LIBERO suites: Spatial, Object, Goal, Long; Simpler: Google Robot & WidowX250 tasks) and real-robot pick-and-place tasks (6-DoF Piper arm with RealSense D455). Action outputs are continuous robot actions (diffusion / Conditional Flow Matching based action chunks of size 4). Tasks include object grasping, placing, opening drawers, stacking blocks, and multi-step long-horizon procedures in both single-view and multi-view settings.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The paper explicitly discusses preserving the pretrained semantic alignment of the VLM by avoiding direct fusion of raw depth into the VLM; depth supervision is applied via a separate depth expert and discrete targets so as not to disrupt the VLM's semantics. Overlap in objects/actions between VLM pretraining and target tasks is not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Simulation (LIBERO single-view): Goal tasks success rate 94.0%, Long tasks 72.6% (reported in paper); Simpler (Google Robot tasks) average success 75.1% with task-specific results e.g. Pick Coke Can 98.3%, Move Near 81.4%, Open/Close Drawer 58.0%, Open Top Drawer & Put Apple In 62.6%; Simpler (WidowX250 tasks) average 68.5% with Put Carrot on Plate 57.5%, Put Eggplant in Basket 95.0%, Put Spoon on Towel 82.0%, Stack Block 39.6%; Multi-view QDepth-VLA reported overall success rate 94.9% on LIBERO in the multi-view configuration. Real-robot: reported improvements up to +20.0% on a banana-picking task and +10.0% on other pick-and-place tasks compared to baseline open-œÄ0 (exact baseline numbers not listed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No direct experiment in this paper isolates 'no language pretraining' for the entire model; baselines compared are other VLA variants (e.g. open-œÄ0, CoT-VLA). The paper does report ablations removing depth supervision or the depth expert (not a removal of VLM pretraining): removal of depth expert caused largest drops (e.g., overall average drop -8.5% on Simpler avg; Stack Block -23.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper does not provide explicit quantitative sample-efficiency curves or counts (episodes/demonstrations) comparing language-pretrained vs non-pretrained variants. Training details (epochs, batch size, optimizer) are provided but not a sample-efficiency comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Introduces a hybrid attention mask design and provides ablation showing the hybrid attention contributes to performance gains (ablation 'w/o Hybrid Attn' reduced average Simpler success from 68.5% to 63.0%). The paper does not provide attention-map visualizations; analysis is limited to architectural description and ablation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Partial analysis: depth predictions are quantized to VQ-VAE code indices and decoded for visualization; reconstructed depth maps preserve object/gripper boundaries (visualized in paper). No in-depth embedding-space analyses (e.g., clustering, t-SNE) of multimodal embeddings are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Empirical evidence: adding quantized depth supervision improves manipulation success rates, particularly for tasks requiring precise spatial relations (stacking, placement). Ablations show removing depth supervision or the dedicated depth expert causes substantial drops (e.g., Stack Block drop of ~23.8%), indicating improved grounding of spatial/action primitives. Qualitative depth reconstructions align with object boundaries, supporting visual grounding of action-relevant geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>The paper motivates preserving pretrained 2D semantic priors while adding geometry via a separate depth expert, but does not present explicit layerwise analyses separating low-level vs high-level features. Improvements are most pronounced on mid/high-level spatial tasks (placement/stacking) suggesting benefit at those levels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Findings: quantized depth supervision helps single-view VLAs approach multi-view performance and supports generalization from simulation to real robot tasks; the paper notes that explicit 3D inputs (point clouds/depth at inference) can give higher absolute performance but QDepth-VLA improves geometric perception without requiring additional sensors at inference. Transfer works well when geometric cues are important (placement/stacking) and when preserving VLM semantics is necessary; direct injection of raw 3D features can disrupt VLM pretraining and hurt transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No explicit controlled comparison splitting objects into 'seen in pretraining' vs 'novel' subsets is reported. Improvements are reported across multiple object types (banana, eggplant, spoon, blocks), but no per-object familiarity analysis is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No zero-shot or few-shot evaluation numbers are reported for QDepth-VLA on new tasks without finetuning; the experiments focus on finetuned performance and ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Component ablations were performed: removing depth loss, removing depth expert, replacing latent token prediction with pixel-wise regression, and replacing hybrid attention. The largest negative impacts come from removing the depth expert (largest overall drop) and switching to pixel-wise regression (worse than quantized tokens), indicating the importance of those components. No systematic per-layer freezing of the VLM is reported beyond stating the VLM remains trainable.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>The paper notes prior work where naive depth regression degraded policy learning; within their ablations, replacing quantized latent prediction with pixel-wise depth regression reduced performance (average down to 64.6% from 68.5% on Simpler) and enforcing proprioception-to-depth attention (w/o Hybrid Attn variant) reduced performance by ~5% on average, indicating cases where certain depth/attention designs hurt control.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct head-to-head comparison to vision-only pretraining (e.g., ImageNet-only models) is provided. Comparisons are primarily among VLA variants and 3D-enhanced VLAs.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Co-training schedule includes an exponentially decaying weight on the depth loss (lambda_t decays over training), intended to establish geometric alignment early and focus on action later; no explicit training curves or temporal representation dynamics are shown.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality or intrinsic-dimension analyses of representations are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1945.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1945.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLi-Gemma-3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLiGemma-3B (PaLi-Gemma 3B VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3B-parameter vision-language model combining a SigLIP-based vision encoder with Gemma language modeling; used as the VLM backbone in QDepth-VLA to provide pretrained multimodal embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaliGemma: A versatile 3B VLM for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLi-Gemma-3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model integrating SigLIP vision encoding and Gemma language decoder; provides 256 visual tokens from main-view RGB that are concatenated with text tokens and processed under full block attention to produce multimodal embeddings. In QDepth-VLA the PaLi-Gemma backbone is pretrained and then kept trainable during downstream training.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (image-text / multimodal VLM pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in detail in this paper beyond citation; described as a pretrained VLM used for transfer. The QDepth-VLA paper does not list the original PaLi-Gemma pretraining corpus details.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Backbone encoder for language-conditioned robotic manipulation policies</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Provides visual-language embeddings for downstream action expert in continuous robotic control tasks (see QDepth-VLA target tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper emphasizes preserving PaLi-Gemma semantic alignment by isolating depth supervision from the VLM (depth tokens are predicted by a separate depth expert prior to language fusion to avoid semantic interference).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Used as the backbone for QDepth-VLA which achieved the reported task success rates (see QDepth-VLA entry). PaLi-Gemma's independent performance numbers on robot tasks are not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported for PaLi-Gemma in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Paper describes full block attention in PaLi-Gemma and a hybrid attention design around it, but provides no visualization of PaLi-Gemma's internal attention for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No direct embedding-space analysis of PaLi-Gemma representations is provided beyond noting their role as multimodal context for action and depth experts.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Used as the semantic backbone; the paper's evidence for grounding comes from the addition of depth expert rather than analysis of PaLi-Gemma itself.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed separately in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>PaLi-Gemma is treated as a transferable VLM; the paper emphasizes careful integration (avoiding direct depth fusion) to maintain its pretrained semantic alignment during transfer to manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not evaluated for PaLi-Gemma specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported for PaLi-Gemma in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layerwise freezing/fine-grained probing of PaLi-Gemma reported; the backbone remains trainable during QDepth-VLA training.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper cautions that injecting raw 3D features into the VLM can disrupt semantic alignment, motivating the separate depth expert; no direct negative transfer numbers for PaLi-Gemma alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not provided specifically for PaLi-Gemma.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1945.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1945.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>open-œÄ0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>open-œÄ0 (open-pi-zero / open-pi-0 baseline VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action flow model used as a primary baseline in the paper; compared in simulation and real-robot evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ùúã 0: A Vision-Language-Action Flow Model for General Robot Control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>open-œÄ0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language-action flow model (œÄ0 family) that provides action generation via flow/diffusion-based policy heads; used as a baseline in QDepth-VLA experiments and as a base architecture for QDepth-VLA's modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>VLA pretraining / flow-based policy pretraining on robotic datasets (as described in cited œÄ0 work); in this paper œÄ0 is treated as a pretrained VLA baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>In the context of the paper, pretraining/finetuning for œÄ0 variants mentioned included datasets like Fractal and LIBERO-90 (the paper references pretraining schedules), i.e., robotic demonstration-style datasets; the paper does not detail œÄ0's full original pretraining corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (same benchmarks: LIBERO, Simpler, real-robot pick-and-place tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Language-conditioned continuous control tasks used for benchmarking in the paper (LIBERO suites, Simpler tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>œÄ0 is described as a general VLA flow model that leverages vision-language alignment; the QDepth-VLA paper compares against œÄ0 to show improvements from depth supervision while preserving semantic grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in paper tables: Open œÄ0 [29] single-view scores on LIBERO (as reported in Table 2) include Spatial 77.2, Object 84.0, Goal 83.6, Long 66.0, Avg 77.7 (numbers copied from paper table layout). QDepth-VLA reports outperforming open-œÄ0 by 6.1% and 7.7% on average success rate in some reported comparisons (paper statement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in this paper for œÄ0.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No internal attention analyses of œÄ0 are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space analyses of œÄ0 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Used as baseline; the paper argues that augmenting VLA with quantized depth yields better grounding than œÄ0 in the tested tasks (empirical success-rate gains).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>œÄ0 variants are used with different pretraining/fine-tuning schedules (Fractal, LIBERO). The paper reports performance differences across tasks but does not dissect œÄ0 transfer conditions in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided in this paper for œÄ0.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>The paper implies that adding explicit depth inputs at inference (as some other baselines do) can improve performance but requires additional sensors; no direct negative transfer data for œÄ0.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1945.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1945.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoT-VLA (Visual Chain-of-Thought for VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA baseline that uses future sub-goal image predictions or chain-of-thought style intermediate predictions to improve temporal reasoning for action generation; compared against QDepth-VLA in benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language-action model that conditions action generation on predicted intermediate visual states (chain-of-thought style) to enhance temporal planning. It synthesizes future-oriented signals but at higher computational cost due to RGB prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>VLA pretraining with future-image/sub-goal prediction objectives (described in cited CoT-VLA work); not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper; CoT-VLA generally relies on datasets with temporal task progress (video or demonstration data) to learn future-image/sub-goal prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (LIBERO subsets and related tasks used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Long-horizon, temporally extended manipulation tasks where future-state reasoning can help.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not quantified here; CoT-VLA is presented as a method to enhance temporal reasoning rather than semantic grounding per se.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>In Table 2 CoT-VLA-7B is reported (as a strong single-view VLA baseline) with an average of 81.1% across LIBERO subsets (table in paper). QDepth-VLA reports outperforming CoT-VLA on Goal and Long tasks by 6.4% and 3.6% respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analysis for CoT-VLA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>CoT-VLA focuses on temporal/sub-goal reasoning; this paper argues that such approaches can be costly and may lack explicit 3D grounding compared to QDepth-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper suggests CoT-VLA's RGB synthesis is computationally costly and may not capture fine-grained geometry as effectively as depth-guided supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>CoT-VLA explicitly targets temporal reasoning; QDepth-VLA emphasizes geometric cues instead.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1945.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1945.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-CAVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-CAVLA (3D Cloud-Augmented VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A depth-enhanced VLA that integrates depth embeddings (projected into VLM token space) or 3D context to improve multiview performance; cited as a strong multi-view baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D-CAVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Depth-enhanced VLA that integrates depth or 3D features (e.g., RoI pooled depth embeddings projected into VLM token space) to support multiview geometric reasoning. Requires explicit depth/3D inputs at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal VLA with explicit 3D/depth inputs (described in cited work); specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper; the model leverages depth or point-cloud-like 3D signals during training/inference.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (multiview, 3D-aware tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multiview manipulation tasks where explicit depth/point-cloud inputs are available and can be fused with VLM features for improved geometric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper notes that explicit 3D inputs (as in 3D-CAVLA) can improve performance but require additional sensors at inference and may not preserve 2D VLM priors if naively fused.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in Table 2 as among the leading multi-view models (3D-CAVLA achieves higher overall results than single-view QDepth-VLA in the paper's Table 2), e.g., 98.2/96.1/98.1 style numbers in multi-view rows (specific per-category numbers shown in the paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in this paper for 3D-CAVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>3D-CAVLA's higher multiview scores serve as evidence that explicit 3D inputs can aid grounding, but QDepth-VLA demonstrates that quantized auxiliary depth supervision can close the gap without requiring 3D sensors at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Requires explicit 3D/depth inputs for best performance (paper contrasts this requirement with QDepth-VLA's single-view depth supervision advantage).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper suggests that naive 3D-2D fusion may disrupt VLM pretraining if not carefully designed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1945.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1945.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeoVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeoVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D/cloud-enhanced VLA that supplies point-cloud or 3D embeddings to modality-specific experts to improve spatial reasoning; cited as a strong multiview/3D baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GeoVLA: Empowering 3D Representations in Vision-Language-Action Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GeoVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLA variant that encodes explicit 3D geometry (point clouds / depth maps) with specialized encoders and integrates those embeddings into action heads to boost spatial perception.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal with explicit 3D geometry inputs (from cited work); specifics are in the referenced GeoVLA paper, not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper; uses data with depth/point-cloud/3D context.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (3D-aware tasks, multiview settings)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Embodied manipulation tasks where 3D geometry is available and important for performance.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper points out that fusing explicit 3D features can risk disrupting VLM 2D priors unless carefully integrated; GeoVLA achieves high performance when explicit 3D is available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Listed among top multi-view methods in Table 2 (GeoVLA reported high average success rates in multi-view comparisons). Exact numeric breakdown in paper's table shows GeoVLA achieving very high multi-view success (e.g., in 90s across categories).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>High multi-view performance indicates strong grounding when explicit 3D inputs are available; QDepth-VLA aims to approximate such geometric grounding without requiring explicit 3D inputs at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when 3D sensors/modalities are available and when integration with VLMs is properly handled.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper notes possible interference with 2D VLM semantics if 3D features are fused incorrectly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1945.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1945.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniVLA (Task-centric Latent Actions VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale latent-action VLA that predicts compressed latent action tokens (task-centric) and demonstrates strong performance and scalability; cited as a leading multiview/latent-embedding approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>UniVLA: Learning to Act Anywhere with Task-centric Latent Actions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Scalable VLA that learns latent action tokens from large-scale human video pretraining; focuses on latent prediction rather than explicit 3D grounding and is presented as highly performant in multi-view/multi-task settings.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Large-scale multimodal video/action pretraining (human video pretraining referenced in related work); not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale human videos used to learn latent actions; the paper notes such latent approaches often lack explicit 3D grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation and generalist embodied action synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Task-centric latent action generation for manipulation; used as a comparator in LIBERO multi-view or generalist VLA comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Latent-action approaches can scale semantic/action reasoning but may lack fine-grained 3D grounding according to the paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Listed among top multi-view models in Table 2 (e.g., high multi-view success rates in the 90s reported in the table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not detailed in this paper; the paper notes latent predictions may lack explicit geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper suggests latent-action models may struggle with fine-grained geometry compared to explicit 3D-aware approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Per paper discussion, latent-action scalability helps transfer but geometry-sensitive tasks may require additional 3D grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Latent-only predictions can lack explicit 3D grounding leading to poorer performance on geometry-critical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1945.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1945.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA approach focused on exploring spatial representations for manipulation; used as a baseline in Simpler benchmark comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLA variant emphasizing spatial representation learning for manipulation tasks; compared quantitatively on Simpler benchmarks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>VLA-style pretraining / finetuning on robot datasets (details in cited SpatialVLA work, not in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here; used as a finetuned baseline on the Simpler tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (Simpler benchmark tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Pick-and-place, placement, and stacking tasks in Simpler (WidowX250 and Google Robot tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not directly analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported baseline numbers in Tables 3 and 4: e.g., SpatialVLA finetuned on Simpler (Google Robot tasks) shown with high numbers for some tasks (e.g., 86.0% on Pick Coke Can in Table 3) and on WidowX250 tasks SpatialVLA finetuned averages 42.7% (table shows varied per-task performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Compared empirically; QDepth-VLA outperforms SpatialVLA on several spatially precise tasks (e.g., stack block improved by 10.4 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not directly analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not explicitly discussed for SpatialVLA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not shown.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1945.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1945.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PointVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PointVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that injects explicit 3D point-cloud representations into VLAs via specialized encoders to provide 3D world information for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PointVLA: Injecting the 3D World into Vision-Language-Action Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PointVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architecture that encodes point-cloud 3D geometry and integrates it with VLM features, typically via modality-specific experts; discussed in related work as representative of direct 3D feature injection approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal pretraining incorporating explicit 3D/point cloud modalities (details are in the cited PointVLA paper, not this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed here; typically requires datasets with point-cloud or depth sensor data.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation with explicit 3D geometry</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Manipulation tasks that leverage point-clouds for geometric precision.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper warns direct 3D injection can disrupt pretrained 2D VLM alignment if not carefully designed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper (PointVLA is cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>PointVLA is cited as providing explicit geometric cues that can help spatial perception, but may come with modality-gap challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when point-cloud/depth sensors are available and when modality gaps are bridged.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper notes modality gap risk when fusing explicit 3D with pretrained 2D VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1945.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1945.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>4D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>4D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A spatiotemporal VLA that augments visual inputs with 3D coordinate embeddings for spatial alignment and temporal reasoning; cited as a depth-enhanced VLA variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>4D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Spatiotemporal VLA approach that integrates spatial (3D coordinate) and temporal signals across scenes to improve calibration and long-horizon reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Spatiotemporal multimodal pretraining (video + spatial cues) as per cited work; specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation and long-horizon tasks needing spatiotemporal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Long-horizon / multiview tasks where temporal and spatial cross-scene calibration helps performance.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper positions 4D-VLA among depth-enhanced VLAs that explicitly inject spatiotemporal geometry; QDepth-VLA compares favorably while avoiding extra inference sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 2 lists 4D-VLA as one of depth-enhanced baselines with competitive numbers (e.g., in the high 80s/90s for multi-view rows); exact numbers in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Cited as improving spatiotemporal calibration; QDepth-VLA argues for compact quantized depth supervision as an alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Relies on spatiotemporal data; not dissected in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>By design 4D-VLA models temporal dynamics; this paper does not present deeper analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1945.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1945.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2/RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 and RT-1 (Robotics Transformer families)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Noted prior vision-language-action/robotic transformer systems that demonstrate transfer of web-scale or large-scale data to robotic control; cited as foundational VLA/robotic transformer work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2 / RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based robotic policies that condition on vision and language to produce actions; RT-2 emphasized transfer of web-scale knowledge into robotic control; RT-1 is an earlier robotics transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Large-scale vision-language / web-scale data pretraining for transfer to robotics (as per cited RT-2/RT-1 papers); details are in those references.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Web-scale image-text and robotic demonstration style datasets used in original RT work; not elaborated here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation and language-conditioned control</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>General robotic tasks where web-scale or large pretrained VLM knowledge transfers aid performance.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>RT-2 is cited as transferring web knowledge to robotics; QDepth-VLA references such prior VLA work as background but does not analyze RT models directly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Cited as demonstrating the value of large pretrained models for robot control; QDepth-VLA builds on the line of grounding VLMs for actions.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>General transfer from web-scale pretraining to robotic tasks is cited as successful in RT-2 literature; this paper does not quantify it.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>RT literature often demonstrates few-shot/zero-shot transfer; QDepth-VLA does not report new zero/few-shot results.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks. <em>(Rating: 2)</em></li>
                <li>PointVLA: Injecting the 3D World into Vision-Language-Action Models. <em>(Rating: 2)</em></li>
                <li>GeoVLA: Empowering 3D Representations in Vision-Language-Action Models. <em>(Rating: 2)</em></li>
                <li>UniVLA: Learning to Act Anywhere with Task-centric Latent Actions. <em>(Rating: 2)</em></li>
                <li>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models. <em>(Rating: 2)</em></li>
                <li>PaliGemma: A versatile 3B VLM for transfer. <em>(Rating: 1)</em></li>
                <li>ùúã 0: A Vision-Language-Action Flow Model for General Robot Control. <em>(Rating: 2)</em></li>
                <li>SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model. <em>(Rating: 2)</em></li>
                <li>4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration. <em>(Rating: 1)</em></li>
                <li>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1945",
    "paper_id": "paper-282139614",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "QDepth-VLA",
            "name_full": "Quantized Depth Prediction for Vision-Language-Action models (QDepth-VLA)",
            "brief_description": "A VLA model that augments a pretrained vision-language backbone with a dedicated depth expert that predicts quantized VQ-VAE depth tokens as an auxiliary task to improve 3D spatial reasoning for robotic manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "QDepth-VLA",
            "model_description": "Unified VLA built on a pretrained PaLi-Gemma-3B vision-language model (SigLIP vision encoder + Gemma language decoder), an action expert (transformer-based action head), and a dedicated depth expert that predicts discrete depth tokens (VQ-VAE codebook indices). Uses a hybrid attention mask that isolates text/image intra-attention, allows depth to attend to image+text, and enables action tokens to attend to all preceding modalities.",
            "pretraining_type": "vision-language pretraining (pretrained VLM backbone) with additional task-specific pretraining/finetuning on robot datasets (Fractal, LIBERO, Bridge) and a separately pretrained VQ-VAE on depth frames",
            "pretraining_data_description": "Pretrained VLM (PaLi-Gemma-3B) provides image-text alignment; datasets used for downstream pretraining/finetuning include Fractal, LIBERO-90 and Bridge (robotic/embodied datasets). Depth targets are generated via a monocular Video-Depth-Anything (ViDA) estimator and tokenized via a VQ-VAE (256-codebook entries). The pretraining data thus contains images paired with language instructions and robot demonstrations; explicit descriptions of verbs/affordances in the VLM pretraining corpus are not provided in this paper.",
            "target_task_name": "Robotic manipulation / Vision-Language-Action control (pick-and-place, long-horizon manipulation, stacking, object placement)",
            "target_task_description": "Language-conditioned robotic manipulation evaluated on simulation benchmarks (LIBERO suites: Spatial, Object, Goal, Long; Simpler: Google Robot & WidowX250 tasks) and real-robot pick-and-place tasks (6-DoF Piper arm with RealSense D455). Action outputs are continuous robot actions (diffusion / Conditional Flow Matching based action chunks of size 4). Tasks include object grasping, placing, opening drawers, stacking blocks, and multi-step long-horizon procedures in both single-view and multi-view settings.",
            "semantic_alignment": "The paper explicitly discusses preserving the pretrained semantic alignment of the VLM by avoiding direct fusion of raw depth into the VLM; depth supervision is applied via a separate depth expert and discrete targets so as not to disrupt the VLM's semantics. Overlap in objects/actions between VLM pretraining and target tasks is not quantified in the paper.",
            "performance_with_language_pretraining": "Simulation (LIBERO single-view): Goal tasks success rate 94.0%, Long tasks 72.6% (reported in paper); Simpler (Google Robot tasks) average success 75.1% with task-specific results e.g. Pick Coke Can 98.3%, Move Near 81.4%, Open/Close Drawer 58.0%, Open Top Drawer & Put Apple In 62.6%; Simpler (WidowX250 tasks) average 68.5% with Put Carrot on Plate 57.5%, Put Eggplant in Basket 95.0%, Put Spoon on Towel 82.0%, Stack Block 39.6%; Multi-view QDepth-VLA reported overall success rate 94.9% on LIBERO in the multi-view configuration. Real-robot: reported improvements up to +20.0% on a banana-picking task and +10.0% on other pick-and-place tasks compared to baseline open-œÄ0 (exact baseline numbers not listed).",
            "performance_without_language_pretraining": "No direct experiment in this paper isolates 'no language pretraining' for the entire model; baselines compared are other VLA variants (e.g. open-œÄ0, CoT-VLA). The paper does report ablations removing depth supervision or the depth expert (not a removal of VLM pretraining): removal of depth expert caused largest drops (e.g., overall average drop -8.5% on Simpler avg; Stack Block -23.8%).",
            "sample_efficiency_comparison": "The paper does not provide explicit quantitative sample-efficiency curves or counts (episodes/demonstrations) comparing language-pretrained vs non-pretrained variants. Training details (epochs, batch size, optimizer) are provided but not a sample-efficiency comparison.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Introduces a hybrid attention mask design and provides ablation showing the hybrid attention contributes to performance gains (ablation 'w/o Hybrid Attn' reduced average Simpler success from 68.5% to 63.0%). The paper does not provide attention-map visualizations; analysis is limited to architectural description and ablation outcomes.",
            "embedding_space_analysis": "Partial analysis: depth predictions are quantized to VQ-VAE code indices and decoded for visualization; reconstructed depth maps preserve object/gripper boundaries (visualized in paper). No in-depth embedding-space analyses (e.g., clustering, t-SNE) of multimodal embeddings are reported.",
            "action_grounding_evidence": "Empirical evidence: adding quantized depth supervision improves manipulation success rates, particularly for tasks requiring precise spatial relations (stacking, placement). Ablations show removing depth supervision or the dedicated depth expert causes substantial drops (e.g., Stack Block drop of ~23.8%), indicating improved grounding of spatial/action primitives. Qualitative depth reconstructions align with object boundaries, supporting visual grounding of action-relevant geometry.",
            "hierarchical_features_evidence": "The paper motivates preserving pretrained 2D semantic priors while adding geometry via a separate depth expert, but does not present explicit layerwise analyses separating low-level vs high-level features. Improvements are most pronounced on mid/high-level spatial tasks (placement/stacking) suggesting benefit at those levels.",
            "transfer_conditions": "Findings: quantized depth supervision helps single-view VLAs approach multi-view performance and supports generalization from simulation to real robot tasks; the paper notes that explicit 3D inputs (point clouds/depth at inference) can give higher absolute performance but QDepth-VLA improves geometric perception without requiring additional sensors at inference. Transfer works well when geometric cues are important (placement/stacking) and when preserving VLM semantics is necessary; direct injection of raw 3D features can disrupt VLM pretraining and hurt transfer.",
            "novel_vs_familiar_objects": "No explicit controlled comparison splitting objects into 'seen in pretraining' vs 'novel' subsets is reported. Improvements are reported across multiple object types (banana, eggplant, spoon, blocks), but no per-object familiarity analysis is provided.",
            "zero_shot_or_few_shot": "No zero-shot or few-shot evaluation numbers are reported for QDepth-VLA on new tasks without finetuning; the experiments focus on finetuned performance and ablations.",
            "layer_analysis": "Component ablations were performed: removing depth loss, removing depth expert, replacing latent token prediction with pixel-wise regression, and replacing hybrid attention. The largest negative impacts come from removing the depth expert (largest overall drop) and switching to pixel-wise regression (worse than quantized tokens), indicating the importance of those components. No systematic per-layer freezing of the VLM is reported beyond stating the VLM remains trainable.",
            "negative_transfer_evidence": "The paper notes prior work where naive depth regression degraded policy learning; within their ablations, replacing quantized latent prediction with pixel-wise depth regression reduced performance (average down to 64.6% from 68.5% on Simpler) and enforcing proprioception-to-depth attention (w/o Hybrid Attn variant) reduced performance by ~5% on average, indicating cases where certain depth/attention designs hurt control.",
            "comparison_to_vision_only": "No direct head-to-head comparison to vision-only pretraining (e.g., ImageNet-only models) is provided. Comparisons are primarily among VLA variants and 3D-enhanced VLAs.",
            "temporal_dynamics": "Co-training schedule includes an exponentially decaying weight on the depth loss (lambda_t decays over training), intended to establish geometric alignment early and focus on action later; no explicit training curves or temporal representation dynamics are shown.",
            "dimensionality_analysis": "No dimensionality or intrinsic-dimension analyses of representations are reported.",
            "uuid": "e1945.0"
        },
        {
            "name_short": "PaLi-Gemma-3B",
            "name_full": "PaLiGemma-3B (PaLi-Gemma 3B VLM)",
            "brief_description": "A 3B-parameter vision-language model combining a SigLIP-based vision encoder with Gemma language modeling; used as the VLM backbone in QDepth-VLA to provide pretrained multimodal embeddings.",
            "citation_title": "PaliGemma: A versatile 3B VLM for transfer.",
            "mention_or_use": "use",
            "model_name": "PaLi-Gemma-3B",
            "model_description": "Vision-language model integrating SigLIP vision encoding and Gemma language decoder; provides 256 visual tokens from main-view RGB that are concatenated with text tokens and processed under full block attention to produce multimodal embeddings. In QDepth-VLA the PaLi-Gemma backbone is pretrained and then kept trainable during downstream training.",
            "pretraining_type": "vision-language pretraining (image-text / multimodal VLM pretraining)",
            "pretraining_data_description": "Not specified in detail in this paper beyond citation; described as a pretrained VLM used for transfer. The QDepth-VLA paper does not list the original PaLi-Gemma pretraining corpus details.",
            "target_task_name": "Backbone encoder for language-conditioned robotic manipulation policies",
            "target_task_description": "Provides visual-language embeddings for downstream action expert in continuous robotic control tasks (see QDepth-VLA target tasks).",
            "semantic_alignment": "Paper emphasizes preserving PaLi-Gemma semantic alignment by isolating depth supervision from the VLM (depth tokens are predicted by a separate depth expert prior to language fusion to avoid semantic interference).",
            "performance_with_language_pretraining": "Used as the backbone for QDepth-VLA which achieved the reported task success rates (see QDepth-VLA entry). PaLi-Gemma's independent performance numbers on robot tasks are not reported in this paper.",
            "performance_without_language_pretraining": "Not reported in this paper.",
            "sample_efficiency_comparison": "Not reported for PaLi-Gemma in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Paper describes full block attention in PaLi-Gemma and a hybrid attention design around it, but provides no visualization of PaLi-Gemma's internal attention for these tasks.",
            "embedding_space_analysis": "No direct embedding-space analysis of PaLi-Gemma representations is provided beyond noting their role as multimodal context for action and depth experts.",
            "action_grounding_evidence": "Used as the semantic backbone; the paper's evidence for grounding comes from the addition of depth expert rather than analysis of PaLi-Gemma itself.",
            "hierarchical_features_evidence": "Not analyzed separately in this paper.",
            "transfer_conditions": "PaLi-Gemma is treated as a transferable VLM; the paper emphasizes careful integration (avoiding direct depth fusion) to maintain its pretrained semantic alignment during transfer to manipulation tasks.",
            "novel_vs_familiar_objects": "Not evaluated for PaLi-Gemma specifically.",
            "zero_shot_or_few_shot": "Not reported for PaLi-Gemma in this work.",
            "layer_analysis": "No layerwise freezing/fine-grained probing of PaLi-Gemma reported; the backbone remains trainable during QDepth-VLA training.",
            "negative_transfer_evidence": "Paper cautions that injecting raw 3D features into the VLM can disrupt semantic alignment, motivating the separate depth expert; no direct negative transfer numbers for PaLi-Gemma alone.",
            "comparison_to_vision_only": "Not provided in this paper.",
            "temporal_dynamics": "Not provided specifically for PaLi-Gemma.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1945.1"
        },
        {
            "name_short": "open-œÄ0",
            "name_full": "open-œÄ0 (open-pi-zero / open-pi-0 baseline VLA)",
            "brief_description": "A vision-language-action flow model used as a primary baseline in the paper; compared in simulation and real-robot evaluations.",
            "citation_title": "ùúã 0: A Vision-Language-Action Flow Model for General Robot Control.",
            "mention_or_use": "use",
            "model_name": "open-œÄ0",
            "model_description": "Vision-language-action flow model (œÄ0 family) that provides action generation via flow/diffusion-based policy heads; used as a baseline in QDepth-VLA experiments and as a base architecture for QDepth-VLA's modifications.",
            "pretraining_type": "VLA pretraining / flow-based policy pretraining on robotic datasets (as described in cited œÄ0 work); in this paper œÄ0 is treated as a pretrained VLA baseline.",
            "pretraining_data_description": "In the context of the paper, pretraining/finetuning for œÄ0 variants mentioned included datasets like Fractal and LIBERO-90 (the paper references pretraining schedules), i.e., robotic demonstration-style datasets; the paper does not detail œÄ0's full original pretraining corpus.",
            "target_task_name": "Robotic manipulation (same benchmarks: LIBERO, Simpler, real-robot pick-and-place tasks)",
            "target_task_description": "Language-conditioned continuous control tasks used for benchmarking in the paper (LIBERO suites, Simpler tasks).",
            "semantic_alignment": "œÄ0 is described as a general VLA flow model that leverages vision-language alignment; the QDepth-VLA paper compares against œÄ0 to show improvements from depth supervision while preserving semantic grounding.",
            "performance_with_language_pretraining": "Reported in paper tables: Open œÄ0 [29] single-view scores on LIBERO (as reported in Table 2) include Spatial 77.2, Object 84.0, Goal 83.6, Long 66.0, Avg 77.7 (numbers copied from paper table layout). QDepth-VLA reports outperforming open-œÄ0 by 6.1% and 7.7% on average success rate in some reported comparisons (paper statement).",
            "performance_without_language_pretraining": "Not reported in this paper for œÄ0.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No internal attention analyses of œÄ0 are provided in this paper.",
            "embedding_space_analysis": "No embedding-space analyses of œÄ0 in this paper.",
            "action_grounding_evidence": "Used as baseline; the paper argues that augmenting VLA with quantized depth yields better grounding than œÄ0 in the tested tasks (empirical success-rate gains).",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "œÄ0 variants are used with different pretraining/fine-tuning schedules (Fractal, LIBERO). The paper reports performance differences across tasks but does not dissect œÄ0 transfer conditions in detail.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not provided in this paper for œÄ0.",
            "negative_transfer_evidence": "The paper implies that adding explicit depth inputs at inference (as some other baselines do) can improve performance but requires additional sensors; no direct negative transfer data for œÄ0.",
            "comparison_to_vision_only": "Not provided.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1945.2"
        },
        {
            "name_short": "CoT-VLA",
            "name_full": "CoT-VLA (Visual Chain-of-Thought for VLA)",
            "brief_description": "A VLA baseline that uses future sub-goal image predictions or chain-of-thought style intermediate predictions to improve temporal reasoning for action generation; compared against QDepth-VLA in benchmarks.",
            "citation_title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models.",
            "mention_or_use": "use",
            "model_name": "CoT-VLA",
            "model_description": "Vision-language-action model that conditions action generation on predicted intermediate visual states (chain-of-thought style) to enhance temporal planning. It synthesizes future-oriented signals but at higher computational cost due to RGB prediction.",
            "pretraining_type": "VLA pretraining with future-image/sub-goal prediction objectives (described in cited CoT-VLA work); not detailed in this paper.",
            "pretraining_data_description": "Not specified in this paper; CoT-VLA generally relies on datasets with temporal task progress (video or demonstration data) to learn future-image/sub-goal prediction.",
            "target_task_name": "Robotic manipulation (LIBERO subsets and related tasks used for comparison)",
            "target_task_description": "Long-horizon, temporally extended manipulation tasks where future-state reasoning can help.",
            "semantic_alignment": "Not quantified here; CoT-VLA is presented as a method to enhance temporal reasoning rather than semantic grounding per se.",
            "performance_with_language_pretraining": "In Table 2 CoT-VLA-7B is reported (as a strong single-view VLA baseline) with an average of 81.1% across LIBERO subsets (table in paper). QDepth-VLA reports outperforming CoT-VLA on Goal and Long tasks by 6.4% and 3.6% respectively.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention analysis for CoT-VLA in this paper.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "CoT-VLA focuses on temporal/sub-goal reasoning; this paper argues that such approaches can be costly and may lack explicit 3D grounding compared to QDepth-VLA.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Not analyzed in detail here.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Paper suggests CoT-VLA's RGB synthesis is computationally costly and may not capture fine-grained geometry as effectively as depth-guided supervision.",
            "comparison_to_vision_only": "Not provided.",
            "temporal_dynamics": "CoT-VLA explicitly targets temporal reasoning; QDepth-VLA emphasizes geometric cues instead.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1945.3"
        },
        {
            "name_short": "3D-CAVLA",
            "name_full": "3D-CAVLA (3D Cloud-Augmented VLA)",
            "brief_description": "A depth-enhanced VLA that integrates depth embeddings (projected into VLM token space) or 3D context to improve multiview performance; cited as a strong multi-view baseline.",
            "citation_title": "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks.",
            "mention_or_use": "use",
            "model_name": "3D-CAVLA",
            "model_description": "Depth-enhanced VLA that integrates depth or 3D features (e.g., RoI pooled depth embeddings projected into VLM token space) to support multiview geometric reasoning. Requires explicit depth/3D inputs at inference.",
            "pretraining_type": "Multimodal VLA with explicit 3D/depth inputs (described in cited work); specifics not provided in this paper.",
            "pretraining_data_description": "Not detailed in this paper; the model leverages depth or point-cloud-like 3D signals during training/inference.",
            "target_task_name": "Robotic manipulation (multiview, 3D-aware tasks)",
            "target_task_description": "Multiview manipulation tasks where explicit depth/point-cloud inputs are available and can be fused with VLM features for improved geometric reasoning.",
            "semantic_alignment": "Paper notes that explicit 3D inputs (as in 3D-CAVLA) can improve performance but require additional sensors at inference and may not preserve 2D VLM priors if naively fused.",
            "performance_with_language_pretraining": "Reported in Table 2 as among the leading multi-view models (3D-CAVLA achieves higher overall results than single-view QDepth-VLA in the paper's Table 2), e.g., 98.2/96.1/98.1 style numbers in multi-view rows (specific per-category numbers shown in the paper table).",
            "performance_without_language_pretraining": "Not reported in this paper.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in this paper for 3D-CAVLA.",
            "embedding_space_analysis": "Not provided here.",
            "action_grounding_evidence": "3D-CAVLA's higher multiview scores serve as evidence that explicit 3D inputs can aid grounding, but QDepth-VLA demonstrates that quantized auxiliary depth supervision can close the gap without requiring 3D sensors at inference.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Requires explicit 3D/depth inputs for best performance (paper contrasts this requirement with QDepth-VLA's single-view depth supervision advantage).",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not provided here.",
            "negative_transfer_evidence": "Paper suggests that naive 3D-2D fusion may disrupt VLM pretraining if not carefully designed.",
            "comparison_to_vision_only": "Not directly compared here.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1945.4"
        },
        {
            "name_short": "GeoVLA",
            "name_full": "GeoVLA",
            "brief_description": "A 3D/cloud-enhanced VLA that supplies point-cloud or 3D embeddings to modality-specific experts to improve spatial reasoning; cited as a strong multiview/3D baseline.",
            "citation_title": "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models.",
            "mention_or_use": "use",
            "model_name": "GeoVLA",
            "model_description": "VLA variant that encodes explicit 3D geometry (point clouds / depth maps) with specialized encoders and integrates those embeddings into action heads to boost spatial perception.",
            "pretraining_type": "Multimodal with explicit 3D geometry inputs (from cited work); specifics are in the referenced GeoVLA paper, not detailed here.",
            "pretraining_data_description": "Not detailed in this paper; uses data with depth/point-cloud/3D context.",
            "target_task_name": "Robotic manipulation (3D-aware tasks, multiview settings)",
            "target_task_description": "Embodied manipulation tasks where 3D geometry is available and important for performance.",
            "semantic_alignment": "Paper points out that fusing explicit 3D features can risk disrupting VLM 2D priors unless carefully integrated; GeoVLA achieves high performance when explicit 3D is available.",
            "performance_with_language_pretraining": "Listed among top multi-view methods in Table 2 (GeoVLA reported high average success rates in multi-view comparisons). Exact numeric breakdown in paper's table shows GeoVLA achieving very high multi-view success (e.g., in 90s across categories).",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper.",
            "embedding_space_analysis": "Not provided here.",
            "action_grounding_evidence": "High multi-view performance indicates strong grounding when explicit 3D inputs are available; QDepth-VLA aims to approximate such geometric grounding without requiring explicit 3D inputs at inference.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Works best when 3D sensors/modalities are available and when integration with VLMs is properly handled.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not provided here.",
            "negative_transfer_evidence": "Paper notes possible interference with 2D VLM semantics if 3D features are fused incorrectly.",
            "comparison_to_vision_only": "Not provided.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1945.5"
        },
        {
            "name_short": "UniVLA",
            "name_full": "UniVLA (Task-centric Latent Actions VLA)",
            "brief_description": "A large-scale latent-action VLA that predicts compressed latent action tokens (task-centric) and demonstrates strong performance and scalability; cited as a leading multiview/latent-embedding approach.",
            "citation_title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions.",
            "mention_or_use": "use",
            "model_name": "UniVLA",
            "model_description": "Scalable VLA that learns latent action tokens from large-scale human video pretraining; focuses on latent prediction rather than explicit 3D grounding and is presented as highly performant in multi-view/multi-task settings.",
            "pretraining_type": "Large-scale multimodal video/action pretraining (human video pretraining referenced in related work); not detailed in this paper.",
            "pretraining_data_description": "Large-scale human videos used to learn latent actions; the paper notes such latent approaches often lack explicit 3D grounding.",
            "target_task_name": "Robotic manipulation and generalist embodied action synthesis",
            "target_task_description": "Task-centric latent action generation for manipulation; used as a comparator in LIBERO multi-view or generalist VLA comparisons.",
            "semantic_alignment": "Latent-action approaches can scale semantic/action reasoning but may lack fine-grained 3D grounding according to the paper's discussion.",
            "performance_with_language_pretraining": "Listed among top multi-view models in Table 2 (e.g., high multi-view success rates in the 90s reported in the table).",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided here.",
            "embedding_space_analysis": "Not detailed in this paper; the paper notes latent predictions may lack explicit geometry.",
            "action_grounding_evidence": "Paper suggests latent-action models may struggle with fine-grained geometry compared to explicit 3D-aware approaches.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Per paper discussion, latent-action scalability helps transfer but geometry-sensitive tasks may require additional 3D grounding.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Latent-only predictions can lack explicit 3D grounding leading to poorer performance on geometry-critical tasks.",
            "comparison_to_vision_only": "Not provided.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1945.6"
        },
        {
            "name_short": "SpatialVLA",
            "name_full": "SpatialVLA",
            "brief_description": "A VLA approach focused on exploring spatial representations for manipulation; used as a baseline in Simpler benchmark comparisons.",
            "citation_title": "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model.",
            "mention_or_use": "use",
            "model_name": "SpatialVLA",
            "model_description": "VLA variant emphasizing spatial representation learning for manipulation tasks; compared quantitatively on Simpler benchmarks in the paper.",
            "pretraining_type": "VLA-style pretraining / finetuning on robot datasets (details in cited SpatialVLA work, not in this paper).",
            "pretraining_data_description": "Not specified here; used as a finetuned baseline on the Simpler tasks.",
            "target_task_name": "Robotic manipulation (Simpler benchmark tasks)",
            "target_task_description": "Pick-and-place, placement, and stacking tasks in Simpler (WidowX250 and Google Robot tasks).",
            "semantic_alignment": "Not directly analyzed in this paper.",
            "performance_with_language_pretraining": "Reported baseline numbers in Tables 3 and 4: e.g., SpatialVLA finetuned on Simpler (Google Robot tasks) shown with high numbers for some tasks (e.g., 86.0% on Pick Coke Can in Table 3) and on WidowX250 tasks SpatialVLA finetuned averages 42.7% (table shows varied per-task performance).",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided here.",
            "embedding_space_analysis": "Not provided here.",
            "action_grounding_evidence": "Compared empirically; QDepth-VLA outperforms SpatialVLA on several spatially precise tasks (e.g., stack block improved by 10.4 percentage points).",
            "hierarchical_features_evidence": "Not directly analyzed.",
            "transfer_conditions": "Not explicitly discussed for SpatialVLA in this paper.",
            "novel_vs_familiar_objects": "Not provided.",
            "zero_shot_or_few_shot": "Not provided.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not shown.",
            "comparison_to_vision_only": "Not provided.",
            "temporal_dynamics": "Not provided.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1945.7"
        },
        {
            "name_short": "PointVLA",
            "name_full": "PointVLA",
            "brief_description": "A method that injects explicit 3D point-cloud representations into VLAs via specialized encoders to provide 3D world information for manipulation.",
            "citation_title": "PointVLA: Injecting the 3D World into Vision-Language-Action Models.",
            "mention_or_use": "mention",
            "model_name": "PointVLA",
            "model_description": "Architecture that encodes point-cloud 3D geometry and integrates it with VLM features, typically via modality-specific experts; discussed in related work as representative of direct 3D feature injection approaches.",
            "pretraining_type": "Multimodal pretraining incorporating explicit 3D/point cloud modalities (details are in the cited PointVLA paper, not this paper).",
            "pretraining_data_description": "Not detailed here; typically requires datasets with point-cloud or depth sensor data.",
            "target_task_name": "Robotic manipulation with explicit 3D geometry",
            "target_task_description": "Manipulation tasks that leverage point-clouds for geometric precision.",
            "semantic_alignment": "Paper warns direct 3D injection can disrupt pretrained 2D VLM alignment if not carefully designed.",
            "performance_with_language_pretraining": "Not reported in this paper (PointVLA is cited in related work).",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided here.",
            "embedding_space_analysis": "Not provided here.",
            "action_grounding_evidence": "PointVLA is cited as providing explicit geometric cues that can help spatial perception, but may come with modality-gap challenges.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Works best when point-cloud/depth sensors are available and when modality gaps are bridged.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not provided here.",
            "negative_transfer_evidence": "Paper notes modality gap risk when fusing explicit 3D with pretrained 2D VLMs.",
            "comparison_to_vision_only": "Not provided.",
            "temporal_dynamics": "Not provided.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1945.8"
        },
        {
            "name_short": "4D-VLA",
            "name_full": "4D-VLA",
            "brief_description": "A spatiotemporal VLA that augments visual inputs with 3D coordinate embeddings for spatial alignment and temporal reasoning; cited as a depth-enhanced VLA variant.",
            "citation_title": "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration.",
            "mention_or_use": "use",
            "model_name": "4D-VLA",
            "model_description": "Spatiotemporal VLA approach that integrates spatial (3D coordinate) and temporal signals across scenes to improve calibration and long-horizon reasoning.",
            "pretraining_type": "Spatiotemporal multimodal pretraining (video + spatial cues) as per cited work; specifics not provided in this paper.",
            "pretraining_data_description": "Not detailed here.",
            "target_task_name": "Robotic manipulation and long-horizon tasks needing spatiotemporal reasoning",
            "target_task_description": "Long-horizon / multiview tasks where temporal and spatial cross-scene calibration helps performance.",
            "semantic_alignment": "Paper positions 4D-VLA among depth-enhanced VLAs that explicitly inject spatiotemporal geometry; QDepth-VLA compares favorably while avoiding extra inference sensors.",
            "performance_with_language_pretraining": "Table 2 lists 4D-VLA as one of depth-enhanced baselines with competitive numbers (e.g., in the high 80s/90s for multi-view rows); exact numbers in paper tables.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in this paper.",
            "embedding_space_analysis": "Not provided here.",
            "action_grounding_evidence": "Cited as improving spatiotemporal calibration; QDepth-VLA argues for compact quantized depth supervision as an alternative.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Relies on spatiotemporal data; not dissected in this paper.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not provided.",
            "temporal_dynamics": "By design 4D-VLA models temporal dynamics; this paper does not present deeper analysis.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1945.9"
        },
        {
            "name_short": "RT-2/RT-1",
            "name_full": "RT-2 and RT-1 (Robotics Transformer families)",
            "brief_description": "Noted prior vision-language-action/robotic transformer systems that demonstrate transfer of web-scale or large-scale data to robotic control; cited as foundational VLA/robotic transformer work.",
            "citation_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.",
            "mention_or_use": "mention",
            "model_name": "RT-2 / RT-1",
            "model_description": "Transformer-based robotic policies that condition on vision and language to produce actions; RT-2 emphasized transfer of web-scale knowledge into robotic control; RT-1 is an earlier robotics transformer.",
            "pretraining_type": "Large-scale vision-language / web-scale data pretraining for transfer to robotics (as per cited RT-2/RT-1 papers); details are in those references.",
            "pretraining_data_description": "Web-scale image-text and robotic demonstration style datasets used in original RT work; not elaborated here.",
            "target_task_name": "Robotic manipulation and language-conditioned control",
            "target_task_description": "General robotic tasks where web-scale or large pretrained VLM knowledge transfers aid performance.",
            "semantic_alignment": "RT-2 is cited as transferring web knowledge to robotics; QDepth-VLA references such prior VLA work as background but does not analyze RT models directly.",
            "performance_with_language_pretraining": "Not reported in this paper.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not analyzed here.",
            "embedding_space_analysis": "Not analyzed here.",
            "action_grounding_evidence": "Cited as demonstrating the value of large pretrained models for robot control; QDepth-VLA builds on the line of grounding VLMs for actions.",
            "hierarchical_features_evidence": "Not discussed here.",
            "transfer_conditions": "General transfer from web-scale pretraining to robotic tasks is cited as successful in RT-2 literature; this paper does not quantify it.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "RT literature often demonstrates few-shot/zero-shot transfer; QDepth-VLA does not report new zero/few-shot results.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not provided here.",
            "comparison_to_vision_only": "Not provided in this paper.",
            "temporal_dynamics": "Not provided here.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1945.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks.",
            "rating": 2
        },
        {
            "paper_title": "PointVLA: Injecting the 3D World into Vision-Language-Action Models.",
            "rating": 2
        },
        {
            "paper_title": "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models.",
            "rating": 2
        },
        {
            "paper_title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions.",
            "rating": 2
        },
        {
            "paper_title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models.",
            "rating": 2
        },
        {
            "paper_title": "PaliGemma: A versatile 3B VLM for transfer.",
            "rating": 1
        },
        {
            "paper_title": "ùúã 0: A Vision-Language-Action Flow Model for General Robot Control.",
            "rating": 2
        },
        {
            "paper_title": "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model.",
            "rating": 2
        },
        {
            "paper_title": "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration.",
            "rating": 1
        },
        {
            "paper_title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.",
            "rating": 1
        }
    ],
    "cost": 0.027537,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models
16 Oct 2025</p>
<p>Yixuan Li liyixuan223@mails.ucas.ac.cn 
Yuhui Chen chenyuhui2022@ia.ac.cn 
Mingcai Zhou mingcai.zhou@ia.ac.cn 
Haoran Li lihaoran2015@ia.ac.cn </p>
<p>School of Artificial Intelligence
University of the Chinese Academy of Sciences
BeijingChina</p>
<p>Institute of Automation
Chinese Academy of Science
BeijingChina</p>
<p>Institute of Automation
Chinese Academy of Science Beijing Zhongke Huiling Robot Technology Co
BeijingChina</p>
<p>Institute of Automation
Chinese Academy of Science
BeijingChina</p>
<p>QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models
16 Oct 20253359E0C33E9EBDA27860E9713AF0A7D0arXiv:2510.14836v1[cs.CV]Vision-Language-Action modelsQuantized depth predictionSpatial reasoningRobotic manipulation
Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks.However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control.To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task.A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues.Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.</p>
<p>INTRODUCTION</p>
<p>Large vision-language-action (VLA) models [4,8,9,16] have recently emerged as a powerful paradigm for robotic learning.By grounding pre-trained vision-language models (VLMs) [2,31,36] with action-generation capabilities, robots acquire strong generalization across diverse instructions and visual contexts.However, when applied to long-horizon or fine-grained manipulation tasks, these models often exhibit substantial performance degradation [12,30,37,46].The primary reason lies in a persistent gap between semantic understanding and geometric reasoning [13,14].</p>
<p>Without reliable 3D understanding, VLAs often misestimate object positions or gripper-object relations, leading to cascading errors during manipulation [30].Therefore, several recent works have explored incorporating geometric information into VLA models to enable a deeper understanding of the 3D physical environment.These approaches can be broadly generalized into three paradigms: direct 3D feature injection , 2D-projected 3D feature integration and auxiliary 3D information prediction.The first category injects encoded 3D representations, such as point clouds [18] or depth maps [3], into the vision-language backbone or the action head.This strategy typically requires an additional encoder to process 3D features, increasing model complexity and computational cost.While providing explicit geometric cues, it may disrupt the powerful 2D priors learned during large-scale VLM pretraining, leading to degraded visual-language reasoning and understanding.The second category projects 3D features into 2D representations and feeds them into the VLM [19].Although this preserves pretrained 2D priors, it inevitably introduces information loss in the projection process, which can hinder fine-grained manipulation performance.Compared to these two paradigms, enhancing geometric understanding through auxiliary visual prediction tasks, such as future depth maps estimation [42], offers a more promising alternative.This approach not only preserves the strong 2D priors of pretrained VLMs, but also avoids the need for additional sensory inputs during inference, while encouraging the model to learn 3D-consistent spatial reasoning.</p>
<p>However, existing works that employ depth-map-based visual prediction as auxiliary tasks [42] have not achieved consistent performance improvements, and in some cases even indicate that introducing depth prediction as an auxiliary loss can be detrimental to policy learning due to noisy supervision and weak geometric grounding.The key challenges lie in three aspects.Firstly, the supervision quality of depth maps is often limited by insufficient spatial-temporal consistency across frames [10,38], introducing substantial noise that weakens geometric grounding.Secondly, pixel-wise depth regression produces highly redundant learning signals, forcing the model to reconstruct every pixel rather than focusing on salient structural cues essential for manipulation.Thirdly, using a vision-language backbone to predict depth maps may interfere with its pre-trained semantic alignment, potentially degrading multimodal reasoning performance.</p>
<p>To address these challenges, we propose QDepth-VLA, which augments large VLAs by introducing quantized depth prediction as an auxiliary supervision signal.Instead of regressing pixel-wise depth values, QDepth-VLA learns discrete depth representations through vector quantization, capturing salient structural information in a compact and optimization-friendly manner.An independent depth expert is also introduced to predict these quantized depth tokens, enabling the model to leverage geometric cues without interfering with the vision-language backbone's pretrained semantic alignment.Our main contributions are summarized as follows:</p>
<p>(1) We introduce QDepth-VLA, a novel VLA model enhanced with quantized depth information.By integrating a depth prediction task, it internalizes geometric understanding, enabling more accurate reasoning about object spatial relationships.</p>
<p>(2) To facilitate more robust depth learning, we design a specialized Depth Expert that predicts quantized depth tokens rather than raw pixel-level depth maps.This formulation effectively mitigates the impact of depth noise and provides a more compact, optimization-friendly supervision signal for geometry-aware policy learning.(3) Comprehensive experiments on both the Simpler [20] and LIBERO [24] benchmarks demonstrate that QDepth-VLA substantially enhances policy performance, outperforming open  0 [29] by 6.1% and 7.7% on average success rate, respectively.Moreover, QDepth-VLA achieves a 10.0% improvement in real-world robotic manipulation, validating its effectiveness and generalizability.</p>
<p>RELATED WORKS 2.1 3D-Enhanced VLA</p>
<p>3D spatial information has been widely explored to overcome the limitations of purely 2D-based models.Early efforts typically enhanced spatial perception by either lifting 2D inputs into 3D [13][14][15] or directly fusing 2D visual features with 3D point clouds [22,28,39].While these approaches demonstrate that incorporating 3D signals can significantly improve spatial perception and action precision, directly fusing 3D and 2D representations or relying solely on 3D features can disrupt the visual-language alignment established in large-scale VLM pretraining.To mitigate this, two alternative directions have been proposed: (1) Projecting 3D features into 2D space, as in BridgeVLA [19], which renders 3D inputs into multi-view 2D images for compatibility with VLMs.(2) Independent 3D encoders encode geometric information for integration into the action head.This paradigm is employed by PointVLA [18] and GeoVLA [32], where specialized point cloud encoders supply 3D embeddings to modality-specific experts.</p>
<p>Despite these advances, point cloud reconstruction may lose finegrained object details, and the modality gap between 2D RGB pretraining and 3D geometry remains a persistent challenge.By contrast, depth maps exhibit a much smaller gap with RGB images and thus offer a more natural bridge between 2D and 3D.Recent depth-based approaches have demonstrated this advantage.3D-CAVLA [3] integrates Region of Interest (RoI) pooling with depth embeddings projected into VLM token space, achieving extraordinary multiview performance, while 4D-VLA [41] augments visual inputs with 3D coordinate embeddings to support both spatial alignment and temporal reasoning.</p>
<p>Motivated by these insights, we adopt depth maps as the 3D augmentation source.Crucially, instead of directly fusing them with RGB features which risks interfering with pre-trained VLM semantics, we reformulate depth as an auxiliary prediction task.This design enables QDepth-VLA to move beyond passive depth perception toward depth understanding, a capability we elaborate on in the next section.</p>
<p>Auxiliary Visual Reasoning Tasks for VLA</p>
<p>While depth maps offer a natural bridge between 2D and 3D for enhancing spatial grounding, another promising direction is to strengthen the reasoning capacity of VLAs through auxiliary visual prediction tasks.Instead of passively mapping inputs to actions, policies can be trained to output intermediate signals that make future-oriented reasoning explicit, thereby providing richer supervision during training and improving long-horizon planning at inference.</p>
<p>A series of works focus on predicting future sub-goals, such as generating sub-goal images or short rollouts that visualize task progress.This strategy, as exemplified by CoT-VLA [43] , enhances temporal reasoning by conditioning action generation on both current and predicted states, but incurs high computational cost due to the difficulty of synthesizing realistic RGB predictions.Other researches [9,16] introduce object-centric signals, such as bounding boxes or spatial relations, which provide structured knowledge of entities and their interactions.More recently, latent future embeddings have been explored, where discrete action tokens predicted in a compressed latent space encode upcoming intentions.AgiBot World Colosseo [1] and UniVLA [7] exemplify this paradigm, showing scalability through large-scale human video pretraining, yet such latent predictions often lack explicit 3D grounding and struggle to capture fine-grained geometry.Finally, some approaches turn to pixel-level 3D supervision, predicting dense depth or semantic maps to reinforce geometric awareness, as in 3D-VLA [45] and DreamVLA [42].While sometimes effective for strengthening spatial reasoning, these signals are difficult to optimize directly and may overemphasize redundant low-level cues rather than the most relevant spatial structures.</p>
<p>Different from previous works, our approach unifies 3D information enhancement and visual reasoning by introducing depth codebook prediction-an auxiliary task that brings 3D cues into reasoning in a compact and semantically meaningful way, while remaining naturally aligned with language-conditioned action policies.</p>
<p>METHODOLOGY 3.1 Depth Annotation</p>
<p>Since existing VLA datasets such as OXE dataset [27] lack sufficient 3D annotations, we first generate monocular depth estimates for training.To ensure high-quality and spatial-temporal consistent depth sequences, we employ Video-Depth-Anything (ViDA) [10], the current state-of-the-art monocular video depth estimation framework built upon a ViT-Large backbone, to acquire depth maps.Specifically, ViDA is applied to the main-view RGB frames from a subset of the OXE [27] and LIBERO [24] datasets to obtain temporally aligned relative depth annotations, providing reliable geometric supervision for depth tokenization and subsequent model training.</p>
<p>VQ-VAE Reconstruction</p>
<p>To represent depth compactly, we pretrain a Vector-Quantized Variational Autoencoder (VQ-VAE) [34].Given a depth frame x, the encoder   (‚Ä¢) produces a latent z  =   (x), which is quantized to the
z ùëû = c ùëó * , ùëó * = arg min ùëó ‚à•z ùëí ‚àí c ùëó ‚à• 2 2 .(1)
We use  = 256 codebook entries of dimension  = 160, and train the VQ-VAE [34] with the standard objective:
L vq = ‚Ñì rec (x, ùëî ùúô (z ùëû )) reconstruction + ‚à•sg[z ùëí ] ‚àí c ùëó * ‚à• 2 2 codebook update +ùõΩ ‚à•z ùëí ‚àí sg[c ùëó * ] ‚à• 2 2 commitment ,(2)
where sg[‚Ä¢] denotes stop-gradient and  = 0.25.In practice, we experiment with latent grid resolutions of 16√ó16 and 32√ó32.We find that the smaller 16√ó16 configuration already achieves accurate depth reconstruction while remaining computationally efficient.The VQ-VAE [34] is pretrained independently on each dataset using AdamW [25] with a learning rate of 1 √ó 10 ‚àí5 to ensure stable convergence and reconstruction quality.The resulting pretrained model produces discretized depth code indices, which serve as supervisory targets for the depth expert in QDepth-VLA.</p>
<p>QDepth-VLA Architecture</p>
<p>QDepth-VLA adopts a unified and modular architecture built upon open  0 [29], extending its VLA pipeline with an additional depth supervision branch.As shown in Fig. 1(a), the model consists of three parameterized modules: a pretrained vision-language model (VLM), an action expert, and a newly introduced depth expert.These modules are coordinated through a mixture-of-experts (MoE) structure We choose PaliGemma-3B [2] as VLM backbone, which integrates SigLIP-based [40] vision encoding with Gemma's [26] language modeling capability.Input instructions are first tokenized using Gemma's [26] tokenizer, while the main-view RGB image is processed by the SigLIP [40] image encoder to obtain 256 visual tokens.These image tokens are concatenated with 20 text prefix tokens and fed into the Gemma [26] decoder under full block attention to produce multimodal embeddings that capture both spatial and semantic cues.This pretrained VLM remains trainable during the training stage, allowing geometric adaptation to manipulation environment.</p>
<p>The action expert is a transformer-based module responsible for translating multimodal embeddings and proprioceptive states into executable robot actions.It consists of stacked transformer layers with MLP-based encoders and decoders that integrate visual-language context from the VLM with proprioceptive features.This module, which is built upon the original open  0 [29] action head, functions as the core control head of QDepth-VLA.</p>
<p>To incorporate geometric reasoning, QDepth-VLA introduces a dedicated depth expert, architecturally aligned with the action expert (illustrated in Table 1).It takes the visual embeddings from the SigLIP encoder as input, before language fusion to avoid semantic interference.These embeddings are projected through a lightweight MLP, processed by a transformer backbone, and then passed to a shallow CNN decoder that predicts 256 depth tokens.Each predicted token corresponding to a latent vector is then aligned with the quantized tokens produced by the pretrained VQ-VAE [34] encoder over its codebook.The pretrained VQ-VAE [34] decoder is subsequently used to reconstruct the spatial depth map from these latent tokens when required.This discrete formulation enables QDepth-VLA to capture compact, structured geometric representations while maintaining optimization stability.</p>
<p>As for hybrid attention mechanism, existing designs typically employ a standard causal attention structure, as seen in DreamVLA [42] and CoT-VLA [43].However, since depth modalities inherently contain noise, directly fusing them under causal attention may introduce undesirable interference, potentially degrading action generation quality [42].To address this issue, we redesign the hybrid attention mechanism (Fig. 1(b)) to more effectively regulate crossmodal information flow among text, image, depth, proprioception, and action tokens.To be specific:</p>
<p>(1) Text and image tokens attend only within their modality to preserve pretrained semantic grounding.</p>
<p>(2) Depth tokens attend to both image and text tokens, contextualizing geometric features with visual semantics.</p>
<p>(3) Action tokens attend to all preceding modalities, integrating fused perceptual and geometric cues for policy generation.</p>
<p>This hierarchical attention design allows depth to enhance spatial understanding while preventing over-interference with the pretrained VLM and keeping computation efficient.</p>
<p>Co-Training Procedures
ùìÅ ùëñ,ùëò = ‚àí 1 ùúè ‚à•ùë• ùëñ ‚àí ùëê ùëò ‚à• 2 2 ,(3)
where  indexes latent spatial positions ,  indexes codebook entries ( = 256) and  represents temperature factor.A cross-entropy loss is applied using ground-truth code indices  *  obtained from the pretrained VQ-VAE [34]:
L depth = ‚àí 1 ùêµ ‚Ä¢ ùëÅ ùêµ‚Ä¢ùëÅ ‚àëÔ∏Å ùëñ=1 log exp(ùìÅ ùëñ,ùëß * ùëñ ) ùêæ ùëò=1 exp(ùìÅ ùëñ,ùëò ) , (4)
where  is batch size and  the number of latent tokens per frame.This loss encourages the visual encoder to learn geometry-aware embeddings aligned with the quantized depth representation.</p>
<p>Action</p>
<p>Modeling.Based on the underlying VLA backbone, the action prediction objective is as follows:</p>
<p>The Conditional Flow Matching (CFM) action loss [23] is identical to that of  0 [4]:
L CFM (ùúÉ ) = E ùëù (ùê¥ùë° |ùëÇùë° ), ùëû ( √ÇùúÜ ùë° |ùê¥ùë° ) ‚à•ùëì ùúÉ ( √ÇùúÜ ùë° , ùëÇ ùë° ) ‚àí ùëî( √ÇùúÜ ùë° |ùê¥ ùë° ) ‚à• 2 2 ,(5)
where the action chunk   = [  ,   +1 , . . .,   + ‚àí1 ] is conditioned on the observation   = [  , ‚Ñì  ,   ], which includes the RGB image, language instruction, and end-effector state.Notably, √Ç  denotes noisy action samples generated from a diffusion-like process:
√ÇùúÜ ùë° = ùúÜùê¥ ùë° + (1 ‚àí ùúÜ)ùúÇ, ùúÇ ‚àº N (0, ùêº ),(6)
and the corresponding noise distribution and flow target are defined as:
ùëû( √ÇùúÜ ùë° | ùê¥ ùë° ) = N (ùúÜùê¥ ùë° , (1 ‚àí ùúÜ)ùêº ), ùëî( √ÇùúÜ ùë° | ùê¥ ùë° ) = ùúÇ ‚àí ùê¥ ùë° .(7)
This formulation enables the model to approximate a continuoustime flow field that transports noisy actions toward their clean ground-truth counterparts.</p>
<p>Co-Training</p>
<p>Objectives.The total loss combines the action and depth objectives:
L total = L action + ùúÜ ùë° ‚Ä¢ L depth ,(8)
where   =  0 ‚Ä¢   exponentially decays over training steps, with  0 = 0.01.This co-training schedule enables the model to first establish stable geometric alignment before gradually focusing on action refinement.</p>
<p>Optimization Setup.</p>
<p>QDepth-VLA is trained using the AdamW [25] optimizer with decoupled weight decay.We set the learning rate for both the action expert and the VLM backbone to 5 √ó 10 ‚àí5 .A cosine learning rate scheduler with 200 warm-up steps and a cycle length of 10 7 steps is applied, ensuring stable optimization throughout training.</p>
<p>EXPERIMENTS</p>
<p>In this section, we conduct comprehensive experiments across both simulation and real-world settings to evaluate the effectiveness of our approach.Specifically, we aim to address the following three questions:</p>
<p>(1) Can depth supervision effectively enhance VLA performance in long-horizon and pick-and-place tasks, particularly those requiring fine-grained manipulation?</p>
<p>(2) Is depth supervision more effective than pixel-level depth prediction?</p>
<p>(3) Does the proposed hybrid attention mask contribute to performance gains?[29] is initially pre-trained for 9 epochs on the Fractal dataset [6], followed by 20 epochs of pre-training on the LIBERO-90 dataset [24].After pre-training, the model is further fine-tuned on the four LIBERO subsets -Spatial, Object, Goal, and Long [24] for around  [20], the model is instead trained from scratch, first using the Bridge dataset [35] for 13 epochs, and then the Fractal dataset [6] for an additional 9 epochs.All experiments are conducted using the Fully Sharded Data Parallel (FSDP) training strategy on 8 √ó NVIDIA H20 GPUs.A per-GPU batch size of 32 is used, yielding a global batch size of 1024 with gradient accumulation, and the action chunk size is fixed at 4.</p>
<p>Simulation Experiments</p>
<p>Evaluation Setup.</p>
<p>For evaluation on LIBERO [24], we adopt its four benchmark suites (Spatial, Object, Goal, and Long).Following the preprocessing method in [17], image resolution is first normalized to 256 √ó 256 and then resized to 224 √ó 224 as model input.We also apply a 180-degree rotation to all images and use only the main-view RGB observations.Each task is evaluated over 50 rollouts, with the average success rate reported.</p>
<p>On the Simpler benchmark [20], the evaluation covers two distinct settings: (1) models trained on the Bridge dataset [35] are tested on tasks involving the WidowX250 robot, and (2) models trained on the Fractal dataset [6] are tested on tasks for the Google Robot.We adopt the visual matching configuration from Simpler [20], evaluating each task across multiple initial positions with 10 rollouts per configuration.Consequently, the total number of evaluations per task ranges from 240 to 2400.</p>
<p>Main Results.</p>
<p>LIBERO Benchmark.QDepth-VLA adopts a single-view setting, where the visual input consists of only one RGB image.This contrasts with multi-view models, which take multiple images as input, including temporally adjacent frames from historical observations.</p>
<p>As shown in Table 2, QDepth-VLA consistently outperforms single-view baselines across the LIBERO suites.It achieves stronger performance on both fine-grained and long-horizon tasks, reaching 94.0% on the Goal tasks and 72.6% on the Long tasks, surpassing the single-view baseline CoT-VLA [43] by 6.4% and 3.6%, respectively.Compared with open  0 [29], QDepth-VLA shows consistent improvements across all four subsets (Spatial, Object, Goal, and Long), with the largest gain of 8.8% observed on the Spatial tasks.</p>
<p>While QDepth-VLA operates with only a single RGB observation, its average success rate remains competitive with multi-view VLAs.Specifically, QDepth-VLA achieves a mean success rate only 0.1% lower than  0 -FAST [29], while exceeding 4D-VLA [41] by 3.1% and DreamVLA [42] by 4.5% on the Goal tasks.Moreover, it surpasses  0 -FAST [4] by 12.4% on the more challenging Long tasks.Although leading multi-view models such as 3D-CAVLA [3], GeoVLA [32] and UniVLA [7] achieve higher overall results, our experimental results demonstrate that depth-augmented supervision effectively compensates for the lack of multi-view observations and brings single-view VLAs closer to multi-view performance levels.</p>
<p>By extension, we further implement a multi-view variant of QDepth-VLA while maintaining the same setting that predicts latent depth tokens corresponding only to the current main-view image.As shown in Table 2, the multi-view QDepth-VLA consistently outperforms single-view baselines.It achieves an average success rate of 94.9%, surpassing DreamVLA by 0.1% and  0 by 0.8% on the Spatial tasks, and reaches 90.0% success on the Long tasks.While 3D-CAVLA [3] and GeoVLA [32] achieve higher average success rates, they require explicit point cloud or depth map inputs during inference -modalities that QDepth-VLA does not rely on.</p>
<p>These results reveal that the QDepth-VLA generalizes effectively to multi-view configurations, further enhancing geometric perception and long-horizon reasoning.</p>
<p>Simpler Benchmark.Tables 3 and 4 present the experimental results of QDepth-VLA in the Simpler [20] simulation environment.</p>
<p>As shown in Table 3, QDepth-VLA achieves a success rate of 98.3% on the pick coke can task, surpassing open  0 [29] by 0.8% and SpatialVLA [28] by 12.3%.On the more complex open top drawer and put apple in task, QDepth-VLA also attains 62.6%, outperforming open  0 [29] by a large margin of 29.7%.This substantial improvement on long-horizon tasks can be attributed to enhanced spatial perception and object localization provided by depth-guided supervision, which improves the model's ability to accurately identify and grasp target objects.As a result, the success probability of intermediate manipulation steps-such as grasping or placing-is increased, leading to higher overall task completion rates.</p>
<p>In Table 4, QDepth-VLA consistently achieves high success rates across various manipulation tasks.Notably, on the stack block task-which demands precise spatial reasoning and fine-grained control-QDepth-VLA reaches a success rate of 39.6%, surpassing SpatialVLA [28] by 10.4%.Furthermore, on the Put Eggplant in Basket and Put Spoon on Towel tasks, QDepth-VLA achieves 95.0% and 82.0%success rates, respectively, outperforming open  0 [29] by 5.4% and 8.3%.These improvements highlight the effectiveness of quantized depth supervision in enhancing spatial reasoning and manipulation precision, particularly for tasks involving object placement and coordination in cluttered 3D environments.</p>
<p>Depth Reconstruction Visualization .To further validate the effectiveness of our depth supervision, we visualize depth reconstructions by passing the quantized predicted features through a trained VQ-VAE [34] decoder.As shown in Fig. 3, the reconstructions preserve structural details and align well with object boundaries, demonstrating that the learned depth representations capture spatial geometry in a meaningful way.For each task, we collect 50 trajectories and fine-tune the model separately on the corresponding dataset.During testing, each task is evaluated over 10 trials.As shown in Fig. 2, all experiments are conducted on a dark-colored wooden desk, where the surface color is visually similar to the robot gripper.This setup increases the difficulty of the tasks by introducing additional perceptual ambiguity.</p>
<p>Real-Robot Experiments</p>
<p>Main Results</p>
<p>. We evaluate QDepth-VLA on a series of pick-and-place tasks with varying difficulty to assess its spatial perception and localization ability.We also compare our method against representative baselines such as ACT [44].As shown in Table 5, QDepth-VLA consistently outperforms ACT [44] across all tasks, while ACT [44] fails to perform reliably in these complex real-world environments, QDepth-VLA achieves robust success rates.Compared to our baseline open  0 [29], QDepth-VLA achieves a 20.0% improvement on the simple task of picking a banana, and further achieves gains of 10.0% on both Task 3 and Task 4, demonstrating stronger generalization to challenging scenarios and tasks.</p>
<p>Ablation Study</p>
<p>To evaluate the contribution of each proposed component, we perform a series of controlled ablation experiments in the Simpler [20] simulation environment.Four ablated variants are considered: (1) removing the depth supervision signal by setting its loss weight to zero (w/o Depth Loss); (2) removing the dedicated depth prediction branch (w/o Depth Expert); (3) replacing latent depth token prediction with pixel-wise regression (w/o Pixel Prediction); and (4) substituting the proposed hybrid attention mask with a standard version that enforces proprioception-to-depth attention (w/o Hybrid Attn).Quantitative results are reported in Table 6.6, performance decreases from 68.5% to 65.6% on average.The degradation is most pronounced in the Carrot (-9.6%) and Eggplant (-12.5%)tasks, both requiring coarse spatial grounding.Conversely, slight improvements are observed in Spoon (+7.2%) and Block (+3.3%) tasks, suggesting that the auxiliary depth objective can occasionally compete with the action policy optimization.Overall, these results confirm that depth supervision provides a meaningful geometric prior that facilitates spatial reasoning beyond the primary control objective.</p>
<p>w/o Depth Expert.</p>
<p>Eliminating the dedicated depth branch results in the largest overall performance degradation (-8.5%), as presented in Table 6.The most significant drop occurs in the Stack Block Task (-23.8%),where precise 3D alignment is critical.Substantial declines are also observed in the Eggplant (-5.4%) and Spoon (-8.3%) tasks, indicating that fine-grained spatial reasoning relies heavily on an explicit and specialized depth pathway.6, replacing latent depth prediction with direct pixel-wise regression lowers average performance to 64.6% (-3.9%).The largest impact is on Eggplant (-14.6%)task, whereas other tasks are only mildly affected.This validates our design choice: quantized latent tokens encourage abstraction of geometric cues, while pixel prediction entangles the model with redundant local detail that is less relevant for manipulation.</p>
<p>w/o Pixel Prediction. As shown in Table</p>
<p>w/o Hybrid Attention.</p>
<p>In this variant, the proposed hybrid attention mask is replaced with a DreamVLA-style [42] configuration, removing dynamic and semantic modalities.This setting tests whether relative depth maps can enhance proprioceptive state perception and thereby improve action generation quality.As expected, the Overall, we find the following answers to the three research questions brought up at the beginning of this section,</p>
<p>‚Ä¢ Depth supervision effectively enhances VLA performance, especially on long-horizon and fine-grained pick-and-place tasks.In particular, tasks such as stacking and precise placement benefit significantly, indicating improved spatial reasoning.‚Ä¢ Compared to pixel-level regression, quantized depth supervision proves more effective, as it reduces redundancy and focuses learning on salient geometric structures, leading to more stable training and stronger downstream performance.‚Ä¢ The proposed hybrid attention mask consistently contributes to performance gains, particularly in placement tasks, by selectively routing depth cues into the policy network and improving cross-modal feature alignment.</p>
<p>CONCLUSIONS</p>
<p>In this paper, we introduced QDepth-VLA, a new vision-languageaction model that incorporates depth supervision and hybrid attention to enhance spatial perception and long-horizon reasoning.Through extensive experiments in both simulation (Simpler and LIBERO) and real-world manipulation tasks, we demonstrate that depth supervision significantly improves manipulation performance.In summary, our work demonstrates that predicting quantized depth tokens at the current timestep is an effective way to enhance policy learning.</p>
<p>Extending this approach to predict future depth tokens for improved reasoning and exploring more efficient VAE-based depth representations for enhanced perception present two promising directions for future research.</p>
<p>Figure 1 :
1
Figure 1: An overview of QDepth-VLA.(a) The overall architecture and training pipeline, where depth supervision is incorporated via a depth expert and latent prediction module.In co-training, the VQ-VAE [34] encoder and codebook are frozen, while PaLI-Gemma 3B [2], the action expert, depth expert, SigLIP [40], and tokenizer are trainable.(b) The proposed hybrid attention mask, which integrates depth and visual tokens to enhance spatial reasoning and manipulation performance.</p>
<ol>
<li>
<p>4 . 1
41
Quantized Depth Supervision.During joint training, the depth expert predicts latent depth tokens, and these tokens are then used to compute logits with the encoded image features over the VQ-VAE[34] codebook:</p>
</li>
<li>
<p>1 . 1
11
Training Recipe.The QDepth-VLA based on open  0</p>
</li>
<li>
<p>2 . 1
21
Environment Setup.In the real-world experiments, we employ a 6-DoF Piper robotic arm, with a RealSense D455 camera positioned directly in front of the arm.The training hyperparameters</p>
</li>
</ol>
<p>Figure 2 :
2
Figure 2: An overview of the main camera view in our real-world task.The environment presents significant challenges for policy learning, including complex lighting conditions, low visual contrast between the gripper and tabletop, and various environmental disturbances that can obscure critical geometric details.</p>
<p>Figure 3 :
3
Figure 3: Depth reconstruction results from the QDepth-VLA.Features predicted by depth expert are decoded using a trained VQ-VAE decoder.The reconstructions demonstrate QDepth-VLA's ability to learn critical depth map features, including object and gripper boundaries, underscoring the success of its depth supervision.</p>
<ol>
<li>3 .
3
1 w/o Depth Loss.In this variant, the depth loss weight is set to zero while preserving the full model capacity.This configuration isolates the contribution of the depth supervision signal without altering the overall parameter scale.As shown in Table</li>
</ol>
<p>Table 1 :
1
Key configurations of the Action and Depth experts of QDepth-VLA.
QDepth-VLAAction ExpertDepth ExpertBackboneTransformerTransformerLayers / Heads18 / 818 / 8Hidden dim10241024Interm. dim40964096InputsProprio + Action RGB-Img tokensOutputsActionsDepth tokensand a carefully designed hybrid attention mask, enabling QDepth-VLA to jointly reason about geometry and control variants withoutdisrupting pretrained representations.</p>
<p>Table 2 :
2
Results of QDepth-VLA on the LIBERO benchmark.
View SettingCategoryMethodSpatial Object Goal Long AvgOpenVLA finetuned [17]84.788.479.2 53.7 76.5General VLACoT-VLA-7B [43]87.591.687.6 69.0 81.1Single-view VLAOpen ùúã 0 [29]77.284.083.6 66.0 77.73D-cloud-enhanced VLA SpatialVLA [28]88.289.978.6 55.5 78.1Depth-enhanced VLA3D-CAVLA [3] QDepth-VLA (ours)86.1 86.094.7 88.882.9 66.8 82.6 94.0 72.6 85.4Diffusion Policy [11]78.392.568.3 50.5 72.4Octo finetuned [33]78.985.784.6 51.1 75.1General VLAùúã 0 -FAST finetuned [4]96.496.888.6 60.2 85.5ùúã 0 finetuned [4]96.898.895.8 85.2 94.2Multi-view VLAUniVLA [7]96.596.895.6 92.0 95.23D-cloud-enhanced VLA GeoVLA [32]98.499.096.6 96.6 97.73D-CAVLA [3]98.299.898.2 96.1 98.1Depth-enhanced VLA4D-VLA [41] DreamVLA [42]88.9 97.595.2 94.090.9 79.1 88.6 89.5 89.5 92.6QDepth-VLA (ours)97.696.695.2 90.0 94.950 epochs. For the Simpler benchmark</p>
<p>Table 3 :
3
Results of QDepth-VLA on Simpler benchmark(Google Robot tasks) Pick Coke Can Move Near Open/Close Drawer Open Top Drawer and Put Apple In Avg
RT-2-X [5]78.777.925.0-60.7Octo-Base [33]17.04.222.7-16.8OpenVLA [17]16.346.235.6-27.7RoboVLM finetuned [21]77.361.743.5-63.4SpatialVLA finetuned [28]86.077.957.4-75.1Open ùúã 0 [29]97.587.168.032.971.4QDepth-VLA(ours)98.381.458.062.675.1</p>
<p>Table 4 :
4
Results of QDepth-VLA on Simpler benchmark(WidowX250 Robot tasks) Put Carrot on Plate Put Eggplant in Basket Put Spoon on Towel Stack Block Avg
Octo-Base [33]8.343.112.50.016.0OpenVLA [17]0.04.10.00.01.0RoboVLM finetuned [21]25.058.329.212.531.3SpatialVLA finetuned [28]25.0100.016.729.242.7Open ùúã 0 [29]61.389.673.715.860.0QDepth-VLA(ours)57.595.082.039.668.5</p>
<p>Table 6 :
6
Ablation study of QDepth-VLA on Simpler tasks.The table reports success rates (%) with module ablations.A checkmark (‚úì) indicates the module is present, while a cross (‚úó) indicates it is removed, changed or set to zero manually.5% on average, with the most substantial drop on the Carrot Task (-15.8%).This result indicates that enforcing proprioception-to-depth attention introduces noise rather than useful guidance, as relative depth lacks absolute positional encoding necessary for stable control.4.3.5Cross-tasksynthesis.Across all ablations, two consistent trends emerge.First, stacking tasks exhibit exceptional sensitivity to the removal of the depth expert, confirming that explicit depth modeling is crucial for accurate vertical alignment and object interaction.Second, placement-oriented tasks such as Carrot and Eggplant tasks benefit most from depth supervision and hybrid attention routing, which jointly enhance mid-level spatial localization and task-level consistency.
ModelDepth Loss Depth Expert Latent Pred Hybrid AttnTasksAvgCarrot Eggplant Spoon BlockQDepth-VLA (full)‚úì‚úì‚úì‚úì57.595.082.0 39.6 68.5w/o Depth Loss‚úó‚úì‚úì‚úì47.982.589.2 42.9 65.6w/o Depth Expert‚úì‚úó‚úì‚úì61.389.673.7 15.8 60.0w/o Pixel Prediction‚úì‚úì‚úó‚úì54.680.482.0 41.3 64.6w/o Hybrid Attn[42]‚úì‚úì‚úì‚úó41.789.678.8 42.0 63.0performance declines by 5.</p>
<p>Qingwen Agibot-World-Contributors, Jisong Bu, Li Cai, Xiuqi Chen, Yan Cui, Siyuan Ding, Shenyuan Feng, Xindong Gao, Xu He, Shu Huang, Yuxin Jiang, Cheng Jiang, Hongyang Jing, Jialun Li, Chiming Li, Yi Liu, Yuxiang Liu, Jianlan Lu, Ping Luo, Yao Luo, Yuehan Mu, Yixuan Niu, Jiangmiao Pan, Yu Pang, Guanghui Qiao, Ren, Jiaqi Cheng-Xing Ruan, Yongjian Shan, Chengshi Shen, Mi Shi, Modi Shi, Chonghao Shi, Jia-Yi Sima, Huijie Song, Wenhao Wang, Dafeng Wang, Chengen Wei, Guo-Liang Xie, Junchi Xu, Cunbiao Yan, Lei Yang, Shu-Xiang Yang, Maoqing Yang, Jiansheng Yao, Chi Zeng, Qingli Zhang, Bin Zhang, Chengyu Zhao, Jiaqi Zhao, Jianchao Zhao, Zhu, ArXiv abs/2503.06669AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems. 2025. 2025276902669</p>
<p>Lucas Beyer, Andreas Steiner, Andr√© Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel M Salz, Maxim Neumann, Ibrahim M Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey A Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Martin Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bovsnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, ArXiv abs/2407.07726PaliGemma: A versatile 3B VLM for transfer. Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, Xiao-Qi Zhai, 2024. 2024</p>
<p>3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks. V S K Pandi, Yu-Hsiang Bhat, Prashanth Lan, Ramesh Krishnamurthy, Farshad Karri, Khorrami, ArXiv abs/2505.058002025. 2025</p>
<p>Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky, ArXiv abs/2410.24164ùúã 0: A Vision-Language-Action Flow Model for General Robot Control. 2024. 2024</p>
<p>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Krzysztof Choromanski, Tianli Ding, ArXiv abs/2307.158182023. 2023</p>
<p>RT-1: Robotics Transformer for Real. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, ArXiv abs/2212.068172022. 2022</p>
<p>UniVLA: Learning to Act Anywhere with Task-centric Latent Actions. Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, Hongyang Li, ArXiv abs/2505.061112025. 2025278481174</p>
<p>GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, Minzhao Zhu, ArXiv abs/2410.061582024. 2024</p>
<p>. Chi-Lam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, ArXiv abs/2507.154932025Jiafeng Xu, and Yichu Yang. 2025. GR-3 Technical Report</p>
<p>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos. Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2025. 2025. 2025</p>
<p>Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, Shuran Song, ArXiv abs/2303.04137Diffusion Policy: Visuomotor Policy Learning via Action Diffusion. 2023. 2023</p>
<p>Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation. Yiguo Fan, Pengxiang Ding, Shuanghao Bai, Xinyang Tong, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Siteng Huang, Zhaoxin Fan, Badong Chen, Donglin Wang, ArXiv abs/2508.199582025. 2025280919002</p>
<p>Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation. Th√©ophile Gervet, Nikolaos Zhou Xian, Katerina Gkanatsios, Fragkiadaki, Conference on Robot Learning. 2023</p>
<p>RVT-2: Learning Precise Manipulation from Few Demonstrations. Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter Fox, ArXiv abs/2406.085452024. 2024</p>
<p>RVT: Robotic View Transformer for 3D Object Manipulation. Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, Dieter Fox, ArXiv abs/2306.148962023. 2023259262273</p>
<p>Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin Leblanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, Ury Zhilinsky, ArXiv abs/2504.16054ùúã 0.5: a Vision-Language-Action Model with Open-World Generalization. 2025. 2025</p>
<p>OpenVLA: An Open-Source Vision-Language-Action Model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag R Lam, Quan Sanketi, Thomas Vuong, Benjamin Kollar, Russ Burchfiel, Dorsa Tedrake, Sergey Sadigh, Percy Levine, Chelsea Liang, Finn, ArXiv abs/2406.092462024. 2024</p>
<p>PointVLA: Injecting the 3D World into Vision-Language-Action Models. Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, Yichen Zhu, ArXiv abs/2503.075112025. 2025</p>
<p>BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models. Peiyan Li, Yixiang Chen, Hongtao Wu, Xiao Ma, Xiangnan Wu, Yan Huang, Liang Wang, Tao Kong, Tieniu Tan, ArXiv abs/2506.079612025. 2025</p>
<p>Evaluating Real-World Robot Manipulation Policies in Simulation. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Ho Vuong, Ted Xiao, ArXiv abs/2405.059412024. 2024</p>
<p>Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu, ArXiv abs/2412.14058Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models. 2024. 2024</p>
<p>Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding. Tao Lin, Gen Li, Yilei Zhong, Yanwen Zou, Bo Zhao, ArXiv abs/2507.004162025. 2025</p>
<p>Flow Matching for Generative Modeling. Yaron Lipman, Ricky T Q Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le, ArXiv abs/2210.027472022. 2022</p>
<p>Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qian Liu, Yuke Zhu, Peter Stone, ArXiv abs/2306.03310LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning. 2023. 2023</p>
<p>Decoupled Weight Decay Regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2017</p>
<p>Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, arXiv:2403.08295abs/2403.08295Gemma: Open Models Based on Gemini Research and Technology. 2024. 2024268379206arXiv preprint</p>
<p>Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, ArXiv abs/2310.08864Open X-Embodiment: Robotic Learning Datasets and RT-X Models. 2023. 2023</p>
<p>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yani Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, Xuelong Li, ArXiv abs/2501.15830SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model. 2025. 2025</p>
<p>open-pi-zero: Re-implementation of the ùúã 0 vision-languageaction (VLA) model. Allen Z Ren, 2024</p>
<p>Re-conVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver. Wenxuan Song, Ziyang Zhou, Han Zhao, Jiayi Chen, Pengxiang Ding, Haodong Yan, Yuxin Huang, Feilong Tang, Donglin Wang, Haoang Li, ArXiv abs/2508.103332025. 2025</p>
<p>Andreas Steiner, Andr√© Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, R Reeve Siyang Qin, Emanuele Ingle, Sahar Bugliarello, Thomas Kazemzadeh, Ibrahim M Mesnard, Lucas Alabdulmohsin, Xiao-Qi Beyer, Zhai, ArXiv abs/2412.03555A Family of Versatile VLMs for Transfer. 2024. 20242</p>
<p>GeoVLA: Empowering 3D Representations in Vision-Language-Action Models. Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, Jiale Cao, ArXiv abs/2508.090712025. 2025</p>
<p>Octo Model Team, Dibya Ghosh, Homer Rich Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Pannag R Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine, ArXiv abs/2405.12213Octo: An Open-Source Generalist Robot Policy. 2024. 2024</p>
<p>Neural Discrete Representation Learning. A√§ron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, Neural Information Processing Systems. 2017</p>
<p>Homer Rich Walke, Kevin Black, Abraham Lee, Jin Moo, Maximilian Kim, Chongyi Du, Tony Zheng, Philippe Zhao, Quan Ho Hansen-Estruch, Andre Wang Vuong, Vivek He, Kuan Myers, Chelsea Fang, Sergey Finn, Levine, BridgeData V2: A Dataset for Robot Learning at Scale. In Conference on Robot Learning. 2023</p>
<p>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Ke-Yang Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin, ArXiv abs/2409.121912024. 2024</p>
<p>RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation. Liudi Yang, Yang Bai, George Eskandar, Fengyi Shen, Mohammad Altillawi, Dong Chen, Soumajit Majumder, Ziyuan Liu, Gitta Kutyniok, Abhinav Valada, ArXiv abs/2506.220072025. 2025</p>
<p>Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao, ArXiv abs/2406.09414Depth Anything V2. 2024. 2024270440448</p>
<p>FP3: A 3D Foundation Policy for Robotic Manipulation. Rujia Yang, Geng Chen, Chuan Wen, Yang Gao, ArXiv abs/2503.089502025. 2025</p>
<p>Sigmoid Loss for Language Image Pre-Training. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, IEEE/CVF International Conference on Computer Vision (ICCV). 2023. 2023. 2023</p>
<p>4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration. Jiahui Zhang, Yurui Chen, Yueming Xu, Ze Huang, Yanpeng Zhou, Yuan Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang, ArXiv abs/2506.222422025. 2025280010742</p>
<p>DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge. Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, He Wang, Zhizheng Zhang, Li Yi, Wenjun Zeng, Xin Jin, ArXiv abs/2507.044472025. 2025</p>
<p>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models. Qingqing Zhao, Yao Lu, Jin Moo, Zipeng Kim, Zhuoyang Fu, Yecheng Zhang, Zhaoshuo Wu, Qianli Li, Song Ma, Chelsea Han, Ankur Finn, Ming-Yu Handa, Donglai Liu, Gordon Xiang, Tsung-Yi Wetzstein, Lin, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2025. 2025. 2025</p>
<p>Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware. Tony Zhao, Vikash Kumar, Sergey Levine, Chelsea Finn, ArXiv abs/2304.137052023. 2023</p>
<p>Xiaowen Haoyu Zhen, Peihao Qiu, Jincheng Chen, Xin Yang, Yilun Yan, Yining Du, Chuang Hong, Gan, ArXiv abs/2403.096313D-VLA: A 3D Vision-Language-Action Generative World Model. 2024. 2024</p>
<p>Bridging VLM and KMP: Enabling Fine-grained robotic manipulation via Semantic Keypoints Representation. Junjie Zhu, Huayu Liu, Jin Wang, Bangrong Wen, Kaixiang Huang, Xiao-Fei Li, Haiyun Zhan, Guodong Lu, ArXiv abs/2503.027482025. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4717 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4717</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4717</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-3c585441b4607b34f8bf4e352ed6e36753fe21ce</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3c585441b4607b34f8bf4e352ed6e36753fe21ce" target="_blank">Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Here it is shown that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building"gap 0"models.</p>
                <p><strong>Paper Abstract:</strong> We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building"gap 0"models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4717.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4717.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-weight state-of-the-art large language model accessed via API; evaluated in this paper as a top performer on static MATH problems but exhibiting a substantial drop on functionalized variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-weight SOTA model (OpenAI GPT-4) accessed via API in the experiments; architecture and training details are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Wide variety: simple arithmetic (rounding, squares), multi-digit arithmetic, factorial/leading-digit reasoning, combinatorics and counting, probability, multi-digit multiplication, algebraic minimization, polynomial-root reasoning, radical/equation solving, symbolic simplification and small-scale symbolic algebra.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Mixture of genuine stepwise (symbolic) reasoning on simpler tasks and memorization/pattern-matching on static dataset items; some arithmetic successes may rely on implicit recall of training examples or learned heuristics rather than algorithmic multi-step computation. Tool-augmentation or specialized prompting (CoT/CoC/ToT) is hypothesized to reveal more true reasoning and reduce the memorization component.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Substantial reasoning gap: GPT-4 solved 1299 static MATH items (25.98% static accuracy) but only achieved 541/5000 (10.82%) functional accuracy over static + 3 snapshots, yielding a 58.35% reasoning gap; 302 fully-generalizable problems were solved across snapshots (indicating these were likely solved by reasoning rather than memorization), and many of those solved problems are simple calculations or operator-property uses consistent with explicit reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Large drop from static to functional variants indicates many static correct answers were due to memorization or dataset contamination rather than robust arithmetic reasoning; failures on functional snapshots and on problems instantiated with large numeric values suggest limits of in-context algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Static accuracy on MATH subset: 25.98% (1299/5000); Functional accuracy (static + 3 snapshots): 10.82% (541/5000); Reasoning gap: 58.35%; Of functional-correct items, 302 were genuinely generalizable across snapshots (6.04% accuracy attributable to snapshot-generalizable solutions) and 239 were ungeneralizable/static (4.78% attributable to static-only items).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No internal probing/ablation was run on GPT-4 in this paper. The paper reports that evaluating across 3 independent functional snapshots stabilizes the measured reasoning gap (i.e., 3 snapshots are sufficient to distinguish memorization from reasoning). The authors also note that specialized prompting (CoT, ToT, CoC, etc.) and tool use are expected to reduce the gap but were not systematically applied in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Large reasoning gap on functional variants indicates failure to generalize beyond static items; susceptibility to dataset contamination/memorization; sometimes reports 'NO SOLUTION' when uncertain; potential inability to handle very large instantiated numeric values (big-number failures); possible evaluator matching issues (formatting differences) and unknown/opaque tool usage in closed API pipeline; default chain-of-thought formatting may not align with evaluation harness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Best-performing model in this paper: lowest reasoning gap (58.35%) among tested models (others ranged up to 80.31%). GPT-4 outperforms closed-model peers and OSS models on both static and functional metrics but still shows a large absolute gap indicating nontrivial memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4717.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4717.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-weight language model (GPT-3.5 family) included among the evaluated SOTA closed models; shows a substantial reasoning gap between static and functional MATH performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-weight model (OpenAI GPT-3.5) accessed via API for evaluation; architecture and training specifics are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same MATH categories as other models: grade-school arithmetic, combinatorics, probability, algebraic reasoning, and multi-step symbolic arithmetic problems drawn from the functionalized MATH subset.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Likely relies more heavily on pattern matching/memorization for static items and less on robust algorithmic multi-step arithmetic; may be improved by specialized prompting or tool augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Part of the reported group of closed models that exhibit large reasoning gaps (aggregate gap range reported as roughly 58–80% across closed and OSS models). The paper reports that many static successes do not transfer to functional snapshots, consistent with memorization over the training snapshot.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not directly probed in this paper; the observed drop between static and functional variants provides indirect evidence against robust algorithmic arithmetic being the primary mechanism for many static successes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports aggregate reasoning gaps for the set of closed models; specific per-model static/functional accuracy numbers for GPT-3.5 are not enumerated, but it lies within the reported overall gap range (58.35%–80.31%).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No model-specific probing reported. Authors note the potential for prompting and tool use to change gaps but did not systematically apply these to GPT-3.5 in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Same general limitations as other closed models: sensitivity to dataset contamination/memorization, failures on functional snapshots, possible formatting/hallucination mismatches, and unknown tool use in closed pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared at aggregate level with GPT-4 and other closed and OSS models; GPT-3.5 underperforms GPT-4 in robustness to functionalization according to the overall gap distribution reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4717.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4717.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 2.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude 2.1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-weight model variant included in the experiments; noted to default to chain-of-thought style outputs which interacts poorly with the evaluation harness in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 2.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-weight model (Anthropic Claude 2.1) accessed via API; the paper does not provide internal architecture or training specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Evaluated on the same functionalized MATH subset: arithmetic, combinatorics, probability, algebraic and symbolic problems generated by functional snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Tends to produce chain-of-thought by default; mechanism may therefore more often expose intermediate reasoning steps, but default CoT output format sometimes conflicts with the evaluation harness and may not improve measured functional accuracy without alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical observation in the paper: Claude 2.1 tends to default to outputting chain-of-thought steps, unlike some other models; authors note this behavior but also that it 'does not work very well with the evaluation harness.' This suggests the model produces intermediate reasoning but that this does not straightforwardly translate into higher measured functional accuracy under the paper's evaluation setup.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite default CoT-style outputs, Claude 2.1 still participates in the observed reasoning gap range (58–80% across models), indicating that default CoT production alone did not eliminate memorization-based successes on static items.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Specific per-model numeric accuracies are not provided for Claude 2.1 in the text; it is included among the closed models that show substantial reasoning gaps (aggregate range 58.35%–80.31%).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No targeted ablations; only qualitative note about default CoT formatting interacting poorly with the evaluation harness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Default CoT formatting can make automated scoring/evaluation brittle; may still rely on memorized/static items; tool usage and other pipeline details unknown for closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared qualitatively with other closed models (GPT-4, GPT-3.5, Mistral), but per-model quantitative comparisons for Claude 2.1 are not enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4717.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4717.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source (OSS) large language model evaluated among OSS baselines; OSS models as a group show much lower static and functional MATH accuracies compared to top closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source foundational model (Meta's LLaMA 2 family) evaluated in OSS experiments; the paper does not detail specific fine-tuning or instruction-tuning variants for the model as used here.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same functionalized MATH subset: grade-school arithmetic, combinatorics, probability, algebra, and symbolic arithmetic drawn from snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>OSS models in this group tend to succeed primarily on simple calculations and operator-property tasks, implying reliance on learned short patterns or shallow arithmetic heuristics rather than deep algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>OSS models collectively show low static accuracy (range reported 1.86%–12.28%) and even lower functional accuracy (0.52%–4.34%), and the subset of problems solved consistently by OSS models are overwhelmingly simple calculations or use of operator properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The inability to generalize to functional snapshots for most problems argues against robust algorithmic arithmetic; the small number of cross-snapshot solutions (19 problems solved by majority of OSS models) indicates limited true generalizable arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>OSS models (group) static accuracy range: 1.86%–12.28%; OSS functional accuracy range: 0.52%–4.34%. Only 19 problems were consistently solved across snapshots by a majority (>=5/10) of OSS models.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No internal probing; paper provides categorization of solved items showing they are dominated by simple calculation subtypes (14 reasoning subtypes identified overall with OSS concentrated in 2 subtypes).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Low coverage on harder subjects; failures beyond grade-school arithmetic; lack of robust generalization to functionalized variants; subject to same evaluation matching and big-number issues as closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>OSS models underperform closed SOTA (e.g., GPT-4) on both static and functional MATH metrics; OSS models show higher fractional drop and far fewer cross-snapshot generalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4717.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4717.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OSS-models-aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-Source Models (aggregate: WizardCoder, Yi, Qwen, StripedHypena, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate description of the open-weight models evaluated (various OSS models including LLaMA 2 70B, WizardCoder, Yi variants, Qwen 7B, StripedHypena variants), showing low static and functional arithmetic performance and limited generalization on functionalized MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OSS models (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Collection of evaluated open-source models: LLaMA 2 70B, WizardCoder Python 34B, Yi 34B and Yi Chat 34B, StripedHypena Nous and Hessian 7B, Qwen 7B, Mixtral variants, etc.; the paper evaluates these with a simple few-shot prompting strategy and pass@1.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grade-school arithmetic, simple combinatorics/probability, basic algebraic simplification and operator-property problems from the functionalized MATH subset.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Performance mainly arises from shallow pattern matching and performed arithmetic/calculation abilities for small inputs rather than robust algorithmic multi-step arithmetic; limited symbolic manipulation capabilities compared to top closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Most problems consistently solved by OSS models are simple calculations or use of operator properties; majority-solved problem set is only 19 items across snapshots. Static-to-functional drop for OSS models is large (static 1.86%–12.28% → functional 0.52%–4.34%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Very limited cross-snapshot generalization demonstrates that successes on static items are often non-generalizable; no reported internal interpretability evidence supporting algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Static accuracies reported for OSS models: 1.86%–12.28%; Functional accuracies across snapshots: 0.52%–4.34%; only 19 problems consistently solved by majority (>=5/10) of OSS models across snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No targeted probing; the paper performs manual classification of solved problems into reasoning subtypes showing dominance of simple calculations for OSS successes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Low absolute performance, poor generalization to functional variants, inability to solve medium/higher-difficulty arithmetic problems, sensitive to big-number instantiations and formatting/equivalence-checking limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>OSS models perform substantially worse than top closed models (GPT-4) both on static and especially on functionalized tests; they solve far fewer generalizable arithmetic items.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code. <em>(Rating: 1)</em></li>
                <li>Proofnet: Autoformalizing and formally proving undergraduate-level mathematics <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4717",
    "paper_id": "paper-3c585441b4607b34f8bf4e352ed6e36753fe21ce",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "OpenAI GPT-4",
            "brief_description": "A closed-weight state-of-the-art large language model accessed via API; evaluated in this paper as a top performer on static MATH problems but exhibiting a substantial drop on functionalized variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-weight SOTA model (OpenAI GPT-4) accessed via API in the experiments; architecture and training details are not specified in this paper.",
            "arithmetic_task_type": "Wide variety: simple arithmetic (rounding, squares), multi-digit arithmetic, factorial/leading-digit reasoning, combinatorics and counting, probability, multi-digit multiplication, algebraic minimization, polynomial-root reasoning, radical/equation solving, symbolic simplification and small-scale symbolic algebra.",
            "mechanism_hypothesis": "Mixture of genuine stepwise (symbolic) reasoning on simpler tasks and memorization/pattern-matching on static dataset items; some arithmetic successes may rely on implicit recall of training examples or learned heuristics rather than algorithmic multi-step computation. Tool-augmentation or specialized prompting (CoT/CoC/ToT) is hypothesized to reveal more true reasoning and reduce the memorization component.",
            "evidence_for_mechanism": "Substantial reasoning gap: GPT-4 solved 1299 static MATH items (25.98% static accuracy) but only achieved 541/5000 (10.82%) functional accuracy over static + 3 snapshots, yielding a 58.35% reasoning gap; 302 fully-generalizable problems were solved across snapshots (indicating these were likely solved by reasoning rather than memorization), and many of those solved problems are simple calculations or operator-property uses consistent with explicit reasoning.",
            "evidence_against_mechanism": "Large drop from static to functional variants indicates many static correct answers were due to memorization or dataset contamination rather than robust arithmetic reasoning; failures on functional snapshots and on problems instantiated with large numeric values suggest limits of in-context algorithmic computation.",
            "performance_metrics": "Static accuracy on MATH subset: 25.98% (1299/5000); Functional accuracy (static + 3 snapshots): 10.82% (541/5000); Reasoning gap: 58.35%; Of functional-correct items, 302 were genuinely generalizable across snapshots (6.04% accuracy attributable to snapshot-generalizable solutions) and 239 were ungeneralizable/static (4.78% attributable to static-only items).",
            "probing_or_intervention_results": "No internal probing/ablation was run on GPT-4 in this paper. The paper reports that evaluating across 3 independent functional snapshots stabilizes the measured reasoning gap (i.e., 3 snapshots are sufficient to distinguish memorization from reasoning). The authors also note that specialized prompting (CoT, ToT, CoC, etc.) and tool use are expected to reduce the gap but were not systematically applied in these experiments.",
            "limitations_and_failure_modes": "Large reasoning gap on functional variants indicates failure to generalize beyond static items; susceptibility to dataset contamination/memorization; sometimes reports 'NO SOLUTION' when uncertain; potential inability to handle very large instantiated numeric values (big-number failures); possible evaluator matching issues (formatting differences) and unknown/opaque tool usage in closed API pipeline; default chain-of-thought formatting may not align with evaluation harness.",
            "comparison_to_other_models": "Best-performing model in this paper: lowest reasoning gap (58.35%) among tested models (others ranged up to 80.31%). GPT-4 outperforms closed-model peers and OSS models on both static and functional metrics but still shows a large absolute gap indicating nontrivial memorization.",
            "uuid": "e4717.0",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "OpenAI GPT-3.5",
            "brief_description": "A closed-weight language model (GPT-3.5 family) included among the evaluated SOTA closed models; shows a substantial reasoning gap between static and functional MATH performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Closed-weight model (OpenAI GPT-3.5) accessed via API for evaluation; architecture and training specifics are not provided in this paper.",
            "arithmetic_task_type": "Same MATH categories as other models: grade-school arithmetic, combinatorics, probability, algebraic reasoning, and multi-step symbolic arithmetic problems drawn from the functionalized MATH subset.",
            "mechanism_hypothesis": "Likely relies more heavily on pattern matching/memorization for static items and less on robust algorithmic multi-step arithmetic; may be improved by specialized prompting or tool augmentation.",
            "evidence_for_mechanism": "Part of the reported group of closed models that exhibit large reasoning gaps (aggregate gap range reported as roughly 58–80% across closed and OSS models). The paper reports that many static successes do not transfer to functional snapshots, consistent with memorization over the training snapshot.",
            "evidence_against_mechanism": "Not directly probed in this paper; the observed drop between static and functional variants provides indirect evidence against robust algorithmic arithmetic being the primary mechanism for many static successes.",
            "performance_metrics": "Paper reports aggregate reasoning gaps for the set of closed models; specific per-model static/functional accuracy numbers for GPT-3.5 are not enumerated, but it lies within the reported overall gap range (58.35%–80.31%).",
            "probing_or_intervention_results": "No model-specific probing reported. Authors note the potential for prompting and tool use to change gaps but did not systematically apply these to GPT-3.5 in reported experiments.",
            "limitations_and_failure_modes": "Same general limitations as other closed models: sensitivity to dataset contamination/memorization, failures on functional snapshots, possible formatting/hallucination mismatches, and unknown tool use in closed pipelines.",
            "comparison_to_other_models": "Compared at aggregate level with GPT-4 and other closed and OSS models; GPT-3.5 underperforms GPT-4 in robustness to functionalization according to the overall gap distribution reported.",
            "uuid": "e4717.1",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Claude 2.1",
            "name_full": "Anthropic Claude 2.1",
            "brief_description": "A closed-weight model variant included in the experiments; noted to default to chain-of-thought style outputs which interacts poorly with the evaluation harness in some cases.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 2.1",
            "model_description": "Closed-weight model (Anthropic Claude 2.1) accessed via API; the paper does not provide internal architecture or training specifics.",
            "arithmetic_task_type": "Evaluated on the same functionalized MATH subset: arithmetic, combinatorics, probability, algebraic and symbolic problems generated by functional snapshots.",
            "mechanism_hypothesis": "Tends to produce chain-of-thought by default; mechanism may therefore more often expose intermediate reasoning steps, but default CoT output format sometimes conflicts with the evaluation harness and may not improve measured functional accuracy without alignment.",
            "evidence_for_mechanism": "Empirical observation in the paper: Claude 2.1 tends to default to outputting chain-of-thought steps, unlike some other models; authors note this behavior but also that it 'does not work very well with the evaluation harness.' This suggests the model produces intermediate reasoning but that this does not straightforwardly translate into higher measured functional accuracy under the paper's evaluation setup.",
            "evidence_against_mechanism": "Despite default CoT-style outputs, Claude 2.1 still participates in the observed reasoning gap range (58–80% across models), indicating that default CoT production alone did not eliminate memorization-based successes on static items.",
            "performance_metrics": "Specific per-model numeric accuracies are not provided for Claude 2.1 in the text; it is included among the closed models that show substantial reasoning gaps (aggregate range 58.35%–80.31%).",
            "probing_or_intervention_results": "No targeted ablations; only qualitative note about default CoT formatting interacting poorly with the evaluation harness.",
            "limitations_and_failure_modes": "Default CoT formatting can make automated scoring/evaluation brittle; may still rely on memorized/static items; tool usage and other pipeline details unknown for closed models.",
            "comparison_to_other_models": "Compared qualitatively with other closed models (GPT-4, GPT-3.5, Mistral), but per-model quantitative comparisons for Claude 2.1 are not enumerated in the paper.",
            "uuid": "e4717.2",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-2-70B",
            "name_full": "LLaMA 2 70B",
            "brief_description": "An open-source (OSS) large language model evaluated among OSS baselines; OSS models as a group show much lower static and functional MATH accuracies compared to top closed models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA 2 70B",
            "model_description": "Open-source foundational model (Meta's LLaMA 2 family) evaluated in OSS experiments; the paper does not detail specific fine-tuning or instruction-tuning variants for the model as used here.",
            "arithmetic_task_type": "Same functionalized MATH subset: grade-school arithmetic, combinatorics, probability, algebra, and symbolic arithmetic drawn from snapshots.",
            "mechanism_hypothesis": "OSS models in this group tend to succeed primarily on simple calculations and operator-property tasks, implying reliance on learned short patterns or shallow arithmetic heuristics rather than deep algorithmic arithmetic.",
            "evidence_for_mechanism": "OSS models collectively show low static accuracy (range reported 1.86%–12.28%) and even lower functional accuracy (0.52%–4.34%), and the subset of problems solved consistently by OSS models are overwhelmingly simple calculations or use of operator properties.",
            "evidence_against_mechanism": "The inability to generalize to functional snapshots for most problems argues against robust algorithmic arithmetic; the small number of cross-snapshot solutions (19 problems solved by majority of OSS models) indicates limited true generalizable arithmetic reasoning.",
            "performance_metrics": "OSS models (group) static accuracy range: 1.86%–12.28%; OSS functional accuracy range: 0.52%–4.34%. Only 19 problems were consistently solved across snapshots by a majority (&gt;=5/10) of OSS models.",
            "probing_or_intervention_results": "No internal probing; paper provides categorization of solved items showing they are dominated by simple calculation subtypes (14 reasoning subtypes identified overall with OSS concentrated in 2 subtypes).",
            "limitations_and_failure_modes": "Low coverage on harder subjects; failures beyond grade-school arithmetic; lack of robust generalization to functionalized variants; subject to same evaluation matching and big-number issues as closed models.",
            "comparison_to_other_models": "OSS models underperform closed SOTA (e.g., GPT-4) on both static and functional MATH metrics; OSS models show higher fractional drop and far fewer cross-snapshot generalizations.",
            "uuid": "e4717.3",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "OSS-models-aggregate",
            "name_full": "Open-Source Models (aggregate: WizardCoder, Yi, Qwen, StripedHypena, etc.)",
            "brief_description": "Aggregate description of the open-weight models evaluated (various OSS models including LLaMA 2 70B, WizardCoder, Yi variants, Qwen 7B, StripedHypena variants), showing low static and functional arithmetic performance and limited generalization on functionalized MATH.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OSS models (aggregate)",
            "model_description": "Collection of evaluated open-source models: LLaMA 2 70B, WizardCoder Python 34B, Yi 34B and Yi Chat 34B, StripedHypena Nous and Hessian 7B, Qwen 7B, Mixtral variants, etc.; the paper evaluates these with a simple few-shot prompting strategy and pass@1.",
            "arithmetic_task_type": "Grade-school arithmetic, simple combinatorics/probability, basic algebraic simplification and operator-property problems from the functionalized MATH subset.",
            "mechanism_hypothesis": "Performance mainly arises from shallow pattern matching and performed arithmetic/calculation abilities for small inputs rather than robust algorithmic multi-step arithmetic; limited symbolic manipulation capabilities compared to top closed models.",
            "evidence_for_mechanism": "Most problems consistently solved by OSS models are simple calculations or use of operator properties; majority-solved problem set is only 19 items across snapshots. Static-to-functional drop for OSS models is large (static 1.86%–12.28% → functional 0.52%–4.34%).",
            "evidence_against_mechanism": "Very limited cross-snapshot generalization demonstrates that successes on static items are often non-generalizable; no reported internal interpretability evidence supporting algorithmic arithmetic.",
            "performance_metrics": "Static accuracies reported for OSS models: 1.86%–12.28%; Functional accuracies across snapshots: 0.52%–4.34%; only 19 problems consistently solved by majority (&gt;=5/10) of OSS models across snapshots.",
            "probing_or_intervention_results": "No targeted probing; the paper performs manual classification of solved problems into reasoning subtypes showing dominance of simple calculations for OSS successes.",
            "limitations_and_failure_modes": "Low absolute performance, poor generalization to functional variants, inability to solve medium/higher-difficulty arithmetic problems, sensitive to big-number instantiations and formatting/equivalence-checking limitations.",
            "comparison_to_other_models": "OSS models perform substantially worse than top closed models (GPT-4) both on static and especially on functionalized tests; they solve far fewer generalizable arithmetic items.",
            "uuid": "e4717.4",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models trained on code.",
            "rating": 1
        },
        {
            "paper_title": "Proofnet: Autoformalizing and formally proving undergraduate-level mathematics",
            "rating": 2
        }
    ],
    "cost": 0.015238499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap</h1>
<p>Saurabh Srivastava,<br>Annarose M B, Anto P V, Shashank Menon, Ajay Sukumar, Adwaith Samod T, Alan Philipose, Stevin Prince, and Sooraj Thomas</p>
<p>Consequent AI</p>
<h2>1 Abstract</h2>
<p>We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap - the percentage difference between the static and functional accuracies. We find reasoning gaps from $58.35 \%$ to $80.31 \%$ among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building "gap 0" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.</p>
<h2>2 Introduction</h2>
<p>Accurately evaluating reasoning performance is critical to improving large language models (LLMs) beyond their current capabilities. The current standard for benchmarking fails to accurately measure reasoning of LLMs. An accurate test should consist of posing a question whose answer can be derived under the assumption of axioms for that domain. Reasoning systems (or humans) should be able to conclude the answer despite not having seen the question before. An ideal system would provide not only the answer but also each step of the derivation. Explicit elucidation of steps is ideal, but not critical. State-of-the-art language models exhibit some reasoning capabilities, but accurate evaluation is lagging. In particular, the capabilities of LLMs may be overestimated because they are tested using benchmarks</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>that are sufficient for language understanding (textual question, answer) but may not accurately measure reasoning.</p>
<p>When testing generalized problem-solving, the aim is to assess whether the tested system can effectively answer questions it has not encountered before. This is the opposite objective when compared to knowledge retrieval. Finding novel questions is challenging when the model has been trained on a snapshot of the internet. Even if datasets exist that are not currently on the internet, it is unreasonable to expect them to remain so, or that we will continue to develop novel datasets. Contamination-testing by checking for $k$-contiguous token sequences fails to account for various other forms in which leakage might occur, especially when the benchmark is a set of static text question-answers ("static QAs"). These may include i) accidental leakage occurring from paraphrasing or compressed representations in the training data or fragments of semantically similar statements; ii) not accounting for the provenance of data used in every stage of training or fine-tuning, which becomes increasingly difficult as more models are trained from existing high-quality starting weights; iii) sourcing synthetic training data from an existing model which could be contaminated, especially if it is closed-source and closed-data, iv) overfitting through preference tuning, which may in the extreme case cause reinforcement feedback allowing the model to evolve to discover correct static answers.</p>
<p>Checking against a static QA is only an indirect test of reasoning. Text questions and answers work well for humans as producing correct answers implies likely correct stepwise reasoning. The same does not apply to model outputs.</p>
<p>We propose an alternative approach that alleviates concerns with static QA testing, but also works seamlessly with existing evaluation harnesses. We propose rewriting each static QA's reasoning into code, whose inputs can be varied to create infinite versions that follow the same reasoning pattern, but the steps needed in each would be different. We call this the functional variant of the benchmark, and running each of those code fragments will create a snapshot that is a drop-in replacement into the existing static evaluation harness. The accuracy on these replacements, with the static variant included, might be lower than that on the static variant alone, which we quantify as the reasoning gap.</p>
<p>For domains where this can be done, employing functional variants would improve on static QA evaluation. We would convert a static benchmark Bench (e.g., MATH or GSM8K or HumanEval) into their functional variant Bench(rngs). The functional variants (denoted here as $\mathrm{QA}(r n g s)$ ) take as inputs a set of deterministic typed random generators $r n g s$ which output a pseudorandom typed value, e.g., natural int, positive or negative float, fractions, string, positive evens or odds, primes. The $r n g s$ when seeded make the instantiation deterministic, generating a snapshot of each test (denoted here as $\mathrm{QA}^{*}$ ) and of the entire benchmark.</p>
<p>Each $\mathrm{QA}^{<em>}$ text pair in this snapshot is different from the original static QA. However, answering $\mathrm{QA}^{</em>}$ requires the same reasoning used in the static QA. To have an unbiased accurate baseline, we created the functionalization manually by rewriting the reasoning of each test into code. Functionalization-a one-time timeconsuming task-results in infinite snapshot instantiations to test against.</p>
<p>Of the popular benchmarks, functionalization is easy for the subcategories of math (MATH [29] and GSM8K [20]), and code (HumanEval [12], MBPP [7], and Natural2Code [71]). The categories of commonsense reasoning (HellaSwag [89], Winogrande [59], PIQA [11], SIQA [60], OpenbookQA [50], ARC-Easy/Challenge [19],</p>
<p>CommonsenseQA [70]), world knowledge (NaturalQuestions [38], TriviaQA [36]), and reading comprehension (BoolQ [18], QuAC [16], DROP [22]) test English understanding and thus are not good candidates for functionalization. Subparts of the aggregated benchmarks (MMLU [28], HELM [43], BBH [69], and AGI Eval [94]) could be functionalized.</p>
<p>As of this work, we have functionalized $41.2 \%$ of the MATH benchmark, with the subset chosen so as to fully cover the static QA tests solved by 4 closed-weight and 9 open-weight state-of-the-art (SOTA) models. This allows us to provide complete metrics over all of MATH for these models, assuming a simple few-shot prompting strategy. We find a reasoning gap of $58.35 \%$ to $80.31 \%$ across these models. The value of the gap when using optimized prompting such as chain-of-thought (CoT [80]), tree-of-thought (ToT [85]), chain-of-code (CoC [41]), amongst others, could be lower and we will resolve that open question when we have built the $100 \%$ functionalized MATH().</p>
<p>In future work, we will present functionalized variants of GSM8K, HumanEval, and MBPP.</p>
<h1>2.1 Contributions</h1>
<p>This work presents the first steps towards reasoning evaluation with functional benchmarks:</p>
<ul>
<li>Framework for evaluation for models that have seen the text of the entire internet: We present a proposal for a long-term solution to the problem of evaluating models that are increasingly trained on all written knowledge. The proposed framework allows a benchmark to be instantiated to infinite snapshots, with a probabilistic guarantee that a new snapshot will not have existed before. For a static QA in the benchmark whose individual test captures a specific reasoning process, each snapshot QA* will follow the same process, but the test will be new. For a model to perform well on a set of snapshots, it will need to be able to perform step-by-step derivations to reach an answer, rather than simply recalling a static final answer.</li>
<li>Functional version of MATH, an important benchmark for reasoning evaluation, and its publicly available snapshots: Our framework is designed to allow the reworking of existing benchmarks on which models are already being tested without modification. We are in the process of functionalizing the entire MATH benchmark. The specific functionalization code will never be publicly accessible. Instead three snapshots will be released every quarter. The {Oct, Nov, Dec}-2023 snapshots are available at https://github.com/ consequentai/fneval/ along with the evaluation code. The repository will be kept up to date with the last quarter's snapshots.</li>
<li>
<p>Robust evaluations that are resistant to contamination: There are infinite snapshots of a functional benchmark, with the reasoning involved in each remaining identical. If a model consistently solves a problem across multiple snapshots (e.g., three snapshots) then there is a high likelihood it does so with proper reasoning, because it should be fairly difficult to get the answer right across many snapshots by chance.</p>
</li>
<li>
<p>Reasoning gap: We define the reasoning gap as the quantitative measure of the difference in accuracy when testing against static vs functional variants. The gap is a measure of reasoning vs memorization, with gap-0 being true reasoning and gap-100 being full memorization. The community should aim to build models that have the highest accuracy while maintaining a gap close to 0 . Minimizing the gap may serve as a training optimization objective that could lead to models that perform better at generalized reasoning.</p>
</li>
<li>Open problem of training gap-0 models: Here we show that even the current best models have large reasoning gaps, with the caveat that the reasoning gap is likely to be smaller with more sophisticated prompting strategies. Our current datasets, training, fine-tuning, and inference strategies might be nonoptimal for training gap-0 models. We leave it as an open problem to train gap-0 models.</li>
</ul>
<h1>3 Illustrative Examples</h1>
<p>We motivate the need for more accurate assessment of the reasoning capabilities of LLMs by examining specific cases in the MATH benchmark. Below and after, we use monotype font to indicate verbatim text from the benchmark. In particular, the benchmark problems include LaTeX formatting, which we leave as is to accurately display the model input. For all problems below, these static QAs are solved by current LLMs - i.e., the inferred answer matches the ground truth answer shown.</p>
<h2>Case 1 - Simple arithmetic</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Compute</span><span class="w"> </span><span class="n">$</span><span class="o">\\</span><span class="n">left</span><span class="o">(-\\\</span><span class="n">sqrt</span><span class="o">{</span><span class="mi">5321</span><span class="o">}\\</span><span class="n">right</span><span class="o">)^</span><span class="mi">2</span><span class="n">$</span><span class="o">.</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="mi">5321</span>
</code></pre></div>

<p>Question: Round 15.49999999 to the nearest whole number.
Answer: 15
Solving problems such as these is in line with known capabilities of current models. We would expect such problems to be solvable in their functionalized form as well. We find this to be the case.</p>
<h2>Case 2 - Non-trivial arithmetic or use of numerical properties</h2>
<div class="codehilite"><pre><span></span><code>Question: $20!$ has 19 digits, the last 18 of which are
    432902008176640000. What is the first digit?
Answer: 2
</code></pre></div>

<p>The solution presented in the benchmark (for human reasoning) exploits a property of divibility by 9 to arrive at the answer. A model equipped with a calculator could explicitly compute 20 ! and arrive at the same answer. We do not expect the functionalized forms to be solvable by open weight models, for whom we can confirm no tools are used in the inference pipeline. We find this to be the case.</p>
<h1>Case 3 - Undergraduate-level mathematical insights and skills</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">$x$</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">minimum</span><span class="w"> </span><span class="n">value</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">$x</span><span class="o">^</span><span class="mi">2</span><span class="o">-</span><span class="mi">14</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">3</span><span class="n">$</span><span class="o">?</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="mi">7</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">equations</span><span class="w"> </span><span class="n">$x</span><span class="o">^</span><span class="mi">3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">5</span><span class="n">x</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">px</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="n">$</span><span class="w"> </span><span class="n">and</span>
<span class="w">    </span><span class="n">$x</span><span class="o">^</span><span class="mi">3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">7</span><span class="n">x</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">px</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="n">$</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">roots</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">common</span><span class="o">.</span>
<span class="w">    </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">third</span><span class="w"> </span><span class="n">root</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">equation</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">represented</span>
<span class="w">    </span><span class="n">by</span><span class="w"> </span><span class="n">$x_1$</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">$x_2$</span><span class="w"> </span><span class="n">respectively</span><span class="o">,</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ordered</span>
<span class="w">    </span><span class="n">pair</span><span class="w"> </span><span class="n">$</span><span class="o">(</span><span class="n">x_1</span><span class="o">,</span><span class="n">x_2</span><span class="o">).</span><span class="n">$</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="o">(-</span><span class="mi">5</span><span class="o">,-</span><span class="mi">7</span><span class="o">)</span>
</code></pre></div>

<p>The first problem requires understanding when a quadratic equation is minimal. The second problem requires taking the difference of two polynomials, observing a cancellation, and using properties of roots of quadratic and cubic polynomials to arrive at the answer. Neither of these are trivial, and we only expect their functionalized forms to be solvable by top models that are capable of symbolic manipulation. We find that to be the case.</p>
<h2>Case 4 - Graduate-level mathematics</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Euler</span><span class="w"> </span><span class="n">discovered</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">polynomial</span><span class="w"> </span><span class="n">$p</span><span class="o">(</span><span class="n">n</span><span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">41</span><span class="n">$</span>
<span class="w">    </span><span class="n">yields</span><span class="w"> </span><span class="n">prime</span><span class="w"> </span><span class="n">numbers</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">small</span><span class="w"> </span><span class="n">positive</span><span class="w"> </span><span class="n">integer</span><span class="w"> </span><span class="n">values</span>
<span class="w">    </span><span class="n">of</span><span class="w"> </span><span class="n">$n$</span><span class="o">.</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">smallest</span><span class="w"> </span><span class="n">positive</span><span class="w"> </span><span class="n">integer</span><span class="w"> </span><span class="n">$n$</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">which</span>
<span class="w">    </span><span class="n">$p</span><span class="o">(</span><span class="n">n</span><span class="o">)</span><span class="n">$</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">$p</span><span class="o">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="o">)</span><span class="n">$</span><span class="w"> </span><span class="n">share</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">common</span><span class="w"> </span><span class="n">factor</span><span class="w"> </span><span class="n">greater</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="n">$1$</span><span class="o">?</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="mi">41</span>
</code></pre></div>

<p>While this problem shows solvable in its static QA form, the graduate-level reasoning required should be outside the planning capabilities of current models. We find that the functional forms are able distinguish such problems and no models are able to solve the functional snapshots.</p>
<h2>Case 5 - Problems where the ground truth answer is problematic.</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Find</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">solutions</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">equation</span><span class="w"> </span><span class="n">$</span><span class="o">\\</span><span class="n">sqrt</span><span class="o">{</span><span class="mi">3</span><span class="n">x</span><span class="o">+</span><span class="mi">6</span><span class="o">}=</span><span class="n">x</span><span class="o">+</span><span class="mi">2</span><span class="n">$</span><span class="o">.</span><span class="w"> </span><span class="n">If</span>
<span class="w">    </span><span class="n">there</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">solutions</span><span class="o">,</span><span class="w"> </span><span class="n">order</span><span class="w"> </span><span class="n">them</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="n">to</span>
<span class="w">    </span><span class="n">greatest</span><span class="o">,</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">comma</span><span class="o">(</span><span class="n">s</span><span class="o">).</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Two</span><span class="w"> </span><span class="n">concentric</span><span class="w"> </span><span class="n">circular</span><span class="w"> </span><span class="n">regions</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">radii</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">inch</span><span class="w"> </span><span class="n">and</span>
<span class="mi">10</span><span class="w"> </span><span class="n">inches</span><span class="o">.</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">area</span><span class="o">,</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">square</span><span class="w"> </span><span class="n">inches</span><span class="o">,</span><span class="w"> </span><span class="n">outside</span>
<span class="n">the</span><span class="w"> </span><span class="n">smaller</span><span class="w"> </span><span class="n">region</span><span class="o">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">inside</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">larger</span><span class="w"> </span><span class="n">region</span><span class="o">?</span><span class="w"> </span><span class="n">Express</span>
<span class="n">your</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">terms</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">$</span><span class="o">\\</span><span class="n">pi$</span><span class="o">.</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="mi">99</span><span class="o">\\</span><span class="n">pi</span><span class="w"> </span><span class="o">\\</span><span class="n">text</span><span class="o">\{\</span><span class="w"> </span><span class="n">Square</span><span class="w"> </span><span class="n">inches</span><span class="o">\}</span>
</code></pre></div>

<p>For the first question, the ground truth that matches the question is " $-2,1$ " and the stated answer is incorrect. For the second question, there is extra verbiage in the answer which is not asked for in the question. Model outputs that match the imprecise ground truth are concerning, and we do find models where they do.</p>
<p>When we test the functionalized, corrected, versions of these problems the matching dissappears.</p>
<p>These cases illustrate how a functional evaluation separates out various cases, and provides a more robust evaluation compared to static QAs.</p>
<h1>4 Approach</h1>
<p>Given a benchmark test written as a "question, answer" pair, we convert it to three functions "problem(inputs), solution(inputs), inputs(rngs, seed)". These functions are designed to have the property that arriving at answer using a well-reasoned derivation from the original question is the same that would yield answer' = solution(in) from question' $=$ problem(in) where in $=$ inputs(rngs, seed) for an arbitrarily chosen seed.</p>
<p>Building these functions requires human insight in: a) picking a set of symbolic inputs that sufficiently generalize both question and answer, b) encoding any implicit constraints on the inputs in the generator function and that the generator explores the input space as the seed is varied, c) translating the text of the specific question and answer, into a respective functions problem and solution that symbolically derive with the same steps as would have been followed in the derivation of answer from question.</p>
<p>Functional variants of a benchmark We call a benchmark, e.g., MATH, that has its test set converted from static "question, answer" form into functionals as above the functional variant, noted as MATH(). Each test in the functional variant cannot be directly used to evaluate, but instead it needs to be instantiated.</p>
<p>Snapshots We call an instantiation of each of the functionalized tests using a seed a snapshot of the benchmark. For our implementation, we use a hash of the month string, e.g., "Dec-2023", combined with a hash of the original text QA, as the seed. When instantiated, each test in the snapshot has the same reasoning as the original test but the steps to the answer will be different. We can run the evaluation scripts unmodified from the original benchmark over the snapshot.</p>
<p>Functional accuracy We define the functional accuracy as the accuracy of solving a test in all $k$ snapshots and the static variant. If a model arrives at an answer given a question, using proper reasoning, then its outcome should be identical over the static benchmark as well as each snapshot of the functional variant. Thus we define the functional accuracy of the model against that benchmark as the fraction of tests it gets correct over $k$-snapshots and the static variant.</p>
<p>Reasoning Gap The reasoning gap is the percent decrease in the accuracy between the static and functional variants.</p>
<p>Example of functionalizing a test Figure 1 shows an example text QA from the counting and probability subset of the MATH test dataset, that is solved in its static form by GPT4. While it is not a particularly complicated counting calculation, it does require more than grade school reasoning of realizing that the problem</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Text</span><span class="w"> </span><span class="nv">problem</span>:
<span class="nv">The</span><span class="w"> </span><span class="nv">letters</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">word</span><span class="w"> </span><span class="s1">&#39;SIXTEEN&#39;</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">randomly</span><span class="w"> </span><span class="nv">arranged</span>.
<span class="nv">What</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">E</span><span class="err">&#39;s are not next to each other?</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">Text</span><span class="w"> </span><span class="nv">solution</span>:
<span class="nv">The</span><span class="w"> </span><span class="nv">best</span><span class="w"> </span><span class="nv">way</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">find</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span>
<span class="nv">two</span><span class="w"> </span><span class="nv">E</span><span class="err">&#39;s are next to each other. There are $\\dfrac{7!}{2}$\$ arrangements of the word SIXTEEN. If we want to find the number</span>
<span class="nv">of</span><span class="w"> </span><span class="nv">arrangements</span><span class="w"> </span><span class="nv">such</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">E</span><span class="err">&#39;s are next to each other, we find</span>
<span class="err">the number of arrangements of the six-letter word SIXT(EE)N</span>
<span class="ss">(</span><span class="nv">where</span><span class="w"> </span><span class="nv">we</span><span class="w"> </span><span class="nv">treat</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">E</span><span class="err">&#39;s as a block), which is $6!$.</span>
<span class="err">So the probability that an arrangement of the word SIXTEEN</span>
<span class="nv">has</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">E</span><span class="err">&#39;s next to each other is</span>
<span class="err">$\\dfrac{6!}{\frac{7!}{2}}=\backslash\backslash \operatorname{dfrac}{2}{7}$.</span>
<span class="nv">So</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">E</span><span class="s1">&#39;s aren&#39;</span><span class="nv">t</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">other</span><span class="w"> </span><span class="nv">is</span>
<span class="mh">$1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span>\\<span class="nv">dfrac</span>{<span class="mi">2</span>}{<span class="mi">7</span>}<span class="w"> </span><span class="o">=</span><span class="w"> </span>\\<span class="nv">boxed</span>{\\<span class="nv">dfrac</span>{<span class="mi">5</span>}{<span class="mi">7</span>}}$.
</code></pre></div>

<p>Figure 1: Example of a text QA from the MATH test data (Counting and Probability, 714.json). Note that the evaluation suite only compares the correctness of the output text enclosed in " $\backslash \backslash$ boxed ${\ldots}$ " in this case the correct answer would be " $\backslash \backslash \operatorname{dfrac}{5}{7}$ " .
can be translated into its negated form, followed by canceling of major terms in the permutation. While it is not impossible that a model would do step-by-step reasoning of such form if it has seen the logic before, inventing that line of reasoning from first principles would be surprising. We convert it to a functional test as shown in Figure 2. Of the three snapshots we take of this, GPT4 solved $1 / 3$ of them.</p>
<p>Figure 3 shows an example text QA from the prealgebra subset of the MATH test dataset, also solved in its static form by GPT4. Figure 4 shows its corresponding functionalization. GPT4 reports "NO SOLUTION" for each of the snapshots, which it has been instructed to do if it does not know the answer, indicating that its answer to the static version was adding spurious accuracy. While hallucinations are not the core focus of this work, we also test how many times the models report NO SOLUTION, which they rarely do, and this is one of the rare occasions.</p>
<h1>5 Results and Discussion</h1>
<h3>5.1 Functionalizing MATH to MATH()</h3>
<p>We have functionalized $41.2 \%(2060 / 5000)$ of the MATH benchmark. This subset was chosen based on the portion of the benchmark that is solvable by a group of SOTA closed and OSS models, without prompting optimizations (e.g., CoT) and with pass@1. This choice makes the evaluation complete for the current set of models, because by definition tests that fail the static version do not count towards overall functional accuracy. The next release of the benchmark will provide $100 \%$ coverage, and also permit interpretable results under model-specific prompting optimization, and options for testing against pass@ $k$ or $\operatorname{maj} @ j$ for $k, j&gt;1$.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">problem</span><span class="p">(</span><span class="n">word</span><span class="p">:</span><span class="w"> </span><span class="nb">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="n">prb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;The letters of the word &#39;{word}&#39; are randomly arranged.&quot;</span>
<span class="w">        </span><span class="n">f</span><span class="s2">&quot;What is the probability that the two E&#39;s are not next to each other?&quot;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">prb</span>
<span class="n">def</span><span class="w"> </span><span class="n">solution</span><span class="p">(</span><span class="n">word</span><span class="p">:</span><span class="w"> </span><span class="nb">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># The numerator is always 2 less than the length of the</span>
<span class="w">    </span><span class="c1"># random string - 2 being the length of the string &#39;EE&#39;</span>
<span class="w">    </span><span class="n">numerator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="c1"># Resulting denominator will always be the length of the string</span>
<span class="w">    </span><span class="n">denominator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">length</span>
<span class="w">    </span><span class="c1"># Simplifying the fraction</span>
<span class="w">    </span><span class="n">common_factor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">math</span><span class="o">.</span><span class="n">gcd</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span><span class="w"> </span><span class="n">denominator</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">common_factor</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span>
<span class="w">        </span><span class="n">numerator</span><span class="w"> </span><span class="o">//=</span><span class="w"> </span><span class="n">common_factor</span>
<span class="w">        </span><span class="n">denominator</span><span class="w"> </span><span class="o">//=</span><span class="w"> </span><span class="n">common_factor</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">dfrac{{{numerator}}}{{{{denominator}}}&quot;</span>
<span class="n">def</span><span class="w"> </span><span class="n">inputs</span><span class="p">(</span><span class="n">rngs</span><span class="p">,</span><span class="w"> </span><span class="nb">seed</span><span class="p">):</span>
<span class="w">    </span><span class="n">rngs</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="nb">seed</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># The string is made up of three parts, pre + &#39;EE&#39; + post</span>
<span class="w">    </span><span class="c1"># We keep the string &#39;EE&#39; constant to align with original problem</span>
<span class="w">    </span><span class="c1"># Length of the generated word</span>
<span class="w">    </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rngs</span><span class="o">.</span><span class="n">int_between</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># Generate a random index where the two &#39;E&#39;s will be placed</span>
<span class="w">    </span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rngs</span><span class="o">.</span><span class="n">int_between</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># A string containing all uppercase letters except &#39;E&#39;</span>
<span class="w">    </span><span class="n">excl_ee</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">string</span><span class="o">.</span><span class="n">ascii_uppercase</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;E&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># A lambda to build a random word with the given length</span>
<span class="w">    </span><span class="n">build</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="n">length</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">rngs</span><span class="o">.</span><span class="n">choose_from</span><span class="p">(</span><span class="n">excl_ee</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">))</span>
<span class="w">    </span><span class="c1"># Construct the string with the three parts</span>
<span class="w">    </span><span class="n">pre</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">build</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="w">    </span><span class="n">post</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">build</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span>
<span class="w">    </span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pre</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s2">&quot;EE&quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">post</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;word&quot;</span><span class="p">:</span><span class="w"> </span><span class="n">word</span><span class="p">}</span>
</code></pre></div>

<p>Figure 2: Functionalization of the text QA from Figure 1</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Text</span><span class="w"> </span><span class="nv">problem</span>:
<span class="nv">Spinner</span><span class="w"> </span><span class="nv">I</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">divided</span><span class="w"> </span><span class="nv">into</span><span class="w"> </span><span class="nv">four</span><span class="w"> </span><span class="nv">equal</span><span class="w"> </span><span class="nv">sections</span><span class="w"> </span><span class="nv">labeled</span><span class="w"> </span><span class="mi">2</span>,<span class="w"> </span><span class="mi">3</span>,<span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">5</span>.
<span class="nv">Spinner</span><span class="w"> </span><span class="nv">II</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">divided</span><span class="w"> </span><span class="nv">into</span><span class="w"> </span><span class="nv">five</span><span class="w"> </span><span class="nv">equal</span><span class="w"> </span><span class="nv">sections</span><span class="w"> </span><span class="nv">labeled</span><span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="mi">3</span>,<span class="w"> </span><span class="mi">5</span>,<span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">9</span>.
<span class="k">If</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">spinner</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">spun</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">resulting</span><span class="w"> </span><span class="nv">numbers</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">multiplied</span>,
<span class="nv">what</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">product</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">two</span><span class="o">-</span><span class="nv">digit</span><span class="w"> </span><span class="nv">even</span><span class="w"> </span><span class="nv">number</span>?
<span class="nv">Express</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">as</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">common</span><span class="w"> </span><span class="nv">fraction</span>.
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">Text</span><span class="w"> </span><span class="nv">solution</span><span class="o">:</span>
<span class="nv">Let</span><span class="w"> </span><span class="nv">results</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">denoted</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="ow">or</span><span class="nv">dered</span><span class="w"> </span><span class="nv">pairs</span><span class="w"> </span><span class="nv">where</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">first</span><span class="w"> </span><span class="nv">coordinate</span>
<span class="nv">corresponds</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">Spinner</span><span class="w"> </span><span class="nv">I</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">second</span><span class="w"> </span><span class="nv">coordinate</span><span class="w"> </span><span class="nv">corresponds</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">Spinner</span><span class="w"> </span><span class="nv">II</span><span class="o">.</span>
<span class="nv">Since</span><span class="w"> </span><span class="nv">all</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">section</span><span class="w"> </span><span class="nv">numbers</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">Spinner</span><span class="w"> </span><span class="nv">II</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">odd</span><span class="p">,</span><span class="w"> </span><span class="nv">Spinner</span><span class="w"> </span><span class="nv">I</span><span class="w"> </span><span class="nv">must</span><span class="w"> </span><span class="nv">given</span><span class="w"> </span><span class="nv">an</span>
<span class="nv">even</span><span class="w"> </span><span class="nv">number</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="ow">or</span><span class="nv">der</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">product</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">even</span><span class="o">.</span><span class="w"> </span><span class="nv">The</span><span class="w"> </span><span class="nv">results</span>
<span class="p">$(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)$,</span><span class="w"> </span><span class="p">$(</span><span class="mi">2</span><span class="p">,</span><span class="mi">7</span><span class="p">)$,</span><span class="w"> </span><span class="p">$(</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">)$,</span><span class="w"> </span><span class="p">$(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">)$,</span><span class="w"> </span><span class="p">$(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">)$,</span><span class="w"> </span><span class="p">$(</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">)$,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="p">$(</span><span class="mi">4</span><span class="p">,</span><span class="mi">9</span><span class="p">)$</span>
<span class="nv">are</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ones</span><span class="w"> </span><span class="nv">whose</span><span class="w"> </span><span class="nv">products</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">two</span><span class="o">-</span><span class="nv">digit</span><span class="w"> </span><span class="nv">even</span><span class="w"> </span><span class="nv">numbers</span><span class="o">.</span>
<span class="nv">Since</span><span class="w"> </span><span class="nv">there</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="p">$</span><span class="mi">5</span>\\<span class="nv">times4</span><span class="o">=</span><span class="mi">20</span><span class="p">$</span><span class="w"> </span><span class="nv">equally</span><span class="w"> </span><span class="nv">likely</span><span class="w"> </span><span class="nv">results</span><span class="p">,</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">of</span>
<span class="nv">obtaining</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">even</span><span class="w"> </span><span class="nv">two</span><span class="o">-</span><span class="nv">digit</span><span class="w"> </span><span class="nv">product</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="p">$</span>\\<span class="nv">boxed</span><span class="p">{</span>\\<span class="nv">frac</span><span class="p">{</span><span class="mi">7</span><span class="p">}{</span><span class="mi">20</span><span class="p">}}</span><span class="o">.</span>
</code></pre></div>

<p>Figure 3: Example of a text QA from the MATH test data (Prealgebra, 1151.json). Note that the evaluation suite only compares the correctness of the output text enclosed in " $\backslash \backslash$ boxed ${\ldots}$ ", in this case the correct answer would be " $\backslash \backslash$ frac ${7}{20}$ ".</p>
<h1>5.2 Reasoning gap for major models</h1>
<p>Models evaluated We ran evaluations over SOTA reasoning models: The closed models accessed through APIs were OpenAI's GPT3.5 and GPT4 [54], Anthropic's Claude 2.1 [5], and Mistral's Mixtral Medium, 7x8B MoE and 7x8B MoE Instruct [33]. The OSS models included were LLaMA 2 70B [72], WizardCoder Python 34B [47], Yi 34B and Yi Chat 34B [2], StripedHypena Nous and Hessian 7B [57], Qwen 7B [9]. We chose this subset of models to sample a diversity of sizes, architectures, training data and recipes, and instruction and preference tuning.</p>
<p>Gap $=\mathbf{5 8 - 8 0 \%}$ for SOTA models Evaluated over the Q1'24 snapshot (consisting of Oct-2023, Nov-2023, Dec-2023), we find that the models have a reasoning gap varying between $58.35 \%$ and $80.31 \%$, as shown in Figure 5(a). Figure 5(b) shows the percentage of problems the model solved correctly that were evaluated using functional snapshots. While we attempted to functionalize every problem, some were not convertible, either because the problem was already as general as possible, or it was so specific that it did not permit any parameterization. Figure 5(c) shows the individual static accuracy vs functional accuracy.</p>
<p>Gap stabilization after three snapshots Figure 6 shows our analysis of gaps with three snapshots, two snapshots (three subsets chosen from Oct, Nov, Dec), and one snapshot (Oct, Nov, Dec taken individually). For two and one snapshot subsets of which there are three each, we take the mean as the representative, since we observed only minor variation in the accuracy. Raw data for all subsets is available in the associated repository for further examination. Figure 6(a) shows the gaps for all models, and Figure 6(b) shows GPT4's gap separately. We find that three snapshots suffice to stabilize the reasoning gap. Including further snapshots does not materially alter the gap. We take this to mean that tests that are solved by</p>
<div class="codehilite"><pre><span></span><code><span class="nx">def</span><span class="w"> </span><span class="nx">problem</span><span class="p">(</span><span class="nx">spinner1</span><span class="p">:</span><span class="w"> </span><span class="nx">list</span><span class="p">,</span><span class="w"> </span><span class="nx">spinner2</span><span class="p">:</span><span class="w"> </span><span class="nx">list</span><span class="p">,</span><span class="w"> </span><span class="nx">num_digits</span><span class="p">:</span><span class="w"> </span><span class="nx">int</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nx">str</span><span class="p">:</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">helper</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">csp</span><span class="err">&#39;</span><span class="w"> </span><span class="nx">formats</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">list</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">comma</span><span class="w"> </span><span class="nx">separated</span><span class="w"> </span><span class="nx">format</span><span class="w"> </span><span class="k">like</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">problem</span><span class="w"> </span><span class="nx">statement</span><span class="w"> </span><span class="err">&#39;</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="mi">5</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">to_word</span><span class="w"> </span><span class="nx">creates</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">english</span><span class="w"> </span><span class="nx">word</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">f</span><span class="s">&quot;Spinner I is divided into four equal sections labeled {csp(spinner1)}. &quot;</span><span class="w"> </span>\
<span class="w">        </span><span class="nx">f</span><span class="s">&quot;Spinner II is divided into five equal sections labeled {csp(spinner2)}. &quot;</span><span class="w"> </span>\
<span class="w">        </span><span class="nx">f</span><span class="s">&quot;If each spinner is spun and the resulting numbers are multiplied, &quot;</span><span class="w"> </span>\
<span class="w">        </span><span class="nx">f</span><span class="s">&quot;what is the probability that the product is a {to_word(num_digits)}-digit &quot;</span><span class="w"> </span>\
<span class="w">        </span><span class="nx">f</span><span class="s">&quot;even number? Express your answer as a common fraction.&quot;</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">solution</span><span class="p">(</span><span class="nx">spinner1</span><span class="p">:</span><span class="w"> </span><span class="nx">list</span><span class="p">,</span><span class="w"> </span><span class="nx">spinner2</span><span class="p">:</span><span class="w"> </span><span class="nx">list</span><span class="p">,</span><span class="w"> </span><span class="nx">num_digits</span><span class="p">:</span><span class="w"> </span><span class="nx">int</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nx">str</span><span class="p">:</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">total</span><span class="w"> </span><span class="nx">possible</span><span class="w"> </span><span class="nx">permutations</span>
<span class="w">    </span><span class="nx">total_permutations</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">len</span><span class="p">(</span><span class="nx">spinner1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">len</span><span class="p">(</span><span class="nx">spinner2</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">result</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">need</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="nx">spinner</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">even</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">product</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">num_digits</span><span class="err">&#39;</span>
<span class="w">    </span><span class="nx">total_count</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">sum</span><span class="p">(</span>
<span class="w">        </span><span class="mi">1</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">spinner1</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nx">j</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">spinner2</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">is_n_digit</span><span class="p">(</span><span class="nx">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">j</span><span class="p">,</span><span class="w"> </span><span class="nx">num_digits</span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">Simplifying</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">fraction</span>
<span class="w">    </span><span class="nx">factor</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">math</span><span class="p">.</span><span class="nx">gcd</span><span class="p">(</span><span class="nx">total_count</span><span class="p">,</span><span class="w"> </span><span class="nx">total_permutations</span><span class="p">)</span>
<span class="w">    </span><span class="nx">numerator</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">total_count</span><span class="w"> </span><span class="c1">// factor</span>
<span class="w">    </span><span class="nx">denominator</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">total_permutations</span><span class="w"> </span><span class="c1">// factor</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">f</span><span class="s">&quot;&quot;&quot;\\frac{{{numerator}}}{{{{denominator}}}&quot;&quot;&quot;</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">inputs</span><span class="p">(</span><span class="nx">rngs</span><span class="p">,</span><span class="w"> </span><span class="nx">seed</span><span class="p">):</span>
<span class="w">    </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">set_seed</span><span class="p">(</span><span class="nx">seed</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">random</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">digit</span><span class="w"> </span><span class="nx">count</span>
<span class="w">    </span><span class="nx">digit_count</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">even_between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span>
<span class="w">    </span><span class="nx">one_num_count</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">digit_count</span><span class="w"> </span><span class="c1">// 2</span>
<span class="w">    </span><span class="nx">lower_limit</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">10</span><span class="o">**</span><span class="w"> </span><span class="p">(</span><span class="nx">one_num_count</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="nx">upper_limit</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="nx">one_num_count</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="nx">d</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">spinner</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">e</span><span class="p">,</span><span class="w"> </span><span class="nx">f</span><span class="p">,</span><span class="w"> </span><span class="nx">g</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">spinner</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">Only</span><span class="w"> </span><span class="nx">numbers</span><span class="w"> </span><span class="nx">generated</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">than</span><span class="w"> </span><span class="nx">half</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">range</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">values</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">will</span><span class="w"> </span><span class="nx">multiply</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">give</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">double</span><span class="w"> </span><span class="nx">digit</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">results</span><span class="p">.</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">So</span><span class="w"> </span><span class="nx">d</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">f</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">fixed</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">having</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">double</span><span class="w"> </span><span class="nx">digit</span><span class="w"> </span><span class="nx">result</span><span class="p">.</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">Rest</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">products</span><span class="w"> </span><span class="nx">will</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">random</span>
<span class="w">    </span><span class="nx">a</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">int_between</span><span class="p">(</span><span class="nx">lower_limit</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">b</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">int_between</span><span class="p">(</span><span class="nx">lower_limit</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">c</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">natural_int</span><span class="p">(</span><span class="mi">100000</span><span class="p">)</span>
<span class="w">    </span><span class="nx">d</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">even_between</span><span class="p">((</span><span class="nx">lower_limit</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span><span class="w"> </span><span class="c1">// 2, upper_limit)</span>
<span class="w">    </span><span class="nx">e</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">odd_between</span><span class="p">(</span><span class="nx">lower_limit</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">f</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">odd_between</span><span class="p">((</span><span class="nx">lower_limit</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span><span class="w"> </span><span class="c1">// 2, upper_limit)</span>
<span class="w">    </span><span class="nx">g</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">odd_between</span><span class="p">(</span><span class="nx">lower_limit</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">h</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">odd_between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">i</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">odd_between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">spinner1</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">[</span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="nx">d</span><span class="p">]</span>
<span class="w">    </span><span class="nx">spinner2</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">[</span><span class="nx">e</span><span class="p">,</span><span class="w"> </span><span class="nx">f</span><span class="p">,</span><span class="w"> </span><span class="nx">g</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">]</span>
<span class="err">#</span><span class="w"> </span><span class="nx">Adding</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">numbers</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">spinners</span>
<span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">range</span><span class="p">(</span><span class="nx">rngs</span><span class="p">.</span><span class="nx">int_between</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)):</span>
<span class="w">    </span><span class="nx">spinner1</span><span class="p">.</span><span class="nx">append</span><span class="p">(</span><span class="nx">rngs</span><span class="p">.</span><span class="nx">int_between</span><span class="p">(</span><span class="nx">lower_limit</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">))</span>
<span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;spinner1&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">spinner1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;spinner2&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">spinner2</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;num_digits&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">digit_count</span><span class="p">}</span>
</code></pre></div>

<p>Figure 4: Functionalization of the text QA from Figure 3</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 5: (a) Reasoning gap (note: x-axis starts at $50 \%$ ), (b) Coverage: fraction of static QA that the model solves correctly that are tested functionally, (c) Static and functional accuracies.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 6: Gap starts at close to final value with a single snapshot, increases slightly with 2 snapshots, and essentially stabilizes at 3 snapshots.
the models consistently with $k=3$ are solved by reasoning about the problem. The likelihood of accidentally solving three separate snapshots, including the static variant, is miniscule.</p>
<p>Gap across difficulty levels in MATH() MATH has problems categorized into five levels. Figure 7 shows the gap against these levels. The levels in the test dataset roughly correspond to the difficulty of the problems, although our subjective assessment indicates that levels 1-3 are hard to discern as distinct from each other. We find that the reasoning gap expectedly increases with difficulty levels, suggesting that the models are capable of simpler reasoning at the lower difficulty levels and solve harder problems with more memorization (Figure 7(a)). The trend is similar for the top model (GPT4), except its gap is lower for each individual level (Figure 7(b)).</p>
<p>Gap across subject levels in MATH() Figure 8 shows the gap against subjects. Precalculus, prealgebra, and intermediate algebra are the easiest problems in</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 7: Gap across levels, aggregated across all models and for GPT4 separately.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 8: Gap across subjects, aggregated across all models and for GPT4 separately.</p>
<p>the dataset and the reasoning gap is expectedly lowest for those, indicating again that the models are solving problems under those subjects with proper reasoning (Figure 8(a)). Similar trends hold across the aggregated set of models and in the top model (GPT4), although GPT4's gap is lower across all subjects (Figure 8(b)).</p>
<h1>5.3 Analysis of problems solved across all snapshots, i.e., with proper reasoning</h1>
<p>Problems solved by the top model (GPT4) GPT4 solves 1299 static QA problems (accuracy $=25.98 \%$ ), and its functional accuracy with 3 -snapshots is $10.82 \%(541 / 5000)$, giving it the lowest reasoning gap of $58.35 \%$ amongst the models tested. Of these:</p>
<ul>
<li>239 were ungeneralizable, and their functional versions are static. This means that $4.78 \%$ of its functional accuracy can be attributed to being static.</li>
<li>302 had their functional snapshots solved, giving it a $6.04 \%$ accuracy on problems that indeed were different across snapshots.</li>
</ul>
<p>We manually categorized each of the 302 that were solved based on a) hardness as indicated by levels and subjects, b) our subjective surprise (low, medium, high) that they were solved based on the individual steps and process required to solve them, c) our subjective tag of the type of reasoning needed to solve.</p>
<p>Solved hardness, by levels: We find an expected dropoff across levels (Figure 9(a)), except for level 1. Human assessment from the team of mathematicians and computer scientists doing the functionalization, we find little difference between levels 1 and 2, and if we aggregate them then we find a monotonic drop in numbers solved.</p>
<p>Solved hardness, by subjects: For subjects (Figure 9(b)), prealgebra and algebra are outliers in being easier to solve. Most of the problems here are of the "grade school math" style problems where the task is to interpret the english description into a simple arithmetic equation. Their high solved counts here reinforce the commonly held perception that SOTA models are good at solving those problems.</p>
<p>224 low-, 63 medium-, 15 high-surprise problems solved: We find 2-4 problems with high surprise ( 15 in total) distributed across all levels (Figure 9(c)). It might be instructive to the community with access to the model and training dataset to understand the model's ability in solving these. The distribution of medium surprise problems follows an expected normal distribution.</p>
<p>14 solving strategies: We identify 14 subjective subtypes (Figure 9(d)) of solving strategies that would be needed for the 302 problems. Of these the extremes are worth noting. The highest count is for simple calculations by a significant margin, which aligns with capabilities, especially, if access to a calculator is in-built. The lowest counts are for figure interpretation ( 1 instance), trigonometry ( 1 instance), simplification insight ( 1 instance), and invent terms to simplify ( 3 instances). All these are worthy of individual investigation as they indicate surprising capabilities.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 9: 302 problems were solved by GPT4 across all snapshots.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 10: Problems solved by the majority $(\geq 5 / 10)$ of OSS models.</p>
<p>Appendix 11 lists a) two illustrative examples for each of the 14 reasoning subtypes, and b) the 15 specific high-surprise problems solved.</p>
<p>Problems solved by a majority of the OSS models The accuracy of OSS models varies between $1.86 \%-12.28 \%$ and it drops to $0.52 \%-4.34 \%$ for the functional snapshots. Only 19 problems are consistently solved across snapshots by the majority $(\geq 5 / 10)$ of models.</p>
<p>Solved hardness, by levels: As argued earlier, levels 1 and 2 are not substantially different, and we find that OSS models essentially solve these levels, except for one outlier in level 3 (Figure 10(a)).</p>
<p>Solved hardness, by subjects: OSS models solutions are limited to the easier subjects of prealgebra, algebra, intermediate algebra, and a single solution in geometry (Figure 10(b)).</p>
<p>19 low-surprise problems solved: There are no medium or high surprise problems consistently solved by the majority of the models (Figure 10(c)).</p>
<p>2 reasoning subtypes: The problems solved consistently were either simple calculations, or involving use of simple properties of operators (Figure 10(d)). Appendix 12 list a) 14 problems solved which were simple calculations, b) 5 problems which required properties of operators.</p>
<h1>6 Threats to the validity of results</h1>
<ul>
<li>More sophisticated prompting will likely change precise reasoning gap: Modelspecific prompting (e.g., Claude2.1 retrieval prompting [1]) or model-agnostic methods such as chain-of-thought [80], tree-of-thought [85], chain-of-code [41], chain-of-thought-decoding [78], tipping/threats/persuasion prompting to elicit more accurate output, are likely to reduce the reasoning gap. In fact, even if accuracy remains the same, the reduced reasoning gap may quantitatively validate the utility of these improvements.</li>
<li>Tool usage: For closed weights models accessed through APIs, it is unclear whether tool usage is part of the inference pipeline. We expect the use of tools, e.g., calculators or theorem provers, to reduce the reasoning gap. In future work, we will evaluate the change in gap with or without tool use with open weight models.</li>
<li>Default chain of thought, i.e., unable to format according to few-shot format: Some models seem to not follow instructions, or learn in-context with few shot examples, as others. In particular, we find Anthropic's Claude 2.1 and Mistral models tend to default output chain-of-thought steps, which does not work very well with the evaluation harness.</li>
<li>Scenarios where functional variant is not general enough: When we could not generalize a test, we marked it as "is static". This could happen at two extremes: i) when the problem is already in symbolic form or, ii) only permits a single formulation. These labelled cases are the reason for less than $100 \%$ coverage in Section 5.2. Aside from these cases, there might be scenarios where we functionalize but the generalization is simple enough that a pattern match against the original static QA would permit solving the functional snapshots. These cases should be rare.</li>
<li>Prompt injection in closed source models or non-determinism in closed source APIs: We do not have access to the additional system prompts being added by the closed source APIs for safety, which might cause reduced accuracy.</li>
<li>
<p>Big numbers: On rare occasions the random number functions might instantiate an excessively large number, e.g., with $10+$ digits, and it would be unreasonable for the model to compute that even if the reasoning used was correct.</p>
</li>
<li>
<p>Output matching: The evaluation code attempts to be liberal in equivalence checking for outputs, but has noticeable limitations. Cases we observe, and might be hard automatically check for, but would be judged as correct by a human evaluator include: a) $\mathrm{C}(21,7)$ and 9 choose 2 instead of their evaluated counterparts, b) expanded output expressions such as $5!=5 \times 4 \times 3 \times 2 \times 1$ and $33(22-12+1)=33 * 11=363$, c) extra formatting characters in output such as LaTeX math delimiters $\$ \backslash f r a c{1}{16} \$$, d) extra english verbiage such as $g(x)=3-2 f(x)$ " (for ground truth " $3-2 f(x)$ ") and
"The remainder when $\$ 2^{\wedge} 8 \$$ is divided by 5 is 1.".</p>
</li>
</ul>
<h1>7 Related Work</h1>
<p>Reasoning benchmarks Many popular benchmarks exist for testing reasoning, focussing on subdomains such abstract reasoning (ARC [17]), mathematical (MATH [29] and GSM8K [20]), code (HumanEval [12], MBPP [7], and Natural2Code [71]), commonsense and world knowledge (HellaSwag [89], Winogrande [59], PIQA [11], SIQA [60], OpenbookQA [50], ARC-Easy/Challenge [19], CommonsenseQA [70], NaturalQuestions [38], TriviaQA [36]), logical (LSAT [95]), or legal (LegalSupport [43]). Aggregated benchmarks collect many of these domains and attempt to give a comprehensive view of a generalized capabilities for foundational models (MMLU [28], HELM [43], BBH [69], and AGI Eval [94]). The aggregated benchmarks cover too much ground and might not be the best metrics for reasoning. There are concerns of contamination and overoptimization towards popular benchmarks leaving a gap between evaluated capabilities, and experienced capabilities over real-world tasks.</p>
<p>To alleviate that concern, there has been a push towards building new, and usually more difficult, text QA benchmarks for science and math (e.g., GPQA [58], ARB [61], JEE [6], CLRS [74], ProofNet [8]), agentic and world model reasoning (e.g., GAIA [49], WorldSense [10]) and code (e.g., CRUXEval [27], SWE-Bench [34], xCodeEval [37]). A recent survey [68] provides a comprehensive list of reasoning tasks by categories. We worry that without a systemic move away from text QAs, such benchmarks are prone to eventual leakage, and overestimation of solving capabilities of existing models.</p>
<p>Alternative benchmarking techniques are being proposed, e.g., through uncertainty quantification [87], ROSCOE step-by-step scoring [25], counterfactual facts [81].</p>
<p>We would suggest using our functionalization or the alternative benchmarking techniques, applied over existing popular or new text benchmarks as the way forward.</p>
<p>Techniques to improve reasoning in language models There is a vast body of ongoing work on improving reasoning capabilities of language models. Especially modifications to training data, or fine tuning recipes. We do not intend cover that literature. Instead, we mention techniques that might impact the reasoning gap during inference, assuming a given base model.</p>
<ul>
<li>Specialized prompting to elicit explicit reasoning: Techniques such as chain of thought [80], tree of thought [85], chain of code [41], chain of hindsight [46], program of thought [13], algorithmic skill prompting [97], progressive hint</li>
</ul>
<p>prompting [93] give models more scratch space to make the reasoning logic explicit, and arguably decompose the argument into simpler more tractable steps. They have been shown to improve accuracy, and we would conjecture they would reduce the gap. In future work we will explore how they change the gap.</p>
<ul>
<li>Augmentation: Delegating the last mile of the inference to a tool [26, 55] (calculator, web search [86], theorem prover, symbolic solver [21], planner [45], interpreter [24]) is bound to help reduce the reasoning gap if the model knows how to decompose the task into appropriate symbolic form.</li>
<li>Self-inspection or related post-processing to improve generalization: Various techniques have been proposed for using the core feedforward network as a unit module around with an inference pipeline can be built. These include using indirect reasoning [92], self-critique [3], divide-and-conquer [48], sampled math or code prompting [32], planner guided decoding [91], self-consistency [77], recursive code template improver [88] logic guide-driven inference [56], selfdebug [14], least-to-most prompting [96], self-discover to compose reasoning structures [98]. It is an open problem how much the accuracy improvements using these techniques translate to lowered reasoning gaps.</li>
</ul>
<p>Understanding the bounds of reasoning, generalization, and memorization in large language models Evaluations are the guides against which we build. They test the end artifact. If we need improved reasoning in these end artifacts, using reasoning-specific training data has shown to help-e.g., ORCA [52, 51], large models as reasoning teachers [30], textbook quality data [42], and code data [83]. Additionally, reasoning-specific fine-tuning algorithms help improve performance after pre-training-e.g., process supervision [44, 75], self-distilling context [66], feedback training for math using oracle teachers [65, 4], self-play finetuning for weak to strong learning [15], refusal-aware fine tuning to reduce hallucinations [90], improving reasoning by removing higher order components [63], and self-instruct [79].</p>
<p>At the other end of the spectrum are investigative analyses that improve our understanding. We now know it is possible to extract training data [53] so some memorization is inevitable, and that the notion of emergent properties might have been a result of choice of metrics [62]. Task contamination is an issue we need to be careful of [40]. Some encouraging results show evidence of multi-hop reasoning [84] while others demonstrating difficulty of such reasoning [39]. Evidence exists for transformer models identifying training data patterns very robustly, while performance degrading outside of distribution [82]. Reasoning might emerge from path-aggregation in the training data [76], and that asking for lengthier answers may improve reasoning [35]. Models are unlikely to know when they are violating formal rules and it is unclear whether they can self-correct [73, 31, 67], but with specific fine-tuning they might self-correct against harmful text [23], and that training on generated data might not be the best approach to preserve reasoning about outlier cases [64].</p>
<p>Given such evolving understanding, a proper scientific approach should innovate on evaluations based on the most recent understanding, so that progress stays stays aligned with real-world experience.</p>
<h1>8 Individual contributors and acknowledgments</h1>
<p>Annarose, Shashank, Anto, Ajay, Adwaith, Alan, and Stevin did the major work of converting each MATH test into code, and Sooraj oversaw their work. Saurabh conceptualized the functionalization and reasoning gap framework, implemented the evaluation code, and wrote the paper. Feedback from initial reviewers Kelly Iknayan, Ashish Agarwal, Prashast Srivastava, Henele Adams, and Soham Mazumdar, helped improve the manuscript.</p>
<h2>9 Future work</h2>
<p>This is part of an ongoing effort to build reasoning metrics that are robust against contamination, and a more accurate representation of model capabilities. In subsequent work, we will functionalize the entire MATH test suite of 5000 problems, and the 1000 GSM8K test problems. We will also use a similar strategy to functionalize the code benchmarks HumanEval, and MBPP.</p>
<p>Additionally, once the benchmarks are $100 \%$ functionalized, we will do studies on the effect of prompting and augmentation strategies (e.g., chain-of-thought, chain-of-code, tree-of-thought, tool usage) on the reasoning gap.</p>
<h2>10 Conclusion</h2>
<p>There is a disconnect between the high benchmark scores in reasoning and the observed below-average reasoning of state-of-the-art models. We propose the reasoning gap metric to quantify this difference, which motivates the open problem of building gap-0 models. Reasoning gaps can be evaluated using the framework of functionalized benchmarks, which are presented as a long term solution to evaluating reasoning on tasks that are known to be within the scope of current models, while presenting them versions they have not seen before. We have functionalized the relevant portion of the popular MATH benchmark, called MATH(), and in future work will extend this to other gold-standard reasoning benchmarks. We are releasing the evaluation code, and the Q1'24 snapshot of MATH(), and will continue to release new snapshots of functionalized benchmarks every quarter.</p>
<h2>References</h2>
<p>[1] Long context prompting for claude 2.1, 2023.
[2] 01.AI. Building the next generation of open-source and bilingual llms, 2023.
[3] Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, and Sanjiv Kumar. Rest meets react: Selfimprovement for multi-step reasoning llm agent, 2023.
[4] Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, and Sanjiv Kumar. Rest meets react: Selfimprovement for multi-step reasoning llm agent, 2023.</p>
<p>[5] Anthropic. Introducing claude 2.1, 2023.
[6] Daman Arora, Himanshu Gaurav Singh, and Mausam. Have llms advanced enough? a challenging problem solving benchmark for large language models, 2023.
[7] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021.
[8] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics, 2023.
[9] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.
[10] Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean-Baptiste Gaya, Xavier Martinet, Grégoire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke Hupkes, and Pascal Vincent. Worldsense: A synthetic benchmark for grounded reasoning in large language models, 2023.
[11] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. CoRR, abs/1911.11641, 2019.
[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.
[13] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author. Please send all correspondence to saurabh@consequent.ai.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2120 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2120</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2120</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-276558281</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.15597v1.pdf" target="_blank">From FAIR to CURE: Guidelines for Computational Models of Biological Systems</a></p>
                <p><strong>Paper Abstract:</strong> Guidelines for managing scientific data have been established under the FAIR principles requiring that data be Findable, Accessible, Interoperable, and Reusable. In many scientific disciplines, especially computational biology, both data and models are key to progress. For this reason, and recognizing that such models are a very special type of “data”, we argue that computational models, especially mechanistic models prevalent in medicine, physiology and systems biology, deserve a complementary set of guidelines. We propose the CURE principles, emphasizing that models should be Credible, Understandable, Reproducible, and Extensible. We delve into each principle, discussing verification, validation, and uncertainty quantification for model credibility; the clarity of model descriptions and annotations for understandability; adherence to standards and open science practices for reproducibility; and the use of open standards and modular code for extensibility and reuse. We outline recommended and baseline requirements for each aspect of CURE, aiming to enhance the impact and trustworthiness of computational models, particularly in biomedical applications where credibility is paramount. Our perspective underscores the need for a more disciplined approach to modeling, aligning with emerging trends such as Digital Twins and emphasizing the importance of data and modeling standards for interoperability and reuse. Finally, we emphasize that given the non-trivial effort required to implement the guidelines, the community moves to automate as many of the guidelines as possible.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2120.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2120.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioSimulations verification service</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioSimulations verification service (BioSimulation resource)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated service that runs submitted computational models across multiple independent simulators to verify simulator consistency and catch implementation/interpretation differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioSimulations: a central registry of simulation engines and services for recommending specific tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BioSimulations verification service</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A web/resource service that executes a given model on multiple SBML‑capable simulators (or other modeling engines) to confirm consistent simulation results and thereby verify both the model encoding and the engines' implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational systems biology; mechanistic biological modeling</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is performed by re-running the same model across independent simulation engines and comparing simulation outputs for consistency (a form of cross‑simulator verification). The paper states the service 'provides a verification service where a given model can be run against multiple independent simulators to help verify the simulation engines.' No experimental wet‑lab procedures are performed — this is a computational conformance/consistency check.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>low-level implementation/conformance check rather than physics fidelity; fidelity depends on simulators used — primarily a correctness/consistency test of numerical implementations rather than aiming for high‑fidelity physical modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper treats this as necessary for verification but not sufficient for full credibility; domain norms require both V&V and UQ, and for biomedical applications experimental validation is also typically required.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical experiments — purpose is cross‑simulator computational verification. The paper notes this helps verify simulation engines but does not replace experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compares simulator outputs to one another to detect discrepancies; paper does not provide numerical comparisons, only describes the approach qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No specific cases of failure reported in this paper; the method is presented as a way to detect inconsistencies (i.e., potential failures of implementation or interpretation across tools).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Not detailed here; success implied when multiple independent simulators produce consistent outputs for the same explicit model representation.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No direct ground‑truth (experimental) comparison — comparison is between simulators (consistency benchmark), not against physical measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Approach is aimed at improving reproducibility by ensuring consistent simulator behavior; paper recommends using such automated checks to increase reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not quantified in the paper; implied low to moderate computational cost depending on number/complexity of simulators and models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Presented as a standard computational practice for mechanistic models (systems biology) to verify software/simulator correctness; domain norms still require experimental validation for biomedical credibility.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not part of this service; focuses on deterministic consistency checks rather than propagating or quantifying parameter/data uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Does not assess whether model predictions match real world (no wet‑lab/experimental comparison); only detects differences in implementation/interpretation across simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2120.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2120.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MEMOTE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MEMOTE (standardized genome‑scale metabolic model testing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated software tool that performs standardized quality checks and deep analyses of genome‑scale metabolic models to assess model quality and detect formulation issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MEMOTE for standardized genome-scale metabolic model testing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MEMOTE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated test suite for genome‑scale metabolic models that performs a battery of structural and annotation checks, scoring and reporting on model completeness, consistency, and best‑practice compliance.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Genome-scale metabolic modeling; systems biology</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Performs automated static and dynamic checks on model files (SBML or similar) to identify annotation completeness, stoichiometric consistency, mass/balance errors, and other model formulation problems; outputs standardized reports and quality scores.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Analytical/structural checks and low‑level algorithmic tests (not physics‑based simulation); fidelity is high for detecting syntactic and many semantic issues but not intended to validate predictions against experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper positions MEMOTE as valuable for model verification/quality assurance; not sufficient alone for predictive validation — experimental comparisons and UQ still required for model credibility in biomedical contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No experimental work; MEMOTE inspects model internals and annotations rather than comparing outputs to empirical data.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compares model structural properties against a set of recommended best practices and tests; no explicit simulator vs experiment comparison described.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No concrete failure case in this paper; limitations noted that MEMOTE cannot by itself confer predictive credibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Used successfully in community to standardize genome‑scale model quality assessments (cited as a 'successful example' of automated software that can do deep dives into genome‑scale models).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not applicable — focuses on model structure and metadata, not ground‑truth predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Intended to improve reproducibility by standardizing quality checks and making issues visible across models.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not quantified; automated computational checks imply modest compute cost compared to full simulations or experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In metabolic modeling, structural and annotation validation is a community norm before attempting predictive validation; MEMOTE supports these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>MEMOTE does not perform UQ; UQ is highlighted elsewhere in the paper as a separate component of credibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Cannot replace experimental validation or uncertainty quantification; mainly detects formulation/annotation problems.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2120.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2120.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SBML Test Suite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SBML Test Suite</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of test models and expected outputs used to verify conformance of SBML‑capable simulators and ensure consistent interpretation of SBML models across tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SBML Test Suite.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SBML Test Suite</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A standardized test collection and reference outputs used to check that different SBML interpreters and simulators implement the SBML specification correctly and reproduce canonical results.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational systems biology; model exchange/standards</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Verification via execution of canonical SBML test models across tools and comparing outputs to reference results; intended to detect inconsistencies in SBML parsing, numerical algorithms, and interpretation of model constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Not about physical fidelity; it's a conformance/consistency validation (high fidelity for detecting spec non‑conformance but not predictive accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient for software verification (ensures tools interpret SBML consistently); insufficient for validating correspondence between model predictions and real biology.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No wet‑lab experiments; purely computational conformance tests.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compares tool outputs against canonical expected outputs; paper does not provide numerical success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Paper does not enumerate failures, but the test suite is explicitly intended to catch tool-specific failures in SBML interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Used broadly to improve simulator interoperability; cited as part of verification practices.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No experimental ground truth involved.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Contributes to reproducibility by ensuring consistent simulation behavior across software.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Community norm for SBML‑based modelling to use the test suite as part of verification.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Does not test biological realism; only software/spec conformance.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2120.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2120.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross‑validation (training/test split)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross‑validation and training/test split methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard statistical method to assess model generalization by holding out a portion of data for testing after training or calibration on the remainder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Cross‑validation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A validation approach in which experimental (or observational) data are split into training and test sets; the model is calibrated on training data and its predictive ability is evaluated on held‑out test data (including k‑fold cross‑validation variants).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General modeling practice; systems biology parameter estimation; machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Computational resampling using experimental datasets: split dataset into calibration/training and independent test sets to evaluate predictive performance; paper recommends this as a way to enhance credibility of parameter estimates and model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Not a simulation fidelity metric — method uses empirical data; fidelity of validation depends on quality/representativeness of held‑out data.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper treats cross‑validation as an important component of credibility; for many biomedical use cases experimental validation beyond cross‑validation may still be required — cross‑validation increases confidence but does not guarantee real‑world generalization if data are biased or limited.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Method uses existing experimental data; the paper does not report particular experimental protocols but recommends cross‑validation as statistical validation practice.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Paper contrasts cross‑validation with using competing models and with the need for independent experimental validation; no numeric comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Paper notes limitations: cross‑validation can fail if data are biased or not representative; nonidentifiability of parameters can remain despite cross‑validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Described generically: successful when calibrated model recovers test set, increasing confidence in model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Ground truth in this approach is the held‑out empirical data; paper notes that success increases confidence but is not absolute proof of real‑world validity.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Cross‑validation aids reproducibility by providing an objective held‑out test; independent replication still recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computational cost depends on model complexity and number of folds; not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In systems biology and machine learning, splitting data into training/validation/test is a normative expectation for assessing predictive models.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Cross‑validation can be used together with UQ to quantify predictive uncertainty due to limited data; paper also recommends Bayesian UQ methods.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Cannot fully resolve non‑identifiability and can be undermined by biased or insufficiently diverse data.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Combines computational resampling with empirical (experimental) datasets; regarded as part of a broader validation strategy that should include UQ and, where possible, independent experimental tests.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2120.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2120.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VVUQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verification, Validation, and Uncertainty Quantification (VVUQ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combined methodological framework for establishing model credibility by checking implementation correctness (verification), comparing model outputs to reality (validation), and quantifying uncertainties (UQ).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>VVUQ</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An integrated approach used in many scientific domains that combines software/model verification, empirical validation against experimental data, and formal uncertainty quantification to characterize confidence in model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational modeling broadly; emphasized for biomedical models and digital twins</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Verification: software/spec conformance and numerical tests (e.g., SBML test suite, cross‑simulator runs); Validation: comparison of model outputs to experimental/observational data for intended uses; UQ: propagation and quantification of uncertainties in parameters/data to model outputs, often via Bayesian or statistical methods.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>VVUQ spans conformance checks and predictive validation — fidelity depends on the underlying model and availability of experimental data; UQ may use first‑principles or empirical uncertainty models.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper presents VVUQ as foundational for credibility and cites regulatory and National Academies guidance endorsing VVUQ; in biomedical domains VVUQ (including experimental validation) is implied to be necessary for high‑stakes use (e.g., clinical applications, digital twins).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Paper discusses VVUQ conceptually; it notes UQ and validation are often emphasized by regulatory documents but does not present new experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Paper compares roles of verification vs validation vs UQ conceptually and recommends all three be automated where possible; no numeric comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Paper notes that lack of UQ and incomplete validation undermines credibility; gives general examples of ML models with spurious correlations (see chest x‑ray example) where validation was insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Regulatory and National Academies documents cited as endorsing VVUQ for credible models; BioSimulations and MEMOTE given as partial automation examples that support aspects of VVUQ.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Validation component explicitly involves comparison to experimental ground truth when available; paper stresses 'intended uses' and scope when defining validation targets.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>VVUQ is presented as necessary for reproducibility and for communicating confidence; automation of VVUQ is recommended to ease practitioner burden.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper notes burden of performing full VVUQ and recommends automation to reduce time and effort; no quantitative estimates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>For biomedical modeling and medical devices, domain norms (and regulators like FDA) expect VVUQ practices including experimental validation and UQ.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Central to VVUQ; paper specifically emphasizes Bayesian techniques and quantification of how parameter/data uncertainty propagates to outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>VVUQ can be effort intensive; limited by availability of high‑quality experimental data and by nonidentifiability; automation can help but cannot replace experimental data where required.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>VVUQ is inherently hybrid: computational verification and UQ combined with experimental validation when available; the paper recommends combining cross‑validation, competing models, experimental perturbation validation, and UQ.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2120.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2120.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CPMS ten rules / rubric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Credible Practice of Modeling & Simulation in Healthcare: Ten Rules (CPMS) and associated rubric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of community rules for credible modeling practice in healthcare focusing on practices (verification, validation, documentation, stakeholder engagement) rather than a single model's properties; a rubric was developed to assess conformance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Credible practice of modeling and simulation in healthcare: ten rules from a multidisciplinary perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CPMS ten rules and rubric</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Guidelines and an assessment rubric emphasizing modeling practices (e.g., purpose statement, verification/validation, provenance, outreach) to improve model credibility in healthcare; rubric supports self‑assessment and community sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Healthcare computational modeling; biomedical simulations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The rules emphasize verification and validation among other practices; the rubric evaluates to what extent recommended credibility practices (including validation against data and UQ) are met. Validation can include experimental comparisons and documentation of intended use, but CPMS focuses on process adherence.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Not a simulation engine — fidelity depends on the modeling study; the rubric assesses whether appropriate fidelity and validation standards were followed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper notes CPMS rules put emphasis on verification/validation as central to credibility; sufficiency depends on study purpose and stakeholder needs — rubric helps characterize sufficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>The CPMS guidance is procedural; it does not itself perform experiments but requires that appropriate validation be documented where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Paper references the rubric as a standardized way to compare conformance to credibility practices across studies; no numerical comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No specific failure cases, but the paper notes that many modelers do not follow all recommendations due to burden, motivating automation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Not enumerated here; CPMS guidance cited as influential community effort to improve credibility.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Rubric expects validation against appropriate ground truth for intended uses, but specifics are study‑dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Rubric and documentation practices aim to enhance reproducibility; authors recommend sharing self‑assessments as supplementary material.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper notes burden of fulfilling all rules and recommends automation to reduce time/effort.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In healthcare modeling, domain norms include rigorous verification, validation, UQ, documentation, and stakeholder engagement — CPMS codifies these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>CPMS rules include UQ as part of credibility assessment; the rubric assesses whether UQ was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Adoption is limited by practitioner effort and resources; the rubric is a self‑assessment and may reflect authors' biases unless independently reviewed.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Encourages combining software verification, empirical validation, and UQ alongside good documentation and stakeholder processes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2120.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2120.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FDA credibility guidance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Assessing the credibility of computational modeling and simulation in medical device submissions (FDA guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Regulatory guidance from the U.S. FDA that outlines expectations for establishing credibility of computational models used in medical device evaluations, emphasizing verification, validation, uncertainty quantification, and plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing the credibility of computational modeling and simulation in medical device submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FDA credibility guidance for computational models</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A regulatory framework describing how model credibility should be assessed for medical device submissions, prioritizing verification, validation, uncertainty quantification, and model plausibility (equations, assumptions, parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Regulatory biomedical modeling; medical devices</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Guidance requires VVUQ: verification of software/algorithms, validation against experimental/clinical data appropriate to intended use, uncertainty quantification, and documentation of model plausibility and limitations. It is regulatory (domain standard) rather than an automated system.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Requires fidelity appropriate to intended use; for medical devices, high fidelity and documented validation are typically expected.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper cites FDA guidance as authoritative for medical device modeling; for regulatory acceptance, the guidance sets a high bar — experimental validation and thorough UQ are commonly required.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Guidance recommends experimental or clinical data where appropriate; the paper references the guidance but performs no experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Guidance contrasts different levels of evidence depending on model purpose; paper uses it to justify baseline requirements but provides no numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not detailed here; guidance exists because inadequate validation has undermined past credibility of certain models.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Not enumerated in this perspective; FDA guidance provides criteria under which a model might be accepted by regulators when validation criteria are met.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Guidance requires comparisons to relevant experimental/clinical data as applicable to intended use.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Emphasizes reproducibility and documentation as part of credibility for regulatory submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not quantified here; implication that meeting regulatory standards can be resource intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>For medical device submissions, adherence to FDA VVUQ expectations is normatively required.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Explicitly required and emphasized by the guidance as foundational to credibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>High burden of evidence for regulatory acceptance; many community models may not meet these standards without additional work.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Regulatory approach mandates a hybrid combination of computational verification, empirical validation, and UQ tailored to the model's intended use.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2120.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2120.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SBML/Model‑checking tools</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model‑checking tools and automated verification for SBML models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated static and dynamic analysis tools (model‑checkers) that detect errors in model formulations and check properties like mass balance and state constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Model‑checking tools for SBML</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Tools that perform automated static analysis (e.g., detection of typographical or formulation errors) and dynamic checks (e.g., no negative concentrations) on explicit model representations (SBML/CellML) to assist verification.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational systems biology; model verification</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Verification via static analysis (semantic/syntactic checks, ontology/annotation checks) and dynamic checks (simulation invariants like non‑negativity); recommended to be included as automated validation tests bundled with model distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Analytical/conformance checks; fidelity is high for the kinds of errors they detect but does not validate real‑world predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Useful and recommended for verification, but insufficient alone for full model credibility which requires experimental validation and UQ when appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No experimental work performed — these are software/static checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Paper suggests providing validation tests in a standard format similar to software engineering unit/system tests; does not provide empirical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Paper does not list specific failure cases but implies many reproducibility problems stem from models lacking such checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Suggested as best practice; specific tool names beyond SBML test suite and MEMOTE are not exhaustively listed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Model‑checking tools increase reproducibility by making implementation errors visible and automating checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not quantified; automation intended to reduce time and burden.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Community expects use of such tools where possible for verification prior to publication or repository submission.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not part of model‑checking tools; separate UQ workflows are recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Cannot substitute for empirical validation; limited to errors amenable to static/dynamic checks.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2120.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2120.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chest x‑ray COVID ML failure example</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Example of ML model failure for COVID‑19 chest x‑ray diagnosis (spurious correlation with hospital coding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited cautionary example where an ML model achieved apparently accurate predictions for COVID‑19 from chest x‑rays but relied on hospital identifier artifacts rather than medical features, demonstrating failure of validation to detect spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>COVID chest x‑ray ML model (failure example)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An ML model trained to predict COVID‑19 from chest x‑rays that appeared accurate but was later shown to use hospital coding (surrogate variable) correlated with case prevalence rather than pathological image features.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Medical imaging; machine learning in medicine</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The paper references this as an illustrative failure of model credibility: model evaluation metrics indicated strong performance, but deeper examination (external validation/inspection) revealed the model used an unrelated confounder (hospital code). This highlights the need for careful validation including inspection for confounders, domain‑aware test sets, and provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Not applicable — this is a data‑driven ML validation failure rather than a simulation fidelity issue.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Serves as an example that conventional training/test metrics can be insufficient for credibility; domain norms require investigation of feature relevance and external validation across sites to ensure generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Paper does not provide numeric metrics for the failed model; it describes the phenomenon qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No wet‑lab experiments — issue demonstrated by data analysis and model inspection; paper uses this to argue that accurate predictions do not guarantee credibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Paper contrasts accurate predictive performance vs. lack of credible causal basis; implies that additional validation (e.g., multi‑site external validation, causal analyses) would be needed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Yes — model relied on surrogate (hospital coding) producing misleading accuracy on internal test data; validation failed to reveal lack of causal basis.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Not applicable; example used as a cautionary failure.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Model predictions matched labels in the training/test data but failed when considering true underlying medical relevance and external contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Paper uses example to stress need for robust external validation and provenance; does not report independent replication.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In medical ML, domain norms include external multi‑site validation, analysis of possible confounders, and clinical plausibility checks in addition to performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not explicitly discussed in relation to this example in the paper; the example highlights that uncertainty metrics alone may not catch confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Demonstrates that conventional cross‑validation or holdout testing can be misled by confounders and that domain knowledge and provenance are essential.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Would require combining statistical holdout testing with external clinical validation and causal/feature‑relevance analyses; the paper recommends such broader validation to avoid similar failures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2120.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2120.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioSimulators registry</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioSimulators: a central registry of simulation engines and services</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A registry that catalogs simulation engines and services, facilitating selection of appropriate tools and supporting reproducibility and verification workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioSimulators: a central registry of simulation engines and services for recommending specific tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BioSimulators registry</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A community resource listing simulation engines and services, intended to help practitioners choose tools and enable automated verification/compatibility checks across simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational systems biology; simulation infrastructure</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Supports validation by recommending simulators and enabling cross‑simulator comparisons; the registry itself is not a validator but facilitates V&V workflows by making compatible tools discoverable.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Registry neutral; fidelity derived from the listed simulators and their capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Registry aids verification workflows but does not replace the need for empirical validation or UQ.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No experiments; enables computational tool selection and verification workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Enables comparison of simulators by making them discoverable; paper cites BioSimulators as a supporting infrastructure for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Cited as part of community infrastructure that supports reproducible and verifiable modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Registry is intended to improve reproducibility by guiding choice of validated simulation engines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Community expects use of registered, SBML‑capable simulators for standard model exchange and verification practices.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not provided by registry.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Does not itself validate models or simulators; users must still perform VVUQ.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Assessing the credibility of computational modeling and simulation in medical device submissions. <em>(Rating: 2)</em></li>
                <li>Assessing the Reliability of Complex Models: Mathematical and Statistical Foundations of Verification, Validation, and Uncertainty Quantification. <em>(Rating: 2)</em></li>
                <li>MEMOTE for standardized genome-scale metabolic model testing. <em>(Rating: 2)</em></li>
                <li>BioSimulations: a central registry of simulation engines and services for recommending specific tools. <em>(Rating: 2)</em></li>
                <li>Credible practice of modeling and simulation in healthcare: ten rules from a multidisciplinary perspective. <em>(Rating: 2)</em></li>
                <li>SBML Test Suite. <em>(Rating: 2)</em></li>
                <li>Concepts of model verification and validation (No. LA-14167). <em>(Rating: 2)</em></li>
                <li>In silico trials: Verification, validation and uncertainty quantification of predictive models used in the regulatory evaluation of biomedical products. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2120",
    "paper_id": "paper-276558281",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "BioSimulations verification service",
            "name_full": "BioSimulations verification service (BioSimulation resource)",
            "brief_description": "An automated service that runs submitted computational models across multiple independent simulators to verify simulator consistency and catch implementation/interpretation differences.",
            "citation_title": "BioSimulations: a central registry of simulation engines and services for recommending specific tools.",
            "mention_or_use": "mention",
            "system_name": "BioSimulations verification service",
            "system_description": "A web/resource service that executes a given model on multiple SBML‑capable simulators (or other modeling engines) to confirm consistent simulation results and thereby verify both the model encoding and the engines' implementations.",
            "scientific_domain": "Computational systems biology; mechanistic biological modeling",
            "validation_type": "simulated",
            "validation_description": "Validation is performed by re-running the same model across independent simulation engines and comparing simulation outputs for consistency (a form of cross‑simulator verification). The paper states the service 'provides a verification service where a given model can be run against multiple independent simulators to help verify the simulation engines.' No experimental wet‑lab procedures are performed — this is a computational conformance/consistency check.",
            "simulation_fidelity": "low-level implementation/conformance check rather than physics fidelity; fidelity depends on simulators used — primarily a correctness/consistency test of numerical implementations rather than aiming for high‑fidelity physical modeling.",
            "validation_sufficiency": "The paper treats this as necessary for verification but not sufficient for full credibility; domain norms require both V&V and UQ, and for biomedical applications experimental validation is also typically required.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical experiments — purpose is cross‑simulator computational verification. The paper notes this helps verify simulation engines but does not replace experimental validation.",
            "validation_comparison": "Compares simulator outputs to one another to detect discrepancies; paper does not provide numerical comparisons, only describes the approach qualitatively.",
            "validation_failures": "No specific cases of failure reported in this paper; the method is presented as a way to detect inconsistencies (i.e., potential failures of implementation or interpretation across tools).",
            "validation_success_cases": "Not detailed here; success implied when multiple independent simulators produce consistent outputs for the same explicit model representation.",
            "ground_truth_comparison": "No direct ground‑truth (experimental) comparison — comparison is between simulators (consistency benchmark), not against physical measurements.",
            "reproducibility_replication": "Approach is aimed at improving reproducibility by ensuring consistent simulator behavior; paper recommends using such automated checks to increase reproducibility.",
            "validation_cost_time": "Not quantified in the paper; implied low to moderate computational cost depending on number/complexity of simulators and models.",
            "domain_validation_norms": "Presented as a standard computational practice for mechanistic models (systems biology) to verify software/simulator correctness; domain norms still require experimental validation for biomedical credibility.",
            "uncertainty_quantification": "Not part of this service; focuses on deterministic consistency checks rather than propagating or quantifying parameter/data uncertainties.",
            "validation_limitations": "Does not assess whether model predictions match real world (no wet‑lab/experimental comparison); only detects differences in implementation/interpretation across simulators.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2120.0"
        },
        {
            "name_short": "MEMOTE",
            "name_full": "MEMOTE (standardized genome‑scale metabolic model testing)",
            "brief_description": "An automated software tool that performs standardized quality checks and deep analyses of genome‑scale metabolic models to assess model quality and detect formulation issues.",
            "citation_title": "MEMOTE for standardized genome-scale metabolic model testing.",
            "mention_or_use": "mention",
            "system_name": "MEMOTE",
            "system_description": "Automated test suite for genome‑scale metabolic models that performs a battery of structural and annotation checks, scoring and reporting on model completeness, consistency, and best‑practice compliance.",
            "scientific_domain": "Genome-scale metabolic modeling; systems biology",
            "validation_type": "simulated",
            "validation_description": "Performs automated static and dynamic checks on model files (SBML or similar) to identify annotation completeness, stoichiometric consistency, mass/balance errors, and other model formulation problems; outputs standardized reports and quality scores.",
            "simulation_fidelity": "Analytical/structural checks and low‑level algorithmic tests (not physics‑based simulation); fidelity is high for detecting syntactic and many semantic issues but not intended to validate predictions against experiments.",
            "validation_sufficiency": "Paper positions MEMOTE as valuable for model verification/quality assurance; not sufficient alone for predictive validation — experimental comparisons and UQ still required for model credibility in biomedical contexts.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "No experimental work; MEMOTE inspects model internals and annotations rather than comparing outputs to empirical data.",
            "validation_comparison": "Compares model structural properties against a set of recommended best practices and tests; no explicit simulator vs experiment comparison described.",
            "validation_failures": "No concrete failure case in this paper; limitations noted that MEMOTE cannot by itself confer predictive credibility.",
            "validation_success_cases": "Used successfully in community to standardize genome‑scale model quality assessments (cited as a 'successful example' of automated software that can do deep dives into genome‑scale models).",
            "ground_truth_comparison": "Not applicable — focuses on model structure and metadata, not ground‑truth predictions.",
            "reproducibility_replication": "Intended to improve reproducibility by standardizing quality checks and making issues visible across models.",
            "validation_cost_time": "Not quantified; automated computational checks imply modest compute cost compared to full simulations or experiments.",
            "domain_validation_norms": "In metabolic modeling, structural and annotation validation is a community norm before attempting predictive validation; MEMOTE supports these norms.",
            "uncertainty_quantification": "MEMOTE does not perform UQ; UQ is highlighted elsewhere in the paper as a separate component of credibility.",
            "validation_limitations": "Cannot replace experimental validation or uncertainty quantification; mainly detects formulation/annotation problems.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2120.1"
        },
        {
            "name_short": "SBML Test Suite",
            "name_full": "SBML Test Suite",
            "brief_description": "A collection of test models and expected outputs used to verify conformance of SBML‑capable simulators and ensure consistent interpretation of SBML models across tools.",
            "citation_title": "SBML Test Suite.",
            "mention_or_use": "mention",
            "system_name": "SBML Test Suite",
            "system_description": "A standardized test collection and reference outputs used to check that different SBML interpreters and simulators implement the SBML specification correctly and reproduce canonical results.",
            "scientific_domain": "Computational systems biology; model exchange/standards",
            "validation_type": "simulated",
            "validation_description": "Verification via execution of canonical SBML test models across tools and comparing outputs to reference results; intended to detect inconsistencies in SBML parsing, numerical algorithms, and interpretation of model constructs.",
            "simulation_fidelity": "Not about physical fidelity; it's a conformance/consistency validation (high fidelity for detecting spec non‑conformance but not predictive accuracy).",
            "validation_sufficiency": "Sufficient for software verification (ensures tools interpret SBML consistently); insufficient for validating correspondence between model predictions and real biology.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "No wet‑lab experiments; purely computational conformance tests.",
            "validation_comparison": "Compares tool outputs against canonical expected outputs; paper does not provide numerical success rates.",
            "validation_failures": "Paper does not enumerate failures, but the test suite is explicitly intended to catch tool-specific failures in SBML interpretation.",
            "validation_success_cases": "Used broadly to improve simulator interoperability; cited as part of verification practices.",
            "ground_truth_comparison": "No experimental ground truth involved.",
            "reproducibility_replication": "Contributes to reproducibility by ensuring consistent simulation behavior across software.",
            "validation_cost_time": "Not discussed.",
            "domain_validation_norms": "Community norm for SBML‑based modelling to use the test suite as part of verification.",
            "uncertainty_quantification": "Not applicable.",
            "validation_limitations": "Does not test biological realism; only software/spec conformance.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2120.2"
        },
        {
            "name_short": "Cross‑validation (training/test split)",
            "name_full": "Cross‑validation and training/test split methods",
            "brief_description": "Standard statistical method to assess model generalization by holding out a portion of data for testing after training or calibration on the remainder.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Cross‑validation",
            "system_description": "A validation approach in which experimental (or observational) data are split into training and test sets; the model is calibrated on training data and its predictive ability is evaluated on held‑out test data (including k‑fold cross‑validation variants).",
            "scientific_domain": "General modeling practice; systems biology parameter estimation; machine learning",
            "validation_type": "hybrid",
            "validation_description": "Computational resampling using experimental datasets: split dataset into calibration/training and independent test sets to evaluate predictive performance; paper recommends this as a way to enhance credibility of parameter estimates and model predictions.",
            "simulation_fidelity": "Not a simulation fidelity metric — method uses empirical data; fidelity of validation depends on quality/representativeness of held‑out data.",
            "validation_sufficiency": "Paper treats cross‑validation as an important component of credibility; for many biomedical use cases experimental validation beyond cross‑validation may still be required — cross‑validation increases confidence but does not guarantee real‑world generalization if data are biased or limited.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "Method uses existing experimental data; the paper does not report particular experimental protocols but recommends cross‑validation as statistical validation practice.",
            "validation_comparison": "Paper contrasts cross‑validation with using competing models and with the need for independent experimental validation; no numeric comparisons provided.",
            "validation_failures": "Paper notes limitations: cross‑validation can fail if data are biased or not representative; nonidentifiability of parameters can remain despite cross‑validation.",
            "validation_success_cases": "Described generically: successful when calibrated model recovers test set, increasing confidence in model predictions.",
            "ground_truth_comparison": "Ground truth in this approach is the held‑out empirical data; paper notes that success increases confidence but is not absolute proof of real‑world validity.",
            "reproducibility_replication": "Cross‑validation aids reproducibility by providing an objective held‑out test; independent replication still recommended.",
            "validation_cost_time": "Computational cost depends on model complexity and number of folds; not quantified.",
            "domain_validation_norms": "In systems biology and machine learning, splitting data into training/validation/test is a normative expectation for assessing predictive models.",
            "uncertainty_quantification": "Cross‑validation can be used together with UQ to quantify predictive uncertainty due to limited data; paper also recommends Bayesian UQ methods.",
            "validation_limitations": "Cannot fully resolve non‑identifiability and can be undermined by biased or insufficiently diverse data.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Combines computational resampling with empirical (experimental) datasets; regarded as part of a broader validation strategy that should include UQ and, where possible, independent experimental tests.",
            "uuid": "e2120.3"
        },
        {
            "name_short": "VVUQ",
            "name_full": "Verification, Validation, and Uncertainty Quantification (VVUQ)",
            "brief_description": "A combined methodological framework for establishing model credibility by checking implementation correctness (verification), comparing model outputs to reality (validation), and quantifying uncertainties (UQ).",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "VVUQ",
            "system_description": "An integrated approach used in many scientific domains that combines software/model verification, empirical validation against experimental data, and formal uncertainty quantification to characterize confidence in model predictions.",
            "scientific_domain": "Computational modeling broadly; emphasized for biomedical models and digital twins",
            "validation_type": "hybrid",
            "validation_description": "Verification: software/spec conformance and numerical tests (e.g., SBML test suite, cross‑simulator runs); Validation: comparison of model outputs to experimental/observational data for intended uses; UQ: propagation and quantification of uncertainties in parameters/data to model outputs, often via Bayesian or statistical methods.",
            "simulation_fidelity": "VVUQ spans conformance checks and predictive validation — fidelity depends on the underlying model and availability of experimental data; UQ may use first‑principles or empirical uncertainty models.",
            "validation_sufficiency": "Paper presents VVUQ as foundational for credibility and cites regulatory and National Academies guidance endorsing VVUQ; in biomedical domains VVUQ (including experimental validation) is implied to be necessary for high‑stakes use (e.g., clinical applications, digital twins).",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "Paper discusses VVUQ conceptually; it notes UQ and validation are often emphasized by regulatory documents but does not present new experiments.",
            "validation_comparison": "Paper compares roles of verification vs validation vs UQ conceptually and recommends all three be automated where possible; no numeric comparisons provided.",
            "validation_failures": "Paper notes that lack of UQ and incomplete validation undermines credibility; gives general examples of ML models with spurious correlations (see chest x‑ray example) where validation was insufficient.",
            "validation_success_cases": "Regulatory and National Academies documents cited as endorsing VVUQ for credible models; BioSimulations and MEMOTE given as partial automation examples that support aspects of VVUQ.",
            "ground_truth_comparison": "Validation component explicitly involves comparison to experimental ground truth when available; paper stresses 'intended uses' and scope when defining validation targets.",
            "reproducibility_replication": "VVUQ is presented as necessary for reproducibility and for communicating confidence; automation of VVUQ is recommended to ease practitioner burden.",
            "validation_cost_time": "Paper notes burden of performing full VVUQ and recommends automation to reduce time and effort; no quantitative estimates provided.",
            "domain_validation_norms": "For biomedical modeling and medical devices, domain norms (and regulators like FDA) expect VVUQ practices including experimental validation and UQ.",
            "uncertainty_quantification": "Central to VVUQ; paper specifically emphasizes Bayesian techniques and quantification of how parameter/data uncertainty propagates to outputs.",
            "validation_limitations": "VVUQ can be effort intensive; limited by availability of high‑quality experimental data and by nonidentifiability; automation can help but cannot replace experimental data where required.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "VVUQ is inherently hybrid: computational verification and UQ combined with experimental validation when available; the paper recommends combining cross‑validation, competing models, experimental perturbation validation, and UQ.",
            "uuid": "e2120.4"
        },
        {
            "name_short": "CPMS ten rules / rubric",
            "name_full": "Credible Practice of Modeling & Simulation in Healthcare: Ten Rules (CPMS) and associated rubric",
            "brief_description": "A set of community rules for credible modeling practice in healthcare focusing on practices (verification, validation, documentation, stakeholder engagement) rather than a single model's properties; a rubric was developed to assess conformance.",
            "citation_title": "Credible practice of modeling and simulation in healthcare: ten rules from a multidisciplinary perspective.",
            "mention_or_use": "mention",
            "system_name": "CPMS ten rules and rubric",
            "system_description": "Guidelines and an assessment rubric emphasizing modeling practices (e.g., purpose statement, verification/validation, provenance, outreach) to improve model credibility in healthcare; rubric supports self‑assessment and community sharing.",
            "scientific_domain": "Healthcare computational modeling; biomedical simulations",
            "validation_type": "hybrid",
            "validation_description": "The rules emphasize verification and validation among other practices; the rubric evaluates to what extent recommended credibility practices (including validation against data and UQ) are met. Validation can include experimental comparisons and documentation of intended use, but CPMS focuses on process adherence.",
            "simulation_fidelity": "Not a simulation engine — fidelity depends on the modeling study; the rubric assesses whether appropriate fidelity and validation standards were followed.",
            "validation_sufficiency": "Paper notes CPMS rules put emphasis on verification/validation as central to credibility; sufficiency depends on study purpose and stakeholder needs — rubric helps characterize sufficiency.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "The CPMS guidance is procedural; it does not itself perform experiments but requires that appropriate validation be documented where applicable.",
            "validation_comparison": "Paper references the rubric as a standardized way to compare conformance to credibility practices across studies; no numerical comparisons in this paper.",
            "validation_failures": "No specific failure cases, but the paper notes that many modelers do not follow all recommendations due to burden, motivating automation.",
            "validation_success_cases": "Not enumerated here; CPMS guidance cited as influential community effort to improve credibility.",
            "ground_truth_comparison": "Rubric expects validation against appropriate ground truth for intended uses, but specifics are study‑dependent.",
            "reproducibility_replication": "Rubric and documentation practices aim to enhance reproducibility; authors recommend sharing self‑assessments as supplementary material.",
            "validation_cost_time": "Paper notes burden of fulfilling all rules and recommends automation to reduce time/effort.",
            "domain_validation_norms": "In healthcare modeling, domain norms include rigorous verification, validation, UQ, documentation, and stakeholder engagement — CPMS codifies these norms.",
            "uncertainty_quantification": "CPMS rules include UQ as part of credibility assessment; the rubric assesses whether UQ was performed.",
            "validation_limitations": "Adoption is limited by practitioner effort and resources; the rubric is a self‑assessment and may reflect authors' biases unless independently reviewed.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Encourages combining software verification, empirical validation, and UQ alongside good documentation and stakeholder processes.",
            "uuid": "e2120.5"
        },
        {
            "name_short": "FDA credibility guidance",
            "name_full": "Assessing the credibility of computational modeling and simulation in medical device submissions (FDA guidance)",
            "brief_description": "Regulatory guidance from the U.S. FDA that outlines expectations for establishing credibility of computational models used in medical device evaluations, emphasizing verification, validation, uncertainty quantification, and plausibility.",
            "citation_title": "Assessing the credibility of computational modeling and simulation in medical device submissions.",
            "mention_or_use": "mention",
            "system_name": "FDA credibility guidance for computational models",
            "system_description": "A regulatory framework describing how model credibility should be assessed for medical device submissions, prioritizing verification, validation, uncertainty quantification, and model plausibility (equations, assumptions, parameters).",
            "scientific_domain": "Regulatory biomedical modeling; medical devices",
            "validation_type": "hybrid",
            "validation_description": "Guidance requires VVUQ: verification of software/algorithms, validation against experimental/clinical data appropriate to intended use, uncertainty quantification, and documentation of model plausibility and limitations. It is regulatory (domain standard) rather than an automated system.",
            "simulation_fidelity": "Requires fidelity appropriate to intended use; for medical devices, high fidelity and documented validation are typically expected.",
            "validation_sufficiency": "Paper cites FDA guidance as authoritative for medical device modeling; for regulatory acceptance, the guidance sets a high bar — experimental validation and thorough UQ are commonly required.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "Guidance recommends experimental or clinical data where appropriate; the paper references the guidance but performs no experiments.",
            "validation_comparison": "Guidance contrasts different levels of evidence depending on model purpose; paper uses it to justify baseline requirements but provides no numeric comparisons.",
            "validation_failures": "Not detailed here; guidance exists because inadequate validation has undermined past credibility of certain models.",
            "validation_success_cases": "Not enumerated in this perspective; FDA guidance provides criteria under which a model might be accepted by regulators when validation criteria are met.",
            "ground_truth_comparison": "Guidance requires comparisons to relevant experimental/clinical data as applicable to intended use.",
            "reproducibility_replication": "Emphasizes reproducibility and documentation as part of credibility for regulatory submissions.",
            "validation_cost_time": "Not quantified here; implication that meeting regulatory standards can be resource intensive.",
            "domain_validation_norms": "For medical device submissions, adherence to FDA VVUQ expectations is normatively required.",
            "uncertainty_quantification": "Explicitly required and emphasized by the guidance as foundational to credibility.",
            "validation_limitations": "High burden of evidence for regulatory acceptance; many community models may not meet these standards without additional work.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Regulatory approach mandates a hybrid combination of computational verification, empirical validation, and UQ tailored to the model's intended use.",
            "uuid": "e2120.6"
        },
        {
            "name_short": "SBML/Model‑checking tools",
            "name_full": "Model‑checking tools and automated verification for SBML models",
            "brief_description": "Automated static and dynamic analysis tools (model‑checkers) that detect errors in model formulations and check properties like mass balance and state constraints.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Model‑checking tools for SBML",
            "system_description": "Tools that perform automated static analysis (e.g., detection of typographical or formulation errors) and dynamic checks (e.g., no negative concentrations) on explicit model representations (SBML/CellML) to assist verification.",
            "scientific_domain": "Computational systems biology; model verification",
            "validation_type": "simulated",
            "validation_description": "Verification via static analysis (semantic/syntactic checks, ontology/annotation checks) and dynamic checks (simulation invariants like non‑negativity); recommended to be included as automated validation tests bundled with model distributions.",
            "simulation_fidelity": "Analytical/conformance checks; fidelity is high for the kinds of errors they detect but does not validate real‑world predictive performance.",
            "validation_sufficiency": "Useful and recommended for verification, but insufficient alone for full model credibility which requires experimental validation and UQ when appropriate.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "No experimental work performed — these are software/static checks.",
            "validation_comparison": "Paper suggests providing validation tests in a standard format similar to software engineering unit/system tests; does not provide empirical comparisons.",
            "validation_failures": "Paper does not list specific failure cases but implies many reproducibility problems stem from models lacking such checks.",
            "validation_success_cases": "Suggested as best practice; specific tool names beyond SBML test suite and MEMOTE are not exhaustively listed in the paper.",
            "ground_truth_comparison": "Not applicable.",
            "reproducibility_replication": "Model‑checking tools increase reproducibility by making implementation errors visible and automating checks.",
            "validation_cost_time": "Not quantified; automation intended to reduce time and burden.",
            "domain_validation_norms": "Community expects use of such tools where possible for verification prior to publication or repository submission.",
            "uncertainty_quantification": "Not part of model‑checking tools; separate UQ workflows are recommended.",
            "validation_limitations": "Cannot substitute for empirical validation; limited to errors amenable to static/dynamic checks.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2120.7"
        },
        {
            "name_short": "Chest x‑ray COVID ML failure example",
            "name_full": "Example of ML model failure for COVID‑19 chest x‑ray diagnosis (spurious correlation with hospital coding)",
            "brief_description": "A cited cautionary example where an ML model achieved apparently accurate predictions for COVID‑19 from chest x‑rays but relied on hospital identifier artifacts rather than medical features, demonstrating failure of validation to detect spurious correlations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "COVID chest x‑ray ML model (failure example)",
            "system_description": "An ML model trained to predict COVID‑19 from chest x‑rays that appeared accurate but was later shown to use hospital coding (surrogate variable) correlated with case prevalence rather than pathological image features.",
            "scientific_domain": "Medical imaging; machine learning in medicine",
            "validation_type": "hybrid",
            "validation_description": "The paper references this as an illustrative failure of model credibility: model evaluation metrics indicated strong performance, but deeper examination (external validation/inspection) revealed the model used an unrelated confounder (hospital code). This highlights the need for careful validation including inspection for confounders, domain‑aware test sets, and provenance.",
            "simulation_fidelity": "Not applicable — this is a data‑driven ML validation failure rather than a simulation fidelity issue.",
            "validation_sufficiency": "Serves as an example that conventional training/test metrics can be insufficient for credibility; domain norms require investigation of feature relevance and external validation across sites to ensure generalization.",
            "validation_accuracy": "Paper does not provide numeric metrics for the failed model; it describes the phenomenon qualitatively.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No wet‑lab experiments — issue demonstrated by data analysis and model inspection; paper uses this to argue that accurate predictions do not guarantee credibility.",
            "validation_comparison": "Paper contrasts accurate predictive performance vs. lack of credible causal basis; implies that additional validation (e.g., multi‑site external validation, causal analyses) would be needed.",
            "validation_failures": "Yes — model relied on surrogate (hospital coding) producing misleading accuracy on internal test data; validation failed to reveal lack of causal basis.",
            "validation_success_cases": "Not applicable; example used as a cautionary failure.",
            "ground_truth_comparison": "Model predictions matched labels in the training/test data but failed when considering true underlying medical relevance and external contexts.",
            "reproducibility_replication": "Paper uses example to stress need for robust external validation and provenance; does not report independent replication.",
            "validation_cost_time": "Not discussed.",
            "domain_validation_norms": "In medical ML, domain norms include external multi‑site validation, analysis of possible confounders, and clinical plausibility checks in addition to performance metrics.",
            "uncertainty_quantification": "Not explicitly discussed in relation to this example in the paper; the example highlights that uncertainty metrics alone may not catch confounding.",
            "validation_limitations": "Demonstrates that conventional cross‑validation or holdout testing can be misled by confounders and that domain knowledge and provenance are essential.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Would require combining statistical holdout testing with external clinical validation and causal/feature‑relevance analyses; the paper recommends such broader validation to avoid similar failures.",
            "uuid": "e2120.8"
        },
        {
            "name_short": "BioSimulators registry",
            "name_full": "BioSimulators: a central registry of simulation engines and services",
            "brief_description": "A registry that catalogs simulation engines and services, facilitating selection of appropriate tools and supporting reproducibility and verification workflows.",
            "citation_title": "BioSimulators: a central registry of simulation engines and services for recommending specific tools.",
            "mention_or_use": "mention",
            "system_name": "BioSimulators registry",
            "system_description": "A community resource listing simulation engines and services, intended to help practitioners choose tools and enable automated verification/compatibility checks across simulators.",
            "scientific_domain": "Computational systems biology; simulation infrastructure",
            "validation_type": "simulated",
            "validation_description": "Supports validation by recommending simulators and enabling cross‑simulator comparisons; the registry itself is not a validator but facilitates V&V workflows by making compatible tools discoverable.",
            "simulation_fidelity": "Registry neutral; fidelity derived from the listed simulators and their capabilities.",
            "validation_sufficiency": "Registry aids verification workflows but does not replace the need for empirical validation or UQ.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "No experiments; enables computational tool selection and verification workflows.",
            "validation_comparison": "Enables comparison of simulators by making them discoverable; paper cites BioSimulators as a supporting infrastructure for verification.",
            "validation_failures": "Not discussed.",
            "validation_success_cases": "Cited as part of community infrastructure that supports reproducible and verifiable modeling.",
            "ground_truth_comparison": "Not applicable.",
            "reproducibility_replication": "Registry is intended to improve reproducibility by guiding choice of validated simulation engines.",
            "validation_cost_time": "Not discussed.",
            "domain_validation_norms": "Community expects use of registered, SBML‑capable simulators for standard model exchange and verification practices.",
            "uncertainty_quantification": "Not provided by registry.",
            "validation_limitations": "Does not itself validate models or simulators; users must still perform VVUQ.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2120.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Assessing the credibility of computational modeling and simulation in medical device submissions.",
            "rating": 2
        },
        {
            "paper_title": "Assessing the Reliability of Complex Models: Mathematical and Statistical Foundations of Verification, Validation, and Uncertainty Quantification.",
            "rating": 2
        },
        {
            "paper_title": "MEMOTE for standardized genome-scale metabolic model testing.",
            "rating": 2
        },
        {
            "paper_title": "BioSimulations: a central registry of simulation engines and services for recommending specific tools.",
            "rating": 2
        },
        {
            "paper_title": "Credible practice of modeling and simulation in healthcare: ten rules from a multidisciplinary perspective.",
            "rating": 2
        },
        {
            "paper_title": "SBML Test Suite.",
            "rating": 2
        },
        {
            "paper_title": "Concepts of model verification and validation (No. LA-14167).",
            "rating": 2
        },
        {
            "paper_title": "In silico trials: Verification, validation and uncertainty quantification of predictive models used in the regulatory evaluation of biomedical products.",
            "rating": 1
        }
    ],
    "cost": 0.02231275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From FAIR to CURE: Guidelines for Computational Models of Biological Systems
21 Feb 2025</p>
<p>Herbert M Sauro hsauro@uw.edu 
Department of Bioengineering
University of Washington
98195-5061SeattleWAUSA</p>
<p>eScience Institute
University of Washington
98195-5061SeattleWAUSA</p>
<p>Eran Agmon agmon@uchc.edu 
Center for Cell Analysis and Modeling
UConn Health
263 Farmington Avenue06030-6406Farmington, ConnecticutUSA</p>
<p>Michael L Blinov blinov@uchc.edu 
Center for Cell Analysis and Modeling
UConn Health
263 Farmington Avenue06030-6406Farmington, ConnecticutUSA</p>
<p>John H Gennari gennari@uw.edu 
Department of Biomedical Informatics &amp; Medical Education
University of Washington
1959 NE Pacific Street98195SeattleWashingtonUSA</p>
<p>Joe Hellerstein joseph.hellerstein@gmail.com 
eScience Institute
University of Washington
98195-5061SeattleWAUSA</p>
<p>Adel Heydarabadipour adelhp@uw.edu 
Peter Hunter p.hunter@auckland.ac.nz 
Auckland Bioengineering Institute
University of Auckland
1010AucklandNew Zealand</p>
<p>Bartholomew E Jardine 
Elebeoba May 
Wisconsin Institute for Discovery
University of Wisconsin-Madison
330 North Orchard Street53715MadisonWIUSA</p>
<p>David P Nickerson d.nickerson@auckland.ac.nz 
Auckland Bioengineering Institute
University of Auckland
1010AucklandNew Zealand</p>
<p>Lucian P Smith lpsmith@uw.edu 
Gary D Bader gary.bader@utoronto.ca 
The Donnelly Centre
University of Toronto
160 College StM5S 3E1TorontoOntarioCanada</p>
<p>Frank Bergmann frank.bergmann@bioquant.uni-heidelberg.de 
COS Heidelberg
Heidelberg University
Im Neuenheimer Feld 23069120HeidelbergGermany</p>
<p>Patrick M Boyle pmjboyle@uw.edu 
Center for Cardiovascular Biology
University of Washington
98195-5061SeattleWAUSA</p>
<p>eScience Institute
University of Washington
98195-5061SeattleWAUSA</p>
<p>Institute for Stem Cell and Regenerative Medicine
University of Washington
98195-5061SeattleWAUSA</p>
<p>Andreas Dräger 
German Center for Infection Research (DZIF)
partner site Tübingen
TübingenGermany</p>
<p>Quantitative Biology Center (QBiC)
Eberhard Karl University of Tübingen
Ottfried-Müller-Str. 3772076TübingenGermany</p>
<p>Data Analytics and Bioinformatics
Martin Luther University Halle-Wittenberg
Von-Seckendorff-Platz 106120</p>
<p>Halle (Saale)
Germany</p>
<p>James R Faeder faeder@pitt.edu 
Department of Computational and Systems Biology
University of Pittsburgh
3500 Fifth Avenue15213PittsburghPennsylvaniaUSA</p>
<p>Song Feng song.feng@pnnl.gov 
Biological Sciences Division
Pacific Northwest National Laboratory
902 Battelle Blvd99354RichlandWAUSA</p>
<p>Juliana Freire juliana.freire@nyu.edu 
Department of Computer Science and Center for Data Science
New York University
11201New YorkNY, New YorkUSA</p>
<p>Fabian Fröhlich fabian.frohlich@crick.ac.uk 
Dynamics of Living Systems Laboratory
The Francis Crick Institute
1 Midland RoadNW1 1ATLondonUK</p>
<p>James A Glazier jaglazier@gmail.com 
Intelligent Systems Engineering and Biocomplexity Institute
Indiana University
47408Street, BloomingtonIndianaUSA</p>
<p>Thomas E Gorochowski thomas.gorochowski@bristol.ac.uk 
School of Biological Sciences
University of Bristol
24 Tyndall AvenueBS8 1TQBristolUK</p>
<p>Tomas Helikar thelikar2@unl.edu 
Department of Biochemistry
University of Nebraska-Lincoln
Beadle Center
68588-0664LincolnNEUSA</p>
<p>Stefan Hoops shoops@virginia.edu 
Biocomplexity Institute
University of Virginia
Town Center Four
3rd Floor, 994 Research Park Boulevard22911CharlottesvilleVAUSA</p>
<p>Sarah M Keating s.keating@ucl.ac.uk 
Advanced Research Computing Centre
University College London
Philippstraße 13WC1E 6BTLondonUK</p>
<p>Matthias Konig koenigmx@hu-berlin.de 
Institute for Biology
Institute for Theoretical Biology
Humboldt-University Berlin
Philippstraße 1310115BerlinGermany</p>
<p>Reinhard Laubenbacher 
Department of Medicine
University of Florida
1600 SW Archer Rd32610-0225GainesvilleFloridaUSA</p>
<p>Leslie M Loew 
Center for Cell Analysis and Modeling
UConn Health
263 Farmington Avenue06030-6406Farmington, ConnecticutUSA</p>
<p>Carlos F Lopez clopez@altoslabs.com 
Multiscale Modeling Group
94065Altos Labs, Redwood CityCAUSA</p>
<p>William W Lytton 
Departments of Physiology &amp; Pharmacology
Downstate Health Science University
11203Neurology, BrooklynNYUSA</p>
<p>Department of Neurology
Kings County Hospital
11203BrooklynNYUSA</p>
<p>Andrew Mcculloch amcculloch@ucsd.edu 
Departments of Bioengineering and Medicine
University of California San Diego
9500 Gilman Drive, La Jolla92093-0412CAUSA</p>
<p>Pedro Mendes pmendes@uchc.edu 
Center for Cell Analysis and Modeling
UConn Health
263 Farmington Avenue06030-6406Farmington, ConnecticutUSA</p>
<p>Chris J Myers chris.myers@coloardo.edu 
Department of Electrical, Computer, and Energy Engineering
University of Colorado Boulder
425 UCB, 80309BoulderColoradoUSA</p>
<p>Jerry G Myers jerry.g.myers@nasa.gov 
NASA-John H. Glenn Research Center
21000 Brookpark Road110-3, 44135ClevelandMS, OhioUSA</p>
<p>Lealem Mulugeta lealem@insilico-labs.com 
InSilico Labs LLC
InSilico Labs LLC
77008HoustonTexasUSA</p>
<p>Medalist Performance
77027HoustonTexasUSA</p>
<p>Anna Niarakis anna.niaraki@univ-tlse3.fr 
Center of Integrative Biology
Molecular, Cellular and Developmental Biology Unit (MCD)
University of Toulouse III-Paul Sabatier
165 Rue Marianne Grunberg-Manago31400ToulouseFrance</p>
<p>Lifeware Group
Building Alan Turing, 1 Rue Honoré d'Estienne d'Orves91120Inria, PalaiseauFrance</p>
<p>David D Van Niekerk ddvniekerk@sun.ac.za 
Wisconsin Institute for Discovery
University of Wisconsin-Madison
330 North Orchard Street53715MadisonWIUSA</p>
<p>Brett G Olivier b.g.olivier@vu.nl 
Amsterdam Institute for Life and Environment
Vrije Universiteit Amsterdam
De Boelelaan 11081081 HZAmsterdamNetherlands</p>
<p>Alexander A Patrie apatrie@uchc.edu 
Center for Cell Analysis and Modeling
UConn Health
263 Farmington Avenue06030-6406Farmington, ConnecticutUSA</p>
<p>Ellen M Quardokus ellenmq@iu.edu 
Intelligent Systems Engineering and Biocomplexity Institute
Indiana University
47408Street, BloomingtonIndianaUSA</p>
<p>Nicole Radde nicole.radde@isa.uni-stuttgart.de 
Institute for Stochastics and Applications
University of Stuttgart
Pfaffenwaldring 970569StuttgartGermany</p>
<p>Johann M Rohwer 
Department of Biochemistry
University of Stellenbosch
Private Bag X17602MatielandSouth Africa</p>
<p>Sven Sahle sven.sahle@bioquant.uni-heidelberg.de 
BioQuant
Im Neuenheimer Feld 26769120HeidelbergGermany</p>
<p>James C Schaff schaff@uchc.edu 
Center for Cell Analysis and Modeling
UConn Health
263 Farmington Avenue06030-6406Farmington, ConnecticutUSA</p>
<p>T J Sego 
Department of Medicine
University of Florida
1600 SW Archer Rd32610-0225GainesvilleFloridaUSA</p>
<p>Janis Shin jshin1@uw.edu 
Jacky L Snoep 
Department of Biochemistry
University of Stellenbosch
Private Bag X17602MatielandSouth Africa</p>
<p>Rajanikanth Vadigepalli rajanikanth.vadigepalli@jefferson.edu 
Department of Pathology and Genomic Medicine
Thomas Jefferson University
1020 Locust St19107PhiladelphiaPennsylvaniaUSA</p>
<p>H Steve Wiley 
Biological Sciences Division
Pacific Northwest National Laboratory
902 Battelle Blvd99354RichlandWAUSA</p>
<p>Dagmar Waltemath dagmar.waltemath@uni-greifswald.de 
Medical Informatics Laboratory
University Medicine Greifswald
D-17489GreifswaldGermany</p>
<p>Ion Moraru moraru@uchc.edu 
Center for Cell Analysis and Modeling
UConn Health
263 Farmington Avenue06030-6406Farmington, ConnecticutUSA</p>
<p>Vrije Universiteit Amsterdam
Amsterdam Institute for Life and Environment
De Boelelaan 11081081 HZAmsterdamNetherlands</p>
<p>From FAIR to CURE: Guidelines for Computational Models of Biological Systems
21 Feb 202518ED264938B41EE4F048D1305FF436D9arXiv:2502.15597v1[q-bio.OT]
Guidelines for managing scientific data have been established under the FAIR principles requiring that data be Findable, Accessible, Interoperable, and Reusable.In many scientific disciplines, especially computational biology, both data and models are key to progress.For this reason, and recognizing that such models are a very special type of "data", we argue that computational models, especially mechanistic models prevalent in medicine, physiology and systems biology, deserve a complementary set of guidelines.We propose the CURE principles, emphasizing that models should be Credible, Understandable, Reproducible, and Extensible.We delve into each principle, discussing verification, validation, and uncertainty quantification for model credibility; the clarity of model descriptions and annotations for understandability; adherence to standards and open science practices for reproducibility; and the use of open standards and modular code for extensibility and reuse.We outline recommended and baseline requirements</p>
<p>for each aspect of CURE, aiming to enhance the impact and trustworthiness of computational models, particularly in biomedical applications where credibility is paramount.Our perspective underscores the need for a more disciplined approach to modeling, aligning with emerging trends such as Digital Twins and emphasizing the importance of data and modeling standards for interoperability and reuse.Finally, we emphasize that given the non-trivial effort required to implement the guidelines, the community moves to automate as many of the guidelines as possible.</p>
<p>Introduction</p>
<p>Wilkinson et al. in 2016 [1] made a good case (dare we say a fair case) for establishing guidelines for the management of scientific data.They arrived at four guiding principles enshrined in the acronym FAIR, namely that data be Findable, Accessible, Interoperable, and Reusable.With the rapid growth of computational modeling, especially the development of mechanistic models of physiological and cellular systems, the question arises of how these principles can be extended so that they can succinctly describe best practices for model management (e.g., model development, model selection, and model interpretation).In this perspective, we introduce a set of complementary guidelines to FAIR that address the specific needs for mechanistic models.We identify four principles: Credibility, Understandability, Reproducibility, and Extensibility.We refer to these as the CURE principles.</p>
<p>We focus on mechanistic models, thereby excluding the growing body of machinelearned (ML) and AI models that are based solely on data.The machine learning community has appropriately encouraged the use of FAIR principles when publishing ML models [2], with an emphasis on ensuring that data are accessible to those who wish to repeat the study.However, the reproducibility of those models is a separate topic that has its own special concerns (e.g., the selection of training, validation, and test data, as well as the choice of hyperparameters).We note in passing that although ML models rarely consider mechanisms, there are situations in which mechanistic models make use of machine learning approaches, such as in the context of parameter estimation or physically informed neural networks (PINNs) [3].</p>
<p>Although the focus of this proposal is on models from the systems biology community, the guidelines and sentiments we describe are broadly applicable to other modeling domains.</p>
<p>Why have Guidelines?</p>
<p>Models are indispensable in many science and engineering disciplines.Examples include circuit simulation in electrical engineering, models of fluid flow in mechanical engineering, and weather modeling in atmospheric science.In some cases, modeling has progressed to the development of digital twins [4,5], in which a simulation model is designed to replicate and interact with the physical system, or virtual populations, which can be used for applications such as clinical trial design [6].Biological modeling has not yet risen to that same level of sophistication, with the possible exception of areas such as protein folding [7] and molecular dynamics [8] where physics and chemistry play a more important role.Even so, biological modeling is a rapidly evolving field.As the field grows, guidelines in the spirit of FAIR will help modelers create more impactful and credible models.We believe these guidelines will be of importance for models that ultimately reach the clinic and particularly for the growing interest in developing biomedical Digital Twins.The Working Group 'Building Immune Digital Twins' tries to address these questions, having as a focus the human immune system and its responses in various pathological contexts [9,10].</p>
<p>Existing Guidelines</p>
<p>Several groups have proposed guidelines to improve best practices in creating biological models over the last 20 years.Of particular note is the creation of standardised languages for biological models such as Systems Biology Markup Language (SBML) [11], CellML [12] and NeuroML [13].These are machine-readable formats that are an explicit representation of the model.By explicit representation, we mean that the model representation only includes elements that are essential for modeling; it does not include implementation details related to simulation.For example, the essential characteristics of a mechanistic model of a well-mixed biochemical system include chemical species, reactions, and rate laws.It does not include software details such as file input/output and control logic for numerical solvers.An explicit representation is independent of its implementation in software.</p>
<p>The choice of an explicit representation for models is driven by the requirements of the communities that develop and use the models.SBML focuses on biochemical models where the representation is in terms of the biological processes.CellML focuses on a mathematical representation of models as differential-algebraic equations.NeuroML focuses on representations of neural systems.These representations have become popular among modelers and software developers.For example, all genomescale models [14] are represented using SBML, and thousands of kinetic models are now stored on publicly accessible model repositories using these formats [15,16].Standards such as SBML avoid the use of potentially confusing and unreusable ad hoc models, allowing models to persist in a reproducible form over long periods of time [17,18].Many authors, however, still publish models in executable formats such as MATLAB, Python, etc., which can pose problems for reproducibility and reuse, particularly when poorly documented [19,20].The logical modeling community, which uses SBML Qualitative format (SBML-Qual) to encode logical and Boolean models, made efforts to define a roadmap toward the annotation and curation of logical models (aka the CALM initiative), including milestones for best practices and recommended standard requirements [21].</p>
<p>To promote data sharing and reuse, the FAIR principles recommend a data dictionary that specifies data types and semantics for data items [1].An analogous requirement exists for models.For example, consider an SBML model with the reaction A → B. Annotations are used to define A, B, and provide information about the reaction (e.g., the organism, cell type, and organelle in which the reaction takes place).Annotation can provide additional ontological and reference information about a model, including its submodels, processes, assumptions, and provenance.Genome-scale models are heavily annotated with process metadata [22,23], and the curators at BioModels [24] regularly add annotations to curated models.As part of these efforts, the systems biology and physiology community developed the MIRIAM standard, which describes the "Minimum Information Required In the Annotation of Models" [25].MIRIAM applies to structured information, such as SBML or CellML, where annotation information can be included in a machine-readable manner.Such information can be of great utility for searching, merging, or disassembling a model into its component parts [26].</p>
<p>Mechanistic Models</p>
<p>The focus of this perspective is on mechanistic models.We define a mechanistic model as: a representation of a biological system that is described in terms of the constituent physical parts and processes that occur between the parts.For example, in a mechanistic model of a cell signaling pathway, we would specify the various proteins and their phosphorylation states and the transformations between these states via enzymatic processes involving kinases and phosphatases.</p>
<p>Often, a mechanistic model is transformed into a computational model by invoking physical laws, such as the conservation of mass and chemical kinetic laws that govern individual transformations.In physiology and systems biology, models are commonly represented as a system of ordinary differential equations [27,28], but other representations are also used, such as systems of Boolean functions, graphs, stochastic systems, and constraint-based models [29].Such models are often shared via model repositories such as BioModels [24], JWS Online [30], or BiGG [15] and KBase [16] for constraint-based models.Models can also be shared using raw executable formats such as Python or MATLAB that describe differential equations.Such 'raw' model code is typically provided as supplementary files to a published paper or stored in code repositories such as GitHub or specialized repositories such as ModelDB [31].In some cases, the model may be described as part of the main text of the published article [32] or be absent altogether.This obviously makes the reproducibility of such work much more complex and sometimes impossible, given the frequency of typological errors in printing mathematical equations or code fragments.Models-as-programs paradigms such as those followed by PySB and others encourage the use of best practices in Python coding and documentation through tools such as Sphinx, but these approaches rely on developer effort to document the code and model at an appropriate level of detail [33,34].They also tie a model to a specific language implementation, making reuse difficult and backward compatibility a problem.</p>
<p>It may be difficult to abstract the underlying physiological processes of some complex systems purely in explicit form, though there have been few efforts to attempt this.</p>
<p>Multi-scale modeling tends to give rise to such challenging scenarios.The context of computational modeling of electrophysiological phenomena in the heart provides an illustrative example [35].Ordinary differential equations describing cell-scale events like ion channel gating and the generation of action potentials can be encoded using CellML or as a biological description in SBML.However, representing the propagation of excitatory wavefronts requires spatial discretization of the governing partial differential equations via finite element analysis; explicit formats for such applications exist [36,37] but have not seen widespread use.One approach to improving interoperability and reproducibility is to create tools for importing model components written in common data formats.For instance, the openCARP simulation environment [35] includes a CellML "translator" that facilitates on-the-fly incorporation of cell-scale representations of different types of cardiac electrophyiology (e.g., cardiac region, species, or disease-specific action potentials) within the fabric of the multi-scale simulation ecosystem.</p>
<p>Mechanistic models of biological systems often contain many parameters whose values are unknown (e.g., kinetic constants and diffusion rates).A popular approach to estimating these parameters is model fitting or calibration [28,[38][39][40].This is where model parameters are adjusted, often with an optimization algorithm, so that the output of the model matches relevant experimental data.Because models tend to have many parameters, the problem is often, if not always, underdetermined.This means that the set of fitted parameters in a model is not unique.In some instances, one or more parameters may be non-identifiable, meaning no single value can be assigned to the parameter [41].Solutions to this problem can involve simplifying the model to reduce the number of parameters and/or eliminate non-identifiable parameters; another approach is to collect additional experimental data.More recent methods have focused on Bayesian techniques to assess the uncertainty in parameter estimates [42,43].</p>
<p>As a side note, we contrast mechanistic models with the great variety of neural network models that can provide accurate predictions for complex problems [44].However, the resulting "black box" models are almost always challenging to understand, a situation referred to as "intellectual debt" [45].An example is the prediction of COVID-19 from chest x-rays [46].A close examination of the model revealed that it ignored the medical features in the x-rays and instead relied on the coding of the hospital at which the imaging was done.This turned out to be a surrogate variable for the patient population since one hospital had more COVID patients than the other.We cite this as an example of a model with accurate predictions that was not credible.</p>
<p>The CURE Guidelines</p>
<p>As with FAIR, we define four specific guidelines to improve best practices in developing mechanistic computational models of biological systems.There is no specific order in the guidelines but the acronym CURE seemed appropriate for the topic in question.</p>
<p>Credible</p>
<p>Understandable</p>
<p>Reproducible Extensible</p>
<p>CURE 1 covers four key ideas, Credibility, Understandability, Reproducibility, and Extensibility, which we describe in the following sections.These guidelines are meant, where possible, to apply to both machine-readable formats, such as SBML, as well as models distributed in executable code such as MATLAB or Python.They also align with previous community efforts to address barriers in comprehensiveness, accessibility, reusability, interoperability, and reproducibility of computational models in systems biology [47].</p>
<p>Credible</p>
<p>We use credibility to mean a perceived measure of believability [48,49].A credible model makes trustworthy and actionable predictions within the range of conditions it was intended to simulate.Prior work on model credibility dates back to 1979 [50], when the Society for Modeling &amp; Simulation International (SCS) described many concepts associated with model credibility.More recently, several groups within the biological modeling community have discussed model credibility, most notably the ten rules developed by CPMS [51], devised by the Committee on Credible Practice of Modeling &amp; Simulation in Healthcare (CMPS) in the US, and in Europe Musuamba et al [52] who describe in some detail criteria and concepts that are important to assess the model credibility.Of note, the CPMS working group considered credibility as a descriptor of the practice of modeling and simulation rather than that of a model.Accordingly, the assessment by the ten CPMS rules are geared towards evaluating the practices followed in the modeling efforts [53].Two core concepts related to model credibility are verification and validation [54].</p>
<p>Verification is "the process of determining that a model implementation accurately represents the developer's conceptual description of the model and the solution to 1 Another organization devoted to the promotion of curation practices of research compendia also uses the acronym CURE:"Curating for Reproducibility".However, there is little overlap between the two usages.</p>
<p>the model" [54].Similar definitions can be found in many other documents [55].In practice, verification means assessing the correctness of: (i) the model representation (e.g., detecting typographical errors); (ii) the numerical algorithms used to simulate the model (e.g., pseudo random numbers have the correct distributions); and (iii) the model implementation in software.Models implemented directly in programming languages such as Python may not have (i), and models implemented using standards such as SBML have few concerns about (iii).</p>
<p>When using standards such as SBML, verification involves ensuring that different software applications interpret the description of the model in the same way [56] and that the software has passed the SBML test suite [57].It is worth noting the close correspondence of verification to software testing, including unit and system tests, and documentation [58].Since models are almost always implemented in software, this should come as no surprise.Static tests can assess whether the model is correct, for example, that the biophysical laws have been entered correctly or that mass balance is not violated.Dynamic tests can offer more subtle checks to the correctness of a model.For example, a model whose variables are concentrations should not be able to reach negative values.</p>
<p>Validation is "the process of determining the degree to which a model is an accurate representation of the real world from the perspective of the intended uses of the model" [54].Of significant importance is the phrase "intended uses".All models have a limited scope (including AI/ML models).That is, they can only make useful predictions within their intended purpose and calibration.This is particularly important for models that might be used in a clinical setting where misuse of a model could have dire consequences.It is important, therefore, that there be a clear statement of the purpose of the model as well as the conditions under which the model is applicable.Validation involves comparing experimental data to predictions made by the model.In practice, given the nature of scientific models, not all model predictions can be validated.Validation is more a measure of confidence in a model's credibility to match reality than an absolute statement of truth.</p>
<p>As noted previously, mechanistic models often have parameters whose values are determined through parameter fitting The credibility of the parameter estimates can impact the credibility of the model and can be enhanced in two ways, either by cross-validation or through the use of competing models.During cross-validation, the experimental dataset is split into a training and a test set.The test set is used to check whether the calibrated model can recapitulate the test set.If the model can recover the test set then there is greater confidence in the model.If the model fails to recover the test set, then the model is inadequate and needs to be reexamined.Likewise, in the context of multi-scale simulations, model credibility can be assessed by experimentally documenting the system-wide response to at least two perturbations (e.g., stimulating the heart from distinct sites [59]).Then, a model parameterized using exclusively data based on the first set of measurements can be convincingly validated by demonstrating its ability to reproduce the second set (despite the model never having seen those data during calibration).</p>
<p>For parameter-free logic-based models in biology, the credibility lies in the causality of the statements used to build the logical rules and functions [60], as well as the binarized interpretation and use of small-and larger-scale experimental data [61].This leads to the question of model selection [62].That is, given the limited amount of experimental data, a given model will not necessarily be unique, and other models could equally likely to explain the data.The literature on model selection is extensive, but certain popular tests have emerged, most notably the Akaike information criterion (AIC) [63] and the Bayes Information Criterion (BIC) [64].The AIC is an approximation, based on maximum likelihood, to the Kullback-Leibler divergence [65], which quantifies the difference between the model and full reality.The AIC test considers both the quality of a fit and the number of parameters in the model.For example, given two models that fit the data equally well within experimental error, the model with fewer parameters is the preferred.Any number of plausible models can be compared in this way since the computation is relatively straightforward.Kirk et al.Kirk et al. provide an excellent review of this topic [62].Model credibility can be enhanced if, given the uncertainty in the underlying biology, a number of models are proposed, with a measure such as AIC, used to indicate their relative credibility.The BIC is based on the relationship to the Bayes Factor, itself the ratio of the likelihood of two hypotheses and used to compare the marginal likelihoods between two models.</p>
<p>Uncertainty Quantification.In recent years, there has been a growing interest in the biomedical community to use uncertainty quantification (UQ) as part of a credibility assessment [66].UQ has been well established in other scientific domains for many decades [67] and involves calculating how uncertainty in the experimental data and model parameters contributes to uncertainty in the model outputs.A model that generates highly uncertain outputs is seen as less credible.Verification, validation, and UQ have been collectively referred to by many practitioners using the acronym VVUQ [55].A recent article by Colebank et al., reviews elements of this topic ??.</p>
<p>Another criterion that can enhance a model's credibility is the provenance of the data used to build the model.Where did the data come from, and was it modified in some way?Table 1 gives a summary of some of the main criteria that can be used to access credibility.However, not all the criteria are equally weighted.Validation and verification are often the most important in this regard.The first four rules in the 10 rules devised by the Committee on Credible Practice of Modeling &amp; Simulation emphasize all these aspects [51].</p>
<p>In many cases, modelers may opt not to consider some or all of these criteria in their work, primarily due to the burden of having to do the checks.We, therefore, recommend automating as many of these criteria as possible.For example, verification of SBML-based models can be achieved using the BioSimulation resource [68].Validation tests could be provided in a standard format as part of the software modeling code just as software engineers often provide validation tests as part of their distributions [69].More difficult is including information about data provenance and model assumptions.However, the use of model standards such as SBML or CellML do allow models to be annotated with this information.The same applies to indicating the scope and purpose of a model.When models have an explicit representation (e.g., SBML) as</p>
<p>Attribute</p>
<p>Criteria</p>
<p>Validation</p>
<p>How well do the model outputs match reality?Verification</p>
<p>Has the model been constructed without error; are the simulation algorithms correctly encoded and operating without error?Uncertainty</p>
<p>Have the effects of uncertainty in the model outputs been reported?Provenance</p>
<p>Can the data that was used to calibrate or validate the model be traced to its original source?Annotation</p>
<p>Have the inputs and output of the model been well defined?Assumptions Have the assumptions used in the model been made explicit?Purpose</p>
<p>Has the purpose of the model been adequately described?Scope</p>
<p>Has the scope, that is, the space within which the model can be used, been specified?Unbiased</p>
<p>Was the model calibrated on unbiased data?This depends on the scope of the model, but if a model is to be used across a diverse population, then clearly, the calibration data needs to be diverse.</p>
<p>Table 1: A range of criteria that can establish the credibility of a model.</p>
<p>opposed to just a software implementation, analysis tools can do deep dives into the model to examine its biophysical assumptions.Also, more in-depth verification can be done on the explicit model representation, such as detecting errors in the formulation of biophysical laws [70].MEMOTE [71] is a successful example of automated software that can do deep dives into genome-scale models to assess the quality of the model.</p>
<p>Credibility is widely used in software development through the commenting of code, using version control for provenance, and unit and system testing for validation [58].</p>
<p>Verification is achieved through in-depth checking of the software compilers and runtime systems.</p>
<p>Understandable</p>
<p>One of the aims of the scientific method is to gain an understanding of how the world works by proposing models and theories to be tested.We understand theories (to paraphrase [72]) to be bodies of knowledge that are broad in scope.Chemical reaction theory or the central dogma in biology are examples of broad scope.In contrast, models are instantiations of theories and, as a result, are narrower in scope and often represent a particular biological process of interest.This includes computational models of metabolism, protein signaling networks, etc.Both models and theories are the most important outcome of the scientific method.They provide a way to rationalize a set of observations and make predictions about the future state of the system.However, the act of "understanding" [73] a model or theory is not an easy concept to define and may encompass a number of different aspects.In particular, how might one quantify "understanding"?Philosophically, de Regt and Dieks [73] define understanding as a given phenomenon if there exists a theory that describes the phenomenon.However, such definitions are hard to quantify.Instead, we will focus on measures that can be used to provide some level of confidence in how understandable a model is.</p>
<p>We note in passing that understanding modern AI [74,75] and machine learning algorithms can be very challenging, if not impossible to understand.It may not even be the goal of such approaches where the underlying physical reality is entirely superfluous to the objective.As AI approaches become increasingly accurate at making predictions [76], the necessity of understanding how systems work may become entirely unnecessary.There are a number of counterarguments to this view.One is that human beings have an innate desire to understand the world, which is difficult to suppress.In addition, understanding a model can lead to its improvement, which is often transferable knowledge that can be applied to other problems.In a clinical situation, there is no guarantee that training has been sufficient to provide reliable diagnosis and treatment.If the system makes a mistake, we cannot easily determine why.In software engineering, writing understandable code is crucial to minimize maintenance costs, reduce bugs, and make the software more reliable and extensible [77][78][79].The obvious counter argument is that the underlying mechanism of many clinical treatments are either poorly understood nor not understood at all and yet they are useful and benefit a great many patients.</p>
<p>Biological systems, even small ones are challenging to understand due to the nonlinear interactions among the components.The problem gets even more acute as the systems we study get larger.Biological systems often interact in complicated and nonlinear ways that result in emergent behaviors [80].For example, there is no amount of understanding of the components of DNA -purines and pyrimidines, or at a lower-scale carbon and hydrogen atoms -that would lead us to predict its complex structure or understand its biological role; the role emerges from the way that the components are put together.</p>
<p>How can we measure understanding?One way is to divide a model's components and attributes into levels of perceived importance.For example, we could understand different aspects of a model such as knowing a model's purpose, its components, the biophysical laws that describe how the components interact, its inputs, outputs, assumptions, and limitations.Figure 1, depicts a pyramid that organizes such a hierarchy of 'understanding'.At the base of the pyramid is the most rudimentary understanding; successive layers indicate increased levels of understanding.At the most rudimentary level (1), we want to know the purpose and objectives of the model as well as its inputs and outputs.Subsequent levels include the system components being modeled (2); the interactions between components that are modeled (3); a mathematical description of these interactions (4); a way to evaluate the mathematical model (e.g., solve a system of differential equations) (5); and finally, if possible, a general theory that explains the behavior of the model (6).</p>
<p>Technology is already available to assist in identifying model components (Level 2) and interactions (Level 3), as well as adding metadata to a model (Level 1).Such information can be provided through model annotations [81].The mathematical model (Level 4) also includes the model's assumptions, which can be added as annotations using the SBO ontology [82].For genome-scale metabolic models, the scientific community developed a list of detailed recommendations to annotate such models at Levels 1 to 4 following an extensive debate at their 2018 annual conference on constraintbased reconstruction and analysis (COBRA) [83].Scientific models do not have to be mathematical, but for our purposes, most models are, and once a model is defined mathematically, it is often possible to convert the model into an executable form so that simulations can be carried out to generate predictions.Simulation information can be annotated using ontologies such as KiSAO [84].The pinnacle of understanding is Level 6, which is the formulation of a theory that explains the behavior seen in the model.</p>
<p>Level 6 is the most difficult to quantify and involves explaining in terms of both fundamental and higher-order concepts how a given behavior emerges.Making this more difficult is that many biological systems show emergent behavior [80], where we observe behavior that is not found in any individual component of the system.Even simple systems can reach this threshold.For example, a system of only two components and some non-linearity can show sustained energy dependent oscillations [28,85].Understanding the individual components is not sufficient to explain the origin of the oscillations [86] but requires a theory to help understand how, as a system, we observe oscillations.In this particular case, we need additional concepts, such as hysteresis and negative feedback, to understand the origin of the oscillations.For very complex systems, there may be multiple levels of abstraction that describe many levels of emergent behavior.Such abstractions are commonly used in electrical engineering and computer science.This is what makes it possible to engineer these highly complex systems.Reverse engineering abstraction layers in biology are, however, difficult [87,88] due to the fact we are dealing with evolved systems that are not always as well structured as our own engineered systems.Unfortunately, no ontology exists to adequately annotate how a system operates.TEDDY is closest to an ontology [82], which can be used to describe dynamics but does not, as yet, have the capacity to describe a theory of operation.</p>
<p>Reproducible</p>
<p>Reproducibility is a cornerstone of the scientific method, and over the last 10-15 years, much discussion has been devoted to this topic [89,90]; mostly about the lack of reproducibility of many scientific results.Often, we think of wet lab experiments in biology as being difficult to reproducible because they are multifaceted and inherently variable.However, it has also been discovered that computational experiments are often not reproducible [19].This is surprising given that computation involves welldefined and often deterministic procedures.We will not revisit the many issues and recommendations concerning the reproducibility of computational experiments [17,18,49,91,92] but one thing that has become clear is that community standards such as SBML have significantly improved the reproducibility of computational models by providing a machine-readable representation in a standard format [19,93].Moreover, recent evidence supports the opinion that reproducible models are more cited and more likely to be reused in subsequent studies [94].Even if reproducibility is not a high priority for its own sake, verification, validation, and reuse of models require a model to be reproducible [95].In recent years we have seen the emergence of software tools and standards that offer support to assist modelers in creating reproducible models [70,[96][97][98].One could even go as far as to say that in systems biology at least, the problem of reproducibility is solved [91].</p>
<p>Extensible and reusable</p>
<p>Science builds on past efforts, standing on the proverbial shoulders of giants.This is no different when building computational models where past computational models can be enhanced, reused, and further validation applied.Unfortunately, many published models cannot be easily reused [51].This is because many models were published without an explicit representation of the model.Instead, the model is embedded in a software implementation such as MATLAB or Python.The software implementation adds many complexities (e.g., file interfaces and control logic for computing a solution), often in the form of multiple files with minimal documentation.Moreover, models deployed in this way are in a mathematical representation that loses considerable biological information.For example, when a biochemical pathway model is reduced to a set of differential equations the stoichiometric structure of the network is either lost or is difficult to reverse engineer.A further concern is that the mathematical representation greatly complicates the ability to query models to discover similar pathways, kinetics, and other characteristics.These considerations are some of the reasons why genome-scale models are published using SBML [11,71] so as to preserve as much biological information as possible.</p>
<p>Hence, the primary concern with publishing a model solely as its software implementation is that it is difficult to reuse the model, either in part or in whole.For example, combining a model written in MATLAB with a model written in Python can be a costly exercise.Likewise, converting models from one format to another, for example, MATLAB to SBML, can also be error-prone and costly [20].Standards such as SBML allow the automated deconstruction [26] and reuse of models [99] through the use of model annotations.Models expressed in SBML are much easier to reuse or extend.</p>
<p>When converted to a human-readable language like Antimony [100], reuse becomes even easier.</p>
<p>Other disciplines employ formats such as Modelica [101] or representations such as Simulink [102] to assist in reuse, but such techniques have not been widely used in developing biological models.</p>
<p>If executable code is used to represent models, then the model should ideally be partitioned into reusable program functions with ample documentation to illustrate how to reuse the model and what the various symbols in the mathematical equations represent.</p>
<p>Recommended Requirements</p>
<p>The previous discussion provides a wide range of criteria that can be used to satisfy the CURE guidelines, and fulfilling all the requirements would be quite onerous.For most academic studies, it might be sufficient to meet a small number of criteria.For models that might be used in a clinical setting, it would be prudent to satisfy as many criteria as possible.Organizations such as the FDA (Food and Drug Administration) have already begun to issue guidelines for models used in medical devices [103], and there is no reason to doubt that such guidance will eventually be offered for more general use of models in clinical settings.</p>
<p>For academic research, we therefore wish to propose a recommended set of requirements from each aspect of CURE that would be sufficient to significantly impact computational modeling.We provide a checklist in Figure 2, which also highlights the baseline requirements.</p>
<p>A key requirement is the need to develop standardized approaches to assess and communicate the extent to which a given model, or a modeling study, satisfies the recommended or baseline requirements.An example to consider is the approach taken by the CPMS working group to develop a rubric that considers the extent of outreach to various stakeholders in satisfying various credible practice guidelines [53].Such evaluations, typically conducted as self-assessments by the respective study authors, can be shared with the community as part of supplementary material in published studies [104,105].Automation could greatly facilitate this assessment process, especially if provided in advance and used during model development.</p>
<p>Baseline Requirements</p>
<p>If the recommended requirements are still too onerous, it is possible to define a baseline requirement.This term refers to the essential or foundational standards that are necessary for basic credibility, though they may not meet the full recommended requirements specified by CURE.The baseline requirement is similar in intent to the recent report [106] from the US National Academies, which emphasizes purpose, verification, validation, uncertainty quantification, and reproducibility, though their statement on reproducibility is vague.We include scope and model limitations into our baseline, which the National Academies documents do not explicitly mention, though it could be considered part of the statement of purpose.The baseline requirements are highlighted in Figure 2, and a more detailed summary is given in Table 2. Fig. 2: Recommended or baseline requirements for CURE that could be used for research-based models.The baseline requirements are highlighted in shaded pink boxes.A score of one out of ten can be given based on the number of criteria met.For example, the baseline requirement will yield a score of 6/10.Note that if publishing models via source code, it is essential to specify the version number of the software platform as well as version numbers of any dependency packages that were used.The use of containerization platforms such as docker can sometimes help in these situations.</p>
<p>Credibility</p>
<p>Conclusion</p>
<p>This paper introduces a set of guidelines for developing robust, credible biological models.These guidelines are meant to complement the FAIR guidelines for data.As with FAIR, we propose a four-word moniker, CURE, that defines four core attributes of good modeling practice.These include credibility, understandability, reproducibility, and extensibility.</p>
<p>The need for CURE models is highlighted by recent interest in Biomedical Digital Twins, a technology that will rely on having robust models of biomedical systems [5,106,110], but it is clear that such guidelines would also benefit the wider biological modeling communities.</p>
<p>For credibility, we recommend the use of verification, validation, and uncertainty quantification; for understandability, we discuss the clarity of model descriptions and the importance of annotations; for reproducibility, our focus is standards and open science practices; and for extensibility, we emphasize open standards and the use of modular Credibility (Baseline requirement):</p>
<ol>
<li>
<p>Clearly define the objectives and scope of the modeling study, including the biological question being addressed and the specific hypotheses to be tested.</p>
</li>
<li>
<p>Use consistent notation and terminology to ensure consistency and clarity in model descriptions.Where possible, follow common notations used in the community.</p>
</li>
</ol>
<p>3.Where possible, verify the model by checking the model with other simulators.Use model-checking tools to identify errors in the model [58].</p>
<ol>
<li>Validate the model against experimental data using accepted statistical procedures, such as crossvalidation, to assess model accuracy and predictive power.</li>
</ol>
<p>5.Where possible, assess how sensitive the model is to uncertainty (UQ) in parameter estimates, model structure, inputs, and assumptions.</p>
<ol>
<li>Clearly document the limitations of the model, including areas where assumptions may not hold or where uncertainties exist, to provide context for interpreting results and guiding future research.</li>
</ol>
<p>Understandability:</p>
<ol>
<li>Provide a representation of the model that is both machine-readable and human-readable.Ideally, this should be in an open community standard and be an explicit representation of the model that is not intertwined with control logic, file input/output, and other implementation details. 2. Provide comprehensive documentation that explains the model structure, equations, and parameter values, e.g., by following the suggestions by Carey et al. [83].When using open community standards such as SBML or CellML, submit the models to a recognized model repository.If the model is expressed in a programming language, deposit your model code at repositories such as GitHub, BioModels [24], or ModelDB [31].</li>
</ol>
<p>3.Where possible, annotate the model to clarify any ambiguous terminology.When using a programming language to express a model, use commenting to annotate the model.</p>
<p>4.</p>
<p>Try to document all assumptions made during model development, including simplifications, approximations, and parameter values, to provide transparency and facilitate reproducibility.5. Try to provide clear graphical illustration of the model.If the model is a biochemical network then use machine-readable formats such as SBGN [107], preferably using a community modeling standard such as SBML Layout [108] and Render [109].</p>
<p>Reproducibility (Baseline requirement):</p>
<ol>
<li>Follow established standards and guidelines for model development, such as the Systems Biology Markup Language (SBML) and Minimum Information Requested in the Annotation of Models (MIRIAM), to enhance interoperability and facilitate model sharing and exchange.If using executable code, make sure best practices for code development are used.</li>
</ol>
<p>2.</p>
<p>Embrace and promote open science practices by openly sharing publicly funded models, data, and code with the scientific community, promoting transparency, reproducibility, and collaboration.</p>
<p>Extensibility and Reuse:</p>
<ol>
<li>
<p>Use open modeling standards where possible.If using executable code such as Python, separate the model code from the runtime code.In this form, the model code, in principle, can be reused with minimal effort by other Python users.However, this would require careful commenting on the components of the model.One approach is to provide the model as a software function that can be called by other code.Try to use open-source licensing so that there are no restrictions on the reuse of the research.</p>
</li>
<li>
<p>If a model is represented using a modeling format such as SBML, reuse should be much easier since the model is expressed in biological terms.If the model is annotated then automated systems can be devised to automate the merging and disassembly of models into individual parts of portions for reuse.</p>
</li>
</ol>
<p>Table 2: Summary of Recommended and Baseline Requirements.models.We outline recommended requirements for each guideline and propose a baseline level below the recommended requirements that is largely in alignment with the National Academies report [106].While all of the CURE principles are important, we wish to highlight credibility, the first principle.Credibility is the degree to which a model can be trusted when applied to a given problem.In a biomedical application where concern is with patients and their well-being, the trustworthiness of a predictive model is paramount.Interestingly, the FDA recently published [103] a guidance document on assessing the credibility of computation models but applied only to medical devices.Many of their recommendations relate to model verification and validation but also contained aspects related to model plausibility, which considers the plausibility of the governing equations, assumptions, and model parameters.The FDA document also emphasizes UQ for estimating uncertainty in the model outputs.These are considered foundational for credible modeling.However, these have not yet infiltrated the biomedical modeling community significantly.The recent National Academies [106] report on Digital Twins emphasizes the same points.The report also stresses the critical need for data and modeling standards to enable interoperability and reuse.As an initial effort to support VVUQ, the BioSimulations resource [68] provides a verification service where a given model can be run against multiple independent simulators to help verify the simulation engines.</p>
<p>Finally, automation is key to making the CURE guidelines workable and practical to reduce the burden on practitioners and accelerate widespread adoption.</p>
<p>Fig. 1 :
1
Fig. 1: Quantifying understanding through a hierarchy of criteria.</p>
<p>This could be an open standard such as SBML, CellML, or program code such as Python.2.For example the Docker platform, although it is not known how well such technologies age over time.
Recommended Requirements for CUREUnderstandabilityReproducibilityExtensibilityDefine objectives and purposeProvide the model code 1Use community standardsUse community modeling standardsVerify the modelAnnotate the modelOrOrValidate the modelDocument model assumptionsUse containerization 2Use modular sourceUse consistent notation 1. Or Include uncertainty quantification Document model limitationsUse best practices when writing executable code, minimize dependenciescode so that model components can be reused. Documentation is crucial in this situation.
Acknowledgments.This work was supported by NIH Biomedical Imaging and Bioengineering award P41 EB023912 through HMS at the Center for Reproducible Biomedical Modeling (https://reproduciblebiomodels.org/).The content expressed here is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health, or the University of Washington.HMS wishes to thank Eric Johnson Chavarria for suggesting the CURE acronym at the 2023 IMAG meeting in Bethesda, MD.HMS also wishes to thank Hunter Robbins for assistance in collating the author names and addresses.T.E.G. was supported by a Royal Society University Research Fellowship (URF\R\221008) and the UKRI-BBSRC Engineering Biology Mission Award CYBER (BB/Y007638/1).S.F. is supported by thePredictive Phenomics Initiative under the Laboratory Directed Research and Development Program at Pacific Northwest National Laboratory, operated by Battelle for the U.S. Department of Energy under Contract No. DE-AC05-76RL01830.J.F. was supported by DARPA through the Automating Scientific Knowledge Extraction and Modeling (ASKEM) program, Agreement No. HR0011262087; NSF awards IIS-2106888 and CMMI-2146306.The views, opinions, and findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense, the U.S. Government, or NSF.ContributionsHMS conceived the study.HMS, EG, JHG, PH, BRJ, EM, GDB, PMB, AD, JF, TEG, CJM, RV, JH, CFL, WWL, AM and PM wrote and edited the manuscript.The remaining authors read and approved the manuscript content.Ethics declarationsCompeting interestsThe authors declare no competing interests.
acknowledges research reported in this publication was supported by NIBIB of the National Institutes of Health under award number NIH grant number P41EB023912. H M S , </p>
<p>acknowledges funding from the following awards: NIH 1 R01 HL169974-01. R L , NIH. 1</p>
<p>acknowledges funding from the following awards: National Institute on Alcohol Abuse and Alcoholism R01 AA018873, National Heart, Lung, and Blood Institute R01 HL161696. The funding sponsors had no role in the design of the study; in the collection, analyses, or interpretation of data. R V , in the writing of the manuscript, and in the decision to publish the results. NR was funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -EXC 2075 -390740016</p>
<p>B acknowledges work was supported by NRNB (U.S. National Institutes of Health, National Center for Research Resources grant number P41 GM103504). G D , </p>
<p>G acknowledges research reported in this publication was supported by NIBIB of the National Institutes of Health under award number NIH grant number P41EB023912. L.M.L acknowledges work was supported by NIH grant R24 GM137787 from the National Institute of. J H , General Medical Sciences. </p>
<p>J , S acknowledges funding from the following award: DST/NRF SARCHI-82813. </p>
<p>acknowledges funding from the following award: DST/NRF SRUG2204173612. P.M. acknowledges work was supported by NIH grant R24 GM137787 from the National Institute of. D V N , General Medical Sciences. </p>
<p>acknowledges work was supported by the Francis Crick Institute, which receives its core funding from Cancer Research UK (CC2242), the UK Medical Research Council (CC2242), and the Wellcome Trust. F F , CC2242</p>
<p>J R , F acknowledges support from NIH grants P41GM10371 and R01GM115805. </p>
<p>acknowledges funding from NSF grant. T J , 2000281</p>
<p>acknowledges support by the German Center for Infection Research (DZIF), grant № 8020708703. A D , </p>
<p>J M R , acknowledges funding from the following award: NRF grant number SRUG2204295377. </p>
<p>The FAIR guiding principles for scientific data management and stewardship. M D Wilkinson, M Dumontier, I J Aalbersberg, G Appleton, M Axton, A Baak, N Blomberg, J.-W Boiten, L B Silva Santos, P E Bourne, Scientific Data. 312016</p>
<p>FAIR principles for AI models with a practical application for accelerated high energy diffraction microscopy. N Ravi, P Chaturvedi, E Huerta, Z Liu, R Chard, A Scourtas, K Schmidt, K Chard, B Blaiszik, I Foster, Scientific Data. 916572022</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational physics. 3782019</p>
<p>Digital twins: Review and challenges. M G Juarez, V J Botti, A S Giret, Journal of Computing and Information Science in Engineering. 213308022021</p>
<p>Toward mechanistic medical digital twins: some use cases in immunology. R Laubenbacher, F Adler, G An, F Castiglione, S Eubank, L L Fonseca, J Glazier, T Helikar, M Jett-Tilton, D Kirschner, Frontiers in Digital Health. 613495952024</p>
<p>A framework for multi-scale intervention modeling: virtual cohorts, virtual clinical trials, and model-to-model comparisons. C T Michael, S A Almohri, J J Linderman, D E Kirschner, Front. Syst. Biol. 32023</p>
<p>The protein-folding problem, 50 years on. K A Dill, J L Maccallum, Science. 33861102012</p>
<p>Dynamics of folded proteins. J A Mccammon, B R Gelin, M Karplus, nature. 2671977</p>
<p>Immune digital twins for complex human pathologies: applications, limitations, and challenges. A Niarakis, R Laubenbacher, G An, Y Ilan, J Fisher, Å Flobak, K Reiche, M Rodríguez Martínez, L Geris, L Ladeira, L Veschini, M L Blinov, F Messina, L L Fonseca, S Ferreira, A Montagud, V Noël, M Marku, E Tsirvouli, M M Torres, L A Harris, T J Sego, C Cockrell, A E Shick, H Balci, A Salazar, K Rian, A A Hemedan, M Esteban-Medina, B Staumont, E Hernandez-Vargas, B Martis, S Madrid-Valiente, A Karampelesis, P Sordo, L Vieira, P Harlapur, A Kulesza, N Nikaein, W Garira, R S Malik Sheriff, J Thakar, V D T Tran, J Carbonell-Caballero, S Safaei, A Valencia, A Zinovyev, J A Glazier, 10.1038/s41540-024-00450-5Systems Biology and Applications. 1011412024</p>
<p>Building digital twins of the human immune system: toward a roadmap. R Laubenbacher, A Niarakis, T Helikar, G An, B Shapiro, R S Malik-Sheriff, T J Sego, A Knapp, P Macklin, J A Glazier, 10.1038/s41746-022-00610-zDigital Medicine. 512022</p>
<p>The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models. M Hucka, A Finney, H M Sauro, H Bolouri, J C Doyle, H Kitano, A P Arkin, B J Bornstein, D Bray, A Cornish-Bowden, Bioinformatics. 1942003</p>
<p>CellML 2.0. M Clerx, M T Cooling, J Cooper, A Garny, K Moyle, D P Nickerson, P M Nielsen, H Sorby, Journal of Integrative Bioinformatics. 172-3202000212020</p>
<p>NeuroML: a language for describing data driven models of neurons and networks with a high degree of biological detail. P Gleeson, S Crook, R C Cannon, M L Hines, G O Billings, M Farinella, T M Morse, A P Davison, S Ray, U S Bhalla, PLoS Computational Biology. 6610008152010</p>
<p>Current status and applications of genome-scale metabolic models. C Gu, G B Kim, W J Kim, H U Kim, S Y Lee, Genome biology. 202019</p>
<p>BiGG models: A platform for integrating, standardizing and sharing genome-scale models. Z A King, J Lu, A Dräger, P Miller, S Federowicz, J A Lerman, A Ebrahim, B O Palsson, N E Lewis, Nucleic Acids Research. 44D12016</p>
<p>KBase: the united states department of energy systems biology knowledgebase. A P Arkin, R W Cottingham, C S Henry, N L Harris, R L Stevens, S Maslov, P Dehal, D Ware, F Perez, S Canon, Nature Biotechnology. 3672018</p>
<p>Practical resources for enhancing the reproducibility of mechanistic modeling in systems biology. M L Blinov, J H Gennari, J R Karr, I I Moraru, D P Nickerson, H M Sauro, Current Opinion in Systems Biology. 271003502021</p>
<p>Publishing reproducible dynamic kinetic models. V Porubsky, L Smith, H M Sauro, Briefings in Bioinformatics. 2231522021</p>
<p>K Tiwari, S Kananathan, M G Roberts, J P Meyer, M U Sharif Shohan, A Xavier, M Maire, A Zyoud, J Men, S Ng, Reproducibility in systems biology modelling. 2021179982</p>
<p>A scalable, open-source implementation of a large-scale mechanistic model for single cell proliferation and death signaling. C Erdem, A Mutsuddy, E M Bensman, W B Dodd, M M Saint-Antoine, M Bouhaddou, R C Blake, S M Gross, L M Heiser, F A Feltus, Nature Communications. 13135552022</p>
<p>Setting the basis of best practices and standards for curation and annotation of logical models in biology-highlights of the [bc]2 2019 colomoto/sysmod workshop. A Niarakis, M Kuiper, M Ostaszewski, R S Malik Sheriff, C Casals-Casas, D Thieffry, T C Freeman, P Thomas, V Touré, V Noël, G Stoll, J Saez-Rodriguez, A Naldi, E Oshurko, I Xenarios, S Soliman, C Chaouiya, T Helikar, L Calzone, 10.1093/bib/bbaa046Briefings in Bioinformatics. 2222020</p>
<p>Linking genome-scale metabolic modeling and genome annotation. E M Blais, A K Chavali, J A Papin, Systems Metabolic Engineering: Methods and Protocols. 2013</p>
<p>Genome-scale metabolic modeling enables in-depth understanding of big data. A Passi, J D Tibocha-Bonilla, M Kumar, D Tec-Campos, K Zengler, C Zuniga, Metabolites. 121142021</p>
<p>BioModels-15 years of sharing computational models in life science. R S Malik-Sheriff, M Glont, T V Nguyen, K Tiwari, M G Roberts, A Xavier, M T Vu, J Men, M Maire, S Kananathan, Nucleic Acids Research. 48D12020</p>
<p>Minimum information requested in the annotation of biochemical models (miriam). N L Novère, A Finney, M Hucka, U S Bhalla, F Campagne, J Collado-Vides, E J Crampin, M Halstead, E Klipp, P Mendes, Nature Biotechnology. 23122005</p>
<p>A c library for retrieving specific reactions from the biomodels database. M L Neal, M Galdzicki, J Gallimore, H M Sauro, Bioinformatics. 3012014</p>
<p>B P Ingalls, Mathematical Modeling in Systems Biology: an Introduction. MIT press2013</p>
<p>H M Sauro, Systems Biology: Introduction to Pathway Modeling. Ambrosius Publishing2020</p>
<p>A review of dynamic modeling approaches and their application in computational strain optimization for metabolic engineering. O D Kim, M Rocha, P Maia, Frontiers in Microbiology. 93788852018</p>
<p>The jws online simulation database. M Peters, J J Eicher, D D Van Niekerk, D Waltemath, J L Snoep, Bioinformatics. 33102017</p>
<p>Twenty years of ModelDB and beyond: building essential modeling tools for the future of neuroscience. R A Mcdougal, T M Morse, T Carnevale, L Marenco, R Wang, M Migliore, P L Miller, G M Shepherd, M L Hines, Journal of computational neuroscience. 422017</p>
<p>Physiological interactions between nav1. 7 and nav1. 8 sodium channels: a computer simulation study. J.-S Choi, S G Waxman, Journal of Neurophysiology. 10662011</p>
<p>Programming biological models in python using pysb. C F Lopez, J L Muhlich, J A Bachman, P K Sorger, Molecular systems biology. 916462013</p>
<p>Sphinx documentation. G Brandl, 2021</p>
<p>The opencarp simulation environment for cardiac electrophysiology. G Plank, A Loewe, A Neic, C Augustin, Y.-L Huang, M A Gsell, E Karabelas, M Nothstein, A J Prassl, J Sánchez, Computer methods and Programs in Biomedicine. 2081062232021</p>
<p>Fieldml, a proposed open standard for the physiome project for mathematical model representation. R D Britten, G R Christie, C Little, A K Miller, C Bradley, A Wu, T Yu, P Hunter, P Nielsen, Medical &amp; biological engineering &amp; computing. 512013</p>
<p>Sbml level 3 package: spatial processes, version 1, release 1. J C Schaff, A Lakshminarayana, R F Murphy, F T Bergmann, A Funahashi, D P Sullivan, L P Smith, Journal of Integrative Bioinformatics. 201202200542023</p>
<p>Non-linear optimization of biochemical pathways: applications to metabolic engineering and parameter estimation. P Mendes, D Kell, Bioinformatics. 14101998</p>
<p>Robust and efficient parameter estimation in dynamic models of biological systems. A Gábor, J R Banga, BMC Systems Biology. 92015</p>
<p>Systems biology: parameter estimation for biochemical models. M Ashyraliyev, Y Fomekong-Nanfack, J A Kaandorp, J G Blom, The FEBS Journal. 27642009</p>
<p>On structural and practical identifiability. F.-G Wieland, A L Hauber, M Rosenblatt, C Tönsing, J Timmer, Current Opinion in Systems Biology. 252021</p>
<p>PyDREAM: high-dimensional parameter inference for biological models in python. E M Shockley, J A Vrugt, C F Lopez, Bioinformatics. 3442018</p>
<p>Model certainty in cellular networkdriven processes with missing data. M W Irvin, A Ramanathan, C F Lopez, PLoS Computational Biology. 19410110042023</p>
<p>Accuracy and data efficiency in deep learning models of protein expression. E Nikolados, A Wongprommoon, O M Aodha, Nature Communications. 1377552022</p>
<p>The hidden costs of automated thinking. J Zittrain, The New Yorker. 2019</p>
<p>How accurate is chest imaging for diagnosing COVID-19. N Islam, S Ebrahimzadeh, J Salameh, S Kazi, N Fabiano, L Treanor, M Absi, Z Hallgrimson, M Leeflang, L Hooft, 2021Cochrane</p>
<p>Addressing barriers in comprehensiveness, accessibility, reusability, interoperability and reproducibility of computational models in systems biology. A Niarakis, D Waltemath, J Glazier, F Schreiber, S M Keating, D Nickerson, C Chaouiya, A Siegel, V Noël, H Hermjakob, T Helikar, S Soliman, L Calzone, 10.1093/bib/bbac212Briefings in Bioinformatics. 2342122022</p>
<p>Model credibility revisited: Concepts and considerations for appropriate trust. L Yilmaz, B Liu, Journal of Simulation. 1632022</p>
<p>Adapting modeling and simulation credibility standards to computational systems biology. L T Tatka, L P Smith, J L Hellerstein, H M Sauro, Journal of Translational Medicine. 2115012023</p>
<p>Terminology for model credibility. S Schlesinger, Simulation. 3231979</p>
<p>Credible practice of modeling and simulation in healthcare: ten rules from a multidisciplinary perspective. A Erdemir, L Mulugeta, J P Ku, A Drach, M Horner, T M Morrison, G C Peng, R Vadigepalli, W W Lytton, J G MyersJr, Journal of Translational Medicine. 1813692020</p>
<p>Scientific and regulatory evaluation of mechanistic in silico drug and disease models in drug development: building model credibility. F Musuamba, I Skottheim Rusten, R Lesage, CPT Pharmacometrics and Systems Pharmacology. 102021</p>
<p>A rubric for assessing conformance to the ten rules for credible practice of modeling and simulation in healthcare. R Vadigepalli, A Manchel, A Erdemir, L Mulugeta, J P Ku, B V Rego, M Horner, W W Lytton, J G Myers, medRxiv. 2024</p>
<p>Concepts of model verification and validation (No. LA-14167). B Thacker, S Doebling, F Hemez, M Anderson, J Pepin, E Rodriguez, 2004Los Alamos, NM (USLos Alamos National Lab</p>
<p>Assessing the Reliability of Complex Models: Mathematical and Statistical Foundations of Verification, Validation, and Uncertainty Quantification. N R Council, D Engineering, P Sciences, B Sciences, T Applications, C Verification, U Quantification, 2012National Academies Press</p>
<p>Comparing simulation results of SBML capable simulators. F T Bergmann, H M Sauro, Bioinformatics. 24172008</p>
<p>M Hucka, A F Independent, E He, S Keating, 10.5281/zenodo.1112521SBML Test Suite. 2022</p>
<p>Recent advances in biomedical simulations: a manifesto for model engineering. J L Hellerstein, S Gu, K Choi, H M Sauro, F1000Research. 82019</p>
<p>A work flow to build and validate patient specific left atrium electrophysiology models from catheter measurements. C Corrado, S Williams, R Karim, G Plank, M O'neill, S Niederer, Medical image analysis. 472018</p>
<p>The status of causality in biological databases: data resources and data retrieval possibilities to support logical modeling. V Touré, Å Flobak, A Niarakis, S Vercruysse, M Kuiper, 10.1093/bib/bbaa390Briefings in Bioinformatics. 2242020</p>
<p>Data integration in logic-based models of biological mechanisms. B A Hall, A Niarakis, 10.1016/j.coisb.2021.100386Current Opinion in Systems Biology. 282021</p>
<p>Model selection in systems and synthetic biology. P Kirk, T Thorne, M P Stumpf, Current opinion in biotechnology. 2442013</p>
<p>A new look at the statistical model identification. H Akaike, IEEE Transactions on Automatic Control. 1961974</p>
<p>Unified tumor growth mechanisms from multimodel inference and dataset integration. S P Beik, L A Harris, M A Kochen, J Sage, V Quaranta, C F Lopez, PLOS Computational Biology. 19710112152023</p>
<p>On information and sufficiency. S Kullback, R A Leibler, The Annals of Mathematical Statistics. 2211951</p>
<p>In silico trials: Verification, validation and uncertainty quantification of predictive models used in the regulatory evaluation of biomedical products. M Viceconti, F Pappalardo, B Rodriguez, M Horner, J Bischoff, F M Tshinanu, Methods. 1852021</p>
<p>An evolution of uncertainty assessment and quantification. J M Booker, T J Ross, Scientia Iranica. 1832011</p>
<p>BioSimulators: a central registry of simulation engines and services for recommending specific tools. B Shaikh, L P Smith, D Vasilescu, G Marupilla, M Wilson, E Agmon, H Agnew, S S Andrews, A Anwar, M E Beber, Nucleic Acids Research. 50W12022</p>
<p>COPASI-a complex pathway simulator. S Hoops, S Sahle, R Gauges, C Lee, J Pahle, N Simus, M Singhal, L Xu, P Mendes, U Kummer, Bioinformatics. 22242006</p>
<p>MEMOTE for standardized genome-scale metabolic model testing. C Lieven, M E Beber, B G Olivier, F T Bergmann, M Ataman, P Babaei, J A Bartell, L M Blank, S Chauhan, K Correia, Nature Biotechnology. 3832020</p>
<p>Theories and models: What they are, what they are for, and what they are about. E I Fried, Psychological Inquiry. 3142020</p>
<p>Understanding Scientific Understanding. D Regt, W , H , 2017Oxford University PressOxford</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nature medicine. 2982023</p>
<p>Probabilistic weather forecasting with machine learning. I Price, A Sanchez-Gonzalez, F E Alet, 10.1038/s41586-024-08252-9Nature. 2024</p>
<p>Software engineering practices for scientific software development: A systematic mapping study. E.-M Arvanitou, A Ampatzoglou, A Chatzigeorgiou, J C Carver, Journal of Systems and Software. 1721108482021</p>
<p>The in silico lab: Improving academic code using lessons from biology. J Y Cain, J S Yu, N Bagheri, 10.1016/j.cels.2022.11.006Cell Systems. 1412023</p>
<p>Writing Clean Code -How It Impacts the Future of Your Product. G Lteif, 2024</p>
<p>Exploring complexity: An introduction. G Nicolis, I Prigogine, 1989</p>
<p>Model storage, exchange and integration. N Le Novère, BMC Neuroscience. 72006</p>
<p>Controlled vocabularies and semantics in systems biology. M Courtot, N Juty, C Knüpfer, D Waltemath, A Zhukova, A Dräger, M Dumontier, A Finney, M Golebiewski, J Hastings, Molecular Systems Biology. 715432011</p>
<p>Community standards to facilitate development and address challenges in metabolic modeling. M A Carey, A Dräger, M E Beber, J A Papin, J T Yurkovich, 10.15252/msb.20199235Molecular Systems Biology. 16892352020</p>
<p>Kinetic simulation algorithm ontology. A Zhukova, D Waltemath, N Juty, C Laibe, N Le Novère, Nature Precedings. 2011</p>
<p>Cell-signalling dynamics in time and space. B N Kholodenko, Nature reviews Molecular cell biology. 732006</p>
<p>Yeast glycolytic oscillations that are not controlled by a single oscillophore: a new definition of oscillophore strength. K A Reijenga, Y M Megen, B W Kooi, B M Bakker, J L Snoep, H W Verseveld, H V Westerhoff, Journal of Theoretical Biology. 23232005</p>
<p>Reverse engineering of biological complexity. M E Csete, J C Doyle, Science. 29555602002</p>
<p>Design patterns of biological cells. S S Andrews, H S Wiley, H M Sauro, BioEssays. 23001882024</p>
<p>1,500 scientists lift the lid on reproducibility. M Baker, Nature. 53376042016</p>
<p>Reproducibility vs. replicability: a brief history of a confused terminology. H E Plesser, Frontiers in neuroinformatics. 11762018</p>
<p>Best practices for making reproducible biochemical models. V L Porubsky, A P Goldberg, A K Rampadarath, D P Nickerson, J R Karr, H M Sauro, Cell Systems. 1122020</p>
<p>Standards, dissemination, and best practices in systems biology. J Shin, V Porubsky, J Carothers, H M Sauro, Current Opinion in Biotechnology. 811029222023</p>
<p>Reproducible research using biomodels. P Mendes, Bulletin of Mathematical Biology. 80122018</p>
<p>Bayesian estimation reveals that reproducible models in systems biology get more citations. S Höpfl, J Pleiss, N E Radde, Scientific Reports. 13126952023</p>
<p>Reliability and reproducibility in computational science: Implementing validation, verification and uncertainty quantification in silico. P V Coveney, D Groen, A G Hoekstra, 2021The Royal Society Publishing</p>
<p>Virtual Cell modelling and simulation software environment. I I Moraru, J C Schaff, B M Slepchenko, M Blinov, F Morgan, A Lakshminarayana, F Gao, Y Li, L M Loew, IET systems biology. 252008</p>
<p>Tellurium: an extensible python-based modeling environment for systems and synthetic biology. K Choi, J K Medley, M König, K Stocking, L Smith, S Gu, H M Sauro, Biosystems. 1712018</p>
<p>Tellurium notebooks-an environment for reproducible dynamical modeling in systems biology. J K Medley, K Choi, M König, L Smith, S Gu, J Hellerstein, S C Sealfon, H M Sauro, PLoS Computational Biology. 14610062202018</p>
<p>A reappraisal of how to build modular, reusable models of biological systems. M L Neal, M T Cooling, L P Smith, C T Thompson, H M Sauro, B E Carlson, D L Cook, J H Gennari, PLoS Computational Biology. 101010038492014</p>
<p>Antimony: a modular model definition language. L P Smith, F T Bergmann, D Chandran, H M Sauro, Bioinformatics. 25182009</p>
<p>Physical system modeling with modelica. S E Mattsson, H Elmqvist, M Otter, Control Engineering Practice. 641998</p>
<p>Mastering Simulink. J B Dabney, T L Harman, 2004Prentice Hall Upper230Saddle River, New Jersey</p>
<p>Assessing the credibility of computational modeling and simulation in medical device submissions. Drug Food, Administration, 2021</p>
<p>Causality analysis and cell network modeling of spatial calcium signaling patterns in liver lobules. A Verma, A N Antony, B A Ogunnaike, J B Hoek, R Vadigepalli, Frontiers in physiology. 913772018</p>
<p>Closed-loop modeling of central and intrinsic cardiac nervous system circuits underlying cardiovascular control. M M Gee, A M Lenhoff, J S Schwaber, B A Ogunnaike, R Vadigepalli, AIChE Journal. 694180332023</p>
<p>Foundational research gaps and future directions for digital twins. 2023National Academies of Sciences, Engineering and Medicine</p>
<p>The systems biology graphical notation. N L Novère, M Hucka, H Mi, S Moodie, F Schreiber, A Sorokin, E Demir, K Wegner, M I Aladjem, S M Wimalaratne, Nature biotechnology. 2782009</p>
<p>A model diagram layout extension for sbml. R Gauges, U Rost, S Sahle, K Wegner, Bioinformatics. 22152006</p>
<p>Sbml level 3 package: render, version 1, release 1. F T Bergmann, S M Keating, R Gauges, S Sahle, K Wengler, Journal of Integrative Bioinformatics. 151201700782018</p>
<p>From bits to bedside: Entering the age of digital twins in cardiac electrophysiology. P Bhagirath, M Strocchi, M J Bishop, P M Boyle, G Plank, 2024Europace in press</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4475 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4475</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4475</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-264824290</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.19740v1.pdf" target="_blank">Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Humans are widely involved in the evaluation of open-ended natural language generation tasks (NLG) that demand creativity, as automatic metrics often exhibit weak correlations with human judgments. Large language models (LLMs) recently have emerged as a scalable and cost-effective alternative to human evaluations. However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements. To explore the synergy between humans and LLM-based evaluators and address the challenges of existing inconsistent evaluation criteria in open-ended NLG tasks, we propose a Collaborative Evaluation pipeline CoEval, involving the design of a checklist of task-specific criteria and the detailed evaluation of texts, in which LLM generates initial ideation, and then humans engage in scrutiny. We conducted a series of experiments to investigate the mutual effects between LLMs and humans in CoEval. Results show that, by utilizing LLMs, CoEval effectively evaluates lengthy texts, saving significant time and reducing human evaluation outliers. Human scrutiny still plays a role, revising around 20% of LLM evaluation scores for ultimate reliability.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4475.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4475.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collaborative Evaluation (LLM-ideation — Human-scrutiny) pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage hybrid evaluation framework where an LLM (ideation) proposes task-specific evaluation criteria and instance-level judgments (conclusion, 1–5 score, explanation), and human annotators scrutinize and revise both criteria and instance evaluations using a small set of atomic actions to ensure reliability and realism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>COEVAL LLM-ideation–Human-scrutiny pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Stage 1: LLM is prompted with task description and I/O fields to generate a candidate checklist of task-specific criteria; human evaluators then inspect each proposed criterion and Apply one of four actions (approve / delete / revise / add) to produce a final checklist. Stage 2: For each instance (input X, output Y), LLM produces a granular evaluation E_M per criterion comprising (1) a clear conclusion (satisfies/does not), (2) a scalar score on a 1–5 Likert scale, and (3) supporting explanations/evidence quoted from the output. Humans then scrutinize each LLM-produced evaluation and may approve, delete, revise, or add to the conclusion/score/evidence, producing final human-vetted evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific checklist finalized via LLM ideation + human scrutiny; examples used in experiments include relevance, factuality, comprehensibility (e.g., 'simple language' for ELI5), coherence, completeness, engagement, clarity, politeness, and length appropriateness. Each criterion is evaluated with a ternary output: qualitative conclusion, 1–5 score, and explanatory evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / explanatory text (long-form QA, storytelling, instruction-following)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanations and open-ended generative outputs (long-form answers, narratives, instruction-following responses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>COEVAL yields substantial practical gains: LLM-proposed criteria comprised >70% of final criteria (humans removed some unrealistic items); using COEVAL reduced per-sample human time vs conventional human evaluation (HUMANEVAL) roughly by half (minutes per sample: ROCStories 5.34 vs 8.70; Self-Instruct 4.41 vs 9.90; ELI5 4.13 vs 8.25). Human annotators revised about 20% of LLM evaluation scores overall (29.34% revision for ELI5). Using LLM evaluations in the pipeline reduced human evaluation outliers by ~62%; Krippendorff's α for annotator agreement improved from 0.64 (baseline) to 0.71 under COEVAL. Approval rates for LLM-generated instance evaluations were high across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: LLM provides initial automated candidate criteria and per-instance evaluations including explanations; humans then apply targeted scrutiny actions to refine and correct LLM outputs. The approach explicitly mixes automated and human judgments rather than replacing humans.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by (a) comparison with a conventional multi-annotator human evaluation baseline (HUMANEVAL) including majority-vote ground truth; (b) inter-annotator agreement metrics (Krippendorff's α) to quantify consistency; (c) average Pearson correlations among evaluator pairs (LLM, COEVAL, HUMANEVAL); (d) analysis of human correction rates, outlier reductions, and time-per-sample measures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM ideation sometimes proposes unrealistic or imprecise criteria (~27% judged unrealistic), LLM evaluations can skew positive and fail on complex criteria (numerical reasoning, information-seeking), sensitivity to prompt/instruction formatting and decoding settings, and cost/time for human scrutiny (qualification and hiring); paper uses a single LLM (ChatGPT) so generalization across LLMs is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Three NLG tasks/datasets used as evaluation benchmarks: ELI5 (long-form explain-to-5-year-old QA), ROCStories (5-sentence story generation), and Self-Instruct (instruction-following examples). 50 sampled instances per task × 4 generation sources (3 models + human) → 200 instances per task; each instance rated by 5 evaluators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4475.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4475.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Checklist / Task-specific criteria</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task Criteria Checklist (LLM-generated, human-refined)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tailored set of evaluation dimensions (criteria) for each open-ended task, generated by an LLM from task description and I/O fields and then curated by human evaluators via approve/delete/revise/add actions to ensure feasibility and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-generated task checklist with human refinement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLM receives task description, input and output fields and lists possible evaluation criteria. Humans examine each criterion and apply one of four atomic actions (approve, delete, revise, add). The final checklist is produced via majority vote among human reviewers. The checklist then governs instance-level evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Domain/task-specific dimensions such as factuality, simplicity/comprehensibility (ELI5), coherence, relevance, completeness, engagement, clarity, grammar, politeness, originality, and adherence to instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (ChatGPT) used to generate candidate criteria</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG / evaluation design (task-conditional criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation frameworks for explanatory and generative text</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLM proposals accounted for >70% of final criteria; humans deleted ~2–3 criteria per task on average when unrealistic; approval rates high (Self-Instruct: researchers >60%, laypeople ~80% approval), deletion and addition statistics show LLM tends to miss certain conventional criteria (e.g., factuality) and may suggest unrealistic ones (e.g., visual aids for text-only tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated ideation (LLM) + human curation to ensure operationalizable, instance-evaluable criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human pre-screening and multi-annotator majority vote; separate groups (NLP researchers and laypeople) inspected feasibility and precision; action rate r_a computed to quantify human responses to each candidate criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM can propose too many or impractical criteria; human effort needed to filter and operationalize criteria; selection depends on evaluator backgrounds introducing subjectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used across ELI5, ROCStories, Self-Instruct to produce per-task checklists; documented in paper Table 1, Table 4 and appendices.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4475.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4475.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instance-level ternary evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instance Evaluation with Conclusion, Score, and Explanation (post-explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Per-criterion evaluation format where the LLM produces (1) a qualitative conclusion (satisfies/does not), (2) a scalar score on a 1–5 Likert scale, and (3) a post-hoc explanation/evidence quoting sentences from the output, intended to aid human scrutiny and error detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Ternary instance evaluation (Conclusion + Likert score + Explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each criterion c_i and instance (X,Y), LLM outputs: a conclusion about whether Y satisfies c_i, a numeric score 1–5, and an explanation/evidence section that quotes or references specific text to justify the judgment. Explanations are produced post-judgment to reflect LLM reasoning shaped by its conclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same per-task criteria from checklist. Each criterion is judged by the ternary output; explanations must quote supporting sentences from the candidate output.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (ChatGPT) used for generating instance evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG/explanatory QA/story generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluative assessments of explanations and generated content</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLM-generated ternary evaluations were accepted at high rates by humans; humans revised ~20–29% of LLM-proposed numeric scores (29.34% revision for ELI5), and corrected ill-founded evidentiary claims. Use of explanatory evidence helped humans detect hallucinated or unclear judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated LLM judgments augmented with human post-hoc corrections; explanations are diagnostic tools for humans to accept or revise LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human scrutiny of LLM outputs with logging of approve/delete/revise/add actions; inter-annotator agreement measured on final scores; comparison to HUMANEVAL majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM explanations can include incorrect evidence or miss subtle failures; score extraction from LLM output required normalization heuristics (regex stripping), and explanations depend on LLM prompt and may vary across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to ELI5, ROCStories, Self-Instruct evaluation instances (5–7 criteria per instance).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4475.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4475.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human scrutiny actions (A)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atomic human scrutiny actions: approve / delete / revise / add</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimal set of four actions available to human evaluators to refine LLM-generated criteria and instance evaluations, used to operationalize human corrections with low effort.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Approve/Delete/Revise/Add human action taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each proposed criterion or LLM evaluation item, a human chooses exactly one action: approve (accept as-is), delete (remove unrealistic/irrelevant item), revise (modify wording or scope), or add (introduce a missing but necessary criterion or comment). Applied both at task criteria stage and instance evaluation stage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>N/A (this is the action set to manipulate criteria/evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Human-in-the-loop evaluation methodology (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human-AI collaborative correction framework</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Action usage quantified: >70% of final criteria originated from LLM but required human approval; r_a (action rate) computed per-action to summarize human reactions; humans revised ~20% of instance-level LLM scores.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-driven corrections applied to automated LLM proposals — operationalizes hybrid evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Computed action rates r_a for approve/delete/revise/add across criteria and instances; used majority vote among annotators to finalize criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Single-action constraint per criterion may oversimplify nuanced disagreements; human actions can introduce subjectivity; requires qualified annotators and a qualification pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used across the three tasks (ELI5, ROCStories, Self-Instruct) in the experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4475.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4475.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action rate (r_a)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human action rate metric (r_a)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric quantifying how human evaluators responded to each LLM-proposed criterion by counting occurrences of each atomic action (approve/delete/revise/add) and reporting percentages – used as a proxy for LLM criterion quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Action rate (r_a) for human responses to LLM-proposed criteria</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each proposed criterion, r_a* = |a*| / |A| * 100 is computed where |a*| is the number of annotators choosing action a* (approve/delete/revise/add) and |A| is total annotators. The majority action defines final treatment of the candidate criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Measures human acceptance (approve %) vs deletion/revision/addition percentages for LLM-generated criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation process audit (NLP evaluation design)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Quality metric for candidate criteria</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to show that many LLM-generated criteria are feasible (>70% acceptance) but some are unrealistic (~27% flagged); example: 1st criterion in Table 1 had r_a_apv = 100%, 8th criterion had r_a_add = 62.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-derived metric summarizing human-LLM interaction rather than an automated metric.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Computed over multiple annotators and datasets; majority vote used to finalize checklists.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>r_a is sensitive to annotator pool composition and subjective judgments; single-number summaries can mask nuanced disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Computed for the candidate criteria generated for ELI5, ROCStories, and Self-Instruct.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4475.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4475.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Krippendorff's α</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Krippendorff's alpha (inter-annotator agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard statistic used to quantify agreement among multiple annotators (or annotator groups), applied here to compare consistency between ChatGPT and human evaluators on 5-point score annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Krippendorff's α inter-annotator agreement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Krippendorff's α on the 5-point Likert scores across annotators (and between ChatGPT and human groups) to measure annotation consistency; used to report agreement levels (e.g., α > 0.7 as high agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Agreement on numeric 1–5 scores and sometimes conclusions across evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (as an annotator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Annotation reliability measurement in NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Reliability metric for judgments about explanations/generated text</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported Krippendorff's α improvements: COEVAL increased α from 0.64 to 0.71; specific groups of humans had α > 0.8 internally; Group 1's agreement with ChatGPT ≈ 0.754 vs Group 2 ≈ 0.487 in one analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Metric applied to both automated (LLM) and human annotators to quantify consistency/harmony between them.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct computation on annotation data collected in experiments; used as an evaluation of improved consistency under COEVAL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>α is sensitive to variance across annotators and number of categories; improvements may reflect reduction of outliers rather than better absolute correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Computed over collected annotations for ELI5 (and presented for other tasks in appendices).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4475.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4475.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pearson correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average Pearson correlation among evaluator pairs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pairwise Pearson correlation coefficient computed between evaluators' numeric scores to quantify similarity of scoring behavior (LLM vs humans; COEVAL vs HUMANEVAL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Average Pearson correlation among evaluator pairs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Pearson correlation coefficients for score vectors between pairs of evaluators (e.g., LLM vs human, COEVAL vs HUMANEVAL) and average them to summarize alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Numeric 1–5 Likert scores across instances and criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 used as one evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistical alignment measurement in NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Comparative scoring correlation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported that LLM scores had relatively weak correlation with humans on ELI5 and ROCStories (specific aggregated values in Table 8); COEVAL vs HUMANEVAL correlation comparable to internal HUMANEVAL correlation, indicating LLM does not strongly bias human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Statistical comparison metric applied post-hoc to automated and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Computed over experiment annotation matrices; compared correlations across evaluator groups and settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Pearson correlation summarizes linear association but can mask systematic biases (e.g., LLM's more positive scoring distribution).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4475.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4475.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Outlier reduction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Outlier human-evaluation reduction (via LLM-assisted pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A measured reduction in extreme or inconsistent human judgments when LLM-produced evaluations are used as references in COEVAL, quantified as percent decrease in outlier human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Outlier reduction measurement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Identify outlier human evaluations in a conventional HUMANEVAL setting and measure how many of those outliers are removed (or agree with majority) when humans work within COEVAL (i.e., with LLM initial judgments). Report percent reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Frequency/count of human evaluations classified as statistical or judgmental outliers relative to majority/HUMANEVAL consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (used to produce initial references)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation consistency analysis for NLG</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human-LLM collaborative effect on inter-annotator variance</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using LLM evaluations reduced outlier human evaluations by approximately 62% while preserving diverse human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: uses LLM outputs as anchors/reference that humans can accept or correct; measured impact on human outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison of outlier counts between HUMANEVAL and COEVAL; majority-vote HUMANEVAL used as ground truth for some analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reduction in outliers may reflect conformity to LLM rather than true error correction; must ensure LLM judgments are correct to avoid removing valid minority opinions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4475.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4475.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Time-per-sample efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation time per sample (COEVAL vs HUMANEVAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical measurement of human time (minutes per sample) required to produce final evaluations under COEVAL vs conventional human-only evaluation, used to quantify efficiency gains from LLM assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Per-sample human time measurement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Record and compare the average human annotation time to evaluate one instance under COEVAL (LLM-first, then human-scrutiny) and HUMANEVAL (human-only full evaluation). Report minutes per sample per task and analyze per-criterion time consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Avg. annotation time (seconds or minutes) per instance and per criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 used to pre-fill evaluations in COEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation workload measurement in NLG</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Operational efficiency metric</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>COEVAL is ~50% faster than HUMANEVAL: ROCStories 5.34 vs 8.70 min/sample; Self-Instruct 4.41 vs 9.90; ELI5 4.13 vs 8.25. Time per criterion varied by task and criterion difficulty (see Appendix A.6/Figure 8).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid measurement: LLM reduces human time by pre-computing candidate judgments; humans then spend less time per item.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct timing logs from the annotation platform, with periodic checks for consistency; qualification tests to ensure annotator quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Annotators slow down mid-session affecting time measures; overhead for qualification and communication; savings depend on quality of LLM outputs and per-criterion difficulty.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-eval: Nlg evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good nlg evaluator? a preliminary study <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Evaluating large language models at evaluating instruction following <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 1)</em></li>
                <li>Evaluating nlg evaluation metrics: A measurement theory perspective <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4475",
    "paper_id": "paper-264824290",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "COEVAL",
            "name_full": "Collaborative Evaluation (LLM-ideation — Human-scrutiny) pipeline",
            "brief_description": "A two-stage hybrid evaluation framework where an LLM (ideation) proposes task-specific evaluation criteria and instance-level judgments (conclusion, 1–5 score, explanation), and human annotators scrutinize and revise both criteria and instance evaluations using a small set of atomic actions to ensure reliability and realism.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "COEVAL LLM-ideation–Human-scrutiny pipeline",
            "evaluation_method_description": "Stage 1: LLM is prompted with task description and I/O fields to generate a candidate checklist of task-specific criteria; human evaluators then inspect each proposed criterion and Apply one of four actions (approve / delete / revise / add) to produce a final checklist. Stage 2: For each instance (input X, output Y), LLM produces a granular evaluation E_M per criterion comprising (1) a clear conclusion (satisfies/does not), (2) a scalar score on a 1–5 Likert scale, and (3) supporting explanations/evidence quoted from the output. Humans then scrutinize each LLM-produced evaluation and may approve, delete, revise, or add to the conclusion/score/evidence, producing final human-vetted evaluations.",
            "evaluation_criteria": "Task-specific checklist finalized via LLM ideation + human scrutiny; examples used in experiments include relevance, factuality, comprehensibility (e.g., 'simple language' for ELI5), coherence, completeness, engagement, clarity, politeness, and length appropriateness. Each criterion is evaluated with a ternary output: qualitative conclusion, 1–5 score, and explanatory evidence.",
            "model_name": "gpt-3.5-turbo-0301 (ChatGPT)",
            "model_size": null,
            "scientific_domain": "Natural Language Generation / explanatory text (long-form QA, storytelling, instruction-following)",
            "theory_type": "Explanations and open-ended generative outputs (long-form answers, narratives, instruction-following responses)",
            "human_comparison": true,
            "evaluation_results": "COEVAL yields substantial practical gains: LLM-proposed criteria comprised &gt;70% of final criteria (humans removed some unrealistic items); using COEVAL reduced per-sample human time vs conventional human evaluation (HUMANEVAL) roughly by half (minutes per sample: ROCStories 5.34 vs 8.70; Self-Instruct 4.41 vs 9.90; ELI5 4.13 vs 8.25). Human annotators revised about 20% of LLM evaluation scores overall (29.34% revision for ELI5). Using LLM evaluations in the pipeline reduced human evaluation outliers by ~62%; Krippendorff's α for annotator agreement improved from 0.64 (baseline) to 0.71 under COEVAL. Approval rates for LLM-generated instance evaluations were high across tasks.",
            "automated_vs_human_evaluation": "Hybrid: LLM provides initial automated candidate criteria and per-instance evaluations including explanations; humans then apply targeted scrutiny actions to refine and correct LLM outputs. The approach explicitly mixes automated and human judgments rather than replacing humans.",
            "validation_method": "Validated by (a) comparison with a conventional multi-annotator human evaluation baseline (HUMANEVAL) including majority-vote ground truth; (b) inter-annotator agreement metrics (Krippendorff's α) to quantify consistency; (c) average Pearson correlations among evaluator pairs (LLM, COEVAL, HUMANEVAL); (d) analysis of human correction rates, outlier reductions, and time-per-sample measures.",
            "limitations_challenges": "LLM ideation sometimes proposes unrealistic or imprecise criteria (~27% judged unrealistic), LLM evaluations can skew positive and fail on complex criteria (numerical reasoning, information-seeking), sensitivity to prompt/instruction formatting and decoding settings, and cost/time for human scrutiny (qualification and hiring); paper uses a single LLM (ChatGPT) so generalization across LLMs is limited.",
            "benchmark_dataset": "Three NLG tasks/datasets used as evaluation benchmarks: ELI5 (long-form explain-to-5-year-old QA), ROCStories (5-sentence story generation), and Self-Instruct (instruction-following examples). 50 sampled instances per task × 4 generation sources (3 models + human) → 200 instances per task; each instance rated by 5 evaluators.",
            "uuid": "e4475.0"
        },
        {
            "name_short": "Checklist / Task-specific criteria",
            "name_full": "Task Criteria Checklist (LLM-generated, human-refined)",
            "brief_description": "A tailored set of evaluation dimensions (criteria) for each open-ended task, generated by an LLM from task description and I/O fields and then curated by human evaluators via approve/delete/revise/add actions to ensure feasibility and coverage.",
            "citation_title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM-generated task checklist with human refinement",
            "evaluation_method_description": "LLM receives task description, input and output fields and lists possible evaluation criteria. Humans examine each criterion and apply one of four atomic actions (approve, delete, revise, add). The final checklist is produced via majority vote among human reviewers. The checklist then governs instance-level evaluations.",
            "evaluation_criteria": "Domain/task-specific dimensions such as factuality, simplicity/comprehensibility (ELI5), coherence, relevance, completeness, engagement, clarity, grammar, politeness, originality, and adherence to instructions.",
            "model_name": "gpt-3.5-turbo-0301 (ChatGPT) used to generate candidate criteria",
            "model_size": null,
            "scientific_domain": "NLG / evaluation design (task-conditional criteria)",
            "theory_type": "Evaluation frameworks for explanatory and generative text",
            "human_comparison": true,
            "evaluation_results": "LLM proposals accounted for &gt;70% of final criteria; humans deleted ~2–3 criteria per task on average when unrealistic; approval rates high (Self-Instruct: researchers &gt;60%, laypeople ~80% approval), deletion and addition statistics show LLM tends to miss certain conventional criteria (e.g., factuality) and may suggest unrealistic ones (e.g., visual aids for text-only tasks).",
            "automated_vs_human_evaluation": "Hybrid: automated ideation (LLM) + human curation to ensure operationalizable, instance-evaluable criteria.",
            "validation_method": "Human pre-screening and multi-annotator majority vote; separate groups (NLP researchers and laypeople) inspected feasibility and precision; action rate r_a computed to quantify human responses to each candidate criterion.",
            "limitations_challenges": "LLM can propose too many or impractical criteria; human effort needed to filter and operationalize criteria; selection depends on evaluator backgrounds introducing subjectivity.",
            "benchmark_dataset": "Used across ELI5, ROCStories, Self-Instruct to produce per-task checklists; documented in paper Table 1, Table 4 and appendices.",
            "uuid": "e4475.1"
        },
        {
            "name_short": "Instance-level ternary evaluation",
            "name_full": "Instance Evaluation with Conclusion, Score, and Explanation (post-explanation)",
            "brief_description": "Per-criterion evaluation format where the LLM produces (1) a qualitative conclusion (satisfies/does not), (2) a scalar score on a 1–5 Likert scale, and (3) a post-hoc explanation/evidence quoting sentences from the output, intended to aid human scrutiny and error detection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Ternary instance evaluation (Conclusion + Likert score + Explanation)",
            "evaluation_method_description": "For each criterion c_i and instance (X,Y), LLM outputs: a conclusion about whether Y satisfies c_i, a numeric score 1–5, and an explanation/evidence section that quotes or references specific text to justify the judgment. Explanations are produced post-judgment to reflect LLM reasoning shaped by its conclusion.",
            "evaluation_criteria": "Same per-task criteria from checklist. Each criterion is judged by the ternary output; explanations must quote supporting sentences from the candidate output.",
            "model_name": "gpt-3.5-turbo-0301 (ChatGPT) used for generating instance evaluations",
            "model_size": null,
            "scientific_domain": "NLG/explanatory QA/story generation",
            "theory_type": "Evaluative assessments of explanations and generated content",
            "human_comparison": true,
            "evaluation_results": "LLM-generated ternary evaluations were accepted at high rates by humans; humans revised ~20–29% of LLM-proposed numeric scores (29.34% revision for ELI5), and corrected ill-founded evidentiary claims. Use of explanatory evidence helped humans detect hallucinated or unclear judgments.",
            "automated_vs_human_evaluation": "Hybrid: automated LLM judgments augmented with human post-hoc corrections; explanations are diagnostic tools for humans to accept or revise LLM outputs.",
            "validation_method": "Human scrutiny of LLM outputs with logging of approve/delete/revise/add actions; inter-annotator agreement measured on final scores; comparison to HUMANEVAL majority vote.",
            "limitations_challenges": "LLM explanations can include incorrect evidence or miss subtle failures; score extraction from LLM output required normalization heuristics (regex stripping), and explanations depend on LLM prompt and may vary across runs.",
            "benchmark_dataset": "Applied to ELI5, ROCStories, Self-Instruct evaluation instances (5–7 criteria per instance).",
            "uuid": "e4475.2"
        },
        {
            "name_short": "Human scrutiny actions (A)",
            "name_full": "Atomic human scrutiny actions: approve / delete / revise / add",
            "brief_description": "A minimal set of four actions available to human evaluators to refine LLM-generated criteria and instance evaluations, used to operationalize human corrections with low effort.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Approve/Delete/Revise/Add human action taxonomy",
            "evaluation_method_description": "For each proposed criterion or LLM evaluation item, a human chooses exactly one action: approve (accept as-is), delete (remove unrealistic/irrelevant item), revise (modify wording or scope), or add (introduce a missing but necessary criterion or comment). Applied both at task criteria stage and instance evaluation stage.",
            "evaluation_criteria": "N/A (this is the action set to manipulate criteria/evaluations)",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Human-in-the-loop evaluation methodology (NLP)",
            "theory_type": "Human-AI collaborative correction framework",
            "human_comparison": true,
            "evaluation_results": "Action usage quantified: &gt;70% of final criteria originated from LLM but required human approval; r_a (action rate) computed per-action to summarize human reactions; humans revised ~20% of instance-level LLM scores.",
            "automated_vs_human_evaluation": "Human-driven corrections applied to automated LLM proposals — operationalizes hybrid evaluation.",
            "validation_method": "Computed action rates r_a for approve/delete/revise/add across criteria and instances; used majority vote among annotators to finalize criteria.",
            "limitations_challenges": "Single-action constraint per criterion may oversimplify nuanced disagreements; human actions can introduce subjectivity; requires qualified annotators and a qualification pipeline.",
            "benchmark_dataset": "Used across the three tasks (ELI5, ROCStories, Self-Instruct) in the experiments.",
            "uuid": "e4475.3"
        },
        {
            "name_short": "Action rate (r_a)",
            "name_full": "Human action rate metric (r_a)",
            "brief_description": "A metric quantifying how human evaluators responded to each LLM-proposed criterion by counting occurrences of each atomic action (approve/delete/revise/add) and reporting percentages – used as a proxy for LLM criterion quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Action rate (r_a) for human responses to LLM-proposed criteria",
            "evaluation_method_description": "For each proposed criterion, r_a* = |a*| / |A| * 100 is computed where |a*| is the number of annotators choosing action a* (approve/delete/revise/add) and |A| is total annotators. The majority action defines final treatment of the candidate criterion.",
            "evaluation_criteria": "Measures human acceptance (approve %) vs deletion/revision/addition percentages for LLM-generated criteria.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Evaluation process audit (NLP evaluation design)",
            "theory_type": "Quality metric for candidate criteria",
            "human_comparison": true,
            "evaluation_results": "Used to show that many LLM-generated criteria are feasible (&gt;70% acceptance) but some are unrealistic (~27% flagged); example: 1st criterion in Table 1 had r_a_apv = 100%, 8th criterion had r_a_add = 62.5%.",
            "automated_vs_human_evaluation": "Human-derived metric summarizing human-LLM interaction rather than an automated metric.",
            "validation_method": "Computed over multiple annotators and datasets; majority vote used to finalize checklists.",
            "limitations_challenges": "r_a is sensitive to annotator pool composition and subjective judgments; single-number summaries can mask nuanced disagreement.",
            "benchmark_dataset": "Computed for the candidate criteria generated for ELI5, ROCStories, and Self-Instruct.",
            "uuid": "e4475.4"
        },
        {
            "name_short": "Krippendorff's α",
            "name_full": "Krippendorff's alpha (inter-annotator agreement)",
            "brief_description": "A standard statistic used to quantify agreement among multiple annotators (or annotator groups), applied here to compare consistency between ChatGPT and human evaluators on 5-point score annotations.",
            "citation_title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
            "mention_or_use": "use",
            "evaluation_method_name": "Krippendorff's α inter-annotator agreement",
            "evaluation_method_description": "Compute Krippendorff's α on the 5-point Likert scores across annotators (and between ChatGPT and human groups) to measure annotation consistency; used to report agreement levels (e.g., α &gt; 0.7 as high agreement).",
            "evaluation_criteria": "Agreement on numeric 1–5 scores and sometimes conclusions across evaluators.",
            "model_name": "gpt-3.5-turbo-0301 (as an annotator)",
            "model_size": null,
            "scientific_domain": "Annotation reliability measurement in NLG evaluation",
            "theory_type": "Reliability metric for judgments about explanations/generated text",
            "human_comparison": true,
            "evaluation_results": "Reported Krippendorff's α improvements: COEVAL increased α from 0.64 to 0.71; specific groups of humans had α &gt; 0.8 internally; Group 1's agreement with ChatGPT ≈ 0.754 vs Group 2 ≈ 0.487 in one analysis.",
            "automated_vs_human_evaluation": "Metric applied to both automated (LLM) and human annotators to quantify consistency/harmony between them.",
            "validation_method": "Direct computation on annotation data collected in experiments; used as an evaluation of improved consistency under COEVAL.",
            "limitations_challenges": "α is sensitive to variance across annotators and number of categories; improvements may reflect reduction of outliers rather than better absolute correctness.",
            "benchmark_dataset": "Computed over collected annotations for ELI5 (and presented for other tasks in appendices).",
            "uuid": "e4475.5"
        },
        {
            "name_short": "Pearson correlation",
            "name_full": "Average Pearson correlation among evaluator pairs",
            "brief_description": "Pairwise Pearson correlation coefficient computed between evaluators' numeric scores to quantify similarity of scoring behavior (LLM vs humans; COEVAL vs HUMANEVAL).",
            "citation_title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
            "mention_or_use": "use",
            "evaluation_method_name": "Average Pearson correlation among evaluator pairs",
            "evaluation_method_description": "Compute Pearson correlation coefficients for score vectors between pairs of evaluators (e.g., LLM vs human, COEVAL vs HUMANEVAL) and average them to summarize alignment.",
            "evaluation_criteria": "Numeric 1–5 Likert scores across instances and criteria.",
            "model_name": "gpt-3.5-turbo-0301 used as one evaluator",
            "model_size": null,
            "scientific_domain": "Statistical alignment measurement in NLG evaluation",
            "theory_type": "Comparative scoring correlation",
            "human_comparison": true,
            "evaluation_results": "Reported that LLM scores had relatively weak correlation with humans on ELI5 and ROCStories (specific aggregated values in Table 8); COEVAL vs HUMANEVAL correlation comparable to internal HUMANEVAL correlation, indicating LLM does not strongly bias human judgments.",
            "automated_vs_human_evaluation": "Statistical comparison metric applied post-hoc to automated and human judgments.",
            "validation_method": "Computed over experiment annotation matrices; compared correlations across evaluator groups and settings.",
            "limitations_challenges": "Pearson correlation summarizes linear association but can mask systematic biases (e.g., LLM's more positive scoring distribution).",
            "uuid": "e4475.6"
        },
        {
            "name_short": "Outlier reduction",
            "name_full": "Outlier human-evaluation reduction (via LLM-assisted pipeline)",
            "brief_description": "A measured reduction in extreme or inconsistent human judgments when LLM-produced evaluations are used as references in COEVAL, quantified as percent decrease in outlier human evaluations.",
            "citation_title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
            "mention_or_use": "use",
            "evaluation_method_name": "Outlier reduction measurement",
            "evaluation_method_description": "Identify outlier human evaluations in a conventional HUMANEVAL setting and measure how many of those outliers are removed (or agree with majority) when humans work within COEVAL (i.e., with LLM initial judgments). Report percent reduction.",
            "evaluation_criteria": "Frequency/count of human evaluations classified as statistical or judgmental outliers relative to majority/HUMANEVAL consensus.",
            "model_name": "gpt-3.5-turbo-0301 (used to produce initial references)",
            "model_size": null,
            "scientific_domain": "Evaluation consistency analysis for NLG",
            "theory_type": "Human-LLM collaborative effect on inter-annotator variance",
            "human_comparison": true,
            "evaluation_results": "Using LLM evaluations reduced outlier human evaluations by approximately 62% while preserving diverse human preferences.",
            "automated_vs_human_evaluation": "Hybrid: uses LLM outputs as anchors/reference that humans can accept or correct; measured impact on human outliers.",
            "validation_method": "Comparison of outlier counts between HUMANEVAL and COEVAL; majority-vote HUMANEVAL used as ground truth for some analyses.",
            "limitations_challenges": "Reduction in outliers may reflect conformity to LLM rather than true error correction; must ensure LLM judgments are correct to avoid removing valid minority opinions.",
            "uuid": "e4475.7"
        },
        {
            "name_short": "Time-per-sample efficiency",
            "name_full": "Evaluation time per sample (COEVAL vs HUMANEVAL)",
            "brief_description": "Empirical measurement of human time (minutes per sample) required to produce final evaluations under COEVAL vs conventional human-only evaluation, used to quantify efficiency gains from LLM assistance.",
            "citation_title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
            "mention_or_use": "use",
            "evaluation_method_name": "Per-sample human time measurement",
            "evaluation_method_description": "Record and compare the average human annotation time to evaluate one instance under COEVAL (LLM-first, then human-scrutiny) and HUMANEVAL (human-only full evaluation). Report minutes per sample per task and analyze per-criterion time consumption.",
            "evaluation_criteria": "Avg. annotation time (seconds or minutes) per instance and per criterion.",
            "model_name": "gpt-3.5-turbo-0301 used to pre-fill evaluations in COEVAL",
            "model_size": null,
            "scientific_domain": "Evaluation workload measurement in NLG",
            "theory_type": "Operational efficiency metric",
            "human_comparison": true,
            "evaluation_results": "COEVAL is ~50% faster than HUMANEVAL: ROCStories 5.34 vs 8.70 min/sample; Self-Instruct 4.41 vs 9.90; ELI5 4.13 vs 8.25. Time per criterion varied by task and criterion difficulty (see Appendix A.6/Figure 8).",
            "automated_vs_human_evaluation": "Hybrid measurement: LLM reduces human time by pre-computing candidate judgments; humans then spend less time per item.",
            "validation_method": "Direct timing logs from the annotation platform, with periodic checks for consistency; qualification tests to ensure annotator quality.",
            "limitations_challenges": "Annotators slow down mid-session affecting time measures; overhead for qualification and communication; savings depend on quality of LLM outputs and per-criterion difficulty.",
            "uuid": "e4475.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "rating": 2,
            "sanitized_title": "is_chatgpt_a_good_nlg_evaluator_a_preliminary_study"
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Evaluating large language models at evaluating instruction following",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_at_evaluating_instruction_following"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 1,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Evaluating nlg evaluation metrics: A measurement theory perspective",
            "rating": 1,
            "sanitized_title": "evaluating_nlg_evaluation_metrics_a_measurement_theory_perspective"
        }
    ],
    "cost": 0.017651999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation
30 Oct 2023</p>
<p>Qintong Li qtli@cs.hku.hk 
The University of Hong Kong ♣ Tencent AI lab</p>
<p>Cui ♣ Leyang leyangcui@tencent.com 
The University of Hong Kong ♣ Tencent AI lab</p>
<p>Lingpeng Kong 
The University of Hong Kong ♣ Tencent AI lab</p>
<p>Wei Bi 
The University of Hong Kong ♣ Tencent AI lab</p>
<p>Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation
30 Oct 2023F514047439F253FB24CCA1EAC5BA2571arXiv:2310.19740v1[cs.CL]
Humans are widely involved in the evaluation of open-ended natural language generation tasks (NLG) that demand creativity, as automatic metrics often exhibit weak correlations with human judgments.Large language models (LLMs) recently have emerged as a scalable and cost-effective alternative to human evaluations.However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements.To explore the synergy between humans and LLMbased evaluators and address the challenges of existing inconsistent evaluation criteria in openended NLG tasks, we propose a Collaborative Evaluation pipeline COEVAL, involving the design of a checklist of task-specific criteria and the detailed evaluation of texts, in which LLM generates initial ideation, and then humans engage in scrutiny.We conducted a series of experiments to investigate the mutual effects between LLMs and humans in COEVAL.Results show that, by utilizing LLMs, COEVAL effectively evaluates lengthy texts, saving significant time and reducing human evaluation outliers.Human scrutiny still plays a role, revising around 20% of LLM evaluation scores for ultimate reliability.</p>
<p>Introduction</p>
<p>Existing open-ended natural language generation (NLG) tasks aim to generate creative texts given different task requirements.For example, story generation, such as ROCStories (Mostafazadeh et al., 2016), aims to generate a five-sentence story about a specified topic, while long form question answering, such as ELI5 (Fan et al., 2019), prioritizes providing a simplified answer, akin to explaining it to a 5-year-old child.The adequate evaluation with human judgment, the community has been actively developing automatic metrics.Traditional metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) primarily quantify lexical overlap but exhibit limited correlation with human judgments for open-ended generation tasks.Alternative metrics using pre-trained language models have emerged, such as MoverScore (Zhao et al., 2019) and BERTscore (Zhang et al., 2020), capturing deep semantics for evaluation.As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for evaluating diverse open-ended NLG tasks (Wang et al., 2023a;Liu et al., 2023b).</p>
<p>Despite its growing popularity, certain shortcomings still persist (Liu et al., 2023c;Zheng et al., 2023;Wang et al., 2023b).In particular, LLM evaluators may not be aligned with humans when understanding evaluation criteria centered around information screening, thus causing misleading evaluations that contradict specific task requirements.</p>
<p>As can be seen from the analysis above, both humans and LLM evaluators have limitations when assessing open-ended NLG tasks.To discard the dross and select the essence of them, we propose a Collaborative Evaluation pipeline COEVAL, depicted in Figure 1.COEVAL adopts a two-stage approach.It first generates a checklist of task-specific criteria and subsequently conducts instance evaluation.Both stages involve the collaboration between LLM and humans.The LLM is employed as an assistant tool for providing diverse criteria and informative evaluations as preliminary references, and then human evaluators scrutinize and make necessary corrections to the outcomes of LLMs, ensuring reliable evaluation.</p>
<p>We demonstrate the usefulness and generality of COEVAL via instantiation on three open-ended NLG tasks.In Stage 1, we find that LLMs can provide diverse high-quality task-specific evaluation criteria, with over 70% of the final criteria originating from LLMs.However, approximately 27% are still deemed to be "unrealistic", highlighting the importance of human involvement.In Stage 2, most LLM evaluations are acceptable, particularly for criteria such as relevance, achieving around 48% time savings compared to human evaluations without LLM assistants.Additionally, through the use of the LLM evaluations, outlier human evaluations that would typically appear in conventional human evaluation processes can be reduced by 62% while still preserving enough individual human preferences.However, human scrutiny remains crucial, especially for certain complex criteria, such as engagement and clarity.Around 20% of the evaluation scores of LLMs are revised through human scrutiny.With the collaborative design of COEVAL, the evaluation agreement of Krippendorff's α notably improves from 0.64 to 0.71.We release our resources at https://github.com/qtli/CoEval.</p>
<p>Related Work</p>
<p>Traditional automatic metrics are well established for judging specific NLP tasks, such as BLEU (Papineni et al., 2002) for machine translation, ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) for text summarization, and CIDEr (Vedantam et al., 2015) for image captioning.To improve the correlation with human judgments, several approaches integrate contextual word embeddings, including MoverScore (Zhao et al., 2019), Sentence Mover's Similarity (Clark et al., 2019), BERTscore (Zhang et al., 2020), and Bartscore (Yuan et al., 2021).Other related works propose task-specific metrics to align specific human assessments, e.g., consistency (Durmus et al., 2020;Honovich et al., 2021;Fabbri et al., 2022), coherence (Durmus et al., 2020;Ye et al., 2021), and grammar (Pratapa et al., 2021).However, no universal metric exists that can accommodate all generation tasks and capture all desirable properties of language (Reiter and Belz, 2009;Garbacea and Mei, 2020).Human evaluation is prevalent in generation tasks (Mathur et al., 2020;Belz et al., 2020;Liu et al., 2023a).</p>
<p>As research in LLMs continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations (Jain et al., 2023;Taori et al., 2023;Chiang et al., 2023).Fu et al. (2023) use LLM's predicted text probability as the automated score, assuming access to the LLM's logits.Along a more blackbox line, the community has turned to induce LLM to directly generate evaluation scores (Wang et al., 2023a;Chiang and Lee, 2023;Liu et al., 2023c,b;Zeng et al., 2023;Zhang et al., 2023) with superior human correlation compared to conventional methods.However, LLM performance may be negatively affected in certain contexts, such as suboptimal prompt formatting (Zhu et al., 2023).Furthermore, there is a lack of standardization in the establishment of evaluation criteria, which are pri-marily determined by domain experts (Xiao et al., 2023).Inspired by recent work in human-machine collaboration (Wu et al., 2022;Wiegreffe et al., 2022;Saunders et al., 2022;Ribeiro and Lundberg, 2022;Rastogi et al., 2023), we propose a collaborative evaluation pipeline where LLM provides preliminary but informative criteria or evaluations, and then humans scrutinize and revise them with minimal effort to ensure their comprehension and trustworthiness.</p>
<p>COEVAL</p>
<p>Design Principles Given the extensive world knowledge encapsulated in LLMs and the expertise of human evaluation experts, COEVAL covers the evaluation process in an LLM-ideation-Human-scrutiny manner, beginning with the creation of evaluation criteria (Stage 1: task criteria establishment, see §3.1) and extending to the generation of evaluation, i.e., conclusion, score, and explanation (Stage 2: instance evaluation with explanation, see §3.2).Both stages employ a collaborative paradigm in which an LLM provides preliminary but informative criteria or evaluation (LLM ideation), then COEVAL uses human ingenuity to refine the outcomes of LLM (human scrutiny).During the collaboration, human evaluators have four scrutiny actions, denoted by A = (a apv , a del , a revise , a add ) to refine LLM's criteria C M and evaluation E M .The four actions allow evaluators to either approve, delete, revise, or add to E M , leading to an appropriate criteria C and accurate evaluation E. A working example of COEVAL is shown in Figure 1.</p>
<p>CoEval can utilize any LLM in theory.In this paper, we employ ChatGPT (gpt-3.5-turbo-0301) in CoEval.The sampling temperature is set to 0 for deterministic results.We detail the human evaluation protocol and strict quality control in Section 3.3 and Section 3.4.</p>
<p>Formulation To illustrate the task criteria establishment stage, suppose we aim to evaluate models in the context of an NLG task, which typically comprises three components: (1) task description T desc , (2) task input field T ipt , and (3) task output field T opt .These components play vital roles in determining the appropriate evaluation criteria C = {c 1 , . . ., c n }.For the instance evaluation stage, we assume there is a test instance (X, Y ) of the given task, where X represents the instance input and Y is the corresponding output generated Task: ELI5 (Fan et al., 2019) • Task Description T desc : Provide an answer to a Reddit forum question that is comprehensible by five-year-olds.either by a model or by a human.COEVAL is designed to generate a set of ternary form evaluation results, denoted as E = {E 1 , . . ., E n }, where E i represents the evaluation results for criteria c i .E i includes an evaluation conclusion E c i , an evaluation score E s i on a five-point Likert scale, and explanation E e i that supports E c i and E s i .Note that the evaluation criteria C and evaluation results E are first generated by LLM, and subsequently scrutinized and corrected by human evaluators.</p>
<p>Evaluation Criteria Establishment</p>
<p>LLM Ideation To generate a set of criteria for a given task, we first present the LLM with the task information (T desc , T ipt , T opt ).Given a customized instruction, an LLM is required to list the possible evaluation criteria C M = LLM(T desc , T ipt , T opt ), such as the first six criteria presented in Table 1.The instruction to the LLM is, "The task is
[T desc ]. The input is [T ipt ]. What capabilities of the [T opt ]
should there be?List directly."</p>
<p>Human Scrutiny We guide human evaluators in what to assess, by providing the task context (T desc , T ipt , T opt ) and candidate criteria list by LLM.To break down potential failures into specific types, COEVAL iterates each criterion c i ∈ C M and asks evaluators to check c i according to the task definition and feasibility of realistic evaluation.CO-EVAL provides evaluators with four actions to refine LLM's criteria C M :</p>
<ol>
<li>a apv : Approve a criterion that is easily implementable in real-world scenarios and aligns well with the task context, e.g., 1st and 2nd criteria in Table 1. 2. a del : Delete a criterion that is unrealistic to apply during instance-level evaluation and goes against the task context, e.g., the 3th-5th criteria in Table 1. 3. a revise : Revise a criterion for better alignment with the task context, e.g., the 6th criterion in Table 1. 4. a add : Add a criterion that is necessary for the evaluation of the given task context, e.g., the 7th and 8th criteria in Table 1.Each criterion is associated with only one possible action.We introduce the action rate r a of human evaluators as the quality estimation of LLMgenerated criteria C M .For all criteria, we could calculate:
r a * = |a * | |A| i |a i | * 100, where |a * | repre-
sents the number of occurrences of action a * and each value of a * is either 0 or 1.A indicates all types of actions.</li>
</ol>
<p>Human evaluators scrutinize and correct C M to produce the final criteria C, which are expected to be sufficient and realistic, and will be used in the instance-level evaluation stage.The task criteria establishment stage involved multiple NLP researchers, including authors.The final version of the evaluation criteria is achieved through a majority vote.Concretely, each criterion c i is executed by selecting the action with the highest action rate.For instance, the 1-st criterion in Table 1 has an action rate (r aapv ) of 100% and the 8-th criterion is assigned an additional action rate (r a add ) of 62.5%.</p>
<p>Instance Evaluation with Explanation</p>
<p>LLM Ideation Given an instance (X, Y ) to be evaluated, LLM is instructed to produce initial evaluation E M based on task context (T desc , T ipt , T opt ).</p>
<p>Notably, relying solely on score predictions is inadequate for reliable evaluation and error detection, as certain criteria require nuanced analysis beyond simple scoring.Building upon recent advancements in explainable NLP that demonstrate the promising performance of LLMs in producing Task Information: Omited, see Table 1 Instance Information:</p>
<p>• Input (Reddit question) X: How is perfume created?• Output (Answer) Y : Smelly thinks in flowers and herbs can be extracted with alcohol.Then they can be condensed, then put in a bottle, then sprayed on girls and boys alike.Criterion ci: Use simple and easy-to-understand language.</p>
<p>Evaluation Ei:</p>
<p>• The answer does not satisfy the criterion as it uses words like "condensed" which may not be familiar to a five-yearold.[✓]</p>
<p>• Score: 4. While the answer mostly uses simple language, there are a few phrases that may be difficult for a child to understand.[✂] Score: 3
• Evidence: [✓]
-The use of the words "condensed" and "smelly thinks in flowers and herbs."[✂, wrong evidence] -The use of the words "alcohol" and "condensed".</p>
<p>-However, the answer does use simple language to explain that perfume is made by extracting scents from flowers and herbs and then putting the condensed scents in a bottle to be sprayed on people.[✓] Table 2: Illustration of the collaborative evaluation stage for the ELI5 task (Fan et al., 2019).The information in the first row is provided during the evaluation process, and the text highlighted in yellow represents the final evaluation that has been revised by human annotators.</p>
<p>explanations to explain their predictions (Wiegreffe and Marasović, 2021;Jung et al., 2022;Yin and Neubig, 2022;Wiegreffe et al., 2022;Ye et al., 2022), the LLM is also tasked with generating explanations to clarify evaluation outcomes, ensuring a trustworthy collaborative evaluation process.</p>
<p>For each evaluation criterion, COEVAL asks LLM to produce an evaluation E M with detailed granularities, i.e., (1) a clear conclusion, (2) a scalar score, and (3) explanations to support the conclusion and score.The prompt to LLMs is:</p>
<p>The task is
[T desc ]. You need to evaluate a can- didate [T opt ] based on a criterion. Criterion: [c] [T ipt ]: [X] [T opt ]: [Y ]
Evaluation Steps:</p>
<ol>
<li>Evaluate whether this [T opt ] satisfies the criterion.Give a conclusion.2. Assign a score for this [T opt ] on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the criterion.3. List evidence by quoting sentences of the [T opt ] to support your conclusion.We adopt post-explanation since evaluation judgment explanations are holistic and meaningful only after the judgment is made (Lampinen et al., 2022;Chen et al., 2023).Besides, post-explanation is shaped by judgment and can serve as a lens to assess the higher-order reasoning of LLMs.</li>
</ol>
<p>Human Scrutiny Upon receiving the evaluation results E M generated by LLM, human evaluators have access to four distinct actions (a apv , a del , a revise , a add ) to scrutinize the reliability and consistency of E M with respect to the information available for the given task and instance.An example of the collaborative evaluation between LLM and humans is presented in table 2. The acceptability of LLM evaluation quality can be inferred from the rates of different human action types, as described in Section 3.1.</p>
<p>Human Evaluation Protocol</p>
<p>Human evaluators in COEVAL engage in a two-step scrutiny process.The first step involves verifying the coverage and reliability of the evaluation criteria generated by LLMs for a given task.In the second step, evaluators provide further scrutiny of the initial LLM-produced evaluation to ensure its accuracy.The first step operates at the task level, focusing on generating criteria and acting as a prerequisite for the subsequent step.The second step evaluates instances based on the criteria established in the first step.COEVAL was conducted on our custom-built evaluation platform.See Appendix C for the evaluation guidelines and Appendix D for screenshots of the evaluation interface.</p>
<p>Quality Control</p>
<p>Evaluators must complete a qualifying exam before participating in the COEVAL: (1) They are first pre-screened with a qualification study, which involves reading an evaluation guideline and evaluating three instances from three datasets.(2) We individually review the submitted evaluations from the qualification study and provide feedback to clarify any misconceptions about the task.(3) Evaluators who performed well on the qualification study and demonstrated a thorough understanding of the evaluation guidelines are selected to participate in the human scrutiny of COEVAL.(4) Throughout the whole process, we maintain constant communication with evaluators to answer any questions.Ultimately, we selected 15 native speakers (5 evaluators per task) from North America to participate in COEVAL.More details about the conduction of human evaluation are in Appendix B.</p>
<p>Source and Example Queries</p>
<p>Source: ELI5 Query: Answer the Reddit forum question in a way that is comprehensible by five-year-olds: Why does it seem like I do not dream?Or when I do, I easily forget them?Source: ROCStories Query: Write a five-sentence story about an everyday topic "pizza night": Source: Self-Instruct Query: Come up with healthy and easy dinner ideas for weeknights.\nOutput:</p>
<p>Evaluation Setup Datasets</p>
<p>We consider three open-ended generation tasks as evaluation objects.(1) ROCStories (Mostafazadeh et al., 2016): generating a brief story from a provided beginning sentence or a title.(2) Self-Instruct (Wang et al., 2022): a useroriented instruction-following dataset focusing on various application scenarios (e.g., email writing and movie analysis).( 3) ELI5 (Fan et al., 2019): producing explanatory answers for questions that are intelligible to five-year-olds.The three chosen datasets exhibit different task features and requirements, which further contribute to the diversity of the evaluation scenarios.ROCStories focuses on coherence and fluency, Self-Instruct places greater emphasis on completeness and correctness, while ELI5 emphasizes factuality and comprehensibility.</p>
<p>Models</p>
<p>We evaluate three off-the-shelf commercial generation models (gpt-3.5-turbo-0301,text-davinci-002, and text-curie-001) from OpenAI API, as well as human-written ground truth, for their effectiveness in completing the aforementioned generation tasks.</p>
<p>Evaluation Instances We randomly sample 50 instances from each task and generate answers using each generation model accordingly, resulting in 200 instances for each task, where ELI5, ROC-Stories, and Self-Instruct tasks have 5, 7, and 5 evaluation criteria per instance, respectively.Each instance is evaluated by 5 evaluators, and a total of 1,000 human instance-level evaluations are collected for each task.We pass these queries to different generative models using the official Open API, with sampling temperature set to 0.7, frequency penalty set to 0, and presence penalty set to 1. See</p>
<p>Potential of LLM-Generated Criteria</p>
<p>Human evaluation methodologies of research papers in NLG differ drastically across papers (Belz et al., 2020).In Table 4, we compare the evaluation criteria generated by COEVAL to those of previous works for each task.The criterion set employed by COEVAL is more comprehensive, with broad coverage, and tailored to the specific task, e.g., considering "simple language" when evaluating answers explained to a 5-year-old child (ELI5).A total of 7 NLP researchers and 15 laypeople participated in the criterion scrutiny.The laypeople were hired through a qualifying exam mentioned in Section 3.4 and Appendix C).NLP researchers, due to their familiarity with the evaluated tasks, can be considered experts in evaluating criteria for providing valuable insights into the suitability of the criteria.Including both researchers and laypeople in this stage ensures that the evaluation considers both the scientific theories of the NLP tasks and the preferences of normal users.</p>
<p>LLM-crafted criteria exhibit high feasibility but medium precision.As displayed in Table 5, the approval rates are high across all domains and different annotation groups.Specifically, for the Self-Instruct tasks, researchers (laypeople) agree with over 60% (80%) of LLM's criteria, suggesting the reasonability of LLM-generated criteria.The deletion rates are also notable, especially among researchers, as LLM criteria may overlook operational challenges (e.g., "originality" for ROC-Stories) or be inconsistent with task requirements  (e.g., "tone imitation" for Self-Instruct).See Appendix A.2 for more details.The existence of added actions implies that certain conventional criteria are ignored by LLMs, such as "factuality".In general, the initial checklist of criteria proposed by LLMs assists humans in determining what to evaluate.</p>
<p>LLM consistency across instances.Upon comparing the criteria established by LLM across various instances (10 iterations with a temperature of 0.7), we observe a general consensus despite differences in phrasing.With respect to the ELI5 task, a total of 7 distinct perspectives are identified in the sampled criteria, of which 5 receive support exceeding 60% from all 10 iterations.Similar trends are observed in ROCStories (with 6/8 unique criteria receiving over 75% support) and Self-Instruct tasks (with 8/11 criteria receiving over 50% support).</p>
<p>Potential of LLM-Generated Evaluation</p>
<p>Most of the LLM-generated evaluations are acceptable.The preferences of human evaluators for the evaluation outcomes of LLM are presented in Table 6.When aggregating the preferences of  We present the correction rates of human annotators on LLM evaluation results, along with a finegrained analysis of human preferences regarding the components of the evaluation results, namely conclusion, score, and evidence.</p>
<p>Figure 2: The distribution of scores assigned by LLM and 5 human evaluators for predictions generated by various language models.To ensure generalizability, human evaluators who participated in different datasets may vary.</p>
<p>all human evaluators of COEVAL, we observe that the approve-action ratios are consistently high for both the overall evaluations across all datasets, indicating the huge potential of LLM in evaluation.</p>
<p>It is worth noting that 29.34% of LLM-generated scores are revised by human evaluators for ELI5.This demonstrates that human involvement is essential in identifying issues overlooked by LLMs.Difference between LLM-based scoring and human judgments across various criteria.The distribution of scores assigned by both LLM and human evaluators is presented in Figure 2. To facilitate comparison, we select two representative criteria for each dataset.For the criteria on the left side of the three tasks, the difference in scoring distribution between LLM and humans is negligible, demonstrating the effectiveness of LLM evaluation on these criteria.However, for criteria on the right side, which pertain to information-seeking and numerical capabilities, there is a significant difference.The difference is apparently large, indicating LLM evaluation still may fail on those complex evaluation criteria.</p>
<p>Consistency of annotations between LLMs and human evaluators.Figure 3 reports interannotator agreement (IAA) using Krippendorff's α for evaluation scores (on a 5-point Likert scale) to showcase the annotation consistency among Chat-GPT and 5 evaluators.Our findings reveal high agreement among evaluators, with α exceeding 0.8 for two groups of evaluators, i.e., (Group 1: A3, A4, and A5) and (Group 2: A1 and A2), indicating inherent variance in the definitions of "best text snippets" among different evaluators.Notably, Group 1 (with α ≈ 0.754) is more consistent with ChatGPT's evaluation scores compared to Group 2 (with α ≈ 0.487), underscoring the importance of involving humans with diverse backgrounds to</p>
<p>Comparison between COEVAL and Conventional Human Evaluation</p>
<p>Following the conventional human evaluation settings of open-ended generation tasks (Wiegreffe et al., 2022;Li et al., 2023;Liang et al., 2023), we conduct an additional human evaluation as HU-MANEVAL to compare the performance of CO-EVAL.In HUMANEVAL, we engaged three evaluators recruited based on their demonstrated proficiency.They were tasked to evaluate 60 instances drawn from three datasets, providing comprehensive assessments that encompassed conclusions, scores, and detailed explanations.</p>
<p>COEVAL is more time-saving than HU-MANEVAL.We compare the time required per sample for both COEVAL and HUMANEVAL across three datasets, as presented in Table 7.The results prove that COEVAL is significantly more efficient, using approximately half the time of HUMANEVAL which highlights the effectiveness achieved by employing collaborative evaluation.In addition, we also record the time consumption for evaluation across different criteria.Details are shown in Appendix A.6.These results demonstrate the enormous potential of collaborative evaluation in the field of NLG evaluation.</p>
<p>LLM evaluations do not inhibit human judgment.unreliable of solely LLM evaluation.With human participation, the correlation between CoEval and HumanEval is comparable to the internal correlation within HumanEval.The correlation value between COEVAL and HUMANEVAL exhibits a slightly lower value compared to the correlation between LLM and HUMANEVAL.One plausible explanation for this observation is the relatively modest inter-correlation among the human evaluators (0.23), while COEVAL still retains a degree of human subjectivity.Overall, the results suggest that humans are not significantly influenced by the prior biases of the LLM.</p>
<p>Reasons behind the relative high inter-annotator agreement of CoEval.Given the improved interannotator agreement of COEVAL, we carefully investigate the factors that improve human agreements.The results are shown in Table 9.We choose the majority vote of HUMANEVAL as ground truth to analyze the behavior of humans involved in CO-EVAL.The elevated values observed in the 'Correction", "Scrutiny", and 'Subjectivity" categories suggest that humans tend to follow their own preferences in most cases.This is evident from their endeavors to revise LLM's evaluations that contradict their own judgments (55.32% on ELI5) without blindly relying on LLM (61.90% on Self-Instruct).Surprisingly, the notable values in the"Outlier" indicate that humans are willing to agree with LLM when it is justified and reasonable.This willingness to align with LLM evaluations leads to higher annotator agreements and removes the outliers existing in HUMANEVAL.</p>
<p>We propose an LLM-ideation-human-scrutiny pipeline to explore the synergy of LLMs and humans in establishing evaluation criteria and conducting multi-dimensional evaluations for openended NLG tasks.We find that LLM's criteria are generally comprehensive but tend to exaggerate unnecessary criteria without considering realistic conditions.Humans can identify errors made by LLMs during the evaluation process, such as skewed positive scoring and hallucination in criteria that require numerical reasoning and information seeking, among others.We hope our results and insights can motivate the development of trustworthy evaluation methods without excessive human effort.</p>
<p>Limitations</p>
<p>In this paper, we adopt ChatGPT as an LLM evaluator to show the effectiveness of COEVAL due to its balanced cost-effectiveness and performance compared to other commercial models, such as GPT-4 (OpenAI, 2023), and non-commercial models, such as LLAMA-2-Chat-70B (Touvron et al., 2023).Another reason why we have not included more LLMs to analyze the evaluation performance among different models is due to the high cost of human scrutiny.Hiring a qualified evaluator to evaluate 200 instances costs us US$700.It takes us over three weeks to recruit evaluators, conduct qualification tests, and collect human evaluation results, whereas it only takes a few hours to train ChatGPT to perform evaluations.</p>
<p>The evaluation results of LLMs may be sensitive to the instruction formats that are used to query the model.Although it is challenging to find a globally optimal evaluation instruction, we conduct pilot experiments and find that the overall results among different instructions are not significantly different on a small subset of instances.The evaluation results of LLMs can also be influenced by their decoding strategies.In our evaluation process, we set the sampling temperature to 0 for deterministic evaluation results.We also experiment with a temperature of 0.7 and sample the evaluation results 10 times to assess any impact on the overall results( §5).We observe that this variation did not have a significant effect for COEVAL.</p>
<p>A Supplementary Experimental Results</p>
<p>A.1 Query Format for Preparing Evaluation Instances</p>
<p>We carefully designed query formats for each dataset to guide models to behave in accordance with the task requirements * .Table 10 displays examples of the evaluated instances across three tasks.</p>
<p>A.2 Derivation of Evaluation Criteria</p>
<p>Table 11 and Table 12 shows the establishment process of evaluation criteria for ROCStories and Self-Instruct task.</p>
<p>A.3 Extraction for LLM's Evaluation Scores</p>
<p>We observe that LLM tends to provide its evaluation scores in various expressions.To extract the scores, we apply three simple rules: (1) We remove string "2." from the output since the evaluation score is behind the evaluation conclusion, LLMs will sometimes say "2.The score . . .". (2) We remove the string "out of 5" and "/5" since LLMs sometimes say "give a score of x out of 5" or "x/5" (3) We use the regular expression to extract the first number in the sequence.</p>
<p>A.4 Evaluation Consistency Between LLM and Humans</p>
<p>Inter-annotator agreements of ROCStories and Self-Instruct tasks are presented in Figure 4 and Figure 5 accordingly.Although the consistencies between evaluator A11 and other evaluators in Figure 5 are not very high, the agreements among other four evalutors are higher than reliable threshold 0.677.</p>
<p>A.5 Evaluation on Data of Varied Qualities</p>
<p>We evaluate sentences from both humans and models with varying quality, expecting that differences between generative sources will be reflected in the evaluation results.The comparison of the scoring patterns of the LLM and human evaluators on the Self-Instruct task is presented in Figure 7.  ChatGPT  ing five to six criteria.We pay them US$35.5 an hour.The local minimum salary in the year 2023 is US$15.5 per hour for part-time jobs.The annotation does not involve any personally sensitive information.Evaluators tend to slow down their evaluation speed in the middle of the evaluation process, which can affect time calculations.To ensure the accuracy of our time calculations and the quality of the annotations, we periodically check the annotator's results every few batches.This helps ensure that the quality of the annotations and the median time taken per annotator are consistent with our pay rate.
A6 A7 A8 A9 A10 ChatGPT</p>
<p>C Annotation Guidelines</p>
<p>Figure 9 and Figure 10 show the evaluation guidelines we used for the whole evaluation pipeline.We ask crowd evaluators to read these guidelines as part of the qualification study.Only evaluators that demonstrated a thorough understanding of the guidelines and task were permitted to participate in the main round of the evaluation procedure.</p>
<p>D Evaluation Platform</p>
<p>We build our platform using Gradio repository † and display the screenshots of evaluation pipeline in Figure 11-13.</p>
<p>Figure 3 :
3
Figure 3: Inter-annotator agreement (ELI5) among LLM and 5 humans (A1 to A5) using Krippendorff's α.LLM's evaluation scores are deemed acceptable, with over 50% of human evaluators showing high agreement (α &gt; 0.7).</p>
<p>Figure  6presents the evaluation score distributions of LLM and humans to distinguish generations from different sources (models and humans) on ELI5 and ROCStories tasks.The LLM tends to assign more positive scores than humans, reflected by the smaller range of low scores.</p>
<p>Figure 4 :
4
Figure 4: Inter-annotator agreement (ROCStories) among ChatGPT and humans using Krippendorff's α.ChatGPT's evaluation scores are deemed acceptable, with over 50% of human evaluators showing high agreement (α &gt; 0.7).</p>
<p>Figure 5 :
5
Figure 5: Inter-annotator agreement (Self-Instruct) among ChatGPT and humans using Krippendorff's α.ChatGPT's evaluation scores are deemed acceptable, with over 50% of human evaluators showing high agreement (α &gt; 0.7).</p>
<p>Figure 6 :Figure 7 :
67
Figure6: The distribution of scores assigned by LLM and human evaluators for generations (ELI5 and ROC-Stories) written by and models with different qualities.The score (1 to 5) ratios across different sources are distinct, suggesting that both LLMs and humans are able to discern these differences.</p>
<p>Figure 8 :
8
Figure 8: Average time consumption (seconds) for conducting evaluation under each criterion across 3 tasks.</p>
<p>Table 1 :
1
• Input Field Tipt: Reddit question • Output Field Topt: Answer Demonstration of the derivation of evaluation criteria for the long-form question-answering task ELI5, achieved through the collaboration of LLM ideation and human evaluator correction (comments in square brackets).✓ signifies approve (a apv ), ✗ indicates delete (a del ), ✂ denotes revise (a revise ), and ✛ represents add (a add ).The ultimate criteria are highlighted in yellow.
Criteria C:1. Use simple and easy-to-understand language. [✓]2. Use examples or analogies that are relatable to a five-year-old's experience. [✓]3. Avoid using technical terms or jargon. [✗ Same as 1st]4. Break down complex ideas into smaller, more manage-able parts. [✗ Same as 1st]5. Use visual aids or illustrations to help explain the an-swer. [✗ Not consider visual modality]6. Be helpful and understand the child's level of compre-hension. [✂ Overall performance] All things considered,answers should be helpful to the person who asked thisquestion.7. Answers should be factually correct and cannot havesubtly incorrect or fabricated information. [✛ factuality]8. Be easy to follow and logically coherent. [✛ coher-ence]</p>
<p>Table 3 :
3
Example queries are drawn from each of the evaluated tasks.These queries are provided to various generation models to generate responses of varying quality for subsequent evaluation.</p>
<p>Table 4 :
4
Table3for example queries from each Previous work employs different evaluation criteria.In contrast, COEVAL adopts the LLM-Ideation-Human-Scrutiny pipeline and considers a more comprehensive criteria set that takes into account the characteristics of the task and has high coverage of previous work.To ensure reliability, humans removed 2, 0, and 3 criteria for the three tasks.Further details about criteria establishment are presented in Table1and Appendix A.2.
ROCStories Story GenerationLogically coherenceRelatedness to inputsGrammarEngagementCharacter developmentClarifyAppropriate lengthWang et al. (2020)✓✓✗✗✗✗✗Guan et al. (2022)✓✓✓✗✗✗✗CoEval (Our)✓✓human-added✓✓✓✓Self-Instruct Instruction FollowingCompletenessRelevance to instructionsFactual correctnessInformationClarityPolitenessWang et al. (2022)✓✗✗✗✗✗Chiang et al. (2023)✓✓✓✓✗✗CoEval (Our)✓✓✓✗✓✓ELI5OverallFactualLogicallySimpleExampleLong-form QAusefulnessaccuracycoherencelanguageusageLi et al. (2020)✓✗✗✗✗Nakano et al. (2021)✓✓✓✗✗CoEval (Our)human-revised human-added human-added✓✓distribution. Additional examples of queries andpredictions from various models can be found inAppendix A.1.</p>
<p>Table 5 :
5
Correction rates of crowd annotators and NLP researchers (labeled as "R") on criteria generated by LLM.The frequent occurrences of 0% indicate most criteria are either accepted or disapproved by humans.</p>
<p>Table 6 :
6</p>
<p>Table 7 :
7
Minutes taken per sample for COEVAL and HU-MANEVAL, with and without the assistance of LLMs.
TaskCOEVAL HUMANEVALROCStories (7 criteria)5.348.70Self-Instruct (5 criteria)4.419.90ELI5 (5 criteria)4.138.25TaskLLM vs. HUE COE vs. HUE HUEELI50.210.310.40ROCStories0.350.430.45Self-Instruct0.370.330.23Table 8: Average Pearson correlation among pairs ofevaluators, i.e., LLM, COEVAL and HUMANEVAL.verify the reliability of LLM's evaluations. Re-sults on the other two datasets are presented inAppendix A.4.</p>
<p>To fairly investigate the effectiveness of COEVAL, we calculated the average Pearson correlation among pairs of evaluators for each dataset in Table 8. Results on ELI5 and ROCStories show that the scores of LLM exhibit a relatively weak correlation with those of humans, indicating the
TaskCorrection Scrutiny Subjectivity OutlierELI542.55% 47 55.32% 4776.47% 1763.64% 22ROCStories 11.43% 35 14.29% 3537.50% 2472.73% 33Self-Instruct 45.45% 11 45.45% 1161.90% 2148.39% 31Table 9: Human evaluator behaviors in COEVAL acrossfour scenarios. Correction: humans in COEVAL ad-just LLM's conflicting evaluations to align with HU-MANEVAL Scrutiny: humans in COEVAL scrutinizeLLM's evaluations inconsistent with HUMANEVAL, de-spite differences. Subjectivity: human evaluators ofCOEVAL show significant disagreements when HU-MANEVAL exhibits large disparities. Outlier: all hu-mans and LLM in COEVAL agree with the majorityvote of HUMANEVAL, despite disagreements in HU-MANEVAL. The subscript number under percentageindicates the corresponding instances of HUMANEVAL.</p>
<p>Table 12 :
12
Demonstration of the derivation of evaluation criteria for the instruction following task Self-Instruct, achieved through the collaborative efforts of LLM ideation and human annotator correction (in square brackets).The ultimate criteria are highlighted in yellow.</p>
<ul>
<li>We adopted the prompt design from https://github. com/bigscience-workshop/promptsource as a reference.
† https://gradio.app/
Ethics StatementWe honor the Code of Ethics.No private data or non-public information is used in this work.For human evaluation, we recruited our evaluators from the linguistics departments of local universities through public advertisement with a specified pay rate.All of our evaluators are senior undergraduate students or graduate students in linguistic majors who took this evaluation as a part-time job.We pay them US$35.5 an hour.The local minimum salary in the year 2023 is US$15.5 per hour for part-time jobs.The annotation does not involve any personally sensitive information.We use the models and datasets in accordance with their intended usage.Specifically, we follow the OpenAI usage policy when using the models of Open AI.Source and Example QueriesSource: ELI5 Query: Answer the Reddit forum question in a way that is comprehensible by five-year-olds: Why does it seem like I do not dream?Or when I do, I easily forget them?Human: keep a dream journal by your bed.every day you wake up write as much down as you can.your brain will get better at it, until you can remember them in greater detail.you can train your brain to do this.gpt-3.5-turbo-0301:Sometimes when we go to sleep, our brain makes up stories in our head.These stories are called dreams.But sometimes we don't remember them when we wake up because our brain forgets them too quickly.It's like when we play with toys and then forget about them the next day.text-davinci-002: There could be a few reasons why it seems like you do not dream, or why you forget your dreams easily.One reason might be that you are not getting enough sleep, and so your brain is not getting enough time to process information and create dreams.Another reason might be that you have a condition called sleep apnea, which means that you stop breathing for short periods of time during the night, which can disrupt your sleep and make it difficult to dream.Finally, some people simply don't remember their dreams because they don't pay attention to them.Dreams can be strange and hard to understand, so it's normal to forget them soon after waking up.text-curie-001: Some people have trouble dreaming because their brain is not used to it.Other people have a hard time remembering their dreams because they are usually boring.Source: ROCStoriesQuery: Write a five-sentence story about an everyday topic "pizza night": Human: Ann and her mom had a girls' night.They watched comedy movies all night.Then they got hungry.They decided to order a pizza.Girls' night became pizza night!gpt-3.5-turbo-0301:It was Friday night, and the family was excited for their weekly tradition of pizza night.They gathered in the kitchen, each placing their order for toppings and sauce.The smell of melting cheese filled the air as the pizzas cooked in the oven.As they sat down to eat, everyone savored the delicious flavors and enjoyed each other's company.Pizza night was always a highlight of their week, bringing them together for a fun and tasty evening.text-davinci-002: Pizza night is a weekly tradition in our house.We always order the same thing: two large pizzas with pepperoni and extra cheese.We never get tired of it.Tonight, we decided to try something different and ordered a pizza with sausage and peppers.It was really good!We might have to switch things up more often.While we were eating, we watched a movie on Netflix.It was the perfect evening.text-curie-001: Tonight was pizza night in the office.Everyone was looking forward to it, especially because it was Tony's turn to make the dough.He was always so excited when it was his turn.He started mixing the flour, yeast, and salt together in a bowl, then added the water and stirred until the dough was smooth.After letting it rise for a few minutes, he transferred it to a greased breadboard and began to shape it into a ball.He put it in the oven and waited for it to come out golden brown.Once it did, he took it out and put it on a cutting board.He then started to add all of the toppings: pepperoni, ham, mushrooms, and onions.He put it back in the oven and let it cook for a few more minutes.When it was finished, he took it out and put it on plates.Everyone enjoyed their pizza, and they were sure that it was thanks to Tony's amazing dough.Source: Self-Instruct Query: Come up with healthy and easy dinner ideas for weeknights.\nOutput:Human: -Chicken &amp; Veggie Stir-Fry -Classic Chicken Noodle Soup -Low-Carb Eggplant Lasagna -Avocado Lime Salmon -Zesty One-Pot Shrimp Pasta -Grilled Filet Mignon Street Tacos -Bruschetta Pasta gpt-3.5-turbo-0301: 1. Grilled chicken breast with roasted vegetables (such as broccoli, carrots, and bell peppers) 2. Quinoa bowl with black beans, avocado, and salsa 3. Baked salmon with asparagus and sweet potato wedges 4. Stir-fry with tofu or shrimp, mixed vegetables, and brown rice 5. Turkey chili with cornbread muffins 6. Grilled portobello mushrooms with roasted garlic and spinach salad 7. Lentil soup with whole grain bread 8. Veggie burger with sweet potato fries 9. Greek yogurt chicken salad with whole wheat pita pockets 10.Zucchini noodles text-davinci-002: 1. Grilled chicken with roasted vegetables 2. Salmon with quinoa and steamed broccoli 3. Turkey burgers with sweet potato fries 4. Shrimp stir-fry with brown rice 5. Vegetable soup with grilled cheese sandwiches 6. Chicken Caesar salad 7. Spaghetti squash with turkey meatballs 8. Grilled salmon with avocado salsa 9. Quinoa chili text-curie-001: -Roasted chicken with vegetables and a side of roasted potatoes -BBQ chicken skewers with a side of roasted sweet potatoes -Shrimp stir-fry with vegetables -Ground beef and vegetable chili -Chicken and black bean enchiladas -Quinoa and black bean bowl with steamed vegetables -Sauteed salmon with asparagus and a side of roasted Brussels sprouts Table 10: Evaluated instances sampled from the evaluated tasks.A.6 Time Cost of Human Evaluation Across Different CriteriaWe computed the time required for human evaluators to perform evaluations on different criteria for the three datasets, as displayed in Figure8.The variance in time cost among the different criteria indicates the varying levels of difficulty in evaluating them and correcting LLM-based evaluations.B Details of Human EvaluationsAnnotator Compensation On average, evaluators spent approximately five minutes on task criteria establishment.We compensate evaluators $2.5 per task.They take around six minutes to complete a single instance evaluation, which involved assess-OverviewHi! We are a team of NLP researchers interested in evalua8ng the quality of open-ended text generated by current AI systems from diverse perspec8ves.In this task, you will use a language model (ChatGPT) to assist in evalua8ng "NLP task instances".We aim to inves8gate whether ChatGPT can completely replace the human evalua8on and rely solely on ChatGPT to assess the reliability of data quality.Please note that we are evalua8ng pure text tasks and do not involve other modali8es such as speech or vision.The input and output of the tasks we evaluate are single-turn, meaning that one input corresponds to one output.Evalua&amp;ng an NLP task o1en requires considering two aspects:• Correctness: This includes evalua8on criteria that are task-independent, such as gramma8cality (the output should not contain gramma8cal errors), seman8c completeness (whether the input requirements are fully expressed), factual accuracy (whether the output contains factual errors or fabricated informa8on), coherence (whether the output has coherent and consistent discourse logic), and so on.• Task-specific characteris2cs: Different tasks have different evalua8on criteria, which need to be set according to the task informa8on.For example, if the task is to provide answers to Reddit ques8ons that can be understood by 5-year-old children, the evalua8on criteria would need to include language expression that is concise and free of obscure terminology.Note:Unless the task explicitly expresses that it is to complete an NLP task, expressions such as "its relevance to the field of NLP" should not appear in the evalua8on criteria.Please carefully read the guidelines below before star8ng on the task.The task compensa8on accounts for the 8me needed to read the guidelines.Figure9: The first page of the evaluation guideline, which is used in the qualification test.At a high level, the tasks of an evaluator can be divided into two stages:Stage 1: Evalua9ng whether ChatGPT's evalua9on criteria are complete.The evaluator will consider the following informa9on:(1) Task informa9on (e.g., answering Reddit forum ques9ons that can be understood by five-year-old children);(2) Task input fields (e.g., Reddit ques9ons);(3) Task output fields (e.g., Reddit answers);(4) The evalua9on criteria that ChatGPT lists for evalua9ng this NLP task.The evaluator will refine ChatGPT's "evalua9on criteria" by checking each criterion for reasonableness based on commonsense and making adjustments, which can include:(1) Approve (the evalua9on criterion is "qualified");(2) Delete (unnecessary or difficult to evaluate criteria);(3) Revise (modify an evalua9on perspec9ve to make it more consistent with common sense and task requirements); (4) Add (supplement evalua9on criteria that ChatGPT has ignored).Stage 2: Based on the established evalua9on criteria from Stage 1, ChatGPT evaluates task instances (input and output), and the evaluator adjusts ChatGPT's evalua9on results.The evaluator will consider the following informa9on:(1) Evalua9on criteria;(2) Instance input;(3) Instance output;(4) ChatGPT's evalua9on for the task instance (including the overall conclusion, evalua9on score, and evalua9on explana9on).The evaluator will refine ChatGPT's "evalua9on conclusion" based on common sense by checking whether ChatGPT's evalua9on results is reasonable and correc9ng any unreasonable or factually incorrect parts (using the same four ac9ons: approve, delete, revise, add).In summary, the evaluators will need to scru9nize ChatGPT's evalua9on criteria and scru9nize ChatGPT's evalua9on conclusions.
Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</li>
</ul>
<p>Disentangling the properties of human evaluation methods: A classification system to support comparability, meta-evaluation and reproducibility testing. Anja Belz, Simon Mille, David M Howcroft, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language Generation2020</p>
<p>Do models explain themselves? counterfactual simulatability of natural language explanations. Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, Kathleen Mckeown, arXiv:2307.086782023arXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2305.019372023arXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>All that's 'human'is not gold: Evaluating human evaluation of generated text. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, Noah A Smith, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Sentence mover's similarity: Automatic evaluation for multi-sentence texts. Elizabeth Clark, Asli Celikyilmaz, Noah A Smith, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization. Esin Durmus, He He, Mona Diab, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Qafacteval: Improved qa-based factual consistency evaluation for summarization. Alexander Richard Fabbri, Chien-Sheng Wu, Wenhao Liu, Caiming Xiong, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2022</p>
<p>Eli5: Long form question answering. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>Cristina Garbacea, Qiaozhu Mei, arXiv:2007.15780Neural language generation: Formulation, methods, and evaluation. 2020arXiv preprint</p>
<p>Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text. Sebastian Gehrmann, Elizabeth Clark, Thibault Sellam, Journal of Artificial Intelligence Research. 772023</p>
<p>Lot: A story-centric benchmark for evaluating chinese long text understanding and generation. Jian Guan, Zhuoer Feng, Yamei Chen, Ruilin He, Xiaoxi Mao, Changjie Fan, Minlie Huang, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Q2:: Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, Omri Abend, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. David M Howcroft, Anya Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, A Sadid, Saad Hasan, Simon Mahamood, Mille, Sashank Emiel Van Miltenburg, Verena Santhanam, Rieser, 2020Association for Computational Linguistics</p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Sameer Jain, Vaishakh Keshava, Mysore Swarnashree, Patrick Sathyendra, Pengfei Fernandes, Liu, arXiv:2306.01200arXiv:2205.11822Maieutic prompting: Logically consistent reasoning with recursive explanations. 2023. 2022arXiv preprintMultidimensional evaluation of text summarization with incontext learning</p>
<p>Genie: Toward reproducible and standardized human evaluation for text generation. Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A Smith, Daniel S Weld, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Can language models learn from explanations in context?. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James Mcclelland, Jane Wang, Felix Hill, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Don't say that! making inconsistent dialogue unlikely with unlikelihood training. Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, Y-Lan Boureau, Kyunghyun Cho, Jason Weston, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Explanation regeneration via information bottleneck. Qintong Li, Zhiyong Wu, Lingpeng Kong, Wei Bi, Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics2023</p>
<p>Open-ended long text generation via masked language modeling. Xiaobo Liang, Zecheng Tang, Juntao Li, Min Zhang, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Tianyi Nelson F Liu, Percy Zhang, Liang, arXiv:2304.09848Evaluating verifiability in generative search engines. 2023aarXiv preprint</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.16634G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023barXiv preprint</p>
<p>Evaluate what you can't evaluate: Unassessable generated responses quality. Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang, Hinrich Schütze, arXiv:2305.146582023carXiv preprint</p>
<p>Tangled up in bleu: Reevaluating the evaluation of automatic machine translation evaluation metrics. Nitika Mathur, Timothy Baldwin, Trevor Cohn, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>A corpus and cloze evaluation for deeper understanding of commonsense stories. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen, Proceedings of the 2016 Conference of the North American Chapter. the 2016 Conference of the North American ChapterHuman Language Technologies2016</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Webgpt: Browser-assisted questionanswering with human feedback. 2021arXiv preprint</p>
<p>CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Evaluating the morphosyntactic well-formedness of generated texts. Adithya Pratapa, Antonios Anastasopoulos, Shruti Rijhwani, Aditi Chaudhary, Graham David R Mortensen, Yulia Neubig, Tsvetkov, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, Saleema Amershi, arXiv:2304.09991Supporting humanai collaboration in auditing llms with llms. 2023arXiv preprint</p>
<p>An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Ehud Reiter, Anja Belz, Computational Linguistics. 3542009</p>
<p>Adaptive testing and debugging of nlp models. Marco Tulio, Ribeiro , Scott Lundberg, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, arXiv:2206.05802Self-critiquing models for assisting human evaluators. Jonathan WardJan Leike. 2022arXiv preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>Cider: Consensus-based image description evaluation. Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, arXiv:2303.04048Is chatgpt a good nlg evaluator? a preliminary study. 2023aarXiv preprint</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023barXiv preprint</p>
<p>Su Wang, Greg Durrett, Katrin Erk, arXiv:2008.07466Narrative interpolation for generating and understanding stories. 2020arXiv preprint</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Reframing human-ai collaboration for generating free-text explanations. Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Teach me to explain: A review of datasets for explainable natural language processing. Sarah Wiegreffe, Ana Marasović, arXiv:2102.120602021arXiv preprint</p>
<p>Style over substance: Evaluation biases for large language models. Minghao Wu, Alham Fikri, Aji , arXiv:2307.030252023arXiv preprint</p>
<p>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. Tongshuang Wu, Michael Terry, Carrie , Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing SystemsJun Cai. 2022</p>
<p>Ziang Xiao, Susu Zhang, Vivian Lai, Vera Liao, arXiv:2305.14889Evaluating nlg evaluation metrics: A measurement theory perspective. 2023arXiv preprint</p>
<p>Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoyanov, Greg Durrett, Ramakanth Pasunuru, arXiv:2211.13892Complementary explanations for effective in-context learning. 2022arXiv preprint</p>
<p>Towards quantifiable dialogue coherence evaluation. Zheng Ye, Liucun Lu, Lishan Huang, Liang Lin, Xiaodan Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Interpreting language models with contrastive explanations. Kayo Yin, Graham Neubig, arXiv:2202.104192022arXiv preprint</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, arXiv:2310.07641Evaluating large language models at evaluating instruction following. 2023arXiv preprint</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, Yongbin Li, arXiv:2308.01862Wider and deeper llm networks are fairer llm evaluators. 2023arXiv preprint</p>
<p>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.05685Judging llm-as-a-judge with mt-bench and chatbot arena. 2023arXiv preprint</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, arXiv:2306.04528Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-217 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-217</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-217</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-11.html">extraction-schema-11</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <p><strong>Paper ID:</strong> paper-7107d06366b48b3593c8128ed2ca67e0b413628c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7107d06366b48b3593c8128ed2ca67e0b413628c" target="_blank">Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is shown that the use of self-sampled correct and partially-correct solutions can benefit learning and help guide the sampling process, leading to more efficient exploration of the solution space and the effectiveness of the method is shown.</p>
                <p><strong>Paper Abstract:</strong> Pretrained language models have shown superior performance on many natural language processing tasks, yet they still struggle at multi-step formal reasoning tasks like grade school math problems. One key challenge of finetuning them to solve such math reasoning problems is that many existing datasets only contain one reference solution for each problem, despite the fact that there are often alternative solutions resembling different reasoning paths to the final answer. This way, the finetuned models are biased towards the limited reference solutions, which limits their generalization to unseen examples. To mitigate this issue, we propose to let the model perform sampling during training and learn from both self-sampled fully-correct solutions, which yield the correct answer upon execution, and partially-correct solutions, whose intermediate state matches an intermediate state of a known correct solution. We show that our use of self-sampled correct and partially-correct solutions can benefit learning and help guide the sampling process, leading to more efficient exploration of the solution space. Additionally, we explore various training objectives to support learning from multiple solutions per example and find they greatly affect the performance. Experiments on two math reasoning datasets show the effectiveness of our method compared to learning from a single reference solution with MLE, where we improve PASS@100 from 35.5% to 44.5% for GSM8K, and 27.6% to 36.2% PASS@80 for MathQA. Such improvements are also consistent across different model sizes. Our code is available at https://github.com/microsoft/TraceCodegen.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e217.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e217.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-reference MLE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Likelihood Estimation from single reference solution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard supervised fine-tuning objective that trains the model to maximize likelihood of the single dataset-provided reference solution per input; shown to encourage overconfidence/overfitting and low diversity of sampled solutions for multi-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M, 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning / program synthesis (generate Python solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Single reference solution per problem (dataset-provided gold program); standard MLE target</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>MathQA-Python original: 19.2K (train); MathQA-Python-Filtered: 6.8K train / 0.7K dev; GSM5.5K-Python: 5.5K train (converted from GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>single canonical solution only, low target diversity, exact (complete) solutions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PASS@k (e.g., PASS@1, PASS@80, PASS@100)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Varies by setup; examples in paper include PASS@100=35.5% (GSM8K baseline reported in abstract) and for dev GSM5.5K with GPT-Neo-125M MLE baseline PASS@100=22.7% (Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Training with single-reference MLE leads to overfitting to the single solution, reduced diversity of sampled solutions and poor PASS@k at larger k (even if PASS@1 may improve).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e217.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e217.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FCS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-sampled Fully-Correct Solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Correct program solutions sampled by the model during training and retained when execution yields the gold final answer; used as additional supervised targets in SFT to increase target diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M, 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning / program synthesis (generate Python solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Self-sampled correct program solutions (execution-verified) added to a persistent per-example buffer</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Not framed as a fixed dataset size; buffer accumulates on-the-fly during training across epochs (experiments: sampling 1 sample per example per step; total epochs produce thousands of samples per example over training)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>execution-verified correctness, multiple diverse reasoning paths per problem, deduplicated via AST/line-length heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PASS@k</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Improvements reported: GSM8K PASS@100 improved from 35.5% → 44.5% (abstract); on GSM5.5K dev with GPT-Neo-125M, adding self-sampled solutions produced +12.3% absolute PASS@100 improvement; GPT-Neo-2.7B had +9.0% absolute (GSM5.5K). For MathQA-Python-Filtered, PASS@80 improved from 27.6% → 36.2% in some reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Single-reference MLE baselines (examples: PASS@100=35.5% on GSM8K baseline; 22.7% on GSM5.5K dev with GPT-Neo-125M)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Absolute lifts reported: +9.0% to +12.3% PASS@100 on GSM variants; +8.6% (2.7B) and +3.1% (125M) PASS@80 on MathQA-Filtered in some comparisons; specific deltas depend on dataset and model size.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Including self-sampled, execution-verified correct solutions as additional SFT targets substantially increases diversity of generated solutions and yields large absolute gains in PASS@k (especially for larger k), though it does not improve PASS@1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e217.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e217.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Partially-Correct Solutions (prefixes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prefixes of sampled programs whose intermediate execution state matches an intermediate state of a known correct solution; these partial solutions are stored and used as additional (shorter) supervised targets to guide sampling and learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M, 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning / program synthesis (generate Python solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Self-sampled solution prefixes identified by state-equivalence to prefixes of known correct solutions (execution-traced intermediate variable states)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Buffer accumulates PCSs during training; paper reports average saved PCSs per problem (e.g., Table/Fig shows ~1.1 PCSs per problem in some settings for GPT-Neo125M on GSM5.5K)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>state-based equivalence (variable-value sets), name-agnostic, shorter/completion-friendly prefixes that enable guided sampling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PASS@k; also number of saved FCSs/PCSs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Including PCSs in addition to FCSs improved performance further: e.g., paper reports an additional +3.0% PASS@100 improvement for GSM8K when adding PCSs (intro) and shows increased number of discovered FCSs (guided sampling effect). Table 5 (GSM5.5K, GPT-Neo-125M) shows MLE-Aug with FCS+PCS PASS@100=35.0% vs FCS-only 32.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>FCS-only training or single-reference MLE (varies by experiment). Example baseline FCS-only MLE-Aug PASS@100=32.3% (GSM5.5K dev, 125M) vs FCS+PCS 35.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Examples: +~2.7 percentage points PASS@100 (GSM5.5K dev, 125M, MLE-Aug); +3.0% reported for GSM8K in paper; effect includes increased discovery of FCSs via guided sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Learning from partially-correct prefixes helps early training (rapidly grows saved PCSs early), guides sampling to discover more fully-correct solutions, and yields additional PASS@k gains beyond learning from fully-correct self-samples alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e217.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e217.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLE-Aug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Augmented Maximum Likelihood (sum over buffer targets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An SFT objective that sums negative log-likelihoods over all saved targets (reference + self-sampled FCSs/PCSs), giving equal gradient weight to each target; empirically found to be the most effective loss for learning from multiple targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M, 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning / program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Multiple targets per input (reference + buffer of self-sampled FCSs/PCSs) treated equally in the loss (sum of log-probabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Same buffers as above (accumulated during training); examples in paper show average #targets per problem (e.g., FCS+PCS average ~3.36 solutions total in some settings)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>equal weighting across multiple supervised targets encourages diversity and balanced learning from multiple reasoning paths</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PASS@k</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>MLE-Aug yields the largest improvements vs other multi-target losses; example: GSM5.5K dev with GPT-Neo-125M and FCS+PCS: MLE-Aug PASS@100=35.0% vs MLE baseline 22.7% (Table 5); with FCS-only MLE-Aug PASS@100=32.3% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>MLE single-reference; MML and beta-MML (other multi-target objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Example absolute lift: +12.3 percentage points PASS@100 (GSM5.5K dev, 125M, comparing MLE baseline 22.7% → MLE-Aug 35.0% with FCS+PCS).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Summing losses across multiple execution-verified targets (MLE-Aug) is most effective: it distributes gradient equally across diverse targets, increases saved FCSs/PCSs, and yields larger PASS@k gains than MML or beta-MML.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e217.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e217.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MML / β-MML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Marginal Likelihood and β-smoothed MML</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Objectives that marginalize over multiple targets (MML weights target gradients proportional to model likelihood; β-MML smooths those weights); evaluated and compared to MLE-Aug in SFT from multiple self-sampled targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From language to programs: Bridging reinforcement learning and maximum marginal likelihood</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M, 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning / program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Multiple targets per input (reference + buffer), with marginalization-weighted objectives: MML uses model-probability weights, β-MML uses softened weights (β in (0,1])</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>As above (buffers accumulated during training); experiments used β=0.25 for β-MML hyperparameter</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>weights targets by the model-assigned likelihood (MML) which creates positive feedback to high-likelihood targets; β-MML interpolates toward equal weighting as β→0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PASS@k</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>MML performed poorly in experiments: e.g., GSM5.5K dev with GPT-Neo-125M FCS+PCS: MML PASS@100=18.7% (worse than MLE 22.7%); β-MML showed intermediate behavior but did not outperform MLE-Aug (β=0.25 used).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>MLE single-reference and MLE-Aug</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Negative or modest: MML sometimes decreased PASS@k compared to MLE when PCSs included (example: -4.0 pp vs MLE in PASS@100 on GSM5.5K dev for that setup).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>MML's likelihood-weighted gradients create a positive feedback loop that concentrates probability on a single high-likelihood target and reduces diversity; this harms performance when learning from multiple self-sampled or partial targets, while β-MML did not outperform MLE-Aug in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e217.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e217.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Guided sampling from PCS prefixes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prefix-guided sampling using partially-correct solution prefixes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>At sampling time during training, the model samples completions conditioned on saved partially-correct prefixes, reducing generation length and more efficiently exploring the solution space, which increases discovery of fully-correct solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M, 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning / program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sampling completions conditioned on saved PCSs (state-matched prefixes) rather than sampling full programs from scratch</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Operates on dynamically accumulated prefix buffer sizes; reported that early in training PCS counts grow quickly (e.g., early steps thousands of PCSs accumulated across problems)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>shorter prefix-conditioned generations, focused exploration of promising prefixes, increases chance to find FCSs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PASS@k; number of saved FCSs/PCSs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Guided sampling with PCS led to more saved FCSs and better downstream PASS@k (e.g., increase in FCS counts and additional PASS@k gains when PCS included; see increases in Fig. 5 and Table 5 where FCS+PCS outperforms FCS-only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Sampling from scratch without conditioning on PCS prefixes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Qualitative/quantitative: increases number of discovered FCSs per problem and yields additional few-percent absolute gains in PASS@k beyond FCS-only sampling (example: FCS-only MLE-Aug PASS@100=32.3% vs FCS+PCS MLE-Aug 35.0% in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Conditioning sampling on partially-correct prefixes speeds exploration and helps discover more fully-correct solutions, improving the effectiveness of self-sampling SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>From language to programs: Bridging reinforcement learning and maximum marginal likelihood <em>(Rating: 2)</em></li>
                <li>Program synthesis with large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Leveraging grammar and reinforcement learning for neural program synthesis <em>(Rating: 1)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-217",
    "paper_id": "paper-7107d06366b48b3593c8128ed2ca67e0b413628c",
    "extraction_schema_id": "extraction-schema-11",
    "extracted_data": [
        {
            "name_short": "Single-reference MLE",
            "name_full": "Maximum Likelihood Estimation from single reference solution",
            "brief_description": "Standard supervised fine-tuning objective that trains the model to maximize likelihood of the single dataset-provided reference solution per input; shown to encourage overconfidence/overfitting and low diversity of sampled solutions for multi-step reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (fine-tuned)",
            "model_size": "125M, 2.7B",
            "training_stage": "SFT",
            "task_type": "math reasoning / program synthesis (generate Python solutions)",
            "is_scientific_domain": false,
            "data_type": "Single reference solution per problem (dataset-provided gold program); standard MLE target",
            "data_size": "MathQA-Python original: 19.2K (train); MathQA-Python-Filtered: 6.8K train / 0.7K dev; GSM5.5K-Python: 5.5K train (converted from GSM8K)",
            "data_properties": "single canonical solution only, low target diversity, exact (complete) solutions",
            "performance_metric": "PASS@k (e.g., PASS@1, PASS@80, PASS@100)",
            "performance_with_data": "Varies by setup; examples in paper include PASS@100=35.5% (GSM8K baseline reported in abstract) and for dev GSM5.5K with GPT-Neo-125M MLE baseline PASS@100=22.7% (Table 5)",
            "performance_baseline": null,
            "performance_lift": null,
            "compares_data_types": true,
            "key_finding": "Training with single-reference MLE leads to overfitting to the single solution, reduced diversity of sampled solutions and poor PASS@k at larger k (even if PASS@1 may improve).",
            "uuid": "e217.0",
            "source_info": {
                "paper_title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "FCS",
            "name_full": "Self-sampled Fully-Correct Solutions",
            "brief_description": "Correct program solutions sampled by the model during training and retained when execution yields the gold final answer; used as additional supervised targets in SFT to increase target diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (fine-tuned)",
            "model_size": "125M, 2.7B",
            "training_stage": "SFT",
            "task_type": "math reasoning / program synthesis (generate Python solutions)",
            "is_scientific_domain": false,
            "data_type": "Self-sampled correct program solutions (execution-verified) added to a persistent per-example buffer",
            "data_size": "Not framed as a fixed dataset size; buffer accumulates on-the-fly during training across epochs (experiments: sampling 1 sample per example per step; total epochs produce thousands of samples per example over training)",
            "data_properties": "execution-verified correctness, multiple diverse reasoning paths per problem, deduplicated via AST/line-length heuristics",
            "performance_metric": "PASS@k",
            "performance_with_data": "Improvements reported: GSM8K PASS@100 improved from 35.5% → 44.5% (abstract); on GSM5.5K dev with GPT-Neo-125M, adding self-sampled solutions produced +12.3% absolute PASS@100 improvement; GPT-Neo-2.7B had +9.0% absolute (GSM5.5K). For MathQA-Python-Filtered, PASS@80 improved from 27.6% → 36.2% in some reported comparisons.",
            "performance_baseline": "Single-reference MLE baselines (examples: PASS@100=35.5% on GSM8K baseline; 22.7% on GSM5.5K dev with GPT-Neo-125M)",
            "performance_lift": "Absolute lifts reported: +9.0% to +12.3% PASS@100 on GSM variants; +8.6% (2.7B) and +3.1% (125M) PASS@80 on MathQA-Filtered in some comparisons; specific deltas depend on dataset and model size.",
            "compares_data_types": true,
            "key_finding": "Including self-sampled, execution-verified correct solutions as additional SFT targets substantially increases diversity of generated solutions and yields large absolute gains in PASS@k (especially for larger k), though it does not improve PASS@1.",
            "uuid": "e217.1",
            "source_info": {
                "paper_title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "PCS",
            "name_full": "Partially-Correct Solutions (prefixes)",
            "brief_description": "Prefixes of sampled programs whose intermediate execution state matches an intermediate state of a known correct solution; these partial solutions are stored and used as additional (shorter) supervised targets to guide sampling and learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (fine-tuned)",
            "model_size": "125M, 2.7B",
            "training_stage": "SFT",
            "task_type": "math reasoning / program synthesis (generate Python solutions)",
            "is_scientific_domain": false,
            "data_type": "Self-sampled solution prefixes identified by state-equivalence to prefixes of known correct solutions (execution-traced intermediate variable states)",
            "data_size": "Buffer accumulates PCSs during training; paper reports average saved PCSs per problem (e.g., Table/Fig shows ~1.1 PCSs per problem in some settings for GPT-Neo125M on GSM5.5K)",
            "data_properties": "state-based equivalence (variable-value sets), name-agnostic, shorter/completion-friendly prefixes that enable guided sampling",
            "performance_metric": "PASS@k; also number of saved FCSs/PCSs",
            "performance_with_data": "Including PCSs in addition to FCSs improved performance further: e.g., paper reports an additional +3.0% PASS@100 improvement for GSM8K when adding PCSs (intro) and shows increased number of discovered FCSs (guided sampling effect). Table 5 (GSM5.5K, GPT-Neo-125M) shows MLE-Aug with FCS+PCS PASS@100=35.0% vs FCS-only 32.3%.",
            "performance_baseline": "FCS-only training or single-reference MLE (varies by experiment). Example baseline FCS-only MLE-Aug PASS@100=32.3% (GSM5.5K dev, 125M) vs FCS+PCS 35.0%.",
            "performance_lift": "Examples: +~2.7 percentage points PASS@100 (GSM5.5K dev, 125M, MLE-Aug); +3.0% reported for GSM8K in paper; effect includes increased discovery of FCSs via guided sampling.",
            "compares_data_types": true,
            "key_finding": "Learning from partially-correct prefixes helps early training (rapidly grows saved PCSs early), guides sampling to discover more fully-correct solutions, and yields additional PASS@k gains beyond learning from fully-correct self-samples alone.",
            "uuid": "e217.2",
            "source_info": {
                "paper_title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "MLE-Aug",
            "name_full": "Augmented Maximum Likelihood (sum over buffer targets)",
            "brief_description": "An SFT objective that sums negative log-likelihoods over all saved targets (reference + self-sampled FCSs/PCSs), giving equal gradient weight to each target; empirically found to be the most effective loss for learning from multiple targets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (fine-tuned)",
            "model_size": "125M, 2.7B",
            "training_stage": "SFT",
            "task_type": "math reasoning / program synthesis",
            "is_scientific_domain": false,
            "data_type": "Multiple targets per input (reference + buffer of self-sampled FCSs/PCSs) treated equally in the loss (sum of log-probabilities)",
            "data_size": "Same buffers as above (accumulated during training); examples in paper show average #targets per problem (e.g., FCS+PCS average ~3.36 solutions total in some settings)",
            "data_properties": "equal weighting across multiple supervised targets encourages diversity and balanced learning from multiple reasoning paths",
            "performance_metric": "PASS@k",
            "performance_with_data": "MLE-Aug yields the largest improvements vs other multi-target losses; example: GSM5.5K dev with GPT-Neo-125M and FCS+PCS: MLE-Aug PASS@100=35.0% vs MLE baseline 22.7% (Table 5); with FCS-only MLE-Aug PASS@100=32.3% (Table 5).",
            "performance_baseline": "MLE single-reference; MML and beta-MML (other multi-target objectives)",
            "performance_lift": "Example absolute lift: +12.3 percentage points PASS@100 (GSM5.5K dev, 125M, comparing MLE baseline 22.7% → MLE-Aug 35.0% with FCS+PCS).",
            "compares_data_types": true,
            "key_finding": "Summing losses across multiple execution-verified targets (MLE-Aug) is most effective: it distributes gradient equally across diverse targets, increases saved FCSs/PCSs, and yields larger PASS@k gains than MML or beta-MML.",
            "uuid": "e217.3",
            "source_info": {
                "paper_title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "MML / β-MML",
            "name_full": "Maximum Marginal Likelihood and β-smoothed MML",
            "brief_description": "Objectives that marginalize over multiple targets (MML weights target gradients proportional to model likelihood; β-MML smooths those weights); evaluated and compared to MLE-Aug in SFT from multiple self-sampled targets.",
            "citation_title": "From language to programs: Bridging reinforcement learning and maximum marginal likelihood",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (fine-tuned)",
            "model_size": "125M, 2.7B",
            "training_stage": "SFT",
            "task_type": "math reasoning / program synthesis",
            "is_scientific_domain": false,
            "data_type": "Multiple targets per input (reference + buffer), with marginalization-weighted objectives: MML uses model-probability weights, β-MML uses softened weights (β in (0,1])",
            "data_size": "As above (buffers accumulated during training); experiments used β=0.25 for β-MML hyperparameter",
            "data_properties": "weights targets by the model-assigned likelihood (MML) which creates positive feedback to high-likelihood targets; β-MML interpolates toward equal weighting as β→0",
            "performance_metric": "PASS@k",
            "performance_with_data": "MML performed poorly in experiments: e.g., GSM5.5K dev with GPT-Neo-125M FCS+PCS: MML PASS@100=18.7% (worse than MLE 22.7%); β-MML showed intermediate behavior but did not outperform MLE-Aug (β=0.25 used).",
            "performance_baseline": "MLE single-reference and MLE-Aug",
            "performance_lift": "Negative or modest: MML sometimes decreased PASS@k compared to MLE when PCSs included (example: -4.0 pp vs MLE in PASS@100 on GSM5.5K dev for that setup).",
            "compares_data_types": true,
            "key_finding": "MML's likelihood-weighted gradients create a positive feedback loop that concentrates probability on a single high-likelihood target and reduces diversity; this harms performance when learning from multiple self-sampled or partial targets, while β-MML did not outperform MLE-Aug in the experiments.",
            "uuid": "e217.4",
            "source_info": {
                "paper_title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Guided sampling from PCS prefixes",
            "name_full": "Prefix-guided sampling using partially-correct solution prefixes",
            "brief_description": "At sampling time during training, the model samples completions conditioned on saved partially-correct prefixes, reducing generation length and more efficiently exploring the solution space, which increases discovery of fully-correct solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (fine-tuned)",
            "model_size": "125M, 2.7B",
            "training_stage": "SFT",
            "task_type": "math reasoning / program synthesis",
            "is_scientific_domain": false,
            "data_type": "Sampling completions conditioned on saved PCSs (state-matched prefixes) rather than sampling full programs from scratch",
            "data_size": "Operates on dynamically accumulated prefix buffer sizes; reported that early in training PCS counts grow quickly (e.g., early steps thousands of PCSs accumulated across problems)",
            "data_properties": "shorter prefix-conditioned generations, focused exploration of promising prefixes, increases chance to find FCSs",
            "performance_metric": "PASS@k; number of saved FCSs/PCSs",
            "performance_with_data": "Guided sampling with PCS led to more saved FCSs and better downstream PASS@k (e.g., increase in FCS counts and additional PASS@k gains when PCS included; see increases in Fig. 5 and Table 5 where FCS+PCS outperforms FCS-only).",
            "performance_baseline": "Sampling from scratch without conditioning on PCS prefixes",
            "performance_lift": "Qualitative/quantitative: increases number of discovered FCSs per problem and yields additional few-percent absolute gains in PASS@k beyond FCS-only sampling (example: FCS-only MLE-Aug PASS@100=32.3% vs FCS+PCS MLE-Aug 35.0% in Table 5).",
            "compares_data_types": true,
            "key_finding": "Conditioning sampling on partially-correct prefixes speeds exploration and helps discover more fully-correct solutions, improving the effectiveness of self-sampling SFT.",
            "uuid": "e217.5",
            "source_info": {
                "paper_title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "From language to programs: Bridging reinforcement learning and maximum marginal likelihood",
            "rating": 2
        },
        {
            "paper_title": "Program synthesis with large language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Leveraging grammar and reinforcement learning for neural program synthesis",
            "rating": 1
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 1
        }
    ],
    "cost": 0.015799,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions</h1>
<p>Ansong $\mathrm{Ni}^{1 *}$ Jeevana Priya Inala ${ }^{2}$ Chenglong Wang ${ }^{2}$<br>Oleksandr Polozov ${ }^{3 \dagger}$ Christopher Meek ${ }^{4 \ddagger}$ Dragomir Radev ${ }^{1}$ Jianfeng Gao ${ }^{2}$<br>${ }^{1}$ Yale University, ${ }^{2}$ Microsoft Research, ${ }^{3}$ Google, ${ }^{4}$ University of Washington<br>ansong.ni@yale.edu, {jinala, chenwang}@microsoft.com</p>
<h4>Abstract</h4>
<p>Pretrained language models have shown superior performance on many natural language processing tasks, yet they still struggle at multi-step formal reasoning tasks like grade school math problems. One key challenge of finetuning them to solve such math reasoning problems is that many existing datasets only contain one reference solution for each problem, despite the fact that there are often alternative solutions resembling different reasoning paths to the final answer. This way, the finetuned models are biased towards the limited reference solutions, which limits their generalization to unseen examples. To mitigate this issue, we propose to let the model perform sampling during training and learn from both selfsampled fully-correct solutions, which yield the correct answer upon execution, and partially-correct solutions, whose intermediate state matches an intermediate state of a known correct solution. We show that our use of self-sampled correct and partially-correct solutions can benefit learning and help guide the sampling process, leading to more efficient exploration of the solution space. Additionally, we explore various training objectives to support learning from multiple solutions per example and find they greatly affect the performance. Experiments on two math reasoning datasets show the effectiveness of our method compared to learning from a single reference solution with MLE, where we improve PASS@100 from $35.5 \%$ to $44.5 \%$ for GSM8K, and $27.6 \%$ to $36.2 \%$ PASS@80 for MathQA. Such improvements are also consistent across different model sizes. Our code is available at https://github.com/microsoft/TraceCodegen.</p>
<h2>1 INTRODUCTION</h2>
<p>Recent progress on pretrained language models shows that they are able to achieve human-level performance on various natural language processing tasks with finetuning(Devlin et al., 2019; Brown et al., 2020; Raffel et al., 2020). However, such models still lack the ability to perform multi-step math reasoning even for problems that are intended for grade-school students (Cobbe et al., 2021). Current methods for solving math problems typically rely on generating solutions (a sequence of computation steps) and executing them to obtain the final answer (Cobbe et al., 2021; Austin et al., 2021; Chen et al., 2021a; Chowdhery et al., 2022), as directly generating the final answer would require computational abilities that even the largest models do not possess (Brown et al., 2020; Chowdhery et al., 2022).</p>
<p>When finetuning such models on math reasoning, existing methods often rely on the MLE objective that aims to maximize the log-likelihood of the reference solution for each natural language input. However, in addition to the reference solution, there are often multiple correct solutions for each question, resembling alternative reasoning paths to the final answer. However, those alternative solutions are unseen during training, and this results in model overfitting: the model becomes overly confident in its predictions because it sees the same solution over multiple epochs of training (Bunel et al., 2018; Austin et al., 2021; Cobbe et al., 2021). This leads to poor generalization on unseen</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>inputs and is reflected by the low PASS@ $k$ performance, where the model is unable to predict the right answer even when allowed multiple attempts per question.</p>
<p>To mitigate this issue, we propose learning from self-sampled solutions. Concretely, during training time, the model samples alternative solutions, and keeps track of all solutions that are semantically correct with respect to the gold execution result, and learns from all of these correct solutions as opposed to only from the reference. To further improve the effectiveness of learning from selfsampled solutions, we allow the model to learn from partially-correct solutions, whose intermediate states are consistent with intermediate states of known correct solutions. This new technique allows the model to maximally utilize the self-sampling and more efficiently explore the solution space. We also study various common loss functions for learning from multiple targets for a single natural language input, including augmented-MLE, Maximize Marginal Likelihood (MML) and $\beta$-smoothed MML (Guu et al., 2017) and find that their different gradient equations greatly affect the learning capabilities of the model.</p>
<p>We perform experiments on two math reasoning tasks, namely MathQA-Python (Austin et al., 2021) and Grade-School-Math (GSM) (Cobbe et al., 2021), and finetune GPT-Neo models (Black et al., 2021) to generate Python program as solutions from the problem description in natural language. Results show that learning from self-sampled solutions can improve the PASS@100 from 35.5\% to $44.5 \%$ for GSM, and $27.6 \%$ to $36.2 \%$ for PASS@80 on a filtered version of MathQA-Python. ${ }^{1}$ Moreover, we find that learning from partially-correct solutions generally improves performance over learning from just fully-correct solutions (e.g., $+3.0 \%$ PASS@100 for GSM8K) as it guides the sampling process, discovering more alternative solutions for learning. Such performance boosts from our proposed methods are also consistent for different model sizes. Ablation on different loss functions shows that MLE-Aug loss is the most effective in learning from multiple targets and yields the most improvements over MLE loss.</p>
<h1>2 OVERVIEW</h1>
<p>Problem formulation. We consider the task of generating solutions from math problem descriptions in natural language (NL). Given an NL input $x \in \mathcal{X}$ and the executor $\mathcal{E}: \mathcal{Y} \rightarrow \mathcal{Z}$, the goal is to generate a solution $y \in \mathcal{Y}$ that executes to the expected answer $z^{<em>} \in \mathcal{Z}$, i.e., $\mathcal{E}(y)=z^{</em>}$.</p>
<p>Standard approach and its limitation. The standard approach is to assume that we have a dataset of paired NL input $x$ and reference solution $y^{<em>}$. Most datasets typically only provide one reference solution for a particular NL input. Then, a parameterized model $P_{\theta}$ is learned with the Maximum Likelihood Estimation (MLE) objective from the NL-Solution pair $\left(x, y^{</em>}\right)$ as:</p>
<p>$$
\mathcal{L}_{\mathrm{MLE}}\left(x, y^{<em>}, P_{\theta}\right)=-\log P_{\theta}\left(y^{</em> \mid} x\right)
$$</p>
<p>The builtin assumption of using Eq. 1 for learning is that only the reference solution $y^{<em>}$ is correct. However, this assumption is clearly untrue for the math reasoning problem as typically multiple reasoning paths can achieve the correct final result. With only one reference solution as target for learning, Eq. 1 would encourage the model to put all probability mass on $y^{</em>}$, which could easily lead to overfitting (Bunel et al., 2018; Austin et al., 2021; Cobbe et al., 2021).</p>
<p>Overview of our approach. While manually collecting additional reference solutions for each specification is a laborious process (Austin et al., 2021; Cobbe et al., 2021; Schuster et al., 2021), in our work, we explore an alternate approach: where the model self-samples additional correct (or partially-correct) solutions and learns from them during training. Fig. 1 shows an example: for the question $x$, our model was able to self-sample an alternative solution $\hat{y}$ that is different from the reference solution $y^{<em>}$ provided in the dataset. Looking at the intermediate states shown on the right, we can see that both these solutions execute to produce the sample desired output, i.e., $\hat{z}=z^{</em>}$, as noted with solid red boxes. Taking this one step further, our approach can also identify partiallycorrect solutions from its samples. For example, on the bottom left, we show a sampled solution $\hat{y}^{\prime}$ that is incorrect only because of an error in its last two steps. But we identify a prefix $\hat{y}<em 5="5">{\leq 5}^{\prime}$ of it as partially-correct because the intermediate state $\hat{s}</em>^{<em>}$ for this prefix matches the intermediate state $s_{5}^{</em>}$ of a known correct solution $y^{<em>}$ (noted as dashed red boxes) and yet syntactically different from $y^{</em>}$. Based on these observations and intuitions, we introduce our approach in the following sections.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of self-sampled correct and partiallycorrect solutions from MathQA (more in Appendix D). The steps and intermediate states marked in red are incorrect.</p>
<h1>3 LEARNING FROM SELF-SAMPLED SOLUTIONS</h1>
<p>We now formally present our approach. There are three main steps: 1) sampling 2) filtering and 3) learning as shown in Alg. 1 Here we mainly introduce the self-sampling framework using only fully-correct solutions and the extensions with partially-correct solutions will be introduced in § 3.3.</p>
<h3>3.1 ONLINE SAMPLING AND Filtering</h3>
<p>For each specification $x$, we maintain a buffer $\mathcal{B}$ to save the different solutions that are correct, i.e., evaluate to the correct result. Note that the buffers are persistent and cumulative across training epochs. To add more solutions in $\mathcal{B}$, we perform online sampling and filtering as follows.
Online sampling (line 4 in Alg. 1): With the NL question $x$ from each example $\left(x, y^{<em>}, z^{</em>}\right)$ as input, the model samples a set candidate solutions $\hat{Y}=\left{\hat{y}<em i="1">{i}\right}</em> \mid x)$;
Filtering incorrect solutions(line 7 in Alg. 1): As not all sampled solutions in $\hat{Y}$ are correct (thus not suitable for learning), we filter out all incorrect solutions in $\hat{Y}$, i.e., $\hat{Y}^{}^{n} \sim P_{\theta}(\hat{y<em>}={\hat{y} \mid \hat{y} \in \hat{Y} ; \mathcal{E}(\hat{y})=z^{</em>}}$; Filtering duplicate solutions (line 8 in Alg. 1): Because the model can sample solutions that are correct but are "trivial variants" of other already saved solutions (e.g., the solution differs from another solution only in white spaces, comments or trivial steps like " $x=x+1,0$ "), we further filter the buffer to remove them. This is essential as all saved solutions will be directly used for learning and such undesired behavior from the model will be encouraged without the filtering process. ${ }^{2}$ Concretely, we first perform filtering based on the linearized abstract syntax trees (ASTs) to eliminate the differences in white space, etc; then we set a constraint on maximum number of lines using the number of lines in $y^{*}$ as the reference to prevent saving solutions with trivial steps.</p>
<h3>3.2 LEARNING FROM MULTIPLE TARGETS</h3>
<p>With self-sampling, each natural language question is paired with multiple solutions as targets for learning. Here we discuss some common loss functions for the multi-target learning problem, with</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Loss Functions $\mathcal{L}(x, \mathcal{B}, P_{\theta})$</th>
<th style="text-align: center;">Gradients $\nabla_{\theta}(x, \mathcal{B}, P_{\theta})$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MLE</td>
<td style="text-align: center;">$-\log P_{\theta}\left(y^{*}</td>
<td style="text-align: center;">x\right)$</td>
</tr>
<tr>
<td style="text-align: left;">MLE-Aug</td>
<td style="text-align: center;">$-\sum_{\hat{y} \in \mathcal{B}} \log P_{\theta}(\hat{y} \mid x)$</td>
<td style="text-align: center;">$-\sum_{\hat{y} \in \mathcal{B}} \nabla_{\theta} \log P_{\theta}(\hat{y} \mid x)$</td>
</tr>
<tr>
<td style="text-align: left;">MML</td>
<td style="text-align: center;">$-\log \sum_{\hat{y} \in \mathcal{B}} P_{\theta}(\hat{y} \mid x)$</td>
<td style="text-align: center;">$-\sum_{\hat{y} \in \mathcal{B}} \frac{P_{\theta}(\hat{y} \mid x)}{\sum_{\hat{y} \in \mathcal{B}} P_{\theta}(\hat{y} \mid x)} \nabla_{\theta} \log P_{\theta}(\hat{y} \mid x)$</td>
</tr>
<tr>
<td style="text-align: left;">$\beta$-MML</td>
<td style="text-align: center;">$-\frac{1}{\beta} \log \sum_{y} P_{\theta}(\hat{y} \mid x)^{\beta}$</td>
<td style="text-align: center;">$-\sum_{\hat{y} \in \mathcal{B}} \frac{P_{\theta}(\hat{y} \mid x)^{\beta}}{\sum_{\hat{y} \in \mathcal{B}} P_{\theta}(\hat{y} \mid x)^{\beta}} \nabla_{\theta} \log P_{\theta}(\hat{y} \mid x)$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of loss functions and their gradients over multiple reference $\mathcal{B}$. Note that they all degenerates to MLE when only the gold reference solution is used as target, i.e., $\mathcal{B}=\left{y^{*}\right}$.
a focus on how each target contributes to the gradient. The loss functions and their gradients are shown in Tab. 1.</p>
<p>Augmented MLE (MLE-Aug): This objective augments MLE with multiple targets simply by summing the loss from multiple solutions in $\mathcal{B}$, which is equivalent as minimizing the KL-divergence from $P_{\theta}(y \mid x)$ to $Q(y \mid x)=\frac{1}{|\mathcal{B}|} \cdot \mathbb{1}<em _theta="\theta">{\mathcal{B}}(y)$, where $\mathbb{1}(\cdot)$ is a set indicator function. It encourages the model to put equal weights on all targets by ensuring that all targets equally contribute to the gradient.
Maximum Marginal Likelihood (MML): MML attempts to approximate $P</em>$ as noted in (Guu et al., 2017).
$\beta$-smoothed MML ( $\beta$-MML): Proposed in Guu et al. (2017), the $\beta$-MML objective is an extension of MML with a hyperparameter $\beta \in(0,1]$ to adjust weights of the gradient from each target. It an interpolation between MML and MLE-Aug objectives, more specifically, it recovers MML when $\beta=1$ and its gradient is equivalent to that of MLE-Aug when $\beta \rightarrow 0$.}\left(z^{*} \mid x\right)$ by marginalizing over the correct solutions in $\mathcal{B}$. However, for each target $\hat{y} \in \mathcal{B}$, the gradient of it is in proportion to the likelihood $P_{\theta}(\hat{y} \mid x)$ given by the model, which results in a positive feedback loop during gradient updates. It encourages the model to still put a majority of the probability on one of the solutions in $\mathcal{B</p>
<p>Empirically, we find that these distinctions between those loss functions greatly affects the model performance (Fig. 3), especially when partially-correct solutions are included for learning.</p>
<h1>3.3 Learning from Partially-Correct Solutions</h1>
<p>Besides learning from self-sampled fully-correct solutions (FCSs), we can also let the model learn from partially-correct solutions (PCSs). Our motivation is that the model often encounter solutions that are close to being correct as they only make mistakes in the last few steps (e.g., Fig. 1), and these partially-correct solutions provide additional learning opportunities. Learning from PCSs could also address the issue that the sampler may have a low chance of encountering fully-correct solutions for complex tasks due to the sparse solution space.</p>
<h3>3.3.1 Identifying Partially-Correct Solutions</h3>
<p>When the model samples a solution that does not produce the desired answer, we want to identify if a prefix of this solution is partially correct, i.e., it performs some of the necessary computation steps needed for the correct solution, so that the model can additionally learn from these potentially unseen prefixes in the next iteration. A challenge here is figuring out when a prefix is partially correct. Ideally, we want to say a prefix $y_{\leq i}$ is partially correct if there exists a suffix $y_{&gt;i}$ such that their concatenation $\left(y_{\leq i} \mid \mid y_{&gt;i}\right)$ is a correct solution. There are two caveats here: (1) if there is no length restriction on the suffix, it is always possible to find a suffix that complements any prefix (e.g., a full gold solution is one such suffix); and (2) it is computationally very expensive to search for all suffixes (even with a length restriction) to check if a prefix can be completed to a correct solution.</p>
<p>To overcome these challenges, we leverage the gold reference solutions and any self-sampled fullycorrect or even partially-correct solutions to help identify new partially-correct prefixes. The idea is to identify a prefix as partially correct if it produces a set of intermediate values (upon execution) that exactly matches the set of intermediate values produced by a prefix of a known correct or partiallycorrect solution. For such a prefix, we know that there exists a reasonable complement suffix based on the suffix of the known solutions. Note that, this definition of partial correctness is conservative compared to the ideal definition above, but it makes the computation significantly tractable.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="w"> </span><span class="nt">SampleSolutions</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">x</span><span class="o">,</span><span class="w"> </span><span class="nt">P_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">partially-correct</span><span class="w"> </span><span class="nt">solutions</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">Model</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">P_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="o">(</span><span class="nt">y</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">x</span><span class="o">)</span><span class="err">\</span><span class="o">);</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">NL</span><span class="w"> </span><span class="nt">input</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">x</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">partially-correct</span><span class="w"> </span><span class="nt">solutions</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Output</span><span class="o">:</span><span class="w"> </span><span class="nt">Solution</span><span class="w"> </span><span class="nt">samples</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">Y</span><span class="p">}</span><span class="err">\</span><span class="o">).</span>
<span class="w">    </span><span class="nt">Select</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">y</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\leq</span><span class="w"> </span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">backslash</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\hat{y</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">E</span><span class="p">}</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">y</span><span class="p">}</span><span class="o">)=</span><span class="nt">z</span><span class="o">^</span><span class="p">{</span><span class="err">*</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}\</span><span class="o">)</span><span class="w"> </span><span class="nt">uniformly</span><span class="w"> </span><span class="nt">at</span><span class="w"> </span><span class="nt">random</span><span class="w"> </span><span class="c">/* sample PCS prefix for completion */</span>
<span class="w">    </span><span class="nt">Sample</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">completions</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">Y_</span><span class="p">{</span><span class="err">p</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="nt">P_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">y</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">&gt;i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">y</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\leq</span><span class="w"> </span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">x</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">Y</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\left|\hat{y</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\leq</span><span class="w"> </span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">|</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">y</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">&gt;i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">\hat{y</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">&gt;i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="nt">Y_</span><span class="p">{</span><span class="err">p</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">*</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">concatenate</span><span class="w"> </span><span class="nt">completions</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">solution</span><span class="w"> </span><span class="nt">prefix</span><span class="w"> </span><span class="o">*/</span>
<span class="w">    </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">Y</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<p>Below, we formally define this notion of partial solutions that leverages existing known fully and partially correct solutions.
Intermediate state. Given a solution $y=\left(u_{1}, \ldots, u_{t}\right)$ where $u_{i}$ is the $i$-th reasoning step, we define the intermediate state $s_{i}$ as the set of all variables values in the scope after executing the first $i$ steps $y_{\leq i}=\left(u_{1}, \ldots, u_{i}\right)$, which we call a prefix of this solution. It is easy to see that the prefixes $y_{\leq i}$ and intermediate states $s_{i}$ of a solution construct a bijective function, which is also illustrated in Fig. 1. Note that the state representation is name-agnostic since variable names do not typically contributes to the semantics of the solutions.
State-based equivalence and partial correctness. Given the definition of the intermediate state, we say the prefixes of two solutions, $y_{\leq i}$ and $y_{\leq j}^{\prime}$, are semantically equivalent if and only if $s_{i}=s_{j}^{\prime}$, i.e., those two solutions produces the exact same set of variable values. And then we define partial correctness as follows: a solution prefix $y_{\leq i}$ is partially-correct if and only if it is semantically equivalent to the prefix of another known partially-correct solution $y_{\leq j}^{*}$. As we keep all known partially-correct solutions in the buffer $\mathcal{B}$, formally:</p>
<p>$$
\operatorname{PartiallyCorrect}\left(y_{\leq i}\right) \Longleftrightarrow \exists y^{<em>} \in \mathcal{B} . \exists j \leq\left|y^{</em>}\right| \text { s.t. } s_{j}^{*}=s_{i}
$$</p>
<h1>3.3.2 MODIFICATIONS TO THE MAIN ALGORITHM</h1>
<p>To support learning from partial solutions, we modify Alg. 1 as follows to enable buffering and sampling from partial solutions. The fully updated algorithm is shown in Appendix C.</p>
<p>Guided-Sampling: In § 3.1, we mentioned that full solutions are sampled for each question $x$ as $\hat{y} \sim$ $P_{\theta}(\hat{y} \mid x)$. With PCS prefixes, compared with sampling a solution from scratch, generating solutions with these prefixes reduces the generation length thus the model can more efficiently explore the solution space. This guided sampling process is described in more detail in Alg. 2. Note that since the empty solution $y^{0}$ is in the buffer $\mathcal{B}$ since initialization, therefore model can still generate and explore the space from scratch and not always follows the existing solution prefixes.
Identify partially-correct prefixes: As mentioned in § 3.3, if a solution $\hat{y}$ does not produce the expected result $z^{*}$ but its prefix $\hat{y}<em _leq="\leq" i="i">{\leq i}$ is partially-correct, the model can still learn from its prefix. However, an important task here is to identify the longest partially-correct prefix for learning, in other words, locate the exact step that the solution deviates from a correct reasoning path. We can achieve this simply by backtracking the intermediate states and find the first state that is equivalent to any of the states from a saved solution. ${ }^{3}$
Filtering solution prefixes: With the inclusion of partially-correct solutions, we need to slightly change the two filtering criteria in $\S 3.1$. For deduplication, while we still use AST to rule out changes with non-semantic tokens such as white space, we also check if the partially-correct solution prefix $\hat{y}</em>$. As for the length constraint, the same principle still applies, but now it is compared against other partiallycorrect solution that executes to the same state.
Learning objective: As partially-correct solutions are solution prefixes $y_{\leq i}$ missing the later part $y_{&gt;i}$, with an auto-regressive generation model, the learning of $P_{\theta}\left(y_{\leq i} \mid x\right)$ is independent of $y_{&gt;i}$. Thus the learning objectives in $\S 3.2$ do not need to change with the inclusion of PCS in the buffer for learning. The only difference is that the end-of-sequence " $\langle$ eos $\rangle$ " token is not appended to the PCS as those solutions are not yet finished.}$ is a prefix of another known PCS in $\mathcal{B}$. For the same reason, when saving a new partiallycorrect solution $\hat{y}$, we need to prune out any existing solution in $\mathcal{B}$ that is a prefix of $\hat{y</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 EXPERIMENTAL SETUP</h3>
<p>Datasets. We evaluate on two math reasoning datasets, in which we generate straight-line Python programs as solutions to solve math problems described in natural language. We finetune the language models to output such program solutions using only the natural language problem description as the input.
$\triangleright$ MathQA-Python-Filtered: The original MathQA-Python consists of 19.2 K training examples of NL and Python program pairs (Austin et al., 2021). However, we find the raw dataset to contain many questions that share the same question templates and only differ in concrete number across the train/dev/test sets. To better understand the generalization of the trained models, we derive a deduplicated version of the dataset by first merging the train and dev data and then perform templatebased deduplication. Partly inspired by Finegan-Dollak et al. (2018), we re-split the train and dev set based on the question templates, resulting in $6.8 \mathrm{~K} / 0.7 \mathrm{~K}$ train/dev data for the filtered version. ${ }^{4}$ While we mainly experiment on the filtered version, we report performance on both versions when compared with previous methods.
$\triangleright$ GSM5.5K-Python: The grade-school-math (GSM8K) dataset (Cobbe et al., 2021) contains 7.5K training data points. Since it only provides natural language solutions with math formulas and does not have a dev set, we first reserved $20 \%$ of the training data as dev set, then automatically converted the formulas to program solutions in the same style as MathQA-Python. As the result, we finetune our models with the 5.5 K successfully converted training examples. Note that the natural language solutions/explanations are not used as input to the models in our experiments.</p>
<p>Evaluation metrics: Following recent work in neural program synthesis (Austin et al., 2021; Chen et al., 2021a; Chowdhery et al., 2022) and math reasoning (Cobbe et al., 2021), we use PASS@ $k$ as our main evaluation metric. It allows the model to sample $k$ solutions for each question and the task is considered solved if any one of the $k$ solutions is correct, so PASS@ $k$ can also be seen as the fraction of problems in the test/dev set being solved given $k$ attempts. More details (e.g., temperature) can be found in Appendix A.</p>
<p>Model training: We use GPT-Neo (Black et al., 2021) as our language model and mainly study two model sizes, 125 M and 2.7 B . ${ }^{5}$ Following previous work (Austin et al., 2021), we evaluate all PASS@ $k$ on the same model checkpoint that has the best PASS@1 score, but note that it might not be the best checkpoint for other $k$ values (more discussion in Appendix E). Detailed hyperparameter settings can also be found in Appendix A.</p>
<h3>4.2 MAIN RESULTS</h3>
<p>Learning from self-sampled solutions improves PASS@ $k$. Fig. 2 shows the performance on the two datasets by learning from self-sampled FCSs and PCSs using MLE-Aug (orange bars), compared with MLE on single reference solution (blue bars). We can see that our proposed method can greatly improve PASS@ $k$, especially for higher $k$ values. By comparing different model sizes, we can see that learning from self-sampled solutions can help with both small and large models, with a $+12.3 \%$ and $+9.0 \%$ PASS@100 improvement on GSM5.5K-Python for GPT-Neo-125M and GPT-Neo-2.7B, respectively and a $+3.1 \%$ and $+8.6 \%$ PASS@80 improvement on MathQA-PythonFiltered for GPT-Neo-125M and GPT-Neo-2.7B, respectively. We note that our approach does not improve PASS@1, which is expected as learning from multiple targets mainly helps with increasing the diversity of the sampled solutions rather than improving the most-probable solution (for which MLE is better suited).</p>
<p>Partially-correct solutions improve model performance. We next show the effects of including partially-correct solutions on PASS@ $k$ performance in Fig. 2 (green bars vs orange bars) and the number of saved FCSs and PCSs in Fig. 5. First, we observe from Fig. 5 that using partial correctness not only results in PCSs being saved and directly learned from, but it also boosts the number of FCSs being found with the guided-sampling process. As a result, most PASS@ $k$ performances drop if we</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Percentage of the problems solved (PASS@k) on the dev set of GSM5.5K-Python and MathQA-Python-Filtered, comparing our self-sampling approach and the common MLE objective. All our methods include partially-correct solutions and use the MLE-Aug loss for learning.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: PASS@100 comparison of various loss functions (§ 3.2) under different self-sampling strategies. Results are on the dev set of GSM5.5K-Python with finetuned GPT-Neo 125M model. $\beta=0.25$ for $\beta$-MML. Full results available as Tab. 5 in Appendix B.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Number of saved FCSs and PCSs per problem for GSM5.5K-Python (left) and MathQA-PythonFiltered (right), with different self-sampling strategies and model sizes. # FCSs includes the reference solution.
do not include partially-correct solutions in the buffer, as the model learns from a smaller number of FCSs and PCSs as targets. The one exception is the GPT-Neo 125M model on the MathQA-PythonFiltered dataset, where we do not observe any advantage/disadvantage of using PCSs.</p>
<p>MLE-Aug loss function works the best. We next study the effects of different objective functions for learning from multiple targets as described in $\S 3.2$. We also experiment under different selfsampling strategies (i.e., FCS only or FCS + PCS), and our experiment results on GSM5.5K-Python with the GPT-Neo 125M model are shown in Tab. 5. We can see that MLE-Aug loss results in the biggest improvement compared to other losses both with just FCSs and with FCSs + PCSs. MML performs the worst: it only marginally improves over MLE with only FCS and performs worse than MLE when also learning from PCSs. As discussed in $\S 3.2$ and Tab. 1, the gradient of MML is in proportional to the likelihood given by the model, thus it encourages the model to put all weight on one solution in the buffer. As MLE already learns from the gold reference solution, it is hard for</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Original Version</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Filtered Version</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Models</td>
<td style="text-align: center;">PASS@1</td>
<td style="text-align: center;">PASS@80</td>
<td style="text-align: center;">PASS@1</td>
<td style="text-align: center;">PASS@80</td>
</tr>
<tr>
<td style="text-align: left;">Previous work:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Codex Davinci ${ }^{\dagger}$ (Chen et al., 2021a)</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">$\mathbf{4 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">LaMDA 68B* (Austin et al., 2021)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LaMDA 137B* (Austin et al., 2021)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Ours:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-Neo 125M w/ self-sampling FCS + PCS</td>
<td style="text-align: center;">$\mathbf{7 7 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 7}$</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">28.2</td>
</tr>
<tr>
<td style="text-align: left;">GPT-Neo 2.7B w/ self-sampling FCS + PCS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{2 0 . 7}$</td>
<td style="text-align: center;">36.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison with previous methods on the original (test set used) and filtered version (dev set used) of the MathQA-Python dataset. *: model not pretrained on code. ${ }^{\dagger}$ : few-shot learning results. -: no results available.</p>
<p>MML to make improvements with self-sampled solutions, and the performance may even decrease when MML puts all weight on an incomplete partially-correct solution. In contrast, the gradients of MLE-Aug objective are equally distributed among the targets, which leads to more diversity in its generation due to a more balanced source of learning signals. $\beta$-MML loss is proposed to alleviate the aforementioned issue for MML loss, but we do not observe an advantage of using it instead of the MLE-Aug loss in our experiments.</p>
<h1>4.3 ADDITIONAL ANALYSIS</h1>
<p>Diversity of the solutions. By inspecting the $k$ generated solutions for each task, we find that there is more diversity in the solutions that the model generates using our method. More specifically, we calculate the ratio of unique solutions from the 100 samples for the comparison in Fig. 2a, and find that $30.5 \%$ of them are unique for our approach but only $20.8 \%$ for the model trained with MLE.</p>
<p>Dynamics between # of PCSs and FCSs saved in the buffer. As discussed above, more saved solutions typically results in better PASS@ $k$ performance. Interestingly, when comparing different model sizes, we can see that while the sum of partially and fully-correct solutions sampled and saved in the buffer are about the same (i.e., 3.36 and 3.31) for GSM5.5K-Python dataset in Fig. 5, around $60 \%$ of them are FCS for the small model while it is $76 \%$ for the larger model. The difference in percentage of PCSs left in the buffer also reflects the model's ability for completing partially-correct solution prefixes. We also find that during early stages of training, the number of PCSs rapidly grows while the model is relatively weak to sample FCSs, thus the PCSs help enriching the learning signal and preventing overfitting early-on. More discussions about this can be found in Appendix E.</p>
<p>Comparison to previous works Here we compare with previous work on both the original and the filtered versions of MathQA-Python datasets in Tab. 2. On the original dataset, self-sampling with GPT-Neo 125M is able to outperform previous methods that finetune 137B model pretrained on natural language. We also compare with Codex model used in a few-shot setting (more details in Appendix A), and find that on the much harder filtered dataset, a 2.7B GPT-Neo model finetuned with our methods obtains much better PASS@1 but with lower PASS@80. By inspecting the output from Codex, we discover that its outputs are much more diverse than finetuned models, which contributes to a higher PASS@80 even under the few-shot setting. Comparison with previous work on the GSM dataset is in Appendix B due to limited space.</p>
<h2>5 LIMITATIONS AND Future Work</h2>
<p>More general definition of (partial) correctness. In this work, we define partial correctness based on state-based solution equivalence. This is a conservative way for defining solution equivalence as it requires exact match of the sets of variable values, but a solution could be partially correct and yet, not have an exact match of variable values because some of these values may not needed for future computation. In the future, we want to explore ways to relax this restriction that will help us find more partially correct solutions in an efficient manner. Besides, our partial correctness definition requires the existence of at least one fully-correct solution and when such reference solution is</p>
<p>not available from the dataset (i.e., in a weakly-supervised setting), we would need to first sample an FCS that matches the gold execution result to begin with. In addition, we simply use the matching of execution results to define correctness, which is susceptible to spurious solutions that achieves the correct result by coincidence. For math reasoning, we find such spurious solutions to be quite rare ${ }^{6}$, as the correct answer is typically numeric which is less likely for a semantically wrong solution to obtain the correct answer by chance. But methods as Zhong et al. (2020); Chen et al. (2022) may be needed for this definition of correctness to be more robust on other domains.</p>
<p>Towards generating general programs. While we focus on the domain of generating solutions for math reasoning in this work, here we reflect on how our method can be applied to program synthesis in general. However, general programs might contain complex structures such as conditions (e.g., if-else) or loops (e.g., while-do) as opposed to straight-line programs in the mathreasoning domain. Dealing with these complex structures poses additional challenges because most neural program synthesis models perform left-to-right auto-regressive generation, and the changes to the control flow break the alignment between program generation and program execution (Chen et al., 2018; 2021b; Nye et al., 2021). There are two potential ways to extend our technique to address the problem. First, we can treat a branch or a loop as an atomic unit (i.e., a block whose state is the state after executing all statements within it), then we can apply state-based equivalence in the same way. Second, because our technique only requires execution after the full programs are generated, we can still evaluate and compare program states based on intermediate states.</p>
<h1>6 Related Work</h1>
<p>Weakly-supervised semantic parsing. Many previous work in learning semantic parsers from weak supervision follows the same process of sampling programs and maximizing the probability of the correct ones (Krishnamurthy et al., 2017; Guu et al., 2017; Min et al., 2019; Ni et al., 2020). Our work differs as our tasks contain one reference solution for each task as opposed to only the final answer like weakly-supervised semantic parsing tasks. Thus, our work leverages the reference solution for sampling and defines partial correctness based on known solutions. Because of the problem setup difference, we found that the conclusions in Guu et al. (2017) about loss functions do not generalize to our case.</p>
<p>Execution-guided code generation. Our work relates to execution-guided code generation as we leverage intermediate states of math solutions to guide the sampling process. In code generation literature, intermediate program execution states are used to prune the search space (Liang et al., 2017; Wang et al., 2018; Li et al., 2022) or condition further generation on the execution states(Chen et al., 2018; Ellis et al., 2019; Nye et al., 2020; Chen et al., 2021b; Nye et al., 2021). The key difference of these methods from ours is that they require doing both decoding and execution at inference time, while our work only uses execution during training, which reduces decoding overhead.</p>
<p>Learning from partial reward for program synthesis. There are parallels between multi-target learning and the reinforcement learning setting with sparse rewards for generating programs (Liang et al., 2017; 2018; Simmons-Edler et al., 2018; Bunel et al., 2018; Agarwal et al., 2019). Similarly, our approach of identifying partial correctness of solutions is similar to partial rewards. But instead of discounting an entire trajectory with a low reward as in RL, we truncate the solution to a partiallycorrect prefix and assign it the "full reward", which is a main contribution of this work.</p>
<h2>7 CONCLUSION</h2>
<p>We propose to let pretrained language models sample additional solutions for each problem and learn from the self-sampled solutions that are correct or partially-correct. We define partial correctness by tracing and matching intermediate execution states. We experiment on different math reasoning tasks and show that such partially-correct solutions can help more efficient exploration of the solution space and provide useful learning signal, which improves the PASS@ $k$ performance. Overall, our proposed method can improve PASS@ $k$ from $3.1 \%$ to $12.3 \%$ compared to learning from a single solution with MLE.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>The authors would like to thank Jackson Woodruff, Pengcheng Yin, and the anonymous reviewers for the useful discussion and comments.</p>
<h2>REFERENCES</h2>
<p>Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. Learning to generalize from sparse and underspecified rewards. In International Conference on Machine Learning, pp. 130-140. PMLR, 2019.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging grammar and reinforcement learning for neural program synthesis. In International Conference on Learning Representations, 2018.</p>
<p>Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a.</p>
<p>Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2018.</p>
<p>Xinyun Chen, Dawn Song, and Yuandong Tian. Latent execution for neural program synthesis beyond domain-specific languages. Advances in Neural Information Processing Systems, 34, 2021b.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.</p>
<p>Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama. Write, execute, assess: Program synthesis with a repl. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Catherine Finegan-Dollak, Jonathan K Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir R Radev. Improving text-to-sql evaluation methodology. In ACL, 2018.</p>
<p>Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, and Percy Liang. From language to programs: Bridging reinforcement learning and maximum marginal likelihood. In ACL, 2017.</p>
<p>Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. Neural semantic parsing with type constraints for semi-structured tables. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1516-1526, 2017.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022.</p>
<p>Chen Liang, Jonathan Berant, Quoc Le, Kenneth Forbus, and Ni Lao. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In ACL, pp. 23-33, 2017.</p>
<p>Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V Le, and Ni Lao. Memory augmented policy optimization for program synthesis and semantic parsing. Advances in Neural Information Processing Systems, 31, 2018.</p>
<p>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. A discrete hard em approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2851-2864, 2019.</p>
<p>Ansong Ni, Pengcheng Yin, and Graham Neubig. Merging weak and active supervision for semantic parsing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 85368543, 2020.</p>
<p>Maxwell Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua B Tenenbaum, and Armando Solar-Lezama. Representing partial programs with blended abstract semantics. In International Conference on Learning Representations, 2020.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.</p>
<p>Tal Schuster, Ashwin Kalyan, Alex Polozov, and Adam Tauman Kalai. Programming puzzles. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.</p>
<p>Riley Simmons-Edler, Anders Miltner, and Sebastian Seung. Program synthesis through reinforcement learning guided tree search. arXiv preprint arXiv:1806.02932, 2018.</p>
<p>Chenglong Wang, Kedar Tatwawadi, Marc Brockschmidt, Po-Sen Huang, Yi Mao, Oleksandr Polozov, and Rishabh Singh. Robust text-to-sql generation with execution-guided decoding. arXiv preprint arXiv:1807.03100, 2018.</p>
<p>Ruiqi Zhong, Tao Yu, and Dan Klein. Semantic evaluation for text-to-sql with distilled test suites. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 396-411, 2020.</p>
<h1>Appendix</h1>
<h2>A EXPERIMENT SETTING DETAILS</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: right;">MathQA</th>
<th style="text-align: right;">GSM8K.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Training Steps</td>
<td style="text-align: right;">50 K</td>
<td style="text-align: right;">25 K</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate (LR)</td>
<td style="text-align: right;">$1.0 \mathrm{e}-4$</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: right;">AdamW</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Adam Betas</td>
<td style="text-align: right;">$(0.9,0.999)$</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Adam Eps</td>
<td style="text-align: right;">$1.0 \mathrm{e}-8$</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Weight Decay</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">LR Scheduler</td>
<td style="text-align: right;">Linear w/ Warmup</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># LR Warm-up Steps</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Effective Batch Size</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">FP Precision</td>
<td style="text-align: right;">FP 32 for 125M, FP16 for 2.7B</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Gradient Clipping</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Table 3: The hyperparameters used for model training on two different types of datasets.</p>
<p>Hyperparameters. All hyperparameters for training is shown in Tab. 3. We use $\beta=0.25$ in the experiments with $\beta$-MML, as a result of enumeration search among the values of ${0.1,0.25,0.5,0.9}$. We use the default AdamW optimizer settings and slightly tuned the learning rate by trying out several values between $1.0 \mathrm{e}-3$ and $1.0 \mathrm{e}-5$. The difference in floating point precision is to fit the GPT-Neo 2.7B model into the memory of the GPUs. All experiments are conducted on V100-32GB GPUs.</p>
<p>PASS@ $k$ evaluation. We use temperature sampling and sample $n$ solutions with $T=0.8$, where $n=80$ for MathQA and $n=100$ for GSM to evaluate PASS@ $n$, to be maximally consistent with previous work (Austin et al., 2021; Cobbe et al., 2021; Chowdhery et al., 2022). We also report PASS@ ${5,10,20,50}$ using the $n$ samples and the unbiased estimator proposed in Chen et al. (2021a). We use $T=0.2$ to sample 1 solution per specification and evaluate PASS@1.</p>
<p>Codex few-shot settings. We estimate the Codex (Chen et al., 2021a) performance under the few-shot settings. More specifically, the prompt consists of a natural language task description "# Generate Python code to solve the following math word problems:" and four examples, following previous work (Chowdhery et al., 2022). Each example consists of the NL specification as a one-line comment and the gold program solutions. We evaluate PASS@ $k$ for Codex using the same sampling methods as above.</p>
<p>Details for self-sampling. During a training step, we sample one solution ${ }^{7}$ for each task (i.e., natural language problem) in the batch, i.e., $|\hat{Y}|=1$ in Alg. 1 and Alg. 2. Thus for each gradient update, we first compute the loss for each task based on the saved solutions in the buffer and loss functions described in Tab. 1, then it is averaged across the 32 tasks in the batch. Note that the total number of samples we generate per task throughout training is also scaled up by the number of training epochs, which is 235 for MathQA-Python-Filtered, 83 for MathQA-Python and 145 for GSM5.5K-Python. For sampling temperature, we use the same setting as inference time, with $T=0.8$.</p>
<h2>B ADDITIONAL EXPERIMENT RESULTS</h2>
<p>Comparing GSM performance with previous work. Here we compare our method with previous work on the original test sets of GSM8K. The results are shown as Tab. 4. On GSM8K, some of the prior works are evaluated on a different format of NL inputs than ours, so they are not directly comparable, but we still include them to help better position the performance of our methods. We test Codex using the same input in a few-shot setting, and we find that similar with the</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">PASS@1</th>
<th style="text-align: left;">PASS@100</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Previous work:</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">OpenAI 6B ${ }^{\star \text { (Cobbe et al., 2021) }}$</td>
<td style="text-align: left;">21.8</td>
<td style="text-align: left;">70.9</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-Coder 540B ${ }^{\star \text { (Chowdhery et al., 2022) }}$</td>
<td style="text-align: left;">$\mathbf{5 0 . 9}$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">LaMDA 137B ${ }^{\star \star \text { (Chowdhery et al., 2022) }}$</td>
<td style="text-align: left;">7.6</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Codex Cushman ${ }^{\dagger}$ (Chen et al., 2021a)</td>
<td style="text-align: left;">5.0</td>
<td style="text-align: left;">58.0</td>
</tr>
<tr>
<td style="text-align: left;">Codex Davinci ${ }^{\dagger}$ (Chen et al., 2021a)</td>
<td style="text-align: left;">$\underline{17.0}$</td>
<td style="text-align: left;">$\underline{\mathbf{7 1 . 0}}$</td>
</tr>
<tr>
<td style="text-align: left;">Ours:</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GPT-Neo 2.7B w/ self-sampling FCS + PCS</td>
<td style="text-align: left;">19.5</td>
<td style="text-align: left;">41.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Compare with previous methods on the original test set of GSM8K dataset. ${ }^{*}$ : model not pretrained on code. ${ }^{\dagger}$ : few-shot learning results. ${ }^{\text {® }}$ : different setting from ours ${ }^{8}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Self-Sampling</th>
<th style="text-align: center;">Loss Func.</th>
<th style="text-align: center;"># Sols. in $\mathcal{B}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PASS@k(\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FCS</td>
<td style="text-align: center;">PCS</td>
<td style="text-align: center;">$k=1$</td>
<td style="text-align: center;">$k=5$</td>
<td style="text-align: center;">$k=10$</td>
<td style="text-align: center;">$k=20$</td>
<td style="text-align: center;">$k=50$</td>
<td style="text-align: center;">$k=100$</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">MLE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">22.7</td>
</tr>
<tr>
<td style="text-align: center;">FCS only</td>
<td style="text-align: center;">MML</td>
<td style="text-align: center;">1.48</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">23.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MLE-Aug</td>
<td style="text-align: center;">1.76</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\beta$-MML</td>
<td style="text-align: center;">1.57</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">27.3</td>
</tr>
<tr>
<td style="text-align: center;">FCS + PCS</td>
<td style="text-align: center;">MML</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">1.10</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">18.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MLE-Aug</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">35.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\beta$-MML</td>
<td style="text-align: center;">1.62</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">27.9</td>
</tr>
</tbody>
</table>
<p>Table 5: Full comparison of various loss functions (§ 3.2) with different self-sampling strategies. Results are on the dev set of GSM5.5K-Python with GPT-Neo 125M as the base model. Best performance within the same category is in bold and ones worse than MLE is underlined. $\beta=0.25$ for $\beta$-MML.
result on MathQA in Tab. 2, our method achieves better PASS@1 while being significantly worse in PASS@100 compared with Codex. We hypothesize that as Codex model is used tested few-shot setting and not finetuned, it does not suffer from the overfitting issue we mentioned. This leads to great diversity but poor accuracy during generation. However, due to the little information we have about Codex (e.g., model size, training data), it is hard to derive any further conclusion.</p>
<p>Ablation results on loss functions. Here we show the full results on the ablation of loss functions in Tab. 5. We can see that trends observed from PASS@100 in Fig. 3 are consistent with other PASS@ $k$ results, as MLE-Aug loss beats other two loss functions on all PASS@ $k$. And using MML loss when adding PCSs for learning results in worse performance than MLE for PASS@1 as well. Moreover, from the number of FCSs and PCSs saved in the buffer $\mathcal{B}$, we can also observe that using MLE-Aug loss results in more FCSs and PCSs being saved, thus further encourages diversity in generation.</p>
<h1>C Full Learning Algorithm with Partial Correctness</h1>
<p>Our general learning framework in shown as Alg. 1 and it is further extended in $\S 3.3$. Here we show a complete version of the algorithm with using partially-correct solutions in Alg. 3. Additionally, here are the detailed explanation of the data structure and functions used in it:
$\triangleright$ Mapping $\mathcal{M}$ : This is a data structure that maps an intermediate state to a set of solution (prefixes) that execute to that state, i.e., $\mathcal{M}: \mathcal{S} \rightarrow \mathcal{S}^{m}$. In this mapping, we save all PCSs and their intermediate states, including all prefixes of any PCS. We use this to significantly speed up the lookup process as mentioned in § 3.3.2;
$\triangleright$ Function PartialCorrectnessCriteria $\left(s_{i}, \mathcal{M}\right)$ : Since all states for all known PCSs are saved in $\mathcal{M}$, to know whether a prefix $\hat{y}_{\leq i}$ is partially-correct, we only need to check if its state matches any</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code>Algorithm <span class="mi">3</span> Training Update <span class="k">with</span> Partially Correctness
    Initialize<span class="p">:</span> <span class="p">(</span>only once before training starts<span class="p">)</span>
        Solutions buffer <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>B<span class="p">}</span><span class="o">=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span>y<span class="err">^</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span> y<span class="err">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span> <span class="k">with</span> an empty <span class="ow">and</span> the reference solution
        Reference solution states <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="o">*</span><span class="p">},</span> s_<span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="o">*</span><span class="p">},</span> <span class="err">\</span>ldots<span class="p">,</span> s_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> where <span class="err">\</span><span class="p">(</span>s_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="o">=</span><span class="err">\</span>mathcal<span class="p">{</span>T<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>y_<span class="p">{</span><span class="err">\</span>leq i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
        State-prefixes mapping <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>M<span class="p">}</span><span class="o">=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span>s_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span> <span class="err">\</span>rightarrow<span class="err">\</span>left<span class="err">\</span><span class="p">{</span>y_<span class="p">{</span><span class="err">\</span>leq i<span class="p">}</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span>_<span class="p">{</span><span class="ss">i</span><span class="o">=</span><span class="mi">1</span><span class="p">}</span><span class="err">^</span><span class="p">{</span>t<span class="p">}</span><span class="err">\</span><span class="p">)</span>
    Input<span class="p">:</span>
        Parameterized model <span class="err">\</span><span class="p">(</span>P_<span class="p">{</span><span class="err">\</span>theta<span class="p">}(</span>y <span class="err">\</span>mid x<span class="p">)</span><span class="err">\</span><span class="p">)</span>
        A training example <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span>x<span class="p">,</span> y<span class="err">^</span><span class="p">{</span><span class="o">*</span><span class="p">},</span> z<span class="err">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
        Tracing function <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>T<span class="p">}:</span> <span class="err">\</span>mathcal<span class="p">{</span>Y<span class="p">}</span> <span class="err">\</span>rightarrow <span class="err">\</span>mathcal<span class="p">{</span>S<span class="p">}</span><span class="err">\</span><span class="p">)</span> to obtain intermediate states
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>hat<span class="p">{</span>Y<span class="p">}</span> <span class="err">\</span>leftarrow<span class="err">\</span><span class="p">)</span> SampleSolutions <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span>x<span class="p">,</span> P_<span class="p">{</span><span class="err">\</span>theta<span class="p">},</span> <span class="err">\</span>mathcal<span class="p">{</span>B<span class="p">}</span><span class="err">\</span>right<span class="p">)</span> <span class="o">/</span> <span class="o">*</span><span class="err">\</span><span class="p">)</span> call Alg<span class="o">.</span> <span class="err">\</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="o">/</span><span class="err">\</span><span class="p">)</span>
    for <span class="err">\</span><span class="p">(</span><span class="err">\</span>hat<span class="p">{</span>y<span class="p">}</span><span class="err">\</span><span class="p">)</span> <span class="k">in</span> <span class="err">\</span><span class="p">(</span>Y<span class="err">\</span><span class="p">)</span> do
        for <span class="err">\</span><span class="p">(</span>i <span class="err">\</span>leftarrow<span class="err">|\</span>hat<span class="p">{</span>y<span class="p">}</span><span class="err">|</span> <span class="p">;</span> i <span class="err">\</span>neq <span class="mi">0</span> <span class="p">;</span> i <span class="err">\</span>leftarrow i-1<span class="err">\</span><span class="p">)</span> do
            <span class="err">\</span><span class="p">(</span>s_<span class="p">{</span>i<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>T<span class="p">}</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>hat<span class="p">{</span>y<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>leq i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span> <span class="o">/</span> <span class="o">*</span><span class="err">\</span><span class="p">)</span> get intermediate state for each solution prefix <span class="err">\</span><span class="p">(</span><span class="err">\</span>hat<span class="p">{</span>y<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>leq i<span class="p">}</span> <span class="o">*</span> <span class="o">/</span><span class="err">\</span><span class="p">)</span>
            <span class="k">if</span> PartialCorrectnessCriteria <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>mathcal<span class="p">{</span>M<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> <span class="k">then</span>
                <span class="err">\</span><span class="p">(</span>Y_<span class="p">{</span>S<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>M<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span> <span class="o">/</span> <span class="o">*</span><span class="err">\</span><span class="p">)</span> get existing prefixes that executes to state <span class="err">\</span><span class="p">(</span>s_<span class="p">{</span>i<span class="p">}</span> <span class="o">*</span> <span class="o">/</span><span class="err">\</span><span class="p">)</span>
                <span class="k">if</span> not isDuplicate <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>hat<span class="p">{</span>y<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>leq i<span class="p">},</span> Y_<span class="p">{</span>S<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> <span class="k">then</span>
                    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>B<span class="p">}</span> <span class="err">\</span>leftarrow<span class="err">\</span><span class="p">)</span> updateBuffer <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>hat<span class="p">{</span>y<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>leq i<span class="p">},</span> <span class="err">\</span>mathcal<span class="p">{</span>B<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
                    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>M<span class="p">}</span> <span class="err">\</span>leftarrow<span class="err">\</span><span class="p">)</span> updateMapping <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>hat<span class="p">{</span>y<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>leq i<span class="p">},</span> <span class="err">\</span>mathcal<span class="p">{</span>M<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
                    end <span class="k">if</span>
                    continue <span class="err">\</span><span class="p">(</span><span class="o">/</span> <span class="o">*</span><span class="err">\</span><span class="p">)</span> we only need the longest matching prefix <span class="err">\</span><span class="p">(</span><span class="o">*</span> <span class="o">/</span><span class="err">\</span><span class="p">)</span>
                    end <span class="k">if</span>
        end for
    end for
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>theta <span class="err">\</span>stackrel<span class="p">{</span><span class="err">\</span>text <span class="p">{</span> update <span class="p">}}{</span><span class="err">\</span>longleftarrow<span class="p">}</span> <span class="err">\</span>nabla_<span class="p">{</span><span class="err">\</span>theta<span class="p">}</span> <span class="err">\</span>mathcal<span class="p">{</span>L<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>x<span class="p">,</span> <span class="err">\</span>mathcal<span class="p">{</span>B<span class="p">},</span> P_<span class="p">{</span><span class="err">\</span>theta<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>of the known states for a PCS, i.e., if $s_{i} \in \mathcal{M}$;
$\triangleright$ Function isDuplicate $\left(\hat{y}<em S="S">{\leq i}, Y</em>}\right)$ : As mentioned in $\S 3.3 .2$, we use AST and length constraint to rule out "trivial variants" and identify new PCSs to save in the buffer $\mathcal{B}$. Here the solutions to compare are the set of solutions $Y_{S}$ that reaches the same intermediate state, i.e., being state-based equivalent; $\triangleright$ Function updateBuffer $\left(\hat{y<em _leq="\leq" i="i">{\leq i}, \mathcal{B}\right)$ : Here we not only need to add the new PCS into the buffer $\mathcal{B}$, but also need to prune out the saved solutions that are prefix of $\hat{y}</em>$;
$\triangleright$ Function updateMapping $\left(\hat{y}<em _leq="\leq" i="i">{\leq i}, \mathcal{M}\right)$ : Here we need to save the states of all prefixes of an identified partially-correct solution, thus we will loop through all prefixes of $\hat{y}</em>$.}$ and obtain its execution state, then update $\mathcal{M}$ accordingly. As mentioned above, existing PCSs may be a prefix of the new PCS, so we also need to prune out such existing PCSs from mapping $\mathcal{M</p>
<h1>D Qualitative Analysis</h1>
<p>In Tab. 6, we show more examples of the fully-correct and partially-correct solutions that the models found during self-sampling, from both the MathQA and GSM datasets. First, we can see that for some NL problems, it is possible that no FCS or PCS can be found with self-sampling, as in MathQA-Example-1 and MathQA-Example-1. Take MathQA-Example-2 as an example, the question is quite straightforward thus it leaves very little room for the existence of other correct solutions, as the reference solution is already very short. Moreover, we can also observe that the ways self-sampled FCS and PCS differ from the reference solution vary a lot. In MathQA-Example-2, GSM-Example-1 and GSM-Example-2 the sampled FCSs complete the task with very different paths compared with the reference solution, and actually result in using fewer lines of code. Another way of getting FCS or PCS is to perform small and local perturbations, e.g., switch the two sides of a addition or re-order the two non-dependent statements, as shown in other examples. We find that these local perturbations are more common in general in both datasets, as such patterns are easier for the model to learn.</p>
<h2>E Tracking Training Progress</h2>
<p>Learning from self-sampled solutions mitigates overfitting. Here we shown the PASS@ $k$ performance curve with respect to the training process in Fig. 6. From the curves, we can observe that for MLE, while PASS@1 and PASS@5 generally improves during training, other PASS@ $k$ for</p>
<table>
<thead>
<tr>
<th style="text-align: center;">NL Problem Descriptions</th>
<th style="text-align: center;">Ref. Solution</th>
<th style="text-align: center;">Self-Sampled FCS</th>
<th style="text-align: center;">Self-Sampled PCS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(MathQA-Example-1): <br> The charge for a single room at hotel P is 70 percent less than the charge for a single room at hotel R and 10 percent less than the charge for a single room at hotel G. The charge for a single room at hotel R is what percent greater than the charge for a single room at hotel G ?</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{n} 0=70.0 \ &amp; \mathrm{n} 1=10.0 \ &amp; \mathrm{t} 0=100.0-\mathrm{n} 0 \ &amp; \mathrm{t} 1=100.0-\mathrm{n} 1 \ &amp; \mathrm{t} 2=\mathrm{t} 0 / \mathrm{t} 1 \ &amp; \mathrm{t} 3=\mathrm{t} 2 * 100.0 \ &amp; \mathrm{t} 4=100.0-\mathrm{t} 3 \ &amp; \mathrm{t} 5=\mathrm{t} 4 / \mathrm{t} 3 \ &amp; \text { answer }=\mathrm{t} 5 * 100.0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{n} 0=70.0 \ &amp; \mathrm{n} 1=10.0 \ &amp; \mathrm{t} 0=100.0-\mathrm{n} 1 \ &amp; \mathrm{t} 1=100.0-\mathrm{n} 0 \ &amp; \mathrm{t} 2=\mathrm{t} 0 / \mathrm{t} 1 \ &amp; \mathrm{t} 3=\mathrm{t} 2 * 100.0 \ &amp; \text { answer }=\mathrm{t} 3-100.0 \end{aligned}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">(MathQA-Example-2): <br> If john runs in the speed of $9 \mathrm{~km} / \mathrm{hr}$ from his house, in what time will he reach the park which is 300 m long from his house?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{n} 0=9.0$ <br> $\mathrm{n} 1=300.0$ <br> $\mathrm{t} 0=\mathrm{n} 0 * 1000.0$ <br> $\mathrm{t} 1=\mathrm{n} 1 / \mathrm{t} 0$ <br> answer $=\mathrm{t} 1 * 60.0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(MathQA-Example-3): <br> A class consists of 15 biology students and 10 chemistry students. If you pick two students at the same time, what's the probability that one is maths and one is chemistry?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{n} 0=15.0$ <br> $\mathrm{n} 1=10.0$ <br> $\mathrm{t} 0=\mathrm{n} 0+\mathrm{n} 1$ <br> $\mathrm{t} 1=\mathrm{n} 0 / \mathrm{t} 0$ <br> $\mathrm{t} 2=\mathrm{n} 1 / \mathrm{t} 0$ <br> $\mathrm{t} 3=\mathrm{t} 0-1.0$ <br> $\mathrm{t} 4=\mathrm{n} 1 / \mathrm{t} 3$ <br> $\mathrm{t} 5=\mathrm{n} 0 / \mathrm{t} 3$ <br> $\mathrm{t} 6=\mathrm{t} 1 * \mathrm{t} 4$ <br> $\mathrm{t} 7=\mathrm{t} 5 * \mathrm{t} 2$ <br> answer $=\mathrm{t} 6+\mathrm{t} 7$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{n} 0=15.0 \ &amp; \mathrm{n} 1=10.0 \ &amp; \mathrm{t} 0=\mathrm{n} 0+\mathrm{n} 1 \ &amp; \mathrm{t} 1=\mathrm{n} 0 / \mathrm{t} 0 \ &amp; \mathrm{t} 2=\mathrm{n} 1 / \mathrm{t} 0 \ &amp; \mathrm{t} 3=\mathrm{t} 0-1.0 \ &amp; \mathrm{t} 4=\mathrm{n} 1 / \mathrm{t} 3 \ &amp; \mathrm{t} 5=\mathrm{n} 0 / \mathrm{t} 3 \ &amp; \mathrm{t} 6=\mathrm{t} 1 * \mathrm{t} 4 \ &amp; \mathrm{t} 7=\mathrm{t} 5 * \mathrm{t} 2 \ &amp; \text { answer }=\mathrm{t} 7+\mathrm{t} 6 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{n} 0=15.0 \ &amp; \mathrm{n} 1=10.0 \ &amp; \mathrm{t} 0=\mathrm{n} 0+\mathrm{n} 1 \ &amp; \mathrm{t} 1=\mathrm{n} 0 / \mathrm{t} 0 \ &amp; \mathrm{t} 2=\mathrm{n} 1 / \mathrm{t} 0 \ &amp; \mathrm{t} 3=\mathrm{t} 0-1.0 \ &amp; \mathrm{t} 4=\mathrm{n} 1 / \mathrm{t} 3 \ &amp; \mathrm{t} 5=\mathrm{n} 0 / \mathrm{t} 3 \ &amp; \mathrm{t} 6=\mathrm{t} 1 * \mathrm{t} 4 \ &amp; \mathrm{t} 7=\mathrm{t} 5 * \mathrm{t} 2 \ &amp; \text { answer }=\mathrm{t} 7+\mathrm{t} 6 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">(GSM-Example-1): <br> Ellie has found an old bicycle in a field and thinks it just needs some oil to work well again. She needs 10 ml of oil to fix each wheel and will need another 5 ml of oil to fix the rest of the bike. How much oil does she need in total to fix the bike?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(GSM-Example-2): <br> There is very little car traffic on Happy Street. During the week, most cars pass it on Tuesday - 25. On Monday, $20 \%$ less than on Tuesday, and on Wednesday, 2 more cars than on Monday. On Thursday and Friday, it is about 10 cars each day. On the weekend, traffic drops to 5 cars per day. How many cars travel down Happy Street from Monday through Sunday?</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{n} 0=20 \ &amp; \mathrm{n} 1=100 \ &amp; \mathrm{n} 2=25 \ &amp; \mathrm{n} 3=2 \ &amp; \mathrm{n} 4=10 \ &amp; \mathrm{t} 0=\mathrm{n} 0 / \mathrm{n} 1 * \mathrm{n} 2 \ &amp; \mathrm{t} 1=\mathrm{n} 2-\mathrm{t} 0 \ &amp; \mathrm{t} 2=\mathrm{t} 1+\mathrm{n} 3 \ &amp; \mathrm{t} 3=\mathrm{n} 4 * \mathrm{n} 3 \ &amp; \mathrm{t} 4=\mathrm{t} 0 * \mathrm{n} 3 \ &amp; \text { answer }=\mathrm{t} 3+\mathrm{n} 2 \ &amp; \backslash \ &amp; +t 2+t 3+t 4 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{n} 0=25 \ &amp; \mathrm{n} 1=2 \ &amp; \mathrm{n} 2=20 \ &amp; \mathrm{n} 3=100 \ &amp; \mathrm{n} 4=10 \ &amp; \mathrm{t} 0=\mathrm{n} 0-\mathrm{n} 1 \ &amp; \mathrm{t} 1=\mathrm{n} 2 / \mathrm{n} 3 * \mathrm{n} 0 \ &amp; \mathrm{t} 2=\mathrm{t} 0-\mathrm{t} 1 \ &amp; \mathrm{t} 3=\mathrm{t} 2+\mathrm{n} 4 \ &amp; \mathrm{t} 4=\mathrm{n} 0-\mathrm{t} 3 \ &amp; \text { answer }=\mathrm{t} 4+\mathrm{n} 3 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{n} 0=25 \ &amp; \mathrm{n} 1=25 \ &amp; \mathrm{n} 2=20 \ &amp; \mathrm{n} 3=100 \ &amp; \mathrm{n} 4=10 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 6: More examples of self-sampled fully-correct (FCS) and partially-correct solutions (PCSs). "MathQA" denotes the MathQA-Python-Filtered dataset and "GSM" denotes the GSM5.5K-Python dataset. All solutions are from the final buffer after training a GPT-Neo 2.7B model, while learning from self-sampled FCS+PCS with the MLE-Aug loss.
higher $k$ actually decreases after reaching the peak performance in early epochs, which is consistent with previous findings (Cobbe et al., 2021). This is due to overfitting: in the early stage of training, the model is less confident about its predictions thus the sampled $k$ solutions are very diverse, and while training continues, it overfits to the one reference solution provided for learning thus leads to poor generalization when evaluated by PASS@ $k$ with high $k$ values. Fig. 6 also shows</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: How PASS@k on the dev set evolve during training. Results shown on GSM5.5K-Python dataset with GPT-Neo 125M model. Exponential moving average smoothing is applied for more clarity, but original curve is shown in shade.
how our proposed self-sampling method can mitigate the overfitting problem, as it keeps improving or maintaining PASS@ ${5,10,20}$ while such performances start decreasing for MLE. Though it also shows improvements for PASS@ ${50,100}$, but the performance still decreases in later training stages. Here we can also see the importance of suitable learning objective, as MML has almost no effect in mitigating such overfitting issue.</p>
<p>Early stopping is needed when prioritizing high $k$ value for PASS@k. In our experiments, we select the model checkpoint with the best PASS@1 performance to evaluate all PASS@ $k$. This setup aims to choose the best model that can solve the task with a small number of attempts (which corresponds to smaller $k$ value), as studied in (Austin et al., 2021). We can also observe that with our methods, the best PASS@ 1 checkpoint also yields the best or close to the best PASS@ ${5,10,20}$ performances. However, in certain applications where large number of attempts are allowed, PASS@ $k$ with high $k$ values should be prioritized. An example is to generate candidate solutions before reranking (Cobbe et al., 2021). In this case, an earlier checkpoint (e.g., one with best PASS@100) should be used instead, which is not the best checkpoint for PASS@ $k$ where $k$ is small. Also note that our proposed method are not suitable for these applications, as we observe no improvement on the peak PASS@ ${50,100}$ performances. We think this because when such peak performance is reached, it is still in the early stage of training thus not many FCSs or PCSs have been saved in the buffer yet.</p>
<p>Partially-correct solutions help in early training stages. To show how self-sampling effects training, in Fig. 7a we show how the size of the buffer progresses during training. From the curves, we can see that in the early training stages (i.e., first 5 k steps), the number of saved PCSs rapidly grows while the number of FCSs only slightly increases. In later stages of training, the growth of buffer size is mainly contributed by more FCSs being sampled and saved while the number of</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) Growth of the number of saved FCS and PCS during training. # FCS includes the gold solutions.
(b) Distribution of the characterization of self-sampled solutions during training.</p>
<p>Figure 7: How self-sampling evolves throughout the training process. Results shown as training the GPT-Neo 125M model on the GSM5.5K-Python dataset with MLE-Aug loss.</p>
<p>PCSs stays steady. Also when compared to learning only with FCSs, learning with FCSs + PCSs eventually accumulates more FCSs in the buffer (green dotted line vs yellow dotted line). In addition, we show how the distribution of the outcomes of self-sampled solutions changes throughout training in Fig. 7b. We can see that in the early training stages, the ratio of not executable/incorrect solutions quickly drops to almost zero. At the same time, the ratio of new FCS or PCS being saved reaches the peak. As training proceeds, the models are mostly sampling known FCS or PCS as the size of the buffer converges as well. But the number of self-sampled fully-correct solutions gradually overtakes the partially-correct ones.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ Natural language explanations of the solutions are used as input and the few-shot exemplars are not in the same format as ours.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
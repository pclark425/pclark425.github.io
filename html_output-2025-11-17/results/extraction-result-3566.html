<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3566 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3566</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3566</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-15586fec836ffcb60eba81491e04c225e6914aac</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/15586fec836ffcb60eba81491e04c225e6914aac" target="_blank">Protein structure generation via folding diffusion</a></p>
                <p><strong>Paper Venue:</strong> Nature Communications</p>
                <p><strong>Paper TL;DR:</strong> This work presents a diffusion-based generative model that generates protein backbone structures via a procedure inspired by the natural folding process, and demonstrates that the resulting model unconditionally generates highly realistic protein structures with complexity and structural patterns akin to those of naturally-occurring proteins.</p>
                <p><strong>Paper Abstract:</strong> The ability to computationally generate novel yet physically foldable protein structures could lead to new biological discoveries and new treatments targeting yet incurable diseases. Despite recent advances in protein structure prediction, directly generating diverse, novel protein structures from neural networks remains difficult. In this work, we present a diffusion-based generative model that generates protein backbone structures via a procedure inspired by the natural folding process. We describe a protein backbone structure as a sequence of angles capturing the relative orientation of the constituent backbone atoms, and generate structures by denoising from a random, unfolded state towards a stable folded structure. Not only does this mirror how proteins natively twist into energetically favorable conformations, the inherent shift and rotational invariance of this representation crucially alleviates the need for more complex equivariant networks. We train a denoising diffusion probabilistic model with a simple transformer backbone and demonstrate that our resulting model unconditionally generates highly realistic protein structures with complexity and structural patterns akin to those of naturally-occurring proteins. As a useful resource, we release an open-source codebase and trained models for protein structure diffusion.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3566.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3566.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESM-IF1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ESM-IF1 (inverse folding model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based inverse folding model (derived from the ESM protein language model family) used to predict amino acid sequences that fold to a given backbone structure; cited here as a comparative inverse-folding method used in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning inverse folding from millions of predicted structures</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ESM-IF1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Refered to as an inverse-folding model based on ESM protein language models (transformer architecture). The paper does not report model size or training hyperparameters; it cites Hsu et al. (2022) as the source.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Inverse folding: generate candidate amino acid sequences conditioned on a target backbone (given structure-to-sequence prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Protein sequence design / de novo protein design (assessing designability of generated backbones).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Self-consistency TM score (scTM): TM-align score between the original generated backbone and the 3D structure predicted from sequences produced by the inverse-folding model (here folded with OmegaFold or AlphaFold without MSAs). A scTM >= 0.5 is considered designable (same fold).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an alternative inverse-folding model used in other work (Lee & Kim 2022) where 10 candidate sequences per structure were generated. Within the paper's evaluation context, the authors state ProteinMPNN yields improved performance compared to ESM-IF1 (referencing their Appendix Tables S1,S2), i.e., ESM-IF1 produced weaker scTM results in their comparisons (no detailed per-model numbers for ESM-IF1 are reported in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared (in the paper's appendices/discussion) against ProteinMPNN in the scTM pipeline; ProteinMPNN produced stronger scTM values in the authors' experiments. The paper also cites other pipelines (e.g., ProteinSGM + Rosetta) for context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>The paper does not provide detailed failure modes specific to ESM-IF1 beyond reporting it gives weaker scTM performance than ProteinMPNN in their comparative evaluation; no model sizes or training details are given here, so limitations specific to ESM-IF1 must be obtained from the original ESM-IF1 paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Protein structure generation via folding diffusion', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3566.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3566.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProteinMPNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProteinMPNN (protein sequence design model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep-learning inverse-folding model used to generate amino acid sequences conditioned on backbone structures; used in this paper's evaluation pipeline to test whether generated backbones are designable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust deep learningbased protein sequence design using proteinmpnn</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProteinMPNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A deep learning model for inverse folding (sequence design conditioned on a backbone), cited from Dauparas et al. (2022). The paper uses ProteinMPNN in Cα-only mode to generate candidate sequences; specific architecture/size details are not provided in this manuscript beyond that it is an inverse-folding model that outperforms ESM-IF1 in their pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Inverse folding: for each generated backbone, ProteinMPNN is used to produce multiple (the authors use 8) candidate amino acid sequences predicted to fit the backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Protein design / validation of generative protein backbones (assessing 'designability' of novel backbone geometries).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Self-consistency TM score (scTM): for each generated backbone s, ProteinMPNN produces 8 candidate sequences; each sequence is folded (with OmegaFold or AlphaFold without MSAs) to produce predicted structures \hat{s}_i; scTM = max_i TM-align(s, \hat{s}_i). A scTM >= 0.5 indicates the sequence folds into the same fold and the backbone is considered designable.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used as the inverse-folding model in the paper's scTM pipeline: ProteinMPNN (generating 8 sequences per backbone) together with OmegaFold folding yielded 177 out of 780 generated backbones (22.7%) with scTM >= 0.5 (designable) without any refinement. Using AlphaFold (no MSAs) instead of OmegaFold gave similar numbers (163/780). The authors report ProteinMPNN produced stronger scTM performance than ESM-IF1 in their comparisons (Appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Directly compared in the paper to ESM-IF1 (worse scTM) and to a naive random-angle baseline (which produced 0 designable structures). Also compared at a pipeline level to results from Trippe et al. (2022) (their method produced 92/780 designable) and to ProteinSGM (reported higher designability but not directly comparable due to Rosetta post-processing).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limitations are methodological/instrumental: scTM is an in silico proxy (depends on inverse-folding and structure prediction tools), and differences in downstream folding (OmegaFold vs AlphaFold vs Rosetta post-processing) affect comparability. The paper also notes broader limitations of the overall generative pipeline: lever-arm error accumulation in the angle representation, occasional self-collisions in generated structures, poorer designability for structures containing β-sheets, and restricted sequence/structure lengths (they generate up to 128 residues). These limitations affect how reliably ProteinMPNN-produced sequences map back to the intended designs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Protein structure generation via folding diffusion', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning inverse folding from millions of predicted structures <em>(Rating: 2)</em></li>
                <li>ProteinSGM: Score-based generative modeling for de novo protein design <em>(Rating: 2)</em></li>
                <li>Singlesequence protein structure prediction using a language model and deep learning <em>(Rating: 1)</em></li>
                <li>Language models are unsupervised multitask learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3566",
    "paper_id": "paper-15586fec836ffcb60eba81491e04c225e6914aac",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "ESM-IF1",
            "name_full": "ESM-IF1 (inverse folding model)",
            "brief_description": "A transformer-based inverse folding model (derived from the ESM protein language model family) used to predict amino acid sequences that fold to a given backbone structure; cited here as a comparative inverse-folding method used in prior work.",
            "citation_title": "Learning inverse folding from millions of predicted structures",
            "mention_or_use": "mention",
            "model_name": "ESM-IF1",
            "model_description": "Refered to as an inverse-folding model based on ESM protein language models (transformer architecture). The paper does not report model size or training hyperparameters; it cites Hsu et al. (2022) as the source.",
            "generation_method": "Inverse folding: generate candidate amino acid sequences conditioned on a target backbone (given structure-to-sequence prediction).",
            "application_domain": "Protein sequence design / de novo protein design (assessing designability of generated backbones).",
            "evaluation_metrics": "Self-consistency TM score (scTM): TM-align score between the original generated backbone and the 3D structure predicted from sequences produced by the inverse-folding model (here folded with OmegaFold or AlphaFold without MSAs). A scTM &gt;= 0.5 is considered designable (same fold).",
            "results_summary": "Mentioned as an alternative inverse-folding model used in other work (Lee & Kim 2022) where 10 candidate sequences per structure were generated. Within the paper's evaluation context, the authors state ProteinMPNN yields improved performance compared to ESM-IF1 (referencing their Appendix Tables S1,S2), i.e., ESM-IF1 produced weaker scTM results in their comparisons (no detailed per-model numbers for ESM-IF1 are reported in the main text).",
            "comparison_to_baselines": "Compared (in the paper's appendices/discussion) against ProteinMPNN in the scTM pipeline; ProteinMPNN produced stronger scTM values in the authors' experiments. The paper also cites other pipelines (e.g., ProteinSGM + Rosetta) for context.",
            "limitations_challenges": "The paper does not provide detailed failure modes specific to ESM-IF1 beyond reporting it gives weaker scTM performance than ProteinMPNN in their comparative evaluation; no model sizes or training details are given here, so limitations specific to ESM-IF1 must be obtained from the original ESM-IF1 paper.",
            "uuid": "e3566.0",
            "source_info": {
                "paper_title": "Protein structure generation via folding diffusion",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "ProteinMPNN",
            "name_full": "ProteinMPNN (protein sequence design model)",
            "brief_description": "A deep-learning inverse-folding model used to generate amino acid sequences conditioned on backbone structures; used in this paper's evaluation pipeline to test whether generated backbones are designable.",
            "citation_title": "Robust deep learningbased protein sequence design using proteinmpnn",
            "mention_or_use": "use",
            "model_name": "ProteinMPNN",
            "model_description": "A deep learning model for inverse folding (sequence design conditioned on a backbone), cited from Dauparas et al. (2022). The paper uses ProteinMPNN in Cα-only mode to generate candidate sequences; specific architecture/size details are not provided in this manuscript beyond that it is an inverse-folding model that outperforms ESM-IF1 in their pipeline.",
            "generation_method": "Inverse folding: for each generated backbone, ProteinMPNN is used to produce multiple (the authors use 8) candidate amino acid sequences predicted to fit the backbone.",
            "application_domain": "Protein design / validation of generative protein backbones (assessing 'designability' of novel backbone geometries).",
            "evaluation_metrics": "Self-consistency TM score (scTM): for each generated backbone s, ProteinMPNN produces 8 candidate sequences; each sequence is folded (with OmegaFold or AlphaFold without MSAs) to produce predicted structures \\hat{s}_i; scTM = max_i TM-align(s, \\hat{s}_i). A scTM &gt;= 0.5 indicates the sequence folds into the same fold and the backbone is considered designable.",
            "results_summary": "Used as the inverse-folding model in the paper's scTM pipeline: ProteinMPNN (generating 8 sequences per backbone) together with OmegaFold folding yielded 177 out of 780 generated backbones (22.7%) with scTM &gt;= 0.5 (designable) without any refinement. Using AlphaFold (no MSAs) instead of OmegaFold gave similar numbers (163/780). The authors report ProteinMPNN produced stronger scTM performance than ESM-IF1 in their comparisons (Appendix).",
            "comparison_to_baselines": "Directly compared in the paper to ESM-IF1 (worse scTM) and to a naive random-angle baseline (which produced 0 designable structures). Also compared at a pipeline level to results from Trippe et al. (2022) (their method produced 92/780 designable) and to ProteinSGM (reported higher designability but not directly comparable due to Rosetta post-processing).",
            "limitations_challenges": "Limitations are methodological/instrumental: scTM is an in silico proxy (depends on inverse-folding and structure prediction tools), and differences in downstream folding (OmegaFold vs AlphaFold vs Rosetta post-processing) affect comparability. The paper also notes broader limitations of the overall generative pipeline: lever-arm error accumulation in the angle representation, occasional self-collisions in generated structures, poorer designability for structures containing β-sheets, and restricted sequence/structure lengths (they generate up to 128 residues). These limitations affect how reliably ProteinMPNN-produced sequences map back to the intended designs.",
            "uuid": "e3566.1",
            "source_info": {
                "paper_title": "Protein structure generation via folding diffusion",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning inverse folding from millions of predicted structures",
            "rating": 2
        },
        {
            "paper_title": "ProteinSGM: Score-based generative modeling for de novo protein design",
            "rating": 2
        },
        {
            "paper_title": "Singlesequence protein structure prediction using a language model and deep learning",
            "rating": 1
        },
        {
            "paper_title": "Language models are unsupervised multitask learners",
            "rating": 1
        }
    ],
    "cost": 0.012732249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Protein structure generation via folding diffusion</h1>
<p>Kevin E. Wu*<br>Stanford University<br>wukevin@stanford.edu</p>
<p>Rianne van den Berg<br>Microsoft Research<br>rvandenberg@microsoft.com</p>
<p>Kevin K. Yang<br>Microsoft Research<br>yang.kevin@microsoft.com<br>James Y. Zou<br>Stanford University<br>jamesz@stanford.edu</p>
<p>Alex X. Lu<br>Microsoft Research<br>lualex@microsoft.com</p>
<p>Ava P. Amini<br>Microsoft Research<br>ava.amini@microsoft.com</p>
<h4>Abstract</h4>
<p>The ability to computationally generate novel yet physically foldable protein structures could lead to new biological discoveries and new treatments targeting yet incurable diseases. Despite recent advances in protein structure prediction, directly generating diverse, novel protein structures from neural networks remains difficult. In this work, we present a new diffusion-based generative model that designs protein backbone structures via a procedure that mirrors the native folding process. We describe protein backbone structure as a series of consecutive angles capturing the relative orientation of the constituent amino acid residues, and generate new structures by denoising from a random, unfolded state towards a stable folded structure. Not only does this mirror how proteins biologically twist into energetically favorable conformations, the inherent shift and rotational invariance of this representation crucially alleviates the need for complex equivariant networks. We train a denoising diffusion probabilistic model with a simple transformer backbone and demonstrate that our resulting model unconditionally generates highly realistic protein structures with complexity and structural patterns akin to those of naturally-occurring proteins. As a useful resource, we release the first open-source codebase and trained models for protein structure diffusion.</p>
<h2>1 Introduction</h2>
<p>Proteins are critical for life, playing a role in almost every biological process, from relaying signals across neurons (Zhou et al., 2017) to recognizing microscopic invaders and subsequently activating the immune response (Mariuzza et al., 1987), from producing energy for cells (Bonora et al., 2012) to transporting molecules along cellular highways (Dominguez \&amp; Holmes, 2011). Misbehaving proteins, on the other hand, cause some of the most challenging ailments in human healthcare, including Alzheimer's disease, Parkinson's disease, Huntington's disease, and cystic fibrosis (Chaudhuri \&amp; Paul, 2006). Due to their ability to perform complex functions with high specificity, proteins have been extensively studied as a therapeutic medium (Leader et al., 2008; Kamionka, 2011; Dimitrov, 2012) and constitute a rapidly growing segment of approved therapies (H Tobin et al., 2014). Thus, the ability to computationally generate novel yet physically foldable protein structures could</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>open the door to discovering novel ways to harness cellular pathways and eventually lead to new treatments targeting yet incurable diseases.</p>
<p>Many works have tackled the problem of computationally generating new protein structures, but have generally run into challenges with creating diverse yet realistic folds. Traditional approaches typically apply heuristics to assemble fragments of experimentally profiled proteins into structures (Schenkelberg \&amp; Bystroff, 2016; Holm \&amp; Sander, 1991). This approach is limited by the boundaries of expert knowledge and available data. More recently, deep generative models have been proposed. However, due to the incredibly complex structure of proteins, these commonly do not directly generate protein structures, but rather constraints (such as pairwise distance between residues) that are heavily post-processed to obtain structures (Anand et al., 2019; Lee \&amp; Kim, 2022). Not only does this add complexity to the design pipeline, but noise in these predicted constraints can also be compounded during post-processing, resulting in unrealistic structures - that is, if the constraints are at all satisfiable to begin with. Other generative models rely on complex equivariant network architectures or loss functions to learn to generate a 3D point cloud that describes a protein structure (Anand \&amp; Achim, 2022; Trippe et al., 2022; Luo et al., 2022; Eguchi et al., 2022). Such equivariant architectures can ensure that the probability density from which the protein structures are sampled is invariant under translation and rotation. However, translation- and rotation-equivariant architectures are often also symmetric under reflection, leading to violations of fundamental structural properties of proteins like chirality (Trippe et al., 2022). Intuitively, this point cloud formulation is also quite detached from how proteins biologically fold - by twisting to adopt energetically favorable configurations (Šali et al., 1994; Englander et al., 2007).</p>
<p>Inspired by the in vivo protein folding process, we introduce a generative model that acts on the inter-residue angles in protein backbones instead of on Cartesian atom coordinates (Figure 1). This treats each residue as an independent reference frame, thus shifting the equivariance requirements from the neural network to the coordinate system itself. A similar angular representation has been used in some protein structure prediction works (Gao et al., 2017; AlQuraishi, 2019; Chowdhury et al., 2022). For generation, we use a denoising diffusion probabilistic model (diffusion model, for brevity) (Ho et al., 2020; Sohl-Dickstein et al., 2015) with a vanilla transformer parameterization without any equivariance constraints. Diffusion models train a neural network to start from noise and iteratively "denoise" it to generate data samples. Such models have been highly successful in a wide range of data modalities from images (Saharia et al., 2022; Rombach et al., 2022) to audio (Rouard \&amp; Hadjeres, 2021; Kong et al., 2021), and are easier to train with better modal coverage than methods like generative adversarial networks (GANs) (Dhariwal \&amp; Nichol, 2021; Nichol \&amp; Dhariwal, 2021). We present a suite of validations to quantitatively demonstrate that unconditional sampling from our model directly generates realistic protein backbones - from recapitulating the natural distribution of protein inter-residue angles, to producing overall structures with appropriate arrangements of multiple structural building block motifs. We show that our generated backbones are diverse and designable, and are thus biologically plausible protein structures. Our work demonstrates the power of biologically-inspired problem formulations and represents an important step towards accelerating the development of new proteins and protein-based therapies.</p>
<h1>2 Related work</h1>
<h3>2.1 Generating new protein structures</h3>
<p>Many generative deep learning architectures have been applied to the task of generating novel protein structures. Anand et al. (2019) train a GAN to sample pairwise distance matrices that describe protein backbone arrangements. However, these pairwise distance matrices must be corrected, refined, and converted into realizable backbones via two independent post-processing steps, the Alternating Direction Method of Multipliers (ADMM) and Rosetta. Crucially, inconsistencies in these predicted constraints can render them unsatisfiable or lead to significant errors when reconstructing the final protein structure. Sabban \&amp; Markovsky (2020) use a long short-term memory (LSTM) GAN to generate $(\phi, \psi)$ dihedral angles. However, their network only generates $\alpha$ helices and relies on downstream post-processing to filter, refine, and fold structures, partly due to the fact that these two dihedrals do not sufficiently specify backbone structure. Eguchi et al. (2022) propose a variational auto-encoder with equivariant losses to generate protein backbones in 3D space. However, their work only targets immunoglobulin proteins and also requires refinement through Rosetta. Non-</p>
<p>deep learning methods have also been explored: Schenkelberg \&amp; Bystroff (2016) apply heuristics to ensembles of similar sequences to perturb known protein structures, while Holm \&amp; Sander (1991) use a database search to find and assemble existing protein fragments that might fit a new scaffold structure. These approaches' reliance on known proteins and hand-engineered heuristics limit them to relatively small deviations from naturally-occurring proteins.</p>
<h1>2.1.1 Diffusion models for protein structure generation</h1>
<p>Several recent works have proposed extending diffusion models towards generating protein structures. These predominantly perform diffusion on the 3D Cartesian coordinates of the residues themselves. For example, Trippe et al. (2022) use an E(3)-equivariant graph neural network to model the coordinates of protein residues. Anand \&amp; Achim (2022) adopt a hybrid approach where they train an equivariant transformer with invariant point attention (Jumper et al., 2021); this model generates the 3D coordinates of $C_{\alpha}$ atoms, the amino acid sequence, and the angles defining the orientation of side chains. Another recent work by Luo et al. (2022) performs diffusion for generating antibody fragments' structure and sequence by modeling 3D coordinates using an equivariant neural network. Note that these prior works all use some form of equivariance to translation, rotation, and/or reflection due to their formulation of diffusion on Cartesian coordinates. Another method, ProteinSGM (Lee \&amp; Kim, 2022), implements a score-based diffusion model (Song et al., 2020) that generates image-like square matrices describing pairwise angles and distances between all residues in an amino acid chain. However, this set of values is highly over-constrained, and must be used as a set of input constraints for Rosetta's folding algorithm (Yang et al., 2020), which in turn produces the final folded output. This is a similar approach to Anand et al. (2019), and is likewise subject to the aforementioned concerns regarding complexity, satisfiability, and cleanliness of predicted constraints. Our work instead uses a minimal set of angles required to specify a protein backbone, and thus directly generates structures without relying on additional methods for refinement. Unfortunately, none of these prior works have publicly-available code, model weights, or generated examples at the time of this writing. Thus, our ability to perform direct comparisons is limited.</p>
<h3>2.2 Diffusion models for small molecules</h3>
<p>A related line of work focuses on creating and modeling small molecules, typically in the context of drug design, using similar generative approaches. These small molecules average 44 atoms in size (Jing et al., 2022). Compared to proteins, which average several hundred residues and thousands of atoms (Tiessen et al., 2012), the relatively small size of small molecules makes them easier to model. The E(3) Equivariant Diffusion Model (Hoogeboom et al., 2022) uses an equivariant transformer to design small molecules by diffusing their coordinates in Euclidean space. Other works have explored torsional diffusion, i.e., modelling the angles that specify a small molecule, to sample from the space of energetically favorable molecular conformations (Jing et al., 2022). This work still requires an $S E(3)$-equivariant model as the input to their model is a 3D point cloud. In contrast, our problem formulation allows us to work entirely in terms of relative angles.</p>
<h2>3 Method</h2>
<h3>3.1 Simplified framing of protein backbones using internal angles</h3>
<p>Proteins are variable-length chains of amino acid residues. There are 20 canonical amino acids, all of which share the same three-atom $N-C_{\alpha}-C$ backbone, but have varying side chains attached to the $C_{\alpha}$ atom (typically denoted $R_{i}$, see illustration in Figure 1). These residues assemble to form polymer chains typically hundreds of residues long (Tiessen et al., 2012). These chains of amino acids fold into 3D structures, taking on a shape that largely determines the protein's functions. These folded structures can be described on four levels: primary structure, which simply captures the linear sequence of amino acids; secondary structure, which describes the local arrangement of amino acids and includes structural motifs like $\alpha$-helices and $\beta$-sheets; tertiary structure, which describes the full spatial arrangement of all residues; and quaternary structure, which describes how multiple different amino acid chains come together to form larger complexes (Sun et al., 2004).
We propose a simplified framing of protein backbones that follows the biological intuition of protein folding while removing the need for complex equivariant networks. Rather than viewing a protein</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We perform diffusion on six angles as illustrated in the schematic in the bottom center (also defined in Table 1). Three of these are dihedral torsion angles (orange), and three are bond angles (green). We start with an experimentally observed backbone described by angles x0 and iteratively add Gaussian noise via the forward noising process q until the angles are indistinguishable from a wrapped Gaussian at xT. We use these examples to learn the "reverse" denoising process pξ.</p>
<p>Table 1: Internal angles used to specify protein backbone structure. Some of these involve multiple residues, indicated via i index subscripts. These are illustrated in Figure 1.</p>
<table>
<thead>
<tr>
<th>Angle</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ψ</td>
<td>Dihedral torsion about N<sup>i</sup> − Cα<sup>i</sup> − C<sup>i</sup> − N<sup>i+1</sup></td>
</tr>
<tr>
<td>ω</td>
<td>Dihedral torsion about Cα<sup>i</sup> − C<sup>i</sup> − N<sup>i+1</sup> − Cα<sup>i+1</sup></td>
</tr>
<tr>
<td>φ</td>
<td>Dihedral torsion about C<sup>i</sup> − N<sup>i+1</sup> − Cα<sup>i+1</sup> − C<sup>i+1</sup></td>
</tr>
<tr>
<td>θ1</td>
<td>Bond angle about N<sup>i</sup> − Cα<sup>i</sup> − C<sup>i</sup></td>
</tr>
<tr>
<td>θ2</td>
<td>Bond angle about Cα<sup>i</sup> − C<sup>i</sup> − N<sup>i+1</sup></td>
</tr>
<tr>
<td>θ3</td>
<td>Bond angle about C<sup>i</sup> − N<sup>i+1</sup> − Cα<sup>i+1</sup></td>
</tr>
</tbody>
</table>
<p>backbone of length N amino acids as a cloud of 3D coordinates (i.e., x ∈ R<sup>N×3</sup> if modeling only Cα atoms, or x ∈ R<sup>3N×3</sup> for a full backbone) as prior works have done, we view it as a sequence of six internal, consecutive angles x ∈ [−π, π]^(N−1)×6. That is, each vector of six angles describes the relative position of all backbone atoms in the next residue given the position of the current residue. These six angles are defined precisely in Table 1 and illustrated in Figure 1. These internal angles can be easily computed using trigonometry, and converted back to 3D Cartesian coordinates by iteratively adding atoms to the protein backbone as described in Parsons et al. (2005), fixing bond distances to average lengths (Appendix A.1, Figure S1). Despite building the structure from a fixed point, this formulation does not appear to overly accumulate errors (Appendix A.2, Figures S2, S3).</p>
<p>This internal angle formulation has several key advantages. Most importantly, since each residue forms its own independent reference frame, there is no need to use an equivariant neural network. No matter how the protein is rotated or shifted, the angles specifying the next residue given the current residue never changes. This allows us to use a simple transformer as the backbone architecture; in fact, we demonstrate that our model fails when substituting our shift- and rotation-invariant internal angle representation with Cartesian coordinates, keeping all other design choices identical (Appendix C.1, Figure S4). This internal angle formulation also closely mimics how proteins actually fold by twisting into more energetically stable conformations.</p>
<h3>3.2 Denoising diffusion probabilistic models</h3>
<p>Denoising diffusion probabilistic models (or diffusion models, for short) leverage a Markov process q(xt | xt−1) to corrupt a data sample x0 over T discrete timesteps until it is indistinguishable from noise at xT. A diffusion model pξ(xt−1 | xt) parameterized by ξ is trained to reverse this forward noising process, "denoising" pure noise towards samples that appear drawn from the native data distribution (Sohl-Dickstein et al., 2015). Diffusion models were first shown to achieve good gener-</p>
<p>ative performance by Ho et al. (2020); we adapt this framework for generating protein backbones, introducing necessary modifications to work with periodic angular values.</p>
<p>We modify the standard Markov forward noising process that adds noise at each discrete timestep $t$ to sample from a wrapped normal instead of a standard normal (Jing et al., 2022):</p>
<p>$$
q\left(x_{t} \mid x_{t-1}\right)=\mathcal{N}<em t="t">{\text {wrapped }}\left(x</em>\right)
$$} ; \sqrt{1-\beta_{t}} x_{t-1}, \beta_{t} I\right) \propto \sum_{k=-\infty}^{\infty} \exp \left(\frac{-\left|x_{t}-\sqrt{1-\beta_{t}} x_{t-1}+2 \pi k\right|^{2}}{2 \beta_{t}^{2}</p>
<p>where $\beta_{t} \in(0,1)_{t=1}^{T}$ are set by a variance schedule. We use the cosine variance schedule (Nichol \&amp; Dhariwal, 2021) with $T=1000$ timesteps:</p>
<p>$$
\beta_{t}=\operatorname{clip}\left(1-\frac{\bar{\alpha}<em t-1="t-1">{t}}{\bar{\alpha}</em>\right)
$$}}, 0.999\right) \quad \bar{\alpha}_{t}=\frac{f(t)}{f(0)} \quad f(t)=\cos \left(\frac{t / T+s}{1+s} \cdot \frac{\pi}{2</p>
<p>where $s=8 \times 10^{-3}$ is a small constant for numerical stability. We train our model for $p_{\xi}\left(x_{t-1} \mid x_{t}\right)$ with the simplified loss proposed by Ho et al. (2020), using a neural network $\mathrm{nn}<em t="t">{\xi}\left(x</em>=0.1 \pi$. While this loss is not as well-motivated as torsional losses proposed by Jing et al. (2022), we find that it achieves strong empirical results.}, t\right)$ that predicts the noise $\epsilon \sim \mathcal{N}(0, I)$ present at a given timestep (rather than the denoised mean values themselves). To handle the periodic nature of angular values, we introduce a function to "wrap" values within the range $[-\pi, \pi): w(x)=((x+\pi) \bmod 2 \pi)-\pi$. We use $w$ to wrap a smooth L1 loss (Girshick, 2015) $L_{w}$, which behaves like L1 loss when error is high, and like an L2 loss when error is low; we set the transition between these two regimes at $\beta_{L</p>
<p>$$
\begin{aligned}
d_{w} &amp; =w\left(\epsilon-\mathrm{nn}<em t="t">{\xi}\left(w\left(\sqrt{\bar{\alpha}</em>}} x_{0}+\sqrt{1-\bar{\alpha<em w="w">{t}} \epsilon\right), t\right)\right) \
L</em> \
\left|d_{w}\right|-0.5 \beta_{L} &amp; \text { otherwise }\end{cases}
\end{aligned}
$$} &amp; = \begin{cases}0.5 \frac{d_{w}^{2}}{\beta_{L}} &amp; \text { if }\left|d_{w}\right|&lt;\beta_{L</p>
<p>During training, timesteps are sampled uniformly $t \sim U(0, T)$. We normalize all angles in the training set to be zero mean by subtracting their element-wise angular mean $\mu$; validation and test sets are shifted by this same offset.</p>
<p>Figure 1 illustrates this overall training process, including our previously described internal angle framing. The internal angles describing the folded chain $x_{0}$ are corrupted until they become indistinguishable from random angles, which results in a disordered mass of residues at $x_{T}$; we sample points along this diffusion process to train our model $\mathrm{nn}<em _xi="\xi">{\xi}$. Once trained, the reverse process of sampling from $p</em>}$ also requires modifications to account for the periodic nature of angles, as described in Algorithm 1. The variance of this reverse process is given by $\sigma_{t}=\sqrt{\frac{1-\bar{\alpha<em t="t">{t-1}}{1-\bar{\alpha}</em>$.}} \cdot \beta_{t}</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Sampling from \(p_{\xi}\) with FoldingDiff
    \(x_{T} \sim w(\mathcal{N}(0, I)) \quad \triangleright\) Sample from a wrapped Gaussian
    for \(t=T, \ldots, 1\) do
        \(z=\mathcal{N}(0, I)\) if \(t&gt;1\) else \(z=0\)
        \(x_{t-1}=w\left(\frac{1}{\sqrt{\bar{\alpha}_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \mathrm{nn}_{\xi}\left(x_{t}, t\right)\right)+\sigma_{t} z\right) \quad \triangleright\) Wrap sampled values about \([-\pi, \pi)\)
    end for
    return \(w\left(x_{0}+\mu\right) \quad \triangleright\) Un-shift generated values by original mean shift
</code></pre></div>

<p>This sampling process can be intuitively described as refining internal angles from an unfolded state towards a folded state. As this is akin to how proteins fold in vivo, we name our method FoldingDiff.</p>
<h1>3.3 Modeling and dataset</h1>
<p>For our reverse (denoising) model $p_{\xi}\left(x_{t}, t\right)$, we adopt a vanilla bidirectional transformer architecture (Vaswani et al., 2017) with relative positional embeddings (Shaw et al., 2018). Our six-dimensional</p>
<p>input is linearly upscaled to the model’s embedding dimension ($d=384$). To incorporate the timestep $t$, we generate random Fourier feature embeddings (Tancik et al., 2020) as done in Song et al. (2020) and add these embeddings to each upscaled input. To convert the transformer’s final per-position representations to our six outputs, we apply a regression head consisting of a densely connected layer, followed by GELU activation (Hendrycks &amp; Gimpel, 2016), layer normalization, and finally a fully connected layer outputting our six values. We train this network with the AdamW optimizer (Loshchilov &amp; Hutter, 2019) over 10,000 epochs, with a learning rate that linearly scales from 0 to $5\times 10^{-5}$ over 1,000 epochs, and back to 0 over the final 9,000 epochs. Validation loss appears to plateau after $\approx$ 1,400 epochs; additional training does not improve validation loss, but appears to lead to a poorer diversity of generated structures. We thus take a model checkpoint at 1,488 epochs for all subsequent analyses.</p>
<p>We train our model on the CATH dataset, which provides a “de-duplicated” set of protein structural folds spanning a wide range of functions where no two chains share more than 40% sequence identity over 60% overlap (Sillitoe et al., 2015). We exclude any chains with fewer than 40 residues. Chains longer than 128 residues are randomly cropped to a 128-residue window at each epoch. A random 80/10/10 training/validation/test split yields 24,316 training backbones, 3,039 validation backbones, and 3,040 test backbones.</p>
<h2>4 Experiments</h2>
<h3>4.1 Generating protein internal angles</h3>
<p>After training our model, we check that it is able to recapitulate the correct marginal distributions of dihedral and bond angles in proteins. We unconditionally generate 10 backbone chains each for every length $l\in[50,128)$, generating a total of 780 backbones as was done in Trippe et al. (2022). We plot the distributions of all six angles, aggregated across these 780 structures, and compare each distribution to that of experimental test set structures less than 128 residues in length (Figures 2, S9). We observe that, across all angles, the generated distribution almost exactly recapitulates the test distribution. This is true both for angles that are nearly Gaussian with low variance $(\omega,\theta_{1},\theta_{2},\theta_{3})$ as well as for angles with highly complex, high-variance distributions $(\phi,\psi)$. Angles that wrap about the $-\pi/\pi$ boundary $(\omega)$ are correctly handled as well. Compared to similar plots generated from other protein diffusion methods (e.g., Figure 1 in Anand &amp; Achim (2022), reproduced with permission in Figure S10), we qualitatively observe that our method produces a much tighter distribution that more closely matches the natural distribution of bond angles.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of the distributions of angular values in held-out test set and in generated samples. Top row shows dihedral angles (torsional angles involving 4 atoms), and bottom row shows bond angles (involving 3 atoms). KL divergence is calculated between $D_{KL}(\text{sampled}|$ test). Figure S9 shows the cumulative distribution function (CDF) corresponding to these histograms.</p>
<p>However, looking at individual distributions of angles alone does not capture the fact that these angles are not independently distributed, but rather exhibit significant correlations. A Ramachandran plot, which shows the frequency of co-occurrence between the dihedrals $(\phi, \psi)$, is commonly used to illustrate these correlations between angles (Ramachandran \&amp; Sasisekharan, 1968). Figure 3 shows the Ramachandran plot for (experimental) test set chains with fewer than 128 residues, as well as that for our 780 generated structures. The Ramachandran plot for natural structures (Figure 3a) contains three major concentrated regions corresponding to right-handed $\alpha$ helices, left-handed $\alpha$ helices, and $\beta$ sheets. All three of these regions are recapitulated in our generated structures (Figure 3b). In other words, FoldingDiff is able to generate all three major secondary structure elements in protein backbones. Furthermore, we see that our model correctly learns that right-handed $\alpha$ helices are much more common than left-handed $\alpha$ helices (Cintas, 2002). Prior works that use equivariant networks, such as Trippe et al. (2022), cannot differentiate between these two types of helices due to network equivariance to reflection. This concretely demonstrates that our internal angle formulation leads to improved handling of chirality (i.e., the asymmetric nature of proteins) in generated backbones.
(a) Ramachandran plot, test set
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Ramachandran plot, generated backbones
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Ramachandran plots comparing the $(\phi, \psi)$ dihedral angles for test set (3a) and generated protein backbones (3b). Each major region of this plot indicates a different secondary structure element, as indicated in panel 3a. All three main structural elements are recapitulated in our generated backbones, along with some less common angle combinations. Lines are artifacts of null values, and appear shifted due to zero centering/uncentering.</p>
<h1>4.2 Analyzing generated structures</h1>
<p>We have shown that our model generates realistic distributions of angles and that our generated joint distributions capture secondary structure elements. We now demonstrate that the overall structures specified by these angles are biologically reasonable. Recall that natural protein structures contain multiple secondary structure elements. We use P-SEA (Labesse et al., 1997) to count the number of secondary structure elements in each test-set backbone of fewer than 128 residues, and generate a 2D histogram describing the frequency of $\alpha / \beta$ co-occurrence counts in Figure 4a. Figure 4b repeats this analysis for our generated structures, which frequently contain multiple secondary structure elements just as naturally-occurring proteins do. FoldingDiff thus appears to generate rich structural information, with consistent performance across multiple generation replicates (Figure S11). This is a nontrivial task - an autoregressive transformer, for example, collapses to a failure mode of endlessly generating $\alpha$ helices (Appendix C.2, Figures S5, S6).
Beyond demonstrating that FoldingDiff's generated backbones contain reasonable structural motifs, it is also important to show that they are designable - meaning that we can find a sequence of amino acids that can fold into our designed backbone structure. After all, a novel protein structure is not useful if we cannot physically realize it. Previous works evaluate this in silico by predicting possible amino acids that fold into a generated backbone and checking whether the predicted structure for these sequences matches the original backbone (Trippe et al. 2022, Lee \&amp; Kim 2022, Appendix B). Following this general procedure, for a generated structure $s$, we use the ProteinMPNN inverse folding model (Dauparas et al., 2022) to generate 8 different amino acid sequences, as it yields</p>
<p>(a) Secondary structure co-occurrence, test
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: 2D histograms describing co-occurrence of secondary structures in test backbones (4a) and generated backbones (4b). Axes indicate the number of secondary structure present in a chain; color indicates the frequency of a specific combination of secondary structure elements. Our generated structures mirror real structures with multiple $\alpha$ helices, multiple $\beta$ sheets, and a mixture of both. See Figure S11 for additional generation replicates.
improved performance compared to ESM-IF1 (Hsu et al. 2022, Appendix C.3, Tables S1, S2). We then use OmegaFold (Wu et al., 2022) to predict the 3D structures $\hat{s}<em 8="8">{1}, \ldots, \hat{s}</em>$ corresponding to each of these sequences. We use TMalign (Zhang \&amp; Skolnick, 2005), which evaluates structural similarity between backbones, to score each of these 8 structures against the original structure $s$. The maximum score $\max <em i="i">{i \in[1,8]} \operatorname{TMalign}\left(s, \hat{s}</em>\right)$ is the self-consistency TM (scTM) score. A scTM score of $\geq 0.5$ is considered to be in the same fold, and thus is "designable."
<img alt="img-5.jpeg" src="img-5.jpeg" />
(a) Backbone designability by length
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) Designability compared to training set similarity</p>
<p>Figure 5: Of our 780 generated backbones, ranging in length from 50-128 residues, 177 are designable $(\mathrm{scTM} \geq 0.5)$ using ProteinMPNN and OmegaFold. Shorter structures of 70 amino acids or fewer tend to have higher scTM scores than longer structures (5a). Generated backbones that are more similar to training examples (greater maximum training TM score) tend to have better designability (5b). The three structures indicated by arrows are illustrated in Figure S12.</p>
<p>With this procedure, we find that 177 of our 780 structures, or 22.7\%, are designable with an scTM score $\geq 0.5$ (Figure 5a) without any refinement or relaxation. This designability is highly consistent across different generation runs (Table S1), and is also consistent when substituting AlphaFold2 without MSAs (Jumper et al., 2021) in place of OmegaFold (163/780 designable with AlphaFold2). Trippe et al. (2022) use an identical scTM pipeline using ProteinMPNN and AlphaFold2, and report a significantly lower proportion of designable structures ( $92 / 780$ designable, $p \ll 10^{-5}$, Chi-square test). Compared to this prior work, FoldingDiff improves upon designability of both short sequences</p>
<p>(up to 70 residues, $76 / 210$ designable compared to $36 / 210$, $p=1 \times 10^{-5}$, Chi-square test) and long sequences (beyond 70 residues, $87 / 570$ designable compared to $56 / 570, p=5.6 \times 10^{-3}$, Chi-square test). While ProteinSGM (ESM-IF1 for inverse folding, AlphaFold2 for fold prediction) reports an even higher designability proportion of $50.5 \%$, this value is not directly comparable, as ProteinSGM generates constraints that are subsequently folded using Rosetta; therefore, their designability does not directly reflect their generative process. The authors themselves note that Rosetta "post-processing" significantly improves the viability of their structures. To further contextualize our scTM scores, we evaluate a naive method that samples from the empirical distribution of dihedral angles. This baseline produces no designable structures whatsoever (Appendix C.3, Figures S7, S8). Conversely, we evaluate experimental structures to establish an upper bound for designability; $87 \%$ of natural structures have an scTM $\geq 0.5$ (Appendix C.3, Figure S7).</p>
<p>We additionally evaluate the similarity of each generated backbone to any training backbone by taking the maximum TM score across the entire training set. The maximum training TM-score is significantly correlated with scTM score (Spearman's $r=0.78, p=7.9 \times 10^{-165}$, Figure 5b), indicating that structures more similar to the training set tend to be more designable. However, this does not suggest that we are merely memorizing the training set; doing so would result in a distribution of training TM scores near 1.0, which is not what we observe. ProteinSGM reports a distribution of training set TMscores much closer to 1.0; this suggests a greater degree of memorization and may indicate that their high designability ratio is partially driven by memorization.</p>
<p>Selected examples of our generated backbones and corresponding OmegaFold predictions of various lengths are visualized using PyMOL (Schrödinger, LLC, 2015) in Figure 6. Interestingly, we find that of our 177 designable backbones, only 16 contain $\beta$ sheets as annotated by P-SEA. Conversely, of our 603 backbones with scTM $&lt;0.5,533$ contain $\beta$ sheets. This suggests that generated structures with $\beta$ sheets may be less designable ( $p \ll 1.0 \times 10^{-5}$, Chi-square test). We additionally cluster our designable backbones and observe a large diversity of structures (Figure S14) comparable to that of natural structures (Figure S15). This suggests that our model is not simply generating small variants on a handful of core structures, which prior works appear to do (Figure S16).
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6: Selected generated protein backbones of varying length that are approximately designable (scTM $\approx 0.5$ ). Top row shows our directly generated backbones; bottom row shows OmegaFold predicted structure for residues inferred by ProteinMPNN to produce our generated backbone. Structures contain both $\alpha$ helices (coils, columns 1-4) and $\beta$ sheets (ribbons, columns 1 and 2), and each appears meaningfully different from its most similar training example (Figure S13).</p>
<h1>5 Conclusion</h1>
<p>In this work, we present a novel parameterization of protein backbone structures that allows for simplified generative modeling. By considering each residue to be its own reference frame, we describe a protein using the resulting relative internal angle representation. We show that a vanilla transformer can then be used to build a diffusion model that generates high-quality, biologically plausible, diverse protein structures. These generated backbones respect protein chirality and exhibit high designability.</p>
<p>While we demonstrate promising results with our model, there are several limitations to our work. Though formulating a protein as a series of angles enables use of simpler models without equivariance mechanisms, this framing allows for errors early in the chain to significantly alter the overall generated structure - a sort of "lever arm effect." Additionally, some generated structures exhibit collisions where the generated structure crosses through itself. Future work could explore methods to avoid these pitfalls using geometrically-informed architectures such as those used in Wu et al. (2022). Our generated structures are still of relatively short lengths compared to natural proteins which typically have several hundred residues; future work could extend towards longer structures, potentially incorporating additional losses or inputs that help "checkpoint" the structure and reduce accumulation of error. We also do not handle multi-chain complexes or ligand interactions, and are only able to generate static structures that do not capture the dynamic nature of proteins. Future work could incorporate amino acid sequence generation in parallel with structure generation, along with guided generation using functional or domain annotations. In summary, our work provides an important step in using biologically-inspired problem formulations for generative protein design.</p>
<h1>Code availability and reproducibility</h1>
<p>All code for training our model and performing downstream analyses is available at https:// github.com/microsoft/foldingdiff. Trained model weights used for generating all results in this manuscript are available there as well.</p>
<h2>Author Contributions</h2>
<p>K.E.W., K.K.Y., A.X.L., and A.P.A. initiated, conceived, and designed the work. K.E.W. performed modeling and analyses, with input from all authors. R.vdB., K.K.Y., and A.P.A. provided guidance on diffusion models and their implementation. A.P.A., K.K.Y., J.Z., and A.X.L. provided guidance on evaluation methods. A.P.A., K.K.Y., and A.X.L. supervised the research. All authors wrote the manuscript.</p>
<h2>Acknowledgments</h2>
<p>We thank Brian Trippe, Jason Yim, Namrata Anand, and Tudor Achim for permission to reuse their figures.</p>
<h2>References</h2>
<p>Mohammed AlQuraishi. End-to-end differentiable learning of protein structure. Cell systems, 8(4): 292-301, 2019.</p>
<p>Namrata Anand and Tudor Achim. Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. arXiv preprint arXiv:2205.15019, 2022.</p>
<p>Namrata Anand, Raphael Eguchi, and Po-Ssu Huang. Fully differentiable full-atom protein backbone generation. In DGS@ICLR, 2019.</p>
<p>Massimo Bonora, Simone Patergnani, Alessandro Rimessi, Elena De Marchi, Jan M Suski, Angela Bononi, Carlotta Giorgi, Saverio Marchi, Sonia Missiroli, Federica Poletti, et al. ATP synthesis and storage. Purinergic Signalling, 8(3):343-357, 2012.</p>
<p>Tapan K Chaudhuri and Subhankar Paul. Protein-misfolding diseases and chaperone-based therapeutic approaches. The FEBS Journal, 273(7):1331-1349, 2006.</p>
<p>Ratul Chowdhury, Nazim Bouatta, Surojit Biswas, Christina Floristean, Anant Kharkar, Koushik Roy, Charlotte Rochereau, Gustaf Ahdritz, Joanna Zhang, George M Church, et al. Singlesequence protein structure prediction using a language model and deep learning. Nature Biotechnology, pp. 1-7, 2022.</p>
<p>Pedro Cintas. Chirality of living systems: a helping hand from crystals and oligopeptides. Angewandte Chemie International Edition, 41(7):1139-1145, 2002.</p>
<p>Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learningbased protein sequence design using proteinmpnn. Science, 378(6615):49-56, 2022.</p>
<p>Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. Advances in Neural Information Processing Systems, 34:8780-8794, 2021.</p>
<p>Dimiter S Dimitrov. Therapeutic proteins. Therapeutic Proteins, pp. 1-26, 2012.
Roberto Dominguez and Kenneth C Holmes. Actin structure and function. Annual Review of Biophysics, 40:169, 2011.</p>
<p>Raphael R Eguchi, Christian A Choe, and Po-Ssu Huang. Ig-vae: Generative modeling of protein structure by direct 3d coordinate generation. PLoS computational biology, 18(6):e1010271, 2022.</p>
<p>S Walter Englander, Leland Mayne, and Mallela MG Krishna. Protein folding and misfolding: mechanism and principles. Quarterly Reviews of Biophysics, 40(4):1-41, 2007.</p>
<p>Yujuan Gao, Sheng Wang, Minghua Deng, and Jinbo Xu. Real-value and confidence prediction of protein backbone dihedral angles through a hybrid method of clustering and deep learning. arXiv preprint arXiv:1712.07244, 2017.</p>
<p>Ross Girshick. Fast R-CNN. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, 2015.</p>
<p>Peter H Tobin, David H Richards, Randolph A Callender, and Corey J Wilson. Protein engineering: a new frontier for biological therapeutics. Current Drug Metabolism, 15(7):743-756, 2014.</p>
<p>Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016.</p>
<p>Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.</p>
<p>Liisa Holm and Chris Sander. Database algorithm for generating protein backbone and side-chain co-ordinates from a $\mathrm{C} \alpha$ trace: application to model building and detection of co-ordinate errors. Journal of Molecular Biology, 218(1):183-194, 1991.</p>
<p>Emiel Hoogeboom, Victor Garcia Satorras, Clément Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3D. In International Conference on Machine Learning, pp. 8867-8887. PMLR, 2022.</p>
<p>Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. Learning inverse folding from millions of predicted structures. In International Conference on Machine Learning, pp. 8946-8970. PMLR, 2022.</p>
<p>Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. arXiv preprint arXiv:2206.01729, 2022.</p>
<p>John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583-589, 2021.</p>
<p>Mariusz Kamionka. Engineering of therapeutic proteins production in Escherichia coli. Current Pharmaceutical Biotechnology, 12(2):268-274, 2011.</p>
<p>Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021.</p>
<p>Gilles Labesse, N Colloc'h, Joël Pothier, and J-P Mornon. P-SEA: a new efficient assignment of secondary structure from $\mathrm{C} \alpha$ trace of proteins. Bioinformatics, 13(3):291-295, 1997.</p>
<p>Benjamin Leader, Quentin J Baca, and David E Golan. Protein therapeutics: a summary and pharmacological classification. Nature reviews Drug discovery, 7(1):21-39, 2008.</p>
<p>Jin Sub Lee and Philip M. Kim. ProteinSGM: Score-based generative modeling for de novo protein design. bioRxiv, 2022. doi: 10.1101/2022.07.13.499967.</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.</p>
<p>Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures. bioRxiv, 2022. doi: 10.1101/2022.07.10.499510.</p>
<p>RA Mariuzza, SEV Phillips, and RJ Poljak. The structural basis of antigen-antibody recognition. Annual Review of Biophysics and Biophysical Chemistry, 16(1):139-159, 1987.</p>
<p>Milot Mirdita, Konstantin Schütze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin Steinegger. Colabfold: making protein folding accessible to all. Nature Methods, pp. 1-4, 2022.</p>
<p>Alexander Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pp. 8162-8171. PMLR, 2021.</p>
<p>Jerod Parsons, J Bradley Holmes, J Maurice Rojas, Jerry Tsai, and Charlie EM Strauss. Practical conversion from torsion space to cartesian space for in silico protein synthesis. Journal of Computational Chemistry, 26(10):1063-1068, 2005.</p>
<p>Zhao Qin, Andrea Fabre, and Markus J Buehler. Structure and mechanism of maximum stability of isolated alpha-helical protein domains at a critical length scale. The European Physical Journal E, 36(5):1-12, 2013.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>GN Ramachandran and V Sasisekharan. Conformation of polypeptides and proteins. Advances in Protein Chemistry, 23:283-437, 1968.</p>
<p>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022.</p>
<p>Simon Rouard and Gaëtan Hadjeres. CRASH: Raw audio score-based generative modeling for controllable high-resolution drum sound synthesis. arXiv preprint arXiv:2106.07431, 2021.</p>
<p>Sari Sabban and Mikhail Markovsky. RamaNet: Computational de novo helical protein backbone design using a long short-term memory generative neural network. bioRxiv, pp. 671552, 2020.</p>
<p>Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.</p>
<p>Andrej Šali, Eugene Shakhnovich, and Martin Karplus. How does a protein fold. Nature, 369(6477): 248-251, 1994.</p>
<p>Christian D Schenkelberg and Christopher Bystroff. Protein backbone ensemble generation explores the local structural space of unseen natural homologs. Bioinformatics, 32(10):1454-1461, 2016.</p>
<p>Schrödinger, LLC. The PyMOL molecular graphics system, version 1.8. November 2015.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.</p>
<p>Ian Sillitoe, Tony E Lewis, Alison Cuff, Sayoni Das, Paul Ashford, Natalie L Dawson, Nicholas Furnham, Roman A Laskowski, David Lee, Jonathan G Lees, et al. CATH: comprehensive structural and functional annotations for genome sequences. Nucleic Acids Research, 43(D1): D376-D381, 2015.</p>
<p>Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256-2265. PMLR, 2015.</p>
<p>Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.</p>
<p>Peter D Sun, Christine E Foster, and Jeffrey C Boyington. Overview of protein structural and functional folds. Current Protocols in Protein Science, 35(1):17-1, 2004.</p>
<p>Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems, 33:7537-7547, 2020.</p>
<p>Martha M Teeter. Water structure of a hydrophobic protein at atomic resolution: Pentagon rings of water molecules in crystals of crambin. Proceedings of the National Academy of Sciences, 81 (19):6014-6018, 1984.</p>
<p>Axel Tiessen, Paulino Pérez-Rodríguez, and Luis José Delaye-Arredondo. Mathematical modeling and comparison of protein size distribution in different plant, animal, fungal and microbial species reveals a negative correlation between protein size and protein number, thus providing insight into the evolution of proteomes. BMC Research Notes, 5(1):1-23, 2012.</p>
<p>Brian L Trippe, Jason Yim, Doug Tischer, Tamara Broderick, David Baker, Regina Barzilay, and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in 3D for the motifscaffolding problem. arXiv preprint arXiv:2206.04119, 2022.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.</p>
<p>Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, Jianzhu Ma, and Jian Peng. High-resolution de novo structure prediction from primary sequence. bioRxiv, 2022. doi: 10.1101/2022.07.21.500999.</p>
<p>Jianyi Yang, Ivan Anishchenko, Hahnbeom Park, Zhenling Peng, Sergey Ovchinnikov, and David Baker. Improved protein structure prediction using predicted interresidue orientations. Proceedings of the National Academy of Sciences, 117(3):1496-1503, 2020.</p>
<p>Yang Zhang and Jeffrey Skolnick. TM-align: a protein structure alignment algorithm based on the TM-score. Nucleic Acids Research, 33(7):2302-2309, 2005.</p>
<p>Qiangjun Zhou, Peng Zhou, Austin L Wang, Dick Wu, Minglei Zhao, Thomas C Südhof, and Axel T Brunger. The primed snare-complexin-synaptotagmin complex for neuronal exocytosis. Nature, 548(7668):420-425, 2017.</p>
<h1>A Internal angle formulation of protein backbones</h1>
<h2>A. 1 Choice of angles for representation</h2>
<p>A protein backbone structure can be fully specified by a total of 9 values per residue: 3 bond distances, 3 bond angles, and 3 dihedral torsional angles. The three bond angles and dihedrals are described in Table 1, and the three bond distances correspond to $N_{i} \rightarrow C \alpha_{i}, C \alpha_{i} \rightarrow C_{i}$, and $C_{i} \rightarrow N_{i+1}$ where $i$ denotes residue index. These 9 values enable a protein backbone to be losslessly converted from Cartesian to internal angle representation, and vice versa. To determine which subset of values to use to formulate proteins in our model, we take a set of experimentally profiled proteins and translate their coordinates from Cartesian to internal angles and distances and back, measuring the TM score between the initial and reconstructed structures. When excluding an angle or distance, we fix all corresponding values to the mean. The reconstruction TM scores of various combinations of values is illustrated in Figure S1. Of these 9 values, the three bond distances are the least important for reliably reconstructing a structure from Cartesian coordinates to the inter-residue representation and back; they can usually be replaced with constant average values without much impact on the recovered structure. In comparison, removing even two bond angles with relatively little variance $\left(\theta_{2}, \theta_{3}\right)$ results in a large loss in reconstruction TM score (third bar). Removing all bond angles and retaining only dihedrals $(\phi, \psi, \omega)$ results in only about half of proteins being able to be reconstructed (last bar). Thus, we model the three dihedrals and the three bond angles (second bar in Figure S1); this simplifies our prediction problem to use only periodic angular values (instead of a mixture of angular and real values) without a substantial loss in the accuracy of described structures. Future work might include additional modeling of these real-valued bond distances.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure S1: Various combinations of angles and distances and their ability to faithfully reconstruct protein backbones. A TM-score of 0.5 (dashed grey line) indicates the minimum similarity for two structures to be considered to have the same general shape. Error bars represent standard deviation in reconstruction TM scores. Using all bond angles, dihedral angles, and bond distances perfectly reconstructs Cartesian coordinates from internal angles (first column). The second column corresponds to the formulation used in the main text, where we model the 3 dihedrals and 3 bond angles, but keep the 3 bond distances fixed to average values. Other columns fix even more values to their respective means and result in reconstruction TM scores that are too low to be reliably useful.</p>
<p>One detail when converting between a $N$-residue set of Cartesian coordinates to a set of $N-1$ angles between consecutive residues is that the latter representation does not capture the first residue's information (as there is no prior residue to orient against). To solve this, we use a fixed set of coordinates to "seed" all generation of Cartesian coordinates, using the $N-1$ specified angles to build out from this fixed point. For all generations, this fixed point is extracted from the coordinates of the $N-C_{\alpha}-C$ atoms in the first residue on the N -terminus of the PDB structure 1CRN (Teeter, 1984). Doing so does not result in any meaningful reconstruction error in natural structures.</p>
<h1>A. 2 Effect of length on structure reconstruction</h1>
<p>One of the primary concerns of using an angle-based formulation is that small errors might propagate across many residues to culminate in a large difference in overall structure. To try and quantify the effect of this, we evaluate the "lossiness" of the representation itself, and the ability of the model to learn long-range angle dependencies.
We start by evaluating the accuracy of our representation itself over different structure lengths. To do this, we sample 5000 structures of varying length from the CATH dataset. For each, we compare the original 3D coordinate representation $x_{c}$ and the 3D coordinates obtained after converting the structure to angles and back to coordinates $\hat{x}<em c="c">{c}$ using the TM score algorithm, i.e., TMscore $\left(x</em>\right)$. We find that longer structures exhibit greater TM score divergences when converted through our representation (Figure S2). However, even at our maximum considered structure length of 128 residues, structures still retain a reconstruction TMscore $\approx 0.9$, which is well above the accepted threshold of 0.5 denoting the same fold. This indicates that while our representation itself is slightly "lossy", the losses do not change the overall fold. Even when considering longer structures up to 512 residues in length, the reconstructed structures still share a TMscore similarity much greater than 0.5 (graph not shown), which suggests that our method could scale up to larger structures.
}, \hat{x}_{c<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure S2: Faithfulness of reconstruction (via TMscore, y-axis) when using the 3 dihedrals and 3 angles described in Table 1 and keeping bond distances fixed to average values, evaluated across 5000 structures of varying length (x-axis). We observe a significant negative correlation between length and reconstruction TM score (Spearman's correlation $r=-0.18, p=7.2 \times 10^{-39}$ ).</p>
<p>Next, we evaluate our model's ability to successfully reconstruct sequences of varying length. For each structure in our held-out test set ( $n=3040$ ), we add $t=750$ timesteps of noise to that structure's angles (recall that we use a total of $T=1000$ timesteps during training). This adds a significant amount of noise to corrupt the structure, while retaining a hint of the target true structure as a weak "guide" to what the model should ultimately denoise towards. We then apply our trained</p>
<p>model to these mostly-noised examples, running them for the requisite 750 iterations to fully denoise and reconstruct the angles. Afterwards, we assess reconstruction accuracy by taking the TMscore between the structure specified by the reconstructed angles, and the true structure specified by the ground truth angles. Comparing structures specified by reconstructed and true angles isolates the effects of model error, irrespective of any minor lossiness induced by the representation itself, which we investigated previously (Figure S2). Figure S3 illustrates the relationship between test set structure length and this reconstruction TM score. We find no significant correlation between length and reconstruction TM score (Spearman's correlation $r=-0.0024, p=0.89$ ). FoldingDiff accurately reconstructs structures with a TM score of greater than 0.95 for $98 \%$ of test set, and achieves an average test reconstruction TM score of 0.988 .
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure S3: Test set structure reconstruction accuracy after injecting 750 timesteps of noise into angles. This is evaluated by taking the TM score between the structured specified by the reconstructed angles, and the structure specified by the ground truth angles. The x -axis denotes length of the test set structure, and each bar denotes the distribution of TM scores within that length.</p>
<p>All in all, these results indicate that (1) our representation itself is indeed lossier with longer structures, but not to a degree that would significantly impact the overall structures, and (2) our model is capable of learning robust relationships between angles regardless of structure length.</p>
<h1>B Additional notes on self-consistency TM score</h1>
<p>Our scTM evaluation pipeline is similar to previous evaluations done by Trippe et al. (2022) and Lee \&amp; Kim (2022), with the primary difference that we use OmegaFold (Wu et al., 2022) instead of AlphaFold (Jumper et al., 2021). OmegaFold is designed without reliance on multiple sequence alignments (MSAs), and performs similarly to AlphaFold while generalizing better to orphan proteins that may not have such evolutionary neighbors (Wu et al., 2022). Furthermore, given that prior works use AlphaFold without MSA information in their evaluation pipelines, OmegaFold appears to be a more appropriate method for scTM evaluation.
OmegaFold is run using default parameters (and release1 weights). We also run AlphaFold without MSA input for benchmarking against Trippe et al. (2022). We provide a single sequence reformatted to mimic a "MSA" to the colabfold tool (Mirdita et al., 2022) with 15 recycling iterations. While the full AlphFold model runs 5 models and picks the best prediction, we use a singular model (model1) to reduce runtime.
Trippe et al. (2022) use ProteinMPNN (Dauparas et al., 2022) for inverse folding and generate 8 candidate sequences per structure, whereas Lee \&amp; Kim (2022) use ESM-IF1 (Hsu et al., 2022) and generate 10 candidate sequences for each structure. We performed self-consistency TM score</p>
<p>evaluation for both these methods, generating 8 candidate sequences using author-recommended temperature values ( $T=1.0$ for ESM-IF1, $T=0.1$ for ProteinMPNN). We use OmegaFold to fold all amino acid sequences for this comparison. We found that ProteinMPNN in $\mathrm{C}_{\alpha}$ mode (i.e., alpha-carbon mode) consistently yields much stronger scTM values (Tables S1, S2); we thus adopt ProteinMPNN for our primary results. While generating more candidate sequences leads to a higher scTM score (as there are more chances to encounter a successfully folded sequence), we conservatively choose to run 8 samples to be directly comparable to Trippe et al. (2022). We also use the same generation strategy as Trippe et al. (2022), generating 10 structures for each structure length $l \in[50,128)$ - thus the only difference in our scTM analyses is the generated structures themselves.</p>
<h1>C Ablations and baselines</h1>
<h2>C. 1 Substituting internal angle formulation for Cartesian coordinates</h2>
<p>We perform an "ablation" of our internal angle representation by replacing our framing of proteins as a series of inter-residue internal angles with a simple Cartesian representation of $C_{\alpha}$ coordinates $x \in \mathbb{R}^{N \times 3}$. Notably, this Cartesian representation is no longer rotation or shift invariant. We train a denoising diffusion model with this Cartesian representation, using the same variance schedule, transformer backbone architecture, and loss function, but sampling from a standard Gaussian and with all usages of our wrapping function $w$ removed. This represents the same modelling approach as our main diffusion model, with only our internal angle formulation removed.
To evaluate the quality of this Cartesian-based diffusion model's generated structures, we calculate the pairwise distances between all $C_{\alpha}$ atoms in its generated structures and compare these with distance matrices calculated for real proteins and for our internal angle diffusion model's generations. For a real protein, this produces a pattern that reveals close proximity between pairwise residues where the protein is folded inwards to produce a compact, coherent structure (Figure S4a). However, similarly visualizing the $C_{\alpha}$ pairwise distances in the Cartesian model's generated structures yields no significant proximity or patterns between any residues (Figure S4b). This suggests that the ablated Cartesian model cannot learn to generate meaningful structure, and instead generates a nondescript point cloud. Our internal angle model, on the other hand, produces a visualization that is very similar to that of real proteins (Figure S4c). Simply put, our model's performance drastically degrades when we change only how inputs are represented. This demonstrates the importance and effectiveness of our internal angle formulation.</p>
<h2>C. 2 Autoregressive baseline for angle generation</h2>
<p>As a baseline method for our generative diffusion model, we also implemented an autoregressive (AR) transformer $f_{\mathrm{AR}}$ that predicts the next set of six angles in a backbone structure (i.e., the same angles used by FoldingDiff, described in Figure 1 and Table 1) given all prior angles.
Architecturally, this model consists of the same transformer backbone as used in FoldingDiff combined with the same regression head converting per-token embeddings to angle outputs, though it is trained using absolute positional embeddings rather than relative embeddings as this improved validation loss. The total length of the sequence is encoded using random Fourier feature embeddings, similarly to how time was encoded in FoldingDiff, and this embedding is similarly added to each position in the sequence of angles. The model is trained to predict the $i$-th set of six angles given all prior angles, using masking to hide the $i$-th angle and onwards. We use the same wrapped smooth L1 loss as our main FoldingDiff model to handle the fact that these angle predictions exist in the range $[-\pi, \pi)$; specifically: $L_{w}\left(x^{(i)}, f_{\mathrm{AR}}\left(x^{(0, \ldots, i-1)}\right)\right)$ where superscripts indicate positional indexing. This approach is conceptually similar to causal language modeling (Radford et al., 2019), with the important difference that the inputs and outputs are continuous values, rather than (probabilities over) discrete tokens.
This model is trained using the same data set and data splits as our main FoldingDiff model with the same preprocessing and normalization. We train $f_{\mathrm{AR}}$ using the AdamW optimizer with weight decay set to 0.01 . We use a batch size of 256 over 10,000 epochs, linearly scaling the learning rate from 0 to $5 \times 10^{-5}$ over the first 1,000 epochs, and back to 0 over the remaining 9,000 epochs.</p>
<p>(a) $C_{\alpha}$ pairwise distances, real structure
<img alt="img-11.jpeg" src="img-11.jpeg" />
(b) $C_{\alpha}$ pairwise distances, Cartesian model
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure S4: Pairwise distances between all $C_{\alpha}$ atoms in various protein backbone structures, all of similar length. All panels use the same color scale. S4a illustrates a set of distances for a real protein structure; note the visual patterns that correspond to various secondary structures and potential contacts and interactions between residues. S4b shows these distances for a structure generated by an ablated model that replaces our proposed internal angle representation with Cartesian coordinates, which results in no coherent structural generation. For comparison, our FoldingDiff model produces structures that compactly fold to create many potential contacts, just as real proteins do (S4c).</p>
<p>To generate structures from $f_{\mathrm{AR}}$, we "seed" the autoregressive model with 4 sets of 6 angles taken from the corresponding first 4 angle sets in a randomly-chosen naturally occurring protein structure. This serves as a random, but biologically realistic, "prompt" for the model to begin generation. We then supply a fixed length $l$ and repeatedly run $f_{\mathrm{AR}}$ to obtain the next $i$-th set of angles, appending each prediction to the existing $i-1$ values in order to predict the $i+1$ set of angles. We repeat this until we reach our desired structure length.</p>
<p>We use the above procedure to generate 10 structures for each structure length $l \in[50,128)$ each with a different set of seed angles. Examples of generated structures of varying lengths are illustrated in Figure S5. All structures generated by this autoregressive approach consist of one singular $\alpha$ helix. This is confirmed by running P-SEA on the generated structures (Figure S6a). Besides being an obvious sign of modal collapse, these extremely long $\alpha$ helices are also not commonly observed in nature (Qin et al., 2013).</p>
<p>Such behavior where autoregressive models generate a single looped pattern endlessly has been observed in language models as well, where it is often circumvented using a temperature parameter to inject randomness. In our continuous regime, we try to do something similar by adding small amounts of noise to each angle during generation, but are unable to meaningfully deviate from the aforementioned modal collapse to endless $\alpha$ helices.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure S5: Structures generated using an autoregressive (AR) baseline approach trained on the same angle-based formulation we propose. This approach predicts the next set of angles given all prior angles, and can be thus used to iteratively generate structures in an autoregressive fashion (see Appendix C. 2 for additional details). However, the structures generated this way are all straight $\alpha$ helices, regardless of initial "prompt" angles (see Figure S6a). This complete lack of diversity and meaningful complexity indicates that while this AR model can produce technically "correct" structures, it cannot be used for generative modeling to any meaningful capacity. Figure 6 analogously illustrates FoldingDiff's generations, which are structurally much more diverse.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure S6: Secondary structure elements (a) and designability (b) for structures generated by the autoregressive baseline described in Appendix C.2. We observe that using P-SEA to annotate these generated structures detects exclusively singular $\alpha$ helices, and no $\beta$ sheets (S6a). This quantifies the observations in Figure S5 that the AR model has "collapsed" into repeatedly generating these coils. We find that these helices exhibit greater designability via scTM scores (computed using ProteinMPNN and OmegaFold) (S6b), with 693 of the 780 structures having scTM $\geq 0.5$, though this increase is not meaningful due to the utter lack of diversity in generated sequences.</p>
<p>We evaluate the AR model's generated structures' scTM designability using ProteinMPNN (CAonly mode) and OmegaFold. We find that 693 of the 780 generated structures are designable, leading to an overall designability ratio of 0.89 . (Figure S6b). Importantly however, this gain in designability is meaningless, as singular endless coils are biologically neither useful nor novel.</p>
<h1>C. 3 Baselines contextualizing scTM scores</h1>
<p>To contextualize FoldingDiff's scTM scores (Figure 5a), we implement a naive angle generation baseline. We take our test dataset, and concatenate all examples into a matrix of $\hat{x} \in[-\pi, \pi)^{\hat{N} \times 6}$, where $\hat{N}$ denotes the total number of angle sets in our test dataset, aggregating across all individual chains. To generate a backbone structure of length $l$, we simply sample $l$ indices from $U(0, \hat{N})$. This creates a chain that perfectly matches the natural distribution of protein internal angles, while also perfectly reproducing the pairwise correlations, i.e., of dihedrals in a Ramachandran plot, but critically loses the correct ordering of these angles. We randomly generate 780 such structures (10</p>
<p>samples for each integer value of $l \in[50,128)$ ). This is the same distribution of lengths as the generated set in our main analysis. For each of these, we perform scTM evaluation with ProteinMPNN (CA-only mode) and OmegaFold. The distribution of scTM scores for these randomly-sampled structures compared to that of FoldingDiff's generated backbones is shown in Figure S7. We observe that this random protein generation method produces significantly poorer scTM scores than FoldingDiff ( $p=1.6 \times 10^{-121}$, Mann-Whitney test). In fact, not a single structure generated this way is designable. This suggests that our model is not simply learning the overall distribution of angles, but is learning orderings and arrangements of angles that comprise folded protein structures.</p>
<h1>scTM scores, naive baseline vs. generated structures</h1>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure S7: Distribution of scTM scores for our generated structures (orange), compared to scTM scores for structures created by randomly shuffling naturally-occurring internal angles (blue). The randomly sampled angles result in no designable structures, despite perfectly capturing the overall distribution and pairwise relations between angles. This suggests our method correctly learns the spatial ordering of angles that folds a valid structure. We additionally take a set of 780 experimental structures and pass them through our scTM pipeline to evaluate the fragility of this pipeline itself. We find that $87 \%$ of natural proteins (green) are designable; this forms a "soft" upper bound for what would be achievable by sampling from the true data distribution.</p>
<p>We take the structures specified by these randomly sampled angles and analyze them for secondary structures using P-SEA, as we did for Figure 4. The resulting 2D histogram is illustrated in Figure S8, and represents a completely different distribution of secondary structures compared to natural structures, or that of FoldingDiff's generations. This further suggests that this random angle baseline cannot generate natural-appearing structures. It is clear that scTM scores and secondary structure analyses cannot be satisfied using this trivial solution.</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure S8: Secondary structures present in structures generated by randomly shuffling naturallyoccurring angles, as described in Appendix C.3. This random baseline produces a few hits to $\beta$ sheets by chance, but notably lacks the $\alpha$ helices present in both natural structures and FoldingDiff's generations (Figures 4, S11).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Work done during an internship at Microsoft Research&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
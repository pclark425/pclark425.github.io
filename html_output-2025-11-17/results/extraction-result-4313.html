<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4313 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4313</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4313</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-278782257</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.15094v1.pdf" target="_blank">SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown impressive capabilities in contextual understanding and reasoning. However, evaluating their performance across diverse scientific domains remains underexplored, as existing benchmarks primarily focus on general domains and fail to capture the intricate complexity of scientific data. To bridge this gap, we construct SciCUEval, a comprehensive benchmark dataset tailored to assess the scientific context understanding capability of LLMs. It comprises ten domain-specific sub-datasets spanning biology, chemistry, physics, biomedicine, and materials science, integrating diverse data modalities including structured tables, knowledge graphs, and unstructured texts. SciCUEval systematically evaluates four core competencies: Relevant information identification, Information-absence detection, Multi-source information integration, and Context-aware inference, through a variety of question formats. We conduct extensive evaluations of state-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their strengths and limitations in scientific context understanding, and offering valuable insights for the future development of scientific-domain LLMs.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4313.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4313.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-QGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Question Generation Pipeline (f_LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses LLMs to generate semantically diverse question-answer pairs from sampled scientific records (text, table rows, or KG triples) by prompting an LLM with a manually designed prompt concatenated with the data record.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-based Question Generation (f_LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each sampled data unit d_i from a source S, a manually authored prompt p is concatenated with d_i and passed to a generative LLM (denoted f_LLM) to produce (q_i, a_i) question-answer pairs. The prompts are specialized per evaluation competency (relevance identification, absence detection, multi-source integration, context-aware inference) to elicit concise, verifiable answers in several formats (QA, MCQ, T/F, cloze). Generated instances are then subjected to noise-injection (embedding-based distractors) and downstream verification.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>unspecified / various advanced generative LLM(s) (not concretely named for the generation step in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (materials science, chemistry, biology, biomedicine, physics)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>extraction of structured data fields and numerical/identifier relationships (e.g., material properties, chemical identifiers, numeric table entries, material IDs, space-group symbols)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>QA pairs and concise structured answers (table cell values, lists of identifiers, chemical formulas, InChIKeys, numeric properties)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Two-stage: (1) automated LLM-based judging (advanced LLMs used to check extractability/deducibility), then (2) manual expert validation by PhD-level reviewers evaluating competency alignment, clarity, and factual support.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Data-quality outcome: 90.83% of generated instances met the high-quality criteria after LLM judge + human expert validation. On downstream tasks using the dataset, top models achieved high info-identification accuracies (e.g., DeepSeek-R1 info-identification 94.13% (Table 4) and GPT-4o info-identification 89.72%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to direct answering without context (Table 7), providing context (generated/preserved) dramatically improved performance; e.g., GPT-4o MatTab direct: 14.64% → with context: 68.79%. No non-LLM automatic question-generation baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper does not claim discovery of high-level quantitative laws; limitations include sensitivity to prompt design, potential generation of ambiguous or low-quality questions without careful prompting, and dependence on downstream verification to remove hallucinated or ungrounded QA pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4313.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4313.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-Judge Automated Validation (GPT-4o example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated verification step using an advanced LLM (e.g., GPT-4o) as an evaluator to determine whether answers are explicitly extractable or logically deducible from provided contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-as-Judge automated evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>After LLM-based question generation, an advanced LLM is prompted with a verification template that includes the question, context, and candidate answer and is asked to output 'Yes' if (1) the answer is extractable or deducible from the context and (2) factually correct and adherent to the context, otherwise 'No'. Only instances marked 'Yes' by the LLM proceed to human expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o (explicitly mentioned as an example of the automated judge)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (same dataset domains)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>validation of extracted structured relationships and numeric/identifier facts (not a discovery method itself)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>binary judgment ('Yes'/'No') on extractability and correctness; used to filter QA/answer instances</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Automated LLM judgment followed by human expert validation (five PhD-level reviewers). The combined pipeline filtered and validated generated instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>After LLM-judge filtering and human expert review, 90.83% of instances met the high-quality criteria. The paper does not report standalone accuracy of the LLM-judge alone.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No explicit non-LLM automated validation baseline provided; human review served as final arbiter.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM-judge can err and still pass incorrect instances; thus human expert validation remains necessary. Reliance on an LLM judge can propagate LLM biases and failure modes into acceptance decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4313.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4313.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (explicit thinking) activation for structured extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an explicit 'thinking' or chain-of-thought activation in LLM inference to guide attention to relevant structured fields and produce more precise, higher-precision/recall extractions from tables and KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Chain-of-Thought (CoT) guided extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Models (e.g., Qwen3-8B) are run with an explicit 'thinking' activation (CoT) that elicits intermediate reasoning traces, constrains search to relevant fields (e.g., a specific table column), and reduces spurious outputs. This focuses the model on the structured targets (e.g., listing material IDs with a given Space Group Symbol) and suppresses irrelevant outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen3-8B (with explicit thinking activation); comparisons included Qwen3-8B without thinking</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science (table extraction) and other domains where structured fields are targeted</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>structured data extraction (lists of material IDs, space-group symbols, numeric/material properties) rather than discovery of new mathematical laws</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>lists of identifiers, constrained structured outputs (e.g., CSV-like enumerations), intermediate 'thought' traces optionally produced</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluated on SciCUEval gold labels (ground-truth answers). Example task: listing materials with Space Group Symbol 'Amm2'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Anecdotal case: Qwen3-8B with 'thinking' achieved 100% precision and recall on the example of listing materials with Space Group Symbol 'Amm2' (paper case study). Aggregate metrics: Qwen3-8B info-identification 88.29% (Table 4) and overall 64.69% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared Qwen3-8B with and without CoT: without thinking the model produced many false positives and lower recall; CoT improved precision/recall considerably in examples. No formal numeric baseline across all tasks beyond these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>CoT can increase verbosity and requires decoding/interpretation of intermediate reasoning; not guaranteed to generalize to all structured extraction tasks; performance can be sensitive to prompt design and CoT formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4313.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4313.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reasoning-Aug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning-Augmented LLMs (e.g., DeepSeek-R1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs augmented with explicit reasoning mechanisms (reasoning-augmented models) that improve factual grounding, multi-source information integration, and context-aware inference across modalities (text, tables, KGs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Reasoning-augmented LLM inference (DeepSeek-R1)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Models like DeepSeek-R1 incorporate explicit reasoning modules or prompting strategies that enforce stepwise reasoning and attention over multiple context entries, enabling more robust multi-source integration and context-aware inference. These models are evaluated on SciCUEval across modalities to assess their ability to synthesize evidence from heterogeneous data.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>DeepSeek-R1 (DeepSeek family) and other reasoning-augmented variants (e.g., DeepSeek-V3), compared to general LLMs and domain LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (materials, chemistry, biology, biomedicine, physics)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>synthesis of multi-source quantitative/relational information (e.g., aggregating table values, deriving relationships across KG triples, constructing compound answers from multiple data rows)—not explicit discovery of new analytic laws but extraction/aggregation of quantitative facts and relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>structured integrated answers (aggregated lists, synthesized numeric/identifier relationships, natural-language conclusions grounded in cited context segments)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluated on SciCUEval ground-truth answers with accuracy metrics per competency and modality (e.g., information-integration and context-aware inference tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DeepSeek-R1 achieved top scores in several competencies: overall 69.72% (Table 3), info-identification 94.13% (Table 4), multi-source integration 72.78% and context-aware inference 79.44% (Table 4). These outperform many baseline general and scientific-domain models in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed many general-purpose and domain-specific models; examples: DeepSeek-R1 info-identification 94.13% vs GPT-4o 89.72% and vs SciGLM-6B 33.24% (Table 4). Domain LLMs like ChemDFM-v1.5-8B and SciGLM-6B had substantially lower performance across competencies.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Despite strong performance, reasoning-augmented models still struggle with information-absence detection (many models show low rejection rates) and with structured modalities (tables/KGs) relative to text; they can still hallucinate and are sensitive to noisy distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4313.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4313.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embed-Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding-based Retrieval and Noise Injection (Sentence-BERT + Top-k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval/augmentation approach that computes embeddings (Sentence-BERT) for candidate data entries and injects top-k semantically-similar distractors into the context to simulate realistic noisy retrievals for robust evaluation and model training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Embedding-based retrieval and distractor injection</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute Sentence-BERT embeddings for all candidate entries in dataset D, then for a target data unit d_i retrieve Top-k nearest neighbors by cosine similarity to form a distractor set N_i. The final sample context is d_i concatenated with N_i. Parameters in the paper: k in [200,300] for structured tables and KGs and k=5 for unstructured text, to simulate realistic noisy contexts for evaluation of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Sentence-BERT for embeddings (reimers & Gurevych, 2019) plus downstream LLMs for extraction/QA (various), e.g., GPT-4o, DeepSeek-R1, Qwen3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (used to create noisy contexts across materials, chemistry, biology, biomedicine, physics datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not a law-discovery algorithm per se; used to stress-test extraction of quantitative/identifier relationships (table numeric entries, material properties, identifiers) under noisy retrieval conditions</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>enriched/noisy context containing target record + top-k distractor entries; downstream outputs are structured QA answers (identifiers, numbers, formulas)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Downstream model performance evaluated on SciCUEval gold labels (accuracy metrics). Noise injection intended to emulate realistic retrieval noise; validation relied on LLM-judge + human review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Noise parameters: k ∈ [200,300] for tables/KGs and k=5 for text. Paper demonstrates that performance degrades under noise for weaker models and that reasoning/CoT mechanisms mitigate distractor effects; no single scalar performance for the retrieval step itself reported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to contexts without distractors (implicit baseline), noisy contexts reduced performance for many models; reasoning-augmented and CoT-enabled models exhibited stronger robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Large k for structured modalities produces highly challenging contexts; this can exceed some models' context windows and cause errors. Embedding similarity can select semantically-similar but misleading distractors that induce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Benchmarking retrieval-augmented generation for medicine <em>(Rating: 2)</em></li>
                <li>Longbench: A bilingual, multitask benchmark for long context understanding. <em>(Rating: 1)</em></li>
                <li>Scibert: A pretrained language model for scientific text. <em>(Rating: 1)</em></li>
                <li>LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. <em>(Rating: 1)</em></li>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4313",
    "paper_id": "paper-278782257",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "LLM-QGen",
            "name_full": "LLM-based Question Generation Pipeline (f_LLM)",
            "brief_description": "A pipeline that uses LLMs to generate semantically diverse question-answer pairs from sampled scientific records (text, table rows, or KG triples) by prompting an LLM with a manually designed prompt concatenated with the data record.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LLM-based Question Generation (f_LLM)",
            "method_description": "For each sampled data unit d_i from a source S, a manually authored prompt p is concatenated with d_i and passed to a generative LLM (denoted f_LLM) to produce (q_i, a_i) question-answer pairs. The prompts are specialized per evaluation competency (relevance identification, absence detection, multi-source integration, context-aware inference) to elicit concise, verifiable answers in several formats (QA, MCQ, T/F, cloze). Generated instances are then subjected to noise-injection (embedding-based distractors) and downstream verification.",
            "llm_model_used": "unspecified / various advanced generative LLM(s) (not concretely named for the generation step in the paper)",
            "scientific_domain": "multi-domain (materials science, chemistry, biology, biomedicine, physics)",
            "number_of_papers": null,
            "type_of_quantitative_law": "extraction of structured data fields and numerical/identifier relationships (e.g., material properties, chemical identifiers, numeric table entries, material IDs, space-group symbols)",
            "extraction_output_format": "QA pairs and concise structured answers (table cell values, lists of identifiers, chemical formulas, InChIKeys, numeric properties)",
            "validation_method": "Two-stage: (1) automated LLM-based judging (advanced LLMs used to check extractability/deducibility), then (2) manual expert validation by PhD-level reviewers evaluating competency alignment, clarity, and factual support.",
            "performance_metrics": "Data-quality outcome: 90.83% of generated instances met the high-quality criteria after LLM judge + human expert validation. On downstream tasks using the dataset, top models achieved high info-identification accuracies (e.g., DeepSeek-R1 info-identification 94.13% (Table 4) and GPT-4o info-identification 89.72%).",
            "baseline_comparison": "Compared to direct answering without context (Table 7), providing context (generated/preserved) dramatically improved performance; e.g., GPT-4o MatTab direct: 14.64% → with context: 68.79%. No non-LLM automatic question-generation baseline reported.",
            "challenges_limitations": "Paper does not claim discovery of high-level quantitative laws; limitations include sensitivity to prompt design, potential generation of ambiguous or low-quality questions without careful prompting, and dependence on downstream verification to remove hallucinated or ungrounded QA pairs.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4313.0",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM-Judge",
            "name_full": "LLM-as-Judge Automated Validation (GPT-4o example)",
            "brief_description": "An automated verification step using an advanced LLM (e.g., GPT-4o) as an evaluator to determine whether answers are explicitly extractable or logically deducible from provided contexts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "LLM-as-Judge automated evaluator",
            "method_description": "After LLM-based question generation, an advanced LLM is prompted with a verification template that includes the question, context, and candidate answer and is asked to output 'Yes' if (1) the answer is extractable or deducible from the context and (2) factually correct and adherent to the context, otherwise 'No'. Only instances marked 'Yes' by the LLM proceed to human expert review.",
            "llm_model_used": "GPT-4o (explicitly mentioned as an example of the automated judge)",
            "scientific_domain": "multi-domain (same dataset domains)",
            "number_of_papers": null,
            "type_of_quantitative_law": "validation of extracted structured relationships and numeric/identifier facts (not a discovery method itself)",
            "extraction_output_format": "binary judgment ('Yes'/'No') on extractability and correctness; used to filter QA/answer instances",
            "validation_method": "Automated LLM judgment followed by human expert validation (five PhD-level reviewers). The combined pipeline filtered and validated generated instances.",
            "performance_metrics": "After LLM-judge filtering and human expert review, 90.83% of instances met the high-quality criteria. The paper does not report standalone accuracy of the LLM-judge alone.",
            "baseline_comparison": "No explicit non-LLM automated validation baseline provided; human review served as final arbiter.",
            "challenges_limitations": "LLM-judge can err and still pass incorrect instances; thus human expert validation remains necessary. Reliance on an LLM judge can propagate LLM biases and failure modes into acceptance decisions.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4313.1",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CoT-Extraction",
            "name_full": "Chain-of-Thought (explicit thinking) activation for structured extraction",
            "brief_description": "Using an explicit 'thinking' or chain-of-thought activation in LLM inference to guide attention to relevant structured fields and produce more precise, higher-precision/recall extractions from tables and KGs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Chain-of-Thought (CoT) guided extraction",
            "method_description": "Models (e.g., Qwen3-8B) are run with an explicit 'thinking' activation (CoT) that elicits intermediate reasoning traces, constrains search to relevant fields (e.g., a specific table column), and reduces spurious outputs. This focuses the model on the structured targets (e.g., listing material IDs with a given Space Group Symbol) and suppresses irrelevant outputs.",
            "llm_model_used": "Qwen3-8B (with explicit thinking activation); comparisons included Qwen3-8B without thinking",
            "scientific_domain": "materials science (table extraction) and other domains where structured fields are targeted",
            "number_of_papers": null,
            "type_of_quantitative_law": "structured data extraction (lists of material IDs, space-group symbols, numeric/material properties) rather than discovery of new mathematical laws",
            "extraction_output_format": "lists of identifiers, constrained structured outputs (e.g., CSV-like enumerations), intermediate 'thought' traces optionally produced",
            "validation_method": "Evaluated on SciCUEval gold labels (ground-truth answers). Example task: listing materials with Space Group Symbol 'Amm2'.",
            "performance_metrics": "Anecdotal case: Qwen3-8B with 'thinking' achieved 100% precision and recall on the example of listing materials with Space Group Symbol 'Amm2' (paper case study). Aggregate metrics: Qwen3-8B info-identification 88.29% (Table 4) and overall 64.69% (Table 3).",
            "baseline_comparison": "Compared Qwen3-8B with and without CoT: without thinking the model produced many false positives and lower recall; CoT improved precision/recall considerably in examples. No formal numeric baseline across all tasks beyond these comparisons.",
            "challenges_limitations": "CoT can increase verbosity and requires decoding/interpretation of intermediate reasoning; not guaranteed to generalize to all structured extraction tasks; performance can be sensitive to prompt design and CoT formatting.",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4313.2",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Reasoning-Aug",
            "name_full": "Reasoning-Augmented LLMs (e.g., DeepSeek-R1)",
            "brief_description": "LLMs augmented with explicit reasoning mechanisms (reasoning-augmented models) that improve factual grounding, multi-source information integration, and context-aware inference across modalities (text, tables, KGs).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Reasoning-augmented LLM inference (DeepSeek-R1)",
            "method_description": "Models like DeepSeek-R1 incorporate explicit reasoning modules or prompting strategies that enforce stepwise reasoning and attention over multiple context entries, enabling more robust multi-source integration and context-aware inference. These models are evaluated on SciCUEval across modalities to assess their ability to synthesize evidence from heterogeneous data.",
            "llm_model_used": "DeepSeek-R1 (DeepSeek family) and other reasoning-augmented variants (e.g., DeepSeek-V3), compared to general LLMs and domain LLMs.",
            "scientific_domain": "multi-domain (materials, chemistry, biology, biomedicine, physics)",
            "number_of_papers": null,
            "type_of_quantitative_law": "synthesis of multi-source quantitative/relational information (e.g., aggregating table values, deriving relationships across KG triples, constructing compound answers from multiple data rows)—not explicit discovery of new analytic laws but extraction/aggregation of quantitative facts and relationships",
            "extraction_output_format": "structured integrated answers (aggregated lists, synthesized numeric/identifier relationships, natural-language conclusions grounded in cited context segments)",
            "validation_method": "Evaluated on SciCUEval ground-truth answers with accuracy metrics per competency and modality (e.g., information-integration and context-aware inference tasks).",
            "performance_metrics": "DeepSeek-R1 achieved top scores in several competencies: overall 69.72% (Table 3), info-identification 94.13% (Table 4), multi-source integration 72.78% and context-aware inference 79.44% (Table 4). These outperform many baseline general and scientific-domain models in the benchmark.",
            "baseline_comparison": "Outperformed many general-purpose and domain-specific models; examples: DeepSeek-R1 info-identification 94.13% vs GPT-4o 89.72% and vs SciGLM-6B 33.24% (Table 4). Domain LLMs like ChemDFM-v1.5-8B and SciGLM-6B had substantially lower performance across competencies.",
            "challenges_limitations": "Despite strong performance, reasoning-augmented models still struggle with information-absence detection (many models show low rejection rates) and with structured modalities (tables/KGs) relative to text; they can still hallucinate and are sensitive to noisy distractors.",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4313.3",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Embed-Retrieval",
            "name_full": "Embedding-based Retrieval and Noise Injection (Sentence-BERT + Top-k)",
            "brief_description": "A retrieval/augmentation approach that computes embeddings (Sentence-BERT) for candidate data entries and injects top-k semantically-similar distractors into the context to simulate realistic noisy retrievals for robust evaluation and model training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Embedding-based retrieval and distractor injection",
            "method_description": "Compute Sentence-BERT embeddings for all candidate entries in dataset D, then for a target data unit d_i retrieve Top-k nearest neighbors by cosine similarity to form a distractor set N_i. The final sample context is d_i concatenated with N_i. Parameters in the paper: k in [200,300] for structured tables and KGs and k=5 for unstructured text, to simulate realistic noisy contexts for evaluation of LLMs.",
            "llm_model_used": "Sentence-BERT for embeddings (reimers & Gurevych, 2019) plus downstream LLMs for extraction/QA (various), e.g., GPT-4o, DeepSeek-R1, Qwen3-8B",
            "scientific_domain": "multi-domain (used to create noisy contexts across materials, chemistry, biology, biomedicine, physics datasets)",
            "number_of_papers": null,
            "type_of_quantitative_law": "not a law-discovery algorithm per se; used to stress-test extraction of quantitative/identifier relationships (table numeric entries, material properties, identifiers) under noisy retrieval conditions",
            "extraction_output_format": "enriched/noisy context containing target record + top-k distractor entries; downstream outputs are structured QA answers (identifiers, numbers, formulas)",
            "validation_method": "Downstream model performance evaluated on SciCUEval gold labels (accuracy metrics). Noise injection intended to emulate realistic retrieval noise; validation relied on LLM-judge + human review.",
            "performance_metrics": "Noise parameters: k ∈ [200,300] for tables/KGs and k=5 for text. Paper demonstrates that performance degrades under noise for weaker models and that reasoning/CoT mechanisms mitigate distractor effects; no single scalar performance for the retrieval step itself reported.",
            "baseline_comparison": "Compared to contexts without distractors (implicit baseline), noisy contexts reduced performance for many models; reasoning-augmented and CoT-enabled models exhibited stronger robustness.",
            "challenges_limitations": "Large k for structured modalities produces highly challenging contexts; this can exceed some models' context windows and cause errors. Embedding similarity can select semantically-similar but misleading distractors that induce hallucination.",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4313.4",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Benchmarking retrieval-augmented generation for medicine",
            "rating": 2,
            "sanitized_title": "benchmarking_retrievalaugmented_generation_for_medicine"
        },
        {
            "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding.",
            "rating": 1,
            "sanitized_title": "longbench_a_bilingual_multitask_benchmark_for_long_context_understanding"
        },
        {
            "paper_title": "Scibert: A pretrained language model for scientific text.",
            "rating": 1,
            "sanitized_title": "scibert_a_pretrained_language_model_for_scientific_text"
        },
        {
            "paper_title": "LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset.",
            "rating": 1,
            "sanitized_title": "llasmol_advancing_large_language_models_for_chemistry_with_a_largescale_comprehensive_highquality_instruction_tuning_dataset"
        },
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp"
        }
    ],
    "cost": 0.01830525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models</p>
<p>Jing Yu yujing17@zju.edu.cn 
ZJU-Hangzhou Global Scientific and Technological Innovation Center
Zhejiang University</p>
<p>The Polytechnic Institute
Zhejiang University</p>
<p>Yuqi Tang 
ZJU-UIUC
Zhejiang University</p>
<p>Kehua Feng 
ZJU-Hangzhou Global Scientific and Technological Innovation Center
Zhejiang University</p>
<p>College of Computer Science and Technology
Zhejiang University</p>
<p>Mingyang Rao 
ZJU-Hangzhou Global Scientific and Technological Innovation Center
Zhejiang University</p>
<p>College of Computer Science and Technology
Zhejiang University</p>
<p>Lei Liang 
Zhiqiang Zhang 
Mengshu Sun 
Wen Zhang 
College of Computer Science and Technology
Zhejiang University</p>
<p>Qiang Zhang 
ZJU-UIUC
Zhejiang University</p>
<p>Keyan Ding dingkeyan@zju.edu.cn 
ZJU-Hangzhou Global Scientific and Technological Innovation Center
Zhejiang University</p>
<p>Huajun Chen huajunsir@zju.edu.cn 
ZJU-Hangzhou Global Scientific and Technological Innovation Center
Zhejiang University</p>
<p>College of Computer Science and Technology
Zhejiang University</p>
<p>Qingyu Chen 
Yan Hu 
Xueqing Peng 
Qianqian Xie 
Qiao Jin 
Aidan Gilson 
XuguangMaxwell B Singer 
Wenhu Chen 
Hanwen Zha 
Zhiyu Chen 
Wenhan Xiong 
Hong Wang 
William Wang 
Hy 
Chengqi Zhao 
Chenyu Deng 
Chong Zhang 
Damai Ruan 
Daya Dai 
Dejian Guo 
Deli Yang 
Dongjie Chen 
Erhang Ji 
Fangyun Li 
Fucong Lin 
Fuli Dai 
Guangbo Luo 
Guanting Hao 
Guowei Chen 
H Li 
Han Zhang 
Bao 
Daya Guo 
Dejian Yang 
Haowei Zhang 
Junxiao Song 
Ruoyu Zhang 
Runxin Xu 
Qihao Zhu 
Shirong Ma 
Yunpeng Huang 
Jingwei Xu 
Junyu Lai 
Zixu Jiang 
Taolue Chen 
Zenan Li 
Yuan Yao 
Xiaoxing Ma 
Li- Juan Yang 
Hao Chen 
Albert Q Jiang 
Alexandre Sablayrolles 
Arthur Men- Sch 
Chris Bamford 
Singh Devendra 
Diego Chaplot 
Patrick Lewis 
Ethan Perez 
Aleksandra Piktus 
Fabio Petroni 
Vladimir Karpukhin 
Naman Goyal 
Aaron Openai 
Adam Hurst 
Adam P Lerer 
Adam Goucher 
Aditya Perelman 
Aidan Ramesh 
A J Clark 
Akila Ostrow 
Alan Welihinda 
Alec Hayes 
Aleksander M Radford 
Alex Ądry 
Alex Baker-Whitcomb 
Alex Beutel 
Alex Borzunov 
Alex Carney 
Alex Chow 
Alex Kirillov 
Alex Nichol 
Alex Paino </p>
<p>Ai
Po-Ting Lai, Zhizheng Wang2025</p>
<p>Charese Smiley
Sameena Shah</p>
<p>Iana Borova
Matt Beane, Ting-Hao HuangDylan Langdon, Reema Moussa, Bryan Routledge</p>
<p>DeepSeek-AI
Bingx-uan Wang
Bing XueAixin Liu, Bei Feng, Bochao Wu</p>
<p>Chengda Lu
Chenggang</p>
<p>Kehua Feng
Keyan Ding, Weijie Wang, Xiang Zhuang, Ming Qin, Yu ZhaoZeyuan Wang, Jianhua Yao</p>
<p>de las Casas
Florian BressandGianna Lengyel</p>
<p>SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models
E7F5316F605BCB375DABF05C51E84C43arXiv:2503.04013.
Large Language Models (LLMs) have shown impressive capabilities in contextual understanding and reasoning.However, evaluating their performance across diverse scientific domains remains underexplored, as existing benchmarks primarily focus on general domains and fail to capture the intricate complexity of scientific data.To bridge this gap, we construct SciCUEval, a comprehensive benchmark dataset tailored to assess the scientific context understanding capability of LLMs.It comprises ten domain-specific subdatasets spanning biology, chemistry, physics, biomedicine, and materials science, integrating diverse data modalities including structured tables, knowledge graphs, and unstructured texts.SciCUEval systematically evaluates four core competencies: Relevant information identification, Information-absence detection, Multisource information integration, and Contextaware inference, through a variety of question formats.We conduct extensive evaluations of state-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their strengths and limitations in scientific context understanding, and offering valuable insights for the future development of scientific-domain LLMs.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated strong capabilities in natural language understanding, reasoning, and generation across a wide range of general-domain tasks (Bai et al., 2023a;OpenAI et al., 2024;Dubey et al., 2024;Qwen et al., 2025).However, their application to scientific domains remains challenging due to the unique characteristics of scientific language and knowledge.Scientific texts are often dense with technical terminology, implicit assumptions, multimodal data representations, and tightly interlinked concepts that require deeper contextual comprehension (Beltagy et al., 2019;Mann et al., 2020).</p>
<p>Existing LLM benchmarks in scientific domains (Chen et al., 2025;Sun et al., 2024;Feng et al., 2024;Saikh et al., 2022;Pedersen et al., 2020;Rubungo et al., 2024;Jiang et al., 2025) primarily focus on direct question-answering tasks, offering limited insight into how well LLMs perform in scientific context understanding, particularly for noisy and lengthy contexts.Additionally, they often neglect the heterogeneous and structured nature of scientific data, which can span textual descriptions, relational graphs (Talmor and Berant, 2018;He et al., 2024), and tabular datasets (Fang et al., 2024).In contrast, robust scientific context understanding demands precise information extraction, the ability to identify gaps or missing elements in context, and the integration of multiple evidence sources to support accurate conclusions.</p>
<p>To address this gap, we introduce SciCUEval, a comprehensive benchmark dataset designed to rigorously evaluate the scientific context understanding capabilities of LLMs.As shown in Figure 1, SciCUEval spans ten domain-specific subdatasets covering diverse disciplines such as biology, chemistry, physics, biomedicine, and materials science.Each subset incorporates rich external knowledge in multiple forms (structured tables, semi-structured knowledge graphs, and unstructured scientific texts) to represent the data modalities commonly encountered in scientific research.</p>
<p>SciCUEval targets four core competencies essential for scientific understanding: (1) Relevant Information Identification that locate and extract relevant information from complex and lengthy inputs;</p>
<p>(2) Information-Absence Detection that recognize missing, ambiguous, or incomplete contextual elements; (3) Multi-source Information Integration that aggregate and compare information from diverse sources; and (4) Context-Aware Inference that deduce accurate conclusions grounded in sci- For the material with CID 13182, what is its inchikey?entific contexts.These competencies are evaluated using diverse question types, including open-ended Q&amp;A, multiple-choice, content completion, and true/false validation.The contributions of this paper are summarized as follows:
CID-13182, •••••••••••••••••••••••••••••••••••••••••• √ CID XXXXX, ••••••••••••••••••••••••••••••••••••••••••••••••••• × CID XXXXX, ••••••••••••••••••••••••••••••••••••••••••••••••••• ×
• Establishing a scientific context understanding benchmark: We establish a benchmark to evaluate context understanding capabilities of LLMs in scientific domains, serving as a standardized evaluation suite for assessing LLMs' capabilities in identifying, detecting, integrating, and reasoning over scientific contexts.</p>
<p>• Constructing a diverse set of domain-specific context understanding datasets: We construct ten sub-datasets across multiple disciplines, encompassing various data modalities and a wide range of question types to ensure comprehensive evaluation.</p>
<p>• Extensive evaluation and analysis of LLMs:</p>
<p>We systematically evaluate and analyze the performance of various state-of-the-art LLMs on SciCUEval, highlighting their strengths and limitations, and offering insights for improvement.</p>
<p>Backgrounds and Related Works</p>
<p>Context Understanding Tasks Large language models (LLMs) have demonstrated remarkable capabilities in context understanding (Huang et al., 2023;Dong et al., 2022).This paradigm allows LLMs to adapt flexibly to new external knowledge without requiring additional fine-tuning or retraining (Lewis et al., 2020;Liu et al., 2024).Importantly, LLMs have shown strong performance across a wide range of domains when equipped with additional context.Furthermore, in-context learning strengthens LLMs' transparency by firmly establishing their arguments in the documents that were obtained (Mialon et al., 2023;Xiong et al., 2024).However, despite its potential, context understanding remains sensitive to prompt design and the quality of the provided context.It also demands that the model possess a robust ability to process and comprehend long texts.In this study, we systematically investigate the robustness and effectiveness of LLMs for context understanding in diverse scientific domains.</p>
<p>Context Understanding Benchmarks The development of robust and comprehensive benchmarks for evaluating long context understanding has gained increasing attention in recent research (Li et al., 2024;Liu et al., 2024;Bai et al., 2023b;Zhang et al., 2024c;Wu et al., 2025;Chen et al., 2020Chen et al., , 2021))</p>
<p>Datasets</p>
<p>This section presents the dataset construction process in SciCUEval, which involves formulating evaluation competencies, collecting scientific data, generating questions and answers, and conducting rigorous verification.</p>
<p>Evaluation Competencies</p>
<p>Inspired by the ability definition in (Chen et al., 2024), we formulate four capabilities essential for evaluating LLMs in scientific contexts:</p>
<p>• Relevant Information Identification: LLMs must effectively distinguish between relevant information and extraneous noise within complex scientific contexts.In real-world scenarios, scientific data often contains contextually related but non-essential information.Robust models should be able to filter out such noise to ensure accurate understanding and responses.</p>
<p>• Information-absence Detection: The ability to abstain from responding when all contextual data is irrelevant or unreliable.Scientific queries often require precise evidence, and when no valid information is present in the context, LLMs should refrain from generating speculative or hallucinated responses.</p>
<p>• Multi-source Information Integration: Scientific queries often require synthesizing data from multiple sources.LLMs must aggregate and compare information across different contextual segments to generate precise and contextually grounded answers.These four competencies form the foundation of SciCUEval and provide a systematic evaluation framework for context understanding in scientific domains.</p>
<p>Source Data Collection</p>
<p>Scientific Domains To evaluate LLMs in scientific contexts comprehensively, we curate data from diverse scientific domains, including Biology, Chemistry, Physics, Biomedicine, and Materials Science.These disciplines are fundamental to modern science, encompassing a wide range of knowledge from theoretical principles to experimental data, ensuring a broad and representative assessment of long-context understanding capabilities in scientific applications.</p>
<p>Data Modalities</p>
<p>To support a broad evaluation of scientific context understanding capabilities, we consider three distinct data modalities: (1) Unstructured Text, (2) Structured Tables , and (3) Semistructured Knowledge Graphs (KGs).Each modality presents unique challenges, enabling a holistic assessment of LLMs' retrieval, synthesis, reasoning, and integration capabilities in scientific domains.Specifically, unstructured text corpora consist of scientific literature, allowing LLMs to retrieve, synthesize, and infer domain knowledge from textual sources.We collect thousands of recent research papers and experimental protocols from open-access repositories such as arXiv.Structured tables contain numerical and categorical data, testing LLMs' capacity to interpret structured knowledge, recognize contextual dependencies, and perform quantitative reasoning.We collect nuclear data from IAEA 1 , material properties from Material Project 2 , and molecular and protein properties from PubChem 3 .Knowledge graphs encode scientific knowledge as interconnected entities and relational networks, enabling the assessment of LLMs' abilities in relational inference, hierarchical knowledge traversal, and crossdomain knowledge synthesis.We collect wellestablished scientific KGs, including Gene Ontology 4 for gene-function relationships, HIPPIE (Alanis-Lobato et al., 2016) for protein-protein interactions, PharmKG (Zheng et al., 2021) for drugtarget interactions, and PrimeKG (Chandak et al., 2023) for clinical entity relationships.</p>
<p>Data Generation</p>
<p>Building on the collected source data, we construct corresponding datasets tailored to assess the proposed four competencies outlined in Sec.3.1.The data generation pipeline is illustrated in Figure 2, which involves (1) question generation, (2) noise injection, and (3) quality control.</p>
<p>Question Generation</p>
<p>We first sample a subset of texts, table rows, or KG triples from the full databases,
D = {d i | d i = ϕ(S), i = 1, 2, . . . , N }, (1)
where S denote the large-scale scientific data source, and ϕ is the sampling operation.d i is a 1 https://www-nds.iaea.org 2 https://next-gen.materialsproject.org 3 https://pubchem.ncbi.nlm.nih.gov 4 https://geneontology.org  single data record or a small set of related entries.</p>
<p>Given each selected data unit d i , the question generation process is defined as:
(q i , a i ) = f LLM (p ⊕ d i ),(2)
where p is a manually designed prompt, and ⊕ denotes string concatenation.f LLM represents the LLM that generates semantically diverse and contextually relevant questions q i and corresponding answers a i based on d i .For each evaluation competency, we craft the prompt to ensure the questions are reasonable and aligned with the required competencies.The detailed prompts are provided in Appendix B. These generated questions span various formats, including Q&amp;A, multiple-choice, content completion, and true/false validation, offering a robust assessment of context understanding abilities.</p>
<p>Noise Injection Following question generation, we extract noisy information from the source data and inject them into the context.Specifically, we inject semantically similar yet unrelated entries into the context using an embedding-based similarity search.Formally, each sample before noise injection is denoted as x i = (q i , a i , d i ).To select distractor entries, we first compute embeddings for all candidate entries in the dataset D using Sentence-BERT (Reimers and Gurevych, 2019).
h d j = f S-BERT (d j ), ∀d j ∈ D,(3)
where f S-BERT denotes the embedding function.</p>
<p>We then employ the cosine similarity to efficiently retrieve the Top-k entries most similar to the se- lected entry d i :
N i = Top -k d j ∈D{d i } sim h d i , h d j ,(4)
where sim(•, •) denotes the cosine similarity between embedding vectors.The final sample after noise injection is represented as xi = (q i , a i , d i ⊕ N i ), where N i contains the k selected distractor entries used to augment the context.We sample k ∈ [200,300] for structured tables and KGs, and set k = 5 for unstructured text.Through this approach, the injected noise closely mimics the type of confusing or misleading information that LLMs may encounter in practice, ensuring the benchmark dataset remains both challenging and realistic.</p>
<p>Quality Control To maintain the rigor of the constructed dataset, we implement a two-stage verification process to ensure data quality:</p>
<p>• LLM as a Judge.we used advanced LLMs (e.g.,  as automated evaluators to check if each answer is directly extractable or logically deducible from the provided context, using a clear prompt.Only instances marked "Yes" were kept.</p>
<p>• Human Expert Validation.Domain experts then manually reviewed the filtered data based on three aspects: (1) whether the question tests the intended competency, (2) whether the question is expressed clearly and logically, and (3) weather the answer is fully supported by contexts and factually correct.Only instances that received a "Yes" for all three criteria were accepted.</p>
<p>As a result, 90.83% of instances in our dataset met the high-quality criteria.Detailed information about data quality control can be found in Appendix C.</p>
<p>The Final Dataset</p>
<p>Based on the data collection, generation, and quality control processes described above, we construct the final SciCUEval</p>
<p>Experiments</p>
<p>In this section, we evaluate the performance of various LLMs on SciCUEval, and provide a thorough analysis of their capabilities in understanding scientific contexts.2024), ChemLLM-7B-Chat (Zhang et al., 2024b), ChemDFM-v1.5-8B (Zhao et al., 2024)).For detailed information about these models, please refer to Appendix F.</p>
<p>Settings To ensure a fair evaluation across all models, we adopt a unified prompting template that standardizes input formatting.Specifically, each input consists of a system prompt that specifies the question type and defines answer format requirements, contexts, and a question designed to assess one of the four core competencies in SciCUEval.</p>
<p>Given that each question in SciCUEval has a deterministic answer, we adopt accuracy as the evaluation metric for all question types across the tasks of relevant information identification, multi-source information integration, and context-aware inference.</p>
<p>For the task of information-absence detection, we use the rejection rate as the evaluation metric.</p>
<p>Overall Results</p>
<p>Table 3  Second, proprietary models such as GPT-4o and Claude-3.5-Sonnetmaintain competitive performance, especially in unstructured text-based domains (e.g., BioText, MatText), benefiting from their superior language understanding and generalization capabilities.Third, scientific-domain LLMs such as ChemDFM-v1.5-8B and SciGLM-6B exhibit substantially lower performance across all datasets.Although designed for scientific domains, these models tend to lack general reasoning capacity and robustness across modalities.Fourth, there is a strong positive correlation between model size and effectiveness.Large-scale models (e.g., GPT-4o, Llama4-Maverick, and Llama3.1-70B)consistently outperform their smaller counterparts (e.g., GPT-4o-mini, Llama4-Scout, and Llama3.1-8B)across most domains.</p>
<p>Evaluation Results of Four Competencies</p>
<p>Relevant Information Identification This competency measures a model's ability to locate and select the correct pieces of information from the provided context.As shown in Figure 4, DeepSeek-R1 leads all of the evaluated models, suggesting that explicit reasoning mechanisms effectively enhance factual grounding.DeepSeek-V3, GPT-4o, and Qwen-8B also exhibit strong performance, showing the advantages of in-context retrieval capabilities.In contrast, scientific-domain LLMs exhibit notably weaker performance in identifying relevant contexts across diverse scenarios.Information-absence Detection This metric evaluates whether a model appropriately withholds an answer when the required information is absent.Claude-3.5-Sonnetand Qwen-8B demonstrate relatively high accuracy, suggesting their conservative answering strategy and stronger understanding of uncertainty.Most models struggle in this competency, with scores below 20%, indicating a tendency to hallucinate answers when uncertain.This highlights the risk of "overconfidence" in current models, which may pose potential safety risks in the scientific domain.
-7 0 B -i t Q w e n 2 . 5 -7 B -i t G L M 4 -9 B L l a m a 3 . 1 -8 B -i t G e m m a 2 -9 B -i t M i n i s t r a l -8 B -i t C h e m D F M -v 1 . 5 -8 B S c i G L M -6 B L l a S M o l -M i s t r a l -7 B C h e m L L M -7 B -C h</p>
<p>Multi-source Information Integration</p>
<p>This task assesses a model's ability to synthesize information from multiple entries to construct a complete answer.DeepSeek-R1 achieves the highest performance, followed by GPT-4o and Llama4-Maverick, suggesting that these models are better equipped to combine multiple data points into coherent and accurate answers.Among smaller opensource models, GLM4-9B shows a competitive score, even surpassing DeepSeek-V3 in this competency.However, scientific LLMs significantly lag behind, indicating that while these domain-specific models are adept at handling scientific text, they face challenges in effectively synthesizing information from diverse sources.</p>
<p>Context-aware Inference This capability reflects a model's ability to reason over contextually relevant information.DeepSeek-R1 achieves the highest performance, and GPT-4o and Qwen3-8B also perform well, indicating that large-scale models and those enhanced with explicit thinking benefit significantly in contextual reasoning tasks.In contrast, models like Claude-3.5-Sonnetand DeepSeek-V3 show moderate capabilities but fall behind on deeper inference.Scientific-domain models such as ChemLLM-7B-Chat and SciGLM perform poorly, indicating limited general reasoning capabilities despite domain specialization.</p>
<p>Evaluation Results of Three Modalities</p>
<p>Figure 4 shows the performance of LLMs across three modalities: Text, Table , and KG, highlighting their strengths and weaknesses in handling diverse scientific data formats.Overall, LLMs tend to perform best on the text modality, reflecting their strong natural language understanding and generation capabilities.Notably, some smaller models even exceed their average overall performance on text, indicating a relative maturity in handling unstructured text data.In the table modality, reasoning-augmented models demonstrate a clear advantage, suggesting that explicit reasoning mechanisms and the ability to process structured data significantly benefit table understanding.</p>
<p>In contrast, general LLMs show weaker performance on tables, implying challenges in leveraging tabular structure with traditional language modeling approaches.Similarly, for KG data, models with reasoning enhancements again lead, reflecting their ability to leverage relational and graphstructured information effectively.Additionally, domain-specific scientific models consistently underperform across all three modalities compared to general-purpose or reasoning-augmented models.</p>
<p>Further Discussions</p>
<p>Our experimental results highlight three key discrepancies in the performance of LLMs on scientific context understanding tasks, underscoring fundamental challenges that require further advancements.</p>
<p>Competency Discrepancy</p>
<p>The evaluation results reveal notable disparities across the four core competencies.While top-performing models exhibit relatively strong capabilities in iden-  tifying relevant information, they struggle with information-absence detection-the ability to abstain from answering when faced with unreliable or insufficient evidence.This suggests that models prioritize generating responses over ensuring accuracy, increasing the risk of hallucinations in scientific applications where factual correctness is critical.To address this, models should incorporate uncertainty quantification techniques, such as confidence-based rejection mechanisms and calibrated probability outputs, to enhance their ability to detect and reject misleading retrievals.Furthermore, reinforcement learning with human feedback and verification-based prompting strategies could help improve the model's reliability in rejecting incorrect information.
B -i t Q w e n 2 . 5 -7 B -i t G L M 4 -9 B -C h a t L l a m a 3 . 1 -8 B -i t G e m m a 2 -9 B -i t M i n i s t r a l -8 B -i t C h e m D F M -v 1 . 5 -8 B S c i G L M -6 B L l a S M o l -M i s t r a l -7 B C h e m L L M -7 B -c h a t Text Table KG
Modality Discrepancy LLMs exhibit relatively better performance on unstructured text compared to structured tables and KGs.This suggests that existing models rely heavily on linguistic patterns and semantic context rather than structured inference and multi-modal data integration.The weaker performance on tables and KGs indicates a bottleneck in structured data comprehension, where models struggle to extract, synthesize, and infer information effectively from unstructured data.To bridge this gap, models need improved cross-modal alignment, integrating structured data reasoning into their training paradigm.Techniques such as joint pretraining on text, tables, and graphs could enhance structured data understanding.</p>
<p>Specialized vs. General Model Discrepancy</p>
<p>Although scientific LLMs are explicitly designed for knowledge-intensive tasks, our evaluation shows that they often fail to outperform generalpurpose models on our dataset.This suggests that current specialized models lack sufficient reasoning depth, robustness, and flexibility to fully lever-age domain knowledge in complex scientific contexts.Their narrower training scope may limit generalization across diverse data modalities and reasoning challenges.To improve their contextual understanding, scientific models should incorporate targeted fine-tuning using curated scientific evidence and adopt domain-aware prompt engineering strategies.These approaches can help balance deep specialization with the adaptability required to tackle a broad range of scientific tasks, enhancing their effectiveness across diverse scenarios.</p>
<p>Conclusion</p>
<p>In this work, we introduced SciCUEval, a comprehensive dataset for evaluating context understanding capabilities in large language models within scientific domains.SciCUEval encompasses multiple data modalities (structured tables, knowledge graphs, and unstructured text), spanning diverse scientific disciplines.By systematically assessing four key competencies (Relevant Information Identification, Information-absence Detection, Multisource Information Integration, and Context-aware Inference), we provide a unified framework to quantify how effectively LLMs perform on scienceintensive tasks.Our experimental findings reveal that, despite notable progress, existing models encounter substantial challenges in accurately interpreting scientific data.The primary challenge lies in the inherent complexity of scientific data, particularly structured formats such as tables and knowledge graphs, which demand high specialization, precise contextual understanding, and the ability to synthesize fragmented and implicitly related information.Even state-of-the-art LLMs show limitations in fully mastering these skills, underscoring the need for significant advancements to enhance their scientific context understanding.</p>
<p>Appendix A More Results on SciCUEval</p>
<p>Table 4 and 5 present the quantitative evaluation results of LLMs across four competencies and three modalities on SciCUEval, respectively.Table 6 presents the more detailed results of SciCUEval.</p>
<p>Table 7 shows the performance comparison between direct answering and answering with context.The results demonstrate that the integration of context consistently enhances performance.</p>
<p>B Prompts for Data Generation</p>
<p>We present distinct prompt templates for each of the four capabilities below.</p>
<p>• A prompt for generating questions about relevant information identification System Message: You're a brilliant in scientific domain.</p>
<p>User Message:</p>
<p>You will be provided with several triples from PriKG that form a path connecting a starting point to an endpoint.Based on this path, you need to generate a scientific question designed to test the respondent's ability to find the correct answer in the noise, with information from the knowledge C Data Quality Verification LLM as a Judge: We use advanced LLMs (e.g., GPT-4o) as automated evaluators to verify that each generated answer is both extractable and logically deducible from the relevant context, ensuring factual consistency and relevance.The prompt is presented below.</p>
<p>System Message:</p>
<p>You're a highly capable evaluator in the scientific domain.</p>
<p>User Message:</p>
<p>Below is a question, its relevant context, and an answer.Your task is to verify whether the answer meets the following standard:</p>
<p>1.The answer must be explicitly extractable or logically deducible from the provided context.</p>
<ol>
<li>
<p>The answer must adhere strictly to the relevant information in the context and be factually correct.</p>
</li>
<li>
<p>If the answer meets the standard, output "Yes".If it does not meet the standard, output "No".
[Relevant Context start] {Context} [Relevant Context end] [Question start] {Question} [Question end] [Answer start] {Answer} [Answer end]
Please evaluate and output either "Yes" or "No" based on the above criteria.</p>
</li>
</ol>
<p>Human Expert Evaluation: To further ensure the quality and accuracy of the generated data, we subjected the data that passed the initial LLM validation to manual review by five PhD-level researchers with strong STEM backgrounds.These experts were tasked with thoroughly evaluating each instance based on the following three criteria:</p>
<ol>
<li>
<p>Whether the question effectively tests the intended competency, ensuring that it is aligned with the targeted skill or knowledge domain and accurately reflects the underlying construct it aims to assess.</p>
</li>
<li>
<p>Whether the question is expressed clearly and logically, such that its wording is unambiguous, coherent, and easily understood by both human evaluators and automated systems, thereby minimizing potential misinterpretations 3. Whether the given contexts fully support the given answer and is factually correct, which requires that the answer not only directly derives from or can be logically inferred based on the supporting materials, but also adheres to facts and scientific evidence.Together, these criteria are designed to ensure the evaluation process's validity, clarity, and reliability.</p>
</li>
</ol>
<p>Only instances that received "Yes" for all three criteria were accepted.After the experts reviewed all instances, the results revealed that 90.83% of the instances met the required high-quality standards.</p>
<p>We invited five PhD-level researchers with STEM backgrounds, including two domain experts in bioinformatics.We compensated them based on the number of questions reviewed.We paid $30 for every 100 questions reviewed, totaling $3,300 for 11k questions.The entire review process took one week.</p>
<p>D Dataset Examples</p>
<p>In this part, we demonstrate several examples of questions aligned with four core competencies.</p>
<p>For each competency, we present three examples corresponding to three distinct data modalities.User Message: How are the genes "nbc 1" and "nbc 3" related?</p>
<p>User Message:</p>
<p>Based on the methods and results described in the first part of the study on epitaxial growth of GaAs on Si(001), which of the following is the most plausible reasoning for the effectiveness of the GaSb buffer layer in reducing defect densities such as threading dislocations and antiphase boundaries in the GaAs layer?(A) The antimonides, such as GaSb, have a significant lattice mismatch with silicon, leading to the generation of interfacial misfit dislocation arrays that efficiently alleviate strain without forming threading dislocations.(B) The presence of the GaSb buffer layer increases the thickness of the overall film, which inherently reduces the formation of threading dislocations and antiphase boundaries in the GaAs layer.(C) The GaSb buffer layer chemically reacts with silicon to form a new compound at the interface, which serves as an ideal seed layer for epitaxial GaAs growth, minimizing defect densities.(D) The GaSb buffer layer promotes planar defects, such as stacking faults, that counterbalance and neutralize threading dislocations and antiphase boundaries in the GaAs layer.</p>
<p>User Message:</p>
<p>Given that there exists a shared intermediate term, fill in the blank: GO:0003399 (cytoneme morphogenesis) _____ GO:0048858 (cell projection morphogenesis).</p>
<p>[GO:0003399, is_a</p>
<p>G Case Studies</p>
<p>In this section, we provide several typical bad cases by LLMs.</p>
<p>Ability: Relevant Information Identification</p>
<p>Question:</p>
<p>Could you determine the chemical formula for the compound identified as mp-775760?</p>
<p>Correct Answer: "LiFeF3"</p>
<p>Prediction of GPT-4o-mini: "C17H20ClN3O2S" × Prediction of GPT-4o: "LiFeF3" ✓ Remarks: GPT-4o-mini accurately identified the target column and provided a chemical formula as the response; however, it incorrectly identified the context data row, leading to a mismatch between the generated formula and the corresponding Material ID.</p>
<p>Ability: Relevant Information Identification</p>
<p>Question:</p>
<p>Ability: Information-absence Detection</p>
<p>Question:</p>
<p>Can you enumerate all the PMIDs related to the interaction between id: 25840 and id: 1528?</p>
<p>Correct Answer:</p>
<p>"I cannot answer the question due to insufficient information in the retrieved data."</p>
<p>Prediction of Claude-3.5-Sonnet:"16239215, 15604093" × Remarks: Claude-3.5-Sonnetfailed to detect the absence of question-relevant context in context.Instead, it identified an incorrect Context Row in KG as the relevant context, and thus did not refuse to answer the question, but rather provided an incorrect answer.</p>
<p>Ability: Multi-source Information Integration</p>
<p>Question:</p>
<p>What are all the pairs of entity names that have a Gene-Gene relationship type?</p>
<p>Correct Answer: "cyp4f2,ggcx", "hras,kdr", "cyb5r3,cyb5a" Prediction of SciGLM-6B: "Gene", "Gene" × Remarks: SciGLM-6B failed to provide the correct answer and merely repeated the vocabulary from the question.It also failed to output the response in the required format.mp-1205400,mp-1219471,mp-1219958,mp-1221742,mp-1222109,mp-1233960,mp-1245579,mp-1272454,mp-1372845,mp-1406912,mp-14107,mp-1411625,mp-1517069,mp-1518293,mp-1518633,mp-15644,mp-1638589,mp-17955,mp-18026,mp-18027,mp-18028,mp-18029,mp-18030,mp-18031,mp-18032,mp-18033,mp-18034,mp-18035,mp-18036,... × Prediction of Qwen2.5-7B-it:mp-1219958 × Remarks: The Qwen3-8B model with thinking activation achieved 100% precision and recall by accurately identifying all three correct material IDs.In contrast, the non-thinking variant generated numerous false positives, indicating a substantial decline in performance.The Chain-of-Thought (CoT) mechanism effectively directed the model's attention to relevant fields-specifically, by constraining the search explicitly to the "Space Group Symbol" column-thereby preventing the inclusion of erroneous data from unrelated columns or positional artifacts.Moreover, CoT activation suppressed redundant and irrelevant output patterns.</p>
<p>Figure 1 :
1
Figure 1: Overview of the SciCUEval dataset.It spans five scientific domains, supports three data modalities (structured tables, knowledge graphs, and unstructured text), and includes four question types.Data are collected from high-quality scientific sources.The dataset enables evaluation across four key competencies: (1) relevant information identification, (2) information-absence detection, (3) multi-source information integration, and (4) context-aware inference.</p>
<p>[</p>
<p>interleukin 1 receptor antagonist protein, L , shock]</p>
<ol>
<li>1
1
Experimental Setup Models We select 18 advanced LLMs, including 3 proprietary models (GPT-4o (OpenAI et al., 2024), Claude-3.5-Sonnet(Anthropic, 2024), GPT-4o-mini), 11 open-source general-purpose models (DeepSeek-V3 (DeepSeek-AI et al., 2024), DeepSeek-R1 (Guo et al., 2025), Qwen2.5-7B-Instruct(Qwen et al., 2025), Qwen3-8B (with explicit thinking) (Yang et al., 2025), Llama3.1-8B-Instruct,Llama3.1-70B-Instruct(Dubey et al., 2024), Llama-4-Maverick-17B-128E-Instruct, Llama-4-Scout-17B-16E-Instruct (Meta, 2025) , Ministral-8B-Instruct (Jiang et al., 2023), GLM4-9B-Chat(GLM et al., 2024), Gemma2-9B-it(Team et al., 2024), 4 open-source scientific-domain models (SciGLM-6B(Zhang et al., 2024a), LlaSMol-Mistral-7B(Yu et al.,</li>
</ol>
<p>Figure 3 :
3
Figure 3: Performance of LLMs across four competencies on SciCUEval.</p>
<p>Figure 4 :
4
Figure 4: Performance of LLMs across three modalities on SciCUEval.</p>
<p>Please answer the scientific questions based on the content.Your answer only needs to include the one or more correct option labels, not the full options.You should give your answer directly without any other characters.User Message: What is the primary objective of the statistical framework proposed in the paper 'Augmented Doubly Robust Post-Imputation Inference for Proteomic Data'?(A) To develop a method for directly measuring protein abundances without missing values.(B) To create a statistical framework that offers valid and efficient inference for proteomic data by addressing the challenge of missing values.(C) To replace the Plugin method with a simpler imputation strategy that discards missing values.(D) To develop a tool that solely relies on low-dimensional covariates for analyzing proteomic data.Corpus 1 ......... (Irrelevant Content) Corpus 2 ......... (Irrelevant Content) Corpus 3 ......... (Correct Content) Corpus 4 ......... (Irrelevant Content) Corpus 5 ......... (Irrelevant Content) Please answer the scientific questions based on the content.You should give your answer directly without any other characters.User Message: For the material with CID 13182, what is its inchikey?cid, mw, mf, xlog... inchikey ... exactmass CID XXXXX ....................................................... × CID 13182 ....................................................... ✓ CID XXXXX ....................................................... × CID XXXXX ....................................................... × Please answer the scientific questions based on the content.You should give your answer directly without any other characters.User Message:How is the gene or protein known as 'GDPD3' connected to the anatomical structure called the 'lymph node'?Please answer the scientific questions based on the content.Your answer only needs to include the one or more correct option labels, not the full options.You should give your answer directly without any other characters.User Message:What key feature of elliptically geared isostatic metamaterials enables their nonlinear topological transitions?(A) The unique soliton-induced mechanical deformation in linear gear mechanisms.(B) The nonlinear Berry phase transition facilitated by geometric nonlinearity.(C) The presence of circular gear geometry that allows reversible deformation.(D) The linear topological index change due to minor gear rotations.Corpus 1 ......... (Irrelevant Content) Corpus 2 ......... (Irrelevant Content) Corpus 3 ......... (Irrelevant Content) Corpus 4 ......... (Irrelevant Content) Corpus 5 ......... (Irrelevant Content) Expected Answer: I cannot answer the question due to insufficient information in the retrieved data.</p>
<p>Material ID, Formula ... Sites ... Volume, Density mp-xxxxx ....................................................... × mp-xxxxx ....................................................... × mp-xxxxx ....................................................... × mp-xxxxx ....................................................... × Expected Answer: I cannot answer the question due to insufficient information in the retrieved data.Knowledge Graph System Message: Please answer the scientific questions based on the content.You should give your answer directly without any other characters.</p>
<p>[</p>
<p>x_name, relation, y_name] [Stiripentol, drug_drug, Sumatriptan ] × [GDPD3, anatomy_protein_present, lymph node] × [TROAP, protein_protein, NBPF19] × [DB00351, drug_drug, Reserpine ] × Expected Answer: I cannot answer the question due to insufficient information in the retrieved data.</p>
<p>Please answer the scientific questions based on the content.Your answer only needs to include the one or more correct option labels, not the full options.You should give your answer directly without any other characters.User Message:Based on the findings of the study, what is the primary long-term effect of local SBRT/IL-12 therapy on the bone marrow of treated mice?(A) A permanent increase in hematopoietic stem cells (HSCs).(B) A transient increase in IL-12 levels followed by long-term activation of myeloid cells.(C) A significant reduction in hematopoietic stem cells (HSCs) accompanied by skewing toward a myeloid lineage bias.(D) A substantial increase in IL-12 and IFNγ concentrations in the bone marrow.Corpus 1 ......... (Irrelevant Content) Corpus 2 ......... (Correct Content) Corpus 3 ......... (Correct Context) Corpus 4 ......... (Irrelevant Content) Corpus 5 ......... (Irrelevant Content) Please answer the scientific questions based on the content.You should give your answer directly without any other characters.User Message: Given the following isotopes ID: NDS-54874, NDS-30453, NDS-69167, NDS-58315, tell me which isotopes has the largest energy?id, Z, N, symbol... energy[kev]... relative intensity NDS-XXXXX ....................................................... × NDS-30453 ....................................................... ✓ NDS-58315 ....................................................... ✓ NDS-XXXXX ....................................................... × NDS-69167....................................................... ✓ NDS-XXXXX ....................................................... × NDS-54874....................................................... ✓ Please answer the scientific questions based on the content.You should give your answer directly without any other characters.User Message: Could you list the substances that have the potential to interact with DB131_HUMAN?Please answer the scientific questions based on the content.Your answer only needs to include the one or more correct option labels, not the full options.You should give your answer directly without any other characters.</p>
<p>Corpus 1 ......... (Irrelevant Content) Corpus 2 ......... (Correct Content) Corpus 3 ......... (Irrelevant Content) Corpus 4 ......... (Irrelevant Content) Corpus 5 ......... (Irrelevant Content) Expected Answer: ATable System Message: Please answer the scientific questions based on the content.Your answer only needs to include the one or more correct option labels, not the full options.You should give your answer directly without any other characters.User Message: Comparing materials mp-760154 and mp-1208151, which statement is correct?(A) Both materials have identical band gaps and belong to the same crystal system.(B) The material mp-1208151 has a much larger volume and higher density than mp-760154.(C) The material mp-760154 is metallic, while mp-1208151 is semiconducting.(D) Both materials are predicted to be stable with similar formation energies.Material Formula ... Sites ... Volume, Density mp-xxxxx ....................................................... × mp-760154 ..................................................... ✓ mp-xxxxx ....................................................... × mp-1208151 ..................................................... ✓ mp-xxxxx ....................................................... × Please answer the scientific questions based on the content.You should give your answer directly without any other characters.</p>
<p>Llama4-Maverick failed to correctly identify the corresponding relation in the knowledge graph and provided a completely irrelevant answer ("C"), whereas DeepSeek-V3 responded correctly.</p>
<p>The result from ChemLLM-7B-Chat is entirely unrelated to the question.For LLMs with weaker context understanding capabilities and instruction-following abilities, the occurrence of such responses is a key reason for their poor performance.Ability: Context-aware InferenceQuestion:Based on the findings of the study, how do ovarian hormones in females influence the metabolic reprogramming effects of Schistosoma mansoni infection on bone marrow-derived macrophages (BMDM)?Options: A.Ovarian hormones enhance the metabolic reprogramming... B.Ovarian hormones do not affect the metabolic... C.Ovarian hormones inhibit the metabolic reprogramming... D.Ovarian hormones cause an increase in glycolysis... Correct Answer: C Prediction of ChemDFM-v1.5-8B:None Error: This model's maximum context length is 8192 tokens.However, you requested 13432 tokens in the messages, Please reduce the length of the messages.Remarks: Some individual papers exceed the maximum length limit of certain models.In such cases, we can only classify them as errors.Ability: Context-aware InferenceQuestion:Based on the first part of the article, what conclusions can be inferred about the role of surface imperfections in the anisotropic Rashba effect observed in the 2D Janus XA2Y monolayers, and what implications might this have for spintronic applications?-4o demonstrates some reasoning ability and selected a correct answer; however, it failed to identify all the correct answers in a multiple-choice question.Ability: Context-aware InferenceQuestion: List all the material IDs that have Space Group Symbol as "Amm2".Correct Answer: mp-1017367, mp-1021395, mp-1219958 Prediction of Qwen3-8B with thinking: <think>Okay, so I need to...</think> mp-1219958,mp-1021395,mp-1017367 ✓ Prediction of Qwen3-8B without thinking:</p>
<p>Based on the methods and experimental results of the article, which factors could likely contribute to the more effective anti-PD1 response in the orthotopic murine model of mismatch repair-deficient lung adenocarcinoma compared to the subcutaneous model?A: The subclonality of tumor cells resulting in increased clonal neoantigen frequency in the orthotopic model.B: Higher overall immune cell infiltration, particularly T cells and dendritic cells, in the orthotopic lung environment.C: A similar proportion of tumor-associated macrophages and CD8+ T cells but different responses to anti-PD1 treatment in the subcutaneous model.D: Lower mutational burden resulting from fewer unique variants in the orthotopic model, affecting the immune response.Correct Answer: A Prediction of DeepSeek-V3: B × Prediction of DeepSeek-R1: <think>Alright, let's tackle...</think> A✓ Remarks: In the task of context-aware inference within unstructured text, DeepSeek-R1 demonstrates stronger reasoning capabilities than DeepSeek-V3.This indicates that reasoning models also possess significant advantages in understanding unstructured text.Ability: Multi-source Information Integration Question: What entity engages in interactions with the entities: epilepsy and diabetes mellitus?Correct Answer: gabapentin Prediction of DeepSeek-R1: <think>Okay, let's...</think> No entity in the provided data interacts with both epilepsy and diabetes mellitus.× Remarks: Even the well-performing reasoning model DeepSeek-R1 struggles to answer complex questions involving knowledge graph multi-source information integration.</p>
<p>Table 1 :
1
Comparison of SciCUEval with existing benchmark datasets.Question Types: QA (Question Answering), MCQ (Multiple Choice Question), T/F (True/False Question), and CC(Cloze Completion).
DatasetsContextsDomainsData ModalitiesQuestion TypesEvaluation Competencies# NumsLongICLBench (Li et al., 2024)✓GeneralTextQAIdentification2,618LongBench (Bai et al., 2023b)✓General, CodeTextQAIdentification, Integration4,750LongBench V2 (Bai et al., 2024)✓General, Law, FinanceTextMCQIdentification, Integration, Inference503RGB(Chen et al., 2024)✓GeneralTextQAIdentification, Detec., Integration, Inference1,000ChemLit-QA (Wellawatte et al., 2025)✓ChemistryTextQAIdentification, Detec., Inference1,054CHEMRAG-BENCH (Zhong et al., 2025)×ChemistryTextQA, MCQIdentification, Inference1,932SciCUEval✓Comprehensive ScienceText, Table, KGQA, MCQ, T/F, CC Identification, Detec., Integration, Inference11,343</p>
<p>Table</p>
<p>Recent studies have shown that honeybees flying through short, narrow tunnels with visually textured walls perform waggle dancesa ...
Selected FragmentsGenerationSelected Rowscidmvmfpolarareacomplexityxlogp561513 419.19C13H8F11NO238.3525.04.9Selected Triples</p>
<p>Text KG Final Dataset Question Context Answer Question：
What can be inferred about theinteraction between Beauvericin(BEA) and Cathepsin B (CTSB)?Options:A. BEA acts as a com...B. BEA binds to a s..C. BEA's binding to C...D. BEA's inhibitory me...Answer：BData SourceSelected EntryLLM as a Judge +Similarity Retrieval Top-k Noisy Entries(Target + Noise)Human Validation ✓ ✓ ...... ...... ...... XNoise InjectionQuality ControlFigure 2: Illustration of data generation pipeline inSciCUEval, mainly consisting of question generation,noise injection, and quality control.</p>
<p>Table 2 :
2
Statistic of the SciCUEval dataset, which comprises ten sub-datasets derived from diverse scientific data.The detailed data sources are listed in Appendix E.
Sub-dataset DomainSourceModality #Info. Indent. # Abs. Detec. # Info. Integ. # Con. Infer. # TotalMatTextMaterialsarXivText216146222356940BioTextBiologyBiorxivText23697318317968MatTabMaterialsMaterial ProjectTable299150287200936IaeaTabPhysicsIAEATable4422222861801130ProtTabBiologyPubchemTable4962493271801252MolTabChemistryPubchemTable5162593501801305GoKGBiologyGene OntologyKG5072542391801180HipKGBiologyHIPPIEKG4702363191401165PhaKGBiomedicine PharmKGKG5122562811681217PriKGBiomedicine PrimeKGKG4102053822531250</p>
<p>Table 3 :
3
Performance of LLMs across ten sub-datasets on SciCUEval.Underline results indicate the best results among all models.Bold results indicate the best results in each category.
ModelsMatTab IaeaTab MolTab ProtTab PhaKG PriKG HipKG GoKG BioText MatText OverallGPT-4o68.7956.5555.7952.6455.7154.8068.5074.3279.0364.5761.52GPT-4o-mini40.7138.8546.6744.5740.5952.6465.2073.1479.2465.0054.57Claude-3.5-Sonnet48.4842.0367.9152.2250.9445.9675.7884.0758.0661.4959.20DeepSeek-R173.7171.8974.6972.4458.6658.2069.6679.1874.7963.0969.72Qwen3-8B63.1459.2070.8069.3355.1654.4874.6873.9869.7355.1164.69DeepSeek-V356.6254.0759.8552.0852.1851.9263.4272.2966.7445.3157.50Llama4-Maverick46.4747.7948.2043.6148.3249.2864.8172.7163.0254.1553.65Llama4-Scout48.9347.7046.9046.1739.7748.0859.5766.2761.8848.5151.16Llama3.1-70B-it38.2539.7344.4441.2944.7044.0059.3170.1766.5351.9149.80Qwen2.5-7B-it28.1032.6543.3039.4636.1545.6053.9962.4668.1859.6846.62GLM4-9B-Chat31.4125.8447.8243.4536.0344.5657.9460.5167.7750.9646.46Llama3.1-8B-it28.8534.3442.7639.7838.2946.5652.6259.3264.2649.3645.50Gemma2-9B-it32.9132.2142.9137.2237.3950.4856.5757.2937.7729.6742.21Ministral-8B-it23.0819.1235.5637.3822.7637.9248.5152.8848.1445.3237.58ChemDFM-v1.5-8B33.6531.1535.5636.8240.4330.7249.7056.4426.1118.9136.80SciGLM-6B11.8611.5017.7014.9419.5620.8821.4628.3144.1731.3521.58LlaSMol-Mistral-7B13.3512.8316.5514.7021.5419.8422.8329.9233.1320.9820.42ChemLLM-7B-chat3.426.028.818.1513.455.925.1515.5139.9422.6712.16</p>
<p>Table 4 :
4
Evaluation results of LLMs across four competencies on SciCUEval.Underline results indicate the best results among all models.Bold results indicate the best results in each category.
ModelsInfo. Ident. Detec. Integ. Infer. Abs. Info. Con. OverallGPT-4o89.72 19.51 54.90 65.9761.52GPT-4o-mini77.81 14.71 47.54 57.6854.57Claude-3.5-Sonnet82.95 49.10 50.85 47.2959.20DeepSeek-R194.13 11.75 72.78 79.4469.72Qwen3-8B88.29 43.57 53.87 64.3864.69DeepSeek-V390.806.0549.80 60.5357.50Llama4-Maverick77.617.1654.16 58.5253.65Llama4-Scout71.38 25.42 43.65 54.9551.16Llama3.1-70B-it81.056.8745.44 47.7649.80Qwen2.5-7B-it69.929.0242.95 50.3446.62GLM4-9B71.482.5350.78 43.8246.46Llama3.1-8B-it75.345.8841.47 39.9945.50Gemma2-9B-it66.972.3828.74 48.2242.20Ministral-8B-it56.804.7631.32 39.6237.58ChemDFM-v1.5-8B 45.49 19.31 22.23 46.4036.80SciGLM-6B33.249.0118.00 29.4821.58LlaSMol-Mistral-7B 31.966.8314.63 26.5920.42ChemLLM-7B-Chat 20.294.0916.857.5712.16</p>
<p _Answer_="&quot;[Answer]&quot;" _Question="&quot;[Question" _answer_:="&quot;answer&quot;:" _question_:="&quot;question&quot;:" _question_type_:="&quot;question_type&quot;:" or="or" rejection_="rejection]&quot;," type_="type]&quot;,">Table 5 :
5
Evaluation results of LLMs across three modalities on SciCUEval.Underline results indicate the best results among all models.Bold results indicate the best results in each category.
ModelsTextTableKGOverallGPT-4o71.91 55.91 63.1361.52GPT-4o-mini72.22 42.98 55.8454.57Claude-3.5-Sonnet59.75 53.41 64.9959.20DeepSeek-R169.00 73.19 66.6469.72Qwen3-8B62.53 66.02 64.2764.69DeepSeek-V356.18 55.68 59.7757.50Llama4-Maverick58.65 46.51 58.5453.65Llama4-Scout55.29 47.31 53.2251.16Llama3.1-70B-it59.33 41.19 54.3049.80Qwen2.5-7B-it69.93 36.58 49.3846.24GLM4-9B-Chat59.49 37.94 49.4646.46Llama3.1-8B-it56.92 37.08 49.0645.50Gemma2-9B-it34.27 36.73 50.2342.21Ministral-8B-it46.75 29.50 41.2437.58ChemDFM-v1.5-8B 22.92 34.44 44.0536.80SciGLM-6B38.48 14.25 22.5121.58LlaSMol-Mistral-7B 27.74 14.49 23.4520.42ChemLLM-7B-chat 32.286.8610.0112.16graph. The question types can be Q&amp;Aor fill-in-the-blank. The answers to QAquestions should be simple, concise, andeasily verifiable phrases, not long sentences.Start Node: {start_node}End Node: {end_node}Path: {data['path']}Triples:{data['triplets']}Please generate a scientific question basedon this information. Ensure that the questionrequires the respondent to find the correctanswer in the noise in the knowledge graphand the difficulty level should be moderate.Please output the question in JSON formatonly. Do not output anything other than theJSON format. The JSON format should looklike this:</p>
<p>Table 6 :
6
Detailed evaluation results of LLMs across four competencies on the ten sub-datasets of SciCUEval
ModelsMatTab Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllIaeaTab Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllGPT-4o88.1658.0029.9764.0068.7981.0026.1344.7652.7856.55GPT-4o-mini71.9139.3310.1039.0040.7157.015.8637.0637.7838.85Claude-3.5-Sonnet77.2156.4619.5741.0048.4856.3123.2937.6837.7842.03DeepSeek-R197.3016.2269.0188.0073.7191.8216.3676.0684.4471.89Qwen3-8B82.9490.0019.1676.5063.1468.7856.7641.9666.1159.20DeepSeek-V393.3128.0024.7469.0056.6279.865.4144.7665.5654.07Llama4-Maverick69.909.3323.0073.0046.4766.2912.6139.5158.8947.79Llama4-Scout53.8573.3315.6871.0048.9359.0534.2336.3654.4447.70Llama3.1-70B69.5726.006.9745.5038.2562.446.3130.7739.4439.73Qwen2.5-7B-instruct48.839.333.1447.0028.1045.708.1132.8730.5632.65GLM4-9B-Chat52.842.6718.8239.0031.4129.640.4540.5624.4425.84Llama3.1-8B58.534.674.8837.0028.8553.856.3125.5234.4434.34Gemma2-9B-it65.552.006.9744.5032.9150.232.7016.7848.8932.21Ministral-8B-it44.152.003.1436.0023.0818.789.019.7947.2219.12ChemDFM-v1.5-8B42.8110.679.4172.0033.6436.6514.4121.3353.8931.15SciGLM-6B5.691.332.7942.0011.8611.990.903.8535.5611.50LlaSMol-Mistral-7B7.695.332.7943.0013.3516.296.311.7530.0012.83ChemLLM-7B-Chat1.672.670.0011.503.429.280.454.906.676.02ModelsMolTab Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllProtTab Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllGPT-4o91.099.2730.2971.1155.7990.5214.4618.9662.2252.64GPT-4o-mini68.9913.1332.2958.8946.6772.589.2419.2762.2244.57Claude-3.5-Sonnet92.8458.5942.5360.5667.9177.0831.4320.1270.0052.22DeepSeek-R196.9014.2975.8693.3374.6996.7717.7465.4393.3372.44Qwen3-8B85.8562.1646.0088.3370.8089.7262.2532.7289.4469.33DeepSeek-V394.385.7939.7177.7859.8596.576.0222.9446.1152.08Llama4-Maverick73.062.7039.4359.4448.2071.771.2022.6362.7843.61Llama4-Scout60.479.2736.5782.2246.9064.9216.4726.6171.1146.17Llama3.1-70B71.901.9330.2954.4444.4479.442.8114.0738.8941.29Qwen2.5-7B-instruct57.755.4137.7168.3343.3061.699.6419.8855.0039.46GLM4-9B66.860.3950.2956.6747.8260.892.0144.3451.1143.45Llama3.1-8B-instruct70.740.0031.1446.6742.7667.742.4121.1048.3339.78Gemma2-9B-it63.570.3936.8656.6742.9162.700.8018.6551.1137.22Ministral-8B-it51.551.1631.7146.6735.5659.071.2023.2453.3337.38ChemDFM-v1.5-8B38.9533.2022.2955.0035.5638.9134.9419.2765.5636.82SciGLM-6B23.262.328.8641.1117.7019.763.216.4233.3314.94LlaSMol-Mistral-7B22.485.7910.0027.7816.5522.185.623.9826.1114.70ChemLLM-7B-Chat11.630.0014.003.338.8113.310.009.482.788.15ModelsPriKG Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllHipKG Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllGPT-4o70.9824.8841.6272.7354.8097.0233.9043.2688.5768.50GPT-4o-mini74.8827.8031.1569.1752.6483.5120.3443.2667.8665.20Claude-3.5-Sonnet63.6628.2929.6656.1345.9697.8575.3243.2677.1475.78DeepSeek-R177.4515.6943.1684.1358.2097.443.3968.3594.2969.66Qwen3-8B79.2730.2435.8662.0654.4894.4742.8057.9998.5774.68DeepSeek-V373.901.9540.5873.9151.9294.479.3252.6675.0063.42Llama4-Maverick69.764.3939.7966.8049.2887.0213.5669.5965.7164.81Llama4-Scout77.3220.9828.8051.7848.0880.4342.8047.6545.0059.57Llama3.1-70B-it74.889.2720.6857.3144.0091.0614.4142.9565.7159.31Qwen2.5-7B-it68.0522.9327.7554.5545.6060.643.3948.9062.1453.99GLM4-9B-Chat70.986.8327.2358.5044.5685.325.5152.9865.7157.94Llama3.1-8B-it75.3718.5427.7550.9946.5682.5510.5945.1440.0052.62Gemma2-9B-it78.786.8330.3770.3650.4891.493.8136.9972.8656.57Ministral-8B-it55.1213.6623.0452.1737.9266.601.2731.0347.5048.51ChemDFM-v1.5-8B42.2034.1510.4739.9230.7250.8572.4614.7387.1449.70SciGLM-6B42.934.394.7122.9220.8833.191.275.3352.8621.46LlaSMol-Mistral-7B33.9010.735.2426.4819.8426.388.479.7265.0022.83ChemLLM-7B-Chat8.053.417.591.985.923.621.278.1510.005.15</p>
<p>Table 7 :
7
Performance comparison on SciCUEval: Direct Answering vs. Answering with Contexts.Material ID, Formula ... Sites ... Volume, Density mp-xxxxx ......................................................... mp-xxxxx ......................................................... mp-xxxxx ......................................................... mp-xxxxx .........................................................
ModelMatTab Direct Context Direct Context Direct Context Direct Context Direct Context IaeaTab MolTab ProtTab PhaKGGPT-4o14.6468.7915.3156.5526.8255.7923.6452.6416.8155.71GPT-4o-mini15.3840.7118.6738.8525.5246.6724.8414.0140.59Claude-3.5-Sonnet15.2248.4823.4542.0332.9567.9131.0752.2226.6250.94DeepSeek-R16.3473.7116.0171.8912.6174.6910.2672.4411.5158.66Qwen3-8B14.1063.149.1259.2013.9570.8013.2669.3311.8555.16DeepSeek-V314.8256.6222.6554.0731.8859.8526.2052.0815.8052.18Llama4-Maverick25.5346.4720.3547.7935.3348.2039.7043.6118.3248.32Llama4-Scout25.4348.9328.9447.7046.2146.9039.7846.1718.4139.77Llama3.1-70B-it10.0438.2516.1939.7321.3044.4422.6041.2913.1544.70Qwen2.5-7B-it14.6428.1018.9432.6521.0743.3020.6939.4615.9336.15GLM4-9B-Chat12.6131.4116.7325.8422.5347.8222.2843.4513.5336.03Llama3.1-8B-it14.2128.8514.7834.3418.5442.7617.9739.7816.3538.29Ministral-8B-it0.8223.088.2919.128.0035.564.6637.3815.0522.76Gemma2-9B-it13.2532.9110.2732.2112.8742.9113.7437.2211.4537.39ChemDFM-v1.5-8B14.3833.6513.5431.1526.3635.5528.6336.8244.5340.43SciGLM-6B12.1811.8610.4411.5015.5617.7013.5814.9413.5619.56LlaSMol-Mistral-7B11.9713.3510.8812.8313.7116.5511.9814.7023.6221.54ChemLLM-7B-chat17.523.4213.456.0219.168.8115.738.1518.0113.45ModelPriKG Direct Context Direct Context Direct Context Direct Context Direct Context HipKG GoKG BioText MatTextGPT-4o17.4454.8014.4268.5043.4774.3253.4179.0341.2864.57GPT-4o-mini16.4852.6410.9965.2042.8073.1455.6879.2448.0965.00claude-3.5-sonnet26.8045.9621.5575.7845.5984.0755.6858.0641.6061.49DeepSeek-R110.2958.2010.6969.6631.4079.1856.4374.7945.3063.09Qwen3-8B12.8054.4810.8274.6831.4473.9848.7669.7339.0455.11Deepsee-V317.3351.9214.7663.4239.7572.2960.0766.7451.1845.31Llama4-Maverick20.1849.2817.8564.8143.8172.7161.7763.0254.7954.15Llama4-Scout23.8448.0821.9759.5737.5466.2755.5861.8842.9848.51Llama3.1-70B-it14.4044.0015.8859.3132.1270.1749.8066.5340.2151.91Qwen2.5-7B-it16.5645.609.8753.9933.6462.4647.1168.1836.8159.68GLM4-9B-Chat16.7244.5611.9357.9430.1760.5147.5267.7736.3850.96Llama3.1-8B-it16.2446.5614.5152.6235.5159.3247.3164.2637.3449.36Gemma2-9B-it15.3650.489.9656.5731.2757.2951.8137.7735.8829.67Ministral-8B-it15.0537.9213.2448.5128.2752.8841.8448.1432.2345.32ChemDFM-v1.5-8B33.6630.7230.2149.7039.8456.4450.8826.1130.8318.91SciGLM-6B15.2020.8818.8021.4625.9328.3133.4444.1721.6331.35LlaSMol-Mistral-7B15.5219.8420.1722.8323.3929.9233.8533.1323.5820.98ChemLLM-7B-chat16.805.9223.865.1527.8015.5145.9239.9430.4422.67</p>
<p>Table System Message:
System
Please answer the scientific questions based on the content.You should give your answer directly without any other characters.
User Message:For the material with ID mp-768851, what isits number of site?</p>
<p>Table 8 :
8
Detailed URL, description, and license of the source data involved in this paper.</p>
<p>bridqa: A dataset of multi-hop question answering over tabular and textual data.arXiv preprint arXiv:2004.07347.
Hippie v2. 0: enhancing meaningfulness and reliability of protein-protein interaction networks. Gregorio Alanis-Lobato, Miguel A Andrade-Navarro, Martin H Schaefer, Nucleic acids research. 9852016</p>
<p>Ai Anthropic, The Claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. 2024</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023aarXiv preprint</p>
<p>Longbench: A bilingual, multitask benchmark for long context understanding. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, arXiv:2308.145082023barXiv preprint</p>
<p>Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, arXiv:2412.15204Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. 2024arXiv preprint</p>
<p>Iz Beltagy, Kyle Lo, Arman Cohan, arXiv:1903.10676Scibert: A pretrained language model for scientific text. 2019arXiv preprint</p>
<p>Building a knowledge graph to enable precision medicine. Payal Chandak, Kexin Huang, Marinka Zitnik, Scientific Data. 101672023</p>
<p>Benchmarking retrievalaugmented generation for medicine. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang, arXiv:2402.131782024arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang ; Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu, arXiv:2505.09388Qwen3 technical report. Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu2025Preprint</p>
<p>LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, arXiv:2402.093912024</p>
<p>SciGLM: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, arXiv:2401.079502024a</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, arXiv:2402.06852ChemLLM: A chemical large language model. 2024b</p>
<p>Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Khai Moo, Xu Hao, Zhen Han, Shuo Leng Thai, Zhiyuan Wang, Liu, arXiv:2402.13718∞bench: Extending long context evaluation beyond 100k tokens. 2024carXiv preprint</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, arXiv:2401.14818ChemDFM: Dialogue foundation model for chemistry. 2024</p>
<p>Pharmkg: a dedicated knowledge graph benchmark for bomedical data mining. Shuangjia Zheng, Jiahua Rao, Ying Song, Jixian Zhang, Xianglu Xiao, Evandro Fei Fang, Yuedong Yang, Zhangming Niu, Briefings in bioinformatics. 2243442021</p>
<p>Benchmarking retrieval-augmented generation for chemistry. Xianrui Zhong, Bowen Jin, Siru Ouyang, Yanzhen Shen, Qiao Jin, Yin Fang, Zhiyong Lu, Jiawei Han, arXiv:2505.076712025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5973 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5973</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5973</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-258060148</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.04920v1.pdf" target="_blank">Advancing Medical Imaging with Language Models: A Journey from N-grams to ChatGPT</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we aimed to provide a review and tutorial for researchers in the field of medical imaging using language models to improve their tasks at hand. We began by providing an overview of the history and concepts of language models, with a special focus on large language models. We then reviewed the current literature on how language models are being used to improve medical imaging, emphasizing different applications such as image captioning, report generation, report classification, finding extraction, visual question answering, interpretable diagnosis, and more for various modalities and organs. The ChatGPT was specially highlighted for researchers to explore more potential applications. We covered the potential benefits of accurate and efficient language models for medical imaging analysis, including improving clinical workflow efficiency, reducing diagnostic errors, and assisting healthcare professionals in providing timely and accurate diagnoses. Overall, our goal was to bridge the gap between language models and medical imaging and inspire new ideas and innovations in this exciting area of research. We hope that this review paper will serve as a useful resource for researchers in this field and encourage further exploration of the possibilities of language models in medical imaging.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5973",
    "paper_id": "paper-258060148",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0068779999999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Advancing Medical Imaging with Language Models: A Journey from N-grams to ChatGPT</p>
<p>Mingzhe Hu 
Department of Computer Science and Informatics
Emory University
GA, AtlantaUSA</p>
<p>Shaoyan Pan 
Department of Computer Science and Informatics
Emory University
GA, AtlantaUSA</p>
<p>Yuheng Li 
Department of Biomedical Engineering
Emory University
GA, AtlantaUSA</p>
<p>Xiaofeng Yang *email:xiaofeng.yang@emory.edu 
Department of Computer Science and Informatics
Emory University
GA, AtlantaUSA</p>
<p>Department of Biomedical Engineering
Emory University
GA, AtlantaUSA</p>
<p>Department of Radiation Oncology
School of Medicine
Winship Cancer Institute
Emory University
GA, AtlantaUSA</p>
<p>Advancing Medical Imaging with Language Models: A Journey from N-grams to ChatGPT
1
In this paper, we aimed to provide a review and tutorial for researchers in the field of medical imaging using language models to improve their tasks at hand. We began by providing an overview of the history and concepts of language models, with a special focus on large language models. We then reviewed the current literature on how language models are being used to improve medical imaging, emphasizing different applications such as image captioning, report generation, report classification, finding extraction, visual question answering, interpretable diagnosis and more for various modalities and organs. The ChatGPT was specially highlighted for researchers to explore more potential applications. We covered the potential benefits of accurate and efficient language models for medical imaging analysis, including improving clinical workflow efficiency, reducing diagnostic errors, and assisting healthcare professionals in providing timely and accurate diagnoses. Overall, our goal was to bridge the gap between language models and medical imaging and inspire new ideas and innovations in this exciting area of research. We hope that this review paper will serve as a useful resource for researchers in this field and encourage further exploration of the possibilities of language models in medical imaging.</p>
<p>Introduction</p>
<p>In recent years, the healthcare industry has seen significant progress with the integration of artificial intelligence (AI) and machine learning (ML) techniques. Generative Pre-trained Transformer (GPT), with its remarkable language modeling capabilities, has become a popular choice for medical professionals and researchers to analyze and interpret medical data. With OpenAI's recent development of AI chatbot ChatGPT, researchers are interested in using it in healthcare areas such as disease diagnosis, drug discovery, and patient care. Its ability to process large amounts of medical data and provide insights has made it a valuable tool for improving patient outcomes and advancing medical research. The versatility of ChatGPT has made it a go-to solution for many healthcare providers and researchers across different domains.</p>
<p>While prior language models such as BERT (Lee and Toutanova 2018) and ELMo (Sarzynska-Wawer, Wawer et al. 2021) were widely used in the medical field for tasks such as medical image analysis and natural language processing of electronic health records, ChatGPT represents a significant improvement in language modeling. Its ability to generate more human-like, coherent, and contextually relevant responses has made it a valuable tool for medical professionals and researchers in a variety of healthcare applications. ChatGPT's advancements in language modeling have opened up new possibilities for medical diagnosis, patient care, and drug discovery, making it a promising solution for natural language processing tasks in the medical domain.</p>
<p>Despite the wide-ranging applications of language model processing of electronic health records and disease diagnosis, there has been a lack of their applications for medical imaging. Medical image applications, such as disease diagnosis (Lee, Hu et al. 2022, Li, Hsu et al. 2022, multiorgan delineation (using Vnet (Zhou, Rahman Siddiquee et al. 2018), Vision transformer (Pan, Tian et al. 2022), and Multi linear layer Mixer (Pan, Chang et al. 2022, Valanarasu and, lesion detection (Hu, Amason et al. 2021), and high-quality image synthesis (using Generative Adversarial Networks (Lei, Jeong et al. 2018, Lei, Harms et al. 2019, Yamashita and Markov 2020, Pan, Flores et al. 2021 and Denoising Diffusion Probabilistic model (Pan, Wang et al. 2023), requires information which may not be able to be learned from natural language model. As such, it can be challenging to integrate the two modalities effectively. Additionally, the interpretation of medical images typically requires specialized training and can be time-consuming and error-prone. Moreover, there is a lack of review papers that focus on the application of language models in medical imaging analysis, making it difficult for researchers to gain a comprehensive understanding of the field and its potential applications. Given these challenges, researchers have attempted to develop language models that can extract meaningful information from medical images and assist medical professionals in their interpretation. While some progress has been made in this area, much work still needs to be done to realize the full potential of language models for medical imaging analysis. Therefore, to address the challenges and promote the use of language models in medical imaging, we decided to write this review paper. Our paper aims to serve as a foundational tutorial for researchers in this field, as well as an inspiration for them to innovate and develop new approaches to using language models to improve medical imaging analysis. In this paper, we will begin by providing an overview of the history and concepts of language models, with a special focus on large language models. We will then review the current literature on how language models are being used to improve medical imaging, emphasizing different applications. Figure 1 showcases the breadth of topics covered in our review paper. Finally, we will summarize the current state of the field and discuss potential future directions for research. Our hope is that this review paper will bridge the gap between language models and medical imaging and inspire new ideas and innovations in this exciting area of research. Figure 1. An overview of the range of topics that are discussed in our review paper. We can see that report generation is the most common application. Some researchers have begun to apply ChatGPT for medical imaging.</p>
<p>Basics of Language Models</p>
<p>Language models have a long history dating back to the 1950s, when researchers first began exploring the possibilities of computer-based natural language processing. In the early days, language models were simple statistical models based on n-gram frequencies, which were used to predict the probability of the next word in a sentence based on the previous words. Over time, more sophisticated models were developed, such as hidden Markov models and neural networks, which allowed for more accurate and nuanced language processing. However, it wasn't until the development of large language models, such as GPT (Radford, Narasimhan et al. 2018) and BERT (Lee and Toutanova 2018), that the field truly began to take off. These models use massive amounts of training data and sophisticated algorithms to understand natural language in a way that was previously impossible, opening up new possibilities for a wide range of applications, including in the field of medical imaging. Recent advances in language models include the development of more specialized models for specific tasks, such as medical text processing and dialogue systems, as well as research into novel training techniques, such as unsupervised and few-shot learning. Additionally, there has been increasing interest in developing multilingual and cross-lingual language models that can understand and process multiple languages, which has important implications for healthcare and medical research in diverse linguistic settings.</p>
<p>In this section, we will provide a reader-friendly overview of some popular language models. We will also specifically highlight some powerful large language models that have made significant contributions to natural language processing, such as GPT and BERT. By the end of this section, readers will have a solid understanding of the key concepts and features of these models, setting the stage for a deeper exploration of their applications in medical imaging.</p>
<p>N-grams</p>
<p>N-grams are a sequence of N words used in NLP (Manning and Schutze 1999). The concept is easy to understand and is used for a variety of things in NLP, such as auto-completion of sentences, auto-spell check, and checking for grammar in a sentence. N-gram probabilities are used in NLP models to predict the occurrence of a word in a sentence. To train an NLP model, a large corpus of data is required. Once the model is trained, it calculates the probability of the occurrence of a word after a certain word. In a bigram model, the model learns the occurrence of every two words to determine the probability of a word occurring after a certain word. The probability is calculated by counting the number of times a word occurs in a required sequence, divided by the number of times the word before the expected word occurs in the corpus. The larger the corpus, the better the predictions. NLP and n-grams are used to train voice-based personal assistant bots, and understanding n-grams can help optimize machine learning models.</p>
<p>Recurrent Neural Networks</p>
<p>Recurrent Neural Networks (RNNs) are a type of neural network that allows us to learn from sequential data by considering the order of observations. Unlike feed-forward neural networks, RNNs evolve a hidden state over time, which incorporates information from the previous hidden state and the latest input (Rumelhart, Hinton et al. 1985). This enables us to chain a sequence of events together and perform backpropagation through time. RNNs can handle arbitrary length inputs and outputs, making them versatile for a broad range of sequence learning tasks. One to many, many to one, many to many (same), and many to many (different) are the general architectures used for various sequence learning tasks. RNNs are particularly useful in natural language processing and speech recognition, where the order of observations is critical for accurate analysis. RNNs have been successfully used in many applications, such as language modeling, machine translation, speech recognition, and image captioning. However, they suffer from some limitations, such as the vanishing gradient problem, which affects the ability of the network to capture long-term dependencies. Therefore, several variants of RNNs have been developed, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU).</p>
<p>Long Short-Term Memory</p>
<p>LSTM (Hochreiter and Schmidhuber 1997) is a type of RNN that excels at processing sequential data with long-term dependencies. Unlike traditional RNNs, LSTM can address the vanishing gradient problem, which arises when gradients become too small as they propagate through the network, making it difficult for the network to learn long-term dependencies. LSTM introduces a memory cell and several gates that control the flow of information into and out of the cell. The memory cell stores information over time, while the gates decide what information to keep or discard at each time step. These gates, namely the forget gate, input gate, and output gate, respectively control the amount of old information to forget, the amount of new information to store, and the amount of information to output. By selectively remembering or forgetting information over long periods of time, LSTM is highly suited for tasks like speech recognition, language translation, and image captioning. Overall, LSTM is a significant improvement over traditional RNNs and has revolutionized the field of deep learning by enabling the efficient processing and analysis of sequential data.</p>
<p>Transformers</p>
<p>Transformers (Vaswani, Shazeer et al. 2017) for natural language processing is a revolutionary model architecture that has outperformed traditional encoder-decoder models based on recurrent or convolutional neural networks. The introduction of attention mechanisms optimized the performance of encoder-decoder models. With the help of attention mechanisms, Transformer, the first model that entirely relies on self-attention mechanisms to calculate input and output representations, has outperformed recurrent and convolutional neural networks. The Transformer model can be trained in parallel, which reduces training time. The encoder consists of six identical blocks, each of which has two sub-layers, a multi-head self-attention mechanism, and a fully connected feed-forward network. The decoder is also made up of six identical blocks, each with three sub-layers, which includes the addition of a multi-head self-attention sub-layer. The Transformer model's self-attention mechanism utilizes a scaled dot-product attention approach, which multiplies the query and key vectors and applies a softmax function to the results to obtain the weightings of the output. While both additive and dot-product attention are common attention functions, dot-product attention is more space-efficient and faster because it can use a highly optimized matrix multiplication code to implement.</p>
<p>Large Language Models</p>
<p>Large Language Models (LLMs) are neural network-based models widely used in NLP tasks, capable of generating human-like text by predicting the next word in a given sentence or generating new sentences. These models are trained on massive amounts of text data, allowing them to learn the underlying patterns and structures of natural language, resulting in the more coherent and contextually relevant text. Additionally, LLMs can capture long-term dependencies in language, generating more coherent and cohesive text. Their ability to perform various language-related tasks, such as text classification, sentiment analysis, and text generation, make them valuable tools in different NLP applications, including medical image analysis. In conclusion, LLMs represent a significant breakthrough in NLP, and their ability to generate high-quality, contextually relevant text makes them an essential tool for researchers and practitioners alike.</p>
<p>BERT</p>
<p>Bidirectional Encoder Representation from Transformers (BERT) (Lee and Toutanova 2018) is a pre-trained LLM that has gained widespread attention in the NLP community due to its exceptional performance on a range of tasks. Unlike previous LLMs that relied solely on unidirectional training, BERT uses a bidirectional training approach, where it learns to predict masked words in a sentence by taking into account both the left and right contexts. This allows the model to capture more complex relationships between words and phrases and achieve a better understanding of the underlying meaning of the text. Furthermore, BERT's pre-training process involves training on a large and diverse corpus of text, which allows it to learn the nuances and intricacies of natural language. This has led to BERT achieving state-of-the-art performance on several benchmarks, including sentiment analysis, question-answering, and natural language inference. Overall, BERT has revolutionized the field of NLP and paved the way for more advanced language models.</p>
<p>paLM</p>
<p>Parameterized Language Model (PaLM) (Peng, Schwartz et al. 2019) is a large language model that was proposed in 2019. Unlike traditional language models that use a fixed number of parameters to generate text, paLM employs a dynamic parameterization approach. This means that the model can adapt its parameters based on the context and input, allowing it to generate more accurate and relevant text. PaLM is trained on large amounts of text data using unsupervised learning techniques, which enables it to learn the underlying patterns and structures of natural language. It is a versatile model that can be fine-tuned for various NLP tasks, including text classification, question answering, and language modeling. One of the key features of paLM is its ability to handle out-of-vocabulary (OOV) words, which are words that are not present in the training data. The model can effectively generate contextually relevant words to replace OOV words, improving its overall text generation performance.</p>
<p>LLaMA</p>
<p>Large Language Model Meta AI (LLaMA) (Touvron, Lavril et al. 2023) is a collection of foundation language models ranging from 7B to 65B parameters. These models were trained on trillions of tokens using unsupervised learning techniques such as masked language modeling and next-sentence prediction. The training data was sourced from publicly available datasets such as Wikipedia, Common Crawl, and OpenWebText. By training exclusively on publicly available data, the LLaMA team has shown that it is possible to achieve state-of-the-art performance without relying on proprietary datasets.</p>
<p>ChatGPT</p>
<p>ChatGPT is a large language model based on the GPT architecture (Radford, Narasimhan et al. 2018), developed by OpenAI. It was first introduced in June 2020, and several versions of the model have been released. The model was trained on a massive amount of text data, including books, articles, and web pages, allowing it to learn the nuances and intricacies of natural language.</p>
<p>One of the unique features of ChatGPT is its ability to generate human-like text in response to a given prompt. This makes it useful for a wide range of natural language processing tasks, including chatbots, language translation, and text summarization, among others. ChatGPT can also be finetuned on specific tasks by providing it with a smaller, task-specific dataset.</p>
<p>The original version of ChatGPT, GPT-1, was trained on a dataset of 40GB of text data, while later versions, such as GPT-2 and GPT-3, GPT-4 were trained on significantly larger datasets. This increase in the amount of training data has allowed ChatGPT to improve its performance significantly, with GPT-4 being capable of generating text that is almost indistinguishable from that written by a human.</p>
<p>Language Models for Medical Imaging</p>
<p>Finding Extraction from Radiology Report</p>
<p>Extracting meaningful information from radiology reports is essential for secondary applications in clinical decision-making, research, and outcomes prediction. However, there remain several challenges in the field, such as reducing the workload of radiologists and improving communication with referring physicians. Additionally, extracting comprehensive semantic representations of radiological findings from reports takes a lot of work.</p>
<p>In this section, we will discuss the current state of the art in finding extraction from radiology reports, including the challenges and limitations of existing methods and the potential benefits of novel frameworks and techniques. By exploring these advancements, we hope to provide valuable insights into using language models in improving clinical decision-making in radiology.</p>
<p>One promising approach is ChestXRayBERT, a framework proposed by (Cai, Liu et al. 2021) that uses a pre-trained BERT-based language model to generate the "impression" section of chest radiology reports automatically. This approach significantly reduces the workload of radiologists and improves communication between radiologists and referring physicians. In experiments, ChestXRayBERT outperformed existing state-of-the-art models in terms of readability, factual correctness, informativeness, and redundancy, achieving an average score of 4.23 out of 5.</p>
<p>Another approach for extracting valuable information from radiology reports is proposed by (Lau, Lybarger et al. 2022) who developed a corpus of radiology reports annotated with clinical findings using an event-based schema. They then used this annotated corpus to train a pre-trained language model called BioBERT to extract clinical findings from radiology reports. Results show that BioBERT achieved an overall F1 score of 95.6% for triggers, 79.1% for span-only arguments, and 89.7% for span-with-value arguments.</p>
<p>Liu et al. (Liu, Zhang et al. 2022) aimed to improve clinical decision-making in radiology by developing an ensemble learning classification model using NLP applied to the Chinese free text of radiological reports to determine their value for liver lesion detection in patients with colorectal cancer. They found that the traditional Tf-idf statistical data model outperformed the word2vec semantic method in structured report classification, achieving an F1 value of 0.98. This study provides valuable insights into the use of NLP and ensemble learning models for improving clinical value and resource utilization in health care management.</p>
<p>Assessment</p>
<p>Although language models for finding extractions from radiology reports have shown promising results, there are still some limitations that need to be addressed. One limitation is the need for more generalizability to diverse datasets, as many models have only been evaluated on limited datasets. Another limitation is the need for manual annotation, which can be time-consuming and potentially prone to errors. Additionally, some models do not consider the context of the report outside of the "findings" section, which may limit their ability to generate summaries that are consistent with the overall report.</p>
<p>From the future perspective, there are opportunities for improvement in language models for finding extractions from radiology reports. One suggestion is to explore more advanced natural language processing techniques to improve the accuracy and efficiency of clinical finding extraction. Another suggestion is to expand the corpus to include more diverse patient populations and imaging modalities to improve generalizability. Standardized reporting using tools such as the Liver Imaging Reporting and Data System can also reduce variability in reporting and improve model performance. Overall, future research should continue to explore the potential of language models in improving clinical decision-making in radiology. A summary of the works we reviewed in this section is given in Table 1.  (Cai, Liu et al. 2021) Chest X-ray None ChestXRayBERT (Lau, Lybarger et al. 2022) General CT, X-ray None BioBERT (Liu, Zhang et al. 2022) Liver CT, X-ray None N-gram</p>
<p>Image/Video Captioning</p>
<p>Generating accurate and reliable automatic report generation systems for medical images poses several challenges, including analyzing limited medical images using machine learning approaches and generating informative captions for images involving multiple organs. Captioning fetal ultrasound images is a particularly challenging task due to the complexity and variability of the images. Fetal ultrasound images are often noisy, low-resolution, and can vary greatly depending on factors such as fetal position, gestational age, and imaging plane. Additionally, there needs to be large-scale annotated datasets for this task. To address these challenges, researchers have proposed deep learning-based approaches that integrate visual and textual information to generate informative captions for images and videos.</p>
<p>Generating captions for limited CT and digital breast tomosynthesis (DBT) images is a challenging task due to the complexity and limited information present in the images. (Alsharid, Cai et al. 2022) presented a novel approach to generate medical captions for limited CT and DBT images using a multilevel transfer learning technique and LSTM framework. The study highlights the challenges of generating captions for medical images involving multiple organs and the difficulty of analyzing limited medical images using machine-learning approaches. The authors propose a deep recurrent architecture that combines Multi Level Transfer Learning (MLTL) framework with a LSTM model. The proposed approach achieved an accuracy of 96.90% and a BLEU score of 76.9%, which outperforms existing methods in generating accurate and informative captions for limited CT and DBT images.</p>
<p>In another study, (Alsharid, El-Bouri et al. 2021) proposed a course-focused dual curriculum method for captioning fetal ultrasound images. The method trains a model to dynamically transition between two different modalities (image and text) as training progresses as shown in figure 2. Two configurations of the course-focused dual curriculum are compared, and the best results are achieved with a curriculum that focuses on image captioning in the early stages of training and then transitions to text-based captioning. The proposed method outperforms existing methods on the fetal ultrasound dataset. The authors suggest that their approach could be applied to other types of medical imaging as well.</p>
<p>Figure 2.</p>
<p>Courtesy of (Alsharid, El-Bouri et al. 2021). The model comprises two branches: the right branch processes image information using a fine-tuned VGG-16 and fully-connected layers, while the left branch embeds tokenized words using a Word2vec embedding vector and passes them through a recurrent neural network. The outputs of both branches are concatenated and used to predict the next word in the captioning sequence.</p>
<p>Caption Generation cannot be only applied to images but also videos. In the author's another work, (Aswiga and Shanthi 2022) proposed a novel approach for automatic captioning of fetal ultrasound videos using a three-way multi-modal deep neural network. The study addresses the challenge of generating informative captions for fetal ultrasound videos, which can assist clinicians in their diagnosis and treatment decisions. The proposed method integrates visual, textual, and gaze information to generate captions. The model was trained on a large dataset of fetal ultrasound videos and achieved state-of-the-art performance. The results demonstrate the potential of this technology in improving the accuracy and efficiency of fetal ultrasound diagnosis.</p>
<p>Assessment</p>
<p>While recent studies have made significant progress in medical image and video captioning using language models, there are still several limitations that need to be addressed. One major challenge is the small size of the datasets used in some studies, which can affect the generalizability of the proposed approach to larger datasets. Additionally, evaluation metrics used in some studies may not fully capture the quality of generated captions. Future research can explore more sophisticated evaluation metrics to better assess the quality of generated captions.</p>
<p>Another area for improvement is the requirement of large amounts of annotated data for training, which can be difficult to obtain in medical imaging applications. To overcome this, future work could explore ways to reduce the reliance on pre-trained models and to improve the generalization of the method. In addition, incorporating additional sources of information, such as clinical metadata or patient history, into the captioning process could provide more context and improve the accuracy and usefulness of the generated captions.</p>
<p>To improve the performance of the proposed methods, future research could also explore more sophisticated transformer-based word embedding models and spatio-temporal visual and textual feature extractors. Using larger and more diverse datasets can also help address limitations such as lack of diversity in the fetal ultrasound videos. These improvements can ultimately lead to more accurate and reliable automatic report generation systems in the medical field. A summary of the works we reviewed in this section is given in Table 2. </p>
<p>Diagnosis Interpretability</p>
<p>Medical image analysis and diagnosis have long been challenging due to the need for more interpretability of deep neural networks, limited annotated data, and complex biomarker information. These challenges have inspired researchers to develop innovative AI-based methods that can automate diagnostic reasoning, provide interpretable predictions, and answer medical questions based on raw images.</p>
<p>One approach that emphasizes interpretability is the encoder-decoder framework used by (Zhao, Tian et al. 2021) in their automatic report generation model for thyroid nodules in ultrasound images. By combining medical features with deep network features, the model offers more interpretable reasons to support decision-making, achieving a 95% description accuracy on clinical ultrasound images. (Monajatipoor, Rouhsedaghat et al. 2022) proposed a different approach to interpretability using BERTHop, a vision-and-language model for chest X-ray disease diagnosis. Their transformerbased model captures associations between medical images and associated text, and PixelHop++ extracts visual representations from X-ray images as shown in figure 3. With an AUC-ROC score of 0.862 on the ChestX-ray14 dataset, BERTHop outperforms state-of-the-art models, reducing medical mistakes in disease diagnosis.  (Monajatipoor, Rouhsedaghat et al. 2022). Overview of the proposed approach for modeling relationships between visual and textual modalities in X-ray image analysis. The approach involves three main steps: (1) visual feature extraction using PixelHop++, (2) text encoding using subword embeddings, and (3) joint transformer modeling to capture implicit alignments between the visual and textual modalities.</p>
<p>Visually interpretable diagnosis is also essential. (Zhang, Chen et al. 2019) presented a pathology whole-slide diagnosis method that uses AI to address the lack of interpretable diagnosis in cancer pathology. Their deep convolutional neural network automates human-like diagnostic reasoning and translates gigapixels directly into interpretable predictions, achieving an accuracy rate of 90.6% on urothelial carcinoma diagnosis. They also propose a novel medical image diagnosis network called MDNet that provides semantically and visually interpretable diagnoses, with an accuracy of 0.78 on the ChestX-ray14 dataset for multi-label classification of thoracic diseases.</p>
<p>More advanced transformer-based structures have also been adopted. (Naseem, Khushi et al. 2022) introduced a novel Vision-Language Transformer for Interpretable Pathology Visual Question Answering (PathVQA), which aims to answer medical questions based on pathology images. Their transformer-based architecture combines visual and textual information to embed vision and language features, achieving an accuracy of 70.2% on the PathVQA dataset. This approach presents a promising avenue for medical question-answering that could have significant implications for healthcare.</p>
<p>Assessment</p>
<p>Despite the promising results demonstrated by the studies mentioned above, there are still some limitations and open questions that need to be addressed for the development of more interpretable language models in medical imaging. One common limitation is the need for larger and more diverse datasets to ensure the generalizability of the proposed models to other medical domains and tasks. Another challenge is to improve the interpretability of the generated reports, which requires more explicit reasoning steps and finer attention localization. Future research could also explore extending these models to other medical modalities and incorporating more advanced visualization techniques to improve interpretability. Despite these limitations, the development of language models for interpretable diagnosis holds great potential for improving healthcare outcomes and reducing diagnostic errors. A summary of the works we reviewed in this section is given in Table 3. </p>
<p>Report Classification</p>
<p>In this session, we will be discussing the challenges faced in report classification for medical imaging. The manual labeling process of radiology reports for computer vision applications is time-consuming and labor-intensive, creating a bottleneck in model development. The goal of this research area is to automate this process, improving efficiency and accuracy in the medical field. Extracting clinical information from these reports is also a challenge, limiting the efficiency and accuracy of clinical decision-making. We will explore the use of deep learningbased natural language processing techniques to accurately classify medical imaging reports, addressing the limitations of traditional machine learning methods.</p>
<p>One study by (Wood, Kafiabadi et al. 2022) aimed to develop a deep learning model to automate the labelling of head MRI datasets for computer vision applications. The authors used type-token ratio and Yules I to calculate the linguistic complexity of their report corpus and compared it to similar-sized head CT and chest radiograph corpora from the radiology literature. The deep learning model achieved an accuracy of 0.94 in assigning labels to corresponding examinations, which is comparable to human performance. The introduction of the attention mechanism can further enhance the classification ability of the model. In another study, (Wood, Lynch et al. 2020) presented Automated Labelling using an Attention model for Radiology reports of MRI scans (ALARM). The authors modified and fine-tuned the state-of-the-art BioBERT language model to create ALARM, which uses attention mechanisms to accurately label reports. The results showed that ALARM's classification performance was only marginally inferior to an experienced neuroradiologist for granular classification, suggesting that ALARM can feasibly be used in realworld applications.</p>
<p>Several studies have demonstrated the usefulness of pre-trained language models for automating medical imaging tasks. Pre-training on large corpora of medical text can improve performance in tasks such as report classification. In their study, (Bressem, Adams et al. 2020) aimed to develop a highly accurate classification system for chest radiographic reports using a deep learning natural language model pre-trained on 3.8 million text reports. The results showed that the model achieved an accuracy of 90.5% in classifying chest radiographic reports, which is significantly higher than previous studies. Zachary Huemann aimed to develop and evaluate a NLP models for automated extraction of Deauville scores (DS) from PET/CT reports. The study used multiple pre-trained language models, including BERT, RoBERTa, and ALBERT, which were adapted to the nuclear medicine domain using masked language modeling. The best NLP model achieved an accuracy of 0.91 in extracting DS from the reports, with an F1 score of 0.89.</p>
<p>By comparing changes in report classification over different time periods, it is possible to reveal subtle trends that may otherwise go unnoticed. (Min, Xu et al. 2021) aimed to assess the changes in the acuity of brain MRI findings during the early COVID-19 pandemic period compared to the pre-pandemic period. The study utilized language model models to categorize reported findings of brain MRI examinations. The NLP model demonstrated an accuracy of 86.19% in categorizing the same set of reports and was used to categorize the remaining 10,370 reports. The results showed a significant increase in acute findings during the early pandemic period compared to pre-pandemic levels (p &lt; 0.001).</p>
<p>Assessment</p>
<p>Although the studies discussed in this section have shown potential in improving the efficiency and accuracy of report classification, they have limitations and require further validation and exploration. One common limitation is a small size or needs for external validation of the training dataset, which may impact the model's generalizability to different populations or modalities. Additionally, some models were only tested on specific types of medical imaging, which may limit their applicability to other areas of radiology. Moreover, variations in reporting styles among radiologists could also impact model performance.</p>
<p>To address these limitations, future research could focus on expanding training datasets and validating models on external datasets with more diverse populations and reporting styles.</p>
<p>Incorporating additional features such as patient demographics or clinical history could also improve model performance. Additionally, efforts should be made to address privacy concerns related to sharing sensitive patient data for research purposes.</p>
<p>Despite these limitations, the potential of language models for report classification is clear. Further research could explore how these models can improve efficiency and accuracy in clinical decisionmaking and enhance patient care, as well as address ethical and legal issues related to their implementation. With continued development and validation, language models could revolutionize the field of radiology and improve healthcare outcomes for patients. A summary of the works we reviewed in this section is given in Table 4. </p>
<p>Report Generation</p>
<p>This section focuses on the development of language models for medical image report generation, which has become an increasingly important task in the field of computer-aided diagnosis. The challenges of generating accurate and readable medical reports from various types of medical images have motivated researchers to explore new methods for automatic report generation. The authors in the studies presented here were particularly inspired by the challenge of automatically generating diagnostic reports with interpretability for computed tomography (CT) volumes, skin pathologies, ultrasound images, and brain CT imaging. The development of accurate and efficient language models for medical report generation has the potential to improve clinical workflow efficiency, reduce diagnostic errors, and assist healthcare professionals in providing timely and accurate diagnoses.</p>
<p>For instance, (Liu, Hsu et al. 2019) developed a method for generating clinically accurate chest Xray reports using natural language generation and computer vision techniques. They used a convolutional neural network (CNN) to extract features from the input images, which are then fed into a long short-term memory (LSTM) network to generate the corresponding report as shown in figure 4. The model achieved state-of-the-art performance on automatic evaluation metrics such as BLEU-4 and ROUGE-L. However, the authors also discussed the limitations of their approach, including the need for post-processing to remove repeated sentences and the lack of consideration for ordered radiographs for a single patient.  (Liu, Hsu et al. 2019). Architecture of the proposed Clinically Coherent Reward (CCR) model for generating coherent image captions with reinforcement policy learning. The model first encodes input images into image embedding maps, which are then used as input to a sentence decoder that generates topics for sentences in a recurrent manner. The word decoder then generates the final caption sequence from the topic, with attention on the original images. The generated captions can be evaluated using either an NLG reward, a clinically coherent reward, or a combined reward for reinforcement policy learning.</p>
<p>For CT images, (Tian, Li et al. 2018) proposed a novel multimodal data and knowledge linking framework between CT volumes and textual reports with a semi-supervised attention mechanism for generating diagnostic reports with interpretability for computed tomography (CT) volumes of liver tumors. The framework includes a CT slices segmentation model and a language model, which allows for visually interpreting the underlying reasons that support the diagnosis results. The system has shown promising results with 76.6% in terms of BLEU@4, potentially reducing the time-consuming and prone to inter-and intra-rater variations task of writing diagnostic reports by radiologists.</p>
<p>Moving on to histopathology images, (Wu, Yang et al. 2022) proposed a deep learning-based image caption framework named the automatic generation network (AGNet) for the automatic generation of skin imaging reports. The proposed AGNet was evaluated on a dataset of 10,000 skin lesion images, achieving an accuracy rate of 85.6% in generating accurate diagnostic reports. The results demonstrate that AGNet is an effective network for generating accurate and efficient reports for skin diseases, which can potentially improve clinical workflow efficiency and reduce diagnostic errors.</p>
<p>Adopted a more novice structure, (Yang, Niu et al. 2021) proposed a method for automatic ultrasound image report generation using a multimodal attention mechanism. The method uses a spatial attention mechanism to focus on important areas of the image and a multimodal attention mechanism to combine information from different sources. The proposed method achieves stateof-the-art performance on two benchmark datasets, with an F1 score of 0.68 and 0.67, respectively.</p>
<p>Moreover, (Nguyen, Nie et al. 2022)proposed an enriched disease embedding-based transformer model, Eddie-Transformer, for medical report generation from X-ray images. The proposed model achieved state-of-the-art performance on two benchmark datasets, MIMIC-CXR and CheXpert, with F1 scores of 0.727 and 0.787, respectively. Additionally, (Niksaz and Ghasemian 2022) presented an approach to improve the performance of chest X-ray report generation by leveraging the text of similar images. The proposed approach achieved a significant improvement in performance compared to the baseline model, with an increase of 3.5 BLEU-4 score points.</p>
<p>Assessment</p>
<p>We would highlight several limitations and opportunities for future research in the field of language models for report generation. One common limitation is the need for improvement in report quality, including eliminating repeated sentences and improving interpretability. Additionally, the generalizability of these models to other medical imaging modalities and datasets needs to be evaluated. Moreover, the proposed methods rely heavily on deep learning techniques and require large amounts of labeled data, which may not always be available in practice. Therefore, researchers should explore ways to improve the efficiency and scalability of these methods.</p>
<p>On the other hand, there are promising future perspectives for these language models. Researchers can incorporate additional modalities, such as clinical data and genetic information, to improve the accuracy of diagnosis. Moreover, these models could be extended to generate more complex reports that include patient history and clinical findings, and natural language generation techniques can be explored to enhance report variability. Also, integrating clinical knowledge and expert feedback into the model training process could enhance the quality and accuracy of generated reports. Finally, improving the model's robustness to image quality and incorporating additional clinical information could further improve the performance of these models. A summary of the works we reviewed in this section is given in Table 5. </p>
<p>Multimodal Learning</p>
<p>Advancements in medical imaging technologies have led to an increase in the volume of image and text data generated in healthcare systems. To analyze these vast amounts of data and support medical decision-making, there is a growing interest in leveraging artificial intelligence (AI) techniques, such as machine learning, for automated image analysis and diagnosis. However, the effectiveness of these techniques is often limited by the challenges associated with bridging multimodal data, such as text and images.</p>
<p>Multimodal learning has emerged as a promising approach for addressing these challenges in medical imaging. By leveraging both visual and textual information, multimodal learning techniques have the potential to improve diagnostic accuracy and enable more efficient analysis of medical imaging data. We will focus on the use of multimodal learning for abnormality detection in medical imaging, with a particular emphasis on studies that leverage natural language processing to extract meaningful labels from radiological reports and combine them with visual information. (Eyuboglu, Angus et al. 2021) developed a weak supervision framework that uses natural language processing to extract abnormality labels from free-text radiology reports and automatically labels each region in a custom ontology of anatomical regions. They used this structured profile to train an attention-based, multi-task system for abnormality detection in whole-body FDG-PET/CT scans. Their model achieved impressive results, outperforming existing supervised machine learning systems. The model structure is shown as figure 5. Figure 5. Courtesy of (Eyuboglu, Angus et al. 2021). Overview of the proposed approach for generating ground truth labels for metabolic abnormalities in FDG-PET/CT examinations using unstructured radiology reports. The approach involves tagging sentences that mention anatomical regions and predicting metabolic abnormalities in those regions. The entire PET/CT scan is encoded using a 3D-CNN, and attention modules are used to extract relevant voxels for each region. A final linear classification layer produces a binary prediction for each region. (Dadoun, Delingette et al. 2023) also used a multimodal learning approach for anomaly detection but in abdominal ultrasound images. They proposed a method that efficiently learns visual concepts from radiological reports using natural language supervision and contrastive learning. The authors constructed a pre-training set of unlabeled examinations, which were processed in three steps to select images containing kidneys and sentences describing them. The proposed method achieved superior performance to other state-of-the-art methods.</p>
<p>Assessment</p>
<p>Multimodal learning has shown great potential for improving medical image analysis and diagnosis, as evidenced by the studies reviewed in this session. However, there are still some limitations that need to be addressed in future research. One of the main limitations is the relatively small size of the datasets used in some of the studies, which may be different from the full range of anomalies that can occur in clinical practice. Additionally, some studies have been limited to detecting specific types of anomalies and have yet to explore the detection of other abnormalities. Another important area for future research is the generalizability of these methods to other medical imaging modalities beyond the ones explored in the studies reviewed. A summary of the works we reviewed in this section is given in Table 6. </p>
<p>Visual Question Answering</p>
<p>Visual question answering (VQA) in medical imaging is an emerging area that combines image processing and natural language processing to enable machines to answer clinical questions presented with medical images. In the dermatology field, the increasing prevalence of skin diseases and the challenges faced by patients in accessing dermatologists, particularly in remote or underdeveloped areas, have inspired researchers to explore innovative approaches to accurately diagnose skin diseases. Existing attempts at image classification for skin disease diagnosis have been limited by small datasets with few classes, highlighting the need for more comprehensive approaches. Meanwhile, the critical role that medical images play in clinical and healthcare domains has emphasized the need for solutions that can accurately interpret these images even for experts. VQA models for medical imagery have the potential to improve the accuracy of diagnosis and treatment by enabling machines to understand and interpret medical images. In this session, we examine recent advancements in VQA models for medical imagery, and discuss their potential impact on diagnosis and treatment in the medical field. (Bazi, Rahhal et al. 2023) presented a vision-language model for VQA in medical imagery. They emphasize the importance of medical images in clinical and healthcare domains and propose a model based on multi-modal transformers. This model is trained on a large dataset of medical images and corresponding textual questions, allowing it to accurately answer clinical questions presented with medical images. Figure 6 shows the image encoder which extract vision features in their work. The authors report that their model achieves state-of-the-art performance on two benchmark datasets, with an accuracy of 72.5% and 68.3%, respectively. These results demonstrate the potential of the proposed model to improve diagnosis and decision-making in the medical field. Figure 6. Courtesy of (Bazi, Rahhal et al. 2023). The architecture of the image encoder.</p>
<p>As another typical example, skin diseases are a growing concern globally, with millions of people affected every year. Early diagnosis and treatment are crucial to managing these diseases effectively. (Kohli, Verma et al. 2022) present Dermatobot, an image-processing-enabled chatbot for the diagnosis and tele-remedy of skin diseases. The authors describe their approach, which uses EfficientNetB4 as the base model with additional layers and regularization, achieving an accuracy of 94% on a dataset of six classes. Their results demonstrate the effectiveness of their approach in accurately diagnosing skin diseases through image processing.</p>
<p>Assessment</p>
<p>Although VQAs showed promising results, there are some limitations to their current approach. Firstly, the system covers a constrained set of disease classes, which may limit its applicability to a wider range of ailments. Additionally, the accuracy of its diagnosis is dependent on the quality of the input image, which may be affected by various factors such as lighting and camera quality. Lastly, its reliance on a database of remedies may limit its ability to suggest novel or unconventional treatments. In terms of future perspectives, it is possible to expand the model's scope to cover an increased number and variety of disease classes. Also, incorporating user feedback into the training process to improve accuracy over time and exploring the use of generative models such as GANs to augment the dataset and improve performance on rare or underrepresented ailments. A summary of the works we reviewed in this section is given in Table  7. </p>
<p>Miscellaneous Topics</p>
<p>Patient/Student Education (Azlan, Rusli et al. 2022) presented a preliminary study on the development of an artificial intelligence chatbot called Dibot that can assist students in diagnostic imaging courses. The study consists of three phases: development and validation, evaluation of student perspectives, and determination of the impact on student knowledge. Dibot was implemented on the Snatchbot platform and later deployed on a Telegram channel and Facebook Messenger. The results showed that immediate, content-related, and high-quality interaction could be beneficial through a chatbot. (Rebelo, Sanders et al. 2022) presented a development study on learning the treatment process in radiotherapy using an artificial intelligence-assisted chatbot. The study aimed to develop a chatbot that can assist in teaching the treatment process to radiation therapy students and patients. The authors used a deep learning model to train the chatbot, which was then evaluated through user testing. Results showed that the chatbot was effective in improving knowledge retention and confidence levels among users, with an average score of 87% on post-test assessments. The study highlights the potential of using artificial intelligence-assisted chatbots as a tool for education and training in healthcare fields such as radiotherapy.</p>
<p>Pre-diagnosis Screening (Wang, Tan et al. 2022) conducted a pilot study to evaluate the feasibility of using a chatbot for COVID-19 screening before radiology appointments. The study aimed to limit human contact and ensure the safety of high-risk patient populations. The chatbot assessed the presence of any symptoms, exposure, and recent testing. User experience was assessed via a questionnaire based on a 5-point Likert scale. Multivariable logistic regression was performed to predict the response rate. The chatbot COVID-19 screening SMS message was sent to 4687 patients, and 2722 (58.1%) responded. Results showed that age, sex, imaging modality, and English preference were important factors to consider for engagement in developing a chatbot. (Fan, Xu et al. 2021) propose an intelligent medical pre-diagnosis framework for breast cancer using a pre-training chatbot called M-Chatbot and an improved neural network model of EfficientNetV2-S named EfficientNetV2-SA. The chatbot communicates with patients and provides professional guidance to assist them in completing the imaging diagnosis process. The mammography network then classifies breast cancer pathological images using transfer learning and returns the diagnostic results to users. The system was tested on the BreaKHis dataset and achieved an accuracy of 94.5%, demonstrating its effectiveness in comprehending user input and accurately diagnosing breast cancer pathology images.</p>
<p>Protocol Determination (Lee 2018) aimed to improve the efficiency of a busy radiology practice by evaluating the feasibility of using a convolutional neural network (CNN) classifier to determine musculoskeletal MRI protocols. The study used a database of MRI examinations, referring department, patient age, and patient gender. The CNN classifier was trained on short-text classification and achieved an accuracy of 0.95 on the test set. The agreements between the protocols determined by the CNN and those determined by musculoskeletal radiologists were evaluated, showing a Cohen's kappa value of 0.87. These results suggest that using a CNN classifier can be a promising approach for determining MRI protocols in a busy radiology practice.</p>
<p>Image Synthesis (Shin, Ihsani et al. 2020) presented a novel approach for synthesizing medical images, specifically MRI to PET synthesis, using Generative Adversarial Networks (GANs) and Bidirectional Encoder Representations from Transformers (BERT). The authors highlight the challenges of synthesizing medical images due to their wider and denser intensity range compared to photographs and digital renderings. They propose a GANBERT model that uses NSP and MLM training objectives of BERT to summarize MRI images into text-like sequences, which are then concatenated with PET sequences. The model is trained on the Alzheimer's Disease Neuroimaging Initiative dataset and achieves a peak signal-to-noise ratio (PSNR) of 31.98 dB and structural similarity index measure (SSIM) of 0.89, demonstrating its effectiveness in synthesizing high-quality PET images from MRI inputs.</p>
<p>ChatGPT for Medical Imaging</p>
<p>This session is particularly devoted to ChatGPT, a powerful and new model that has brought revolutionary changes to the field of natural language processing. We aim to raise awareness of its potential applications in medical image research. With the ability to understand and generate human-like responses, ChatGPT can play a crucial role in visual question answering and diagnosis, where medical images are presented alongside clinical questions. By harnessing the power of ChatGPT, we can potentially improve the accuracy and accessibility of medical image diagnosis, particularly in remote or underdeveloped areas with limited access to expert dermatologists. The potential of ChatGPT in medical image research is still largely unexplored, but we believe that it has the potential to transform the field and revolutionize the way we approach medical imaging.</p>
<p>Image Captioning (Chen, Guo et al. 2022) proposed the VisualGPT model, which is a combination of a pre-trained language model (PLM) and a vision model. Specifically, they used GPT-2 as the PLM and ResNet-101 as the vision model. They also designed a novel encoder-decoder attention mechanism with an unsaturated rectified gating function to bridge the semantic gap between different modalities. The proposed method achieved state-of-the-art results on IU X-ray and outperformed strong baseline models on MS COCO and Conceptual Captions datasets.</p>
<p>Interactive CAD (Wang, Zhao et al. 2023) present ChatCAD, an interactive computer-aided diagnosis system that utilizes large language models to diagnose medical images. The system includes a disease classifier, lesion segmentor, and report generator as shown in figure 7. The authors evaluate the performance of their proposed method with two other report-generation methods and focus on five kinds of observations. Precision (PR), recall (RC), and F1-score (F1) are reported in Table 1. The results show that ChatCAD outperforms the other methods in terms of PR, RC, and F1-score. The authors demonstrate the effectiveness of their approach by generating a report for a medical image with an accuracy of 92.3%. Figure 7. Courtesy of (Wang, Zhao et al. 2023). Overview of the proposed strategy for bridging visual and linguistic information. The approach involves processing images using various networks to generate diverse outputs, which are then transformed into text descriptions that serve as a link between visual and linguistic information. These text descriptions are combined as inputs to a large language model (LLM), which can reason logically and provide a condensed report on the findings in the image. Moreover, the LLM can offer interactive explanations and medical recommendations based on its knowledge of the medical field.</p>
<p>Report Simplification (Jeblick, Schachtner et al. 2022) presented an exploratory case study on the use of ChatGPT, a language model capable of generating human-like text, to simplify radiology reports. The authors note that current radiology reports can be difficult for patients to understand due to their technical language and complexity. To address this issue, the authors fine-tuned ChatGPT on a dataset of simplified radiology reports. The authors conducted a survey with radiologists to assess the quality of the generated reports in terms of readability and accuracy. The survey results showed that the majority of radiologists found the generated reports to be accurate and easy to read. The authors conclude that ChatGPT has the potential to improve patient understanding of radiology reports and suggest further research in this area, particularly in evaluating its performance on larger datasets and in clinical settings.</p>
<p>Similarly, (Lyu, Tan et al. 2023) conducted a study to evaluate the effectiveness of using ChatGPT and GPT-4 with prompt learning to translate radiology reports into plain language for improved healthcare. The authors collected radiology reports from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI metastases screening scans in the first half of February. The study used three prompts to record ChatGPT responses, which were then evaluated by two experienced radiologists. The authors found that using a detailed prompt significantly improved the overall quality of translation from 55.2% to 77.2%, with measures on information completely omitted, partially translated, and misinterpreted reduced to 9.2%, 13.6%, and 0% respectively. A good example of using a detailed prompt is the translation of lung nodule 1, where there were no translations keeping the information in the report with a vague prompt, but eight out of ten translations presented the information on this nodule with a detailed prompt. The study suggests that ChatGPT and GPT-4 with prompt learning have the potential for translating radiology reports into plain language, but further research is needed to address limitations such as the need for detailed prompts and potential risks associated with using artificial intelligence in healthcare settings.</p>
<p>Zero Shot Learning (Pellegrini, Keicher et al. 2023) proposed a zero-shot method for automated diagnosis prediction from medical images that leverages the power of large generic language models and large domainspecific contrastive models. The authors address the challenge of scarce annotated data in the medical domain by utilizing contrastive pretraining on pairs of radiology reports and images. The proposed method achieves performance on par with radiologists, making it a valuable resource to support clinical decision-making. The authors also emphasize the importance of transparency and explainability in medical image diagnosis, which is achieved through detailed radiology reports and image descriptors. Specifically, the results show that the proposed method achieves an accuracy of 87% on a dataset of chest X-rays with 14 different diagnoses. Furthermore, the authors compare the initial ChatGPT output to their refined prompts and observe an improvement through refinement, indicating that including domain knowledge further improves the method. This approach highlights the potential of combining large generic language models with large domainspecific contrastive models in medical image diagnosis prediction.</p>
<p>Assessment</p>
<p>There are several reasons why ChatGPT has not been widely applied for medical imaging. Firstly, medical image analysis requires specialized knowledge and expertise in both image processing and clinical domain, which may not be fully captured by a generic language model like ChatGPT. Secondly, medical imaging datasets are often highly imbalanced and may contain sensitive patient information, which requires careful handling and preprocessing. Thirdly, the performance of language models like ChatGPT is highly dependent on the quality and quantity of training data, which can be challenging to obtain in medical imaging due to regulatory and ethical considerations.</p>
<p>Despite these challenges, there is growing interest in exploring the potential of language models for medical imaging, particularly for tasks such as visual question answering and clinical decision support. With continued research and development, it is likely that ChatGPT and other language models will play an increasingly important role in medical imaging and healthcare more broadly. A summary of the works we reviewed in this section is given in Table 8.  (Wang, Zhao et al. 2023) General General PCAM, R2GenCMN ChatGPT (Chen, Guo et al. 2022) General X-ray Vision Transformer BERT, GPT2 (Jeblick, Schachtner et al. 2022) General General None ChatGPT (Lyu, Tan et al. 2023) Lung General None ChatGPT (Pellegrini, Keicher et al. 2023) Chest, Lung X-ray CNN ChatGPT</p>
<p>Discussion</p>
<p>This work introduces several unique natural language algorithms, including N-grams, RNN, LSTM, transformer models, and LLMs, including BERT, paLM, LLaMA, and Chatgpt, with increasing performance in real-world practical applications. We used the following pattern to search related papers in Google Scholar and PubMed: ("Language Model" OR Chatbot) AND (Medical OR CT OR MR OR Ultrasound OR X-ray OR OCT OR Pathology) AND (Image OR Imaging). Then the duplicate papers will be removed. We set the qualified publication date to 2019. The remaining papers will go through qualitative synthesis and quantitative synthesis. A total number of 40 papers were left for detailed review. Many works demonstrated the significant performance of the LLMs in medical image tasks. Firstly, the LLMs can be applied for medical image/video captioning. Researchers propose language models and other deep learning-based models to integrate visual and textual information to generate informative captions for CT/DBT/ultrasound images and videos, achieving state-of-the-art performance. In addition, researchers have proposed LLMs to interpret deep learning algorithms in medical applications, such as chest X-ray disease diagnosis, pathology whole-slide diagnosis for cancer pathology, and Vision-Language Transformer for Interpretable Pathology Visual Question Answering. The LLMs allow end-to-end deep learning methods to generate diagnostic reasons for their prediction, improving the interpretability of the networks.</p>
<p>Furthermore, LLMs can be applied to generate accurate and readable reports from various types of medical images to improve clinical workflow efficiency, reduce diagnostic errors, and assist healthcare professionals in providing timely and accurate diagnoses. Many works have proposed deep learning-based image caption frameworks and multimodal attention mechanisms for generating diagnostic reports for computed tomography (CT) volumes, skin pathologies, ultrasound images, and chest X-ray images. These studies have achieved state-of-the-art performance on various benchmark datasets and demonstrated the potential for improving clinical workflow efficiency and reducing diagnostic errors. LLMs also demonstrated their application in visual question answering, patient/medical student education, pre-diagnosis screening, protocol determination, and image synthesis.</p>
<p>On the other hand, as the state-of-the-art LLM so far, ChatGPT also has been deployed by many works in medical image applications. For example, VisualGPT was proposed for X-Ray captioning and demonstrated state-of-the-art accuracy. Furthermore, ChatCAD was proposed as an interactive computer-aided diagnosis system, combining ChatGPT with a disease classifier and lesion detector to diagnose medical images and present remarkable performance. Additionally, Pellegrini, Keicher, et al. propose a zero-shot method for automated diagnosis prediction from medical images by leveraging the power of ChatGPT and large domain-specific contrastive models. The method shows the possibility of using ChatGPT on zero-shot medical image tasks to improve deep learning-based diagnosis accuracy. Furthermore, it highlights the potential of combining ChatGPT with large domain-specific contrastive models in medical image diagnosis prediction.</p>
<p>While ChatGPT is a powerful language model and has demonstrated its power on medical image tasks, it is not specifically designed for medical image analysis. ChatGPT is trained on text data and can generate natural language responses based on the input it receives. However, medical image analysis involves tasks such as image segmentation, classification, and detection, which require vision information outside ChatGPT's capabilities. In addition, ChatGPT is trained on large amounts of generic text data, which may not capture the specific nuances of medical language and terminology, which could lead to errors in the model's ability to understand and generate medical image reports accurately.</p>
<p>Moreover, ChatGPT lacks interpretability. As a black-box model, it cannot be easy to understand how ChatGPT arrives at its predictions. Such difficulties of interpretation could limit its usefulness in a clinical setting, where it is essential to understand the reasoning behind a diagnosis or treatment recommendation. Also, developing accurate and reliable models for medical image analysis requires large amounts of annotated data, which may not always be available or accessible. Again, this could limit the ability to train ChatGPT on various medical images and conditions.</p>
<p>Conclusion</p>
<p>In conclusion, in this paper, we highlighted the potential of language models, in advancing medical imaging analysis. We have discussed the challenges and opportunities in using language models for healthcare applications and provided a foundational tutorial for researchers in this field. Our hope is that this paper will serve as an inspiration for researchers to innovate and develop new approaches to using language models to improve medical imaging analysis. We encourage researchers in the medical imaging domain to learn from this review paper and use it as a launchpad for further exploration of the possibilities of language models in healthcare. With continued research and development, we believe that language models will play an increasingly important role in improving patient outcomes and advancing medical research.</p>
<p>Disclosure</p>
<p>Figure 3 .
3Courtesy of</p>
<p>Figure 4 .
4Courtesy of</p>
<p>Table 1 .
1Overview of language models for finding extractions.References 
ROI 
Modality 
Vision Model 
Language Model </p>
<p>Table 2 .
2Overview of language models for image/video captioning.References 
ROI 
Modality 
Vision Model 
Language Model 
(Aswiga and Shanthi 2022) 
Breast 
CT, DBT 
MLTL 
LSTM 
(Alsharid, El-Bouri et al. 2021) 
Fetal 
Ultrasound 
VGG16 
LSTM-RNN 
(Alsharid, Cai et al. 2022) 
Fetal 
Ultrasound 
VGG16 
LSTM, Word2vec </p>
<p>Table 3 .
3Overview of language models for improving the diagnosis interpretability.References 
ROI 
Modality 
Vision Model 
Language Model 
(Zhao, Tian et al. 2021) 
Thyroid 
Ultrasound 
Unet, VGG16 
RNN 
(Monajatipoor, 
Rouhsedaghat et al. 2022) </p>
<p>Chest 
X-ray 
PixelHop ++ 
BlueBERT </p>
<p>(Zhang, Chen et al. 2019) 
Bladder 
Histopathology 
CNN 
LSTM 
(Naseem, Khushi et al. 
2022) </p>
<p>General 
Histopathology 
ResNet-152 
BioELMo </p>
<p>(Zhang, Xie et al. 2017) 
Bladder 
Histopathology 
ResNet 
LSTM </p>
<p>Table 4 .
4Overview of language models for report classification.References 
ROI 
Modality 
Vision Model 
Language Model 
(Wood, Lynch et al. 2020) 
Head 
MRI 
None 
BioBERT 
(Min, Xu et al. 2021) 
Brain 
MRI 
None 
Task Specific 
Decoder 
(Wood, Kafiabadi et al. 2022) 
Head 
MRI 
None 
BioBERT 
(Huemann, Lee et al. 2023) 
General 
PET/CT 
ViT, EfficientNet BERT, RoBERTa, 
ALBET 
(Bressem, Adams et al. 2020) 
Chest 
CT 
None 
BERT </p>
<p>Table 5 .
5Overview of language models for report generation.References 
ROI 
Modality 
Vision Model 
Language Model 
(Liu, Hsu et al. 2019) 
Chest 
X-ray 
CNN 
LSTM 
(Tian, Li et al. 2018) 
General 
CT 
CNN 
LSTM 
(Wu, Yang et al. 2022) 
Skin 
Histopathology 
CNN 
LSTM 
(Yang, Niu et al. 2021) 
General 
Ultrasound 
CNN 
LSTM 
(Nguyen, Nie et al. 
2022) </p>
<p>Chest 
X-ray 
DenseNet-121 
Eddie-Transformer </p>
<p>(Niksaz and Ghasemian 
2022) </p>
<p>Chest 
X-ray 
CNN 
LSTM </p>
<p>(Liu, Liao et al. 2021) 
Chest 
CT 
R-CNN 
VLBERT 
(Yang, Ji et al. 2021) 
Brain 
CT 
Weakly Guided 
Attention Model 
(WGAM) </p>
<p>Keywords-driven 
Interactive Recurrent 
Network (KIRN) </p>
<p>Table 6 .
6Overview of language models for multimodal learning.References 
ROI 
Modality 
Vision Model Language Model 
(Eyuboglu, Angus et al. 2021) 
Whole 
Body </p>
<p>FDG-PET/CT 
3D CNN 
BERT </p>
<p>(Dadoun, Delingette et al. 2023) 
Kidney 
Ultrasound 
ResNet50 
CamemBERT </p>
<p>. </p>
<p>Table 7 .
7Overview of language models for visual question answering.References 
ROI 
Modality 
Vision Model 
Language Model 
(Kohli, Verma et 
al. 2022) </p>
<p>Skin 
Histopathology 
Not Mentioned 
InceptionV3, 
ResNet50, 
EfficientNetB4 
(Bazi, Rahhal et 
al. 2023) </p>
<p>General 
Histopathology, 
Radiography </p>
<p>Image Encoder 
Transformer </p>
<p>Table 8 .
8Overview of ChatGPT models for medical imaging.References 
ROI 
Modality 
Vision Model 
Language Model </p>
<p>The authors are not aware of any affiliations, memberships, funding, or financial holds that might be perceived as affecting the objectivity of this review.
Gaze-assisted automatic captioning of fetal ultrasound videos using three-way multi-modal deep neural networks. M Alsharid, Y Cai, H Sharma, L Drukker, A T Papageorghiou, J A Noble, Medical Image Analysis. 82102630Alsharid, M., Y. Cai, H. Sharma, L. Drukker, A. T. Papageorghiou and J. A. Noble (2022). "Gaze-assisted automatic captioning of fetal ultrasound videos using three-way multi-modal deep neural networks." Medical Image Analysis 82: 102630.</p>
<p>A coursefocused dual curriculum for image captioning. M Alsharid, R El-Bouri, H Sharma, L Drukker, A T Papageorghiou, J A Noble, IEEE 18th International Symposium on Biomedical Imaging (ISBI). IEEEAlsharid, M., R. El-Bouri, H. Sharma, L. Drukker, A. T. Papageorghiou and J. A. Noble (2021). A course- focused dual curriculum for image captioning. 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), IEEE.</p>
<p>A Multilevel Transfer Learning Technique and LSTM Framework for Generating Medical Captions for Limited CT and DBT Images. R Aswiga, A Shanthi, Journal of Digital Imaging. 353Aswiga, R. and A. Shanthi (2022). "A Multilevel Transfer Learning Technique and LSTM Framework for Generating Medical Captions for Limited CT and DBT Images." Journal of Digital Imaging 35(3): 564-580.</p>
<p>A Preliminary Study: Chatbot Development As A Virtual Personal Assistant For Mobile Learning In Diagnostic Imaging Course. N Azlan, N Rusli, M Mohamad, K Mohamad Nassir, N Mohamed Sharif, Physics and Technology in Medicine. 31Azlan, N., N. Rusli, M. Mohamad, K. Mohamad Nassir and N. Mohamed Sharif (2022). "A Preliminary Study: Chatbot Development As A Virtual Personal Assistant For Mobile Learning In Diagnostic Imaging Course." Physics and Technology in Medicine 3(1): 14-22.</p>
<p>Vision-Language Model for Visual Question Answering in Medical Imagery. Y Bazi, M M A Rahhal, L Bashmal, M Zuair, Bioengineering. 103380Bazi, Y., M. M. A. Rahhal, L. Bashmal and M. Zuair (2023). "Vision-Language Model for Visual Question Answering in Medical Imagery." Bioengineering 10(3): 380.</p>
<p>Highly accurate classification of chest radiographic reports using a deep learning natural language model pre-trained on 3.8 million text reports. K K Bressem, L C Adams, R A Gaudin, D Trltzsch, B Hamm, M R Makowski, C.-Y Schle, J L Vahldiek, S M Niehues, Bioinformatics. 3621Bressem, K. K., L. C. Adams, R. A. Gaudin, D. Trltzsch, B. Hamm, M. R. Makowski, C.-Y. Schle, J. L. Vahldiek and S. M. Niehues (2020). "Highly accurate classification of chest radiographic reports using a deep learning natural language model pre-trained on 3.8 million text reports." Bioinformatics 36(21): 5255- 5261.</p>
<p>Chestxraybert: A pretrained language model for chest radiology report summarization. X Cai, S Liu, J Han, L Yang, Z Liu, T Liu, IEEE Transactions on Multimedia. Cai, X., S. Liu, J. Han, L. Yang, Z. Liu and T. Liu (2021). "Chestxraybert: A pretrained language model for chest radiology report summarization." IEEE Transactions on Multimedia.</p>
<p>Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. J Chen, H Guo, K Yi, B Li, M Elhoseiny, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionChen, J., H. Guo, K. Yi, B. Li and M. Elhoseiny (2022). Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</p>
<p>Joint representation learning from french radiological reports and ultrasound images. H Dadoun, H Delingette, A.-L Rousseau, E De Kerviler, N Ayache, IEEE ISBI 2023-International Symposium on Biomedical Imaging. Dadoun, H., H. Delingette, A.-L. Rousseau, E. de Kerviler and N. Ayache (2023). Joint representation learning from french radiological reports and ultrasound images. IEEE ISBI 2023-International Symposium on Biomedical Imaging.</p>
<p>Multi-task weak supervision enables anatomically-resolved abnormality detection in whole-body FDG-PET/CT. S Eyuboglu, G Angus, B N Patel, A Pareek, G Davidzon, J Long, J Dunnmon, M P Lungren, Nature communications. 1211880Eyuboglu, S., G. Angus, B. N. Patel, A. Pareek, G. Davidzon, J. Long, J. Dunnmon and M. P. Lungren (2021). "Multi-task weak supervision enables anatomically-resolved abnormality detection in whole-body FDG- PET/CT." Nature communications 12(1): 1880.</p>
<p>A medical pre-diagnosis system for histopathological image of breast cancer. S Fan, R Xu, Z Yan, 14th International Congress on Image and Signal Processing. IEEEFan, S., R. Xu and Z. Yan (2021). A medical pre-diagnosis system for histopathological image of breast cancer. 2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), IEEE.</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 98Hochreiter, S. and J. Schmidhuber (1997). "Long short-term memory." Neural computation 9(8): 1735- 1780.</p>
<p>Deep learning approach for automated detection of retinal pathology from ultra-widefield retinal images. M Hu, J Amason, T Lee, Q Gao, D Borkar, M Pajic, M Hadziahmetovic, Investigative Ophthalmology &amp; Visual Science. 628Hu, M., J. Amason, T. Lee, Q. Gao, D. Borkar, M. Pajic and M. Hadziahmetovic (2021). "Deep learning approach for automated detection of retinal pathology from ultra-widefield retinal images." Investigative Ophthalmology &amp; Visual Science 62(8): 2129-2129.</p>
<p>Domain-adapted large language models for classifying nuclear medicine reports. Z Huemann, C Lee, J Hu, S Y Cho, T Bradshaw, arXiv:2303.01258arXiv preprintHuemann, Z., C. Lee, J. Hu, S. Y. Cho and T. Bradshaw (2023). "Domain-adapted large language models for classifying nuclear medicine reports." arXiv preprint arXiv:2303.01258.</p>
<p>ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports. K Jeblick, B Schachtner, J Dexl, A Mittermeier, A T Stber, J Topalis, T Weber, P Wesp, B Sabel, J Ricke, arXiv:2212.14882arXiv preprintJeblick, K., B. Schachtner, J. Dexl, A. Mittermeier, A. T. Stber, J. Topalis, T. Weber, P. Wesp, B. Sabel and J. Ricke (2022). "ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports." arXiv preprint arXiv:2212.14882.</p>
<p>Dermatobot: An Image Processing Enabled Chatbot for Diagnosis and Tele-remedy of Skin Diseases. S Kohli, U Verma, V V Kirpalani, R Srinath, 3rd International Conference for Emerging Technology (INCET). IEEEKohli, S., U. Verma, V. V. Kirpalani and R. Srinath (2022). Dermatobot: An Image Processing Enabled Chatbot for Diagnosis and Tele-remedy of Skin Diseases. 2022 3rd International Conference for Emerging Technology (INCET), IEEE.</p>
<p>Event-Based Clinical Finding Extraction from Radiology Reports with Pre-trained Language Model. W Lau, K Lybarger, M L Gunn, M Yetisgen, Journal of Digital Imaging. Lau, W., K. Lybarger, M. L. Gunn and M. Yetisgen (2022). "Event-Based Clinical Finding Extraction from Radiology Reports with Pre-trained Language Model." Journal of Digital Imaging: 1-14.</p>
<p>Pre-training of deep bidirectional transformers for language understanding. J Lee, K Toutanova, arXiv:1810.04805arXiv preprintLee, J. and K. Toutanova (2018). "Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805.</p>
<p>Evaluation of a deep learning supported remote diagnosis model for identification of diabetic retinopathy using wide-field Optomap. T Lee, M Hu, Q Gao, J Amason, D Borkar, D D&apos;alessio, M Canos, A Shariff, M Pajic, M Hadziahmetovic, Lee, T., M. Hu, Q. Gao, J. Amason, D. Borkar, D. D'Alessio, M. Canos, A. Shariff, M. Pajic and M. Hadziahmetovic (2022). "Evaluation of a deep learning supported remote diagnosis model for identification of diabetic retinopathy using wide-field Optomap."</p>
<p>Efficiency improvement in a busy radiology practice: determination of musculoskeletal magnetic resonance imaging protocol using deep-learning convolutional neural networks. Y H Lee, Journal of digital imaging. 31Lee, Y. H. (2018). "Efficiency improvement in a busy radiology practice: determination of musculoskeletal magnetic resonance imaging protocol using deep-learning convolutional neural networks." Journal of digital imaging 31: 604-610.</p>
<p>MRIonly based synthetic CT generation using dense cycle consistent generative adversarial networks. Y Lei, J Harms, T Wang, Y Liu, H.-K Shu, A B Jani, W J Curran, H Mao, T Liu, X Yang, Medical Physics. 468Lei, Y., J. Harms, T. Wang, Y. Liu, H.-K. Shu, A. B. Jani, W. J. Curran, H. Mao, T. Liu and X. Yang (2019). "MRI- only based synthetic CT generation using dense cycle consistent generative adversarial networks." Medical Physics 46(8): 3565-3581.</p>
<p>. Y Lei, J J Jeong, T Wang, H.-K Shu, P Patel, S Tian, T Liu, H Shim, H Mao, A Jani, W Curran, X , Lei, Y., J. J. Jeong, T. Wang, H.-K. Shu, P. Patel, S. Tian, T. Liu, H. Shim, H. Mao, A. Jani, W. Curran and X.</p>
<p>MRI-based pseudo CT synthesis using anatomical signature and alternating random forest with iterative refinement model. Yang , Journal of Medical Imaging. 5443504Yang (2018). "MRI-based pseudo CT synthesis using anatomical signature and alternating random forest with iterative refinement model." Journal of Medical Imaging 5(4): 043504.</p>
<p>. Y Li, J S Hsu, N Bari, X Qiu, M Viswanathan, W Shi, F Giuste, Y Zhong, J Sun, M D Wang, Li, Y., J. S. Hsu, N. Bari, X. Qiu, M. Viswanathan, W. Shi, F. Giuste, Y. Zhong, J. Sun and M. D. Wang (2022).</p>
<p>Interpretable Evaluation of Diabetic Retinopathy Grade Regarding Eye Color Fundus Images. IEEE 22nd International Conference on Bioinformatics and Bioengineering (BIBE). IEEEInterpretable Evaluation of Diabetic Retinopathy Grade Regarding Eye Color Fundus Images. 2022 IEEE 22nd International Conference on Bioinformatics and Bioengineering (BIBE), IEEE.</p>
<p>Clinically accurate chest x-ray report generation. G Liu, T.-M H Hsu, M Mcdermott, W Boag, W.-H Weng, P Szolovits, M Ghassemi, PMLRMachine Learning for Healthcare Conference. Liu, G., T.-M. H. Hsu, M. McDermott, W. Boag, W.-H. Weng, P. Szolovits and M. Ghassemi (2019). Clinically accurate chest x-ray report generation. Machine Learning for Healthcare Conference, PMLR.</p>
<p>Medicalvlbert: Medical visual language bert for covid-19 ct report generation with alternate learning. G Liu, Y Liao, F Wang, B Zhang, L Zhang, X Liang, X Wan, S Li, Z Li, S Zhang, IEEE Transactions on Neural Networks and Learning Systems. 329Liu, G., Y. Liao, F. Wang, B. Zhang, L. Zhang, X. Liang, X. Wan, S. Li, Z. Li and S. Zhang (2021). "Medical- vlbert: Medical visual language bert for covid-19 ct report generation with alternate learning." IEEE Transactions on Neural Networks and Learning Systems 32(9): 3786-3797.</p>
<p>Using a classification model for determining the value of liver radiological reports of patients with colorectal cancer. W Liu, X Zhang, H Lv, J Li, Y Liu, Z Yang, X Weng, Y Lin, H Song, Z Wang, Frontiers in Oncology. 12Liu, W., X. Zhang, H. Lv, J. Li, Y. Liu, Z. Yang, X. Weng, Y. Lin, H. Song and Z. Wang (2022). "Using a classification model for determining the value of liver radiological reports of patients with colorectal cancer." Frontiers in Oncology 12.</p>
<p>Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. Q Lyu, J Tan, M E Zapadka, J Ponnatapuram, C Niu, G Wang, C T Whitlow, arXiv:2303.09038arXiv preprintLyu, Q., J. Tan, M. E. Zapadka, J. Ponnatapuram, C. Niu, G. Wang and C. T. Whitlow (2023). "Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential." arXiv preprint arXiv:2303.09038.</p>
<p>Foundations of statistical natural language processing. C Manning, H Schutze, MIT pressManning, C. and H. Schutze (1999). Foundations of statistical natural language processing, MIT press.</p>
<p>COVID-19 Pandemic-Associated Changes in the Acuity of Brain MRI Findings: A Secondary Analysis of Reports Using Natural Language Processing. T L Min, L Xu, J D Choi, R Hu, J W Allen, C Reeves, D Hsu, R DuszakJr, J Switchenko, G Sadigh, Current Problems in Diagnostic Radiology. Min, T. L., L. Xu, J. D. Choi, R. Hu, J. W. Allen, C. Reeves, D. Hsu, R. Duszak Jr, J. Switchenko and G. Sadigh (2021). "COVID-19 Pandemic-Associated Changes in the Acuity of Brain MRI Findings: A Secondary Analysis of Reports Using Natural Language Processing." Current Problems in Diagnostic Radiology.</p>
<p>Berthop: An effective vision-and-language model for chest x-ray disease diagnosis. M Monajatipoor, M Rouhsedaghat, L H Li, C.-C. Jay Kuo, A Chien, K.-W Chang, 25th International Conference. SingaporeSpringer2022Proceedings, Part VMonajatipoor, M., M. Rouhsedaghat, L. H. Li, C.-C. Jay Kuo, A. Chien and K.-W. Chang (2022). Berthop: An effective vision-and-language model for chest x-ray disease diagnosis. Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference, Singapore, September 18- 22, 2022, Proceedings, Part V, Springer.</p>
<p>Vision-language transformer for interpretable pathology visual question answering. U Naseem, M Khushi, J Kim, IEEE Journal of Biomedical and Health Informatics. Naseem, U., M. Khushi and J. Kim (2022). "Vision-language transformer for interpretable pathology visual question answering." IEEE Journal of Biomedical and Health Informatics.</p>
<p>EDDIE-Transformer: Enriched Disease Embedding Transformer for X-Ray Report Generation. H T Nguyen, D Nie, T Badamdorj, Y Liu, L Hong, J Truong, L Cheng, IEEE 19th International Symposium on Biomedical Imaging (ISBI). IEEENguyen, H. T., D. Nie, T. Badamdorj, Y. Liu, L. Hong, J. Truong and L. Cheng (2022). EDDIE-Transformer: Enriched Disease Embedding Transformer for X-Ray Report Generation. 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), IEEE.</p>
<p>Improving Chest X-Ray Report Generation by Leveraging Text of Similar Images. S Niksaz, F Ghasemian, Available at SSRN 4211036Niksaz, S. and F. Ghasemian (2022). "Improving Chest X-Ray Report Generation by Leveraging Text of Similar Images." Available at SSRN 4211036.</p>
<p>Abdomen CT Multi-organ Segmentation Using Token-based MLP-Mixer. S Pan, C W Chang, T Wang, J Wynne, M Hu, Y Lei, T Liu, P Patel, J Roper, X Yang, Medical Physics. Pan, S., C. W. Chang, T. Wang, J. Wynne, M. Hu, Y. Lei, T. Liu, P. Patel, J. Roper and X. Yang (2022). "Abdomen CT Multi-organ Segmentation Using Token-based MLP-Mixer." Medical Physics.</p>
<p>Generative adversarial networks and radiomics supervision for lung lesion synthesis. S Pan, J Flores, C T Lin, J W Stayman, G Gang, SPIEPan, S., J. Flores, C. T. Lin, J. W. Stayman and G. Gang (2021). Generative adversarial networks and radiomics supervision for lung lesion synthesis, SPIE.</p>
<p>2D medical image synthesis using transformer-based denoising diffusion probabilistic model. S Pan, Z Tian, Y Lei, T Wang, J Zhou, M Mcdonald, J Bradley, T Liu, X Yang, ; Spie, S Pan, T Wang, R L Qiu, M Axente, C.-W Chang, Physics in Medicine and Biology. J. Peng, A. B. Patel, J. Shelton, S. A. Patel and J. RoperCVT-Vnet: a convolutional-transformer model for head and neck multi-organ segmentationPan, S., Z. Tian, Y. Lei, T. Wang, J. Zhou, M. McDonald, J. Bradley, T. Liu and X. Yang (2022). CVT-Vnet: a convolutional-transformer model for head and neck multi-organ segmentation, SPIE. Pan, S., T. Wang, R. L. Qiu, M. Axente, C.-W. Chang, J. Peng, A. B. Patel, J. Shelton, S. A. Patel and J. Roper (2023). "2D medical image synthesis using transformer-based denoising diffusion probabilistic model." Physics in Medicine and Biology.</p>
<p>Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis. C Pellegrini, M Keicher, E zsoy, P Jiraskova, R Braren, N Navab, arXiv:2303.13391arXiv preprintPellegrini, C., M. Keicher, E. zsoy, P. Jiraskova, R. Braren and N. Navab (2023). "Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis." arXiv preprint arXiv:2303.13391.</p>
<p>PaLM: A hybrid parser and language model. H Peng, R Schwartz, N A Smith, arXiv:1909.02134arXiv preprintPeng, H., R. Schwartz and N. A. Smith (2019). "PaLM: A hybrid parser and language model." arXiv preprint arXiv:1909.02134.</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, Radford, A., K. Narasimhan, T. Salimans and I. Sutskever (2018). "Improving language understanding by generative pre-training."</p>
<p>Learning the Treatment Process in Radiotherapy Using an Artificial Intelligence-Assisted Chatbot: Development Study. N Rebelo, L Sanders, K Li, J C Chow, JMIR Formative Research. 61239443Rebelo, N., L. Sanders, K. Li and J. C. Chow (2022). "Learning the Treatment Process in Radiotherapy Using an Artificial Intelligence-Assisted Chatbot: Development Study." JMIR Formative Research 6(12): e39443.</p>
<p>Learning internal representations by error propagation. D E Rumelhart, G E Hinton, R J Williams, California Univ San Diego La Jolla Inst for Cognitive ScienceRumelhart, D. E., G. E. Hinton and R. J. Williams (1985). Learning internal representations by error propagation, California Univ San Diego La Jolla Inst for Cognitive Science.</p>
<p>Detecting formal thought disorder by deep contextualized word representations. J Sarzynska-Wawer, A Wawer, A Pawlak, J Szymanowska, I Stefaniak, M Jarkiewicz, L Okruszek, Psychiatry Research. 304114135Sarzynska-Wawer, J., A. Wawer, A. Pawlak, J. Szymanowska, I. Stefaniak, M. Jarkiewicz and L. Okruszek (2021). "Detecting formal thought disorder by deep contextualized word representations." Psychiatry Research 304: 114135.</p>
<p>GANBERT: Generative adversarial networks with bidirectional encoder representations from transformers for MRI to PET synthesis. H.-C Shin, A Ihsani, S Mandava, S T Sreenivas, C Forster, J Cha, A S D N Initiative, arXiv:2008.04393arXiv preprintShin, H.-C., A. Ihsani, S. Mandava, S. T. Sreenivas, C. Forster, J. Cha and A. s. D. N. Initiative (2020). "GANBERT: Generative adversarial networks with bidirectional encoder representations from transformers for MRI to PET synthesis." arXiv preprint arXiv:2008.04393.</p>
<p>A diagnostic report generator from ct volumes on liver tumor with semi-supervised attention mechanism. J Tian, C Li, Z Shi, F Xu, 21st International Conference. Granada, SpainSpringerProceedings, Part II 11Tian, J., C. Li, Z. Shi and F. Xu (2018). A diagnostic report generator from ct volumes on liver tumor with semi-supervised attention mechanism. Medical Image Computing and Computer Assisted Intervention- MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11, Springer.</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozire, N Goyal, E Hambro, F Azhar, arXiv:2302.13971arXiv preprintTouvron, H., T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozire, N. Goyal, E. Hambro and F. Azhar (2023). "Llama: Open and efficient foundation language models." arXiv preprint arXiv:2302.13971.</p>
<p>UNeXt: MLP-based Rapid Medical Image Segmentation Network. J M J Valanarasu, V M Patel, arXiv:2203.04967arXiv preprintValanarasu, J. M. J. and V. M. Patel (2022). "UNeXt: MLP-based Rapid Medical Image Segmentation Network." arXiv preprint arXiv:2203.04967.</p>
<p>Advances in neural information processing systems 30. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez,  Kaiser, I Polosukhin, Attention is all you needVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, . Kaiser and I. Polosukhin (2017). "Attention is all you need." Advances in neural information processing systems 30.</p>
<p>Chatcad: Interactive computer-aided diagnosis on medical image using large language models. S Wang, Z Zhao, X Ouyang, Q Wang, D Shen, arXiv:2302.07257arXiv preprintWang, S., Z. Zhao, X. Ouyang, Q. Wang and D. Shen (2023). "Chatcad: Interactive computer-aided diagnosis on medical image using large language models." arXiv preprint arXiv:2302.07257.</p>
<p>Initial Experience with a COVID-19 Screening Chatbot Before Radiology Appointments. W T Wang, N Tan, J A Hanson, C A Crubaugh, A K Hara, Journal of Digital Imaging. 355Wang, W. T., N. Tan, J. A. Hanson, C. A. Crubaugh and A. K. Hara (2022). "Initial Experience with a COVID- 19 Screening Chatbot Before Radiology Appointments." Journal of Digital Imaging 35(5): 1303-1307.</p>
<p>Deep learning to automate the labelling of head MRI datasets for computer vision applications. D A Wood, S Kafiabadi, A Busaidi, E L Guilhem, J Lynch, M K Townend, A Montvila, M Kiik, J Siddiqui, N Gadapa, European Radiology. 32Wood, D. A., S. Kafiabadi, A. Al Busaidi, E. L. Guilhem, J. Lynch, M. K. Townend, A. Montvila, M. Kiik, J. Siddiqui and N. Gadapa (2022). "Deep learning to automate the labelling of head MRI datasets for computer vision applications." European Radiology 32: 725-736.</p>
<p>Automated Labelling using an Attention model for Radiology reports of MRI scans (ALARM). D A Wood, J Lynch, S Kafiabadi, E Guilhem, A Busaidi, A Montvila, T Varsavsky, J Siddiqui, N Gadapa, M Townend, PMLRMedical Imaging with Deep Learning. Wood, D. A., J. Lynch, S. Kafiabadi, E. Guilhem, A. Al Busaidi, A. Montvila, T. Varsavsky, J. Siddiqui, N. Gadapa and M. Townend (2020). Automated Labelling using an Attention model for Radiology reports of MRI scans (ALARM). Medical Imaging with Deep Learning, PMLR.</p>
<p>AGNet: Automatic generation network for skin imaging reports. F Wu, H Yang, L Peng, Z Lian, M Li, G Qu, S Jiang, Y Han, Computers in biology and medicine. 141105037Wu, F., H. Yang, L. Peng, Z. Lian, M. Li, G. Qu, S. Jiang and Y. Han (2022). "AGNet: Automatic generation network for skin imaging reports." Computers in biology and medicine 141: 105037.</p>
<p>Medical Image Enhancement Using Super Resolution Methods. Computational Science -ICCS 2020. K Yamashita, K Markov, Springer International PublishingChamYamashita, K. and K. Markov (2020). Medical Image Enhancement Using Super Resolution Methods. Computational Science -ICCS 2020, Cham, Springer International Publishing.</p>
<p>Weakly guided hierarchical encoder-decoder network for brain ct report generation. S Yang, J Ji, X Zhang, Y Liu, Z Wang, 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEEYang, S., J. Ji, X. Zhang, Y. Liu and Z. Wang (2021). Weakly guided hierarchical encoder-decoder network for brain ct report generation. 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), IEEE.</p>
<p>Automatic ultrasound image report generation with adaptive multimodal attention mechanism. S Yang, J Niu, J Wu, Y Wang, X Liu, Q Li, Neurocomputing. 427Yang, S., J. Niu, J. Wu, Y. Wang, X. Liu and Q. Li (2021). "Automatic ultrasound image report generation with adaptive multimodal attention mechanism." Neurocomputing 427: 40-49.</p>
<p>Pathologist-level interpretable whole-slide cancer diagnosis with deep learning. Z Zhang, P Chen, M Mcgough, F Xing, C Wang, M Bui, Y Xie, M Sapkota, L Cui, J Dhillon, Nature Machine Intelligence. 15Zhang, Z., P. Chen, M. McGough, F. Xing, C. Wang, M. Bui, Y. Xie, M. Sapkota, L. Cui and J. Dhillon (2019). "Pathologist-level interpretable whole-slide cancer diagnosis with deep learning." Nature Machine Intelligence 1(5): 236-245.</p>
<p>Mdnet: A semantically and visually interpretable medical image diagnosis network. Z Zhang, Y Xie, F Xing, M Mcgough, L Yang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhang, Z., Y. Xie, F. Xing, M. McGough and L. Yang (2017). Mdnet: A semantically and visually interpretable medical image diagnosis network. Proceedings of the IEEE conference on computer vision and pattern recognition.</p>
<p>An automatically thyroid nodules feature extraction and description network for ultrasound images. Y Zhao, M Tian, J Jin, Q Wang, J Song, Y Shen, IEEE International Ultrasonics Symposium (IUS). IEEEZhao, Y., M. Tian, J. Jin, Q. Wang, J. Song and Y. Shen (2021). An automatically thyroid nodules feature extraction and description network for ultrasound images. 2021 IEEE International Ultrasonics Symposium (IUS), IEEE.</p>
<p>UNet++: A Nested U-Net Architecture for Medical Image Segmentation. Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Z Zhou, M M Rahman Siddiquee, N Tajbakhsh, J Liang, Springer International PublishingChamZhou, Z., M. M. Rahman Siddiquee, N. Tajbakhsh and J. Liang (2018). UNet++: A Nested U-Net Architecture for Medical Image Segmentation. Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Cham, Springer International Publishing.</p>            </div>
        </div>

    </div>
</body>
</html>
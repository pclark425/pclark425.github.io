<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5006 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5006</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5006</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-263605725</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.00836v1.pdf" target="_blank">Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence. Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non trivial manual effort. Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems. Consequently, there is a growing interest in using LLMs for logical reasoning via natural language. This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning. To offer a thorough analysis, we have compiled a benchmark titled LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive, and inductive reasoning. We have standardized these datasets into Seq2Seq tasks to facilitate straightforward training and evaluation for future research. Utilizing LogiGLUE as a foundation, we have trained an instruction fine tuned language model, resulting in LogiT5. We study single task training, multi task training, and a chain of thought knowledge distillation fine tuning technique to assess the performance of model across the different logical reasoning categories. By this comprehensive process, we aim to shed light on the capabilities and potential pathways for enhancing logical reasoning proficiency in LLMs, paving the way for more advanced and nuanced developments in this critical field.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5006.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5006.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-fine-tuned seq2seq T5 variant used as the primary base model for experiments; fine-tuned on individual logical-reasoning datasets and used as the student in CoT distillation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-fine-tuned T5 sequence-to-sequence model used as the base model for single-task and multi-task fine-tuning as well as chain-of-thought (CoT) distillation experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogiGLUE (in-domain and out-of-domain subsets including LogiQA, ProofWriter, ANLI, αNLI, CLUTTR, bAbi, ReClor, PrOntoQA, FOLIO, Rulebert-Union, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LogiGLUE is a standardized collection of 24 datasets covering deductive, abductive and inductive reasoning presented in seq2seq format; tasks span MCQA, NLI, fact verification (FV), and free-form QA.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Single-task supervised fine-tuning on each dataset; also used as the student model in chain-of-thought knowledge distillation (Fine-tune-CoT) where CoT rationales were generated by a larger LLM and used to fine-tune Flan-T5; also evaluated with CoT-style prompting in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>In the paper's in-domain experiments Flan-T5-large obtains an average score of 38.63 across LogiGLUE in-domain datasets (Table 2). It performs substantially better on synthetic datasets than on handcrafted/human-created datasets. Using CoT-distilled data (LogiQA): when distilled with 15K CoT samples (from Llama-7B) Flan-T5 saw about a +4% improvement on LogiQA; smaller CoT datasets (3K, 6K) did not meaningfully improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Strong IID (in-domain) performance on synthetic data but poor out-of-distribution generalization; struggles on handcrafted human-authored datasets; CoT distillation requires large numbers of teacher-generated rationales and longer/higher-rate training to be effective; zero-shot CoT prompting provided little advantage in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperformed by the multi-task fine-tuned model LogiT5 (average 54.50 vs 38.63). In zero-shot/out-of-domain settings, Llama-2 (7B) produced poor or inconsistent results compared to Flan-T5-large's structured outputs. CoT-distilled Flan-T5 improved when sufficient distilled data available but still limited on hardest tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Single-task vs multi-task: single-task fine-tuning can reach near-perfect IID on some datasets but generalizes poorly; multi-task fine-tuning (weighted sampling) improved low-resource tasks (examples: +5% on αARCT and +8% on FOLIO). CoT distillation ablation: 3K and 6K teacher-generated CoT samples did not help, while 15K samples produced ~4% improvement on LogiQA; larger epochs and higher initial learning rate (3e-4) helped training stability for CoT distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5006.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5006.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogiT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogiT5 (multi-task instruction-finetuned model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A seq2seq model derived by multi-task fine-tuning Flan-T5-large on the LogiGLUE in-domain datasets with weighted sampling; demonstrates stronger average and generalization performance on logical reasoning tasks compared to the base Flan-T5-large.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogiT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-task fine-tuned version of Flan-T5-large trained on the assembled LogiGLUE in-domain datasets using a weighted sampling scheme to handle dataset size imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogiGLUE (in-domain) and evaluated on out-of-domain datasets in LogiGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same LogiGLUE benchmark described above (24 datasets across deductive, abductive, inductive reasoning presented as seq2seq tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Multi-task supervised fine-tuning on all in-domain LogiGLUE datasets with weighted sampling; then evaluated zero-shot on out-of-domain datasets; additionally the model was fine-tuned further on single datasets as a probe (no large gain observed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LogiT5 achieves an average in-domain score of 54.50 across LogiGLUE (Table 2), substantially outperforming the base Flan-T5-large (38.63 average). Multi-task training provided notable improvements on low-resource tasks (e.g., +5% on αARCT and +8% on FOLIO). Further single-dataset fine-tuning of LogiT5 produced only marginal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Multi-task training did not improve tasks that already had large training sets (αNLI, ANLI). Still struggles with the most challenging human-authored datasets despite overall gains. Additional fine-tuning on single datasets yields minimal improvement, suggesting saturation of task-specific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms single-task fine-tuned Flan-T5-large on average and especially on small-data tasks. Compared to open LLMs like Llama-2 (7B) in zero-shot settings, LogiT5 shows stronger structured answer generation and better generalization in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Weighted sampling for multi-task training performed better than random sampling (appendix). Multi-task training effect is dataset-size dependent: helps low-resource datasets significantly but offers little or no benefit for large-data tasks. Further fine-tuning LogiT5 on single datasets yields only marginal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5006.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5006.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation LLM evaluated zero-shot and used as a teacher to generate chain-of-thought rationales for distillation; its free-form outputs posed evaluation/faithfulness challenges in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source large language model (7B) used for zero-shot evaluation with CoT prompting and as the teacher model to generate CoT rationales for distillation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Out-of-domain LogiGLUE datasets; generation of chain-of-thought rationales for LogiQA distillation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated on the LogiGLUE benchmark (out-of-domain splits) and used to produce multi-step rationales for LogiQA items used in teacher–student distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation, chain-of-thought prompting by appending 'let's think step by step' for zero-shot reasoning, and sampled multiple CoT rationales per question (1 or 10) to produce training sets of varying sizes (3K, 6K, 15K) for student distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Preliminary zero-shot results on out-of-domain tests were relatively poor; in the paper's Table 2 Llama (non-CoT) column shows an average of 43.63 while the Llama-CoT column average is 37.78 (suggesting CoT prompting did not improve zero-shot performance in this evaluation). As a CoT teacher, Llama-7B produced ~3K usable single-answer samples, 6K when multiple attempts per question were used, and 15K when multiple valid reasoning paths were included.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Outputs are often free-form and not in the structured answer options format, making automatic evaluation difficult; sometimes generates synonyms or paraphrases of correct answers; occasionally ignores the provided input and outputs answers based on pretraining memorized knowledge; CoT prompting did not reliably improve zero-shot reasoning in these experiments; teacher-generated CoTs can be unfaithful or incomplete, requiring filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms the multi-task-fine-tuned LogiT5 on the LogiGLUE in-domain evaluations. However, as a teacher for CoT distillation, Llama-7B can generate many rationales—yet only large volumes (15K) yielded meaningful gains when used to fine-tune a smaller student model (Flan-T5).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>CoT prompting for Llama-2 did not improve zero-shot performance in this setup; distillation data produced by Llama-7B required size scaling (3K/6K insufficient, 15K produced measurable improvement). Manual inspection revealed synonymy and context-ignoring failures, motivating use of ConceptNet synonym matching during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5006.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5006.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-distillation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Knowledge Distillation (Fine-tune-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where a larger LLM generates step-by-step rationales (CoTs) for questions, filtered by answer correctness, and those examples (question + rationale + answer) are used to fine-tune smaller student models to improve reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5 (student) with Llama-7B (teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Teacher LLM (Llama-7B) generates rationales; student model (Flan-T5-large) is fine-tuned on the teacher-generated CoT dataset. Training used larger LR (3e-4) and many epochs (e.g., 40) for best results.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogiQA (used as primary target for CoT distillation experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LogiQA is a challenging multiple-choice logical reasoning dataset drawn from exam-like, human-authored questions requiring multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Generate CoT rationales from the teacher model; filter samples by correctness of final answer; aggregate 1 or multiple correct rationales per item to produce datasets of different sizes (3K, 6K, 15K); fine-tune student model on these CoT-augmented examples with increased learning rate and many epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Distillation with 3K or 6K CoT samples did not meaningfully improve the student's performance on LogiQA. Using 15K CoT samples produced an approximate +4% improvement on LogiQA compared to the non-distilled Flan-T5 baseline. Training with CoT data required longer training and benefited from a larger initial learning rate (3e-4) and more epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Effectiveness strongly depends on the quantity (and quality) of teacher-generated CoTs; teacher errors and incomplete/unfaithful proofs can propagate; distillation increases training time and computation; not all datasets benefit from this method unless dataset size is sufficiently large.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>When sufficient distilled data exists (15K), CoT-distilled Flan-T5 outperformed the non-distilled Flan-T5 on LogiQA by ~4%; however, CoT-distillation did not outperform the multi-task LogiT5 broadly, and small distilled sets (3K/6K) were ineffective.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Dataset-size ablation: 3K and 6K distilled samples ineffective; 15K produced measurable gains. Training hyperparameter ablation: higher initial learning rate (3e-4) and more epochs (40) were beneficial. The authors note that multiple correct proof paths exist and including multiple paths increased the distilled dataset (to 15K) and improved results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5006.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5006.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogiGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogiGLUE benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark assembled in this work consisting of 24 logical-reasoning datasets standardized into a seq2seq format to evaluate deductive, abductive, and inductive reasoning capabilities of language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogiGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Collection of 24 datasets (10 in-domain, 12 out-of-domain) covering diverse logical reasoning types (deductive, abductive, inductive) and task formats (MCQA, NLI, FV/FC, free-form QA), all converted to a common seq2seq format for ease of training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Aggregate of tasks: examples include LogiQA, ProofWriter, ANLI, αNLI, CLUTTR, bAbi (deductive and inductive splits), ReClor, PrOntoQA, FOLIO, Rulebert-Union, Winologic, WaNLI, and others.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Designed to probe strict logical reasoning in natural language across dataset creation styles (synthetic vs handcrafted) and to include an explicit out-of-domain test suite to assess generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Standardized dataset formatting to seq2seq; selection criteria emphasize diversity and generalization; includes weighted sampling strategy for multi-task training to mitigate dataset-size imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Used to evaluate Flan-T5-large (avg 38.63), LogiT5 (avg 54.50), and Llama-2 (7B) variants (averages reported in Table 2). Models generally perform well on synthetic rule-based datasets but poorly on handcrafted human-authored datasets; multi-task training improved low-resource datasets substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Excludes many NLI datasets that rely on broad background knowledge (e.g., SNLI, MultiNLI) and tasks requiring retrieval of external knowledge to isolate pure logical reasoning; difficulty varies strongly with dataset creation method (synthetic templates easier, human-authored harder).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Benchmark used to compare single-task fine-tuning, multi-task fine-tuning (LogiT5), CoT distillation and zero-shot LLMs; demonstrated that multi-task fine-tuning (LogiT5) yields stronger average performance and better low-resource generalization than single-task Fine-tune Flan-T5, while open LLMs like Llama-2 produced inconsistent zero-shot behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5006.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5006.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 / ChatGPT / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large autoregressive language models referenced in the survey as examples of models that can perform few-shot and chain-of-thought reasoning but which may rely on memorized steps or degrade on out-of-distribution logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 / ChatGPT / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained autoregressive LLMs (OpenAI) mentioned in related-work: shown in prior literature to be effective few-shot reasoners and to generate CoT rationales, but also to sometimes recite memorized steps and to show OOD degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Various logical reasoning benchmarks referenced in the literature (e.g., some LogiGLUE analogs and other reasoning datasets); this paper cites prior work noting GPT-family performance on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not specific to this paper's experiments; cited that ChatGPT and GPT-4 perform well on some benchmarks but worse on new/out-of-distribution datasets (citing Liu et al., 2023b) and that LLMs can sometimes retrieve rather than reason.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Discussed in context of few-shot in-context learning and chain-of-thought prompting; not experimentally evaluated by the authors in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No quantitative results provided in this paper; cited literature states few-shot and CoT capabilities but reports that ChatGPT/GPT-4 performance can drop substantially on out-of-distribution tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>May retrieve or recite previously seen facts and reasoning steps instead of performing genuine reasoning; performance can be brittle out-of-distribution; CoT explanations can be unfaithful.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Cited relative to Flan-T5 and LogiT5 as examples where large LLMs can be good few-shot reasoners but reliability and generalization to novel logical tasks remain concerns according to referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are reasoning teachers <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>RuleBERT: Teaching soft rules to pre-trained lms <em>(Rating: 2)</em></li>
                <li>APOLLO: A simple approach for adaptive pretraining of language models for logical reasoning <em>(Rating: 2)</em></li>
                <li>MERIt: Meta-path guided contrastive learning for logical reasoning <em>(Rating: 1)</em></li>
                <li>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought <em>(Rating: 1)</em></li>
                <li>LogiQA: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5006",
    "paper_id": "paper-263605725",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "Flan-T5",
            "name_full": "Flan-T5-large",
            "brief_description": "Instruction-fine-tuned seq2seq T5 variant used as the primary base model for experiments; fine-tuned on individual logical-reasoning datasets and used as the student in CoT distillation experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-large",
            "model_description": "An instruction-fine-tuned T5 sequence-to-sequence model used as the base model for single-task and multi-task fine-tuning as well as chain-of-thought (CoT) distillation experiments in this paper.",
            "model_size": null,
            "logical_reasoning_task": "LogiGLUE (in-domain and out-of-domain subsets including LogiQA, ProofWriter, ANLI, αNLI, CLUTTR, bAbi, ReClor, PrOntoQA, FOLIO, Rulebert-Union, etc.)",
            "task_description": "LogiGLUE is a standardized collection of 24 datasets covering deductive, abductive and inductive reasoning presented in seq2seq format; tasks span MCQA, NLI, fact verification (FV), and free-form QA.",
            "method_or_approach": "Single-task supervised fine-tuning on each dataset; also used as the student model in chain-of-thought knowledge distillation (Fine-tune-CoT) where CoT rationales were generated by a larger LLM and used to fine-tune Flan-T5; also evaluated with CoT-style prompting in zero-shot.",
            "performance": "In the paper's in-domain experiments Flan-T5-large obtains an average score of 38.63 across LogiGLUE in-domain datasets (Table 2). It performs substantially better on synthetic datasets than on handcrafted/human-created datasets. Using CoT-distilled data (LogiQA): when distilled with 15K CoT samples (from Llama-7B) Flan-T5 saw about a +4% improvement on LogiQA; smaller CoT datasets (3K, 6K) did not meaningfully improve performance.",
            "limitations_or_failure_cases": "Strong IID (in-domain) performance on synthetic data but poor out-of-distribution generalization; struggles on handcrafted human-authored datasets; CoT distillation requires large numbers of teacher-generated rationales and longer/higher-rate training to be effective; zero-shot CoT prompting provided little advantage in some settings.",
            "comparison": "Outperformed by the multi-task fine-tuned model LogiT5 (average 54.50 vs 38.63). In zero-shot/out-of-domain settings, Llama-2 (7B) produced poor or inconsistent results compared to Flan-T5-large's structured outputs. CoT-distilled Flan-T5 improved when sufficient distilled data available but still limited on hardest tasks.",
            "ablation_or_analysis_results": "Single-task vs multi-task: single-task fine-tuning can reach near-perfect IID on some datasets but generalizes poorly; multi-task fine-tuning (weighted sampling) improved low-resource tasks (examples: +5% on αARCT and +8% on FOLIO). CoT distillation ablation: 3K and 6K teacher-generated CoT samples did not help, while 15K samples produced ~4% improvement on LogiQA; larger epochs and higher initial learning rate (3e-4) helped training stability for CoT distillation.",
            "uuid": "e5006.0",
            "source_info": {
                "paper_title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LogiT5",
            "name_full": "LogiT5 (multi-task instruction-finetuned model)",
            "brief_description": "A seq2seq model derived by multi-task fine-tuning Flan-T5-large on the LogiGLUE in-domain datasets with weighted sampling; demonstrates stronger average and generalization performance on logical reasoning tasks compared to the base Flan-T5-large.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LogiT5",
            "model_description": "Multi-task fine-tuned version of Flan-T5-large trained on the assembled LogiGLUE in-domain datasets using a weighted sampling scheme to handle dataset size imbalance.",
            "model_size": null,
            "logical_reasoning_task": "LogiGLUE (in-domain) and evaluated on out-of-domain datasets in LogiGLUE",
            "task_description": "Same LogiGLUE benchmark described above (24 datasets across deductive, abductive, inductive reasoning presented as seq2seq tasks).",
            "method_or_approach": "Multi-task supervised fine-tuning on all in-domain LogiGLUE datasets with weighted sampling; then evaluated zero-shot on out-of-domain datasets; additionally the model was fine-tuned further on single datasets as a probe (no large gain observed).",
            "performance": "LogiT5 achieves an average in-domain score of 54.50 across LogiGLUE (Table 2), substantially outperforming the base Flan-T5-large (38.63 average). Multi-task training provided notable improvements on low-resource tasks (e.g., +5% on αARCT and +8% on FOLIO). Further single-dataset fine-tuning of LogiT5 produced only marginal gains.",
            "limitations_or_failure_cases": "Multi-task training did not improve tasks that already had large training sets (αNLI, ANLI). Still struggles with the most challenging human-authored datasets despite overall gains. Additional fine-tuning on single datasets yields minimal improvement, suggesting saturation of task-specific knowledge.",
            "comparison": "Outperforms single-task fine-tuned Flan-T5-large on average and especially on small-data tasks. Compared to open LLMs like Llama-2 (7B) in zero-shot settings, LogiT5 shows stronger structured answer generation and better generalization in many cases.",
            "ablation_or_analysis_results": "Weighted sampling for multi-task training performed better than random sampling (appendix). Multi-task training effect is dataset-size dependent: helps low-resource datasets significantly but offers little or no benefit for large-data tasks. Further fine-tuning LogiT5 on single datasets yields only marginal gains.",
            "uuid": "e5006.1",
            "source_info": {
                "paper_title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama-2 (7B)",
            "brief_description": "An open foundation LLM evaluated zero-shot and used as a teacher to generate chain-of-thought rationales for distillation; its free-form outputs posed evaluation/faithfulness challenges in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2 (7B)",
            "model_description": "Open-source large language model (7B) used for zero-shot evaluation with CoT prompting and as the teacher model to generate CoT rationales for distillation experiments.",
            "model_size": "7B",
            "logical_reasoning_task": "Out-of-domain LogiGLUE datasets; generation of chain-of-thought rationales for LogiQA distillation",
            "task_description": "Evaluated on the LogiGLUE benchmark (out-of-domain splits) and used to produce multi-step rationales for LogiQA items used in teacher–student distillation.",
            "method_or_approach": "Zero-shot evaluation, chain-of-thought prompting by appending 'let's think step by step' for zero-shot reasoning, and sampled multiple CoT rationales per question (1 or 10) to produce training sets of varying sizes (3K, 6K, 15K) for student distillation.",
            "performance": "Preliminary zero-shot results on out-of-domain tests were relatively poor; in the paper's Table 2 Llama (non-CoT) column shows an average of 43.63 while the Llama-CoT column average is 37.78 (suggesting CoT prompting did not improve zero-shot performance in this evaluation). As a CoT teacher, Llama-7B produced ~3K usable single-answer samples, 6K when multiple attempts per question were used, and 15K when multiple valid reasoning paths were included.",
            "limitations_or_failure_cases": "Outputs are often free-form and not in the structured answer options format, making automatic evaluation difficult; sometimes generates synonyms or paraphrases of correct answers; occasionally ignores the provided input and outputs answers based on pretraining memorized knowledge; CoT prompting did not reliably improve zero-shot reasoning in these experiments; teacher-generated CoTs can be unfaithful or incomplete, requiring filtering.",
            "comparison": "Underperforms the multi-task-fine-tuned LogiT5 on the LogiGLUE in-domain evaluations. However, as a teacher for CoT distillation, Llama-7B can generate many rationales—yet only large volumes (15K) yielded meaningful gains when used to fine-tune a smaller student model (Flan-T5).",
            "ablation_or_analysis_results": "CoT prompting for Llama-2 did not improve zero-shot performance in this setup; distillation data produced by Llama-7B required size scaling (3K/6K insufficient, 15K produced measurable improvement). Manual inspection revealed synonymy and context-ignoring failures, motivating use of ConceptNet synonym matching during evaluation.",
            "uuid": "e5006.2",
            "source_info": {
                "paper_title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CoT-distillation",
            "name_full": "Chain-of-Thought Knowledge Distillation (Fine-tune-CoT)",
            "brief_description": "A method where a larger LLM generates step-by-step rationales (CoTs) for questions, filtered by answer correctness, and those examples (question + rationale + answer) are used to fine-tune smaller student models to improve reasoning performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5 (student) with Llama-7B (teacher)",
            "model_description": "Teacher LLM (Llama-7B) generates rationales; student model (Flan-T5-large) is fine-tuned on the teacher-generated CoT dataset. Training used larger LR (3e-4) and many epochs (e.g., 40) for best results.",
            "model_size": null,
            "logical_reasoning_task": "LogiQA (used as primary target for CoT distillation experiments)",
            "task_description": "LogiQA is a challenging multiple-choice logical reasoning dataset drawn from exam-like, human-authored questions requiring multi-step reasoning.",
            "method_or_approach": "Generate CoT rationales from the teacher model; filter samples by correctness of final answer; aggregate 1 or multiple correct rationales per item to produce datasets of different sizes (3K, 6K, 15K); fine-tune student model on these CoT-augmented examples with increased learning rate and many epochs.",
            "performance": "Distillation with 3K or 6K CoT samples did not meaningfully improve the student's performance on LogiQA. Using 15K CoT samples produced an approximate +4% improvement on LogiQA compared to the non-distilled Flan-T5 baseline. Training with CoT data required longer training and benefited from a larger initial learning rate (3e-4) and more epochs.",
            "limitations_or_failure_cases": "Effectiveness strongly depends on the quantity (and quality) of teacher-generated CoTs; teacher errors and incomplete/unfaithful proofs can propagate; distillation increases training time and computation; not all datasets benefit from this method unless dataset size is sufficiently large.",
            "comparison": "When sufficient distilled data exists (15K), CoT-distilled Flan-T5 outperformed the non-distilled Flan-T5 on LogiQA by ~4%; however, CoT-distillation did not outperform the multi-task LogiT5 broadly, and small distilled sets (3K/6K) were ineffective.",
            "ablation_or_analysis_results": "Dataset-size ablation: 3K and 6K distilled samples ineffective; 15K produced measurable gains. Training hyperparameter ablation: higher initial learning rate (3e-4) and more epochs (40) were beneficial. The authors note that multiple correct proof paths exist and including multiple paths increased the distilled dataset (to 15K) and improved results.",
            "uuid": "e5006.3",
            "source_info": {
                "paper_title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LogiGLUE",
            "name_full": "LogiGLUE benchmark",
            "brief_description": "A benchmark assembled in this work consisting of 24 logical-reasoning datasets standardized into a seq2seq format to evaluate deductive, abductive, and inductive reasoning capabilities of language models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LogiGLUE",
            "model_description": "Collection of 24 datasets (10 in-domain, 12 out-of-domain) covering diverse logical reasoning types (deductive, abductive, inductive) and task formats (MCQA, NLI, FV/FC, free-form QA), all converted to a common seq2seq format for ease of training and evaluation.",
            "model_size": null,
            "logical_reasoning_task": "Aggregate of tasks: examples include LogiQA, ProofWriter, ANLI, αNLI, CLUTTR, bAbi (deductive and inductive splits), ReClor, PrOntoQA, FOLIO, Rulebert-Union, Winologic, WaNLI, and others.",
            "task_description": "Designed to probe strict logical reasoning in natural language across dataset creation styles (synthetic vs handcrafted) and to include an explicit out-of-domain test suite to assess generalization.",
            "method_or_approach": "Standardized dataset formatting to seq2seq; selection criteria emphasize diversity and generalization; includes weighted sampling strategy for multi-task training to mitigate dataset-size imbalance.",
            "performance": "Used to evaluate Flan-T5-large (avg 38.63), LogiT5 (avg 54.50), and Llama-2 (7B) variants (averages reported in Table 2). Models generally perform well on synthetic rule-based datasets but poorly on handcrafted human-authored datasets; multi-task training improved low-resource datasets substantially.",
            "limitations_or_failure_cases": "Excludes many NLI datasets that rely on broad background knowledge (e.g., SNLI, MultiNLI) and tasks requiring retrieval of external knowledge to isolate pure logical reasoning; difficulty varies strongly with dataset creation method (synthetic templates easier, human-authored harder).",
            "comparison": "Benchmark used to compare single-task fine-tuning, multi-task fine-tuning (LogiT5), CoT distillation and zero-shot LLMs; demonstrated that multi-task fine-tuning (LogiT5) yields stronger average performance and better low-resource generalization than single-task Fine-tune Flan-T5, while open LLMs like Llama-2 produced inconsistent zero-shot behavior.",
            "uuid": "e5006.4",
            "source_info": {
                "paper_title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-family",
            "name_full": "GPT-3 / ChatGPT / GPT-4",
            "brief_description": "Large autoregressive language models referenced in the survey as examples of models that can perform few-shot and chain-of-thought reasoning but which may rely on memorized steps or degrade on out-of-distribution logical reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3 / ChatGPT / GPT-4",
            "model_description": "Large pretrained autoregressive LLMs (OpenAI) mentioned in related-work: shown in prior literature to be effective few-shot reasoners and to generate CoT rationales, but also to sometimes recite memorized steps and to show OOD degradation.",
            "model_size": null,
            "logical_reasoning_task": "Various logical reasoning benchmarks referenced in the literature (e.g., some LogiGLUE analogs and other reasoning datasets); this paper cites prior work noting GPT-family performance on benchmarks.",
            "task_description": "Not specific to this paper's experiments; cited that ChatGPT and GPT-4 perform well on some benchmarks but worse on new/out-of-distribution datasets (citing Liu et al., 2023b) and that LLMs can sometimes retrieve rather than reason.",
            "method_or_approach": "Discussed in context of few-shot in-context learning and chain-of-thought prompting; not experimentally evaluated by the authors in this paper.",
            "performance": "No quantitative results provided in this paper; cited literature states few-shot and CoT capabilities but reports that ChatGPT/GPT-4 performance can drop substantially on out-of-distribution tasks.",
            "limitations_or_failure_cases": "May retrieve or recite previously seen facts and reasoning steps instead of performing genuine reasoning; performance can be brittle out-of-distribution; CoT explanations can be unfaithful.",
            "comparison": "Cited relative to Flan-T5 and LogiT5 as examples where large LLMs can be good few-shot reasoners but reliability and generalization to novel logical tasks remain concerns according to referenced work.",
            "uuid": "e5006.5",
            "source_info": {
                "paper_title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are reasoning teachers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_reasoning_teachers"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "RuleBERT: Teaching soft rules to pre-trained lms",
            "rating": 2,
            "sanitized_title": "rulebert_teaching_soft_rules_to_pretrained_lms"
        },
        {
            "paper_title": "APOLLO: A simple approach for adaptive pretraining of language models for logical reasoning",
            "rating": 2,
            "sanitized_title": "apollo_a_simple_approach_for_adaptive_pretraining_of_language_models_for_logical_reasoning"
        },
        {
            "paper_title": "MERIt: Meta-path guided contrastive learning for logical reasoning",
            "rating": 1,
            "sanitized_title": "merit_metapath_guided_contrastive_learning_for_logical_reasoning"
        },
        {
            "paper_title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "rating": 1,
            "sanitized_title": "language_models_are_greedy_reasoners_a_systematic_formal_analysis_of_chainofthought"
        },
        {
            "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        }
    ],
    "cost": 0.021938,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models</p>
<p>Man Luo 
Arizona State University</p>
<p>Mayo Clinic</p>
<p>Shrinidhi Kumbhar skumbha4@asu.edu 
Arizona State University</p>
<p>Ming Shen 
Arizona State University</p>
<p>Mihir Parmar 
Arizona State University</p>
<p>Neeraj Varshney 
Arizona State University</p>
<p>Pratyay Banerjee 
Amazon Alexa AI</p>
<p>Somak Aditya 
IIT KGP</p>
<p>Chitta Baral 
Arizona State University</p>
<p>Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models
0028F5B29808AF7D6E1D462D7FDAFB68
Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence.Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non-trivial manual effort.Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems.Consequently, there's a growing interest in using LLMs for logical reasoning via natural language.This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning.To offer a thorough analysis, we've compiled a benchmark titled LogiGLUE.This includes 24 varied datasets encompassing deductive, abductive, and inductive reasoning.We have standardized these datasets into Seq2Seq tasks to facilitate straightforward training and evaluation for future research.Utilizing LogiGLUE as a foundation, we have trained an instruction fine-tuned language model, resulting in LogiT5.We study single-task training, multi-task training, and a "chain-of-thought" knowledge distillation finetuning technique to assess the model's performance across the different logical reasoning categories.By this comprehensive process, we aim to shed light on the capabilities and potential pathways for enhancing logical reasoning proficiency in LLMs, paving the way for more advanced and nuanced developments in this critical field.</p>
<p>Introduction</p>
<p>With logical reasoning, humans can explain an answer to a question via step-wise deduction, make robust planning and decision, or even reason about 0 *Equal contribution the workings in an unseen universe.As an example of logical reasoning, physicist Stephen Hawking derived the area theorem in 1971, which indicates the boundary beyond which nothing can ever escape (black hole).If Hawking's area theorem holds, then using logical reasoning, we can conclude that the horizon area of the new black hole should not be smaller than the total horizon area of its parent black holes.This theorem was later confirmed by real observation by MIT researchers in 2015, fifty years later after the theorem had been derived1 .</p>
<p>In the field of Artificial Intelligence (AI), there has been significant attention directed towards the aspiration to develop machines equipped with logical reasoning capabilities (McCarthy, 1989;Colmerauer and Roussel, 1996).Early approaches in logical reasoning were primarily dedicated to the design of formal logic languages to encapsulate rules and knowledge, along with the development of automated theorem provers (Lifschitz, 2019).This paradigm, however, necessitated a deep understanding of the syntax and semantics of the formal logic for manual rule formulation -making knowledge representation and knowledge acquisition hard and expert-driven endeavor.Due to these challenges, contemporary research has progressively turned towards addressing logical reasoning tasks (Clark et al., 2020;Tian et al., 2021a;Han et al., 2022) by employing transformerbased (Vaswani et al., 2017) pre-trained language models (Devlin et al., 2019a;Brown et al., 2020).</p>
<p>The Language models (LMs) that are pretrained using objectives such as the mask language modeling (Devlin et al., 2019a) and next word prediction (Brown et al., 2020) enables them to acquire adequate syntax and semantics of language, alongside commonsense knowledge.These language models can excel in numerous natural language understanding tasks, owing to the unsupervised pretraining on a vast array of unstructured text data.However, it is unclear if the current pretraining objectives are sufficient enough for the models to infer logical reasoning because this involves understanding structure; coupled with inductive, deductive, and abductive reasoning skills.This question has drawn intense attention and inspired different research directions to examine if LMs can learn logical reasoning ability (Wu et al., 2023;Lanham et al., 2023;Clark et al., 2020;Joshi et al., 2020).For instance, Clark et al. (2020) shows that pre-trained language models can serve as a "soft-reasoner" based on their near-perfect performance on synthetic datasets.Creswell et al. (2022) showed that large LMs are few-shot logical reasoners.On the other hand, Liu et al. (2020); Joshi et al. (2020); Han et al. (2022) shows that logical reasoning remains challenging for language models.Furthermore, Wu et al. (2023); Lanham et al. (2023) showed that LLMs maybe retrieving or reciting previously seen facts and steps, instead of actually reasoning.Liu et al. (2023b) shows that while Chat-GPT and GPT-4 generally perform well on some benchmarks, their performance noticeably diminishes when faced with new or out-of-distribution datasets.</p>
<p>To better understand the progress of logical reasoning ability in the current language model era, we first provide a concise survey of its role within current language models.Based on the insights gathered through the survey, we assembled a logical reasoning benchmark termed as LogiGLUE.Subsequently, we trained a model on this benchmark by utilizing diverse training strategies; our contributions are summarized below.</p>
<p>Concise Survey.We provide a brief survey of the recent development of logical reasoning using natural language (see Figure 1).First we discuss three types of logical reasoning.Then we focus on the relevant benchmarks and the methodologies for applying LMs to logical reasoning tasks.</p>
<p>LogiGLUE.One result of this survey is a benchmark for logical reasoning (LogiGLUE), with the aim to facilitate a consistent progress of logical reasoning in NLP.The importance for the LogiGLUE benchmark arises from several critical considerations.First, it encompasses diverse logical reasoning tasks and generalization evaluation, ensuring a comprehensive assessment of how a model performs across varied logical paradigms.Second, the unique format of each dataset within LogiGLUE simplifies both training and evaluation processes, facilitating swift integration into research workflows.Lastly, researchers can easily compare with established baselines, and the LogiGLUE offers the flexibility to seamlessly integrate new datasets in the future, ensuring its lasting relevance in logical reasoning evaluation.</p>
<p>LogiT5.Drawing inspiration from recent successes in multi-task learning and instruction-finetuned models, we trained seq2seq models, specifically Flan-T5 (Chowdhery et al., 2022), using multi-task learning on LogiGLUE's in-domain data.The resulting model, named LogiT5, demonstrated effective generalization on out-of-domain data.</p>
<p>A Concise Survey of Logical Reasoning</p>
<p>in NLP: Types of Reasoning, Datasets and Language Models Approach</p>
<p>The advent of large language models has been transformative for the AI community; prompting many to speculate that we are on the cusp of achieving general artificial intelligence (GAI).Yet, as astounding as their capabilities are, these models grapple with numerous challenges, particularly with logical reasoning (Han et al., 2022;Valmeekam et al., 2023Valmeekam et al., , 2022;;Guan et al., 2023).</p>
<p>Recognizing the significance of this, our survey aims to provide a timely comprehensive overview of advancements in logical reasoning within the context of language models, elucidating their performance, limitations, and the obstacles that remain, which casts a vision for future research directions.While other surveys have touched upon the broader theme of logical reasoning using natural language (Helwe et al., 2022;Yu et al., 2023;Yang et al., 2023), our survey has led us to propose a comprehensive bnechmark collection, and include a systematic review of techniques to adopt LLMs for logical reasoning tasks.More importantly, we categorize different ways of using LMs on logical reasoning tasks, highlighting the intricacies and challenges faced by models for such tasks.</p>
<p>Three Types of Logical Reasoning</p>
<p>Deductive Reasoning.In this predominant form of reasoning, we start with a set of premises which can be facts or rules, and derive a specific conclusion based on the available premises with a valid logical derivation path.In short, deductive reasoning derives specific conclusion(s) from generic observation(s) (Byrne et al., 2019).There are two characteristics related to a deductive reasoning system, validity and soundness.A conclusion is valid if and only if it is fully supported by the premises irrespective of the factuality of the premises.A conclusion is sound if and only if it is valid and the premises are true.For example in Figure 2, the conclusion is valid but it is not sound because it is not true that "All kids love animals."Most of the synthetic deductive reasoning datasets such as RuleTaker (Clark et al., 2020) has valid conclusions, but may not be sound as the rules in the premises are often synthetically generated and may be untrue in the real world.Datasets such as PrOn-toQA (Saparov and He, 2023a) offer a broader view, by sourcing the premise rules from a true, a false and a fictional ontology.</p>
<p>Inductive Reasoning.For inductive reasoning, one starts with a set of observations, and derives a general conclusion that is merely true, but not certain (Heit, 2007;Sauce and Matzel, 2017).In contrast to deductive reasoning, inductive reasoning is a bottom-up reasoning process which starts from specific observations and derives a generic conclusion.Many Knowledge graph completion task requires inductive reasoning such as WN18RR2 .</p>
<p>To apply inductive reasoning, one usually relies on a large number of observations (both positive and negative in support or against an induced rule).</p>
<p>Since large language models are pretrained on large amount of free-text, it learns several generic patterns or conclusions, therefore reasoning inductively (even if the rules may not be represented symbolically or a human-readable fashion) (Han et al., 2023).In general, commonsense reasoning tasks in NLP require both inductive and deductive reasoning.</p>
<p>Abductive Reasoning.Abductive reasoning typically begins with an incomplete set of observations and proceeds to derive most likely explanations for the observations to be true (Paul, 1993;Hobbs et al., 1993).Similar to inductive reasoning, this also involves uncertainty, as there can be different explanations.Compared to inductive reasoning, the deductive reasoning is a process from known facts or rules to derive a new conclusion, while abductive reasoning is from an observation to "guess" what can be the reason to cause the observation.It is used more often in our daily decision-making, such as medical diagnoses based on a set of incomplete symptoms.</p>
<p>In previous paragraphs, we mentioned how both inductive and abductive reasoning inherently encompass uncertainty.In fact, deductive reason- ing can also operate within the realm of uncertainty (De Raedt et al., 2007;Richardson and Domingos, 2006;Lee and Wang, 2016;Bach et al., 2017;Lee and Luo, 2018).Such reasoning paradigm uses "soft rules" to indicate the likelihood of a rule being true rather than its absolute truth.Consequently, conclusions derived may carry probabilistic true/false values.Reasoning under uncertainty is particularly useful because the real world is inherently unpredictable and full of unknown variables.While many datasets operate under the assumption that rules are unequivocally true, Rulebert (Saeed et al., 2021) deviates by attributing weight values to each rule.</p>
<p>Logical Reasoning Tasks and Datasets</p>
<p>We discuss the task and datasets in terms of format of the tasks and how they are created.</p>
<p>Four Types of Tasks</p>
<p>Multiple Choice Question Answering (MCQA).In the MCQA task, the given inputs are a paragraph which forms a context, a question, and a list of answer candidates (typically four choices).The goal is to predict which candidate is (most likely) correct.All datasets are pure-text (Yu et al., 2019;Liu et al., 2020) 3 .</p>
<p>Free Form Question Answering.Unlike MCQA, where a set of answer choices are given, freeform QA only has a context and a question, and the answer to the question can be any format, including but not limited to a single word, a list of words, and a number (Weston et al., 2015b;Banerjee et al., 2020).</p>
<p>Fact Checking.In fact verification, given a context and a fact, the goal is to classify the binary truth value of the fact according to the given information (Clark et al., 2020;Saeed et al., 2021;He et al., 2021).</p>
<p>Natural language inference (NLI) NLI is the task of detecting inferential relationships between a premise and a hypothesis.For most NLI datasets, there are three relationships, entailment (the hypothesis follows or can be inferred by the premise), neutral (the truth of hypothesis is undetermined by the premise), and contradiction (the hypothesis contradicts the premise or some facts in the premise) (Tian et al., 2021a).</p>
<p>Dataset Creation Techniques</p>
<p>Human Annotation.Crowdsourcing is one of the major approaches to create datasets, such as for NLI tasks.The advantages of this methodology include a richer linguistic grammar and potentially increased task complexity.However, it comes with drawbacks.In addition to being a costintensive process, crowdsourced datasets tend to harbor biases (as highlighted in numerous previous studies (Yu et al., 2019)).These biases can be leveraged by neural models to artificially inflate accuracy scores.Furthermore, assembling a dataset for logical reasoning tasks demands a level of expertise that poses a significant challenge.</p>
<p>Extraction from Academic Challenge.It is hard for crowdsourcing workers to produce questions requiring complicated logical reasoning since such reasoning tasks require extensive training and practice.Fortunately, questions in some standardized tests are aligned with the goal of logical reasoning and can be utilized to create such datasets after some preprocessing (Yu et al., 2019;Liu et al., 2020).However, the domains of these examinations are limited and the dataset size is small.Synthetic Generation.Synthetic generation is more efficient to create large data than manually created ones (Luo et al., 2022b).There are two ways, simulation based (Weston et al., 2015b) and rule-based (Clark et al., 2020;Saeed et al., 2021;Banerjee et al., 2020).In rule based methods, logic programs (either written by humans or mined from knowledge graphs) are generated, and then implications are drawn by automatic theorem prover.Last, the rules and facts in the logic programs are converted into English form using natural language patterns.Synthetic generation has issues that the rules or facts do not have real-world meaning and the language could be simple.</p>
<p>Language Models for Logical Reasoning over Natural Language</p>
<p>Language models (LMs) have been actively studied these days for logical reasoning tasks.Dasgupta et al. (2022) demonstrates that large language mdoels (LLMs) show human-level abstract reasoning skill.Creswell et al. (2022) proposes a selection-inference pipeline that given a context and question, the model can firstly select which facts or rules given in the context are important to answer the question and decompose the question into step by step reasoning.Wei et al. (2022) demonstrates that language models have the capacity to engage in chain-of-thoughts (CoT) reasoning.This approach facilitates a step-by-step reasoning process that enhances the performance of the model in downstream tasks such as mathematical reasoning.In following section, we summarize the five prevalent trends in utilizing language models for logical reasoning over language.</p>
<p>Supervised Finetuning</p>
<p>Fine-tuning a language model on the downstream tasks has been a standard way to teach a model to perform a task.Such a paradigm has also been the prevalent method for logical reasoning tasks (Clark et al., 2020;Liu et al., 2021;Tian et al., 2021b;Saeed et al., 2021;Han et al., 2022;Chan et al., 2023).In general, such a method is usually applied to a moderate size of the language model such as BERT (Devlin et al., 2019b), GPT2 (Radford et al.), RoBERTa (Liu et al., 2019), and XL-Net (Yang et al., 2019).It has been shown that transformer based models perform better than the other types of neural models such as LSTM (Yu et al., 2019;Liu et al., 2020), probably because such pretrained models have a certain degree of commonsense and logical reasoning (Huang et al., 2019).This has been further proven in (Clark et al., 2020).They show that when every word in the passage is replaced by a random word resulting in no grammaticality, the performance of a transformerbased model dramatically decreases.In addition, the larger model performs better than smaller ones, indicating that the deeper a model is, the more complicated reasoning it can execute (He et al., 2021).</p>
<p>While the IID performance of a fine-tuned model can be nearly perfect, such model has poor generalization.For example, model can not generalize from lower depth to higher depth reasoning (Clark et al., 2020), from low level language diversity to high level diversity (Richardson and Sabharwal, 2021;Tafjord et al., 2021), from one domain to another domain (Banerjee et al., 2020).Such observations indicate that models might just learn the inductive pattern in the training data rather than the underline logical reasoning skill (Zhang et al., 2022).</p>
<p>Logical Reasoning Pretraining</p>
<p>The next word prediction or mask language modeling pretraining tasks allow the language models to learn the language syntax and semantic as well as the world knowledge, however, it does not guarantee a model to learn logical operations.Thus, researchers have been exploring logical-oriented pretraining tasks to teach a model of logical reasoning from large free data.APOLLO (Sanyal et al., 2022)</p>
<p>Proof Generation</p>
<p>Proof generation is found to be harder than answer generation (Saha et al., 2020;Tafjord et al., 2021).However, models developed for a proof generation task have better performance on out-of-domain datasets or unseen depth reasoning (e.g., train on lower depth and test on higher depth).Kaiyu Yang and Chen (2022) introduce NLProofS, a novel method for generating step-by-step logically valid and relevant proofs given a set of supporting facts and hypothesis.In their proposed method, they employ a prover which generates candidate proofs step-by-step, a verifier to measure the validity of generated proof steps to avoid the prover from hallucinating proof steps, and an algorithm for retrieving the entire proof with highest validity score by aggregating proof step scores.ProofWriter (Tafjord et al., 2021) proposed two ways to generate proofs based on T5 models.The first one is to predict the sequence of proof in one output; the second one is to iteratively generate a proof and specifically, predict one intermediate conclusion and combine it with the given facts and rules as a new input to predict the following conclusion, and repeat this process until no new conclusion is predicted.</p>
<p>CoT Knowledge Distillation</p>
<p>The previous approach relies on the proof annotations in the datasets, however, in many cases, the dataset does not come with the proof.It is shown that large language model (LLM) can generate step-by-step reasoning (similar as the proof) (Saparov and He, 2023a;Liu et al., 2023c).Namgyu Ho (2022) propose Fine-tune-CoT (i.e.chain-ofthought (Wei et al., 2022)) approach which involves three key steps.In the first step, a large teacher model is prompted to address intricate queries, generating multi-step reasoning explanations.These explanations are then filtered based on the accuracy of the final prediction.In the second step, a reasoning sample is constructed, incorporating the question, rationale, and answer, thereby forming a comprehensive prompt and multi-step solution.This collection of carefully curated reasoning samples is leveraged to fine-tune a compact student model, imbuing it with the ability to engage in reasoning tasks.Nonetheless, LLMs encounter difficulties in planning proofs, occasionally making wrong selections when presented with multiple valid choices.This challenge leads to proofs that are not fully developed and consequently produces inaccurate responses.</p>
<p>Neural Symbolic</p>
<p>Recent advancements in pre-trained language models have demonstrated impressive reasoning abilities using explanations or "chain-of-thought" for in-context learning.Conversely, reasoning tasks are considered more straightforward for symbolic programming.A promising way is to use LLM to translate a natural language input into a symbolic program which can be consumed by a symbolic solver.Such a paradigm has been shown to effectively avoid unfaithfulness of LLM (Pan et al., 2023).Hanlin Zhang1 (2022) employs LLMS as Logic Programmers (LMLP) to learn logic rules and examples and reason over knowledge bases (KBs) using Prolog's backward chaining algorithm.They show that LMLP outperforms CoT in deductive reasoning settings, achieving over 25% higher accuracy on length generalization benchmarks, even with fewer parameters.Pan et al. (2023) propose Logic-LM to handle deductive reasoning, first-order logic, and constraint programming tasks.They leverage GPT-3 and in-context learning (providing a few examples) to translate a natural language input to a formal language formulation that can be executed by symbolic engines.They also show that the error messages of the symbolic engines can refine the output of an LLM.Such a paradigm has been investigated for addressing other challenges, wherein LLMs act as planners, and external tools are utilized to execute the plan (Lu et al., 2023;Sumers et al., 2023;Paranjape et al., 2023;Guan et al., 2023;Schick et al., 2023).</p>
<p>Survey Summary</p>
<p>The survey delineates how current datasets address three types of logical reasoning distributed across four task formats.Additionally, the curation process of a dataset can influence its inherent difficulty level.We've also identified five approaches for utilizing LLMs in addressing these reasoning tasks.This structured insight serves as a foundation for future research, offering a roadmap to optimize model performance and curation methodologies.</p>
<p>In the following section, we will present a logical reasoning benchmark, positioned alongside established benchmarks like SuperGlue (Wang et al., 2019), BigBench (Srivastava et al., 2023), and Unicorn (Lourie et al., 2021), all aimed at exhaustively gauging system capabilities.</p>
<p>LogiGLUE: General Logical Reasoning Benchmark</p>
<p>As mentioned the introduction, the reasoning ability of language models as assessed by various studies seems to differ.One plausible explanation for this variance is the inconsistency in the benchmarks used or differences in task formats, leading to performance disparities.To rectify this, our goal is to offer a standardized testbed.It becomes imperative to meticulously formulate our selection criteria to create a testbed that evaluates a system's logical reasoning capabilities.Guiding our dataset choice are two primary principles outlined in §3.1.These endeavors have led to the formation of a diverse and comprehensive logical reasoning benchmark, which we've named LogiGLUE ( §3.2).</p>
<p>Principle of Collecting LogiGLUE</p>
<p>Numerous logical reasoning datasets have been accessible since 2015, such as bAbi (Weston et al., 2015b).Our selection process for including a dataset in LogiGLUE is primarily driven by principles of diversity and generalization (Gokhale et al., 2022).</p>
<p>Diversity.There are two aspects to Diversity.First aspect concerns the types of reasoning in the dataset.We ensure that our coverage encompasses three main reasoning types, which collectively represent the full spectrum of logical reasoning.These three categories have been previously discussed.</p>
<p>The second aspects concerns the level of difficulty, with datasets ranging from easy to hard.Our experimental results indicate a varied model performance across different datasets -excelling in some, delivering average results on others, and struggling significantly on a few.We discovered a strong correlation between the complexity of a dataset and the methodology employed in its creation.Datasets built using simple templates and basic rule structures tend to be easier.In contrast, those with more sophisticated rules and uncertain elements are relatively more challenging.However, the most difficult datasets are those meticulously crafted by human hands.</p>
<p>Generalization.We also consider the axis of generalization, which aims to quantify (or assess) whether a model trained on the logical reasoning tasks can genuinely acquire reasoning skills.Previous studies have found that the superior performance of a fine-tuned language model primarily stems from learning the patterns exhibited within the dataset, which unfortunately often leads to poor generalization to other datasets.Consequently, the model's performance tends to be overestimated due to the identical and independently distributed (IID) nature of the testing data.To counteract this, LogiGLUE includes an out-of-domain testing set that also encompasses the three types of reasoning.</p>
<p>The out-of-domain testing set is readily adaptable to incorporate future logical reasoning tasks.</p>
<p>Excluded Datasets.Lexicographically speaking, reasoning is defined as "the process of forming conclusions, judgments, or inferences from facts or premises."4Reasoning is usually associated with an entailment relation, where given a premises, the truth value of a hypothesis depends on if the latter is entailed by the premise or not.</p>
<p>There are many datasets that require reasoning which we decided to exclude from the scope of this work.This includes some well-known NLI datasets, such as, SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018).These datasets use many linguistic forms, unstated background knowledge, and sometimes unsupported inference steps (Clark et al., 2020).We also exclude datasets where reasoning with external domain knowledge is required since for such tasks, retrieving the external knowledge is essential and it is hard to diagnose whether the noisy retrieved knowledge affects systems or systems lack of logical reasoning capacity.This includes QuAIL (Rogers et al., 2020), WSC (Levesque et al., 2012), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019).Commonsense reasoning datasets are not covered in this survey either since they focus on solving a task using commonsense knowledge (Sap et al., 2020) and thus it is more important to acquire the commonsense knowledge rather than to do logical reasoning.Other datasets that we exclude are ones that require logical reasoning but are not presented in the natural language form such as logical entailment (Evans et al., 2018), NeuroSAT (Selsam et al., 2019), and LTL (Hahn et al., 2020).</p>
<p>Statistic of LogiGLUE</p>
<p>A suite of natural language logical reasoning benchmarks with 10 in-domain and 12 out-domain datasets that cover different types of logical reasoning.In addition, LogiGLUE includes three task formats, multiple choice question answer (MCQA), natural language inference (NLI), and fact verification (FV)/ fact checking (FC).Table 1 shows the statistics.</p>
<p>Unique Format.There are many existing practicing for standardizing different datasets into a consistent format (Mishra et al., 2022;Lourie et al., 2021), such as transforming all tasks to question answering (McCann et al., 2018) or NLI (Poliak et al., 2018) styles.Through our model analysis, it's evident that certain models can only manage specific reasoning tasks.For instance, classification models are commonly used for NLI and MCQA tasks, where the number of classification heads matches the number of choices (like 3 for NLI and 4 for MCQA).Yet, these models struggle when confronted with free-form question answering, thus limiting their versatility.Hence, to develop a model adept at logical reasoning regardless of task structure, we convert them into a singular format.An added advantage of this standardized format is that it ensures consistency in the input, ruling out performance disparities arising from different inputs.Every dataset is then adapted to this specific format.</p>
<p>In the case of MCQA/FV/NLI tasks, each instance encompasses a context, a question, and potential answer options.Conversely, FF tasks don't present any answer choices.The correct answer is the expected output for every instance.For FV tasks, we use the statement as the question with true/false as potential answers.In NLI tasks, the options include natural, contradictory, and entailment.</p>
<p>Experiments and Results</p>
<p>We selected Flan-T5-large (Chowdhery et al., 2022) as our base model for training due to two pivotal reasons.Firstly, Flan-T5 stands as an instructionfine-tuned iteration of T5, exhibiting enhanced performance when compared to its peers.Secondly, Flan-T5's manageable size renders it to be trainable on a machine that is conducive to an academic setting.In the following, we present results that encompass both quantitative and qualitative aspects.</p>
<p>In-Domain Performance</p>
<p>Single Task Fine-tuning</p>
<p>We fine-tune the Flan-T5-large on each individual dataset and the result is presented in Table 2.We draw some interesting observations.It is apparent that the model exhibits superior performance when operating on synthetic data compared to handcrafted alternatives.In support of this, the datasets that garnered the top 5 performances are predominantly synthetic.This trend holds even when considering the ANLI dataset, which, despite having a more substantial training set than its synthetic counterparts, yielded inferior results.Moreover, we ventured to explore if the model displayed a predilection for one form of reasoning over another.Preliminary insights suggest a potential preference towards abductive reasoning in comparison to deductive reasoning, as evidenced in the disparity in performance between the αNLI and ANLI datasets -both of which are similar in terms of training size and are hand-crafted.This, however, is a mild observation and warrants further exploration to derive a conclusive statement.For instance, our statistical analysis revealed that the average context length for ANLI is 105, whereas for αNLI, it is 66, potentially leading to varying degrees of difficulty.</p>
<p>Multitask Fine-tuning</p>
<p>We fine-tune the Flan-T5 on all in-domain datasets utilizing a weighted sampling technique to accommodate for the unbalanced size of the training datasets.We find that this sampling is better than random sampling and the comparison is given in the Appendix.We termed this model as LogiT5.</p>
<p>One benefit of multi-task training compared to the single task training is that the low resources data can benefit from other tasks (Parmar et al., 2022;Luo et al., 2022a).From Table 2, it is apparent that the multi-task training model holds a significant advantage when dealing with tasks with small training set.It showcases higher proficiency compared to its single-task counterpart, notably performing better by 5% and 8% on the αARCT and FOLIO tasks, respectively.These datasets, characterized by their smaller training size (limited to 1/2 K training samples), benefited notably from the multi-task training approach.Contrastingly, the tasks with large trainig set did not reap any benefits from multi-task training, such as αNLI and ANLI datasets.A potential explanation for this could be the substantial training set already facilitates optimum learning for the model, rendering the multitask training approach redundant.This observation underlines a critical limitation in leveraging multitask training when the individual training datasets are already sufficiently large.</p>
<p>Fine-tuned LogiT5 on Single Dataset</p>
<p>Here, we further fine-tune LogiT5 on each dataset.However, upon analyzing the performance displayed in Table 2, we did not observe any notable advantages from this additional fine-tuning even though small margin gains are achieved.This suggests that LogiT5 has likely already learned the majority of knowledge from these tasks.</p>
<p>Out-of-Domain Generation</p>
<p>When we study the out-of-domain generalization, we compare three models, Flan-T5, LogiT5, and LLama-2 (7B) (Touvron et al., 2023).In addition, for Llama-2, we also study the chain-of-thought prompting (Wei et al., 2022).Here, we evaluate the model's zero-shot capabilities rather than its fewshot in-context learning performance (Luo et al., 2023).Investigating the latter will be reserved for future research.More specifically, we add a prompt "let's think step by step" after the question.However, by our results, we do not see the advantage of CoT prompting, probably because the model already generates the reasoning even without such a prompt.Evaluating the LLama-2 answer poses a challenge since the output is usually a free form and not use the exact answer option.On the other hand Flan-T5 generate answer in a more structure way that is easier for evaluation, probably because Flan-T5 is already trained on instruction fine-tuned data  which are already in a structured templates.The preliminary results of LLama-2 were poor.Upon manually reviewing the predictions, we observed that LLama-2 occasionally produces synonyms of the ground truth.To address this, we employed ConceptNET (Speer et al., 2017) to identify synonyms and verify if the prediction aligns with any of them, a strategy akin to the one explored in (Luo et al., 2021).Furthermore, on the babi dataset, we have seen that sometimes the llama model ignores the input text and generates answer based on its pretrained knowledge.This is similar to the findings revealed in Varshney et al. (2023).</p>
<p>Flan-T5 CoT, 3K CoT, 6K CoT, 15K 0.37 0.38 0.37 0.41</p>
<p>CoT Distillation</p>
<p>As shown by previous work (Namgyu Ho, 2022), distill the chain-of-thoughts from a large model to a small student model can boost the performance of the student model.We apply such a CoT finetun-ing strategy and conduct experiments on LogiQA, identified as the most challenging task, by distilling the CoT from LLama-7B to Flan-T5.Initially, we generated a single answer for each question, retaining only the samples where the predicted answer was correct, resulting in approximately 3K valid samples.Alternatively, we created 10 answers for each question and preserved the samples with at least one correct predicted answer, which generated a unique set of 6K questions.It is worth noting that some questions offered multiple correct reasoning paths.In such cases, we either opted for a singular path or utilized all available paths, the latter approach amassing a total of 15K training samples.</p>
<p>With the CoT fine-tuning, we observe that the finetuning takes longer time and a larger learning rate in the beginning is helpful.Thus, instead of using 1e-4 as the learning rate, we use 3e-4.we train the model with 40 epochs.We do see that the model performance increase when the number of epoch increase.</p>
<p>Following this, we trained the Flan-T5 model utilizing datasets consisting of 3K, 6K, and 15K samples, derived from the generated CoT,], with the results delineated in Table 4.Our findings indicate that the training with 3K and 6K samples did not enhance the CoT's fine-tuning efficacy.However, an increased dataset size of 15K samples facilitated a 4% improvement in performance, suggesting that CoT distillation becomes more beneficial with a larger volume of data.</p>
<p>Conclusion</p>
<p>In this study, we concentrate our efforts on a crucial area of research: logical reasoning over natural language.Initially, we offer a survey to provide a thorough comprehension of this domain, emphasizing the role of large language models in addressing this demanding task.Following this, we assemble a benchmark for logical reasoning named LogiGLUE, set to be publicly available to aid forthcoming research.Finally, we refine a language model utilizing LogiGLUE, demonstrating encouraging results across both in-domain and outof-domain datasets.</p>
<p>Figure 1 :
1
Figure 1: Logical Reasoning Survey: Datasets and Language Model Application.</p>
<p>Figure 2 :
2
Figure 2: Examples (top) of three types of logical reasoning and explanations (bottom) correlating each example with its respective reasoning type.</p>
<p>(Jiao et al., 2023))he second pretraining task is entailment classification which aims to classify if there is an entailment relationship within a masked sentence or not.MERIt(Fangkai Jiao, 2022)proposes a meta-path-guided pretraining task to teach a model to learn logical reasoning by selfsupervised learning.They construct the training data by converting any document into a graph with entities as the node and the relation between the entities as edges.Then, given a pair of entities, the positive candidates are the sentences that connect this pair of entities, and the negative candidates are obtained by data augmentation.Such training data allows the model trained by contrastive learning manner to identify the positive sentence from the negative sentences.MERIt +(Jiao et al., 2023)combines MERIt with the autoregression training objective: rather than using contrastive learning, MERIt + optimizes the probability of positive candidate sentences.</p>
<p>improves the logical reasoning of a model by two pre-training tasks.The first pretraining task is selective mask language modeling (MLM).Unlike the naive MLM which randomly masks the words, s-MLM selects and masks the logical words (de-fined</p>
<p>Table 1 :
1
Statistics of In-domain (IID) and out-of-domain (OOD) datasets of LogiGLUE benchmark.
DatasetTrain size Dev size Test size Synthetic Task Type Reasoning TypeIn-domain datasetsαARCT 20192420632888✗MCQAAbductiveαNLI 2019169,654-1532✗NLIAbductiveCLUTTR-Robust 201910,100-144✓FFInductiveAbductionRule-Animal 201923,1003,3006,600✓FFAbductiveANLI 2020162,8653,2003,200✗NLIDeductiveLogiQA 20217,376651651✗MCQAMixedLogicNLI 2021b16,0002,0002000✓NLIDeductiveProofWriter 202169,81410,15820,058✓FVDeductiveRulebert-Union 202156,0004,6669,334✓FVDeductiveFOLIO 20221004204227✗FVDeductiveOut-of-domain datasetsbAbi 2015a--5000✓FFInductivebAbi 2015a--5000✓FFDeductiveCLUTTR-Systematic 2019--10100✓FFInductiveAbductionRule-person 2019--4,864✓FFAbductiveReClor 2020--500✗MCQAMixedBird-Electricity 2021--5270✗FVDeductiveNatlLang 2021--8,008✗FVDeductiveWinologic 2021--562✗FVDeductiveWaNLI 2022--5000✓NLIDeductiveRulebert-Union 2021--5000✓FVDeductiveBigBench 2022--1300✗FFDeductiveBigBench 2022--32✗FFInductiveLogiQA 2.0 2023a--3238✗NLIDeductivePrOntoQA 2023b--200✗MCQADeductive</p>
<p>Table 2 :
2
Three training strategies for models and the performance on In-domain Dataset.
DatasetFlan-T5 LogiT5 LLAMA LLAMA-CoTbAbi(induction) 2015a59.4413.1233.4627.06bAbi(deduction) 2015a7.3616.7458.7667.36CLUTTR-Systematic 201915.4310038.8240.31AbductionRule-person 201900.0095.9731.3143.19ReClor 202039.8046.8039.8039.00Bird-Electricity 202141.2965.1450.0045.73NatlLang 202156.1670.2449.5250.43Winologic 202168.8662.1048.9348.39WaNLI 202250.9662.2832.8015.54Rulebert-Union 202127.2962.9287.4061.50BigBench(logical-deduction) 202249.8538.0022.3823.23BigBench(logical args) 202259.3840.6240.6225.00LogiQA 2.0 2023a52.6655.0050.7450.46PrOntoQA 2023b6.5056.5070.0029.50Average38.6354.5043.6337.78</p>
<p>Table 3 :
3
Performance on out-domain Datasets.</p>
<p>Table 4 :
4
Performance of Flan-T5 trained with CoT finetuning on the LogiQA dataset.First column is without using CoT and trained on the given training set of LogiQA.</p>
<p>https://news.mit.edu/2021/hawkings-black-holetheorem-confirm-0701
Here, we exclude this task since we are more interested in natural language input. In this paper, we do not discuss about knowledge graph completion tasks since most of them are not in natural language forms.
ReClor and LogiQA sources the datasets from real examination questions that may involve images or charts. But they remove such questions and retain only those which are self-contained and answerable from the provided text.
https://www.dictionary.com/browse/reasoning</p>
<p>Hinge-loss markov random fields and probabilistic soft logic. H Stephen, Matthias Bach, Bert Broecheler, Lise Huang, Getoor, J. Mach. Learn. Res. 1812017</p>
<p>Pratyay Banerjee, Chitta Baral, Man Luo, Arindam Mitra, Kuntal Pal, Tran C Son, Neeraj Varshney, arXiv:2012.09938Can transformers reason about effects of actions. 2020</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen-Tau Downey, Yejin Yih, Choi, International Conference on Learning Representations. 2019</p>
<p>A large annotated corpus for learning natural language inference. Samuel Bowman, Gabor Angeli, Christopher Potts, Christopher D Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Human reasoning: The psychology of deduction. Jonathan Ruth Mj Byrne, St Bt, Stephen E Evans, Newstead, 2019Psychology Press</p>
<p>Chunkit Chan, Xin Liu, Tsz Ho Chan, Jiayang Cheng, Yangqiu Song, Ginny Wong, Simon See, arXiv:2309.08303Self-consistent narrative prompts on abductive natural language inference. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537IJCAI. 2020</p>
<p>The birth of prolog. Alain Colmerauer, Philippe Roussel, History of programming languages-II. 1996</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv:2207.07051Language models show human-like content effects on reasoning. 2022arXiv preprint</p>
<p>Problog: A probabilistic prolog and its application in link discovery. Luc De Raedt, Angelika Kimmig, Hannu Toivonen, Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI'07. the 20th International Joint Conference on Artifical Intelligence, IJCAI'07San Francisco, CA, USAMorgan Kaufmann Publishers Inc2007</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT. 2019a</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423NAACL. Minneapolis, Minnesota. ACL2019b</p>
<p>Can neural networks understand logical entailment. Richard Evans, David Saxton, David Amos, 2018In ICLR</p>
<p>Merit: Meta-path guided contrastive learning for logical reasoning. Xuemeng Song, Liqiang Nie, Fangkai Jiao, Yangyang Guo, arXiv:2203.003572022arXiv preprint</p>
<p>Generalized but not robust? comparing the effects of data modification methods on out-of-domain generalization and adversarial robustness. Tejas Gokhale, Swaroop Mishra, Man Luo, Findings of the Association for Computational Linguistics: ACL 2022. 2022Bhavdeep Sachdeva, and Chitta Baral</p>
<p>Leveraging pretrained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2305.149092023arXiv preprint</p>
<p>Teaching temporal logics to neural networks. Christopher Hahn, Frederik Schmitt, Jens U Kreber, Markus Norman Rabe, Bernd Finkbeiner, 2020In ICLR</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Folio: Natural language reasoning with firstorder logic. 2022arXiv preprint</p>
<p>Inductive reasoning in humans and large language models. Simon Jerome, Han , Keith J Ransom, Andrew Perfors, Charles Kemp, Cognitive Systems Research. 1011552023</p>
<p>The impact of symbolic representations on in-context learning for few-shot reasoning. arXiv:2212.08686Li Erran Li3 Eric Xing Hanlin Zhang1, Yi-Fan Zhang22022arXiv preprint</p>
<p>WinoLogic: A zero-shot logic-based diagnostic dataset for Winograd Schema Challenge. Weinan He, Canming Huang, Yongmei Liu, Xiaodan Zhu, EMNLP. 2021</p>
<p>Logitorch: A pytorch-based library for logical reasoning on natural language. Evan Heit, 10.1017/CBO9780511619304.002Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingSystem Demonstrations2007. 2022Chadi Helwe, Chloé Clavel, and Fabian Suchanek</p>
<p>Interpretation as abduction. Artificial intelligence. Jerry R Hobbs, Mark E Stickel, Douglas E Appelt, Paul Martin, 199363</p>
<p>Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/D19-1243EMNLP-IJCNLP. 2019</p>
<p>Abduction-based explanations for machine learning models. Alexey Ignatiev, Nina Narodytska, Joao Marques-Silva, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Logicllm: Exploring self-supervised logic-enhanced training for large language models. Fangkai Jiao, Zhiyang Teng, Shafiq Joty, Bosheng Ding, Aixin Sun, Zhengyuan Liu, Nancy F Chen, arXiv:2305.137182023arXiv preprint</p>
<p>TaxiNLI: Taking a ride up the NLU hill. Pratik Joshi, Somak Aditya, Aalok Sathe, Monojit Choudhury, 10.18653/v1/2020.conll-1.4CoNLL. 2020</p>
<p>Generating natural language proofs with verifier-guided search. Jia Deng, Kaiyu Yang, Danqi Chen, arXiv:2205.124432022arXiv preprint</p>
<p>Measuring faithfulness in chainof-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam Mccandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, ; Samuel, R Bowman, Ethan Perez, 10.48550/arXiv.2307.13702CoRR, abs/2307.13702Jan Brauner,. 2023Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan</p>
<p>Strong equivalence for lpmln programs. Joohyung Lee, Man Luo, 35th International Conference on Logic Programming. 2018. 2019</p>
<p>Weighted rules under the stable model semantics. Joohyung Lee, Yi Wang, 2016In KRR</p>
<p>The winograd schema challenge. Hector Levesque, Ernest Davis, Leora Morgenstern, 2012In KRR</p>
<p>Answer set programming. Vladimir Lifschitz, 2019SpringerBerlin</p>
<p>Reasoning over paragraph effects in situations. Kevin Lin, Oyvind Tafjord, Peter Clark, Matt Gardner, MRQA. 2019</p>
<p>Wanli: Worker and ai collaboration for natural language inference dataset creation. Alisa Liu, Swabha Swayamdipta, Noah A Smith, Yejin Choi, arXiv:2201.059552022arXiv preprint</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, 10.1109/TASLP.2023.3293046IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312023a</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023barXiv preprint</p>
<p>Logicot: Logical chain-of-thought instruction-tuning data collection with gpt-4. Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, Yue Zhang, arXiv:2305.121472023carXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, 10.24963/ijcai.2020/501IJCAI. 2020</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019</p>
<p>Nicholas Lourie, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, arXiv:2103.13009Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. 2021arXiv preprint</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, arXiv:2304.09842Chameleon: Plug-and-play compositional reasoning with large language models. 2023arXiv preprint</p>
<p>Choose your qa model wisely: A systematic study of generative and extractive readers for question answering. Man Luo, Kazuma Hashimoto, Semih Yavuz, Zhiwei Liu, Chitta Baral, Yingbo Zhou, Spa-NLP. 72022a. 2022</p>
<p>Akarshan Sajja, and Chitta Baral. 2021. 'just because you are right, doesn't mean i am wrong': Overcoming a bottleneck in development and evaluation of open-ended vqa tasks. Man Luo, Keyur Shailaja, Riley Sampat, Yankai Tallman, Manuha Zeng, Vancha, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</p>
<p>Biotabqa: Instruction learning for biomedical table question answering. Man Luo, Sharad Saxena, Swaroop Mishra, CEUR Workshop Proceedings. CEUR-WS2022b3180Mihir Parmar, and Chitta Baral</p>
<p>Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, Vincent Y Zhao, arXiv:2305.14128Dr. icl: Demonstration-retrieved in-context learning. 2023arXiv preprint</p>
<p>The natural language decathlon: Multitask learning as question answering. Bryan Mccann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher, arXiv:1806.087302018arXiv preprint</p>
<p>Artificial intelligence, logic and formalizing common sense. John Mccarthy, Philosophical logic and artificial intelligence. Springer1989</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Se-Young Yun, Namgyu Ho, Laura Schmid, arXiv:2212.10071Large language models are reasoning teachers. 2022arXiv preprint</p>
<p>Adversarial nli: A new benchmark for natural language understanding. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Probing neural network comprehension of natural language arguments. Timothy Niven, Hung-Yu Kao, ACL. 2019</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , arXiv:2303.09014Automatic multistep reasoning and tool-use for large language models. Art2023arXiv preprint</p>
<p>Murad Mohammad, and Chitta Baral. 2022. In-boxbart: Get instructions into biomedical multitask learning. Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Findings of the Association for Computational Linguistics: NAACL 2022. </p>
<p>Approaches to abductive reasoning: an overview. Gabriele Paul, Artificial intelligence review. 721993</p>
<p>Collecting diverse natural language inference problems for sentence representation evaluation. Adam Poliak, Aparajita Haldar, Rachel Rudinger, J Edward Hu, Ellie Pavlick, Aaron Steven White, Benjamin Van Durme, 10.18653/v1/D18-1007Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, </p>
<p>Pushing the limits of rule reasoning in transformers through natural language satisfiability. Kyle Richardson, Ashish Sabharwal, arXiv:2112.090542021</p>
<p>Markov logic networks. Matthew Richardson, Pedro Domingos, 10.1007/s10994-006-5833-1Mach. Learn. 621-22006</p>
<p>Getting closer to ai complete question answering: A set of prerequisite real tasks. Anna Rogers, Olga Kovaleva, Matthew Downey, Anna Rumshisky, AAAI. 202034</p>
<p>RuleBERT: Teaching soft rules to pre-trained lms. Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti, EMNLP. 2021</p>
<p>Prover: Proof generation for interpretable reasoning over rules. Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal, EMNLP. 2020</p>
<p>Apollo: A simple approach for adaptive pretraining of language models for logical reasoning. Soumya Sanyal, Yichong Xu, Shuohang Wang, Ziyi Yang, Reid Pryzant, Wenhao Yu, Chenguang Zhu, Xiang Ren, arXiv:2212.092822022arXiv preprint</p>
<p>Introductory tutorial: Commonsense reasoning for natural language processing. Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, Dan Roth, 2020. 202027</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Inductive reasoning. Encyclopedia of animal cognition and behavior. Bruno Sauce, Louis D Matzel, 20176</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Learning a sat solver from single-bit supervision. Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo De Moura, David L Dill, ICLR. 2019</p>
<p>Clutrr: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, EMNLP. 2019</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201731</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, Transactions on Machine Learning Research. 2023</p>
<p>Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023arXiv preprint</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings-ACL-IJCNLP. 2021</p>
<p>Quartz: An open-domain dataset of qualitative relation questions. Oyvind Tafjord, Matt Gardner, Kevin Lin, Peter Clark, arXiv:1909.035532019</p>
<p>Diagnosing the firstorder logical reasoning ability through LogicNLI. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, EMNLP. 2021a</p>
<p>Diagnosing the first-order logical reasoning ability through logicnli. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021b</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>On the planning abilities of large language models (a critical investigation with a proposed benchmark. Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, Subbarao Kambhampati, arXiv:2302.067062023arXiv preprint</p>
<p>Can nlp models correctly reason over contexts that break the common assumptions?. Neeraj Varshney, Mihir Parmar, Nisarg Patel, Divij Handa, Sayantan Sarkar, Man Luo, Chitta Baral, arXiv:2305.120962023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, NeurIPS. Curran Associates, Inc201730</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing. 201932</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, arXiv:1502.056982015aarXiv preprint</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, arXiv:1502.056982015b</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics20181</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim, 10.48550/arXiv.2307.02477CoRR, abs/2307.024772023</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, arXiv:2303.12023Xinya Du, Rui Mao, Jinjie Ni, and Erik Cambria. 2023. Logical reasoning over natural language as knowledge representation: A survey. Neurips, 201932arXiv preprintCarbonell</p>
<p>Fei Yu, Hongbo Zhang, Benyou Wang, arXiv:2303.14725Nature language reasoning, a survey. 2023arXiv preprint</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, ICLR. 2019</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. 2020</p>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den Broeck, arXiv:2205.11502On the paradox of learning to reason from data. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
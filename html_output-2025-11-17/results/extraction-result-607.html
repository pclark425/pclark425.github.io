<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-607 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-607</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-607</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-d1b21442aa4a6af708d64082d61f6e63e1d4cdf1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d1b21442aa4a6af708d64082d61f6e63e1d4cdf1" target="_blank">Sum-product networks: A new deep architecture</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> The question: what are the most general conditions under which the partition function is tractable?</p>
                <p><strong>Paper Abstract:</strong> The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are the most general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum-product networks (SPNs) and will present in this abstract.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e607.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e607.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sum-Product Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A directed acyclic computational graph with indicator leaves and internal sum and product nodes (weighted edges) that compactly represents the network polynomial of a probability distribution and permits exact, tractable computation of partition functions and marginals when completeness and consistency conditions hold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sum-Product Network (SPN) hybrid reasoning system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SPNs are rooted directed acyclic graphs whose leaves are indicator functions for variable assignments and whose internal nodes are sum and product units with non-negative weights on sum edges. Sum nodes implement mixtures (implicit discrete hidden variables) over their children; product nodes implement factorization / conjunction (compositional features). SPNs are arranged in alternating sum/product layers (or can be organized that way) and represent the network polynomial of a distribution; when the SPN is complete (children of a sum have same scope) and consistent (no variable appears negated in one child of a product and non-negated in another) it is valid and computes unnormalized probabilities, marginals, the partition function, and MPE (by replacing sums with maxes) in time linear in its size. Architecturally, SPNs combine symbolic structured representation (indicator leaves, explicit DAG encoding of algebraic sums and products / network polynomial) with procedural/parameterized components (learnable non-negative weights on sum edges, deep layered composition, gradient-based learning or EM/hard-EM).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>A structured symbolic/probabilistic representation: the network polynomial expressed by a DAG of algebraic sum and product operators over indicator functions (logical indicators [X = v]). Sum nodes correspond to implicit discrete hidden variables (mixture selectors); product nodes represent logical conjunction / factor composition. The representation can encode grammars (PCFGs), junction-tree style clique potentials and other symbolic probabilistic constructs via explicit scopes and indicator leaves.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Procedural / neural-style components: learnable non-negative edge weights on sum nodes trained by gradient descent (backpropagation) or EM (including hard EM). Deep layered architecture (tens of layers in experiments), max-sum substitution for MPE, and procedural algorithms for upward/downward passes to compute node values and derivatives (differentiation-based marginal computation). Use of online hard EM to avoid gradient diffusion is emphasized as a learning algorithmic strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight integration as a single differentiable (and EM-compatible) computation graph: the declarative algebraic structure (sums/products of indicators) is the computational substrate; the imperative component (weights, learning algorithm) is embedded as parameters on sum edges. Training is end-to-end on the SPN graph via backpropagation (computing ∂S/∂w and ∂S/∂S_i with an upward and downward pass) or EM interpreting sum nodes as marginalized hidden variables; hard EM replaces marginals with MPE (max nodes) to produce discrete update counts. Enforcing S(*)=1 (normalization) is done by renormalizing weights during training. Pruning of zero-weight edges yields a valid, compact model. For MPE inference sums are replaced by max in the upward pass and argmax selections done on the downward pass.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Exact, tractable inference (partition function, all marginals) in deep architectures where standard deep probabilistic models require approximate inference; compact representation enabling reuse of subcomponents and exponential savings versus hierarchical mixture models or junction trees in some cases; compositional feature hierarchies with probabilistic mixture semantics; ability to represent distributions with no conditional independences compactly (e.g., parity/even-ones distributions) via reused subcircuits; stable deep learning with many layers when using hard EM (avoids gradient diffusion); exact MPE computation (for decomposable/consistent SPNs) by substituting max operations; interpretable node semantics (sum nodes ↔ hidden mixture variables, product nodes ↔ factors/features) and the ability to compute node marginals and attribute responsibility via differentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Image completion (left-half or bottom-half completion) on Caltech-101 categories and Olivetti faces; additionally object-recognition evaluation (AUC of precision-recall) on Caltech-101 categories faces, motorbikes, and cars.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Image completion mean squared error (MSE) on completed pixels — LEFT half, Caltech (ALL): SPN MSE = 3475 (units: pixel-intensity-squared); DBM = 9043; DBN = 4778; PCA = 4234; NN = 4887. BOTTOM half, Caltech (ALL): SPN MSE = 3538; DBM = 9792; DBN = 4492; PCA = 4465; NN = 5505. Olivetti faces LEFT SPN MSE = 942, DBM = 1866, DBN = 2386, PCA = 1076, NN = 1527. Object recognition (AUC): SPN AUC: Faces = 0.99, Motorbikes = 0.99, Cars = 0.98.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>Neural-style baselines reported in paper: Deep Boltzmann Machine (DBM) and Deep Belief Network (DBN) — image completion MSEs as above (e.g., DBM LEFT Caltech(ALL) MSE = 9043; DBN LEFT Caltech(ALL) MSE = 4778); AUC for top-layer convolutional DBN (CDBN) reported lower than SPN (CDBN AUC: Faces = 0.95, Motorbikes = 0.81, Cars = 0.87).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Empirical evidence in the paper shows better generalization on held-out image completion tasks and higher AUCs in object-recognition experiments compared to DBNs/DBMs and simple baselines. The representational properties (reuse of subcomponents, composition of mixtures and factors) are argued to support compositional generalization and compact representation of distributions lacking conditional independences. The paper does not present formal out-of-distribution or systematic compositionality tests; claims of superior generalization are empirical (image completion and classification) and supported by experiment but not proven theoretically for OOD regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: SPNs provide clear node-level semantics (sum nodes ↔ implicit hidden discrete variables, product nodes ↔ composition of feature/subdistribution), enabling inspection of mixture components and computation of node marginals and posterior probabilities for hidden variables via differentiation. MPE/Viterbi-style assignments trace which sum children were selected, providing an interpretable discrete explanation for predictions. The DAG structure and scopes make it possible to map parts of the representation to interpretable sub-distributions (e.g., grammar productions when implementing PCFGs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Some empirical weaknesses noted: completed images can look 'blocky'; not all graphical models can be compactly represented as polynomial-size SPNs (some models still require exponential size); gradient-based learning and EM suffer from gradient diffusion in very deep SPNs (mitigated experimentally by hard EM); architecture design (choice of subsets and decompositions) requires heuristics or domain choices and remains an open design problem; continuous-variable nodes rely on tractable integrals (e.g., Gaussian choices) which may constrain modeling; SPNs as presented are still biologically unrealistic despite cortical analogies.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Yes — SPNs are grounded in the network polynomial formalism and algebraic distributive reorganization: completeness and consistency conditions ensure the SPN expansion equals the network polynomial and thus validity (Theorem 1). The paper frames tractability of the partition function as the representability of the network polynomial by a polynomial-size sum-product expression, and proves representational comparisons (SPNs strictly more general than some tractable graphical model classes). Decomposability/consistency notions and proofs for exactness of MPE under decomposability are given, and learning rationale (EM interpretation of sum nodes as marginalized hidden variables, and hard EM to avoid gradient diffusion) is provided as procedural/theoretical guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sum-product networks: A new deep architecture', 'publication_date_yy_mm': '2011-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e607.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e607.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCFG-as-SPN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Context-Free Grammar implemented as a decomposable Sum-Product Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper states that probabilistic context-free grammars (PCFGs) and statistical parsers can be implemented straightforwardly as decomposable SPNs by mapping non-terminal symbols to sum (or max) nodes and productions to product nodes, enabling chart-parser-like SPN representations of grammars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PCFG implemented as decomposable SPN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A PCFG maps naturally to a decomposable SPN: sum nodes represent non-terminals (mixtures over productions or parses), product nodes represent concatenation/production application (conjunction of subspans). The decomposability constraint ensures that product children have disjoint scopes (chart spans), matching the span decomposition in parsing. The resulting SPN is equivalent to a bounded-size chart parser encoded as a DAG where inference (marginals, MPE parses) can be computed by the SPN upward/downward passes.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Probabilistic Context-Free Grammar (PCFG) — a symbolic grammar with non-terminals, productions, and probabilistic weights over productions (discrete symbolic derivation structure).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Sum-Product Network machinery: learnable weights on sum nodes, upward/downward pass inference, EM or hard-EM training, max-substitution for MPE parse; effectively using SPN learning/inference algorithms (gradient descent, EM, hard EM) as procedural components.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Structural mapping: encode grammar non-terminals as sum nodes and productions as product nodes forming a decomposable SPN whose scopes correspond to span intervals. Training/inference uses the SPN imperative algorithms (exact upward/downward passes for marginals, EM/hard-EM for learning weights), turning grammar induction/parameter estimation into SPN parameter learning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables efficient exact inference and MPE parse computation through SPN algorithms; allows learning a bounded-size chart parser as a single SPN via end-to-end EM/backprop; supports reuse of subparses (shared subcircuits) leading to compact representations for grammars with overlapping substructures; can represent unrestricted probabilistic grammars with bounded recursion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Statistical parsing / PCFG inference (mentioned conceptually; no empirical parsing experiments reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Conceptual claim: SPN encoding of PCFGs allows reuse of subparses and bounded-size chart parsing representations, which can generalize via shared substructures; can represent grammars with bounded recursion; no empirical evaluation of generalization presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: the grammar's non-terminals and productions retain their symbolic interpretation in the SPN mapping, so parses and production responsibilities can be inspected via sum node marginals and MPE selections, providing human-readable parse structure explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Only discussed conceptually; practical limitations include requirement of decomposability for guaranteed MPE correctness and the need to bound chart/parser size to keep SPN polynomial-sized. No empirical results or complexity tradeoffs are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Mapping of PCFG parsing to decomposable SPN semantics with decomposability guaranteeing that MPE via max-substitution yields valid parses; leverages the SPN completeness/consistency/decomposability theoretical results to ensure correctness of parsing inference when encoded as an SPN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sum-product networks: A new deep architecture', 'publication_date_yy_mm': '2011-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A differential approach to inference in Bayesian networks <em>(Rating: 2)</em></li>
                <li>Learning arithmetic circuits <em>(Rating: 2)</em></li>
                <li>Learning efficient Markov networks <em>(Rating: 2)</em></li>
                <li>Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations <em>(Rating: 2)</em></li>
                <li>Hierarchical models of object recognition in cortex <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-607",
    "paper_id": "paper-d1b21442aa4a6af708d64082d61f6e63e1d4cdf1",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "SPN",
            "name_full": "Sum-Product Network",
            "brief_description": "A directed acyclic computational graph with indicator leaves and internal sum and product nodes (weighted edges) that compactly represents the network polynomial of a probability distribution and permits exact, tractable computation of partition functions and marginals when completeness and consistency conditions hold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Sum-Product Network (SPN) hybrid reasoning system",
            "system_description": "SPNs are rooted directed acyclic graphs whose leaves are indicator functions for variable assignments and whose internal nodes are sum and product units with non-negative weights on sum edges. Sum nodes implement mixtures (implicit discrete hidden variables) over their children; product nodes implement factorization / conjunction (compositional features). SPNs are arranged in alternating sum/product layers (or can be organized that way) and represent the network polynomial of a distribution; when the SPN is complete (children of a sum have same scope) and consistent (no variable appears negated in one child of a product and non-negated in another) it is valid and computes unnormalized probabilities, marginals, the partition function, and MPE (by replacing sums with maxes) in time linear in its size. Architecturally, SPNs combine symbolic structured representation (indicator leaves, explicit DAG encoding of algebraic sums and products / network polynomial) with procedural/parameterized components (learnable non-negative weights on sum edges, deep layered composition, gradient-based learning or EM/hard-EM).",
            "declarative_component": "A structured symbolic/probabilistic representation: the network polynomial expressed by a DAG of algebraic sum and product operators over indicator functions (logical indicators [X = v]). Sum nodes correspond to implicit discrete hidden variables (mixture selectors); product nodes represent logical conjunction / factor composition. The representation can encode grammars (PCFGs), junction-tree style clique potentials and other symbolic probabilistic constructs via explicit scopes and indicator leaves.",
            "imperative_component": "Procedural / neural-style components: learnable non-negative edge weights on sum nodes trained by gradient descent (backpropagation) or EM (including hard EM). Deep layered architecture (tens of layers in experiments), max-sum substitution for MPE, and procedural algorithms for upward/downward passes to compute node values and derivatives (differentiation-based marginal computation). Use of online hard EM to avoid gradient diffusion is emphasized as a learning algorithmic strategy.",
            "integration_method": "Tight integration as a single differentiable (and EM-compatible) computation graph: the declarative algebraic structure (sums/products of indicators) is the computational substrate; the imperative component (weights, learning algorithm) is embedded as parameters on sum edges. Training is end-to-end on the SPN graph via backpropagation (computing ∂S/∂w and ∂S/∂S_i with an upward and downward pass) or EM interpreting sum nodes as marginalized hidden variables; hard EM replaces marginals with MPE (max nodes) to produce discrete update counts. Enforcing S(*)=1 (normalization) is done by renormalizing weights during training. Pruning of zero-weight edges yields a valid, compact model. For MPE inference sums are replaced by max in the upward pass and argmax selections done on the downward pass.",
            "emergent_properties": "Exact, tractable inference (partition function, all marginals) in deep architectures where standard deep probabilistic models require approximate inference; compact representation enabling reuse of subcomponents and exponential savings versus hierarchical mixture models or junction trees in some cases; compositional feature hierarchies with probabilistic mixture semantics; ability to represent distributions with no conditional independences compactly (e.g., parity/even-ones distributions) via reused subcircuits; stable deep learning with many layers when using hard EM (avoids gradient diffusion); exact MPE computation (for decomposable/consistent SPNs) by substituting max operations; interpretable node semantics (sum nodes ↔ hidden mixture variables, product nodes ↔ factors/features) and the ability to compute node marginals and attribute responsibility via differentiation.",
            "task_or_benchmark": "Image completion (left-half or bottom-half completion) on Caltech-101 categories and Olivetti faces; additionally object-recognition evaluation (AUC of precision-recall) on Caltech-101 categories faces, motorbikes, and cars.",
            "hybrid_performance": "Image completion mean squared error (MSE) on completed pixels — LEFT half, Caltech (ALL): SPN MSE = 3475 (units: pixel-intensity-squared); DBM = 9043; DBN = 4778; PCA = 4234; NN = 4887. BOTTOM half, Caltech (ALL): SPN MSE = 3538; DBM = 9792; DBN = 4492; PCA = 4465; NN = 5505. Olivetti faces LEFT SPN MSE = 942, DBM = 1866, DBN = 2386, PCA = 1076, NN = 1527. Object recognition (AUC): SPN AUC: Faces = 0.99, Motorbikes = 0.99, Cars = 0.98.",
            "declarative_only_performance": null,
            "imperative_only_performance": "Neural-style baselines reported in paper: Deep Boltzmann Machine (DBM) and Deep Belief Network (DBN) — image completion MSEs as above (e.g., DBM LEFT Caltech(ALL) MSE = 9043; DBN LEFT Caltech(ALL) MSE = 4778); AUC for top-layer convolutional DBN (CDBN) reported lower than SPN (CDBN AUC: Faces = 0.95, Motorbikes = 0.81, Cars = 0.87).",
            "has_comparative_results": true,
            "generalization_properties": "Empirical evidence in the paper shows better generalization on held-out image completion tasks and higher AUCs in object-recognition experiments compared to DBNs/DBMs and simple baselines. The representational properties (reuse of subcomponents, composition of mixtures and factors) are argued to support compositional generalization and compact representation of distributions lacking conditional independences. The paper does not present formal out-of-distribution or systematic compositionality tests; claims of superior generalization are empirical (image completion and classification) and supported by experiment but not proven theoretically for OOD regimes.",
            "interpretability_properties": "High: SPNs provide clear node-level semantics (sum nodes ↔ implicit hidden discrete variables, product nodes ↔ composition of feature/subdistribution), enabling inspection of mixture components and computation of node marginals and posterior probabilities for hidden variables via differentiation. MPE/Viterbi-style assignments trace which sum children were selected, providing an interpretable discrete explanation for predictions. The DAG structure and scopes make it possible to map parts of the representation to interpretable sub-distributions (e.g., grammar productions when implementing PCFGs).",
            "limitations_or_failures": "Some empirical weaknesses noted: completed images can look 'blocky'; not all graphical models can be compactly represented as polynomial-size SPNs (some models still require exponential size); gradient-based learning and EM suffer from gradient diffusion in very deep SPNs (mitigated experimentally by hard EM); architecture design (choice of subsets and decompositions) requires heuristics or domain choices and remains an open design problem; continuous-variable nodes rely on tractable integrals (e.g., Gaussian choices) which may constrain modeling; SPNs as presented are still biologically unrealistic despite cortical analogies.",
            "theoretical_framework": "Yes — SPNs are grounded in the network polynomial formalism and algebraic distributive reorganization: completeness and consistency conditions ensure the SPN expansion equals the network polynomial and thus validity (Theorem 1). The paper frames tractability of the partition function as the representability of the network polynomial by a polynomial-size sum-product expression, and proves representational comparisons (SPNs strictly more general than some tractable graphical model classes). Decomposability/consistency notions and proofs for exactness of MPE under decomposability are given, and learning rationale (EM interpretation of sum nodes as marginalized hidden variables, and hard EM to avoid gradient diffusion) is provided as procedural/theoretical guidance.",
            "uuid": "e607.0",
            "source_info": {
                "paper_title": "Sum-product networks: A new deep architecture",
                "publication_date_yy_mm": "2011-07"
            }
        },
        {
            "name_short": "PCFG-as-SPN",
            "name_full": "Probabilistic Context-Free Grammar implemented as a decomposable Sum-Product Network",
            "brief_description": "The paper states that probabilistic context-free grammars (PCFGs) and statistical parsers can be implemented straightforwardly as decomposable SPNs by mapping non-terminal symbols to sum (or max) nodes and productions to product nodes, enabling chart-parser-like SPN representations of grammars.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "PCFG implemented as decomposable SPN",
            "system_description": "A PCFG maps naturally to a decomposable SPN: sum nodes represent non-terminals (mixtures over productions or parses), product nodes represent concatenation/production application (conjunction of subspans). The decomposability constraint ensures that product children have disjoint scopes (chart spans), matching the span decomposition in parsing. The resulting SPN is equivalent to a bounded-size chart parser encoded as a DAG where inference (marginals, MPE parses) can be computed by the SPN upward/downward passes.",
            "declarative_component": "Probabilistic Context-Free Grammar (PCFG) — a symbolic grammar with non-terminals, productions, and probabilistic weights over productions (discrete symbolic derivation structure).",
            "imperative_component": "Sum-Product Network machinery: learnable weights on sum nodes, upward/downward pass inference, EM or hard-EM training, max-substitution for MPE parse; effectively using SPN learning/inference algorithms (gradient descent, EM, hard EM) as procedural components.",
            "integration_method": "Structural mapping: encode grammar non-terminals as sum nodes and productions as product nodes forming a decomposable SPN whose scopes correspond to span intervals. Training/inference uses the SPN imperative algorithms (exact upward/downward passes for marginals, EM/hard-EM for learning weights), turning grammar induction/parameter estimation into SPN parameter learning.",
            "emergent_properties": "Enables efficient exact inference and MPE parse computation through SPN algorithms; allows learning a bounded-size chart parser as a single SPN via end-to-end EM/backprop; supports reuse of subparses (shared subcircuits) leading to compact representations for grammars with overlapping substructures; can represent unrestricted probabilistic grammars with bounded recursion.",
            "task_or_benchmark": "Statistical parsing / PCFG inference (mentioned conceptually; no empirical parsing experiments reported in the paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Conceptual claim: SPN encoding of PCFGs allows reuse of subparses and bounded-size chart parsing representations, which can generalize via shared substructures; can represent grammars with bounded recursion; no empirical evaluation of generalization presented in this paper.",
            "interpretability_properties": "High: the grammar's non-terminals and productions retain their symbolic interpretation in the SPN mapping, so parses and production responsibilities can be inspected via sum node marginals and MPE selections, providing human-readable parse structure explanations.",
            "limitations_or_failures": "Only discussed conceptually; practical limitations include requirement of decomposability for guaranteed MPE correctness and the need to bound chart/parser size to keep SPN polynomial-sized. No empirical results or complexity tradeoffs are provided in the paper.",
            "theoretical_framework": "Mapping of PCFG parsing to decomposable SPN semantics with decomposability guaranteeing that MPE via max-substitution yields valid parses; leverages the SPN completeness/consistency/decomposability theoretical results to ensure correctness of parsing inference when encoded as an SPN.",
            "uuid": "e607.1",
            "source_info": {
                "paper_title": "Sum-product networks: A new deep architecture",
                "publication_date_yy_mm": "2011-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A differential approach to inference in Bayesian networks",
            "rating": 2
        },
        {
            "paper_title": "Learning arithmetic circuits",
            "rating": 2
        },
        {
            "paper_title": "Learning efficient Markov networks",
            "rating": 2
        },
        {
            "paper_title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical models of object recognition in cortex",
            "rating": 1
        }
    ],
    "cost": 0.01411925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sum-Product Networks: A New Deep Architecture</h1>
<p>Hoifung Poon and Pedro Domingos<br>Computer Science \&amp; Engineering<br>University of Washington<br>Seattle, WA 98195, USA<br>{hoifung, pedrod}@cs.washington.edu</p>
<h4>Abstract</h4>
<p>The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sumproduct networks (SPNs). SPNs are directed acyclic graphs with variables as leaves, sums and products as internal nodes, and weighted edges. We show that if an SPN is complete and consistent it represents the partition function and all marginals of some graphical model, and give semantics to its nodes. Essentially all tractable graphical models can be cast as SPNs, but SPNs are also strictly more general. We then propose learning algorithms for SPNs, based on backpropagation and EM. Experiments show that inference and learning with SPNs can be both faster and more accurate than with standard deep networks. For example, SPNs perform image completion better than state-of-the-art deep networks for this task. SPNs also have intriguing potential connections to the architecture of the cortex.</p>
<h2>1 INTRODUCTION</h2>
<p>The goal of probabilistic modeling is to represent probability distributions compactly, compute their marginals and modes efficiently, and learn them accurately. Graphical models [22] represent distributions compactly as normalized products of factors: $P(X=x)=\frac{1}{Z} \prod_{k} \phi_{k}\left(x_{{k}}\right)$, where $x \in \mathcal{X}$ is a $d$-dimensional vector, each potential $\phi_{k}$ is a function of a subset $x_{{k}}$ of the variables (its scope), and $Z=\sum_{x \in \mathcal{X}} \prod_{k} \phi_{k}\left(x_{{k}}\right)$ is the partition function. Graphical models have a number of important limitations. First, there are many distributions that admit a compact representation, but not in the form above. (For example,
the uniform distribution over vectors with an even number of 1's.) Second, inference is still exponential in the worst case. Third, the sample size required for accurate learning is worst-case exponential in scope size. Fourth, because learning requires inference as a subroutine, it can take exponential time even with fixed scopes (unless the partition function is a known constant, which requires restricting the potentials to be conditional probabilities).</p>
<p>The compactness of graphical models can often be greatly increased by postulating the existence of hidden variables $y: P(X=x)=\frac{1}{Z} \sum_{y} \prod_{k} \phi_{k}\left((x, y)_{{k}}\right)$. Deep architectures [2] can be viewed as graphical models with multiple layers of hidden variables, where each potential involves only variables in consecutive layers, or variables in the shallowest layer and $x$. Many distributions can only be represented compactly by deep networks. However, the combination of non-convex likelihood and intractable inference makes learning deep networks extremely challenging. Classes of graphical models where inference is tractable exist (e.g., mixture models [17], thin junction trees [5]), but are quite limited in the distributions they can represent compactly. This paper starts from the observation that models with multiple layers of hidden variables allow for efficient inference in a much larger class of distributions. Surprisingly, current deep architectures do not take advantage of this, and typically solve a harder inference problem than models with one or no hidden layers.</p>
<p>This can be seen as follows. The partition function $Z$ is intractable because it is the sum of an exponential number of terms. All marginals are sums of subsets of these terms; thus if $Z$ can be computed efficiently, so can they. But $Z$ itself is a function that can potentially be compactly represented using a deep architecture. $Z$ is computed using only two types of operations: sums and products. It can be computed efficiently if $\sum_{x \in \mathcal{X}} \prod_{k} \phi_{k}\left(x_{{k}}\right)$ can be reorganized using the distributive law into a computation involving only a polynomial number of sums and products. Given a graphical model, the inference problem in a nutshell is to perform this reorganization. But we can instead learn from the outset a model that is already in efficiently computable form,</p>
<p>viewing sums as implicit hidden variables. This leads naturally to the question: what is the broadest class of models that admit such an efficient form for $Z$ ?</p>
<p>We answer this question by providing conditions for tractability of $Z$, and showing that they are more general than previous tractable classes. We introduce sum-product networks (SPNs), a representation that facilitates this treatment and also has semantic value in its own right. SPNs can be viewed as generalized directed acyclic graphs of mixture models, with sum nodes corresponding to mixtures over subsets of variables and product nodes corresponding to features or mixture components. SPNs lend themselves naturally to efficient learning by backpropagation or EM. Of course, many distributions cannot be represented by polynomial-sized SPNs, and whether these are sufficient for the real-world problems we need to solve is an empirical question. Our experiments show they are quite promising.</p>
<h2>2 SUM-PRODUCT NETWORKS</h2>
<p>For simplicity, we focus first on Boolean variables. The extension to multi-valued discrete variables and continuous variables is discussed later in this section. The negation of a Boolean variable $X_{i}$ is represented by $\bar{X}<em i="i">{i}$. The indicator function [.] has value 1 when its argument is true, and 0 otherwise. Since it will be clear from context whether we are referring to a variable or its indicator, we abbreviate $\left[X</em>}\right]$ by $x_{i}$ and $\left[\bar{X<em i="i">{i}\right]$ by $\bar{x}</em>$.</p>
<p>We build on the ideas of Darwiche [7], and in particular the notion of network polynomial. Let $\Phi(x) \geq 0$ be an unnormalized probability distribution. The network polynomial of $\Phi(x)$ is $\sum_{x} \Phi(x) \Pi(x)$, where $\Pi(x)$ is the product of the indicators that have value 1 in state $x$. For example, the network polynomial for a Bernoulli distribution over variable $X_{i}$ with parameter $p$ is $p x_{i}+(1-p) \bar{x}<em 1="1">{i}$. The network polynomial for the Bayesian network $X</em>} \rightarrow$ $X_{2}$ is $P\left(x_{1}\right) P\left(x_{2} \mid x_{1}\right) x_{1} x_{2}+P\left(x_{1}\right) P\left(\bar{x<em 1="1">{2} \mid x</em>}\right) x_{1} \bar{x<em 1="1">{2}+$ $P\left(\bar{x}</em>}\right) P\left(x_{2} \mid \bar{x<em 1="1">{1}\right) \bar{x}</em>} x_{2}+P\left(\bar{x<em 2="2">{1}\right) P\left(\bar{x}</em>} \mid \bar{x<em 1="1">{1}\right) \bar{x}</em>$.} \bar{x}_{2</p>
<p>The network polynomial is a multilinear function of the indicator variables. The unnormalized probability of evidence (partial instantiation of $X$ ) $e$ is the value of the network polynomial when all indicators compatible with $e$ are set to 1 and the remainder are set to 0 . For example, $\Phi\left(X_{1}=1, X_{3}=0\right)$ is the value of the network polynomial when $\bar{x}<em 3="3">{1}$ and $x</em>$ are set to 0 and the remaining indicators are set to 1 throughout. The partition function is the value of the network polynomial when all indicators are set to 1 . For any evidence $e$, the cost of computing $P(e)=\Phi(e) / Z$ is linear in the size of the network polynomial. Of course, the network polynomial has size exponential in the number of variables, but we may be able to represent and evaluate it in polynomial space and time using a sum-product network.</p>
<p>Definition 1 A sum-product network (SPN) over variables</p>
<p>Figure 1: Top: SPN implementing a naive Bayes mixture model (three components, two variables). Bottom: SPN implementing a junction tree (clusters $\left(X_{1}, X_{2}\right)$ and $\left(X_{1}, X_{3}\right)$, separator $\left.X_{1}\right)$.
<img alt="img-0.jpeg" src="img-0.jpeg" />
$x_{1}, \ldots, x_{d}$ is a rooted directed acyclic graph whose leaves are the indicators $x_{1}, \ldots, x_{d}$ and $\bar{x}<em d="d">{1}, \ldots, \bar{x}</em>$ is the value of node $j$. The value of an SPN is the value of its root.}$ and whose internal nodes are sums and products. Each edge $(i, j)$ emanating from a sum node $i$ has a non-negative weight $w_{i j}$. The value of a product node is the product of the values of its children. The value of a sum node is $\sum_{j \in C h(i)} w_{i j} v_{j}$, where $C h(i)$ are the children of $i$ and $v_{j</p>
<p>Figure 1 shows examples of SPNs. In this paper we will assume (without loss of generality) that sums and products are arranged in alternating layers, i.e., all children of a sum are products or leaves, and vice-versa.</p>
<p>We denote the sum-product network $S$ as a function of the indicator variables $x_{1}, \ldots, x_{d}$ and $\bar{x}<em d="d">{1}, \ldots, \bar{x}</em>}$ by $S\left(x_{1}, \ldots, x_{d}, \bar{x<em d="d">{1}, \ldots, \bar{x}</em>}\right)$. When the indicators specify a complete state $x$ (i.e., for each variable $X_{i}$, either $x_{i}=1$ and $\bar{x_{i}}=0$ or $x_{i}=0$ and $\bar{x_{i}}=1$ ), we abbreviate this as $S(x)$. When the indicators specify evidence $e$ we abbreviate it as $S(e)$. When all indicators are set to 1 , we abbreviate it as $S(*)$. The subnetwork rooted at an arbitrary node $n$ in the SPN is itself an SPN, which we denote by $S_{n}($.$) . The values of S(x)$ for all $x \in \mathcal{X}$ define an unnormalized probability distribution over $\mathcal{X}$. The unnormalized probability of evidence $e$ under this distribution is $\Phi_{S}(e)=\sum_{x \in e} S(x)$, where the sum is over states consistent with $e$. The partition function of the distribution defined by $S(x)$ is $Z_{S}=\sum_{x \in \mathcal{X}} S(x)$. The scope of an SPN $S$ is the set of variables that appear in $S$. A variable $X_{i}$ appears negated in $S$ if $\bar{x<em i="i">{i}$ is a leaf of $S$ and non-negated if $x</em>$ is a leaf of $S$.</p>
<p>For example, for the SPN in Figure 1, $S\left(x_{1}, x_{2}, \bar{x}<em 2="2">{1}, \bar{x}</em>}\right)=$ $0.5\left(0.6 x_{1}+0.4 \bar{x<em 2="2">{1}\right)\left(0.3 x</em>}+0.7 \bar{x<em 1="1">{2}\right)+0.2\left(0.6 x</em>+\right.$</p>
<p>$0.4 \bar{x}<em 2="2">{1}\left(0.2 x</em>}+0.8 \bar{x<em 1="1">{2}\right)+0.3\left(0.9 x</em>}+0.1 \bar{x<em 2="2">{1}\right)\left(0.2 x</em>}+0.8 \bar{x<em 1="1">{2}\right)$. The network polynomial is $(0.5 \times 0.6 \times 0.3+0.2 \times 0.6 \times$ $0.2+0.3 \times 0.9 \times 0.2) x</em>=1$, then $S(e)=S(1,1,0,1)$. Finally, $S(*)=S(1,1,1,1)$.} x_{2}+\ldots$ If a complete state $x$ is $X_{1}=1, X_{2}=0$, then $S(x)=S(1,0,0,1)$. If the evidence $e$ is $X_{1</p>
<p>Definition 2 A sum-product network $S$ is valid iff $S(e)=$ $\Phi_{S}(e)$ for all evidence $e$.</p>
<p>In other words, an SPN is valid if it always correctly computes the probability of evidence. In particular, if an SPN $S$ is valid then $S(*)=Z_{S}$. A valid SPN computes the probability of evidence in time linear in its size. We would like to learn only valid SPNs, but otherwise have as much flexibility as possible. We thus start by establishing general conditions for the validity of an SPN.</p>
<p>Definition 3 A sum-product network is complete iff all children of the same sum node have the same scope.</p>
<p>Definition 4 A sum-product network is consistent iff no variable appears negated in one child of a product node and non-negated in another.</p>
<p>Theorem 1 A sum-product network is valid if it is complete and consistent.</p>
<p>Proof. Every SPN $S$ can be expressed as a polynomial $\sum_{k} s_{k} \prod_{k}(\ldots)$, where $\prod_{k}(\ldots)$ is a monomial over the indicator variables and $s_{k} \geq 0$ is its coefficient. We call this the expansion of the SPN; it is obtained by applying the distributive law bottom-up to all product nodes in the SPN, treating each $x_{i}$ leaf as $1 x_{i}+0 \bar{x}<em i="i">{i}$ and each $\bar{x}</em>}$ leaf as $0 x_{i}+1 \bar{x<em x="x">{i}$. An SPN is valid if its expansion is its network polynomial, i.e., the monomials in the expansion and the states $x$ are in one-to-one correspondence: each monomial is non-zero in exactly one state (condition 1), and each state has exactly one monomial that is non-zero in it (condition 2). From condition 2, $S(x)$ is equal to the coefficient $s</em>=S(e)$ and the SPN is valid.
We prove by induction from the leaves to the root that, if an SPN is complete and consistent, then its expansion is its network polynomial. This is trivially true for a leaf. We consider only internal nodes with two children; the extension to the general case is immediate. Let $n^{0}$ be an arbitrary internal node with children $n^{1}$ and $n^{2}$. We denote the scope of $n^{0}$ by $V^{0}$, a state of $V^{0}$ by $x^{0}$, the expansion of the subgraph rooted at $n^{0}$ by $S^{0}$, and the unnormalized probability of $x_{0}$ under $S_{0}$ by $\Phi^{0}\left(x^{0}\right)$; and similarly for $n^{1}$ and
$n^{2}$. By the induction hypothesis, $S^{1}=\sum_{x^{1}} \Phi^{1}\left(x^{1}\right) \Pi\left(x^{1}\right)$ and $S^{2}=\sum_{x^{2}} \Phi^{2}\left(x^{2}\right) \Pi\left(x^{2}\right)$.
If $n^{0}$ is a sum node, then $S^{0}=w_{01} \sum_{x^{1}} \Phi^{1}\left(x^{1}\right) \Pi\left(x^{1}\right)+$ $w_{02} \sum_{x^{2}} \Phi^{2}\left(x^{2}\right) \Pi\left(x^{2}\right)$. If $V^{1} \neq V^{2}$, then each state of $V^{1}$ (or $V^{2}$ ) corresponds to multiple states of $V^{0}=V^{1} \cup V^{2}$, and therefore each monomial from $V^{1}\left(V^{2}\right)$ is non-zero in more than one state of $V^{0}$, breaking the correspondence between monomials of $S^{0}$ and states of $V^{0}$. However, if the SPN is complete then $V^{0}=V^{1}=V^{2}$, and their states are in one-to-one correspondence. Therefore by the induction hypothesis the monomials of $V^{1}$ and $V^{2}$ are also in one-to-one correspondence and $S^{0}=\sum_{x^{0}}\left(w_{01} \Phi^{1}\left(x^{0}\right)+\right.$ $\left.w_{02} \Phi^{2}\left(x^{0}\right)\right) \Pi\left(x^{0}\right)$; i.e., the expansion of $S^{0}$ is its network polynomial.
If $n^{0}$ is a product node, then $S^{0}=$ $\left(\sum_{x^{1}} \Phi^{1}\left(x^{1}\right) \Pi\left(x^{1}\right)\right)\left(\sum_{x^{2}} \Phi^{2}\left(x^{2}\right) \Pi\left(x^{2}\right)\right)$. If $V_{1} \cap V_{2}=\emptyset$, it follows immediately that the expansion of $V_{0}$ is its network polynomial. In the more general case, let $V_{12}=V_{1} \cap V_{2}, V_{1-}=V_{1} \backslash V_{2}$ and $V_{2-}=V_{2} \backslash V_{1}$ and let $x^{12}, x^{1-}$ and $x^{2-}$ be the corresponding states. Since each $\Phi^{1}\left(x^{1}\right)$ is non-zero in exactly one state $x^{1}$ and similarly for $\Phi^{2}\left(x^{2}\right)$, each monomial in the product of $S^{1}$ and $S^{2}$ is nonzero in at most one state of $V^{0}=V^{1} \cup V^{2}$. If the SPN is not consistent, then at least one monomial in the product contains both the positive and negative indicators of a variable, $x_{i}$ and $\bar{x}}$ of the monomial that is non-zero in it, and therefore $\Phi_{S}(e)=\sum_{x \in e} S(x)=\sum_{x \in e} s_{x}=\sum_{k} s_{k} n_{k}(e)$, where $n_{k}(e)$ is the number of states $x$ consistent with $e$ for which $\Pi_{k}(x)=1$. From condition 1, $n_{k}=1$ if the state $x$ for which $\Pi_{k}(x)=1$ is consistent with the evidence and $n_{k}=0$ otherwise, and therefore $\Phi_{S}(e)=$ $\sum_{k: \Pi_{k}(e)=1} s_{k<em i="i">{i}$. Since no monomial in the network polynomial contains both $x</em>}$ and $\bar{x<em 0="0">{i}$, this means the expansion of $S^{0}$ is not equal to its network polynomial. To ensure that each monomial in $S^{0}$ is non-zero in at least one state of $V</em>$ indicators, i.e., the SPN must be consistent.}$, for every $\Pi\left(x^{1-}, x^{12}\right), \Pi\left(x^{12}, x^{2-}\right)$ pair there must exist a state $x^{0}=\left(x^{1-}, x^{12}, x^{2-}\right)$ where both $\Pi\left(x^{1-}, x^{12}\right)$ and $\Pi\left(x^{12}, x^{2-}\right)$ are 1 , and therefore the indicators over $x^{12}$ in both monomials must be consistent. Since by the induction hypothesis they completely specify $x^{12}$, they must be the same in the two monomials. Therefore all $\Pi\left(x^{1-}, x^{12}\right)$ and $\Pi\left(x^{12}, x^{2-}\right)$ monomials must have the same $x^{12</p>
<p>Completeness and consistency are not necessary for validity; for example, the network $S\left(x_{1}, x_{2}, \bar{x}<em 2="2">{1}, \bar{x}</em>}\right)=$ $\frac{1}{2} x_{1} x_{2} \bar{x<em 1="1">{2}+\frac{1}{2} x</em> S(x)$ for all evidence $e$. However, completeness and consistency are necessary for the stronger property that every subnetwork of $S$ be valid. This can be proved by refutation. The input nodes are valid by definition. Let S be a node that violates either completeness or consistency but all of its descendants satisfy both conditions. We can show that S is not valid since it either undercounts the summation (if it is incomplete) or overcounts it (if it is inconsistent).}$ is incomplete and inconsistent, but satisfies $\Phi_{S}(e)=\sum_{x \in e</p>
<p>If an SPN $S$ is complete but inconsistent, its expansion includes monomials that are not present in its network polynomial, and $S(e) \geq \Phi_{S}(e)$. If $S$ is consistent but in-</p>
<p>Figure 2: A sum node $i$ can be viewed as the result of summing out a hidden variable $Y_{i} ; y_{i j}$ represents the indicator $\left[Y_{i}=j\right]$ and $j$ ranges over the children of $i$.
<img alt="img-1.jpeg" src="img-1.jpeg" />
complete, some of its monomials are missing indicators relative to the monomials in its network polynomial, and $S(e) \leq \Phi_{S}(e)$. Thus invalid SPNs may be useful for approximate inference. Exploring this is a direction for future work.</p>
<p>Completeness and consistency allow us to design deep architectures where inference is guaranteed to be efficient. This in turn makes learning them much easier.</p>
<p>Definition 5 An unnormalized probability distribution $\Phi(x)$ is representable by a sum-product network $S$ iff $\Phi(x)=S(x)$ for all states $x$ and $S$ is valid.
$S$ then correctly computes all marginals of $\Phi(x)$, including its partition function.</p>
<p>Theorem 2 The partition function of a Markov network $\Phi(x)$, where $x$ is a d-dimensional vector, can be computed in time polynomial in $d$ if $\Phi(x)$ is representable by a sumproduct network with a number of edges polynomial in $d$.</p>
<p>Proof. Follows immediately from the definitions of SPN and representability.</p>
<p>Definition 6 A sum-product network is decomposable iff no variable appears in more than one child of a product node.</p>
<p>Decomposability is more restricted than consistency (e.g., $S\left(x_{1}, \bar{x}<em 1="1">{1}\right)=x</em>$ is consistent but not decomposable.) This makes SPNs more general than representations that require decomposability, like arithmetic circuits [7], probabilistic context-free grammars [6], mixture models [32], junction trees [5], and others. (See also Section 3.)
SPNs can be extended to multi-valued discrete variables simply by replacing the Boolean indicators $\left[X_{i}=1\right]$, $\left[X_{i}=0\right]$ with indicators for the variable's possible values $x_{i}^{j}:\left[X_{i}=x_{i}^{1}\right], \ldots,\left[X_{i}=x_{i}^{m}\right]$, or $x_{i}^{1}, \ldots, x_{i}^{m}$ for short. For example, the multinomial distribution over $X_{i}$ is represented by $\sum_{j=1}^{m} p_{i}^{j} x_{i}^{j}$, where $p_{i}^{j}=P\left(X_{i}=x_{i}^{j}\right)$.
If an SPN $S$ is complete and consistent, and for every sum node $i, \sum_{j \in C h(i)} w_{i j}=1$, where $C h(i)$ are the children of $i$, then $Z_{S}=1$. In this case, we can view each sum node $i$ as the result of summing out an implicit hidden variable
$Y_{i}$ whose values correspond to its children $C h(i)$ (see Figure 2). This is because a variable is summed out by setting all its indicators to 1 , and children of product nodes whose value is 1 can be omitted. Thus the SPN rooted at node $i$ can be viewed as a mixture model, with its children being the mixture components, which in turn are products of mixture models. If $i$ has no parent (i.e., it is the root), its children's weights are $Y_{i}$ 's prior distribution: $w_{i j}=P\left(Y_{i}=j\right)$. Otherwise $w_{i j}=P\left(Y_{i}=j \mid \pi_{i}\right)$, where $\pi_{i}$ is the condition that, on at least one path from $Y_{i}$ to the root, all of $Y_{i}$ 's ancestors have the values that lead to $Y_{i}$ (the ancestors being the hidden variables corresponding to the sum nodes on the path). If the network is also decomposable, the subnetwork rooted at the $j$ th child then represents the distribution of the variables in it conditioned on $Y_{i}=j$. Thus an SPN can be viewed as a compact way to specify a mixture model with exponentially many mixture components, where subcomponents are composed and reused in larger ones. From this perspective, we can naturally derive an EM algorithm for SPN learning. (See Section 4.)} x_{1</p>
<p>SPNs can be generalized to continuous variables by viewing these as multinomial variables with an infinite number of values. The multinomial's weighted sum of indicators $\sum_{j=1}^{m} p_{i}^{j} x_{i}^{j}$ then becomes the integral $\int p(x) d x$, where $p(x)$ is the p.d.f. of $X$. For example, $p(x)$ can be a univariate Gaussian. Thus SPNs over continuous variables have integral nodes instead of sum nodes with indicator children (or instead of indicators, since these can be viewed as degenerate sum nodes where one weight is 1 and the others are 0 ). We can then form sums and products of these nodes, as before, leading to a rich yet compact language for specifying high-dimensional continuous distributions. During inference, if the evidence includes $X=x$, the value of an integral node $n$ over $x$ is $p_{n}(x)$; otherwise its value is 1. Computing the probability of evidence then proceeds as before.</p>
<p>Given a valid SPN, the marginals of all variables (including the implicit hidden variables $Y$ ) can be computed by differentiation [7]. Let $n_{i}$ be an arbitrary node in SPN $S, S_{i}(x)$ be its value on input instance $x$, and $P a_{i}$ be its parents. If $n_{i}$ is a product node, its parents (by assumption) are sum nodes, and $\partial S(x) / \partial S_{i}(x)=\sum_{k \in P a_{i}} w_{k i} \partial S(x) / \partial S_{k}(x)$. If $n_{i}$ is a sum node, its parents (by assumption) are product nodes, and $\partial S(x) / \partial S_{i}(x)=$ $\sum_{k \in P a_{i}}\left(\partial S(x) / \partial S_{k}(x)\right) \prod_{l \in C h \ldots(k)} S_{l}(x), \quad$ where $C h \ldots_{i}(k)$ are the children of the $k$ th parent of $n_{i}$ excluding $n_{i}$. Thus we can evaluate $S_{i}$ 's in an upward pass from input to the root, with parents following their children, and then compute $\partial S(x) / \partial w_{i j}$ and $\partial S(x) / \partial S_{i}(x)$ in a downward pass from the root to input, with children following parents. The marginals for the nodes can be derived from these partial derivatives [7]. In particular, if $n_{i}$ is a child of a sum node $n_{k}$, then $P\left(Y_{k}=i \mid e\right) \propto w_{k i} \partial S(e) / \partial S_{k}(e)$; if $n_{i}$ is an indicator $\left[X_{s}=t\right]$, then $P\left(X_{s}=t \mid e\right) \propto \partial S(e) / \partial S_{i}(e)$.</p>
<p>The continuous case is similar except that we have marginal densities rather than marginal probabilities.</p>
<p>The MPE state $\arg \max _{X, Y} P(X, Y \mid e)$ can be computed by replacing sums by maximizations. In the upward pass, a max node outputs the maximum weighted value among its children instead of their weighted sum. The downward pass then starts from the root and recursively selects the (or a) highest-valued child of a max node, and all children of a product node. Based on the results in Darwiche [7], we can prove that this will find the MPE state if the SPN is decomposable. Extension of the proof to consistent SPNs is straightforward since by definition no conflicting input indicators will be chosen. The continuous case is similar, and straightforward as long as computing the max and argmax of $p(x)$ is easy (as is the case with Gaussians).</p>
<h2>3 SUM-PRODUCT NETWORKS AND OTHER MODELS</h2>
<p>Let $R_{M}(D)$ be the most compact representation of distribution $D$ under moder class $M$, size $(R)$ be the size of representation $R, c&gt;0$ be a constant, and $\exp (x)$ be an exponential function. We say that model class $M_{1}$ is more general than model class $M_{2}$ iff for all distributions $D \operatorname{size}\left(R_{M_{1}}(D)\right) \leq c \cdot \operatorname{size}\left(R_{M_{2}}(D)\right)$ and there exist distributions for which $\operatorname{size}\left(R_{M_{2}}(D)\right) \geq \exp \left(\operatorname{size}\left(R_{M_{1}}(D)\right)\right.$. In this sense, sum-product networks are more general than both hierarchical mixture models [32] and thin junction trees [5]. Clearly, both of these can be represented as SPNs without loss of compactness (see Figure 1). SPNs can be exponentially more compact than hierarchical mixture models because they allow mixtures over subsets of variables and their reuse. SPNs can be exponentially more compact than junction trees when context-specific independence and determinism are present, since they exploit these and junction trees do not. This holds even when junction trees are formed from Bayesian networks with contextspecific independence in the form of decision trees at the nodes, because decision trees suffer from the replication problem [21] and can be exponentially larger than a DAG representation of the same function.</p>
<p>Figure 3 shows an SPN that implements a uniform distribution over states of five variables with an even number of 1's, as well as the corresponding mixture model. The distribution can also be non-uniform if the weights are not uniform. In general, SPNs can represent such distributions in size linear in the number of variables, by reusing intermediate components. In contrast, a mixture model (hierarchical or not) requires an exponential number of components, since each component must correspond to a complete state, or else it will assign non-zero probability to some state with an odd number of 1's.</p>
<p>Graphical models with junction tree clique potentials that</p>
<p>Figure 3: Top: SPN representing the uniform distribution over states of five variables containing an even number of 1's. Bottom: mixture model for the same distribution. For simplicity, we omit the uniform weights.
<img alt="img-2.jpeg" src="img-2.jpeg" />
cannot be simplified to polynomial size cannot be represented compactly as SPNs. More interestingly, as the previous example shows, SPNs can compactly represent some classes of distributions in which no conditional independences hold. Multi-linear representations (MLRs) also have this property [24]. Since MLRs are essentially expanded SPNs, an SPN can be exponentially more compact than the corresponding MLR.</p>
<p>SPNs are closely related to data structures for efficient inference like arithmetic circuits [7] and AND/OR graphs [8]. However, to date these have been viewed purely as compilation targets for Bayesian network inference and related tasks, and have no semantics as models in their own right. As a result, the problem of learning them has not generally been considered. The two exceptions we are aware of are Lowd and Domingos [18] and Gogate et al. [12]. Lowd and Domingos's algorithm is a standard Bayesian network structure learner with the complexity of the resulting circuit as the regularizer, and does not have the flexibility of SPN learning. Gogate et al.'s algorithm learns Markov networks representable by compact circuits, but does not reuse subcircuits. Case-factor diagrams [19] are another compact representation, similar to decomposable SPNs. No algorithms for learning them or for computing the probability of evidence in them have been proposed to date.</p>
<p>We can view the product nodes in an SPN as forming a feature hierarchy, with the sum nodes representing distributions over them; in contrast, standard deep architectures</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">LearnSPN</span>
<span class="w">    </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">Set</span><span class="w"> </span>\<span class="p">(</span><span class="n">D</span>\<span class="p">)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">instances</span><span class="w"> </span><span class="n">over</span><span class="w"> </span><span class="n">variables</span><span class="w"> </span>\<span class="p">(</span><span class="n">X</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">Output</span><span class="p">:</span><span class="w"> </span><span class="n">An</span><span class="w"> </span><span class="n">SPN</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">learned</span><span class="w"> </span><span class="n">structure</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">parameters</span><span class="o">.</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">S</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">GenerateDenseSPN</span><span class="w"> </span>\<span class="p">((</span><span class="n">X</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">InitializeWeights</span><span class="w"> </span>\<span class="p">((</span><span class="n">S</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">repeat</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">all</span><span class="w"> </span>\<span class="p">(</span><span class="n">d</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">D</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="n">UpdateWeights</span><span class="p">(</span><span class="w"> </span>\<span class="p">(</span><span class="n">S</span>\<span class="p">),</span><span class="w"> </span><span class="n">Inference</span><span class="w"> </span>\<span class="p">((</span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="p">))</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="n">until</span><span class="w"> </span><span class="n">convergence</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">S</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">PruneZeroWeights</span><span class="w"> </span>\<span class="p">((</span><span class="n">S</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span>\<span class="p">(</span><span class="n">S</span>\<span class="p">)</span>
</code></pre></div>

<p>explicitly represent only the features, and require the sums to be inefficiently computed by Gibbs sampling or otherwise approximated. Convolutional networks [15] alternate feature layers with pooling layers, where the pooling operation is typically max or average, and the features in each layer are over a subset of the input variables. Convolutional networks are not probabilistic, and are usually viewed as a vision-specific architecture. SPNs can be viewed as probabilistic, general-purpose convolutional networks, with average-pooling corresponding to marginal inference and max-pooling corresponding to MPE inference. Lee at al. [16] have proposed a probabilistic version of max-pooling, but in their architecture there is no correspondence between pooling and the sum or max operations in probabilistic inference, as a result of which inference is generally intractable. SPNs can also be viewed as a probabilistic version of competitive learning [27] and sigma-pi networks [25]. Like deep belief networks, SPNs can be used for nonlinear dimensionality reduction [14], and allow objects to be reconstructed from the reduced representation (in the case of SPNs, a choice of mixture component at each sum node).</p>
<p>Probabilistic context-free grammars and statistical parsing [6] can be straightforwardly implemented as decomposable SPNs, with non-terminal nodes corresponding to sums (or maxes) and productions corresponding to products (logical conjunctions for standard PCFGs, and general products for head-driven PCFGs). Learning an SPN then amounts to directly learning a chart parser of bounded size. However, SPNs are more general, and can represent unrestricted probabilistic grammars with bounded recursion. SPNs are also well suited to implementing and learning grammatical vision models (e.g., [10, 33]).</p>
<h2>4 LEARNING SUM-PRODUCT NETWORKS</h2>
<p>The structure and parameters of an SPN can be learned together by starting with a densely connected architecture and learning the weights, as in multilayer perceptrons. Algorithm 1 shows a general learning scheme with online learning; batch learning is similar.</p>
<p>First, the SPN is initialized with a generic architecture. The only requirement on this architecture is that it be valid (complete and consistent). Then each example is processed in turn by running inference on it and updating the weights. This is repeated until convergence. The final SPN is obtained by pruning edges with zero weight and recursively removing non-root parentless nodes. Note that a weighted edge must emanate from a sum node and pruning such edges will not violate the validity of the SPN. Therefore, the learned SPN is guaranteed to be valid.</p>
<p>Completeness and consistency are general conditions that leave room for a very flexible choice of architectures. Here, we propose a general scheme for producing the initial architecture: 1 . Select a set of subsets of the variables. 2. For each subset $R$, create $k$ sum nodes $S_{1}^{R}, \ldots, S_{k}^{R}$, and select a set of ways to decompose $R$ into other selected subsets $R_{1}, \ldots, R_{l} .3$. For each of these decompositions, and for all $1 \leq i_{1}, \ldots, i_{l} \leq k$, create a product node with parents $S_{j}^{R}$ and children $S_{i_{1}}^{R_{1}}, \ldots, S_{i_{l}}^{R_{l}}$. We require that only a polynomial number of subsets is selected and for each subset only a polynomial number of decompositions is chosen. This ensures that the initial SPN is of polynomial size and guarantees efficient inference during learning and for the final SPN. For domains with inherent local structure, there are usually intuitive choices for subsets and decompositions; we give an example in Section 5 for image data. Alternatively, subsets and decompositions can be selected randomly, as in random forests [4]. Domain knowledge (e.g., affine invariances or symmetries) can also be incorporated into the architecture, although we do not pursue this in this paper.</p>
<p>Weight updating in Algorithm 1 can be done by gradient descent or EM. We consider each of these in turn.</p>
<p>SPNs lend themselves naturally to efficient computation of the likelihood gradient by backpropagation [26]. Let $n_{j}$ be a child of sum node $n_{i}$. Then $\partial S(x) / \partial w_{i j}=$ $\left(\partial S(x) / \partial S_{i}(x)\right) S_{j}(x)$ and can be computed along with $\partial S(x) / \partial S_{i}(x)$ using the marginal inference algorithm described in Section 2. The weights can then be updated by a gradient step. (Also, if batch learning is used instead, quasi-Newton and conjugate gradient methods can be applied without the difficulties introduced by approximate inference.) We ensure that $S(<em>)=1$ throughout by renormalizing the weights at each step, i.e., projecting the gradient onto the $S(</em>)=1$ constraint surface. Alternatively, we can let $Z=S(<em>)$ vary and optimize $S(X) / S(</em>)$.</p>
<p>SPNs can also be learned using EM [20] by viewing each sum node $i$ as the result of summing out a corresponding hidden variable $Y_{i}$, as described in Section 2. Now the inference in Algorithm 1 is the E step, computing the marginals of the $Y_{i}$ 's, and the weight update is the M step, adding each $Y_{i}$ 's marginal to its sum from the previous iterations and renormalizing to obtain the new weights.</p>
<p>In either case, MAP learning can be done by placing a prior on the weights. In particular, we can use a sparse prior, leading to a smaller SPN after pruning zero weights and thus to faster inference, as well as combatting overfitting.</p>
<p>Unfortunately, both gradient descent and EM as described above give poor results when learning deep SPNs. Gradient descent falls prey to the gradient diffusion problem: as more layers are added, the gradient signal rapidly dwindles to zero. This is the key difficulty in deep learning. EM also suffers from this problem, because its updates also become smaller and smaller as we go deeper. We propose to overcome this problem by using hard EM, i.e., replacing marginal inference with MPE inference. Algorithm 1 now maintains a count for each sum child, and the M step simply increments the count of the winning child; the weights are obtained by normalizing the counts. This avoids the gradient diffusion problem because all updates, from the root to the inputs, are of unit size. In our experiments, this made it possible to learn accurate deep SPNs, with tens of layers instead of the few typically used in deep learning.</p>
<h2>5 EXPERIMENTS</h2>
<p>We evaluated SPNs by applying them to the problem of completing images. This is a good test for a deep architecture, because it is an extremely difficult task, where detecting deep structure is key. Image completion has been studied quite extensively in graphics and vision communities (e.g., [31, 3]), but the focus tends to be restoring small occlusions (e.g., eyeglasses) to facilitate recognition tasks. Some recent machine learning works also showed selected image completion results [16, 1, 30], but they were limited and often focused on small images. In contrast, we conducted extensive evaluations where the half of each image is occluded.</p>
<p>We conducted our main evaluation on Caltech-101 [9], a well-known dataset containing images in 101 categories such as faces, helicopters, and dolphins. For each category, we set aside the last third (up to 50 images) for test and trained an SPN using the rest. For each test image, we covered half of the image and applied the learned SPN to complete the occlusion. Additionally, we also ran experiments on the Olivetti face dataset [28] containing 400 faces.</p>
<p>To initialize the SPN, we used an architecture that leverages local structure in image data. Specifically, in GenerateDenseSPN, all rectangular regions are selected, with the smallest regions corresponding to pixels. For each rectangular region, we consider all possible ways to decompose it into two rectangular subregions.</p>
<p>SPNs can also adopt multiple resolution levels. For example, for large regions we may only consider coarse region decompositions. In preliminary experiments, we found that this made learning much faster with little degradation in accuracy. In particular, we adopted an architecture that uses decompositions at a coarse resolution of $m$-by- $m$ for large regions, and finer decompositions only inside each $m$-by$m$ block. We set $m$ to 4 in our experiments.</p>
<p>The SPNs learned in our experiments were very deep, containing 36 layers. In general, in our architecture there are $2(d-1)$ layers between the root and input for $d \times d$ images. The numbers for SPNs with multiple resolution levels can be computed similarly.</p>
<p>We used mini-batches in online hard EM; processing of instances in a batch can be trivially parallelized. Running soft EM after hard EM yielded no improvement. The best results were obtained using sums on the upward pass and maxes on the downward pass (i.e., the MPE value of each hidden variable is computed conditioning on the MPE values of the hidden variables above it and summing out the ones below). We initialized all weights to zero and used add-one smoothing when evaluating nodes. We penalized non-zero weights with an $L_{0}$ prior with parameter 1.</p>
<p>To handle gray-scale intensities, we normalized the intensities of input images to have zero mean and unit variance, and treated each pixel variable $X_{i}$ as a continuous sample from a Gaussian mixture with $k$ unit-variance components. For each pixel, the intensities of training examples are divided into $k$ equal quantiles and the mean of each component is set to that of the corresponding quantile. We used four components in our experiments. (We also tried using more components and learning the mixing parameters, but it yielded no improvement in performance.)</p>
<p>We compared SPNs with deep belief networks (DBNs) [14] and deep Boltzmann machines (DBMs) [29]. These are state-of-the-art deep architectures and their codes are publicly available. DBNs and DBMs both consist of several layers of restricted Boltzmann machines (RBMs), but they differ in the probabilistic model and training procedure.</p>
<p>We also compared SPNs with principal component analysis (PCA) and nearest neighbor. PCA has been used extensively in previous image completion works [31]. We used 100 principal components in our experiments. (Results with higher or lower numbers are similar.) Despite its simplicity, nearest neighbor can give quite good results if an image similar to the test one has been seen in the past [13]. For each test image, we found the training image with most similar right (top) half using Euclidean distance, and returned its left (bottom) half.</p>
<p>We report mean square errors of the completed pixels of test images for these five algorithms. Table 1 show the average result among all Caltech-101 categories, as well as</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Mean squared errors on completed image pixels in the left or bottom half. NN is nearest neighbor.</p>
<p>| LEFT | SPN | DBM | DBN | PCA | NN |
| Caltech (ALL) | 3475 | 9043 | 4778 | 4234 | 4887 |
| Face | 1815 | 2998 | 4960 | 2851 | 2327 |
| Helicopter | 2749 | 5935 | 3353 | 4056 | 4265 |
| Dolphin | 3099 | 6898 | 4757 | 4232 | 4227 |
| Olivetti | 942 | 1866 | 2386 | 1076 | 1527 |</p>
<p>| BOTTOM | SPN | DBM | DBN | PCA | NN |
| Caltech (ALL) | 3538 | 9792 | 4492 | 4465 | 5505 |
| Face | 1924 | 2656 | 3447 | 1944 | 2575 |
| Helicopter | 3064 | 7325 | 4389 | 4432 | 7156 |
| Dolphin | 2767 | 7433 | 4514 | 4707 | 4673 |
| Olivetti | 918 | 2401 | 1931 | 1265 | 1793 |</p>
<p>the results for a few example categories and Olivetti. ${ }^{2}$ Note that the DBN results are not directly comparable with others. Using the original images and without additional preprocessing, the learned DBN gave very poor results, despite our extensive effort to experiment using the code from Hinton and Salakhutdinov [14]. Hinton and Salakhutdinov [14] reported results for image reconstruction on Olivetti faces, but they used reduced-scale images ( $25 \times 25$ compared to the original size of $64 \times 64$ ) and required a training set containing over 120,000 images derived via transformations like rotation, scaling, etc. By converting to the reduced scale and initializing with their learned model, the results improve significantly and so we report these results instead. Note that reducing the scale artificially lowers the mean square errors by reducing the overall variance. So although DBN appears to have lower errors than DBM and nearest neighbor, their completions are actually much worse (see examples in Figure 5).</p>
<p>Overall, SPN outperforms all other methods by a wide margin. PCA performs surprisingly well in terms of mean square errors compared to methods other than SPN, but their completions are often quite blurred since they are a linear combination of prototypical images. Nearest neighbor can give good completions if there is a similar image in training, but in general their completions can be quite poor. Figure 4 shows the scatter plots comparing SPNs with DBMs, PCA, and nearest neighbor, which confirms the advantage of SPN. The differences are statistically significant by the binomial sign test at the $p&lt;0.01$ level.</p>
<p>Compared to state-of-the-art deep architectures [14, 16, 29], we found that SPNs have three significant advantages.</p>
<p>First, SPNs are considerably simpler, theoretically more well-founded, and potentially more powerful. SPNs ad-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Figure 4: Scatter plots comparing SPNs with DBMs, PCA, and nearest neighbor in mean square errors on Caltech-101. Each point represents an image category. The $y$ axes have the same scale as the $x$ axes. Top: left completion. Bottom: bottom completion.
<img alt="img-3.jpeg" src="img-3.jpeg" />
mit efficient exact inference, while DBNs and DBMs require approximate inference. The problem of gradient diffusion limits most learned DBNs and DBMs to a few layers, whereas with online hard EM, very deep accurate SPNs were learned in our experiments. In practice, DBNs and DBMs also tend to require substantially more engineering. For example, we set the hyperparameters for SPNs in preliminary experiments and found that these values worked well for all datasets. We also used the same architecture throughout and let learning adapt the SPN to the details of each dataset. In contrast, DBNs and DBMs typically require a careful choice of parameters and architectures for each dataset. (For example, the default learning rate of 0.1 leads to massive divergence in learning Caltech images with DBNs.) SPN learning terminates when the average log-likelihood does not improve beyond a threshold (we used 0.1 , which typically converges in around 10 iterations; 0.01 yielded no improvement in initial experiments). For DBNs/DBMs, however, the number of iterations has to be determined empirically using a large development set. Further, successful DBN/DBM training often requires extensive preprocessing of the examples, while we used essentially none for SPNs.</p>
<p>Second, SPNs are at least an order of magnitude faster in both learning and inference. For example, learning Caltech faces takes about 6 minutes with 20 CPUs, or about 2 hours with one CPU. In contrast, depending on the number of learning iterations and whether a much larger transformed dataset is used (as in [14, 29]), learning time for DBNs/DBMs ranges from 30 hours to over a week. For inference, SPNs took less than a second to find the MPE completion of an image, to compute the likelihood of such a completion, or to compute the marginal probability of a variable, and all these results are exact. In contrast, esti-</p>
<p>Figure 5: Sample face completions. Top to bottom: original, SPN, DBM, DBN, PCA, nearest neighbor. The first three images are from Caltech-101, the rest from Olivetti.
<img alt="img-4.jpeg" src="img-4.jpeg" />
mating likelihood in DBNs or DBMs is a very challenging problem [30]; estimating marginals requires many Gibbs sampling steps that may take minutes or even hours, and the results are approximate without guarantee on the quality.</p>
<p>Third, SPNs appear to learn much more effectively. For example, Lee et al. [16] show five faces with completion results in Figure 6 of their paper. Their network was only able to complete a small portion of the faces, leaving the rest blank (starting with images where the visible side already contained more than half the face). ${ }^{3}$ The completions generated by DBMs look plausible in isolation, but they are often at odds with the observed portion and the same completions are often reused in different images. The mean square error results in Table 1 confirmed that the DBM completions are often not very good. Among all categories, DBMs performed relatively well in Caltech and Olivetti faces. So we contrast example completions in Figure 5, which shows the results for completing the left halves of previously unseen faces. The DBM completions often seem to derive from the nearest neighbor according to its learned model, which suggests that they might not have learned very deep regularities. In comparison, the SPN successfully completed most faces by hypothesizing the correct locations and types of various parts like hair, eye, mouth, and face shape and color. On the other hand, the SPN also has some weaknesses. For example, the completions often look blocky.</p>
<p>We also conducted preliminary experiments to evaluate the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Comparison of the area under the precision-recall curve for three classification problems (one class vs. the other two).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Architecture</th>
<th style="text-align: right;">Faces</th>
<th style="text-align: right;">Motorbikes</th>
<th style="text-align: left;">Cars</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SPN</td>
<td style="text-align: right;">0.99</td>
<td style="text-align: right;">0.99</td>
<td style="text-align: left;">0.98</td>
</tr>
<tr>
<td style="text-align: left;">CDBN (top layer)</td>
<td style="text-align: right;">0.95</td>
<td style="text-align: right;">0.81</td>
<td style="text-align: left;">0.87</td>
</tr>
</tbody>
</table>
<p>potential of SPNs for object recognition. Lee et al. [16] reported results for convolutional DBNs (CDBNs) by training a CDBN for each of the three Caltech-101 categories faces, motorbikes, and cars, and then computed area under precision-recall curve (AUC) by comparing the probabilities for positive and negative examples in each classification problem (one class vs. others). We followed their experimental setting and conducted experiments using SPNs. Table 2 compares the results with those obtained using top layer features in convolutional DBNs (CDBNs) (see Figure 4 in [16]). SPNs obtained almost perfect results in all three categories whereas CDBNs' results are substantially lower, particularly in motorbikes and cars. ${ }^{4}$</p>
<h2>6 SUM-PRODUCT NETWORKS AND THE CORTEX</h2>
<p>The cortex is composed of two main types of cells: pyramidal neurons and stellate neurons. Pyramidal neurons excite the neurons they connect to; most stellate neurons inhibit them. There is an interesting analogy between these two types of neurons and the nodes in SPNs, particularly when MAP inference is used. In this case the network is composed of max nodes and sum nodes (logs of products). (Cf. Riesenhuber and Poggio [23], which also uses max and sum nodes, but is not a probabilistic model.) Max nodes are analogous to inhibitory neurons in that they select the highest input for further propagation. Sum nodes are analogous to excitatory neurons in that they compute a sum of their inputs. In SPNs the weights are at the inputs of max nodes, while the analogy with the cortex suggests having them at the inputs of sum (log product) nodes. One can be mapped to the other if we let max nodes ignore their children's weights and consider only their values. Possible justifications for this include: (a) it potentially reduces computational cost by allowing max nodes to be merged; (b) ignoring priors may improve discriminative performance [11]; (c) priors may be approximately encoded by the number of units representing the same pattern, and this may facilitate online hard EM learning. Unlike SPNs, the cortex has no single root node, but it is straighforward to extend SPNs to have multiple roots, corresponding to simultaneously computing multiple distributions with shared structure. Of course, SPNs are still biologically unrealistic in</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>many ways, but they may nevertheless provide an interesting addition to the computational neuroscience toolkit.</p>
<h2>7 CONCLUSION</h2>
<p>Sum-product networks (SPNs) are DAGs of sums and products that efficiently compute partition functions and marginals of high-dimensional distributions, and can be learned by backpropagation and EM. SPNs can be viewed as a deep combination of mixture models and feature hierarchies. Inference in SPNs is faster and more accurate than in previous deep architectures. This in turn makes learning faster and more accurate. Our experiments indicate that, because of their robustness, SPNs require much less manual engineering than other deep architectures. Much remains to be explored, including other learning methods for SPNs, design principles for SPN architectures, extension to sequential domains, and further applications.</p>
<p>Acknowledgements We thank Ruslan Salakhutdinov for help in experiments with DBNs. This research was partly funded by ARO grant W911NF-08-1-0242, AFRL contract FA8750-09-C-0181, NSF grant IIS-0803481, and ONR grant N00014-08-10670. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ARO, AFRL, NSF, ONR, or the United States Government.</p>
<h2>References</h2>
<p>[1] R. Adams, H. Wallach and Z. Ghahramani. Learning the structure of deep sparse graphical models. In Proc. AISTATS-10, 2010.
[2] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2009.
[3] M. Bertalmio, G. Sapiro, V. Caselles and C. Ballester. Image inpainting. In Proc. SIGGRAPH-00, 2000.
[4] L. Breiman. Random forests. Machine Learning, 2001.
[5] A. Chechetka and C. Guestrin. Efficient principled learning of thin junction trees. In Proc. NIPS-08, 2008.
[6] M. Collins. Head-driven statistical models for natural language parsing. Computational Linguistics, 2003.
[7] A. Darwiche. A differential approach to inference in Bayesian networks. Journal of the ACM, 2003.
[8] R. Dechter and R. Mateescu. AND/OR search spaces for graphical models. Artificial Intelligence, 2006.
[9] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples. In Proc. CVPR Wkshp. on Generative Model-Based Vision, 2004.
[10] P. Felzenszwalb and D. McAllester. Object detection grammars. Tech. Rept., Dept. CS, Univ. Chicago, 2010.
[11] J. H. Friedman. On bias, variance, $0 / 1$ loss, and the curse of dimensionality. Data Mining and Knowledge Discovery, 1997.
[12] V. Gogate, W. Webb and P. Domingos. Learning efficient Markov networks. In Proc. NIPS-10, 2010.
[13] J. Hays and A. Efros. Scene completion using millions of photographs. In Proc. SIGGRAPH-07, 2007.
[14] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 2006.
[15] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1989.
[16] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proc. ICML-09, 2009.
[17] D. Lowd and P. Domingos. Naive Bayes models for probability estimation. In Proc. ICML-05, 2005.
[18] D. Lowd and P. Domingos. Learning arithmetic circuits. In Proc. UAI-08, 2008.
[19] D. McAllester, M. Collins, and F. Pereira. Case-factor diagrams for structured probabilistic modeling. In Proc. UAI04, 2004.
[20] R. Neal and G. Hinton. A view of the EM algorithm that justifies incremental, sparse, and other variants. In M. Jordan, editor, Learning in Graphical Models, Kluwer, 1998.
[21] G. Pagallo. Learning DNF by decision trees. In Proc. IJCAI89, 1989.
[22] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988.
[23] M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature Neuroscience, 1999.
[24] D. Roth and R. Samdani. Learning multi-linear representations of distributions for efficient inference. Machine Learning, 2009.
[25] D. Rumelhart, G. Hinton, and J. McClelland. A general framework for parallel distributed processing. In D. Rumelhart and J. McClelland, editors, Parallel Distributed Processing, vol. 1. MIT Press, 1986.
[26] D. Rumelhart, G. Hinton, and R. Williams. Learning internal representations by error propagation. In D. Rumelhart and J. McClelland, editors, Parallel Distributed Processing, vol. 1. MIT Press, 1986.
[27] D. Rumelhart and D. Zipser. Feature discovery by competitive learning. In D. E. Rumelhart and J. McClelland, editors, Parallel Distributed Processing, vol. 1. MIT Press, 1986.
[28] F. Samaria and A. Harter. Parameterisation of a stochastic model for human face identification. In Proc. 2nd IEEE Wkshp. on Applications of Computer Vision, 1994.
[29] R. Salakhutdinov and G. Hinton. Deep Boltzmann Machines. In Proc. AISTATS-09, 2009.
[30] R. Salakhutdinov and G. Hinton. An efficient learning procedure for deep Boltzmann machines. Tech. Rept., MIT CSAIL, 2010.
[31] M. Turk and A. Pentland. Eigenfaces for recognition. J. Cognitive Neuroscience, 1991.
[32] N. Zhang. Hierarchical latent class models for cluster analysis. JMLR, 2004.
[33] L. Zhu, Y. Chen, and A. Yuille. Unsupervised learning of probabilistic grammar-Markov models for object categories. IEEE Trans. PAMI, 2009.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We were unable to obtain their code for head-to-head comparison. We should note that the main purpose of their figure is to illustrate the importance of top-down inference.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ We should note that the main point of their results is to show that features in higher layers are more class-specific.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1411 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1411</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1411</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-db6965ab7ae91e7c81d5dc606861159f83bfd3a4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/db6965ab7ae91e7c81d5dc606861159f83bfd3a4" target="_blank">Vector Quantized Models for Planning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work significantly outperforms an offline version of MuZero on a stochastic interpretation of chess where the opponent is considered part of the environment and scales to DeepMind Lab, a first-person 3D environment with large visual observations and partial observability.</p>
                <p><strong>Paper Abstract:</strong> Recent developments in the field of model-based RL have proven successful in a range of environments, especially ones where planning is essential. However, such successes have been limited to deterministic fully-observed environments. We present a new approach that handles stochastic and partially-observable environments. Our key insight is to use discrete autoencoders to capture the multiple possible effects of an action in a stochastic environment. We use a stochastic variant of Monte Carlo tree search to plan over both the agent's actions and the discrete latent variables representing the environment's response. Our approach significantly outperforms an offline version of MuZero on a stochastic interpretation of chess where the opponent is considered part of the environment. We also show that our approach scales to DeepMind Lab, a first-person 3D environment with large visual observations and partial observability.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1411.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1411.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VQM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector-Quantized Planning Model (VQ Model / VQM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned discrete latent world model that combines a state VQVAE (discrete autoencoder) with a learned transition model and MCTS to plan over both agent actions and discrete latent variables representing environment stochasticity and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VQ Model (VQM / VQMMCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage learned world model: (1) a state VQVAE that encodes observations (and optionally action history) into discrete latent codes; (2) an autoregressive transition model (recurrent conv / transformer depending on domain) that alternates predicting agent actions and discrete latent codes, with prediction heads for policy (π), latent-code policy (τ), value (v) and reward (r); combined with a modified MCTS that branches over action nodes and stochastic (latent-code) nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete latent VQVAE + learned transition + planning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chess (symbolic board), DeepMind Lab (first-person 3D visual navigation), general planning/stochastic partially-observed RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss and prediction error measures: cross-entropy reconstruction loss for chess board (piece classification), Mean Squared Error (MSE) for pixel/frame reconstructions, PSNR for frame-level reconstruction quality, and Mean Best Reconstruction Error (MBRE) for long-horizon sampled-rollout fidelity. Training also optimizes policy/value/reward prediction losses (cross-entropy, value/rwd losses) similar to MuZero.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitative: achieves competitive MBRE and reconstruction performance vs continuous VAE baselines on DeepMind Lab (figures show lower MBRE); frame-level VQVAE decoder PSNR ~32 dB reported; deterministic LSTM baseline PSNR ~24 dB. In chess, VQHybrid/VQPure recover performance to the level of two-player MuZeroChess in the single-player (stochastic opponent-as-environment) setting (text: 'as well on single-player chess as MuZero does on two-player chess'). No single scalar end-to-end MSE/MBRE numbers are tabulated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: discrete latent codes provide an explicit, enumerable abstraction of future observations (cardinality and temporal granularity are design parameters), making the model's branching space and sampled outcomes more inspectable than unconstrained continuous latents; however, encoder/decoder and transition networks remain neural networks (black-box), so latent semantics beyond code indices are not guaranteed or fully explained.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Inspection of discrete latent code distributions during planning (policy τ over codes), visualization of sampled rollouts (Figure 7), and sampling/statistics methods (quasirandom sampling described in Appendix) are used; no systematic attribution or disentanglement procedures are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: state VQVAE and transition model trained in two stages (and for DeepMind Lab a frame-level VQVAE pretraining stage). Example costs reported: chess state VQVAE trained for 1.2M steps (batch 1024), transition models trained 200k–400k steps; architectures include deep residual conv stacks (e.g., 30-layer residual conv with 1024 hiddens for g'), codebooks sizes (chess: 2 codebooks × 64 codes × 256 dims; DeepMind Lab: frame-level 512 codes×64 dims; state-level 32 latents each with 512 codes×64 dims). Evaluation uses MCTS with simulation budgets up to 1200 simulations per move. Large dataset for DeepMind Lab: 101,325,000 episodes collected. Exact GPU/TPU counts and wall-clock times not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared qualitatively: VQM requires a pretraining phase (extra compute) but yields improved planning under stochasticity; MuZero with increased compute (threefold) showed only slight improvement and still failed in single-player setting. VQM's discrete latent search introduces extra branching (latent codes) which increases search complexity proportional to code cardinality, motivating a trade-off between cardinality (expressivity) and search cost. No exact FLOPs/throughput comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Chess: VQHybrid and VQPure (worst-case chance-node setting) recover performance comparable to two-player MuZeroChess on single-player (opponent-as-environment) chess; outperform single-player MuZero variants. DeepMind Lab: VQM obtains better MBRE than deterministic LSTM and competitive trade-off versus continuous VAE baselines (figures show substantially better best-match reconstructions over long horizons).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity reconstruction helps sample trajectories that match possible environment outcomes, and discrete latent sampling enables MCTS planning over stochastic outcomes; in chess this yields strong policy performance even when opponent moves are unobserved. Paper emphasizes that modeling multimodality (via discrete codes) improves task performance where the environment is stochastic or partially observed; high reconstruction quality alone (e.g., deterministic mean predictors) is insufficient for good planning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Explicit trade-offs discussed: larger latent-cardinality increases expressivity/fidelity but raises search branching and computational cost; lower cardinality yields lossy compression but cheaper search and possibly more abstract representations; increasing MCTS simulation budget can hurt deterministic MuZero in stochastic single-player chess (because model doesn't capture stochasticity) whereas VQM benefits from planning over discrete latents; adversarial vs neutral prior choice (ĤQ) affects performance—using prior knowledge (adversarial) improves chess results. Temporal abstraction (VQJumpy) can reduce planning frequency at cost of requiring accurate jumpy predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: use discrete VQVAE latents rather than continuous latents; two-stage training (state VQVAE then transition) with optional frame-level pretraining for high-dim visuals; alternate action and latent predictions in transition model (VQHybrid), or factorize action vs environment latents (VQPure), or make jumpy multi-step latents (VQJumpy); choose latent cardinality and number of latents per timestep (e.g., DeepMind Lab uses 32 latents per timestep each with 512 codes); choose adversarial/neutral/ cooperative setting for ĤQ at stochastic nodes in MCTS; use quasirandom sampling in stochastic nodes to better match P(k|s).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to MuZero: MuZero assumes determinism/full observability and plans only over actions—fails in single-player stochastic chess; VQM generalizes MuZero by planning over discrete latent outcomes and recovers performance. Compared to deterministic next-frame LSTM: VQM produces multimodal samples and far better MBRE/best-match reconstructions. Compared to sequential continuous VAE baselines: VQM attains a favorable trade-off between reconstruction quality (PSNR ~32 dB for frame-level VQVAE) and predictive/sample fidelity (MBRE), performing competitively or better in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper provides design heuristics rather than a single optimum: select latent cardinality balancing reconstruction fidelity vs search cost; prefer VQHybrid/VQPure depending on need to decouple action space or to plan entirely in discrete latent space; for adversarial domains use adversarial ĤQ in stochastic nodes; pretrain state/frame-level VQVAE to improve sample quality in high-dimensional visual domains; temporal abstraction (VQJumpy) recommended to reduce planning frequency but requires model support. No single numeric optimal config given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vector Quantized Models for Planning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1411.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1411.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>State VQVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>State Vector-Quantized Variational Autoencoder (State VQVAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional VQVAE encoder-decoder that maps sequences of past states and actions to a discrete latent code representing the next-state residual information, used as the atomic stochastic 'environment action' in planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>State VQVAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder takes s_{1:t+1}, a_{1:t} and outputs continuous latent z_u which is quantized via a codebook to a discrete index k_{t+1}; decoder reconstructs s_{t+1} conditioned on s_{1:t}, a_{1:t} and the quantized embedding e_{k}. Uses standard VQVAE training (reconstruction loss + commitment loss + straight-through gradient).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>discrete latent autoencoder (conditional VQVAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chess (symbolic board), DeepMind Lab (visual frames), general state representation for planning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss: cross-entropy for chess board piece prediction, MSE/PSNR for frame-level reconstructions; MBRE used for sampled rollout fidelity when composing with transition model.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Chess: state VQVAE architecture and quantization settings reported (chess variant used 2 codebooks of 64 codes×256 dims) and trained for 1.2M steps; DeepMind Lab: frame-level VQVAE decoder PSNR ~32 dB. Quantitative MBRE plots (Figure 8) show better sample-match performance vs baselines, but numerical MBRE values not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Discrete codes give an interpretable, enumerated set of possible next-state outcomes (indices can be inspected and sampled). However the semantics of codes are not explicitly decoded into human-interpretable attributes within the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of sampled frames decoded from codes (Figure 7) and inspection of code selection distributions during MCTS; no additional disentanglement or code-labeling is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Codebook sizes and architecture depths specified (e.g., chess VQVAE encoder/decoder 16-layer residual conv stacks); training steps and batch sizes given (e.g., chess: 1.2M steps, batch 1024). Exact wall-clock/GPU resource consumption not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Discrete latent compression reduces the need for pixel-level rollout reconstruction during search compared to full-frame predictors, enabling faster planning in principle; explicit comparisons of inference latency vs alternatives not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables VQM to capture multimodal next-state distributions; in chess combined with transition and MCTS yields parity with two-player MuZero in single-player setting; in DeepMind Lab it supports better MBRE than deterministic baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>By capturing residual information of next states into discrete codes, the state VQVAE provides a compact set of plausible outcomes that the planner can branch over; this helps planning under stochasticity where deterministic predictors fail.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Choosing codebook cardinality trades fidelity against search cost: larger cardinality yields better reconstruction but increases branching during MCTS and computational cost; authors study cardinality effect in supplement (qualitative comment).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Conditional VQVAE (condition on past states/actions), choice of number of codebooks and codes per book, straight-through gradient for quantization, exponential moving average updates for codebook embeddings, spatial mean pooling before quantization for chess.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to continuous-latent sequential VAEs and deterministic next-frame predictors: discrete VQVAE yields multimodal discrete outcomes enabling better best-match rollouts (MBRE) and more useful branching for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends picking cardinality to balance expressivity and search cost; for large visual domains a frame-level VQVAE pretraining stage followed by state-level VQVAE is effective; no single numeric optimum is given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vector Quantized Models for Planning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1411.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1411.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transition Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoregressive Transition Model with discrete-latent head</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned dynamics model that alternates between predicting agent actions and discrete latent codes (τ) and also predicts value and reward for each unroll step, permitting MCTS to expand both action and stochastic latent branches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VQ Transition Model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent convolutional or transformer-based model that, given a root state embedding, unrolls a planning path s, a^0, k^1, a^1, ... up to depth M. At even steps predicts agent action policy π and reward/Q estimates; at odd steps predicts discrete-latent policy τ and value. Trained with teacher forcing on offline trajectories with combined cross-entropy losses for actions and latents plus value and reward losses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>learned neural transition model (autoregressive over actions and discrete latents)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chess, DeepMind Lab, general planning/ offline RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Policy and latent cross-entropy losses, value and reward prediction losses; end-to-end MBRE for sampled rollouts when combined with state VQVAE.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Enables planning over environment stochasticity; in chess VQHybrid/VQPure with this transition model match two-player MuZero performance in single-player setting; in DeepMind Lab it supports sampling trajectories with lower MBRE than deterministic and comparable to continuous VAE baselines. No single numerical prediction-error for transition provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Architecture exposes discrete latent-policy τ that can be inspected during planning; however the internal state representations remain neural and not directly interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Inspection of τ distributions in MCTS, use of adversarial/neutral/cooperative priors (ĤQ) at stochastic nodes to encode environment assumptions; no further interpretability techniques described.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Architectural descriptions and training steps provided (e.g., chess g' is 30-layer residual conv with 1024 hiddens; DeepMind Lab uses a causal Transformer for transitions). Training iterations reported (e.g., VQHybrid 200k steps, VQPure 400k). Exact compute/GPU usage not given.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to MuZero's deterministic transition model, VQ transition model adds extra prediction head τ and has to handle discrete-code branching in MCTS, increasing search complexity; but it enables planning under stochasticity where MuZero fails. No quantitative efficiency-speed tradeoff numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Critical for enabling VQM to plan over stochastic outcomes; improves robustness of planning in partially observed/stochastic domains (chess single-player, DeepMind Lab).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Predicting discrete latents allows MCTS to capture multimodal futures; value estimations after latent nodes represent state values whereas after action nodes represent Q-values, aligning model outputs with planning needs.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Adding latent predictions increases branching factor and planning cost; using fewer codes reduces cost but hurts fidelity; adversarial/neutral choices for ĤQ change behavior and outcome quality.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Alternate action and latent predictions; train with teacher-forcing on offline data; include τ head; use different dynamics functions for action-driven steps g and latent-driven steps g' (chess); choice of recurrence type (RNN/convs vs Transformer) per domain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to MuZero's transition, VQ transition models explicitly model environment stochasticity via discrete codes and outperform MuZero in single-player stochastic scenarios; compared to continuous-latent transitions, discrete latent transitions make MCTS straightforward to apply.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests matching planning path variant (VQHybrid, VQPure, VQJumpy) to the representation learned by the state VQVAE and selecting code cardinality and planning depth M appropriately. No single fixed optimal config is prescribed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vector Quantized Models for Planning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1411.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1411.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Frame-level VQVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frame-level VQVAE (pixel-level discrete autoencoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A feed-forward VQVAE trained to compress individual visual frames into discrete frame-level codes, used as a base layer for a higher-level state VQVAE in visual domains (DeepMind Lab).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Frame-level VQVAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Convolutional encoder/decoder with a quantization codebook (DeepMind Lab setup: 512 codes × 64 dims) that maps raw pixels to discrete frame codes; used in a hierarchical stack where the state VQVAE decodes to frame-level codes which are then decoded to pixels by the frame-level decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>discrete latent autoencoder (pixel-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>DeepMind Lab first-person visual environment</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>PSNR for frame reconstruction (~32 dB for the frame-level VQVAE decoder), and MBRE for long-horizon sampled trajectory best-matches.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Frame-level decoder PSNR reported ~32 dB; deterministic LSTM baseline had PSNR ~24 dB for reference. VQ approach shows better MBRE than deterministic LSTM and competitive trade-offs vs continuous VAE baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Discrete frame codes provide a compressive, indexable representation of visual frames; decoded samples are visualized (Figure 7), aiding qualitative interpretation of model outputs but not necessarily per-code semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of decoded sampled frames and MBRE-based best-match selection; no explicit per-code labeling or disentangling.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Codebook size (512×64), conv encoder/decoder training in initial stage prior to state-level VQVAE; exact training compute not specified. Used as a way to reduce pixel-level decoding cost during transition sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Hierarchical use of frame-level VQVAE plus state-level VQVAE reduces the cost of full-resolution pixel generation during planning, compared to naively using pixel predictors at every step. No wall-clock or throughput numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Improves sample quality and supports generation of varied plausible rollouts for DeepMind Lab, contributing to lower MBRE when composing with state-level VQVAE and transition model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Provides a compact discrete representation of frames used by the higher-level state model, which helps generate realistic visual rollouts while keeping decode cost manageable.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Larger codebook improves fidelity (PSNR) but increases embedding memory and possible decode cost; hierarchical setup adds modeling complexity but pays off in sample quality.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>512 codes of 64 dimensions for frame VQ in DeepMind Lab; pretrain frame-level VQVAE before state-level training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms deterministic next-frame LSTM and provides a different fidelity/efficiency trade-off compared to continuous-latent VAEs (which were trained with GECO at target distortion levels).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests frame-level pretraining and a sufficiently large codebook (example: 512 codes) to achieve ~32 dB PSNR; choice should balance memory and reconstruction goals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vector Quantized Models for Planning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1411.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1411.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deterministic LSTM baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deterministic next-frame LSTM predictor (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic recurrent model (LSTM + conv torso) trained to reconstruct/predict next frames with MSE loss; used as a baseline to show the need for stochastic/multimodal modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deterministic LSTM next-frame predictor</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LSTM-based sequential predictor with convolutional encoder/decoder, trained with mean-squared-error to reconstruct each frame given preceding frames; lacks explicit latent stochasticity or multimodality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>deterministic pixel predictor (non-probabilistic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>DeepMind Lab visual prediction baseline</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>MSE (Mean Squared Error) and PSNR for frame reconstructions; MBRE used to evaluate best-match rollouts sampled from stochastic models vs deterministic baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported reconstruction PSNR ~24 dB (worse than frame-level VQVAE ~32 dB), and performs significantly worse in MBRE evaluation (figure shows much higher MBRE than latent-variable models).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Standard neural predictor; no additional interpretability described.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Architecture mirrors state VQVAE except for quantization; training and inference cost unspecified. Likely lower search cost since deterministic but poor sample diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Deterministic models are simpler but produce mean predictions that poorly capture multimodality, leading to poor best-match samples for stochastic environments; VQ latent models outperform in MBRE despite additional complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Produces blurry/averaged predictions and fails to produce plausible alternative rollouts—poor MBRE and lower PSNR relative to VQVAE-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Deterministic prediction is insufficient for planning in multimodal stochastic domains because it collapses multi-possible futures to an average, reducing usefulness for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simplicity vs expressivity: deterministic predictor is computationally simpler but lacks multimodal generation capability required for good planning under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Standard LSTM + conv predictor trained with MSE; used as a simple baseline to demonstrate necessity of discrete/multimodal latent structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Performs worse than VQ-based discrete latent models and continuous sequential VAEs in MBRE and PSNR metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not applicable; authors use this baseline to show deficiencies of deterministic predictors in stochastic/multimodal environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vector Quantized Models for Planning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1411.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1411.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequential VAE baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Continuous VAE Baselines (GECO-trained variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequential continuous-latent variational autoencoder baselines trained with GECO to enforce reconstruction distortion targets (different PSNR levels) to compare continuous-latent approaches to discrete VQVAE approaches on prediction and sampling fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sequential continuous VAE baselines (trained with GECO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variational sequential generative models (sequential VAEs) with continuous latents and expressive posterior/prior configurations; trained under GECO (Lagrangian constrained optimization) to meet target average distortion tolerances (PSNR targets of 25 dB and 33 dB used).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (continuous variational)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>DeepMind Lab visual rollouts and MBRE evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>PSNR (target distortion levels), MBRE for sampled rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Two variants trained for distortion targets 25 dB and 33 dB; results show trade-offs: higher reconstruction fidelity can hurt long-range sample-match (MBRE) performance; VQM shown to achieve a competitive balance against these baselines (Figure 8). Exact MBRE numeric values not given in text.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Continuous latent VAEs are generally black-box; no interpretability methods are described in paper for these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned beyond standard evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>GECO optimization and flexible posterior/prior imply nontrivial training, but concrete compute costs not reported. Likely comparable to VQ approaches for similar model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>VQ discrete approach achieves competitive reconstruction and often better sample-match (MBRE) trade-offs compared to continuous sequential VAEs under the GECO settings used; exact compute/efficiency comparisons not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Performs worse than VQM in MBRE in experiments (figures indicate VQM is competitive or superior across horizons); continuous VAEs show a trade-off curve between distortion and predictive matching.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Continuous VAEs can achieve high-fidelity reconstructions but may underperform in sampling plausible multimodal long-horizon trajectories compared to discrete VQ approaches, depending on reconstruction constraint choice.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Tuning reconstruction tolerance (GECO) demonstrates a trade-off between per-frame reconstruction fidelity and long-horizon predictive/sampling performance; discrete latents capture multimodality more directly.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>GECO-based constrained optimization with two distortion levels tested (25 dB and 33 dB); selection affects predictive sampling utility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly to VQ approach in MBRE plots: VQ shows favorable balance; deterministic LSTM baseline is worse.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No single optimal setting reported; authors illustrate a continuum of distortion targets and show that VQ provides a practical alternative balancing fidelity and sampling utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vector Quantized Models for Planning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1411.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1411.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero (learned model for planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based planning algorithm that learns an implicit deterministic dynamics and reward/value/policy predictors to perform MCTS without access to a simulator; used as a baseline and point of comparison throughout the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a sequential deterministic state representation and transition models that, given a root state and a sequence of agent (and opponent) actions, predicts hidden states h^m and outputs policy π, value v, and reward r at each step; used with MCTS where π provides priors and v estimates leaf values.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (implicit deterministic transition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Two-player games (chess, go, shogi), single-player Atari (where stochasticity is limited); general model-based planning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Policy cross-entropy, value/reward prediction losses; task performance measured via MCTS-based play strength (win/draw rates) against engines like Stockfish and other agents.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Two-player MuZeroChess achieves strong performance in two-player chess setting; single-player MuZero (without opponent actions observed) performs poorly on single-player (stochastic-opponent) chess as reported in this paper (performance drops catastrophically). Increasing MuZero compute (MuZero@600k) yields only slight improvements but does not overcome single-player failure.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>MuZero uses learned hidden states which are neural and not explicitly interpretable; planning is over actions only and does not expose explicit stochastic outcome codes.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper beyond standard use of policy/value heads as priors/estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Typical MuZero training/inference costs are heavy; in this paper MuZero baselines were trained for up to 600k steps in larger-compute ablation. MCTS budgets used up to 1200 simulations per move in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>MuZero is efficient when environment is deterministic and fully observed, but fails in the stochastic/partially-observed single-player chess setting where VQM (with discrete latents) succeeds; MuZero with more compute gives only limited improvement for this failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong in deterministic two-player chess (as prior work shows); fails when opponent moves are unobserved (single-player chess) — performance degrades as simulation budget increases due to incorrect handling of stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuZero's deterministic implicit dynamics suffice when transitions are effectively deterministic and fully observed, but its planning over actions only lacks mechanisms to account for unobserved stochastic environment responses; therefore high predictive fidelity (deterministic predictions) does not translate into robust planning under unobserved stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Deterministic learned dynamics are simpler and effective in deterministic domains, but they trade off robustness to stochasticity and partial observability compared to stochastic/discrete-latent models like VQM.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>MuZero interleaves player and opponent actions in planning for two-player games; in single-player use-case it plans only over player actions. This design choice limits its applicability when opponent/environment behavior is unobserved.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper compares MuZero to VQM: MuZero performs worse in single-player stochastic chess while VQM matches two-player MuZero performance by explicitly modeling environment stochasticity with discrete latents.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>For MuZero, the optimal regime is deterministic, fully-observed environments where planning over agent actions suffices. The paper suggests extending MuZero-style planning with stochastic latent predictions (as VQM does) for stochastic/partially-observed domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vector Quantized Models for Planning', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural discrete representation learning <em>(Rating: 2)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 1)</em></li>
                <li>Mastering atari with discrete world models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1411",
    "paper_id": "paper-db6965ab7ae91e7c81d5dc606861159f83bfd3a4",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "VQM",
            "name_full": "Vector-Quantized Planning Model (VQ Model / VQM)",
            "brief_description": "A learned discrete latent world model that combines a state VQVAE (discrete autoencoder) with a learned transition model and MCTS to plan over both agent actions and discrete latent variables representing environment stochasticity and partial observability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VQ Model (VQM / VQMMCTS)",
            "model_description": "Two-stage learned world model: (1) a state VQVAE that encodes observations (and optionally action history) into discrete latent codes; (2) an autoregressive transition model (recurrent conv / transformer depending on domain) that alternates predicting agent actions and discrete latent codes, with prediction heads for policy (π), latent-code policy (τ), value (v) and reward (r); combined with a modified MCTS that branches over action nodes and stochastic (latent-code) nodes.",
            "model_type": "latent world model (discrete latent VQVAE + learned transition + planning)",
            "task_domain": "Chess (symbolic board), DeepMind Lab (first-person 3D visual navigation), general planning/stochastic partially-observed RL",
            "fidelity_metric": "Reconstruction loss and prediction error measures: cross-entropy reconstruction loss for chess board (piece classification), Mean Squared Error (MSE) for pixel/frame reconstructions, PSNR for frame-level reconstruction quality, and Mean Best Reconstruction Error (MBRE) for long-horizon sampled-rollout fidelity. Training also optimizes policy/value/reward prediction losses (cross-entropy, value/rwd losses) similar to MuZero.",
            "fidelity_performance": "Qualitative: achieves competitive MBRE and reconstruction performance vs continuous VAE baselines on DeepMind Lab (figures show lower MBRE); frame-level VQVAE decoder PSNR ~32 dB reported; deterministic LSTM baseline PSNR ~24 dB. In chess, VQHybrid/VQPure recover performance to the level of two-player MuZeroChess in the single-player (stochastic opponent-as-environment) setting (text: 'as well on single-player chess as MuZero does on two-player chess'). No single scalar end-to-end MSE/MBRE numbers are tabulated in the main text.",
            "interpretability_assessment": "Partially interpretable: discrete latent codes provide an explicit, enumerable abstraction of future observations (cardinality and temporal granularity are design parameters), making the model's branching space and sampled outcomes more inspectable than unconstrained continuous latents; however, encoder/decoder and transition networks remain neural networks (black-box), so latent semantics beyond code indices are not guaranteed or fully explained.",
            "interpretability_method": "Inspection of discrete latent code distributions during planning (policy τ over codes), visualization of sampled rollouts (Figure 7), and sampling/statistics methods (quasirandom sampling described in Appendix) are used; no systematic attribution or disentanglement procedures are reported.",
            "computational_cost": "Training: state VQVAE and transition model trained in two stages (and for DeepMind Lab a frame-level VQVAE pretraining stage). Example costs reported: chess state VQVAE trained for 1.2M steps (batch 1024), transition models trained 200k–400k steps; architectures include deep residual conv stacks (e.g., 30-layer residual conv with 1024 hiddens for g'), codebooks sizes (chess: 2 codebooks × 64 codes × 256 dims; DeepMind Lab: frame-level 512 codes×64 dims; state-level 32 latents each with 512 codes×64 dims). Evaluation uses MCTS with simulation budgets up to 1200 simulations per move. Large dataset for DeepMind Lab: 101,325,000 episodes collected. Exact GPU/TPU counts and wall-clock times not provided.",
            "efficiency_comparison": "Compared qualitatively: VQM requires a pretraining phase (extra compute) but yields improved planning under stochasticity; MuZero with increased compute (threefold) showed only slight improvement and still failed in single-player setting. VQM's discrete latent search introduces extra branching (latent codes) which increases search complexity proportional to code cardinality, motivating a trade-off between cardinality (expressivity) and search cost. No exact FLOPs/throughput comparisons provided.",
            "task_performance": "Chess: VQHybrid and VQPure (worst-case chance-node setting) recover performance comparable to two-player MuZeroChess on single-player (opponent-as-environment) chess; outperform single-player MuZero variants. DeepMind Lab: VQM obtains better MBRE than deterministic LSTM and competitive trade-off versus continuous VAE baselines (figures show substantially better best-match reconstructions over long horizons).",
            "task_utility_analysis": "High-fidelity reconstruction helps sample trajectories that match possible environment outcomes, and discrete latent sampling enables MCTS planning over stochastic outcomes; in chess this yields strong policy performance even when opponent moves are unobserved. Paper emphasizes that modeling multimodality (via discrete codes) improves task performance where the environment is stochastic or partially observed; high reconstruction quality alone (e.g., deterministic mean predictors) is insufficient for good planning.",
            "tradeoffs_observed": "Explicit trade-offs discussed: larger latent-cardinality increases expressivity/fidelity but raises search branching and computational cost; lower cardinality yields lossy compression but cheaper search and possibly more abstract representations; increasing MCTS simulation budget can hurt deterministic MuZero in stochastic single-player chess (because model doesn't capture stochasticity) whereas VQM benefits from planning over discrete latents; adversarial vs neutral prior choice (ĤQ) affects performance—using prior knowledge (adversarial) improves chess results. Temporal abstraction (VQJumpy) can reduce planning frequency at cost of requiring accurate jumpy predictions.",
            "design_choices": "Key choices: use discrete VQVAE latents rather than continuous latents; two-stage training (state VQVAE then transition) with optional frame-level pretraining for high-dim visuals; alternate action and latent predictions in transition model (VQHybrid), or factorize action vs environment latents (VQPure), or make jumpy multi-step latents (VQJumpy); choose latent cardinality and number of latents per timestep (e.g., DeepMind Lab uses 32 latents per timestep each with 512 codes); choose adversarial/neutral/ cooperative setting for ĤQ at stochastic nodes in MCTS; use quasirandom sampling in stochastic nodes to better match P(k|s).",
            "comparison_to_alternatives": "Compared to MuZero: MuZero assumes determinism/full observability and plans only over actions—fails in single-player stochastic chess; VQM generalizes MuZero by planning over discrete latent outcomes and recovers performance. Compared to deterministic next-frame LSTM: VQM produces multimodal samples and far better MBRE/best-match reconstructions. Compared to sequential continuous VAE baselines: VQM attains a favorable trade-off between reconstruction quality (PSNR ~32 dB for frame-level VQVAE) and predictive/sample fidelity (MBRE), performing competitively or better in experiments.",
            "optimal_configuration": "Paper provides design heuristics rather than a single optimum: select latent cardinality balancing reconstruction fidelity vs search cost; prefer VQHybrid/VQPure depending on need to decouple action space or to plan entirely in discrete latent space; for adversarial domains use adversarial ĤQ in stochastic nodes; pretrain state/frame-level VQVAE to improve sample quality in high-dimensional visual domains; temporal abstraction (VQJumpy) recommended to reduce planning frequency but requires model support. No single numeric optimal config given.",
            "uuid": "e1411.0",
            "source_info": {
                "paper_title": "Vector Quantized Models for Planning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "State VQVAE",
            "name_full": "State Vector-Quantized Variational Autoencoder (State VQVAE)",
            "brief_description": "A conditional VQVAE encoder-decoder that maps sequences of past states and actions to a discrete latent code representing the next-state residual information, used as the atomic stochastic 'environment action' in planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "State VQVAE",
            "model_description": "Encoder takes s_{1:t+1}, a_{1:t} and outputs continuous latent z_u which is quantized via a codebook to a discrete index k_{t+1}; decoder reconstructs s_{t+1} conditioned on s_{1:t}, a_{1:t} and the quantized embedding e_{k}. Uses standard VQVAE training (reconstruction loss + commitment loss + straight-through gradient).",
            "model_type": "discrete latent autoencoder (conditional VQVAE)",
            "task_domain": "Chess (symbolic board), DeepMind Lab (visual frames), general state representation for planning",
            "fidelity_metric": "Reconstruction loss: cross-entropy for chess board piece prediction, MSE/PSNR for frame-level reconstructions; MBRE used for sampled rollout fidelity when composing with transition model.",
            "fidelity_performance": "Chess: state VQVAE architecture and quantization settings reported (chess variant used 2 codebooks of 64 codes×256 dims) and trained for 1.2M steps; DeepMind Lab: frame-level VQVAE decoder PSNR ~32 dB. Quantitative MBRE plots (Figure 8) show better sample-match performance vs baselines, but numerical MBRE values not tabulated in text.",
            "interpretability_assessment": "Discrete codes give an interpretable, enumerated set of possible next-state outcomes (indices can be inspected and sampled). However the semantics of codes are not explicitly decoded into human-interpretable attributes within the paper.",
            "interpretability_method": "Visualization of sampled frames decoded from codes (Figure 7) and inspection of code selection distributions during MCTS; no additional disentanglement or code-labeling is reported.",
            "computational_cost": "Codebook sizes and architecture depths specified (e.g., chess VQVAE encoder/decoder 16-layer residual conv stacks); training steps and batch sizes given (e.g., chess: 1.2M steps, batch 1024). Exact wall-clock/GPU resource consumption not reported.",
            "efficiency_comparison": "Discrete latent compression reduces the need for pixel-level rollout reconstruction during search compared to full-frame predictors, enabling faster planning in principle; explicit comparisons of inference latency vs alternatives not provided.",
            "task_performance": "Enables VQM to capture multimodal next-state distributions; in chess combined with transition and MCTS yields parity with two-player MuZero in single-player setting; in DeepMind Lab it supports better MBRE than deterministic baselines.",
            "task_utility_analysis": "By capturing residual information of next states into discrete codes, the state VQVAE provides a compact set of plausible outcomes that the planner can branch over; this helps planning under stochasticity where deterministic predictors fail.",
            "tradeoffs_observed": "Choosing codebook cardinality trades fidelity against search cost: larger cardinality yields better reconstruction but increases branching during MCTS and computational cost; authors study cardinality effect in supplement (qualitative comment).",
            "design_choices": "Conditional VQVAE (condition on past states/actions), choice of number of codebooks and codes per book, straight-through gradient for quantization, exponential moving average updates for codebook embeddings, spatial mean pooling before quantization for chess.",
            "comparison_to_alternatives": "Compared to continuous-latent sequential VAEs and deterministic next-frame predictors: discrete VQVAE yields multimodal discrete outcomes enabling better best-match rollouts (MBRE) and more useful branching for planning.",
            "optimal_configuration": "Paper recommends picking cardinality to balance expressivity and search cost; for large visual domains a frame-level VQVAE pretraining stage followed by state-level VQVAE is effective; no single numeric optimum is given.",
            "uuid": "e1411.1",
            "source_info": {
                "paper_title": "Vector Quantized Models for Planning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Transition Model",
            "name_full": "Autoregressive Transition Model with discrete-latent head",
            "brief_description": "A learned dynamics model that alternates between predicting agent actions and discrete latent codes (τ) and also predicts value and reward for each unroll step, permitting MCTS to expand both action and stochastic latent branches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VQ Transition Model",
            "model_description": "Recurrent convolutional or transformer-based model that, given a root state embedding, unrolls a planning path s, a^0, k^1, a^1, ... up to depth M. At even steps predicts agent action policy π and reward/Q estimates; at odd steps predicts discrete-latent policy τ and value. Trained with teacher forcing on offline trajectories with combined cross-entropy losses for actions and latents plus value and reward losses.",
            "model_type": "learned neural transition model (autoregressive over actions and discrete latents)",
            "task_domain": "Chess, DeepMind Lab, general planning/ offline RL",
            "fidelity_metric": "Policy and latent cross-entropy losses, value and reward prediction losses; end-to-end MBRE for sampled rollouts when combined with state VQVAE.",
            "fidelity_performance": "Enables planning over environment stochasticity; in chess VQHybrid/VQPure with this transition model match two-player MuZero performance in single-player setting; in DeepMind Lab it supports sampling trajectories with lower MBRE than deterministic and comparable to continuous VAE baselines. No single numerical prediction-error for transition provided.",
            "interpretability_assessment": "Architecture exposes discrete latent-policy τ that can be inspected during planning; however the internal state representations remain neural and not directly interpretable.",
            "interpretability_method": "Inspection of τ distributions in MCTS, use of adversarial/neutral/cooperative priors (ĤQ) at stochastic nodes to encode environment assumptions; no further interpretability techniques described.",
            "computational_cost": "Architectural descriptions and training steps provided (e.g., chess g' is 30-layer residual conv with 1024 hiddens; DeepMind Lab uses a causal Transformer for transitions). Training iterations reported (e.g., VQHybrid 200k steps, VQPure 400k). Exact compute/GPU usage not given.",
            "efficiency_comparison": "Compared to MuZero's deterministic transition model, VQ transition model adds extra prediction head τ and has to handle discrete-code branching in MCTS, increasing search complexity; but it enables planning under stochasticity where MuZero fails. No quantitative efficiency-speed tradeoff numbers given.",
            "task_performance": "Critical for enabling VQM to plan over stochastic outcomes; improves robustness of planning in partially observed/stochastic domains (chess single-player, DeepMind Lab).",
            "task_utility_analysis": "Predicting discrete latents allows MCTS to capture multimodal futures; value estimations after latent nodes represent state values whereas after action nodes represent Q-values, aligning model outputs with planning needs.",
            "tradeoffs_observed": "Adding latent predictions increases branching factor and planning cost; using fewer codes reduces cost but hurts fidelity; adversarial/neutral choices for ĤQ change behavior and outcome quality.",
            "design_choices": "Alternate action and latent predictions; train with teacher-forcing on offline data; include τ head; use different dynamics functions for action-driven steps g and latent-driven steps g' (chess); choice of recurrence type (RNN/convs vs Transformer) per domain.",
            "comparison_to_alternatives": "Compared to MuZero's transition, VQ transition models explicitly model environment stochasticity via discrete codes and outperform MuZero in single-player stochastic scenarios; compared to continuous-latent transitions, discrete latent transitions make MCTS straightforward to apply.",
            "optimal_configuration": "Paper suggests matching planning path variant (VQHybrid, VQPure, VQJumpy) to the representation learned by the state VQVAE and selecting code cardinality and planning depth M appropriately. No single fixed optimal config is prescribed.",
            "uuid": "e1411.2",
            "source_info": {
                "paper_title": "Vector Quantized Models for Planning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Frame-level VQVAE",
            "name_full": "Frame-level VQVAE (pixel-level discrete autoencoder)",
            "brief_description": "A feed-forward VQVAE trained to compress individual visual frames into discrete frame-level codes, used as a base layer for a higher-level state VQVAE in visual domains (DeepMind Lab).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Frame-level VQVAE",
            "model_description": "Convolutional encoder/decoder with a quantization codebook (DeepMind Lab setup: 512 codes × 64 dims) that maps raw pixels to discrete frame codes; used in a hierarchical stack where the state VQVAE decodes to frame-level codes which are then decoded to pixels by the frame-level decoder.",
            "model_type": "discrete latent autoencoder (pixel-level)",
            "task_domain": "DeepMind Lab first-person visual environment",
            "fidelity_metric": "PSNR for frame reconstruction (~32 dB for the frame-level VQVAE decoder), and MBRE for long-horizon sampled trajectory best-matches.",
            "fidelity_performance": "Frame-level decoder PSNR reported ~32 dB; deterministic LSTM baseline had PSNR ~24 dB for reference. VQ approach shows better MBRE than deterministic LSTM and competitive trade-offs vs continuous VAE baselines.",
            "interpretability_assessment": "Discrete frame codes provide a compressive, indexable representation of visual frames; decoded samples are visualized (Figure 7), aiding qualitative interpretation of model outputs but not necessarily per-code semantics.",
            "interpretability_method": "Visualization of decoded sampled frames and MBRE-based best-match selection; no explicit per-code labeling or disentangling.",
            "computational_cost": "Codebook size (512×64), conv encoder/decoder training in initial stage prior to state-level VQVAE; exact training compute not specified. Used as a way to reduce pixel-level decoding cost during transition sampling.",
            "efficiency_comparison": "Hierarchical use of frame-level VQVAE plus state-level VQVAE reduces the cost of full-resolution pixel generation during planning, compared to naively using pixel predictors at every step. No wall-clock or throughput numbers provided.",
            "task_performance": "Improves sample quality and supports generation of varied plausible rollouts for DeepMind Lab, contributing to lower MBRE when composing with state-level VQVAE and transition model.",
            "task_utility_analysis": "Provides a compact discrete representation of frames used by the higher-level state model, which helps generate realistic visual rollouts while keeping decode cost manageable.",
            "tradeoffs_observed": "Larger codebook improves fidelity (PSNR) but increases embedding memory and possible decode cost; hierarchical setup adds modeling complexity but pays off in sample quality.",
            "design_choices": "512 codes of 64 dimensions for frame VQ in DeepMind Lab; pretrain frame-level VQVAE before state-level training.",
            "comparison_to_alternatives": "Outperforms deterministic next-frame LSTM and provides a different fidelity/efficiency trade-off compared to continuous-latent VAEs (which were trained with GECO at target distortion levels).",
            "optimal_configuration": "Paper suggests frame-level pretraining and a sufficiently large codebook (example: 512 codes) to achieve ~32 dB PSNR; choice should balance memory and reconstruction goals.",
            "uuid": "e1411.3",
            "source_info": {
                "paper_title": "Vector Quantized Models for Planning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Deterministic LSTM baseline",
            "name_full": "Deterministic next-frame LSTM predictor (baseline)",
            "brief_description": "A deterministic recurrent model (LSTM + conv torso) trained to reconstruct/predict next frames with MSE loss; used as a baseline to show the need for stochastic/multimodal modeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Deterministic LSTM next-frame predictor",
            "model_description": "LSTM-based sequential predictor with convolutional encoder/decoder, trained with mean-squared-error to reconstruct each frame given preceding frames; lacks explicit latent stochasticity or multimodality.",
            "model_type": "deterministic pixel predictor (non-probabilistic)",
            "task_domain": "DeepMind Lab visual prediction baseline",
            "fidelity_metric": "MSE (Mean Squared Error) and PSNR for frame reconstructions; MBRE used to evaluate best-match rollouts sampled from stochastic models vs deterministic baseline.",
            "fidelity_performance": "Reported reconstruction PSNR ~24 dB (worse than frame-level VQVAE ~32 dB), and performs significantly worse in MBRE evaluation (figure shows much higher MBRE than latent-variable models).",
            "interpretability_assessment": "Standard neural predictor; no additional interpretability described.",
            "interpretability_method": "None mentioned.",
            "computational_cost": "Architecture mirrors state VQVAE except for quantization; training and inference cost unspecified. Likely lower search cost since deterministic but poor sample diversity.",
            "efficiency_comparison": "Deterministic models are simpler but produce mean predictions that poorly capture multimodality, leading to poor best-match samples for stochastic environments; VQ latent models outperform in MBRE despite additional complexity.",
            "task_performance": "Produces blurry/averaged predictions and fails to produce plausible alternative rollouts—poor MBRE and lower PSNR relative to VQVAE-based models.",
            "task_utility_analysis": "Deterministic prediction is insufficient for planning in multimodal stochastic domains because it collapses multi-possible futures to an average, reducing usefulness for planning.",
            "tradeoffs_observed": "Simplicity vs expressivity: deterministic predictor is computationally simpler but lacks multimodal generation capability required for good planning under uncertainty.",
            "design_choices": "Standard LSTM + conv predictor trained with MSE; used as a simple baseline to demonstrate necessity of discrete/multimodal latent structure.",
            "comparison_to_alternatives": "Performs worse than VQ-based discrete latent models and continuous sequential VAEs in MBRE and PSNR metrics.",
            "optimal_configuration": "Not applicable; authors use this baseline to show deficiencies of deterministic predictors in stochastic/multimodal environments.",
            "uuid": "e1411.4",
            "source_info": {
                "paper_title": "Vector Quantized Models for Planning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Sequential VAE baselines",
            "name_full": "Sequential Continuous VAE Baselines (GECO-trained variants)",
            "brief_description": "Sequential continuous-latent variational autoencoder baselines trained with GECO to enforce reconstruction distortion targets (different PSNR levels) to compare continuous-latent approaches to discrete VQVAE approaches on prediction and sampling fidelity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Sequential continuous VAE baselines (trained with GECO)",
            "model_description": "Variational sequential generative models (sequential VAEs) with continuous latents and expressive posterior/prior configurations; trained under GECO (Lagrangian constrained optimization) to meet target average distortion tolerances (PSNR targets of 25 dB and 33 dB used).",
            "model_type": "latent world model (continuous variational)",
            "task_domain": "DeepMind Lab visual rollouts and MBRE evaluation",
            "fidelity_metric": "PSNR (target distortion levels), MBRE for sampled rollouts.",
            "fidelity_performance": "Two variants trained for distortion targets 25 dB and 33 dB; results show trade-offs: higher reconstruction fidelity can hurt long-range sample-match (MBRE) performance; VQM shown to achieve a competitive balance against these baselines (Figure 8). Exact MBRE numeric values not given in text.",
            "interpretability_assessment": "Continuous latent VAEs are generally black-box; no interpretability methods are described in paper for these baselines.",
            "interpretability_method": "None mentioned beyond standard evaluation metrics.",
            "computational_cost": "GECO optimization and flexible posterior/prior imply nontrivial training, but concrete compute costs not reported. Likely comparable to VQ approaches for similar model sizes.",
            "efficiency_comparison": "VQ discrete approach achieves competitive reconstruction and often better sample-match (MBRE) trade-offs compared to continuous sequential VAEs under the GECO settings used; exact compute/efficiency comparisons not quantified.",
            "task_performance": "Performs worse than VQM in MBRE in experiments (figures indicate VQM is competitive or superior across horizons); continuous VAEs show a trade-off curve between distortion and predictive matching.",
            "task_utility_analysis": "Continuous VAEs can achieve high-fidelity reconstructions but may underperform in sampling plausible multimodal long-horizon trajectories compared to discrete VQ approaches, depending on reconstruction constraint choice.",
            "tradeoffs_observed": "Tuning reconstruction tolerance (GECO) demonstrates a trade-off between per-frame reconstruction fidelity and long-horizon predictive/sampling performance; discrete latents capture multimodality more directly.",
            "design_choices": "GECO-based constrained optimization with two distortion levels tested (25 dB and 33 dB); selection affects predictive sampling utility.",
            "comparison_to_alternatives": "Compared directly to VQ approach in MBRE plots: VQ shows favorable balance; deterministic LSTM baseline is worse.",
            "optimal_configuration": "No single optimal setting reported; authors illustrate a continuum of distortion targets and show that VQ provides a practical alternative balancing fidelity and sampling utility.",
            "uuid": "e1411.5",
            "source_info": {
                "paper_title": "Vector Quantized Models for Planning",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero (learned model for planning)",
            "brief_description": "A model-based planning algorithm that learns an implicit deterministic dynamics and reward/value/policy predictors to perform MCTS without access to a simulator; used as a baseline and point of comparison throughout the paper.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "use",
            "model_name": "MuZero",
            "model_description": "Learns a sequential deterministic state representation and transition models that, given a root state and a sequence of agent (and opponent) actions, predicts hidden states h^m and outputs policy π, value v, and reward r at each step; used with MCTS where π provides priors and v estimates leaf values.",
            "model_type": "latent world model (implicit deterministic transition)",
            "task_domain": "Two-player games (chess, go, shogi), single-player Atari (where stochasticity is limited); general model-based planning",
            "fidelity_metric": "Policy cross-entropy, value/reward prediction losses; task performance measured via MCTS-based play strength (win/draw rates) against engines like Stockfish and other agents.",
            "fidelity_performance": "Two-player MuZeroChess achieves strong performance in two-player chess setting; single-player MuZero (without opponent actions observed) performs poorly on single-player (stochastic-opponent) chess as reported in this paper (performance drops catastrophically). Increasing MuZero compute (MuZero@600k) yields only slight improvements but does not overcome single-player failure.",
            "interpretability_assessment": "MuZero uses learned hidden states which are neural and not explicitly interpretable; planning is over actions only and does not expose explicit stochastic outcome codes.",
            "interpretability_method": "Not discussed in this paper beyond standard use of policy/value heads as priors/estimators.",
            "computational_cost": "Typical MuZero training/inference costs are heavy; in this paper MuZero baselines were trained for up to 600k steps in larger-compute ablation. MCTS budgets used up to 1200 simulations per move in evaluations.",
            "efficiency_comparison": "MuZero is efficient when environment is deterministic and fully observed, but fails in the stochastic/partially-observed single-player chess setting where VQM (with discrete latents) succeeds; MuZero with more compute gives only limited improvement for this failure mode.",
            "task_performance": "Strong in deterministic two-player chess (as prior work shows); fails when opponent moves are unobserved (single-player chess) — performance degrades as simulation budget increases due to incorrect handling of stochasticity.",
            "task_utility_analysis": "MuZero's deterministic implicit dynamics suffice when transitions are effectively deterministic and fully observed, but its planning over actions only lacks mechanisms to account for unobserved stochastic environment responses; therefore high predictive fidelity (deterministic predictions) does not translate into robust planning under unobserved stochasticity.",
            "tradeoffs_observed": "Deterministic learned dynamics are simpler and effective in deterministic domains, but they trade off robustness to stochasticity and partial observability compared to stochastic/discrete-latent models like VQM.",
            "design_choices": "MuZero interleaves player and opponent actions in planning for two-player games; in single-player use-case it plans only over player actions. This design choice limits its applicability when opponent/environment behavior is unobserved.",
            "comparison_to_alternatives": "Paper compares MuZero to VQM: MuZero performs worse in single-player stochastic chess while VQM matches two-player MuZero performance by explicitly modeling environment stochasticity with discrete latents.",
            "optimal_configuration": "For MuZero, the optimal regime is deterministic, fully-observed environments where planning over agent actions suffices. The paper suggests extending MuZero-style planning with stochastic latent predictions (as VQM does) for stochastic/partially-observed domains.",
            "uuid": "e1411.6",
            "source_info": {
                "paper_title": "Vector Quantized Models for Planning",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural discrete representation learning",
            "rating": 2
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 1
        },
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 1
        }
    ],
    "cost": 0.02107275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Vector Quantized Models for Planning</h1>
<p>Sherjil Ozair ${ }^{<em> 12}$ Yazhe $\mathbf{L i}^{</em> 1}$ Ali Razavi ${ }^{1}$ Ioannis Antonoglou ${ }^{1}$ Aäron van den Oord ${ }^{1}$ Oriol Vinyals ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Recent developments in the field of model-based RL have proven successful in a range of environments, especially ones where planning is essential. However, such successes have been limited to deterministic fully-observed environments. We present a new approach that handles stochastic and partially-observable environments. Our key insight is to use discrete autoencoders to capture the multiple possible effects of an action in a stochastic environment. We use a stochastic variant of Monte Carlo tree search to plan over both the agent's actions and the discrete latent variables representing the environment's response. Our approach significantly outperforms an offline version of MuZero on a stochastic interpretation of chess where the opponent is considered part of the environment. We also show that our approach scales to DeepMind Lab, a first-person 3D environment with large visual observations and partial observability.</p>
<h2>1. Introduction</h2>
<p>Making predictions about the world may be a necessary ingredient towards building intelligent agents, as humans use these predictions to devise and enact plans to reach complex goals (Lake et al., 2017). However, in the field of reinforcement learning (RL), a tension still exists between model-based and model-free RL. Model-based RL and planning have been key ingredients in many successes such as games like chess (Shannon, 1950; Silver et al., 2017a), Go (Silver et al., 2016b; 2017b), and Poker (Moravčík et al., 2017; Brown et al.). However, their applicability to richer environments with larger action and state spaces remains limited due to some of the key assumptions made in such approaches. Other notable results have not used any form of model or planning, such as playing complex video games</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Dota 2 (OpenAI et al., 2019) and StarCraft II (Vinyals et al., 2019), or robotics (OpenAI et al., 2018).</p>
<p>In this work we are motivated by widening the applicability of model-based planning by devising a solution which removes some of the key assumptions made by the MuZero algorithm (Schrittwieser et al., 2019). Table 1 and Figure 1 summarize the key features of model-based planning algorithms discussed in this paper. MuZero lifts the crucial requirement of having access to a perfect simulator of the environment dynamics found in previous model-based planning approaches (Silver et al., 2017a; Anthony et al., 2017). In many cases such a simulator is not available (eg., weather forecasting), is expensive (eg., scientific modeling), or is cumbersome to run (e.g. for complex games such as Dota 2 or StarCraft II).</p>
<p>However, MuZero still makes a few limiting assumptions. It assumes the environment to be deterministic, limiting which environments can be used. It assumes full access to the state, also limiting which environments can be used. The search and planning is over future agent(s) actions, which could be millions in environments with complex action spaces. The search occurs at every agent-environment interaction step, which may be too fine grained and wasteful.</p>
<p>Largely inspired by both MuZero and the recent successes of VQVAEs (van den Oord et al., 2017; Razavi et al., 2019) and large language models (Radford et al.; Brown et al., 2020), we devise VQ models for planning, which in principle can remove most of these assumptions.</p>
<p>Our approach uses a state VQVAE and a transition model. The state VQVAE encodes future observations into discrete latent variables. This allows the use of Monte Carlo tree search (MCTS, (Coulom, 2006)) for planning not only over</p>
<p>Table 1. Key features of different planning algorithms.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Learned <br> Model</th>
<th style="text-align: center;">Agent <br> Perspective</th>
<th style="text-align: center;">Stochastic</th>
<th style="text-align: center;">Abstract <br> Actions</th>
<th style="text-align: center;">Temporal <br> Abstraction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AlphaZero</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">Two-player <br> MuZero</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">Single-player <br> MuZero</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">VQHybrid</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">VQPure</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">VQJumpy</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Comparison of model-based planning formulations. (c) AlphaZero plans with both player and opponent actions and the groundtruth states with the help of a simulator; (b) Two-player version of MuZero plans with both player and opponent actions; (c) Single-player MuZero plans only with the player actions; (d) VQHybrid plans with player actions and discrete latent variables; (e) VQPure plans with the player action for the first step and discrete latent variables thereafter; (f) VQJumpy plans with discrete latent variables that expand for more than a single agent step.
Notations: The rounded squares denote states. Circles are actions in real action space. Diamonds are discrete latent variables. P denotes the player's action. E denotes the environment's action.
future actions, but also over future observations, thus allowing planning in stochastic or partially-observed environments.</p>
<p>We also propose a "pure" version of our model which encodes both future observations and actions into discrete latent variables, which would allow planning entirely in discrete latent variables. These discrete latent variables can be designed to have cardinality and time granularity independent of the actions, thus enabling planning in large action spaces over longer time horizons.</p>
<p>To demonstrate that the proposed solution works well in practice, we devise two evaluation frameworks using the game of chess. The first is the classic two-player chess framework where agents get to observe and plan over both their own and their opponent's actions. This is the framework used by techniques like MuZero and its previous iterations. In this setting, all four assumptions above perfectly hold, i.e. action and time granularity is already at the right level of abstraction, there is no stochasticity, and agents observe the full board. Then we remove the ability to enumerate opponent actions, and make the opponent's move part of the environment dynamics. That is agents can only observe their own states and actions. This makes the environment stochastic, since the transition is not a deterministic function but depends on an unknown opponent's action from a potentially stochastic policy. We refer to this framework as single-player chess.</p>
<p>We show that MuZero's performance drops catastrophically on single-player chess compared to two-player chess, demonstrating that MuZero depends on access to data from the opponent's perspective. Our approach which only uses player perspective data at training and playing time performs as well on single-player chess as MuZero does on twoplayer chess. This suggests that our approach is a promising way to generalize MuZero-style planning to partiallyobservable and stochastic environments.</p>
<p>To investigate how well our approach can scale, we also evaluate on DeepMind Lab (Beattie et al., 2016), which has a complex observation and action space with partial observability and stochasticity, showing that VQ planning models capture the uncertainty and offer the best results among all planning models we compare against.</p>
<h2>2. Related Work</h2>
<p>Models for Planning Oh et al. (2015); Chiappa et al. (2017); Kaiser et al. (2019) use video prediction models as environment simulators. However, such models are not feasible for planning since they require observation reconstruction which would make planning prohibitively slow. van Hasselt et al. (2019) argue that experience replay can be regarded as a non-parametric model and that Dyna-based</p>
<p>methods are unlikely to outperform model-free methods. Schrittwieser et al. (2019); Oh et al. (2017) learn an implicit deterministic sequence model by predicting future reward, value and policy from current states and future actions. However, these models are in principle limited to deterministic or weakly stochastic environments such as Atari (Machado et al., 2018).</p>
<p>Stochastic models promise to capture uncertainty. PILCO (Deisenroth \&amp; Rasmussen, 2011) used Gaussian processes for transition model and achieves remarkable sample efficiency by capturing model uncertainty. However, it is not scalable to high dimensional state spaces. Depeweg et al. (2016) model uncertainty of the transition function with Bayesian neural networks (BNNs). Kurutach et al. (2018); Chua et al. (2018) use model ensembles to capture epistemic uncertainty that arise from scarcity of data. Variational autoencoders (VAE, Kingma \&amp; Welling (2013); Rezende et al. (2014)) have fuelled a range of stochastic models for RL. Moerland et al. (2017) builds models for RL with conditional VAE (Sohn et al., 2015). Buesing et al. (2018) investigate stochastic state-space models. Ha \&amp; Schmidhuber (2018) train a VAE to compress the observation into continuous latent variables and use an RNN to serve as the predictive model. Hafner et al. (2018; 2019) learn a full forward model using VAE framework. They incorporate multi-step prediction ("latent overshooting") to minimize compounding errors, which changes the optimization objective, while our approach uses data likelihood as the only objective. Hafner et al. (2020) propose a discrete autoencoders model and learn it with straight-through gradients. This is perhaps the most similar to our approach. However, the model is used to generate synthetic data and not used for planning. Lastly, Rezende et al. (2020) study environment models from a causal perspective. They propose adding stochastic nodes using backdoors (Pearl et al., 2016). This approach requires the backdoor variable to be observed and recorded during data generation. Our approach doesn't alter the data generating process, therefore works in offline RL setting.</p>
<p>Model-based policies Models can be used in different ways to materialize a policy. Oh et al. (2015); Kaiser et al. (2019) use environment models in the Dyna (Sutton \&amp; Barto, 2018) framework, which proposes to learn a policy with model-free algorithms using synthetic experiences generated by models. However, the accumulated error in synthesized data could hurt performance compared to an online agent. This loss of performance has been studied by van Hasselt et al. (2019). Ha \&amp; Schmidhuber (2018); Hafner et al. (2018) do policy improvement through black-box optimization such as CMA-ES, which is compatible with continuous latent variable models. Henaff et al. (2017) extends policy optimization to discrete action space. Following AlphaGo
(Silver et al., 2016b) and AlphaZero (Silver et al., 2017a), Schrittwieser et al. (2019) prove that MCTS is scalable and effective for policy improvement in model-based learning. Our approach is a generalization of MuZero that is able to incorporate stochasticity and abstract away planning from agent actions. Continuous action spaces and MCTS have also been combined with some success, e.g. (Couëtoux et al., 2011) and (Yee et al., 2016). However, our choice of discrete latent space makes it possible to leverage all the recent advances made in MCTS. In a specific multi-agent setup, where the focus is to find policies that are less exploitable, models can be used with counterfactual regret minimization (Moravcík et al., 2017) or fictitious play (Heinrich \&amp; Silver, 2016) to derive Nash equilibrium strategy.</p>
<p>Offline RL While our model-based approach is applicable generally, we evaluate it in the offline RL setting. Previous model-based offline RL approaches (Argenson \&amp; DulacArnold, 2020; Yu et al., 2020; Kidambi et al., 2020) have focused on continuous control problems (Tassa et al., 2020; Gulcehre et al., 2020). Our work focuses on environments with large observation spaces and complex strategies which require planning such as chess and DeepMind Lab (Beattie et al., 2016).</p>
<h2>3. Background</h2>
<p>Vector-Quantized Variational AutoEncoders (VQVAE, van den Oord et al. (2017)) make use of vector quantization (VQ) to learn discrete latent variables in a variational autoencoder. VQVAE comprises of neural network encoder and decoder, a vector quantization layer, and a reconstruction loss function. The encoder takes as input the data sample $\mathbf{x}$, and outputs vector $\mathbf{z}<em k="k">{u}=f(\mathbf{x})$. The vector quantization layer maintains a set of embeddings $\left{\mathbf{e}</em>\right}<em c="c">{k=1}^{K}$. It outputs an index $c$ and the corresponding embedding $\mathbf{e}</em>}$, which is closest to the input vector $\mathbf{z<em c="c">{u}$ in Euclidean distance. The decoder neural network uses the embedding $\mathbf{e}</em>}$ as its input to produce reconstructions $\hat{\mathbf{x}}$. The full loss is $\mathcal{L}^{t}=\mathcal{L}^{r}(\hat{\mathbf{x}}, \mathbf{x})+\beta\left|\mathbf{z<em c="c">{u}-s g\left(\mathbf{e}</em>$, where $s g(\cdot)$ is the stop gradient function. The second term is the commitment loss used to regularize the encoder to output vectors close to the embeddings so that error due to quantization is minimized. The embeddings are updated to the exponential moving average of the minibatch average of the unquantized vectors assigned to each latent code. In the backwards pass, the quantization layer is treated as an identity function, referred to as straight-through gradient estimation (Bengio et al., 2013). For more details, see van den Oord et al. (2017).}\right)\right|^{2</p>
<p>Monte Carlo Tree Search (MCTS, Coulom (2006)) is a tree search method for estimating the optimal action given access to a simulator, typically used in two-player games. MCTS builds a search tree by recursively expanding the tree</p>
<p>and assessing the value of the leaf node using Monte Carlo (MC) simulation. Values of leaf nodes are used to estimate the Q-values of all the actions in the root node.</p>
<p>The policy for child node selection during expansion is crucial to the performance of MCTS. The most popular method for this is UCT (stands for "Upper Confidence Bounds applied to trees", Kocsis \&amp; Szepesvári (2006)), which is based on Upper Confidence Bound (UCB, Auer et al. (2002)). UCT suggests that this problem can be seen as a Bandit problem where the optimal solution is to combine the value estimation with its uncertainty.</p>
<p>AlphaGo (Silver et al., 2016a) combined MCTS with neural networks by using them for value and policy estimations. The benefits of this approach are twofold: value estimation no longer incurs expensive Monte Carlo simulations, allowing for shallow searches to be effective, and the policy network serves as context for the tree expansion and limits the branching factor.</p>
<p>At each search iteration, the MCTS algorithm used in AlphaGo consists of 3 steps: selection, expansion and value backup. During selection stage, MCTS descends the search tree from the root node by picking the action that maximizes the following upper confidence bound:</p>
<p>$$
\arg \max _{a}[Q(s, a)+P(a \mid s) U(s, a)]
$$</p>
<p>where</p>
<p>$$
U(s, a)=\frac{\sqrt{N(s)}}{1+N(s, a)}\left[c_{1}+\log \left(\frac{N(s)+c_{2}+1}{c_{2}}\right)\right]
$$</p>
<p>and $N(s, a)$ is the visit counts of taking action $a$ at state $s, N(s)=\sum_{b} N(s, b)$ is the number of times $s$ has been visited, $c_{1}$ and $c_{2}$ are constants that control the influence of the policy $P(a \mid s)$ relative to the value $Q(s, a)$.</p>
<p>Following the action selection, the search tree receives the next state. If the next state doesn't already exist in the search tree, a new leaf node is added and this results in an expansion of the tree. The value of the new leaf node is evaluated with the learned value function. Finally, the estimated value is backed up to update MCTS's value statistics of the nodes along the descending path:</p>
<p>$$
Q_{\text {tree }}^{t+1}(s, a)=\frac{Q_{\text {tree }}^{t}(s, a) N^{t}(s, a)+Q(s, a)}{N^{t}(s, a)+1}
$$</p>
<p>Here $Q_{\text {tree }}^{t}(s, a)$ is the action value estimated by the tree search at iteration $t ; N^{t}(s, a)$ is the visit count at iteration $t$.</p>
<p>MuZero Schrittwieser et al. (2019) introduce further advances in MCTS, where a sequential model is learned from trajectories $\left{s_{0}, a_{0}, \ldots, s_{T-1}, a_{T-1}, s_{T}\right}$. At each timestep, the model uses the trajectory to formulate a planning path at timestep $t: s_{t}, a_{t}, \ldots, a_{t+M-1}, a_{t+M}$ with $s_{t}$
being the root state. To simplify the notation, we omit the subscript and use superscript for indexing actions on the planning path. So the same sequence is written as $s, a^{0}, \ldots, a^{M-1}, a^{M}$. Given the starting root state $s$ and a sequence of actions $a^{0: m}$, the model outputs a hidden state $h^{m}$ and predicts action policy $\pi^{m}$, value $v^{m}$ and reward $r^{m}$. The training objective is as follows:</p>
<p>$$
\begin{aligned}
\frac{1}{M} \sum_{m=1}^{M}[ &amp; L^{\pi}\left(a^{m}, \pi\left(h^{m}\right)\right)+\alpha \mathcal{L}^{v}\left(v_{\text {target }}^{m}, v\left(h^{m}\right)\right) \
&amp; \left.+\beta \mathcal{L}^{r}\left(r_{\text {env }}^{m}, r\left(h^{m}\right)\right)\right]
\end{aligned}
$$</p>
<p>where $h^{m}$ is the hidden state on the planning path. $L^{\pi}\left(a^{m}, \pi\left(h^{m}\right)\right)$ is the cross entropy loss between the action and learned parametric policy $\pi . \mathcal{L}^{v}$ is the value loss function, $v_{\text {target }}$ is the value target and $v$ is the value prediction. $\mathcal{L}^{r}$ is the reward loss function, $r_{\text {env }}$ is the environment reward and $r$ is the reward prediction. $\alpha$ and $\beta$ are the weights.</p>
<p>During search, $\pi$ is used as the prior for action selection; $r$ gives the reward instead of using reward from the simulator; $v$ estimates the value of the leaf state rather than using Monte Carlo rollouts.</p>
<p>Comparing to AlphaZero, MuZero model eliminates the need of a simulator to generate the groundtruth state along the planning path. In two player games, MuZero's planning path interleaves the player's action and opponent's action. Whereas in single player version, only player's actions are seen by the model.</p>
<h2>4. Model-based Planning with VQVAEs</h2>
<p>Our approach uses a state VQVAE model and a transition model. We refer to the full model as VQ Model (VQM), and the resulting agent when combined with MCTS as VQMMCTS.</p>
<p>We first describe the components of VQM in detail. Then, we explain how the model is used with MCTS.</p>
<h3>4.1. VQ Model</h3>
<p>Our VQ model is trained with a two-stage training process ${ }^{1}$. We first train the state VQVAE model which encodes the observations into discrete latent variables (Figure 3a). Then we train the transition model using the discrete latent variables learned by the state VQVAE model (Figure 3b).</p>
<p>Notation We use $s_{t}, a_{t}$, and $r_{t}$ to denote the state at time $t$, the action following state $s_{t}$, and the resulting reward, respectively. An episode is a sequence of interleaved states, actions, and rewards $\left(s_{1}, a_{1}, \ldots, s_{t}, a_{t}, \ldots, s_{T}\right)$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Complete encoder/decoder architecture of the state VQVAE. Encoder compresses $s_{1: t+1}$ and $a_{1: t}$ to a continuous latent $z_{t+1}$. The quantization layer returns the nearest code $e_{t+1}$, as well as the corresponding index $k_{t+1}$, in its codebook $E$. Decoder uses $s_{1: t}, a_{1: t}$ and the code $e_{t+1}=E\left[k_{t+1}\right]$ to reconstruct $s_{t+1}$.</p>
<p>State VQVAE The purpose of the state VQVAE is to encode a sequence of states and actions into a sequence of discrete latent variables and actions that can reconstruct back the original sequence. This is done by learning a conditional VQVAE encoder-decoder pair. The encoder takes in states $s_{1}, \ldots, s_{t}, s_{t+1}$ and actions $a_{1}, \ldots, a_{t}$, and produces a discrete latent variable $k_{t+1}=f_{\text {enc }}\left(s_{1: t+1}, a_{1: t}\right)$. The decoder takes in the discrete latent variable and the states and actions until time $t$ and reconstructs the state at time $t+1$, i.e. $\hat{s}<em _dec="{dec" _text="\text">{t+1}=f</em>$ given the previous states and actions.}}\left(s_{1: t}, a_{1: t}, k_{t+1}\right)$. Thus, $k_{t+1}$ represents the additional information in the state $s_{t+1</p>
<p>The state VQVAE is trained using the VQVAE technique introduced in van den Oord et al. (2017) (reviewed in Section 3). The cardinality of the discrete latent variable is a design choice. Larger values are more expressive but potentially expensive at search time. Lower values would lead to lossy compression but could potentially yield more abstract representations. We show the effect of cardinality size on reconstruction quality in the supplementary material. Figure 2 depicts the state VQVAE.</p>
<p>Transition Model We obtain latent variable $k_{t}$ from state $s_{t}$ using the state VQVAE. We construct a planning path which comprises of a state followed by a sequence of interleaved actions and latent variables until a maximum depth $M$ is reached, i.e. $s, a^{0}, k^{1}, a^{1} \ldots, a^{M-1}, k^{M}$. Thus, instead of planning over only the agent's actions, this allows us to also plan over the outcomes of those actions.</p>
<p>Similar to the environment model of MuZero, our transition model predicts reward and value function at every step. Unlike MuZero, our model not only has a policy head $\pi$ but also a discrete latent code head $\tau$. We alternate the prediction of action and discrete latent code along the sequence. Note that the value prediction after the discrete latent code
<img alt="img-2.jpeg" src="img-2.jpeg" />
(a) The state VQVAE encodes a sequence of observations $s$ and actions $a$ into discrete latent variables $k$.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) An autoregressive transition model outputs a policy $\pi$ over the actions, a policy $\tau$ over the discrete latent codes and a value function $v$.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(c) MCTS branches over both actions and state latent variables.</p>
<p>Figure 3. The main components of the proposed agent.
corresponds to an estimate of the state value functions, while value prediction after the action corresponds to an estimation of the Q-function.</p>
<p>To train all the components, and again following MuZero, we use teacher forcing of trajectories generated by a behavior policy (human experts or other agents in all our experiments) based on the observed states $s$, actions $a$, and latent variables $k$. The total loss combining all the prediction losses is</p>
<p>$$
\begin{aligned}
&amp; \frac{1}{M} \sum_{m=0}^{M-1} \operatorname{CE}\left(a^{m}, \pi\left(h^{2 m}\right)\right)+\frac{1}{M} \sum_{m=1}^{M} \operatorname{CE}\left(k^{m}, \tau\left(h^{2 m-1}\right)\right) \
+ &amp; \frac{\alpha}{2 M} \sum_{m=0}^{2 M} \mathcal{L}^{v}\left(v_{\text {target }}^{m}, v\left(h^{m}\right)\right)+\frac{\beta}{2 M} \sum_{m=0}^{2 M} \mathcal{L}^{r}\left(r_{\text {env }}^{m}, r\left(h^{m}\right)\right)
\end{aligned}
$$</p>
<p>where CE is the cross entropy loss. The total loss is similar to MuZero loss. The main difference is that we also predict latent variables at every odd timestep.</p>
<p>Throughout this Section, we explain our VQM-MCTS based on the VQHybrid planning path (Figure 1d). However, VQHybrid is not the only choice available. Depending on the information encoded by the state VQVAE, the planning path can be structured differently.</p>
<p>VQPure (Figure 1e) requires a factorized state VQVAE which provides two sets of latent space: one for environment stochasticity, same as in VQHybrid, and the other for the agent's actions. VQPure allows the transition model to decouple from the action space and unroll purely in discrete latent spaces. In contrast, VQHybrid interleaves between state latent variables and actions. For VQJumpy (Figure 1f), the state VQVAE makes a jumpy prediction of the state $s_{t+m}$ instead of predicting the immediate state $s_{t+1}$.</p>
<p>This enables "temporal abstractions" during planning and provides a principled way to unlock planning in a stochastic environment and decoupling the action space tied to environment both in branching and in time. Although we provide some result for VQPure in our chess experiment, these two planning path alternatives remain largely unexplored and left for future works.</p>
<h3>4.2. Monte Carlo Tree Search with VQ Planning Model</h3>
<p>In order to use the VQ planning model, we modify Monte Carlo Tree Search (MCTS) algorithm to incorporate the VQ "environment action". The main difference with the MCTS used in MuZero (reviewed in Section 3) is that, instead of predicting agent (or opponent) actions, our MCTS also predicts next discrete latent variables $k$ given the past. Unlike classical MCTS which anchors in the real action space and imposes an explicit turn-based ordering for multi-agent environment, our MCTS leverages the abstract discrete latent space induced by the state VQVAE.</p>
<p>The search tree in MCTS consists of two types of nodes: action node and stochastic node. During selection stage, MCTS descends the search tree from the root node: for action nodes, Equation 1 is used to select an action; for stochastic nodes,</p>
<p>$$
\underset{k}{\arg \max }\left[\hat{Q}(s, k)+P(k \mid s) U(s, k)\right]
$$</p>
<p>is used to select a discrete latent code. We obtain $U(s, k)$ by replacing $a$ with $k$ in $U(s, a)$ from Equation 1. $P(k \mid s)$ is computed with the learned policy $\tau$ of the discrete latent code. $\hat{Q}(s, k)$ can be 0 for a neutral environment, $Q(s, k)$ if the environment is known to be cooperative or $-Q(s, k)$ if the environment is known to be adversarial. When $\hat{Q}(s, k)=0$, our algorithm is similar to Expectimax search (Michie, 1966; Russell \&amp; Norvig, 2009) where the
expectation of children's Q-value is computed at the stochastic nodes. In zero-sum games like Chess and Go, we can use the adversarial setting. As we will see in Section 5.1, adding this prior knowledge improves agent performance.</p>
<h2>5. Experiments</h2>
<p>Our experiments aim at demonstrating all the key capabilities of the VQ planning model: handling stochasticity, scaling to large visual observations and being able to generate long rollouts, all without any performance sacrifices when applied to environments where MCTS has shown state-of-the-art performance.</p>
<p>We conducted two sets of experiments: in Section 5.1, we use chess as a test-bed to show that we can drop some of the assumptions and domain knowledge made in prior work, whilst still achieving state-of-the-art performance; in Section 5.2, with a rich 3D environment (DeepMind Lab), we probe the ability of the model in handling large visual observations in partially observed environment and producing high quality rollouts without degradation.</p>
<h3>5.1. Chess</h3>
<p>Chess is an ancient game widely studied in artificial intelligence (Shannon, 1950). Although state transitions in chess are deterministic, the presence of the opponent makes the process stochastic from the agent's perspective, when the opponent is considered part of the environment.</p>
<h3>5.1.1. DATASETS</h3>
<p>To evaluate our approach, we follow the two-stage training of VQM and use MCTS evaluation steps illustrated in Section 4. We use the offline reinforcement learning setup by training the models with a fix dataset. We use a combination of Million Base dataset ( 2.5 million games) and FICS Elo $&gt;2000$ dataset ( 960 k games $)^{2}$. The validation set consists of 45 k games from FICS Elo $&gt;2000$ from 2017. The histogram of player ratings in the datasets is reported in the supplementary material.</p>
<h3>5.1.2. MODEL ARCHITECTURES</h3>
<p>The state VQVAE uses feed-forward convolutional encoder and decoder, along with a quantization layer in the bottleneck. The quantization layer has 2 codebooks, each of them has 128 codes of 64 dimensions. The final discrete latent is formed by concatenating the 2 codes, forming a 2 -hot encoding vector.</p>
<p>The transition model consists of a recurrent convolutional model which either takes an action or a discrete latent code</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 4. Performance of agents playing against Stockfish 10 skill level 15 .
as input at each unroll step. The model predicts the policies over action and discrete latent code, as well as the value function. We use Monte Carlo return of the game as the target value for training.</p>
<h3>5.1.3. EVALUATIONS</h3>
<p>The performance of our agent is evaluated by playing against:</p>
<ol>
<li>Stockfish version 10 (T. Romstad) (44 threads, 32G hash size and 15 s per move);</li>
<li>Q-value agent: Action with the highest Q value is picked. The Q value is computed by unrolling the model for one step and using the learned value function to estimate the value of the next state.</li>
<li>Imitation agent: Agent chooses the most probable action according to the learned policy.</li>
<li>MuZeroChess agent: MuZero with the same backbone architecture as VQ planning model.</li>
</ol>
<p>Each agent is evaluated for 200 games playing as white and black respectively for 100 games. We present results for both the worst case and neutral scenario of VQM-MCTS with VQHybrid and VQPure planning path.</p>
<h3>5.1.4. RESULTS</h3>
<p>Figure 4 reports our main results. Performance of Singleplayer MuZeroChess (which doesn't observe opponent actions) is considerably worse than two-player MuZeroChess. In addition, Figure 6 shows that using more MCTS simulations hurts single-player MuZeroChess performance, because the model is not correctly accounting for the stochasticity introduced by the unobserved opponent's actions. Both VQHybrid and VQPure agents with worst case chance nodes are able to recover the performance to the same level as the two-player MuZeroChess agent. We then further remove the assumption of the adversarial environment by using neutral case chance nodes during MCTS. The resulting agents, VQHybrid Neutral and VQPure Neutral, don't perform as well as VQHybrid and VQPure. This shows that
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5. Performance of agents playing against each other.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6. Agent performance as function of Stockfish strength and simulation budget. Left column shows win and draw rates of the agent evaluated by playing against different levels of Stockfish 10 with a fixed simulation budget of 1200 per move. Right column shows the impact of simulation budget on agent performance playing against level 15 .
prior knowledge of the environment can indeed help the agent perform better.</p>
<p>We also note that when assuming the environment is neutral, increasing the simulation budget doesn't seem to improve the performance (Figure 6). This is because MCTS searches over the expected behavior of players from a broad range of Elo ratings as human data is used for training. This</p>
<p>expectation could deviate significantly from the Stockfish agent, especially for higher skill levels. The full results for the agents can be found in the supplementary material.</p>
<p>In Figure 5, we report win and draw rates of a VQHybrid agent playing against other baseline agents. The results confirm that the VQ agent's performance is on par with two-player MuZeroChess: VQHybrid and MuZeroChess achieve very similar performance playing against the same imitation agent; when playing directly against each other, VQHybrid and MuZeroChess reach similar win rate.</p>
<h3>5.2. DeepMind Lab</h3>
<p>DeepMind Lab is a first-person 3D environment that features large and complex visual observations, and stochasticity from procedural level generation and partial observability. Due to these properties, it makes for a good environment to test the scalability of our VQ planning model.</p>
<h3>5.2.1. DATASET</h3>
<p>We used an A2C agent (Mnih et al., 2016) to collect a dataset of $101,325,000$ episodes from the explore_rat_goal_locations_small level in DeepMind Lab, 675, 500 of which is held out as the test set. Each episode has 128 timesteps in which the agent is randomly spawned in a maze-like environment, which it observes from first-person view. The agent is rewarded when it captures one of the apples that are randomly placed in the environment, at which point it is transported to a new random location in the map. The collection of episodes is started with a randomly initialized agent and is continued as the training of the agent progresses and it learns about the environment. The collected dataset thus comprises a variety of episodes corresponding to different experience level of the A2C agent.</p>
<h3>5.2.2. Model Architecture and Training</h3>
<p>In addition to the approach described in Section 4, we add a frame-level VQVAE training stage at the start, which uses feed-forward convolutional encoder and decoder trained to map observed frames to a frame-level latent space. The codebook has 512 codes of 64 dimensions. Then as discussed in Section 4, in the second stage, we train the state VQVAE on top of the frame-level VQ representations. This model captures the temporal dynamics of trajectories in a second latent layer, consisting of a stack of 32 separate latent variables at each timestep, each with their separate codebook comprising of 512 codes of 64 dimensions. The architecture for this component consists of a convolutional encoder torso, an encoder LSTM, a quantization layer, a decoder LSTM and finally a convolutional decoder head that maps the transition discrete latents back to the frame-level VQ space. Finally in the third stage, we fit the transition
model with a hybrid "planning path" using a deep, causal Transformer (Vaswani et al., 2017).</p>
<p>To generate new samples from the model, we first sample state latent variables from the prior network. These are then fed to the state VQVAE decoder for mapping back to the frame-level discrete latent space. The resulting framelevel codes are mapped to the pixel space by the frame-level VQVAE decoder.</p>
<h3>5.2.3. BASELINES</h3>
<p>We compare our proposed approach based on VQM with several baselines. As the simplest baseline, we train a deterministic next-frame prediction model with an LSTM architecture closely mimicking the state VQVAE architecture except for the quantization layer. The network is trained to reconstruct each frame given the preceding frames with mean-squared-error (MSE). Additionally, we train several sequential continuous VAE baselines with different posterior and prior configurations to compare our discrete approach with continuous latent variables. We use GECO (Jimenez Rezende \&amp; Viola, 2018) to mitigate the well-known challenges of training variational autoencoders with flexible decoder and prior models. In particular, we assign a target average distortion tolerance for the decoder and minimize, using the Lagrange multiplier method, the KL-divergence of the posterior to the prior subject to this distortion constraint, as described in Jimenez Rezende \&amp; Viola (2018). We choose two different distortion levels of 25 dB and 33 dB PSNR. The former is based on our experiments with the deterministic LSTM predictor (the first baseline described above), which achieves reconstruction PSNR of about 24 dB , and the latter is set slightly higher than the reconstruction PSNR of our frame-level VQVAE decoder at 32dB.</p>
<h3>5.2.4. Evaluation Metric</h3>
<p>For every trajectory in the test set we take $k(=1000)$ sample episodes from the model using the initial prefix of $T_{0}(=16)$ frames from each ground-truth trajectory. For each groundtruth trajectory, we find the sample with minimum (cumulative) reconstruction error (measured as Mean Squared Error or MSE) from the end of the prefix up to a target timestep $t(=128)$, and average the error for this best-match sample over the test trajectories. We refer to this metric as Mean Best Reconstruction Error or MBRE defined as</p>
<p>$$
\operatorname{MBRE}\left(S, G, T_{0}, T\right)=\frac{1}{|G|} \sum_{i=1}^{|G|} \min <em i="i">{s \in S</em>
$$}} \sum_{t=T_{0}}^{T}\left|s(t)-G_{i}(t)\right|^{2</p>
<p>where $G$ is a set of ground-truth trajectories, and $S$ is a set of sampled episodes- $k$ episodes sampled using the initial $T_{0}$ frames of each ground-truth episode $G_{i}$ as prefix. We compare sampled episodes with their corresponding ground truth episode at the target frame $T$. An ideal model that</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7. Different random rollouts from the same prefix video in DeepMind Lab. The first row is a ground-truth episode from the validation set. Each of the following rows show a sample from the model conditioned on the same 16 starting frames (not shown in the figure). Videos of more samples can be seen in https://sites.google.com/view/vqmodels/home.
generalizes well to the test set would place a non-negligible probability mass on the ground-truth trajectory, and thus an episode close to the ground-truth should be sampled given a sufficiently large number of trials.</p>
<h3>5.2.5. RESULTS</h3>
<p>Rollouts generated by the model can be seen in Figure 7. Figure 8 shows the results of evaluating MBRE for our proposed approach against the aforementioned baselines. The LSTM baseline performs significantly worse than all latent variable models. This is expected because the generative model is not expressive enough for stochastic and multi-modal data, and as a result predicts the mean of all possible outcomes, which results in poor best matches against ground-truth. Comparisons with the two sequential VAE baselines demonstrate the trade-off between the reconstruction quality of a model and its predictive performance as measured by MBRE. Furthermore, it shows that our VQM approach is able to achieve competitive performance with respect to both reconstruction quality and long range predictive performance.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8. Mean Best Reconstruction Error (MBRE) for our proposed VQVAE approach and baselines. X-axis is length of trajectory. Top: computed over frames up to the target frame, Bottom: computed for the target frame only.</p>
<h2>6. Conclusion</h2>
<p>In this work, we propose a solution to generalize modelbased planning for stochastic and partially-observed environments. Using discrete autoencoders, we learn discrete abstractions of the state and actions of an environment, which can then be used with discrete planning algorithms such as MCTS (Coulom, 2006). We demonstrated the efficacy of our approach on both an environment which requires deep tactical planning and a visually complex environment with high-dimensional observations. Further we successfully applied our method in the offline setting. Our agent learns from a dataset of human chess games and outperforms modelfree baselines and performs competitively against offline MuZero and Stockfish Level 15 while being a more general algorithm. We believe the combination of model-based RL and offline RL has potential to unlock a variety of useful applications in the real world.</p>
<h2>Acknowledgements</h2>
<p>We'd like to thank Ivo Danihelka and Nando de Freitas for providing valuable feedback on early drafts of the paper. We'd like to thank Julian Schrittwieser for helping with the MuZero baselines. We'd also like to thank Sander Dielman, David Silver, Yoshua Bengio, Jakub Sygnowski, and Aravind Srinivas for useful discussions and suggestions.</p>
<h2>References</h2>
<p>Anthony, T., Tian, Z., and Barber, D. Thinking fast and slow with deep learning and tree search. arXiv preprint arXiv:1705.08439, 2017.</p>
<p>Argenson, A. and Dulac-Arnold, G. Model-based offline planning. arXiv preprint arXiv:2008.05556, 2020.</p>
<p>Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time</p>
<p>analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235-256, 2002.</p>
<p>Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., Küttler, H., Lefrancq, A., Green, S., Valdés, V., Sadik, A., et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.</p>
<p>Bengio, Y., Léonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.</p>
<p>Brown, N., Sandholm, T., and Machine, S. Libratus: The superhuman ai for no-limit poker.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p>
<p>Buesing, L., Weber, T., Racaniere, S., Eslami, S. M. A., Rezende, D., Reichert, D. P., Viola, F., Besse, F., Gregor, K., Hassabis, D., and Wierstra, D. Learning and querying fast generative models for reinforcement learning, 2018.</p>
<p>Chiappa, S., Racaniere, S., Wierstra, D., and Mohamed, S. Recurrent environment simulators. arXiv preprint arXiv:1704.02254, 2017.</p>
<p>Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models, 2018.</p>
<p>Couëtoux, A., Hoock, J.-B., Sokolovska, N., Teytaud, O., and Bonnard, N. Continuous upper confidence trees. In International Conference on Learning and Intelligent Optimization, pp. 433-445. Springer, 2011.</p>
<p>Coulom, R. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pp. 72-83. Springer, 2006.</p>
<p>Deisenroth, M. and Rasmussen, C. Pilco: A model-based and data-efficient approach to policy search. In Getoor, L. and Scheffer, T. (eds.), Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML '11, pp. 465-472, New York, NY, USA, June 2011. ACM. ISBN 978-1-4503-0619-5.</p>
<p>Depeweg, S., Hernández-Lobato, J. M., Doshi-Velez, F., and Udluft, S. Learning and policy search in stochastic dynamical systems with bayesian neural networks. arXiv preprint arXiv:1605.07127, 2016.</p>
<p>Glickman, M. E. Example of the glicko-2 system.
Gulcehre, C., Wang, Z., Novikov, A., Paine, T. L., Colmenarejo, S. G., Zolna, K., Agarwal, R., Merel, J., Mankowitz, D., Paduraru, C., et al. Rl unplugged: Benchmarks for offline reinforcement learning. arXiv preprint arXiv:2006.13888, 2020.</p>
<p>Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, pp. 2450-2462, 2018.</p>
<p>Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.</p>
<p>Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.</p>
<p>Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J. Mastering atari with discrete world models, 2020.</p>
<p>Heinrich, J. and Silver, D. Deep reinforcement learning from self-play in imperfect-information games. CoRR, abs/1603.01121, 2016. URL http://arxiv.org/ abs/1603.01121.</p>
<p>Henaff, M., Whitney, W. F., and LeCun, Y. Model-based planning with discrete and continuous actions. arXiv preprint arXiv:1705.07177, 2017.</p>
<p>Jimenez Rezende, D. and Viola, F. Taming VAEs. ArXiv e-prints, October 2018.</p>
<p>Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.</p>
<p>Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. Morel: Model-based offline reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.</p>
<p>Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization, 2014.</p>
<p>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Kocsis, L. and Szepesvári, C. Bandit based monte-carlo planning. In European conference on machine learning, pp. 282-293. Springer, 2006.</p>
<p>Kurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.</p>
<p>Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017.</p>
<p>Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523-562, 2018.</p>
<p>Michie, D. Game-playing and game-learning automata. 1966.</p>
<p>Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 19281937, 2016.</p>
<p>Moerland, T. M., Broekens, J., and Jonker, C. M. Learning multimodal transition dynamics for model-based reinforcement learning. arXiv preprint arXiv:1705.00470, 2017.</p>
<p>Moravčík, M., Schmid, M., Burch, N., Lisỳ, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowling, M. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.</p>
<p>Moravcík, M., Schmid, M., Burch, N., Lisý, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowling, M. H. Deepstack: Expert-level artificial intelligence in no-limit poker. CoRR, abs/1701.01724, 2017. URL http://arxiv.org/abs/1701.01724.</p>
<p>Oh, J., Guo, X., Lee, H., Lewis, R., and Singh, S. Actionconditional video prediction using deep networks in atari games, 2015.</p>
<p>Oh, J., Singh, S., and Lee, H. Value prediction network. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 6120-6130, 2017.</p>
<p>OpenAI, Andrychowicz, M., Baker, B., Chociej, M., Józefowicz, R., McGrew, B., Pachocki, J. W., Pachocki, J., Petron, A., Plappert, M., Powell, G., Ray, A., Schneider, J., Sidor, S., Tobin, J., Welinder, P., Weng, L., and Zaremba, W. Learning dexterous in-hand manipulation. CoRR, abs/1808.00177, 2018. URL http: //arxiv.org/abs/1808.00177.</p>
<p>OpenAI, :, Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R., Gray, S., Olsson, C., Pachocki, J., Petrov, M., de Oliveira Pinto, H. P., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S.,</p>
<p>Sutskever, I., Tang, J., Wolski, F., and Zhang, S. Dota 2 with large scale deep reinforcement learning, 2019.</p>
<p>Pearl, J., Glymour, M., and Jewell, N. P. Causal inference in statistics: A primer. John Wiley \&amp; Sons, 2016.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners.</p>
<p>Razavi, A., van den Oord, A., and Vinyals, O. Generating diverse high-fidelity images with vq-vae-2. In Advances in Neural Information Processing Systems, pp. 1486614876, 2019.</p>
<p>Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models, 2014.</p>
<p>Rezende, D. J., Danihelka, I., Papamakarios, G., Ke, N. R., Jiang, R., Weber, T., Gregor, K., Merzic, H., Viola, F., Wang, J., Mitrovic, J., Besse, F., Antonoglou, I., and Buesing, L. Causally correct partial models for reinforcement learning, 2020.</p>
<p>Russell, S. J. and Norvig, P. Artificial Intelligence: a modern approach. Pearson, 3 edition, 2009.</p>
<p>Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019.</p>
<p>Shannon, C. E. Xxii. programming a computer for playing chess. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 41(314):256-275, 1950.</p>
<p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nature, 529:484-503, 2016a. URL http://www.nature.com/nature/journal/ v529/n7587/full/nature16961.html.</p>
<p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016b.</p>
<p>Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. Mastering chess and shogi by self-play</p>
<p>with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017a.</p>
<p>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. Mastering the game of go without human knowledge. Nature, 550(7676):354-359, 2017b.</p>
<p>Sohn, K., Lee, H., and Yan, X. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems, 28: $3483-3491,2015$.</p>
<p>Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. URL http://incompleteideas.net/ book/the-book-2nd.html.
T. Romstad, M. Costalba, J. K. Stockfish: A strong open source chess engine.</p>
<p>Tassa, Y., Tunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez, S., Merel, J., Erez, T., Lillicrap, T., and Heess, N. dm_control: Software and tasks for continuous control, 2020.
van den Oord, A., Vinyals, O., et al. Neural discrete representation learning. In Advances in Neural Information Processing Systems, pp. 6306-6315, 2017.
van Hasselt, H., Hessel, M., and Aslanides, J. When to use parametric models in reinforcement learning?, 2019.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.</p>
<p>Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J. P., Jaderberg, M., Vezhnevets, A. S., Leblond, R., Pohlen, T., Dalibard, V., Budden, D., Sulsky, Y., Molloy, J., Paine, T. L., Gulcehre, C., Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, D., Wünsch, D., McKinney, K., Smith, O., Schaul, T., Lillicrap, T., Kavukcuoglu, K., Hassabis, D., Apps, C., and Silver, D. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782): 350-354, Nov 2019. ISSN 1476-4687. doi: 10.1038/ s41586-019-1724-z. URL https://doi.org/10. 1038/s41586-019-1724-z.</p>
<p>Yee, T., Lisỳ, V., Bowling, M. H., and Kambhampati, S. Monte carlo tree search in continuous action spaces with execution uncertainty. In IJCAI, pp. 690-697, 2016.</p>
<p>Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma, T. Mopo: Model-based offline policy optimization. arXiv preprint arXiv:2005.13239, 2020.</p>
<h2>A. Appendix</h2>
<h2>A.1. Chess datasets</h2>
<p>The histogram of Glicko-2 ratings (Glickman) of the players in the training set is shown in Figure 9.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 9. Histogram of Elo rating for players in Million Base dataset and FICS Elo2000 training set.</p>
<h2>A.2. Details for Chess Experiments</h2>
<h2>A.2.1. Input Representations</h2>
<p>We use symbolic representation for the chess board where each piece is represented as an integer. Following the same protocol as Silver et al. (2017a), the action space is represented as $8 \times 8 \times 73$ discrete actions.</p>
<h2>A.2.2. Baseline Models</h2>
<p>MuZeroChess The baseline MuZeroChess model is similar to that of the MuZero (Schrittwieser et al., 2019). In stead of using a deep convolutional network extracting features from the board state, we simply embed the board state to get the initial state $s$. The dynamics function is consist of 2 convolutional networks $g$ and $f$. It works as follows: set $z^{0}=s$ for the initial state; $z^{m+1}=g\left(z^{m}, a^{m}\right), g$ takes in the embedded action input $a^{m}$ and previous state $z^{m}$ and outputs the next state $z^{m+1} ; h^{m+1}=f\left(z^{m+1}\right), f$ produces the final state for prediction heads. We have two prediction heads $v$ and $\pi$ for value and policy respectively. Both $g$ and $f$ are 20-layer residual convolution stacks with 1024 hiddens and 256 bottleneck hiddens. All the heads have a hidden size 256. The action predictor uses kernel size 1 and strides 1 and its output is flattened to make the prediction.</p>
<p>We train with sequence length 10 . If the training sequence reaches the end of the game, then after game terminates, we pad random actions and target value, as well as mask the action prediction loss.</p>
<p>We use a batch size of 2048 for training and use Adam optimizer (Kingma \&amp; Ba, 2014) with learning rate $3 e^{-4}$ and exponential decay of with decay rate 0.9 and decay steps 100000. To stabilize the training, we apply gradient clipping with maximum clipping value 1 . The model is trained for 200k steps.</p>
<p>Q-value and Imitation Agents The same MuZeroChess model as described above is used. For Q-value agent, we unroll the dynamics function for 1 step with each legal action. The estimated action value of the Q-value agent is the value prediction of the next state. For imitation agent, we use the policy prediction of the model as the imitation agent's policy.</p>
<h2>A.2.3. VQ Model</h2>
<p>State VQVAE The state VQVAE for chess has a encoder and a decoder, which are both 16-layer residual convolution stack with 256 hiddens and 64 bottleneck hiddens. The quantization layer has 2 codebooks, each of them has 64 codes of 256 dimensions. Before feeding into the quantization layer, we apply spatial mean pooling on the features. The reconstruction of the board state is cast as a classification problem where the model predicts the piece type at each position. Therefore, the reconstruction loss is the cross entropy loss. The training batch size is 1024. We use Adam optimizer (Kingma \&amp; Ba, 2014) with learning rate $3 e^{-4}$ and exponential decay of with decay rate 0.9 and decay steps 100000. The model is trained for 1.2 million steps.</p>
<p>Transition model Our transition model for chess is similar to the model used for MuZeroChess. In addition to the dynamics function $g$ which takes the action as input at each step, we introduce another dynamics function $g^{\prime}$ which takes the input of discrete latent codes. At alternating steps, instead of using $g$, we obtain the next state by $z^{2 m+1}=g^{\prime}\left(z^{2 m}, k^{2 m}\right)$ where $m&gt;0$. We also introduce an additional prediction head $\tau$ to predict the discrete latent codes. The additional function $g^{\prime}$ is a 30-layer residual convolution stack with 1024 hiddens and 256 bottleneck hiddens. Unlike for actions, discrete latent codes after termination of the game don't need random padding or masking.</p>
<p>We use the same training setup and optimizer parameters as MuZeroChess. VQHybrid model is trained for 200k steps; VQPure model is trained for 400k steps.</p>
<p>MCTS The hyperparameters used for the MCTS are the same for baseline MuZeroChess and VQM-MCTS: discount $=1.0$, UCB parameters $c_{\text {base }}=19652.0$ and $c_{\text {init }}=1.25$. No temperature is applied on the acting visit count policy. Same as MuZero (Schrittwieser et al., 2019),</p>
<p>we limit the agent action to all legal moves. MuZeroChess and VQHybrid agents don’t have terminal state. VQPure agent reaches terminal state when the same VQ code is taken for 10 consecutive steps in the search tree branch.</p>
<p>For experiments where VQ agent is playing against Q-value, imitation and MuZeroChess agents, we employ similar strategy used for data generation in <em>Schrittwieser et al. (2019)</em>. This is because both agents are deterministic, playing among them would result in deterministic games making the evaluation less meaningful. Specifically, instead of selecting the action with the highest visit count at the end of the tree search, we use a stochastic variant. We keep a pool of possible actions which have an action count of at least 1% the total count $\mathcal{A}=\left{a:N(a)&gt;0.01N_{max}\right}$, and sample an action $a$ according to the probability induced by visit counts $p(a)=\frac{N(a)}{\sum_{a^{\prime} \in \mathcal{A}} N\left(a^{\prime}\right)}$. This stochastic variant of the tree policy is used for the first 30 steps of the game for all MCTS-based agents.</p>
<h3>A.3. Quasirandom Sampling</h3>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 10. Quasirandom sampling produces empirical distributions closer to the true distribution than random sampling. The plotted error is the Euclidean distance between the probability distributions. For this analysis, we sampled the probabilities for the Multinomial distribution from a Dirichlet distribution with $\left{\alpha_{i}\right}_{i=1}^{N}=1$ where $N=64$.</p>
<p>As explained in Section 4.2, our MCTS implementation takes the following form:</p>
<p>$$
\arg \max _{k} \hat{Q}(s, k)+P(k \mid s) U(s, k)
$$</p>
<p>where</p>
<p>$$
\begin{aligned}
U(s, a) &amp; =\frac{\sqrt{N(s)}}{1+N(s, a)}\left[c_{1}+\log \left(\frac{N(s)+c_{2}+1}{c_{2}}\right)\right] \
\hat{Q}(s, k) &amp; = \begin{cases}Q(s, k) &amp; \text { cooperative } \
0 &amp; \text { neutral } \
-Q(s, k) &amp; \text { adversarial }\end{cases}
\end{aligned}
$$</p>
<p>If we assume the environment to be neutral, the empirical distribution of the selected discrete latent codes at planning time should match the estimated distribution of codes
$P(k \mid s)$. A straightforward way to obtain such a distribution is to sample i.i.d from the estimated distribution. However, in practice, we found that using the above equation with $\hat{Q}(s, k)=0$ works better. This corresponds to a quasirandom Monte Carlo sample of a multinomial distribution $p_{i}$ where we simply select the value which has the largest value of $\frac{p_{i}}{N(i)+1}$, where $N$ is the number of times $i$ has been sampled already.</p>
<h3>A.4. Chess Agents Performance against Stockfish</h3>
<p>Because VQ Models have a pre-training phase, a confounding factor for the performance comparison is the amount of computational resources required. We increase the computational resources for MuZeroChess baselines for 3 times. We call the resulting agents MuZeroChess@600k and MuZeroChess Single@600k. This increase is more than that of the pre-training phase since our state VQVAE is trained on a pair of observations whereas the transition model is trained on a sequence of 10 steps. Figure 11 reports the full results of MuZeroChess and VQ agents playing against Stockfish. As Figure 11 shows, with the increase of computational budget, MuZeroChess agent does have a slight performance improvement. However, this is not significant enough to affect the conclusions. Most importantly, MuZeroChess Single agent does not perform better even with significantly more compute. In fact, we see a decrease in win rate across all the Stockfish levels with 1200 simulations.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 11. Agent performances evaluated against Stockfish 10 with 1, 10, 100, 400, 800 and 1200 simulations per step. Compare agent performance with different number of simulation budget per move. Reported for agents: two-player MuZeroChess, two-player MuZeroChess@600k, single-player MuZeroChess, single-player MuZeroChess@600k, VQHybrid and VQPure searching over worst case scenario, VQHybrid and VQPure searching over neutral scenario. Stockfish 10 skill levels is varied between $0,5,10,15$ and 20 to control the strength of the engine.Red bar shows the win rate; blue bar shows the draw rate of the agents.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://www.ficsgames.org/download.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
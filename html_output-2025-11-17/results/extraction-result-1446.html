<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1446 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1446</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1446</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-26.html">extraction-schema-26</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <p><strong>Paper ID:</strong> paper-287a30043ad1c3e349095af7e3e42d3be3b6c0c9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/287a30043ad1c3e349095af7e3e42d3be3b6c0c9" target="_blank">SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> SNIP, a Symbolic-Numeric Integrated Pre-training model, which employs contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the embeddings is introduced, revealing that symbolic supervision enhances the embeddings of numeric data and vice versa.</p>
                <p><strong>Paper Abstract:</strong> In an era where symbolic mathematical equations are indispensable for modeling complex natural phenomena, scientific inquiry often involves collecting observations and translating them into mathematical expressions. Recently, deep learning has emerged as a powerful tool for extracting insights from data. However, existing models typically specialize in either numeric or symbolic domains, and are usually trained in a supervised manner tailored to specific tasks. This approach neglects the substantial benefits that could arise from a task-agnostic multi-modal understanding between symbolic equations and their numeric counterparts. To bridge the gap, we introduce SNIP, a Symbolic-Numeric Integrated Pre-training model, which employs contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the embeddings. By performing latent space analysis, we observe that SNIP provides cross-domain insights into the representations, revealing that symbolic supervision enhances the embeddings of numeric data and vice versa. We evaluate SNIP across diverse tasks, including symbolic-to-numeric mathematical property prediction and numeric-to-symbolic equation discovery, commonly known as symbolic regression. Results show that SNIP effectively transfers to various tasks, consistently outperforming fully supervised baselines and competing strongly with established task-specific methods, especially in the low data regime scenarios where available data is limited. Code and model are available at: https://github.com/deep-symbolic-mathematics/Multimodal-Math-Pretraining</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1446.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1446.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SNIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic-Numeric Integrated Pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-encoder Transformer pre-training model that learns a joint embedding space for symbolic mathematical expressions and their numeric observations via contrastive learning, enabling cross-modal tasks and improved symbolic regression through latent-space optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SNIP</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Dual Transformer encoders (symbolic and numeric) trained with a symmetric contrastive (InfoNCE) objective on ~60M synthetic (symbolic, numeric) pairs; numeric encoder uses tokenized floating-point tokens, embedder, multi-layer Transformer and attention pooling; symbolic encoder tokenizes prefix-order expressions into a Transformer with positional embeddings; embeddings are used for downstream predictors and for generative decoding via an expression decoder and a mapping network.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Mathematics / Symbolic Regression (equation discovery) and numerical-symbolic cross-modal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Numeric-to-symbolic equation discovery (symbolic regression) and cross-modal prediction of numeric function properties (e.g., non-convexity ratio, upwardness). SNIP is used to generate symbolic expressions from numeric datasets by adding an expression decoder on top of the numeric encoder and performing Latent Space Optimization (LSO) in the pretrained embedding space to find expressions that trade off accuracy and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper does not characterize discoveries as 'incremental' or 'transformational' (no explicit terminology in the text). It frames SNIP as a 'pioneering pre-training method' and as providing state-of-the-art performance on standard SR benchmarks, but does not assign an incremental/transformational label.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Quantitative evaluations use NMSE and R^2 for regression/fitting accuracy, Acc_{0.1} (percentage within absolute tolerance 0.1 after normalization) for cross-modal property prediction, Pareto front analysis of R^2 versus expression complexity (node count) for symbolic regression quality/complexity trade-offs, low-data regime R^2 curves (learning curves), and qualitative latent-space analyses (t-SNE) and interpolation experiments. In LSO, fitness is R^2 on training data; constants refined with BFGS; optimization of latent vectors uses gradient-free optimizers. Benchmarks used include SRBench datasets (Feynman, ODE-Strogatz, Black-box).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation is performed by: (1) held-out test sets for property prediction and SRBench test splits; (2) comparison against established SR baselines (E2E transformer SR, Operon, AIFeynman, other SRBench baselines) via Pareto rankings and numeric metrics; (3) ablation studies (LSO, choice of optimizer) reported in appendix; (4) qualitative latent interpolation experiments showing semantic interpolation between functions; (5) constant optimization (BFGS) and final evaluation on holdout test sets. No wet-lab or physical experiment validation is performed — validation is computational and benchmark-based.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is argued via (a) the multimodal symbolic-numeric pre-training paradigm (joint contrastive learning between symbolic and numeric modalities), (b) demonstrated transfer to multiple downstream tasks (cross-modal property prediction and symbolic regression) with strong low-data generalization, and (c) the use of SNIP's interpolatable generative latent space plus LSO to improve SR search. The paper assesses novelty by outperforming or matching state-of-the-art baselines on SRBench and by showing qualitatively meaningful latent-space interpolation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Reported numeric results used to demonstrate impact include: SNIP placed on the first Pareto-front across SRBench datasets; Strogatz accuracy R^2 = 0.928; Feynman accuracy R^2 = 0.882 (vs AIFeynman 0.798); Black-box complexity median 47.52 vs Operon 64.95; Feynman complexity median 31.63 (SNIP) vs Operon 69.87. For cross-modal property prediction, SNIP (finetuned) NMSEs and Acc_{0.1}: e.g., Non-Convexity Ratio NMSE 0.0683 and Acc_{0.1} 92.1%; Upwardness NMSE 0.0400 and Acc_{0.1} 90.1%. Low-data regime example: with 100 training samples supervised baseline R^2 = 0.292 while SNIP variants > 0.745.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Comparisons are made primarily to other automated SR methods (E2E transformer SR, GP-based Operon, AIFeynman, and SRBench baselines). Key findings: SNIP attains Pareto-frontier positions (best trade-offs) across datasets, superior R^2 on Strogatz and Feynman vs baselines, and lower complexity than Operon on some datasets. The paper does not directly compare SNIP-generated discoveries to human-derived scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>The paper does not report a single scalar 'success rate' for discoveries; evaluation is reported with benchmark metrics and Pareto-front rankings (examples: R^2 values, complexity reductions, high Acc_{0.1} values). It reports consistent top/ranked performance across SRBench dataset groups rather than a percent success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Authors note limits relevant to discovery evaluation: SNIP struggles with data not expressible as closed-form mathematics; constrained by the synthetic data generation protocol (operator vocabulary, max input dimensionality D ≤ 10); dependence on quality/diversity of pretraining data; combinatorial limits of expression vocabularies; the paper does not address epistemic validation of 'scientific' novelty beyond benchmark fits (i.e., no downstream experimental confirmation). Distinguishing incremental vs transformational discoveries is not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1446.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1446.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-End symbolic regression with transformers (E2E)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based end-to-end symbolic regression model that maps numeric datasets to symbolic expressions; used in this paper as a pre-trained decoder initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-to-end symbolic regression with transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>E2E transformer SR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pretrained transformer decoder trained to generate symbolic expressions from numeric input encodings (Kamienny et al., 2022). In this paper E2E's pretrained SR decoder is reused (initialized) and paired with SNIP's numeric embeddings via a learned mapping network.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Symbolic Regression / Mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Generates symbolic expressions from numeric data; here used as the expression-generation decoder component initialized from prior E2E pretraining to produce symbolic candidates from SNIP embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>Paper references E2E as a state-of-the-art SR method but does not apply incremental/transformational labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluated (as a baseline in literature) by fitting accuracy (R^2), expression complexity, and benchmark performance on SRBench; in this paper E2E's decoder is evaluated indirectly through downstream SR performance when combined with SNIP embeddings and LSO.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation in literature via SRBench and benchmark comparisons; in this paper, validation is via comparative performance (Pareto fronts) when E2E decoder is initialized with SNIP-derived inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>E2E is presented as a modern transformer SR approach; novelty relative to older GP or specialized SR methods is implied but not deeply discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Used as a strong baseline/initialization; impact assessed by final SRBench rankings when used with SNIP mapping network.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Paper uses E2E decoder as a component rather than directly comparing it as a stand-alone baseline in the main results; comparisons in results include E2E among SRBench baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported here (E2E reported elsewhere); in this work it serves as an initialization improving SNIP-based SR generation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>E2E is constrained by its pretraining regime and by decoder compatibility; SNIP uses a mapping module to adapt SNIP embeddings to E2E decoder input.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1446.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1446.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming for Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical evolutionary search methods that evolve symbolic expressions (trees) using genetic operators (mutation, crossover) to fit numeric data; historically foundational for SR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Operon c++: An efficient genetic programming framework for symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Programming (GP) symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Population-based evolutionary search over expression trees using mutation, crossover, and fitness selection (Schmidt & Lipson and Operon frameworks). The paper references GP as the foundational approach for SR and notes its combinatorial search properties and limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Symbolic Regression / Machine Learning / Computational Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>GP methods search the space of symbolic expressions to find equations that fit numeric data, enabling automated discovery of empirical laws; discussed as historical/related work and baseline in SRBench comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>Paper does not label GP discoveries as incremental or transformational.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>GP methods are generally evaluated by fitting metrics (R^2), complexity of resulting expressions (tree size), and benchmark comparisons (SRBench). The paper references Operon as an efficient GP implementation used in SRBench.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Comparison to known ground-truth symbolic functions in benchmarks, held-out test performance, and Pareto trade-offs between fit and complexity in SRBench.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>GP is characterized as foundational; novelty critiques in paper focus on limitations (computational intensity, limited semantic priors) rather than novelty of GP itself.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Operon baseline complexity and accuracy values are reported for comparison (Operon complexity values: e.g., 64.95 on Black-box median; Operon complexity 69.87 on Feynman as reported in SNIP results summary).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>GP methods are compared as baselines (Operon) using Pareto plots; SNIP reports lower complexity or better accuracy in some dataset groups relative to Operon.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Paper does not provide a single success rate for GP; Operon baseline metrics are used for median comparisons in Pareto analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper highlights GP limitations: computational intensity, limited semantic depth, need to restart search per dataset, and propensity for large numeric behavior deviations from genetic operations (mutation/crossover).</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1446.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1446.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIFeynman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics-inspired symbolic regression method designed to discover closed-form formulas from data by leveraging physics-inspired decompositions and search techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ai feynman: A physics-inspired method for symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A specialized SR algorithm combining dimensional analysis, symbolic manipulation, and heuristic searches tailored to physical-law discovery; referenced as a strong baseline for the Feynman dataset within SRBench.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Symbolic Regression / Physics</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Automatically discovers closed-form expressions representing physical laws from numeric data; in this paper it is cited as a baseline (AIFeynman) for comparison on Feynman datasets where SNIP reports higher R^2.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>No characterization of discoveries as incremental or transformational is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluated on Feynman benchmark problems using R^2 accuracy and complexity; compared in Pareto analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Benchmark comparisons (SRBench) against ground-truth Feynman equations and other SR methods.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Presented here as a strong specialized baseline for physics-inspired SR; novelty relative to SNIP is not deeply discussed beyond empirical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Paper reports comparative R^2: SNIP R^2 0.882 vs AIFeynman 0.798 on Feynman datasets (median/aggregate reporting in SNIP results).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>SNIP is compared favorably to AIFeynman on Feynman dataset accuracy (reported R^2 values).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not given as a single success-rate; presented via benchmark R^2 comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>AIFeynman is specialized for physics-like formulas and may not generalize across all SR problem classes; paper does not elaborate further.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1446.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1446.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Operon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Operon C++ Genetic Programming Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An efficient GP framework for symbolic regression used as a state-of-the-art classical baseline in SRBench comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Operon c++: An efficient genetic programming framework for symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Operon (GP framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>C++ implementation of genetic programming optimized for symbolic regression tasks (Burlacu et al., 2020); used in SRBench as a competitive baseline for accuracy and expression complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Symbolic Regression / Evolutionary Computation</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Operon searches for symbolic expressions that fit datasets using evolutionary operators; compared in SNIP via complexity and accuracy medians on SRBench dataset groups.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>No incremental/transformational labeling is provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Operon is evaluated in SRBench by R^2 and expression complexity (node count); SNIP compares median complexity and accuracy against Operon (e.g., Operon complexity 64.95 on Black-box, 69.87 on Feynman per SNIP summary).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Benchmark comparisons on SRBench datasets against ground-truth where available; Pareto-front comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Operon is framed as an efficient GP baseline; novelty not assessed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Complexity and accuracy medians reported for baseline comparison; SNIP reports lower complexity or better accuracy on several dataset groups.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>SNIP achieves lower median complexity and competitive or better accuracy compared to Operon on SRBench groups as reported in Pareto plots.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>No single success-rate reported; performance reported via Pareto rankings and median metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Operon, as a GP method, shares GP limitations discussed in paper: computational intensity and limited semantic priors.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>End-to-end symbolic regression with transformers <em>(Rating: 2)</em></li>
                <li>Neural symbolic regression that scales <em>(Rating: 2)</em></li>
                <li>Ai feynman: A physics-inspired method for symbolic regression <em>(Rating: 2)</em></li>
                <li>Contemporary symbolic regression methods and their relative performance <em>(Rating: 2)</em></li>
                <li>Distilling free-form natural laws from experimental data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1446",
    "paper_id": "paper-287a30043ad1c3e349095af7e3e42d3be3b6c0c9",
    "extraction_schema_id": "extraction-schema-26",
    "extracted_data": [
        {
            "name_short": "SNIP",
            "name_full": "Symbolic-Numeric Integrated Pre-training",
            "brief_description": "A dual-encoder Transformer pre-training model that learns a joint embedding space for symbolic mathematical expressions and their numeric observations via contrastive learning, enabling cross-modal tasks and improved symbolic regression through latent-space optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SNIP",
            "system_description": "Dual Transformer encoders (symbolic and numeric) trained with a symmetric contrastive (InfoNCE) objective on ~60M synthetic (symbolic, numeric) pairs; numeric encoder uses tokenized floating-point tokens, embedder, multi-layer Transformer and attention pooling; symbolic encoder tokenizes prefix-order expressions into a Transformer with positional embeddings; embeddings are used for downstream predictors and for generative decoding via an expression decoder and a mapping network.",
            "discovery_domain": "Mathematics / Symbolic Regression (equation discovery) and numerical-symbolic cross-modal reasoning",
            "discovery_description": "Numeric-to-symbolic equation discovery (symbolic regression) and cross-modal prediction of numeric function properties (e.g., non-convexity ratio, upwardness). SNIP is used to generate symbolic expressions from numeric datasets by adding an expression decoder on top of the numeric encoder and performing Latent Space Optimization (LSO) in the pretrained embedding space to find expressions that trade off accuracy and complexity.",
            "discovery_type": "",
            "discovery_type_justification": "The paper does not characterize discoveries as 'incremental' or 'transformational' (no explicit terminology in the text). It frames SNIP as a 'pioneering pre-training method' and as providing state-of-the-art performance on standard SR benchmarks, but does not assign an incremental/transformational label.",
            "evaluation_methods": "Quantitative evaluations use NMSE and R^2 for regression/fitting accuracy, Acc_{0.1} (percentage within absolute tolerance 0.1 after normalization) for cross-modal property prediction, Pareto front analysis of R^2 versus expression complexity (node count) for symbolic regression quality/complexity trade-offs, low-data regime R^2 curves (learning curves), and qualitative latent-space analyses (t-SNE) and interpolation experiments. In LSO, fitness is R^2 on training data; constants refined with BFGS; optimization of latent vectors uses gradient-free optimizers. Benchmarks used include SRBench datasets (Feynman, ODE-Strogatz, Black-box).",
            "validation_approaches": "Validation is performed by: (1) held-out test sets for property prediction and SRBench test splits; (2) comparison against established SR baselines (E2E transformer SR, Operon, AIFeynman, other SRBench baselines) via Pareto rankings and numeric metrics; (3) ablation studies (LSO, choice of optimizer) reported in appendix; (4) qualitative latent interpolation experiments showing semantic interpolation between functions; (5) constant optimization (BFGS) and final evaluation on holdout test sets. No wet-lab or physical experiment validation is performed — validation is computational and benchmark-based.",
            "novelty_assessment": "Novelty is argued via (a) the multimodal symbolic-numeric pre-training paradigm (joint contrastive learning between symbolic and numeric modalities), (b) demonstrated transfer to multiple downstream tasks (cross-modal property prediction and symbolic regression) with strong low-data generalization, and (c) the use of SNIP's interpolatable generative latent space plus LSO to improve SR search. The paper assesses novelty by outperforming or matching state-of-the-art baselines on SRBench and by showing qualitatively meaningful latent-space interpolation.",
            "impact_metrics": "Reported numeric results used to demonstrate impact include: SNIP placed on the first Pareto-front across SRBench datasets; Strogatz accuracy R^2 = 0.928; Feynman accuracy R^2 = 0.882 (vs AIFeynman 0.798); Black-box complexity median 47.52 vs Operon 64.95; Feynman complexity median 31.63 (SNIP) vs Operon 69.87. For cross-modal property prediction, SNIP (finetuned) NMSEs and Acc_{0.1}: e.g., Non-Convexity Ratio NMSE 0.0683 and Acc_{0.1} 92.1%; Upwardness NMSE 0.0400 and Acc_{0.1} 90.1%. Low-data regime example: with 100 training samples supervised baseline R^2 = 0.292 while SNIP variants &gt; 0.745.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "Comparisons are made primarily to other automated SR methods (E2E transformer SR, GP-based Operon, AIFeynman, and SRBench baselines). Key findings: SNIP attains Pareto-frontier positions (best trade-offs) across datasets, superior R^2 on Strogatz and Feynman vs baselines, and lower complexity than Operon on some datasets. The paper does not directly compare SNIP-generated discoveries to human-derived scientific discoveries.",
            "success_rate": "The paper does not report a single scalar 'success rate' for discoveries; evaluation is reported with benchmark metrics and Pareto-front rankings (examples: R^2 values, complexity reductions, high Acc_{0.1} values). It reports consistent top/ranked performance across SRBench dataset groups rather than a percent success rate.",
            "challenges_limitations": "Authors note limits relevant to discovery evaluation: SNIP struggles with data not expressible as closed-form mathematics; constrained by the synthetic data generation protocol (operator vocabulary, max input dimensionality D ≤ 10); dependence on quality/diversity of pretraining data; combinatorial limits of expression vocabularies; the paper does not address epistemic validation of 'scientific' novelty beyond benchmark fits (i.e., no downstream experimental confirmation). Distinguishing incremental vs transformational discoveries is not discussed.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1446.0",
            "source_info": {
                "paper_title": "SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "E2E",
            "name_full": "End-to-End symbolic regression with transformers (E2E)",
            "brief_description": "A transformer-based end-to-end symbolic regression model that maps numeric datasets to symbolic expressions; used in this paper as a pre-trained decoder initialization.",
            "citation_title": "End-to-end symbolic regression with transformers",
            "mention_or_use": "use",
            "system_name": "E2E transformer SR",
            "system_description": "Pretrained transformer decoder trained to generate symbolic expressions from numeric input encodings (Kamienny et al., 2022). In this paper E2E's pretrained SR decoder is reused (initialized) and paired with SNIP's numeric embeddings via a learned mapping network.",
            "discovery_domain": "Symbolic Regression / Mathematics",
            "discovery_description": "Generates symbolic expressions from numeric data; here used as the expression-generation decoder component initialized from prior E2E pretraining to produce symbolic candidates from SNIP embeddings.",
            "discovery_type": "",
            "discovery_type_justification": "Paper references E2E as a state-of-the-art SR method but does not apply incremental/transformational labels.",
            "evaluation_methods": "Evaluated (as a baseline in literature) by fitting accuracy (R^2), expression complexity, and benchmark performance on SRBench; in this paper E2E's decoder is evaluated indirectly through downstream SR performance when combined with SNIP embeddings and LSO.",
            "validation_approaches": "Validation in literature via SRBench and benchmark comparisons; in this paper, validation is via comparative performance (Pareto fronts) when E2E decoder is initialized with SNIP-derived inputs.",
            "novelty_assessment": "E2E is presented as a modern transformer SR approach; novelty relative to older GP or specialized SR methods is implied but not deeply discussed here.",
            "impact_metrics": "Used as a strong baseline/initialization; impact assessed by final SRBench rankings when used with SNIP mapping network.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "Paper uses E2E decoder as a component rather than directly comparing it as a stand-alone baseline in the main results; comparisons in results include E2E among SRBench baselines.",
            "success_rate": "Not reported here (E2E reported elsewhere); in this work it serves as an initialization improving SNIP-based SR generation.",
            "challenges_limitations": "E2E is constrained by its pretraining regime and by decoder compatibility; SNIP uses a mapping module to adapt SNIP embeddings to E2E decoder input.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1446.1",
            "source_info": {
                "paper_title": "SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GP",
            "name_full": "Genetic Programming for Symbolic Regression",
            "brief_description": "Classical evolutionary search methods that evolve symbolic expressions (trees) using genetic operators (mutation, crossover) to fit numeric data; historically foundational for SR.",
            "citation_title": "Operon c++: An efficient genetic programming framework for symbolic regression",
            "mention_or_use": "mention",
            "system_name": "Genetic Programming (GP) symbolic regression",
            "system_description": "Population-based evolutionary search over expression trees using mutation, crossover, and fitness selection (Schmidt & Lipson and Operon frameworks). The paper references GP as the foundational approach for SR and notes its combinatorial search properties and limitations.",
            "discovery_domain": "Symbolic Regression / Machine Learning / Computational Discovery",
            "discovery_description": "GP methods search the space of symbolic expressions to find equations that fit numeric data, enabling automated discovery of empirical laws; discussed as historical/related work and baseline in SRBench comparisons.",
            "discovery_type": "",
            "discovery_type_justification": "Paper does not label GP discoveries as incremental or transformational.",
            "evaluation_methods": "GP methods are generally evaluated by fitting metrics (R^2), complexity of resulting expressions (tree size), and benchmark comparisons (SRBench). The paper references Operon as an efficient GP implementation used in SRBench.",
            "validation_approaches": "Comparison to known ground-truth symbolic functions in benchmarks, held-out test performance, and Pareto trade-offs between fit and complexity in SRBench.",
            "novelty_assessment": "GP is characterized as foundational; novelty critiques in paper focus on limitations (computational intensity, limited semantic priors) rather than novelty of GP itself.",
            "impact_metrics": "Operon baseline complexity and accuracy values are reported for comparison (Operon complexity values: e.g., 64.95 on Black-box median; Operon complexity 69.87 on Feynman as reported in SNIP results summary).",
            "comparison_to_human_discoveries": false,
            "comparison_details": "GP methods are compared as baselines (Operon) using Pareto plots; SNIP reports lower complexity or better accuracy in some dataset groups relative to Operon.",
            "success_rate": "Paper does not provide a single success rate for GP; Operon baseline metrics are used for median comparisons in Pareto analyses.",
            "challenges_limitations": "Paper highlights GP limitations: computational intensity, limited semantic depth, need to restart search per dataset, and propensity for large numeric behavior deviations from genetic operations (mutation/crossover).",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1446.2",
            "source_info": {
                "paper_title": "SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "AIFeynman",
            "name_full": "AI Feynman",
            "brief_description": "A physics-inspired symbolic regression method designed to discover closed-form formulas from data by leveraging physics-inspired decompositions and search techniques.",
            "citation_title": "Ai feynman: A physics-inspired method for symbolic regression",
            "mention_or_use": "mention",
            "system_name": "AI Feynman",
            "system_description": "A specialized SR algorithm combining dimensional analysis, symbolic manipulation, and heuristic searches tailored to physical-law discovery; referenced as a strong baseline for the Feynman dataset within SRBench.",
            "discovery_domain": "Symbolic Regression / Physics",
            "discovery_description": "Automatically discovers closed-form expressions representing physical laws from numeric data; in this paper it is cited as a baseline (AIFeynman) for comparison on Feynman datasets where SNIP reports higher R^2.",
            "discovery_type": "",
            "discovery_type_justification": "No characterization of discoveries as incremental or transformational is provided in this paper.",
            "evaluation_methods": "Evaluated on Feynman benchmark problems using R^2 accuracy and complexity; compared in Pareto analyses.",
            "validation_approaches": "Benchmark comparisons (SRBench) against ground-truth Feynman equations and other SR methods.",
            "novelty_assessment": "Presented here as a strong specialized baseline for physics-inspired SR; novelty relative to SNIP is not deeply discussed beyond empirical comparisons.",
            "impact_metrics": "Paper reports comparative R^2: SNIP R^2 0.882 vs AIFeynman 0.798 on Feynman datasets (median/aggregate reporting in SNIP results).",
            "comparison_to_human_discoveries": false,
            "comparison_details": "SNIP is compared favorably to AIFeynman on Feynman dataset accuracy (reported R^2 values).",
            "success_rate": "Not given as a single success-rate; presented via benchmark R^2 comparison.",
            "challenges_limitations": "AIFeynman is specialized for physics-like formulas and may not generalize across all SR problem classes; paper does not elaborate further.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1446.3",
            "source_info": {
                "paper_title": "SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Operon",
            "name_full": "Operon C++ Genetic Programming Framework",
            "brief_description": "An efficient GP framework for symbolic regression used as a state-of-the-art classical baseline in SRBench comparisons.",
            "citation_title": "Operon c++: An efficient genetic programming framework for symbolic regression",
            "mention_or_use": "mention",
            "system_name": "Operon (GP framework)",
            "system_description": "C++ implementation of genetic programming optimized for symbolic regression tasks (Burlacu et al., 2020); used in SRBench as a competitive baseline for accuracy and expression complexity.",
            "discovery_domain": "Symbolic Regression / Evolutionary Computation",
            "discovery_description": "Operon searches for symbolic expressions that fit datasets using evolutionary operators; compared in SNIP via complexity and accuracy medians on SRBench dataset groups.",
            "discovery_type": "",
            "discovery_type_justification": "No incremental/transformational labeling is provided in the paper.",
            "evaluation_methods": "Operon is evaluated in SRBench by R^2 and expression complexity (node count); SNIP compares median complexity and accuracy against Operon (e.g., Operon complexity 64.95 on Black-box, 69.87 on Feynman per SNIP summary).",
            "validation_approaches": "Benchmark comparisons on SRBench datasets against ground-truth where available; Pareto-front comparisons.",
            "novelty_assessment": "Operon is framed as an efficient GP baseline; novelty not assessed in this paper.",
            "impact_metrics": "Complexity and accuracy medians reported for baseline comparison; SNIP reports lower complexity or better accuracy on several dataset groups.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "SNIP achieves lower median complexity and competitive or better accuracy compared to Operon on SRBench groups as reported in Pareto plots.",
            "success_rate": "No single success-rate reported; performance reported via Pareto rankings and median metrics.",
            "challenges_limitations": "Operon, as a GP method, shares GP limitations discussed in paper: computational intensity and limited semantic priors.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1446.4",
            "source_info": {
                "paper_title": "SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "End-to-end symbolic regression with transformers",
            "rating": 2
        },
        {
            "paper_title": "Neural symbolic regression that scales",
            "rating": 2
        },
        {
            "paper_title": "Ai feynman: A physics-inspired method for symbolic regression",
            "rating": 2
        },
        {
            "paper_title": "Contemporary symbolic regression methods and their relative performance",
            "rating": 2
        },
        {
            "paper_title": "Distilling free-form natural laws from experimental data",
            "rating": 1
        }
    ],
    "cost": 0.016745999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training</h1>
<p>Kazem Meidani ${ }^{\text {+1 }}$, Parshin Shojaee ${ }^{+2}$, Chandan K. Reddy ${ }^{2}$, Amir Barati Farimani ${ }^{1,3}$<br>${ }^{1}$ Department of Mechanical Engineering, Carnegie Mellon University<br>${ }^{2}$ Department of Computer Science, Virginia Tech<br>${ }^{3}$ Machine Learning Department, Carnegie Mellon University</p>
<h4>Abstract</h4>
<p>In an era where symbolic mathematical equations are indispensable for modeling complex natural phenomena, scientific inquiry often involves collecting observations and translating them into mathematical expressions. Recently, deep learning has emerged as a powerful tool for extracting insights from data. However, existing models typically specialize in either numeric or symbolic domains, and are usually trained in a supervised manner tailored to specific tasks. This approach neglects the substantial benefits that could arise from a task-agnostic multi-modal understanding between symbolic equations and their numeric counterparts. To bridge the gap, we introduce SNIP, a Symbolic-Numeric Integrated Pre-training model, which employs contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the embeddings. By performing latent space analysis, we observe that SNIP provides cross-domain insights into the representations, revealing that symbolic supervision enhances the embeddings of numeric data and vice versa. We evaluate SNIP across diverse tasks, including symbolic-to-numeric mathematical property prediction and numeric-to-symbolic equation discovery, commonly known as symbolic regression. Results show that SNIP effectively transfers to various tasks, consistently outperforming fully supervised baselines and competing strongly with established task-specific methods, especially in the low data regime scenarios where available data is limited ${ }^{1}$.</p>
<h2>1 INTRODUCTION</h2>
<p>Throughout the history of science, symbolic mathematics has been unreasonably effective in representing natural phenomena (Wigner, 1960). Complex patterns of natural systems, represented as numeric data observations, can be elegantly abstracted using mathematical formulas. Mathematical symbolism has given us the language to describe, understand, and predict the natural world. The challenge of bridging the gap between the numeric observations and their mathematical symbolic representations has been a consistent focus in many scientific and engineering domains. Recognizing and exploring this connection is crucial, as it promises to drive advancements in various fields.
In recent years, deep learning has demonstrated promising capabilities in learning from symbolic mathematics language as well as extracting insights from numeric data observations. Transformer models (Vaswani et al., 2017), in particular, have emerged as frontrunners in this field, effectively capturing patterns within mathematical expressions and solving complex tasks such as differential equations and function integration (Lample \&amp; Charton, 2020; Welleck et al., 2022). However, these models, while powerful, are not inherently designed to handle numeric data inputs. While some pretrained symbolic regression models have been introduced to map numeric datasets to their governing mathematical expressions in a supervised manner (Biggio et al., 2021; Kamienny et al., 2022), a gap still remains in developing a task-agnostic pre-training model capable of mutual understanding between the modalities of symbolic mathematical equations and their corresponding numeric data.
Multi-modal pre-training models, exemplified by groundbreaking models like Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021), have found a significant place in the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>deep learning landscape. CLIP has particularly set new standards in vision-language tasks, bridging the understanding between visual content and natural language descriptions. Expanding beyond traditional vision-language domains, recent studies have broadened multi-modal pre-training to include other modalities, such as audio and tabular data (Liu et al., 2021; Zhang et al., 2023; Hager et al., 2023). Additionally, previously untouched scientific domains, like molecular representation, are also benefiting from these advancements in multi-modal representations (Su et al., 2022; Cao et al., 2023). Nevertheless, the symbolic-numeric domain remains relatively unexplored. Considering the foundational role of symbolic mathematics in science and the ubiquity of numeric data, an in-depth exploration of their mutual learning is not only timely but essential.
In this work, we present Symbolic-Numeric Integrated Pre-training (SNIP) to connect the two often distinct worlds of symbolic mathematical expressions and their corresponding numeric manifestations. The architecture of SNIP, depicted in Fig. 1, incorporates dual Transformer encoders, with each encoder dedicated to learning the symbolic or numeric representations of mathematical functions. Subsequently, a task-agnostic contrastive objective is employed to enhance the similarity between (symbolic, numeric) pairs of data. The multi-modal pre-training of SNIP provides capabilities to understand and generate cross-modal content. Our experiments show that SNIP achieves remarkable performance in cross-modal mathematical understanding and prediction tasks. Additionally, by combining SNIP with an equation generation decoder and exploiting its interpolatable latent space, we can effectively harness SNIP's mutual knowledge for the task of numeric-to-symbolic equation discovery (known as symbolic regression), achieving competitive results with state-of-the-art baselines. The major contributions of this work can be summarized as follows:</p>
<ul>
<li>Proposing SNIP, a pioneering pre-training method that integrates mathematical symbolic and numeric domains through joint representation learning. This approach captures mutual relationships, delivering embeddings that are informed and enhanced by both domains.</li>
<li>Evaluating SNIP in cross-modal comprehension across different mathematical property prediction tasks. Our results indicate that SNIP outperforms the fully supervised baselines, particularly in low data regime scenarios. Visualizing the latent embeddings also confirms that SNIP's pretrained representations reveal patterns linked to these cross-modal mathematical properties.</li>
<li>Leveraging SNIP for numeric-to-symbolic equation generation task, commonly known as symbolic regression. In this task, after training an expression generation decoder on top of SNIP's numeric encoder, we exploit the high-quality semantic within SNIP's continuous and lowdimensional latent representations to perform latent space optimization with the objective of finding equations with balanced accuracy-complexity. Results show that SNIP achieves state-of-theart performance on the well-known SRBench (La Cava et al., 2021) benchmark.</li>
</ul>
<h1>2 RELATED WORK</h1>
<p>Large-scale Pre-training. Our work is built upon an extensive body of research advocating the advantages of pre-training large models on large datasets (Zhou et al., 2023; Zong et al., 2023). Initially, pre-training was single-modal, with self-supervised learning (SSL) as a key paradigm that used data as its own supervision, especially useful where labeled data was limited (Balestriero et al., 2023). This paved the way for the emergence of multi-modal pre-training, where models are trained to understand relationships across different modalities (Wang et al., 2023). Vision and language have traditionally played the two main characters of pre-training models. For instance, CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), and FLAVA (Singh et al., 2022) utilize image-caption pairs to construct jointly learned embedding spaces. These models are trained to align the embeddings of corresponding image-caption pairs while distancing unrelated pairs. The success of multi-modal pre-training in vision and language spurred its adoption in other domains. For example, recent works have extended this approach to videos, audio, and even tabular data (Liu et al., 2021; Dong et al., 2022; Hager et al., 2023). Specialized scientific domains have also embraced this paradigm. For instance, different models have emerged to learn joint representations of molecules (Su et al., 2022; Cao et al., 2023). Our work introduces a fresh perspective, intertwining symbolic mathematics with numeric observations. To this end, we use multi-modal pre-training's potential to deepen the symbolic-numeric mutual understanding.
Deep Symbolic Mathematics. Recently, deep learning models have made significant performance in the field of mathematical reasoning (Saxton et al., 2019; Lu et al., 2023). The Transformer models, originally designed for NLP tasks (Vaswani et al., 2017), have been repurposed with remarkable success in the domain of symbolic mathematics. It has powered models that can integrate functions (Lample \&amp; Charton, 2020; Welleck et al., 2022), prove mathematical theorems (Lample et al.,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The SNIP Framework: A schematic representation of the dual-encoder pre-training scheme for mutual learning between symbolic equations and their numerical observations. Both symbolic and numeric encoders work in tandem, capturing the paired similarities and essence of their respective modalities.
2022), and perform numerical calculations, such as arithmetic operations (Charton, 2022; Jelassi et al., 2023). These achievements underscore the flexibility and potential of deep learning models in abstract reasoning. Beyond pure symbolic reasoning, there is also a growing interest in supplementing these models with numerical knowledge for improved mathematical understanding. For example, recent works have studied to enhance language models with numeric representations, aiming to improve their skills in mathematical word problem-solving (Peng et al., 2021; Liang et al., 2022; Thawani et al., 2021; Geva et al., 2020). Some recent studies have also explored different strategies for tokenizing and encoding numeric data, such as using multi-hot or continuous representation of numbers (Biggio et al., 2021; Becker et al., 2023; Golkar et al., 2023). Our work contributes a new angle to this growing field by integrating symbolic and numeric understanding in a unified multi-moal pre-training framework. By doing so, we not only capture the abstract representations of mathematical symbolic concepts but also their tangible numeric behaviors.
Symbolic Regression. Symbolic regression (SR) concentrates on discovering mathematical expressions for complex systems and representing data patterns in interpretable symbolic form. It has broad implications in both science and engineering, facilitating the modeling of diverse physical phenomena (Cranmer et al., 2020; Rudy et al., 2017; Meidani \&amp; Barati Farimani, 2023). Genetic Programming (GP) algorithms laid the foundation for SR, offering methods to search the vast space of mathematical expressions (Schmidt \&amp; Lipson, 2009; Cranmer, 2023). The ascent of deep learning subsequently gave rise to neural network-centric methods to reinforce SR's representational capabilities (Petersen et al., 2021). Some pioneering works also combined the evolutionary strengths of GP with the adaptability of neural networks, aiming for a better SR search (Udrescu \&amp; Tegmark, 2020; Mundhenk et al., 2021). However, these methods often struggle with challenges such as computational intensity, limited semantic depth, and the necessity to reinitiate search for different datasets. Inspired by the success of pre-trained Transformers in NLP, recent works introduced pre-trained models for SR (Biggio et al., 2021; Kamienny et al., 2022; Shojaee et al., 2023), using synthetic data and pre-trained priors for equation generation. Our multi-modal pre-trained model, SNIP, advances this research towards a more insightful SR direction, leveraging rich encodings that harmoniously bridge symbolic equations with their numeric counterparts.</p>
<h1>3 Pre-training</h1>
<p>As depicted in Fig. 1, the SNIP architecture comprises two Transformer encoders, each tailored for learning the symbolic or numeric representations of mathematical functions. These symbolic and numeric encoders are jointly trained with a task-agnostic contrastive learning objective to predict correct pairings within a batch of (symbolic, numeric) examples. During pre-training, SNIP receives synthetically created symbolic equations and their associated numeric data as inputs to the symbolic and numeric heads, respectively. In total, SNIP is pre-trained on approximately 60 million synthetic paired examples.</p>
<h3>3.1 Numeric Encoder</h3>
<p>The numeric encoder's foundation is rooted in the recent advancements of Transformer models for encoding numeric observations into latent spaces (Kamienny et al., 2022; D’Ascoli et al., 2022; Biggio et al., 2021). In this framework, the numeric encoder-represented as $\mathcal{E}_{\theta}^{V}$-integrates an</p>
<p>embedder, a multi-layer Transformer, and an attention pooling approach, to map numeric observations $(\boldsymbol{x}, \boldsymbol{y})$ into a condensed latent vector $\boldsymbol{Z}<em _emb="{emb" _text="\text">{V}$.
Tokenization. Following (Charton, 2022; Kamienny et al., 2022), numeric inputs are tokenized using base-10 floating-point notation. They are rounded to four significant digits and subsequently represented as sequences of three tokens: sign, mantissa (0-9999 range), and exponent ( $E$-100 to $E 100$ ). For instance, the number 5.432 is tokenized as $[+, 5432, E-3]$.
Encoding. Given a batch of $N$ numeric input points $(\boldsymbol{x}, \boldsymbol{y}) \in \mathbb{R}^{D+1}$, each is represented by $3(D+$ 1) tokens. With increasing $D$ and $N$, the input sequence length grows, challenging the quadratic complexity of Transformers. To address this, we employ an embedder, as suggested by (Kamienny et al., 2022), before the Transformer encoder. This embedder maps each input point to a unique embedding space. The resulting embeddings, with dimension $d</em>}}$, are then fed into the encoder. For the numeric encoder, we utilize a multi-layer Transformer architecture (Vaswani et al., 2017). Notably, due to the permutation invariance of the $N$ input points for each batch sample, we exclude positional embeddings, aligning with the approach in (Biggio et al., 2021). This encoder variant is denoted as $E n c^{V}$. The representation at its $l$-th layer is given by $\boldsymbol{V<em l="l">{l}=E n c</em>}^{V}\left(\boldsymbol{V<em V="V">{l-1}\right)$, where $l$ ranges from 1 to $L</em>$ signifies number of layers within the numeric encoder.
Attention-based Distillation. To distill the information from the Transformer's output into a compact representation for the whole sequence of observations, we employ an attention-based pooling mechanism, following (Santos et al., 2016). Let $\mathcal{A}}$, and $L_{V<em V="V">{V}$ denote the attention weights, which are computed as: $\mathcal{A}</em>}=\operatorname{softmax}\left(\boldsymbol{W<em L__V="L_{V">{a} \cdot \boldsymbol{V}</em>}}^{T}\right)$, where $\boldsymbol{W<em _emb="{emb" _text="\text">{a} \in \mathbb{R}^{d</em>}}}$ is a learnable weight matrix, and we take the transpose of $\boldsymbol{V<em V="V">{L</em>}} \in \mathbb{R}^{N \times d_{\text {emb }}}$ to apply softmax along the sequence dimension $N$. The compact sequence-level representation, $\boldsymbol{Z<em V="V">{V}$, is then obtained by: $\boldsymbol{Z}</em>}=\mathcal{A<em L__V="L_{V">{V} \cdot \boldsymbol{V}</em>$. This attention mechanism allows the model to focus on the most informative parts of the data points, effectively compressing the information into a fixed-size embedding.}</p>
<h1>3.2 SYMBOLIC ENCODER</h1>
<p>The symbolic encoder in our framework also draws inspiration from recent advancements in Transformer models for encoding symbolic mathematical functions, as demonstrated in works such as (Welleck et al., 2022; Lample \&amp; Charton, 2020). Here, the symbolic encoder-denoted as $\mathcal{E}<em S="S">{v}^{S}$ —is a composite entity parameterized by $\psi$, encapsulating the embedder, a multi-layer Transformer, and attention-based pooling mechanisms. Given an input symbolic expression $f$, this encoder outputs a condensed representation $\boldsymbol{Z}</em>$.
Tokenization. Mathematical expressions are tokenized by prefix order of their trees, following the principles outlined in (Lample \&amp; Charton, 2020). This process employs self-contained tokens to represent operators, variables, and integers, while constants are encoded using the same methodology as discussed in Sec. 3.1, representing each with three tokens. In alignment with (Lample \&amp; Charton, 2020), we use special tokens $[\langle B O S\rangle]$ and $[\langle E O S\rangle]$ to mark sequence start and end.
Encoding. Given a batch of symbolic expressions with $M$ tokens, each symbolic input is represented as $\boldsymbol{S}<em O="O" S_right_="S\right]" _mid_left_B="\mid\left[B">{0}=\left[\boldsymbol{E}</em>}\right]: \boldsymbol{E<em 1="1">{t</em>}} ; \ldots ; \boldsymbol{E<em M="M">{t</em>}} ; \boldsymbol{E<em 0="0">{\mid\left[E O S\right]}\right]+\boldsymbol{S}^{\text {poss }}$, where $\boldsymbol{S}</em>} \in \mathbb{R}^{(M+2) \times d_{\text {emb }}}$. Here, $\boldsymbol{E}$ refers to the embedding matrix, $t_{i}$ denotes the $i$-th token, $M$ signifies the number of tokens in the symbolic expression, $d_{\text {emb }}$ is the embedding dimension, and $\boldsymbol{S}^{\text {poss }}$ represents the positional embedding matrix. In the symbolic encoder, we use a Transformers model with the same architecture as in Sec. 3.1. This variant of the encoder, denoted as $E n c^{S}$, processes the symbolic inputs. The $l$-th layer representation is described as $\boldsymbol{S<em l="l">{l}=E n c</em>}^{S}\left(\boldsymbol{S<em S="S">{l-1}\right)$, where $l$ varies from 1 to $L</em>$ indicates number of layers within the symbolic encoder.
Attention-based Distillation. The symbolic encoder also employs attention-based pooling, as in Sec. 3.1. This mechanism computes weighted sums to distill information from the symbolic expression into a compact representation $\boldsymbol{Z}}$, and $L_{S<em S="S">{S}=\mathcal{A}</em>} \cdot \boldsymbol{S<em S="S">{L</em>$ through softmax along the symbolic sequence.}}$, using attention weights $\mathcal{A}_{S</p>
<h3>3.3 Unified Pre-Training Objective</h3>
<p>Our work introduces a multi-modal symbolic-numeric pre-training approach, SNIP, which aims to facilitate a mutual understanding of both domains, enabling advanced cross-modal reasoning.
Training Objective. SNIP's pre-training objective is inspired by the joint training used in CLIP (Radford et al., 2021). Incorporating both a numeric and symbolic encoder, the model optimizes a symmetric cross-entropy loss over similarity scores. It employs a contrastive loss (InfoNCE (Oord et al., 2018) objective) to learn the correspondence between numeric and symbolic data pairs. Specif-</p>
<p>ically, this approach learns to align embeddings of corresponding symbolic-numeric pairs while distancing unrelated pairs. The objective function can be defined as:</p>
<p>$$
\mathcal{L}=-\sum_{(v, s) \in B}\left(\log \operatorname{NCE}\left(\boldsymbol{Z}<em V="V">{S}, \boldsymbol{Z}</em>}\right)+\log \operatorname{NCE}\left(\boldsymbol{Z<em S="S">{V}, \boldsymbol{Z}</em>\right)\right)
$$</p>
<p>where $B$ represents the batch of (symbolic, numeric) data pairs, $\operatorname{NCE}\left(\boldsymbol{Z}<em V="V">{S}, \boldsymbol{Z}</em>}\right)$ and $\operatorname{NCE}\left(\boldsymbol{Z<em S="S">{V}, \boldsymbol{Z}</em>}\right)$ denote the contrastive losses on symbolic-to-numeric and numeric-to-symbolic similarities, respectively. The symbolic-to-numeric contrastive loss, $\operatorname{NCE}\left(\boldsymbol{Z<em V="V">{S}, \boldsymbol{Z}</em>\right)$, is calculated as:</p>
<p>$$
\operatorname{NCE}\left(\boldsymbol{Z}<em V="V">{S}, \boldsymbol{Z}</em>}\right)=\frac{\exp \left(\boldsymbol{Z<em V="V">{S} \cdot \boldsymbol{Z}</em>}^{+}\right)}{\sum_{\boldsymbol{Z} \in\left{\boldsymbol{Z<em V="V">{V}^{+}, \boldsymbol{Z}</em>
$$}^{-}\right}} \exp \left(\frac{\boldsymbol{Z}_{S} \cdot \boldsymbol{Z}}{z}\right)</p>
<p>where $\tau$ is temperature, $\boldsymbol{Z}<em S="S">{V}^{+}$represents positive SNIP numeric embeddings that overlap with SNIP symbolic embedding $\boldsymbol{Z}</em>}$, and $\boldsymbol{Z<em V="V">{V}^{-}$are negative numeric embeddings implicitly formed by other numeric embeddings in the batch. A symmetric equivalent, $\operatorname{NCE}\left(\boldsymbol{Z}</em>\right)$, also defines the numeric-to-symbolic contrastive loss. More implementation details are provided in App. B.}, \boldsymbol{Z}_{S</p>
<h1>3.4 Pre-training Data</h1>
<p>In our SNIP approach, pre-training relies on a vast synthetic dataset comprising paired numeric and symbolic data. We follow the data generation mechanism in (Kamienny et al., 2022), where each example consists of $N$ data points $(x, y) \in \mathbb{R}^{D+1}$ and a corresponding mathematical function $f$, where $y=f(x)$. Data generation proceeds in several steps, ensuring diverse and informative training examples. More details about each of the following steps are provided in App. A.
Sampling of functions. We create random mathematical expressions using a process detailed in (Kamienny et al., 2022; Lample \&amp; Charton, 2020). This process involves selecting an input dimension $D$, determining the number of binary operators, constructing binary trees, assigning variables to leaf nodes, inserting unary operators, and applying random affine transformations. This method ensures a diverse set of functions for training.
Sampling of datapoints. After generating a function, we sample $N$ input points and find their corresponding target values. To maintain data quality, we follow guidelines from (Kamienny et al., 2022), discarding samples with inputs outside the function's domain or exceptionally large output values. Our approach includes drawing inputs for each expression from various distributions, enhancing training diversity. The generation process of datapoints also involves selecting cluster weights and parameters, sampling input points for each cluster, and normalization along each dimension. To emphasize on the function's numeric behavior rather than the range of values, we also normalize the target values $\boldsymbol{y}$ between $(0,1)$.</p>
<h2>4 Using SNIP for Cross-MODAL PROPERTY PREDICTION</h2>
<p>To evaluate SNIP's capability for cross-modal comprehension between symbolic and numeric domains, we conducted targeted experiments. These tests aimed to assess the model's aptitude for predicting specific numeric mathematical properties based on the symbolic inputs-a non-trivial task requiring mutual understanding of both domains. For this purpose, we identified a set of mathematical properties; details can be found in App. C. In this section, we focus on two numeric properties for one-dimensional datasets: Non-Convexity Ratio (NCR), and Function Upwardness. The $N C R$ approximates function convexity with values between $\mathrm{NCR}=0$ (fully convex) and $\mathrm{NCR}=1$ (fully concave). Upwardness quantifies the function's directionality by assessing the segments where data increases within the training domain, ranging from $\mathrm{UP}=-1$ for strictly decreasing functions to $\mathrm{UP}=1$ for increasing ones. Due to space limitations, only results for $N C R$ and Upwardness are discussed here. A complete list of properties with their detailed prediction results and corresponding chance levels, as well as their SNIP's pre-trained representations, are provided in App. C.</p>
<h3>4.1 MODELS AND TRAINING</h3>
<p>To assess property prediction on top of SNIP's embeddings, we employ a predictor head that passes these embeddings through a single-hidden-layer MLP to yield the predicted values. We adopt a Mean Squared Error (MSE) loss function for training on continuous properties. We consider three key training configurations to probe the efficacy of SNIP's learned representations:</p>
<ul>
<li>Supervised Model: Utilizes the same encoder architecture as SNIP but initializes randomly.</li>
<li>SNIP (frozen): Keeps the encoder weights fixed, training only the predictor head.</li>
<li>SNIP (finetuned): Initializes encoder from pretrained SNIP, allowing full updates during training. For a fair comparison, all model variants are trained on identical datasets comprising 10 K equations and subsequently tested on a distinct 1 K -equation evaluation dataset. These datasets are generated using the technique described in Sec. 3.4.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: 2D t-SNE representations of the encoded vectors across three model variants, colored for (top) Non-Convexity Ratio and (bottom) Function Upwardness prediction tasks.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: $R^{2}$ scores for $N C R$ property prediction task vs. the number of training samples.</p>
<h3>4.2 RESULTS</h3>
<p>Quantitative Results. Table 1 presents the Normalized Mean Squared Error (NMSE) and accuracy metric $A c c_{0.1}$ for all three models across the tasks of predicting $N C R$ and Upwardness. Here, $A c c_{0.1}$ reflects the percentage of predictions within absolute tolerance $\tau=0.1$ of the true normalized values: $A c c_{\tau}=\sum_{i=1}^{l} \sum_{i} \mathbb{1}\left{\left|\hat{p}<em i="i">{i}-p</em>$ are the true and predicted property values for the $i$-th example. Results reveal a significant gap in performance between the purely supervised model and those benefiting from SNIP's prior knowledge. This performance gap can be attributed to SNIP's pre-trained, semantically rich representations, enabling enhanced generalization to unseen functions. Additionally, fine-tuning the SNIP encoder results in marginal performance gains, indicating the model's capability to adapt to different downstream tasks.
Qualitative Findings. To delve deeper into the power of SNIP's representations, we compared its pre-finetuning and post-finetuning latent spaces against that of a supervised model lacking pretraining, using t-distributed Stochastic Neighbor Embedding (t-SNE) (van der Maaten \&amp; Hinton, 2008). The visualizations are color-coded by the corresponding properties (Fig. 2). Consistent with the quantitative outcomes, the supervised model's latent space, shown in Fig. 2(a), exhibits limited structural coherence. In contrast, SNIP's latent space in Fig. 2(b) shows pronounced clustering and distinct property trends. Notably, further fine-tuning of the encoder for these prediction tasks, depicted in Fig. 2(c), results in a more structured latent space, marked by clearer linear trends in properties. This finding underscores SNIP's quantitative advantages and its flexibility in adapting to downstream tasks.
Low Data Regime Analysis. We evaluated how training sample size influences the test $R^{2}=1-N M S E$ scores for predicting $N C R$, assessing three model variants on a fixed 1 K -sample test set (Fig. 3). In low data regime scenarios with as low as just 100 training samples, the supervised model's score fell sharply to 0.292 , while both SNIP variants maintained scores above 0.745 . Upon increasing the training sample size to 1 M , all models showed improvement; however, SNIP variants continued to lead. We observe that the supervised baseline model might approach SNIP's performance with more training data, which is reasonable, since this model is specialized only for the prediction of this property. However, SNIP's value lies in its flexibility - the pre-trained representations can be efficiently adapted to new tasks. These results emphasize SNIP's superior generalization from limited data, underscoring the SNIP's rich semantic encodings.}\right| \leq \tau\right}$ where $p_{i}$ and $\hat{p}_{i</p>
<h2>5 Using SNIP for Symbolic Regression</h2>
<p>SNIP aims to synergize symbolic and numeric reasoning through mutual learning, offering enhanced capabilities for tasks that require both numeric-symbolic understanding and generation. A paramount task in this context is Symbolic Regression (SR), which identifies interpretable symbolic equations to represent observed data. Essentially, SR transforms numeric observations into underlying mathematical expressions, thereby making it a numeric-to-symbolic generation task. The significance of SR extends to revealing functional relations between variables and offers an ideal</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Using SNIP for Symbolic Regression: (a) Training includes adding an expression generation module atop SNIP's numeric encoder; (b) Inference aims to enhance expressions by optimizing within SNIP's interpolatable latent space.
benchmark for evaluating SNIP's pre-trained numeric representations. Recent advancements in SR leverage encoder-decoder Transformer frameworks (Biggio et al., 2021; Kamienny et al., 2022). Therefore, to effectively undertake SNIP for SR, we perform the following two steps: First, training an expression generation decoder on top of the SNIP's numeric encoder for generating the symbolic functions. Second, conducting latent space optimization (LSO) within SNIP's interpolatable latent space, enriched by pre-training, to further enhance the equation generation.</p>
<h1>5.1 SR Model Architecture and Training</h1>
<p>We build the SR model upon SNIP's numeric encoder $\mathcal{E}<em _omega="\omega">{g}^{V}$ which transforms numeric data into semantically rich embeddings. On top of this encoder, we implement an expression generation module $\mathcal{G}</em>}$ that integrates an expression decoder $\mathcal{D<em _gamma="\gamma">{\phi}$ and a mapping network $g</em>}$ to generate symbolic expressions: $\mathcal{G<em _phi="\phi">{\omega}=\mathcal{D}</em>} \circ g\left(\boldsymbol{Z<em _phi="\phi">{V} ; \gamma\right)$.
Expression Decoder. To use SNIP for SR, we overlay an expression generation decoder $\mathcal{D}</em>$ and actual functions $f$.
Mapping Network. Inspired by the ClipCap approach (Mokady et al., 2021) in the field of image captioning, which integrates CLIP's pre-trained image embeddings with GPT-2 pre-trained text generation model through a learnable mapping network, we adopt a similar strategy for SR. As shown in Fig. 4(a), to facilitate integration with the E2E's (Kamienny et al., 2022) pre-trained SR decoder $\left(\mathcal{D}}$, after SNIP's numeric encoder (shown in Fig. 4(a)). This decoder, which utilizes a multi-layer Transformer (Biggio et al., 2021; Kamienny et al., 2022), is trained to map numeric encodings into symbolic expressions, aiming to minimize the divergence between the predicted $\hat{f<em _gamma="\gamma">{\phi}^{E 2 E}\right)$, we introduce a learnable Mapping Network $g</em>}$. This module translates SNIP's numeric embeddings $\boldsymbol{Z<em _phi="\phi">{V}$ into a compatible input for $\mathcal{D}</em>$ reshapes SNIP embeddings into a sequence with maximum length $M$. This approach lets us leverage the existing pre-trained SR decoder without the need for training from scratch.
Training. The training objective is to minimize the token-matching cross-entropy loss $\mathcal{L}$ between the predicted $\hat{f}$ and ground-truth $f$ symbolic expressions: $\mathcal{L}(\hat{f}, f)=$ $-\frac{1}{|f|} \sum_{j} \log P\left(\hat{t}}^{E 2 E}$. Specifically, $g: \mathbb{R}^{d_{e m b}} \rightarrow \mathbb{R}^{M \times d_{e m b}<em 1="1">{j} \mid t</em>}, \ldots, t_{j-1} ; \mathcal{G<em j="j">{\omega}\right)$, where $P\left(\hat{t}</em>$, given the preceding true tokens. Here, the decoder is initialized from pre-trained weights (Kamienny et al., 2022) and trained jointly with the mapping network to learn numeric-tosymbolic expression generation. More details on the model designand training implementation can be found in App. E.} \mid t_{1}, \ldots, t_{j-1} ; \mathcal{G}_{\omega}\right)$ is the conditional probability of the $j$-th token in $\hat{f</p>
<h3>5.2 Semantic Latent Insights for SR</h3>
<p>Traditional SR methods rely on searching within the vast equation landscape, dealing with the dual challenges of combinatorial complexity and limited prior knowledge (Burlacu et al., 2020; Schmidt \&amp; Lipson, 2009). Recent approaches incorporate deep learning to better navigate this space, integrating learned numeric-to-symbolic priors into the search process (Udrescu \&amp; Tegmark, 2020; Petersen et al., 2021; Biggio et al., 2021; Kamienny et al., 2022). Yet, these are also often constrained by their reliance on the function search techniques at the decoding stage (Mundhenk et al., 2021; Holt et al., 2023; Landajuela et al., 2022), perpetuating the limitations. For example, in the Genetic Programming (GP) function search techniques, mutation and breeding steps across 'winning' subexpressions are prone to significant deviations in a function's numeric behavior. This emphasizes the</p>
<p>necessity for a better search strategy attuned to the semantics of the function. Recently, alternative strategies, like latent space learning of symbolic functions through Variational Autoencoders (VAEs) <em>Popov et al. (2023); Mežnar et al. (2023)</em>, trained exclusively for symbolic function reconstruction, do show promise but fall short by neglecting numeric behaviors essential for SR tasks.</p>
<p>In contrast, SNIP offers a novel solution through a task-agnostic joint learning paradigm. This joint learning approach imprints the latent space with a wealth of integrated symbolic and numeric semantics that serve as a high-dimensional ‘semantic fingerprint’ for various function behaviors and their inherent similarities. Therefore, unlike the latent space in <em>Popov et al. (2023); Mežnar et al. (2023)</em>, SNIP’s task-agnostic latent space embodies a robust numeric-symbolic prior, providing an ideal landscape for SR search. By augmenting SNIP’s numeric encoder with an expression generation decoder (as shown in Fig 4), we can create a generative latent space—a crucial asset for the numeric-to-symbolic generation task of SR. Our empirical investigations on the generative latent space further enrich this narrative. The innate <em>interpolatability</em> of SNIP’s latent space, as demonstrated in Fig.5, suggests a meaningful correlation between latent space representations and their corresponding numeric behaviors. In this figure, for a source function $\boldsymbol{Z}<em V="V">{V}^{s}$ (blue curve) and a destination function $\boldsymbol{Z}</em>}^{d}$ (orange curves), we linearly interpolate within the numeric encoded vectors to obtain $\boldsymbol{Z<em w="w">{V}^{int}$. This interpolated embedding is decoded into a symbolic function $\hat{f}=\mathcal{G}</em>}(\boldsymbol{Z<em _text_emb="\text{emb">{V}^{int.})$. Upon computing $\hat{f}$ over dataset $\boldsymbol{x}$, we find that <em>the interpolated function exhibits a behavior that is semantically in between the source and destination functions.</em> This is a significant advantage for nuanced search and explorations during the symbolic discovery process. Moreover, the fixed dimension $d</em>$ of this space, which is substantially lower than the combinatorial optimization space of equations, streamlines the search process. Given these attributes, SNIP’s generative latent space stands as a compelling candidate for a more effective approach to SR.}</p>
<h3>5.3 SNIP LATENT SPACE OPTIMIZATION</h3>
<p>As shown in Fig. 5, SNIP latent space interpolation shows a meaningful correlation with the functions’ numeric pattern. This observation compels us to undertake a more comprehensive exploration of the latent space. Specifically, to fully harness the expressive capabilities of pre-trained SNIP embeddings in the context of SR, we employ Latent Space Optimization (LSO) as outlined in Fig. 4(b). This optimization process involves a stochastic search over latent space $\boldsymbol{Z}<em 1="1">{V}$, with the objective of maximizing numerical fitness accuracy. To benefit from both prior knowledge of pre-trained model and capabilities of search method, we initialize the search population by augmenting the given dataset into a partitioned population $\mathcal{P}={\mathcal{P}</em>},\mathcal{P<em 3="3">{2},\mathcal{P}</em>}}$. Specifically, $\mathcal{P<em 2="2">{1}$ contains encodings from random sub-samples, with size $n&lt;N$ of the original data; $\mathcal{P}</em>}$ includes encodings from sampled inputs with their target values $\boldsymbol{y}$ perturbed by random Gaussian noise (perturb and then encode); and $\mathcal{P<em V="V">{3}$ includes perturbed encodings from a fixed sampled data (encode and then perturb). Each agent $p$ with representation $\boldsymbol{Z}</em>}^{p}$ is evaluated using a fitness function based on the $R^{2}$ fitting metric. Candidate symbolic functions are generated for each agent by feeding encodings to the expression generation module $\hat{f<em w="w">{p}=\mathcal{G}</em>}(\boldsymbol{Z<em _text_emp="\text{emp">{V}^{p})$. The functions’ constants are then refined using BFGS <em>Fletcher (1987)</em>, with a goal of optimizing the $R^{2}$ score against training data <em>Kamienny et al. (2022)</em>. Then, updates to the latent population are carried out using a <em>gradient-free optimizer</em>, which accommodates the non-differentiable characteristics of the function generation evaluation metrics. This latent optimization process runs for $T$ iterations or until achieving a predefined $R</em>$ is then evaluated on a holdout test set. Overall, LSO leverages SNIP’s rich latent space to efficiently transform symbolic regression’s combinatorial search into continuous optimization of fitting performance. Details on the LSO algorithm and implementation are in App. E. An ablation study analyzing the impact of LSO and choice of optimization algorithm is also provided in App. E.5.}}^{2}$ criterion. The optimal symbolic function $\hat{f}^{*</p>
<h3>5.4 EVALUATION ON SRBENCH</h3>
<p>Datasets. SNIP was assessed on PMLB datasets <em>Olson et al. (2017)</em> outlined in SRBench <em>La Cava et al. (2021)</em>, including: 119 <em>Feynman</em> equations <em>Udrescu &amp; Tegmark (2020)</em>, 14 <em>ODE-Strogatz</em> challenges <em>La Cava et al. (2016)</em>, and 57 <em>Black-box</em> regression tasks without known underlying func</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Pareto plots comparing $R^{2}$ and equation complexity of all methods across SRBench datasets: (a) Strogatz, (b) Black-box, and (c) Feynman. Using SNIP for SR yields strong fitting-complexity tradeoff, evidenced by its first Pareto-front locating in all datasets. Here, each point depicts a method's median ranking within the data group, with lines/colors signifying Pareto dominance. The "*" marks SR methods in the Black-box datasets.
tions. For specifics on each dataset, refer to App. E. Leveraging the E2E's SR decoder (Kamienny et al., 2022) for our decoder initialization, which is trained for $D \leq 10$, we similarly constrained SNIP's pre-training and evaluation to datasets with continuous features and dimensionality $D \leq 10$. Also, since the range of target values $\boldsymbol{y}$ is important, especially for predicting the constants, we do not normalize $\boldsymbol{y}$ for this task. More details on the experiment settings are provided in App. E.
Results. Fig. 6 illustrates SNIP's performance against the recent end-to-end (E2E) transformer SR model (Kamienny et al., 2022) and all the SRBench baselines. The Pareto plots exhibit rankings for Fitting Accuracy against Model Complexity. The model's accuracy is evaluated using $R^{2}$ and its complexity is evaluated as the number of nodes in the expression tree of the generated equation (La Cava et al., 2021). Here, SNIP shows a strong accuracy-complexity balance, placing on the first Pareto-front across all datasets. On Strogatz datasets, SNIP demonstrates top-tier accuracy of 0.928 , outperforming all the leading baselines. For Black-box datasets, SNIP again shows competitive accuracy while achieving lower complexity (47.52) than the competitive Operon baseline (64.95). On Feynman datasets, SNIP locates the Pareto frontier, offering better complexity than Operon ( 31.63 vs. 69.87 ) and better accuracy than AIFeynman ( 0.882 vs. 0.798 ) baselines. More detailed results on the SRBench datasets can be found in App. E.</p>
<h1>6 DISCUSSION AND CONCLUSION</h1>
<p>We introduced SNIP, a multi-modal symbolic-numeric pre-training model that learns how to associate the symbolic and numeric aspects of mathematical functions. We showed that SNIP exhibits remarkable capabilities in estimating cross-modal mathematical properties, particularly in low data regime scenarios, outperforming fully-supervised models. Also, by leveraging the latent space that SNIP constructs-capturing both functional behaviors and symbolic forms-the model demonstrates competitive performance in symbolic regression, even when compared to leading GP baselines. While SNIP showcases robustness and versatility in integrating symbolic and numeric learning, it has notable limitations. It struggles with data patterns that cannot be clearly expressed as closed-form mathematical functions. Also, its performance is tied to the pre-defined data generation protocol, adopted from (Lample \&amp; Charton, 2020; Kamienny et al., 2022), which sets constraints on factors such as input dimensionality, and the vocabulary of mathematical operators. For example, the current protocol limits input dimensions to $D \leq 10$ due to the exponential increase in expression complexity at higher dimensions. Exploring higher-dimensional settings is an interesting avenue for future research that would likely require significant updates to the data generation protocol. Despite these limitations, SNIP has a wide range of capabilities, presenting a powerful tool in the intersection of symbolic and numeric mathematics. Future research can focus on potential applications of SNIP, from using numeric guidance in symbolic-to-symbolic tasks such as function integration to using symbolic guidance for numeric-to-numeric tasks such as zero-shot extrapolation and superresolution. Also, the SNIP's learned representations could serve as a foundation for innovative evaluation metrics of symbolic-numeric proximity, as well as efficient data and feature valuation.</p>
<h1>REFERENCES</h1>
<p>Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, et al. A cookbook of selfsupervised learning. arXiv preprint arXiv:2304.12210, 2023.</p>
<p>Sören Becker, Michal Klein, Alexander Neitz, Giambattista Parascandolo, and Niki Kilbertus. Predicting ordinary differential equations with transformers. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1978-2002. PMLR, 23-29 Jul 2023. URL https: //proceedings.mlr.press/v202/becker23a.html.</p>
<p>Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascandolo. Neural symbolic regression that scales. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 936-945. PMLR, 18-24 Jul 2021. URL https: //proceedings.mlr.press/v139/biggio21a.html.</p>
<p>Bogdan Burlacu, Gabriel Kronberger, and Michael Kommenda. Operon c++: An efficient genetic programming framework for symbolic regression. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion, GECCO '20, pp. 1562-1570, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450371278. doi: 10.1145/3377929.3398099. URL https://doi.org/10.1145/3377929.3398099.</p>
<p>Zhonglin Cao, Rishikesh Magar, Yuyang Wang, and Amir Barati Farimani. Moformer: Selfsupervised transformer model for metal-organic framework property prediction. Journal of the American Chemical Society, 145(5):2958-2967, 2023. doi: 10.1021/jacs.2c11420. URL https://doi.org/10.1021/jacs.2c11420. PMID: 36706365.</p>
<p>Francois Charton. Linear algebra with transformers. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=Hp4g7FAXXG.</p>
<p>Miles Cranmer. Interpretable machine learning for science with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582, 2023.</p>
<p>Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and Shirley Ho. Discovering symbolic models from deep learning with inductive biases. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. ISBN 9781713829546.</p>
<p>Stéphane D'Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, and Francois Charton. Deep symbolic regression for recurrence prediction. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 4520-4536. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/ d-ascoli22a.html.</p>
<p>Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei, Michael C Kampffmeyer, Xiaoyong Wei, Minlong Lu, Yaowei Wang, and Xiaodan Liang. M5product: Self-harmonized contrastive learning for e-commercial multi-modal pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21252-21262, 2022.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 889-898, Melbourne, Australia, jul 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://aclanthology.org/P18-1082.</p>
<p>Roger Fletcher. Practical Methods of Optimization. John Wiley \&amp; Sons, New York, NY, USA, second edition, 1987.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 946-958, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/ v1/2020.acl-main.89. URL https://aclanthology.org/2020.acl-main.89.</p>
<p>Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et al. xval: A continuous number encoding for large language models. arXiv preprint arXiv:2310.02989, 2023.</p>
<p>Paul Hager, Martin J Menten, and Daniel Rueckert. Best of both worlds: Multimodal contrastive learning with tabular and imaging data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 23924-23935, 2023.</p>
<p>Samuel Holt, Zhaozhi Qian, and Mihaela van der Schaar. Deep generative symbolic regression. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=o7koEEMA1bR.</p>
<p>Samy Jelassi, Stéphane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and François Charton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400, 2023.</p>
<p>Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4904-4916. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr. press/v139/jia21b.html.</p>
<p>Pierre-Alexandre Kamienny, Stéphane d'Ascoli, Guillaume Lample, and Francois Charton. Endto-end symbolic regression with transformers. In Advances in Neural Information Processing Systems, 2022.</p>
<p>William La Cava, Kourosh Danai, and Lee Spector. Inference of compact nonlinear dynamic models by epigenetic local search. Engineering Applications of Artificial Intelligence, 55:292-306, 2016. ISSN 0952-1976. doi: https://doi.org/10.1016/j.engappai.2016.07.004. URL https://www. sciencedirect.com/science/article/pii/S0952197616301294.</p>
<p>William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabricio de Franca, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason Moore. Contemporary symbolic regression methods and their relative performance. In J. Vanschoren and S. Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/ file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper-round1.pdf.</p>
<p>Guillaume Lample and François Charton. Deep learning for symbolic mathematics. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=SleZYeHFDS.</p>
<p>Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for neural theorem proving. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 26337-26349. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/a8901c5e85fb8e1823bbf0f755053672-Paper-Conference.pdf.</p>
<p>Mikel Landajuela, Chak Lee, Jiachen Yang, Ruben Glatt, Claudio P. Santiago, Ignacio Aravena, Terrell N. Mundhenk, Garrett Mulcahy, and Brenden K. Petersen. A unified framework for deep symbolic regression. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id=2FNnBhwJsHK.</p>
<p>Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, and Xiangliang Zhang. Mwp-bert: Numeracy-augmented pre-training for math word problem solving. In Findings of NAACL 2022, pp. 997-1009, 2022.</p>
<p>Jing Liu, Xinxin Zhu, Fei Liu, Longteng Guo, Zijia Zhao, Mingzhen Sun, Weining Wang, Hanqing Lu, Shiyu Zhou, Jiajun Zhang, et al. Opt: Omni-perception pre-trainer for cross-modal understanding and generation. arXiv preprint arXiv:2107.00249, 2021.</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14605-14631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.817. URL https://aclanthology.org/2023.acl-long. 817.</p>
<p>Kazem Meidani and Amir Barati Farimani. Identification of parametric dynamical systems using integer programming. Expert Systems with Applications, 219:119622, 2023. ISSN 0957-4174. doi: https://doi.org/10.1016/j.eswa.2023.119622.</p>
<p>Sebastian Mežnar, Sašo Džeroski, and Ljupčo Todorovski. Efficient generator of mathematical expressions for symbolic regression. Machine Learning, Sep 2023. ISSN 1573-0565. doi: 10.1007/ s10994-023-06400-2. URL https://doi.org/10.1007/s10994-023-06400-2.</p>
<p>Seyedali Mirjalili, Seyed Mohammad Mirjalili, and Andrew Lewis. Grey wolf optimizer. Advances in Engineering Software, 69:46-61, 2014. ISSN 0965-9978. doi: https://doi.org/10. 1016/j.advengsoft.2013.12.007. URL https://www.sciencedirect.com/science/ article/pii/S0965997813001853.</p>
<p>Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021.</p>
<p>Terrell N. Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P. Santiago, Daniel faissol, and Brenden K. Petersen. Symbolic regression via deep reinforcement learning enhanced genetic programming seeding. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview. net/forum?id=tjwQaOI9tdy.</p>
<p>Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore. Pmlb: a large benchmark suite for machine learning evaluation and comparison. BioData Mining, 10(1):36, Dec 2017. ISSN 1756-0381. doi: 10.1186/s13040-017-0154-4. URL https://doi. org/10.1186/s13040-017-0154-4.</p>
<p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</p>
<p>Shuai Peng, Ke Yuan, Liangcai Gao, and Zhi Tang. Mathbert: A pre-trained model for mathematical formula understanding. arXiv preprint arXiv:2105.00377, 2021.</p>
<p>Brenden K Petersen, Mikel Landajuela Larma, Terrell N. Mundhenk, Claudio Prata Santiago, Soo Kyung Kim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=m5Qsh0kBQG.</p>
<p>Sergei Popov, Mikhail Lazarev, Vladislav Belavin, Denis Derkach, and Andrey Ustyuzhanin. Symbolic expression generation via variational auto-encoder. PeerJ Computer Science, 9:e1241, 2023.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8748-8763. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/radford21a. html.</p>
<p>J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https://GitHub. com/FacebookResearch/Nevergrad, 2018.</p>
<p>Samuel H. Rudy, Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Data-driven discovery of partial differential equations. Science Advances, 3(4):e1602614, 2017. doi: 10.1126/ sciadv. 1602614 . URL https://www.science.org/doi/abs/10.1126/sciadv. 1602614 .</p>
<p>Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling networks. arXiv preprint arXiv:1602.03609, 2016.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1gR5iR5FX.</p>
<p>Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science Advance, 324(5923):81-85, 2009. ISSN 0036-8075. doi: 10.1126/science. 1165893.</p>
<p>Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, and Chandan K. Reddy. Transformerbased planning for symbolic regression. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=0rVXQEeFEL.</p>
<p>Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. $15638-15650,2022$.</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and JiRong Wen. A molecular multimodal foundation model associating molecule graphs with natural language. arXiv preprint arXiv:2209.05481, 2022.</p>
<p>Kenichi Tamura and Marcus Gallagher. Quantitative measure of nonconvexity for black-box continuous functions. Information Sciences, 476:64-82, 2019. ISSN 0020-0255. doi: https://doi. org/10.1016/j.ins.2018.10.009. URL https://www.sciencedirect.com/science/ article/pii/S0020025518308053.</p>
<p>Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Szekely. Representing numbers in NLP: a survey and a vision. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 644-656, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.53. URL https://aclanthology.org/2021.naacl-main.53.</p>
<p>Silviu-Marian Udrescu and Max Tegmark. Ai feynman: A physics-inspired method for symbolic regression. Science Advances, 6(16):eaay2631, 2020. doi: 10.1126/sciadv.aay2631. URL https://www.science.org/doi/abs/10.1126/sciadv.aay2631.</p>
<p>Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):2579-2605, 2008. URL http://jmlr.org/papers/v9/ vandermaaten08a.html.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.</p>
<p>Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: A comprehensive survey. Machine Intelligence Research, pp. 1-36, 2023.</p>
<p>Sean Welleck, Peter West, Jize Cao, and Yejin Choi. Symbolic brittleness in sequence models: On systematic generalization in symbolic mathematics. Proceedings of the AAAI Conference on Artificial Intelligence, 36(8):8629-8637, Jun. 2022. doi: 10.1609/aaai.v36i8.20841. URL https://ojs.aaai.org/index.php/AAAI/article/view/20841.</p>
<p>Eugene P. Wigner. The unreasonable effectiveness of mathematics in the natural sciences. richard courant lecture in mathematical sciences delivered at new york university, may 11, 1959. Communications on Pure and Applied Mathematics, 13(1):1-14, 1960. doi: https://doi.org/10.1002/ cpa. 3160130102 . URL https://onlinelibrary.wiley.com/doi/abs/10.1002/ cpa. 3160130102.</p>
<p>Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. Meta-transformer: A unified framework for multimodal learning. arXiv preprint arXiv:2307.10802, 2023.</p>
<p>Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419, 2023.</p>
<p>Yongshuo Zong, Oisin Mac Aodha, and Timothy Hospedales. Self-supervised multimodal learning: A survey. arXiv preprint arXiv:2304.01008, 2023.</p>
<h1>APPENDIX</h1>
<h2>A Pre-Training Data Details</h2>
<p>We provide additional details regarding the pre-training data employed for pre-training SNIP. In our approach, SNIP is pre-trained on a large synthetic dataset of paired numeric and symbolic data, utilizing the data generation technique from (Kamienny et al., 2022). Each example consists of a set of $N$ points $(\boldsymbol{x}, y) \in \mathbb{R}^{D+1}$ and an associated mathematical function $f(\cdot)$, such that $y=f(\boldsymbol{x})$. These examples are generated by first sampling a function $f$, followed by sampling $N$ numeric input points $\boldsymbol{x}<em i="i">{i} ; i=1, \ldots, N \in \mathbb{R}^{D}$ from $f$, and then calculating the target value $y</em>\right)$.}=f\left(\boldsymbol{x}_{i</p>
<h2>A. 1 SAMPLING OF FUNCTIONS</h2>
<p>To generate random functions $f$, we employ the strategy outlined in (Kamienny et al., 2022; Lample \&amp; Charton, 2020), building random trees with mathematical operators as nodes and variables/constants as leaves. This process includes:
Input Dimension Selection. We begin by selecting the input dimension $D$ for the functions from a uniform distribution $\mathcal{U}\left(1, D_{\max }\right)$. This step ensures variability in the number of input variables.
Binary Operator Quantity Selection. Next, we determine the quantity of binary operators $b$ by sampling from $\mathcal{U}\left(D-1, D+b_{\max }\right)$ and selecting $b$ operators randomly from the set $\mathcal{U}(+,-, \times)$. This step introduces variability in the complexity of the generated functions.
Tree Construction. Using the chosen operators and input variables, we construct binary trees, simulating the mathematical function's structure. The construction process is performed following the method proposed in (Kamienny et al., 2022; Lample \&amp; Charton, 2020).
Variable Assignment to Leaf Nodes. Each leaf node in the binary tree corresponds to a variable, which is sampled from the set of available input variables ( $x_{d}$ for $d=1, \ldots, D$ ).
Unary Operator Insertion. Additionally, we introduce unary operators by selecting their quantity $u$ from $\mathcal{U}\left(0, u_{\max }\right)$ and randomly inserting them from a predefined set $\left(\mathcal{O}<em u="u">{u}\right)$ of unary operators where $\mathcal{O}</em>, \sin , \cos , \tan , \arctan , \log , \exp ]$.}=[\mathrm{inv}, \mathrm{abs}, \mathrm{pow} 2, \mathrm{pow} 3, \mathrm{sqrt</p>
<p>Affine Transformation. To further diversify the functions, we apply random affine transformations to each variable $\left(x_{d}\right)$ and unary operator $(u)$. These transformations involve scaling (a) and shifting (b) by sampling values from $D_{\text {aff }}$. In other words, we replace $x_{d}$ with $a x_{d}+b$ and $u$ with $a u+b$, where $(a, b)$ are samples from $D_{\text {aff }}$. This step enhances the variety of functions encountered during pre-training and ensures the model encounters a unique function each time, aiding in mitigating the risk of overfitting as well as memorization.</p>
<h2>A. 2 SAMPLING OF DATAPOINTS</h2>
<p>Once have generated a sample function $f$, we proceed to generate $N$ input points $x_{i} \in \mathbb{R}^{D}$ and calculate their corresponding target value $y_{i}=f\left(x_{i}\right)$. To maintain data quality and relevance, we follow the guidelines from (Kamienny et al., 2022), which include: Discarding and Restarting: If any input point $x_{i}$ falls outside the function's defined domain or if the target value $y_{i}$ exceeds $10^{100}$, we discard the sample function and restart the generation process. This ensures that the model learns meaningful and well-behaved functions. Avoidance and Resampling: Avoidance and resampling of out-of-distribution $x_{i}$ values provide additional insights into $f$ as it allows the model to learn its domain. This practice aids the model in handling input variations. Diverse Input Distributions: To expose the model to a broad spectrum of input data distributions, we draw input points from a mixture of distributions, such as uniform or Gaussian. These distributions are centered around $k$ randomly chosen centroids, introducing diversity and challenging the model's adaptability.
The generation of input points involves the following steps:
Cluster and Weight Selection. We start by sampling the number of clusters $k$ from a uniform distribution $\mathcal{U}\left(1, k_{\max }\right)$. Additionally, we sample $k$ weights $\left{w_{j} \sim \mathcal{U}(0,1)\right}<em j="j">{j=1}^{k}$, which are normalized to $\sum</em>=1$.} w_{j</p>
<p>Cluster Parameters. For each cluster, we sample a centroid $\mu_{j} \sim \mathcal{N}(0,1)^{D}$, a vector of variances $\sigma_{j} \sim \mathcal{U}(0,1)^{D}$, and a distribution shape $D_{j}$ from ${\mathcal{N}, \mathcal{U}}$ (Gaussian or uniform). These parameters define the characteristics of each cluster.</p>
<p>Input Point Generation. We sample $\left[w_{j} N\right]$ input points from the distribution $D_{j}\left(\mu_{j}, \sigma_{j}\right)$ for each cluster $j$. This sampling with different weights from different distributions ensures the sampling of a diverse set of input points with varying characteristics.</p>
<p>Normalization. Finally, all generated input points are concatenated and normalized by subtracting the mean and dividing by the standard deviation along each dimension.</p>
<h1>B Pre-Training Implementation Details</h1>
<h2>B. 1 Model Design Details</h2>
<p>Numeric Encoder. The numeric encoding mechanism of our SNIP closely follows the design presented by (Kamienny et al., 2022), as highlighted in Sec. 3. Firstly, for each instance in a given batch, the encoder receives $N=200$ numeric input points, $(\boldsymbol{x}, \boldsymbol{y})$, from a space $\mathbb{R}^{D+1}$. Each of these points is tokenized into a sequence of length $3(D+1)$. An embedding module maps these tokens into a dense representation with an embedding size of $d_{\text {emb }}=512$. The sequences are then processed in the embedder module by a 2-layer feedforward neural network. This network projects input points to the desired dimension, $d_{\text {emb }}$. The output from the embedder is passed to a Transformer encoder, a multi-layer architecture inspired by (Vaswani et al., 2017). Our specific implementation has 8 layers, utilizes 16 attention heads, and retains an embedding dimension of 512. A defining characteristic of our task is the permutation invariance across the $N$ input points. To accommodate this, we've adopted the technique from (Kamienny et al., 2022), omitting positional embeddings within the numeric Transformer encoder. In our design, this specialized encoder variant is termed $E n c^{V}$. The representation generated at the $l$-th layer of the encoder is represented as $\boldsymbol{V}<em l="l">{l}$. The process can be summarized as $\boldsymbol{V}</em>}=E n c_{l}^{V}\left(\boldsymbol{V<em V="V">{l-1}\right)$. Here, the index $l$ spans from 1 to $L</em>}$, where $L_{V}=8$ denotes our encoder's total layers. Post encoding, for each instance in the batch, the numeric encoder's sequence outputs, $\boldsymbol{V<em V="V">{L</em>}} \in \mathbb{R}^{N \times d_{\text {emb }}}$, are compressed into a representation for the whole sequence, $\boldsymbol{Z<em _emb="{emb" _text="\text">{V} \in \mathbb{R}^{d</em>$. This representation captures the essence of the entire numeric sequence and is achieved through an attention-pooling mechanism, detailed in Sec. 3.1.}}</p>
<p>Symbolic Encoder. Our SNIP's symbolic encoding component draws inspiration from the model used in (Lample \&amp; Charton, 2020), as highlighted in Sec. 3. This encoder is designed to process mathematical symbolic expressions with a maximum length of 200 . These expressions encapsulate the true functional relationships underlying the numeric data fed to the numeric encoder. The expressions are tokenized using a prefix order tree traversal. We employ the vocabulary defined by (Kamienny et al., 2022), crafted to comprehensively represent mathematical equations. It includes symbolic entities like variables and operators, along with numeric constants. Constants are tokenized into three parts, consistent with the tokenization method outlined in Sec. 3.1. Sequence boundaries are indicated with special tokens $[\langle B O S\rangle]$ and $[\langle E O S\rangle]$. Tokens are transformed into dense vectors of dimension $d_{\text {emb }}=512$ using an embedder module. This module essentially functions as an embedding matrix for the employed vocabulary. To maintain uniform input lengths, sequences are padded to a maximum length of $M=200$ and then projected to the desired embedding dimension. This dimensionality is aligned with the numeric encoder's. The embedded sequences are processed through a Transformer encoder, characterized by its multi-layer architecture as described by (Vaswani et al., 2017). Similarly, our specific configuration for this encoder consists of 8 layers, utilizes 16 attention heads, and retains an embedding dimension of 512 . Contrary to the numeric encoder, the sequence order in symbolic expressions holds significance. Consequently, we are including positional embeddings into this Transformer encoder variant. We denote this encoder as $E n c^{S}$, and its layer-wise representations are articulated as $\boldsymbol{S}<em l="l">{l}=E n c</em>}^{S}\left(\boldsymbol{S<em S="S">{l-1}\right)$, iterating from layer 1 to the maximum layer $L</em>}=8$. Similar to the numeric encoder's approach, the symbolic encoder condenses its Transformer outputs $\boldsymbol{S<em S="S">{L</em>}} \in \mathbb{R}^{M \times d_{\text {emb }}}$ for each expression into a compact representation, $\boldsymbol{Z<em _emb="{emb" _text="\text">{S} \in \mathbb{R}^{d</em>$. This aggregation leverages the attention-pooling technique detailed in Sec. 3.2.}}</p>
<h2>B. 2 Training Details</h2>
<p>Following the extraction of coarse representations from both symbolic and numeric encoders, our focus shifts to harmonizing the embeddings from these encoders. The aim is to closely align em-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Properties are qualitatively illustrated using five sample functions. Within each row, the plots are arranged according to their respective property values. Colors represent distinct function phases corresponding to the property (e.g., convexity vs. nonconvexity in the first row, upward vs. downward in the second row). Additionally, in the third row, red points highlight instances of change in the y-coordinate.
beddings representing corresponding symbolic-numeric pairs, while ensuring a discernible distance between unrelated pairs. As discussed in Sec. 3.3, this alignment process leverages a symmetric cross-entropy loss calculated over similarity scores, with the specific approach being informed by a contrastive loss mechanism. This ensures effective learning of the correspondence between numeric and symbolic data pairs. Our optimization process is facilitated by the Adam optimizer, operating on a batch size of $B=256$ (symbolic, numeric) data pairs. The learning rate initiation is set at a low $10^{-7}$, which is then gradually warmed up to $4 \times 10^{-5}$ over an initial span of 100 K steps. Subsequently, in line with the recommendations of (Vaswani et al., 2017), we apply an inverse square root decay based on the step count to adjust the learning rate. Our model undergoes training for a total of $\approx 220$ epochs, with each epoch comprising 1,000 steps. This translates to the processing of $256 \times 1 \mathrm{~K}=256 \mathrm{~K}$ (symbolic, numeric) pair samples for each epoch. Given the on-the-fly data generation mechanism, as highlighted in Sec. A, the cumulative volume of data encountered during pre-training approximates a substantial 60 M (symbolic, numeric) pair samples. For training, we utilize 4 GPUs, each equipped with 48 GB of memory. Given this configuration, the processing time for a single epoch is approximately two hours.</p>
<h1>C Details of Using SNIP for Cross-Modal Property Prediction</h1>
<h2>C. 1 Properties Definition</h2>
<p>In this section, we define the numeric mathematical properties that we use to evaluate the pretrained SNIP model. The experiments include understanding and predicting numeric properties, i.e., properties that describe the behavior of numeric dataset, from symbolic forms of functions. The formal definitions of these properties are described in the following paragraphs and Fig. 7 qualitatively illustrates what each of the numeric properties represent.</p>
<p>Non-Convexity Ratio: Non-Convexity Ratio (NCR) is defined to quantify the relative convexity (or non-convexity) of the functions as one of the properties depending on the numeric behavior of the functions. Hence, directly predicting this property from the symbolic form of the function is a complex task. To quantify the non-convexity ratio, we employ Jensen's inequality as a fundamental measure (Tamura \&amp; Gallagher, 2019). In our approach, we focus on the one-dimensional equations with numeric dataset ${\boldsymbol{x}, \boldsymbol{y}}$. Considering a function $f: \mathcal{D} \rightarrow \mathbb{R}$ where $\mathcal{D}$ is a convex subset of $\mathcal{R}$, $f$ is a convex function if $\forall x_{1}, x_{2} \in \mathcal{D}$ and $\forall \lambda \in[0,1]$ :</p>
<p>$$
f\left(\lambda x_{1}+(1-\lambda) x_{2}\right) \leq \lambda f\left(x_{1}\right)+(1-\lambda) f\left(x_{2}\right)
$$</p>
<p>We rely on the training datasets with non-regularly sampled points to calculate the approximate NCR. To this end, we perform multiple trials to examine Jensen's inequality criterion. For each trial, we randomly select three data points $\left{\left(x_{i}, f\left(x_{i}\right)\right),\left(x_{j}, f\left(x_{j}\right)\right),\left(x_{k}, f\left(x_{k}\right)\right)\right}$ which are sorted based on $x$ in ascending order. The convexity criterion holds on these points if</p>
<p>$$
f\left(x_{j}\right) \leq \frac{\left(x_{k}-x_{j}\right) \cdot f\left(x_{i}\right)+\left(x_{j}-x_{i}\right) \cdot f\left(x_{k}\right)}{x_{k}-x_{i}}+\epsilon
$$</p>
<p>where $\epsilon$ is a very small number $\left(\epsilon=10^{-9}\right)$ to avoid numerical precision errors. Therefore, for trial $t$, we define the success as</p>
<p>$$
\xi_{t}= \begin{cases}1 &amp; \text { if (3) holds } \ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>Finally, the non-convexity ratio (NCR) is computed over the total number of trials $T$ as</p>
<p>$$
N C R=1-\frac{1}{T} \sum_{t=1}^{T} \xi_{t}
$$</p>
<p>Therefore, if a function is always convex over the range of training data points, $\mathrm{NCR}=0$, and if it is always non-convex, it would have NCR=1. Functions that have both convex and non-convex sections in the range of $x$ will have $\operatorname{NCR} \in(0,1)$.</p>
<p>Upwardness: The 'Upward/Downwardness' of a one-dimensional numeric dataset is defined to gauge the proportion of points within the training range where the function exhibits increasing or decreasing behavior. To compute this metric on the sorted dataset $\left{\boldsymbol{x}<em _boldsymbol_s="\boldsymbol{s">{\boldsymbol{s}}, \boldsymbol{f}\left(\boldsymbol{x}</em>$ as follows:}}\right)\right}$, we examine every consecutive pair of points $\left{x_{i}, x_{i+1}\right}$ to determine if they demonstrate an upward or downward trend. We then define $u_{i</p>
<p>$$
u_{i}= \begin{cases}1 &amp; \text { if } f\left(x_{i+1}\right)&gt;f\left(x_{i}\right)+\epsilon \ -1 &amp; \text { if } f\left(x_{i+1}\right)&lt;f\left(x_{i}\right)-\epsilon \ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>Finally, the upwardness metric UP is computed as the average upwardness UP $=\sum_{i=1}^{N-1} u_{i}$, where $N$ is the number of points in the dataset. Therefore, if a function is monotonically increasing the range of $x$ in training points, the upwardness measure is 1 , and if it is monotonically decreasing, the metric will be -1 . Functions that have both sections in the range of $x$ will have UP $\in(-1,1)$.</p>
<p>Oscillation For this metric, we aim to quantify the degree of oscillatory behavior exhibited by the numeric data. This is approximated by counting the instances where the direction of $y$ changes. Determining the direction of data points follows a similar process to that of the upwardness metric for each consecutive pair. Thus, we tally the occurrences of direction changes while traversing the sorted dataset. Due to the potential variation in the number of changes, we opt for a logarithmic scale to color the plots.</p>
<p>Average of Normalized $y$ The overall behavior of the numeric data points ${\boldsymbol{x}, \boldsymbol{y}}$ are better represented when the values of $y$ are scaled to a fixed range (here $(0,1)$ ), giving ${\boldsymbol{x}, \boldsymbol{Y}}$. The average of the normalized values, $Y$ can be a measure to distinguish different numeric behaviors, and it can roughly approximate the numerical integral of the normalized function in the defined range of training $\boldsymbol{x}$.</p>
<h1>C. 2 Additional Quantitative Results of Cross-Modal Property Prediction</h1>
<p>Evaluation Metrics Overview. We continue to use NMSE as our primary regression metric, providing a standard comparison across different model variants for each cross-modal property prediction task. We also report results for the Accuracy within Tolerance ( $A c c_{\tau}$ ) evaluation metric, reflecting how closely the predicted values align with the true values, within a specified tolerance level. To this end, we first normalize the true and predicted values for each property in the range of $(0,1)$ based on the range of true values. Subsequently, we calculate the accuracy over $N_{\text {test }}=1000$ test examples as $A c c_{\tau}=\frac{1}{N_{\text {test }}} \sum_{i=1}^{N_{\text {test }}} \mathbb{1}\left{\left|\hat{p}<em i="i">{i}-p</em>$ are the normalized true and predicted values of the property for the $i$-th example, respectively. Here, we consider an absolute tolerance $\tau=0.1$.}\right| \leq \tau\right}$, where $p_{i}$ and $\hat{p}_{i</p>
<p>Table 2: Full results of cross-modal property prediction on four properties showcase SNIP's superiority over the supervised baseline.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Non-Convexity Ratio</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Upwardness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Normalized Average $y$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Log Oscillations</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\downarrow$ NMSE</td>
<td style="text-align: center;">$\uparrow A c c_{0.1}$</td>
<td style="text-align: center;">$\downarrow$ NMSE</td>
<td style="text-align: center;">$\uparrow A c c_{0.1}$</td>
<td style="text-align: center;">$\downarrow$ NMSE</td>
<td style="text-align: center;">$\uparrow A c c_{0.1}$</td>
<td style="text-align: center;">$\downarrow$ NMSE</td>
<td style="text-align: center;">$\uparrow A c c_{0.1}$</td>
</tr>
<tr>
<td style="text-align: left;">Base</td>
<td style="text-align: center;">1.0000</td>
<td style="text-align: center;">$2.4 \%$</td>
<td style="text-align: center;">1.0000</td>
<td style="text-align: center;">$22.4 \%$</td>
<td style="text-align: center;">1.0000</td>
<td style="text-align: center;">$52.4 \%$</td>
<td style="text-align: center;">1.0000</td>
<td style="text-align: center;">$23.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Supervised</td>
<td style="text-align: center;">0.5299</td>
<td style="text-align: center;">$56.5 \%$</td>
<td style="text-align: center;">0.5356</td>
<td style="text-align: center;">$56.3 \%$</td>
<td style="text-align: center;">1.0406</td>
<td style="text-align: center;">$49.3 \%$</td>
<td style="text-align: center;">0.3079</td>
<td style="text-align: center;">$75.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SNIP (frozen)</td>
<td style="text-align: center;">0.0731</td>
<td style="text-align: center;">$86.1 \%$</td>
<td style="text-align: center;">0.0540</td>
<td style="text-align: center;">$84.7 \%$</td>
<td style="text-align: center;">0.4532</td>
<td style="text-align: center;">$64.5 \%$</td>
<td style="text-align: center;">0.0683</td>
<td style="text-align: center;">$92.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SNIP (finetuned)</td>
<td style="text-align: center;">$\mathbf{0 . 0 6 8 3}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 1 \%}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 4 0 0}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 1 \%}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 0 7 4}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 7 \%}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 5 8 1}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 6 \%}$</td>
</tr>
</tbody>
</table>
<p>Chance Level Baselines. For each metric, we establish a baseline or chance level to set a comparative standard to better show task difficulty. For NMSE, the chance level is at $N M S E=1$, representing a prediction that averages the property values without considering the input. The chance level baseline for $A c c_{0.1}$ is calculated based on the assumption that all predictions are equal to the mean (average) property value, $\hat{p}_{i}=\bar{p}$.</p>
<p>Detailed Results. To offer a more detailed perspective on the performance of SNIP for crossmodal property prediction, we delve into its performance across four fundamental properties: NonConvexity Ratio (NCR), Upwardness, Average of $y$, and Oscillations. Table 2 showcases a thorough comparison of the results from different model variants on these specified mathematical properties. A key aspect of these experiments is the exclusive use of symbolic equations as input for all models, aligning with the cross-modal essence of the tasks. The numeric properties are then predicted for these symbolic inputs, demonstrating the models' ability to bridge symbolic and numeric domains. For consistency in our evaluations, all models were trained on uniform datasets, each consisting of 10 K equations, and then assessed using a separate set of 1 K equations for evaluation. These datasets were constructed following the methodology outlined in Sec. A. It's imperative to highlight that, in the context of cross-modal property prediction, SNIP operates with the same quantity of labeled examples as the supervised baselines. However, a critical distinction lies in SNIP's pre-training phase, where it was not exposed to any labeled data. Instead, it engaged in a multi-modal unsupervised learning process, focusing on capturing mutual symbolic-numeric similarities in representations. The results presented in Table 2 demonstrate that SNIP, both in its original 'frozen' state and when finetuned, consistently surpasses the performance of supervised models across all evaluated properties. This superiority is evident in both metrics - $N M S E$ and $A c c_{0.1}$. The variation in chance levels across different properties highlights the unique challenges inherent to each property. This variance underscores the adaptability and robustness of the SNIP model in navigating the diverse landscape of cross-modal property prediction tasks.</p>
<h1>C. 3 Additional Qualitative Findings of Cross-MODAL PROPERTY PREDICTION</h1>
<p>In addition to numerical results, we include visual representations of the model's latent features for each property. These visualizations offer a qualitative perspective on how our model captures and represents the underlying characteristics of different properties in representations. Fig. 8 shows a qualitative comparison of pre-finetuning and post-finetuning latent spaces of SNIP against that of supervised task prediction models, using 2-dimensional t-SNE visualizations of the encoded representations. The first two rows (NCR and Upwardness) are replicated from the main body (Fig. 2) for ease of comparison. In each task (row), the plots are colored by the values of the corresponding property. In each task, a training dataset with 10 K samples was used to train the model.
The observations from Fig. 8 show that the latent spaces of supervised models (without pre-trained SNIP) are very weakly structured and barely exhibit a recognizable trend for the properties. On the other hand, when the pre-trained SNIP is used, the latent spaces are shaped by the symbolic-numeric similarities of the functions such that numeric properties can be clustered and/or show visible trends in the symbolic encoded representation space $\boldsymbol{Z}_{S}$. Furthermore, fine-tuning the encoder, as shown in Fig. 8(c), leads to more organized latent spaces with distinct linear property trends.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: 2D t-SNE plots of the symbolic encoded representations for the tasks of predicting numeric mathematical properties: Non-Convexity Ratio, Function Upwardness, Average of $y_{i}$ and Oscillations. The plots compare the (a) supervised models without pre-training, (b) frozen pre-trained SNIP encoder, and (c) fine-tuned SNIP encoders for each task.</p>
<h1>D Additional Visualizations of SNIP Pre-Trained Latent Space</h1>
<p>Numeric Encoded Representations. We show that similar to how symbolic encoded representations are shaped by numeric behaviors, the numeric encoded vectors $\boldsymbol{Z}_{V}$ are likewise influenced by the symbolic attributes of the corresponding governing equations. Fig. 9 showcases 2D t-SNE visualizations of the learned latent space of SNIP's numeric encoded vectors, color-coded by function (a) complexity and (b) an arbitrarily defined categorization of the functions based on their dominant operators. Further details regarding these two symbolic features are provided below:
Function Complexity: Function complexity, as defined in Symbolic Regression (SR) tasks, pertains to the length of the function expressed in prefix order notation,i.e., the number of nodes in the expression tree. Intuitively, functions with a greater number of operators and variables (resulting in longer equations) are considered more complex, often exhibiting correspondingly complex behaviors.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: 2D t-SNE plots of the pretrained SNIP numeric encoded representations $\left(Z_{V}\right)$ colored by (a) Function Complexity, and (b) Function Classes based on Operators.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: 2D t-SNE plot of the pretrained SNIP symbolic encoded representations $\left(Z_{S}\right)$ colored by NonConvexity Ratio property. Adjacent to the corresponding locations of points in the latent space, the numeric behaviors of selected sample equations are displayed, illustrating the interplay between their symbolic forms and numeric properties. This visualization underscores how both the symbolic and numeric characteristics of functions influence their representation in SNIP's latent space.</p>
<p>Function Operator Classes: Mathematical functions can be broadly classified into different classes based on the operators utilized in their expressions, which in turn influence the behavior of the data they describe. It is important to note that a single function may incorporate multiple operators, contributing to the overall complexity of the data's behavior. Additionally, certain operators within a function may hold more significance than others, exerting greater influence on the range and pattern of the data. To categorize the functions, we employ the following guidelines:
First, we consider a prioritized set of unary operators: $\mathcal{O}={\operatorname{arctan}, \tan , \exp , \operatorname{sqrt}, \operatorname{inv}, \cos , \sin$, pow3, pow2}. If a function exclusively employs one of these operators, it is categorized accordingly. For simplicity, we designate both pow2 and pow3 as Polynomial, and we employ sin for both $\sin$ and $\cos$. In the event that a function incorporates more than one operator, it is assigned to the category corresponding to the operator of higher priority. It is worth noting that this categorization</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{+}$Equal Contribution. Contact email: mmeidani@andrew.cmu.edu
${ }^{+}$Code and model are available at: https://github.com/deep-symbolic-mathematics/ Multimodal-Math-Pretraining&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
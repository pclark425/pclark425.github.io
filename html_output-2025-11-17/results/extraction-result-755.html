<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-755 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-755</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-755</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-235485289</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2106.10110v1.pdf" target="_blank">Towards Distraction-Robust Active Visual Tracking</a></p>
                <p><strong>Paper Abstract:</strong> In active visual tracking, it is notoriously difficult when distracting objects appear, as distractors often mislead the tracker by occluding the target or bringing a confusing appearance. To address this issue, we propose a mixed cooperative-competitive multi-agent game, where a target and multiple distractors form a collaborative team to play against a tracker and make it fail to follow. Through learning in our game, diverse distracting behaviors of the distractors naturally emerge, thereby exposing the tracker's weakness, which helps enhance the distraction-robustness of the tracker. For effective learning, we then present a bunch of practical methods, including a reward function for distractors, a cross-modal teacher-student learning strategy, and a recurrent attention mechanism for the tracker. The experimental results show that our tracker performs desired distraction-robust active visual tracking and can be well generalized to unseen environments. We also show that the multi-agent game can be used to adversarially test the robustness of trackers.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e755.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e755.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Game</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixed Cooperative-Competitive Multi-Agent Game</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulation paradigm where a tracker competes against a cooperating team (target + distractors) that adversarially learns behaviors to produce distracting situations; used to generate diverse, emergent distractors for training and testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Mixed Cooperative-Competitive Multi-Agent Game</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Formulates active visual tracking as a partially-observable multi-agent Markov game with three heterogeneous agent roles (tracker, target, distractor(s)). The target and distractors share goals (cooperate) to make the tracker fail, while the tracker competes to follow the target. Policies are parameterized by neural networks and trained via RL (A3C) in self-play to produce emergent distractor behaviors without hand-crafted trajectories. A model pool of saved adversary checkpoints creates a natural curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>UnrealCV based virtual labs (Simple Room, Meta-x / Nav-x, Urban City, Parking Lot)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive, open-ended 3D simulation environments (based on UnrealCV) allowing multiple controllable agents, randomized distractor counts (0–4 during training), and grounded-state access for meta policies; environments support active experimentation via agent policies and adversarial training.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Adversarial generation of distractors via learned agent policies and curriculum (self-play + model pool) rather than explicit variable-selection; distractors are modeled as agents that receive shaped rewards to encourage entering tracker view and cooperation with target.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>confusing/irrelevant moving objects acting as visual distractors, occlusions, misleading appearance similarity (irrelevant variables leading to spurious correlations)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection via emergent behaviors and tracker observation: the framework uses tracker-centric relative pose and visual observation to determine which agents are currently distractors (no separate detector); the reward structure uses a relative-distance measure d(1,i) to determine distractor salience.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not an explicit statistical downweighting; influence of distractors is controlled via adversarial training of the tracker against distractor policies and reward shaping that encourages distractors to enter view (i.e., training the tracker to be robust), rather than reweighting observed variables.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Adversarial testing (train adversaries to find failure trajectories) to reveal and validate non-causal/spurious failure modes; the model pool is used to replay varied adversarial behaviors to refute fragile policies.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Self-play RL to actively explore adversarial trajectories; sampling saved adversary checkpoints (model pool) to produce a curriculum of increasing difficulty; adversarial RL to search for adversarial test trajectories against frozen trackers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Ours (full pipeline) — Table 1: Nav-4: AR=250, EL=401, SR=0.54; Meta-2: AR=141, EL=396, SR=0.44. In adversarial testing our tracker maintains higher reward than baselines across runs (qualitatively leads others).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Not applicable as baseline; ablations below quantify components removed (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>randomized 0-4 in training; evaluations reported for 0–4 distractors; adversarial testing typically used 2 distractors</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a learned adversarial multi-agent game produces diverse, realistic distractor behaviors and an emergent curriculum that significantly improves tracker robustness compared to simple environment augmentation with scripted distractors; adversarially-trained distractors expose tracker weaknesses and provide better training/testing cases than hand-crafted distractor motion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Distraction-Robust Active Visual Tracking', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e755.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e755.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistractorReward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distraction-aware Reward Structure (relative-distance sharing and penalty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reward design that shares the target's negative reward with distractors to encourage cooperation, while penalizing each distractor according to its tracker-centric relative distance to measure its unique contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Distraction-aware Reward Structure (r1, r2, r3 formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Tracker reward r1 = 1 - d(1,2); target receives r2 = -r1 (zero-sum); each distractor j gets r3_j = r2 - d(1,j), where d(1,i) is a normalized tracker-centric relative distance combining radial and angular differences. This both (1) encourages distractors to cooperate with the target (via sharing r2) and (2) gives per-distractor credit by penalizing distractors that are far from the tracker, guiding distractors to enter the tracker's view.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>UnrealCV based virtual labs (Simple Room, Meta-x / Nav-x)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive 3D simulation where agent poses are available for grounded-state meta policies; reward signals are computed from relative poses and drive learning dynamics in multi-agent self-play.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Reward shaping that makes distractors explicitly optimize to influence the tracker (cooperation bonus from target reward + per-distractor relative-distance penalty); implicitly detects and prioritizes distractors that are in view.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>visual confounders due to similar appearance and occlusion (irrelevant moving objects producing spurious correlations)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses the relative-distance metric d(1,i) computed from grounded poses (ρ and θ differences normalized by maxima) to measure whether a distractor is salient (in tracker's observable sector) and thus likely to cause confusion.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Downweighting of low-contribution distractors is implemented as an additive penalty -d(1,j) in distractor rewards, discouraging distractors far from the tracker from gaining credit; not a statistical reweighting of observations but an RL-level shaping to focus adversaries.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>N/A (the reward is a training mechanism rather than an explicit refutation test); adversarial testing later used to find failure cases produced under this reward.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Not an experimental design per se; shapes behavior of distractor agents which actively explore entering the tracker's view as part of their learned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Contributes to final 'Ours' performance (see Multi-Agent Game entry); ablation when removing or altering reward design not separately tabulated but paper reports improved emergent distractor frequency and difficulty due to this design.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>0–4 during training; experiments include 2 and 4 distractor settings</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly crediting distractors via shared target reward plus a per-distractor distance penalty leads distractors to learn to enter the tracker's view and cooperate with the target, producing more challenging and realistic distractor behaviors than scripted/random walkers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Distraction-Robust Active Visual Tracking', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e755.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e755.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CrossModalTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-modal Teacher-Student Learning (grounded-state teacher → visual student; DAGGER-like)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage learning strategy where meta policies trained on grounded state (teacher) supervise a vision-based student tracker via interactive imitation (DAGGER-style) to efficiently transfer robust behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Cross-modal Teacher-Student Learning (interactive cloning / DAGGER)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Stage 1: train meta policies using grounded low-dimensional state (relative poses) in self-play RL to produce strong teacher policies. Stage 2: collect (visual observation, teacher action) pairs while the visual student acts in the environment (sampler) and optimize the student by minimizing KL divergence between student policy and teacher action distribution (Learner) using buffer-based DAGGER-like interactive imitation; sample adversary policies from a saved model pool to recreate a curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>UnrealCV Simple Room with model pool (Meta-x / Nav-x configs)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive virtual laboratory where teachers access grounded poses but students only see visual first-person observations; supports asynchronous multiple samplers and learners and replay of historical adversary checkpoints for curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>The teacher, trained against learned distractors, implicitly supplies corrective action labels that guide the student to ignore spurious visual distractors; the curriculum (sampling different adversary checkpoints) exposes the student to increasingly difficult distractor behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>visual distractors causing spurious correlations in pixel observations (appearance-based confounding and occlusion)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Teacher uses grounded states to disambiguate true target vs distractor (so the student learns from corrected actions); no explicit statistical detector in the student, detection is learned via supervised imitation of teacher suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit: student learns to rely less on spurious visual signals because training labels come from grounded-state-optimal teacher; no explicit downweighting algorithm (e.g., weighting features) is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Interactive DAGGER-style training reduces compounding error by having the student act and receive teacher suggestions in states outside the teacher's demonstrations; adversarial testing used post-training to find remaining failure cases.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Interactive dataset collection where the student acts and teacher provides corrective labels (DAGGER), and adversary policies are sampled from a model pool to produce informative states (curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Full method (teacher-student present): Nav-4 AR=250, EL=401, SR=0.54; Meta-2 AR=141, EL=396, SR=0.44 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>w/o teacher-student learning (trained visual tracker by RL directly): Nav-4 AR=76, EL=290, SR=0.22; Meta-2 AR=79, EL=340, SR=0.40 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Student training used 2 distractors (visual policy training), model pool collected with 0–4 distractors; evaluations across 0–4 distractor settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cross-modal teacher-student learning substantially improves performance: cloning a grounded-state meta tracker produces a visual tracker far more robust to distractors than direct pixel-level RL; removing the teacher yields large drops in AR and SR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Distraction-Robust Active Visual Tracking', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e755.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e755.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdversarialTesting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Testing via Learned Adversaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A testing protocol where target and distractors are trained (kept adaptive) to find model-specific adversarial trajectories that cause a frozen tracker to fail, revealing robustness weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Adversarial Testing with Learned Adversaries</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Initialize adversary networks (target + distractors) from meta policies and train them for a fixed number of interactions (100K) against a frozen tracker model; adversaries optimize to minimize tracker reward, producing targeted adversarial trajectories that demonstrate and quantify fragility of trackers.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>UnrealCV Simple Room (adversarial testing experiments reported with 2 distractors)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive test-bed where adversaries actively explore trajectories to reduce tracker performance; the tracker is frozen while adversaries learn to exploit its weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>This is a stress-test for distractor robustness: adversaries explicitly learn collaborative distractor behaviors (e.g., occlusion, rotation around target) to induce spurious following of distractors by trackers.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>occlusion-induced disappearance, misleading moving objects with similar appearance (irrelevant variables), dynamic confounding induced by adversarial agent coordination</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Not a detection algorithm; reveals spurious signals by searching for trajectories that cause tracker to latch onto distractors — observed via declines in tracker reward and success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>N/A (the method tests whether trackers downweight spurious signals; does not itself downweight).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Uses adversarially-found failures to refute claimed robustness; demonstrates that many trackers (e.g., DiMP, ATOM) suffer steep performance drops when facing learned adversaries (DiMP/ATOM reward falling into −60 to −100 range in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Adversarial RL (optimize adversary policies while tracker is fixed) to actively seek counter-examples and failure modes; repeated runs with different seeds quantify sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Our tracker maintains higher reward under adversarial testing than baseline trackers across runs; exact curves shown in Fig. 8 (qualitative lead).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baselines (DiMP, ATOM) show dramatic reward drops under adversarial training (mentioned rewards in range −60 to −100), indicating lack of distractor robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Adversarial testing in paper used 2 distractors</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adversarially-trained distractors quickly find failure modes for most trackers; adversarial testing is an effective method for uncovering spurious correlation failures and for quantifying robustness differences between trackers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Distraction-Robust Active Visual Tracking', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e755.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e755.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RecurrentAttn</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Attention Mechanism (ConvLSTM-based attention branch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ConvLSTM-based recurrent attention branch that produces a spatial-temporal attention map to amplify target-relevant features and improve robustness to transient distractors and occlusions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Recurrent Attention via ConvLSTM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Extract convolutional features from raw RGB input, generate an attention map via a ConvLSTM recurrent branch (with hardsigmoid activation), multiply attention map with CNN feature map to obtain target-aware features, then encode through further convolution and LSTM layers for actor output. The ConvLSTM preserves spatial structure and encodes temporal consistency to help disambiguate true target across occlusions and distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>UnrealCV Simple Room and photo-realistic transfer environments (Urban City, Parking Lot)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive first-person visual observation environments where target appearance and distractors may be similar and occlusions common; recurrent attention leverages temporal continuity of active tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Spatial-temporal attention emphasizes consistent target features across frames, reducing susceptibility to transient distractor-induced spurious correlations; not a causal inference method but a representation-level robustness approach.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>transient visual distractors, occlusion-induced disappearance, appearance-similarity confounders</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Learnt attention highlights regions consistent over time and salient relative to target-aware features; no explicit statistical test—detection is implicit in learned attention weights.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit downweighting via multiplicative attention on CNN feature maps that lowers influence of irrelevant spatial locations (e.g., distractors) in downstream actor network.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Ablation: removing recurrent attention degrades Nav-4 from AR=250,EL=401,SR=0.54 to AR=128,EL=193,SR=0.27 and Meta-2 from AR=141,EL=396,SR=0.44 to AR=75,EL=331,SR=0.32 (Table 1), showing significant contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>See above (w/o recurrent attention results are the 'without' numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Evaluations reported across 0–4 distractors and specific Meta-2/Nav-4 cases in ablation table.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recurrent spatial-temporal attention substantially improves robustness to distractors and occlusions by producing more consistent target-aware representations; removing it reduces AR and SR considerably.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Distraction-Robust Active Visual Tracking', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e755.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e755.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HardNeg & DataAug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hard-negative mining & Data Augmentation (related work / baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional techniques from passive visual tracking literature cited as prior approaches to handle distractors: collecting hard negatives and data augmentation to train discriminative appearance models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distractor-aware siamese networks for visual object tracking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Hard-negative mining; distractor-aware siamese networks; discriminative model prediction (DiMP/ATOM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prior passive-tracking methods address distractors by mining hard negative samples and augmenting training sets with similar but negative examples to improve classifier discriminativeness (e.g., DaSiamRPN's distractor awareness, DiMP's discriminative model prediction trained with few iterations). These are used as baselines in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Not specific to paper's virtual lab (used as baselines in UnrealCV environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Passive visual tracker algorithms applied to images produced by the UnrealCV environments; typically non-interactive models adapted online to the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Hard-negative mining and training discriminative appearance models; online adaptation in DiMP/ATOM; distractor-aware template updates in DaSiamRPN.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>appearance-similarity distractors and background clutter (irrelevant variables in appearance space)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Hard-negative mining identifies confusing negatives in training data; distractor-aware modules detect close-looking distractors via specialized modules in network architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Model-based (classifier) downweighting via discriminative training and online adaptation rather than explicit statistical reweighting.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>As baselines: in no-distractor Simple Room DiMP achieves SR=1.0 in Nav-0; performance drops substantially with increasing distractors (qualitative and quantitative degradations reported in Fig.7 and adversarial tests).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Evaluated across 0–4 distractors in paper's experiments; adversarial tests used 2 distractors where baselines failed dramatically.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Classic appearance-based defenses (hard-negative mining, discriminative models) are helpful in simple cases but insufficient for active tracking where control and viewpoint change matter; they are vulnerable when distractors occlude the target or when spatial-temporal consistency is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Distraction-Robust Active Visual Tracking', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distractor-aware siamese networks for visual object tracking <em>(Rating: 2)</em></li>
                <li>Learning discriminative model prediction for tracking <em>(Rating: 2)</em></li>
                <li>A reduction of imitation learning and structured prediction to no-regret online learning <em>(Rating: 2)</em></li>
                <li>AD-VAT: An asymmetric dueling mechanism for learning visual active tracking <em>(Rating: 2)</em></li>
                <li>Robust adversarial reinforcement learning <em>(Rating: 1)</em></li>
                <li>Asynchronous methods for deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Unrealcv: Virtual worlds for computer vision <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-755",
    "paper_id": "paper-235485289",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "Multi-Agent Game",
            "name_full": "Mixed Cooperative-Competitive Multi-Agent Game",
            "brief_description": "A simulation paradigm where a tracker competes against a cooperating team (target + distractors) that adversarially learns behaviors to produce distracting situations; used to generate diverse, emergent distractors for training and testing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Mixed Cooperative-Competitive Multi-Agent Game",
            "method_description": "Formulates active visual tracking as a partially-observable multi-agent Markov game with three heterogeneous agent roles (tracker, target, distractor(s)). The target and distractors share goals (cooperate) to make the tracker fail, while the tracker competes to follow the target. Policies are parameterized by neural networks and trained via RL (A3C) in self-play to produce emergent distractor behaviors without hand-crafted trajectories. A model pool of saved adversary checkpoints creates a natural curriculum.",
            "environment_name": "UnrealCV based virtual labs (Simple Room, Meta-x / Nav-x, Urban City, Parking Lot)",
            "environment_description": "Interactive, open-ended 3D simulation environments (based on UnrealCV) allowing multiple controllable agents, randomized distractor counts (0–4 during training), and grounded-state access for meta policies; environments support active experimentation via agent policies and adversarial training.",
            "handles_distractors": true,
            "distractor_handling_technique": "Adversarial generation of distractors via learned agent policies and curriculum (self-play + model pool) rather than explicit variable-selection; distractors are modeled as agents that receive shaped rewards to encourage entering tracker view and cooperation with target.",
            "spurious_signal_types": "confusing/irrelevant moving objects acting as visual distractors, occlusions, misleading appearance similarity (irrelevant variables leading to spurious correlations)",
            "detection_method": "Implicit detection via emergent behaviors and tracker observation: the framework uses tracker-centric relative pose and visual observation to determine which agents are currently distractors (no separate detector); the reward structure uses a relative-distance measure d(1,i) to determine distractor salience.",
            "downweighting_method": "Not an explicit statistical downweighting; influence of distractors is controlled via adversarial training of the tracker against distractor policies and reward shaping that encourages distractors to enter view (i.e., training the tracker to be robust), rather than reweighting observed variables.",
            "refutation_method": "Adversarial testing (train adversaries to find failure trajectories) to reveal and validate non-causal/spurious failure modes; the model pool is used to replay varied adversarial behaviors to refute fragile policies.",
            "uses_active_learning": true,
            "inquiry_strategy": "Self-play RL to actively explore adversarial trajectories; sampling saved adversary checkpoints (model pool) to produce a curriculum of increasing difficulty; adversarial RL to search for adversarial test trajectories against frozen trackers.",
            "performance_with_robustness": "Ours (full pipeline) — Table 1: Nav-4: AR=250, EL=401, SR=0.54; Meta-2: AR=141, EL=396, SR=0.44. In adversarial testing our tracker maintains higher reward than baselines across runs (qualitatively leads others).",
            "performance_without_robustness": "Not applicable as baseline; ablations below quantify components removed (see other entries).",
            "has_ablation_study": true,
            "number_of_distractors": "randomized 0-4 in training; evaluations reported for 0–4 distractors; adversarial testing typically used 2 distractors",
            "key_findings": "Using a learned adversarial multi-agent game produces diverse, realistic distractor behaviors and an emergent curriculum that significantly improves tracker robustness compared to simple environment augmentation with scripted distractors; adversarially-trained distractors expose tracker weaknesses and provide better training/testing cases than hand-crafted distractor motion.",
            "uuid": "e755.0",
            "source_info": {
                "paper_title": "Towards Distraction-Robust Active Visual Tracking",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "DistractorReward",
            "name_full": "Distraction-aware Reward Structure (relative-distance sharing and penalty)",
            "brief_description": "A reward design that shares the target's negative reward with distractors to encourage cooperation, while penalizing each distractor according to its tracker-centric relative distance to measure its unique contribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Distraction-aware Reward Structure (r1, r2, r3 formulation)",
            "method_description": "Tracker reward r1 = 1 - d(1,2); target receives r2 = -r1 (zero-sum); each distractor j gets r3_j = r2 - d(1,j), where d(1,i) is a normalized tracker-centric relative distance combining radial and angular differences. This both (1) encourages distractors to cooperate with the target (via sharing r2) and (2) gives per-distractor credit by penalizing distractors that are far from the tracker, guiding distractors to enter the tracker's view.",
            "environment_name": "UnrealCV based virtual labs (Simple Room, Meta-x / Nav-x)",
            "environment_description": "Interactive 3D simulation where agent poses are available for grounded-state meta policies; reward signals are computed from relative poses and drive learning dynamics in multi-agent self-play.",
            "handles_distractors": true,
            "distractor_handling_technique": "Reward shaping that makes distractors explicitly optimize to influence the tracker (cooperation bonus from target reward + per-distractor relative-distance penalty); implicitly detects and prioritizes distractors that are in view.",
            "spurious_signal_types": "visual confounders due to similar appearance and occlusion (irrelevant moving objects producing spurious correlations)",
            "detection_method": "Uses the relative-distance metric d(1,i) computed from grounded poses (ρ and θ differences normalized by maxima) to measure whether a distractor is salient (in tracker's observable sector) and thus likely to cause confusion.",
            "downweighting_method": "Downweighting of low-contribution distractors is implemented as an additive penalty -d(1,j) in distractor rewards, discouraging distractors far from the tracker from gaining credit; not a statistical reweighting of observations but an RL-level shaping to focus adversaries.",
            "refutation_method": "N/A (the reward is a training mechanism rather than an explicit refutation test); adversarial testing later used to find failure cases produced under this reward.",
            "uses_active_learning": true,
            "inquiry_strategy": "Not an experimental design per se; shapes behavior of distractor agents which actively explore entering the tracker's view as part of their learned policy.",
            "performance_with_robustness": "Contributes to final 'Ours' performance (see Multi-Agent Game entry); ablation when removing or altering reward design not separately tabulated but paper reports improved emergent distractor frequency and difficulty due to this design.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": "0–4 during training; experiments include 2 and 4 distractor settings",
            "key_findings": "Explicitly crediting distractors via shared target reward plus a per-distractor distance penalty leads distractors to learn to enter the tracker's view and cooperate with the target, producing more challenging and realistic distractor behaviors than scripted/random walkers.",
            "uuid": "e755.1",
            "source_info": {
                "paper_title": "Towards Distraction-Robust Active Visual Tracking",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "CrossModalTS",
            "name_full": "Cross-modal Teacher-Student Learning (grounded-state teacher → visual student; DAGGER-like)",
            "brief_description": "A two-stage learning strategy where meta policies trained on grounded state (teacher) supervise a vision-based student tracker via interactive imitation (DAGGER-style) to efficiently transfer robust behaviors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Cross-modal Teacher-Student Learning (interactive cloning / DAGGER)",
            "method_description": "Stage 1: train meta policies using grounded low-dimensional state (relative poses) in self-play RL to produce strong teacher policies. Stage 2: collect (visual observation, teacher action) pairs while the visual student acts in the environment (sampler) and optimize the student by minimizing KL divergence between student policy and teacher action distribution (Learner) using buffer-based DAGGER-like interactive imitation; sample adversary policies from a saved model pool to recreate a curriculum.",
            "environment_name": "UnrealCV Simple Room with model pool (Meta-x / Nav-x configs)",
            "environment_description": "Interactive virtual laboratory where teachers access grounded poses but students only see visual first-person observations; supports asynchronous multiple samplers and learners and replay of historical adversary checkpoints for curriculum.",
            "handles_distractors": true,
            "distractor_handling_technique": "The teacher, trained against learned distractors, implicitly supplies corrective action labels that guide the student to ignore spurious visual distractors; the curriculum (sampling different adversary checkpoints) exposes the student to increasingly difficult distractor behaviors.",
            "spurious_signal_types": "visual distractors causing spurious correlations in pixel observations (appearance-based confounding and occlusion)",
            "detection_method": "Teacher uses grounded states to disambiguate true target vs distractor (so the student learns from corrected actions); no explicit statistical detector in the student, detection is learned via supervised imitation of teacher suggestions.",
            "downweighting_method": "Implicit: student learns to rely less on spurious visual signals because training labels come from grounded-state-optimal teacher; no explicit downweighting algorithm (e.g., weighting features) is applied.",
            "refutation_method": "Interactive DAGGER-style training reduces compounding error by having the student act and receive teacher suggestions in states outside the teacher's demonstrations; adversarial testing used post-training to find remaining failure cases.",
            "uses_active_learning": true,
            "inquiry_strategy": "Interactive dataset collection where the student acts and teacher provides corrective labels (DAGGER), and adversary policies are sampled from a model pool to produce informative states (curriculum).",
            "performance_with_robustness": "Full method (teacher-student present): Nav-4 AR=250, EL=401, SR=0.54; Meta-2 AR=141, EL=396, SR=0.44 (Table 1).",
            "performance_without_robustness": "w/o teacher-student learning (trained visual tracker by RL directly): Nav-4 AR=76, EL=290, SR=0.22; Meta-2 AR=79, EL=340, SR=0.40 (Table 1).",
            "has_ablation_study": true,
            "number_of_distractors": "Student training used 2 distractors (visual policy training), model pool collected with 0–4 distractors; evaluations across 0–4 distractor settings.",
            "key_findings": "Cross-modal teacher-student learning substantially improves performance: cloning a grounded-state meta tracker produces a visual tracker far more robust to distractors than direct pixel-level RL; removing the teacher yields large drops in AR and SR.",
            "uuid": "e755.2",
            "source_info": {
                "paper_title": "Towards Distraction-Robust Active Visual Tracking",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "AdversarialTesting",
            "name_full": "Adversarial Testing via Learned Adversaries",
            "brief_description": "A testing protocol where target and distractors are trained (kept adaptive) to find model-specific adversarial trajectories that cause a frozen tracker to fail, revealing robustness weaknesses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Adversarial Testing with Learned Adversaries",
            "method_description": "Initialize adversary networks (target + distractors) from meta policies and train them for a fixed number of interactions (100K) against a frozen tracker model; adversaries optimize to minimize tracker reward, producing targeted adversarial trajectories that demonstrate and quantify fragility of trackers.",
            "environment_name": "UnrealCV Simple Room (adversarial testing experiments reported with 2 distractors)",
            "environment_description": "Interactive test-bed where adversaries actively explore trajectories to reduce tracker performance; the tracker is frozen while adversaries learn to exploit its weaknesses.",
            "handles_distractors": true,
            "distractor_handling_technique": "This is a stress-test for distractor robustness: adversaries explicitly learn collaborative distractor behaviors (e.g., occlusion, rotation around target) to induce spurious following of distractors by trackers.",
            "spurious_signal_types": "occlusion-induced disappearance, misleading moving objects with similar appearance (irrelevant variables), dynamic confounding induced by adversarial agent coordination",
            "detection_method": "Not a detection algorithm; reveals spurious signals by searching for trajectories that cause tracker to latch onto distractors — observed via declines in tracker reward and success rate.",
            "downweighting_method": "N/A (the method tests whether trackers downweight spurious signals; does not itself downweight).",
            "refutation_method": "Uses adversarially-found failures to refute claimed robustness; demonstrates that many trackers (e.g., DiMP, ATOM) suffer steep performance drops when facing learned adversaries (DiMP/ATOM reward falling into −60 to −100 range in experiments).",
            "uses_active_learning": true,
            "inquiry_strategy": "Adversarial RL (optimize adversary policies while tracker is fixed) to actively seek counter-examples and failure modes; repeated runs with different seeds quantify sensitivity.",
            "performance_with_robustness": "Our tracker maintains higher reward under adversarial testing than baseline trackers across runs; exact curves shown in Fig. 8 (qualitative lead).",
            "performance_without_robustness": "Baselines (DiMP, ATOM) show dramatic reward drops under adversarial training (mentioned rewards in range −60 to −100), indicating lack of distractor robustness.",
            "has_ablation_study": true,
            "number_of_distractors": "Adversarial testing in paper used 2 distractors",
            "key_findings": "Adversarially-trained distractors quickly find failure modes for most trackers; adversarial testing is an effective method for uncovering spurious correlation failures and for quantifying robustness differences between trackers.",
            "uuid": "e755.3",
            "source_info": {
                "paper_title": "Towards Distraction-Robust Active Visual Tracking",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "RecurrentAttn",
            "name_full": "Recurrent Attention Mechanism (ConvLSTM-based attention branch)",
            "brief_description": "A ConvLSTM-based recurrent attention branch that produces a spatial-temporal attention map to amplify target-relevant features and improve robustness to transient distractors and occlusions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Recurrent Attention via ConvLSTM",
            "method_description": "Extract convolutional features from raw RGB input, generate an attention map via a ConvLSTM recurrent branch (with hardsigmoid activation), multiply attention map with CNN feature map to obtain target-aware features, then encode through further convolution and LSTM layers for actor output. The ConvLSTM preserves spatial structure and encodes temporal consistency to help disambiguate true target across occlusions and distractors.",
            "environment_name": "UnrealCV Simple Room and photo-realistic transfer environments (Urban City, Parking Lot)",
            "environment_description": "Interactive first-person visual observation environments where target appearance and distractors may be similar and occlusions common; recurrent attention leverages temporal continuity of active tracking.",
            "handles_distractors": true,
            "distractor_handling_technique": "Spatial-temporal attention emphasizes consistent target features across frames, reducing susceptibility to transient distractor-induced spurious correlations; not a causal inference method but a representation-level robustness approach.",
            "spurious_signal_types": "transient visual distractors, occlusion-induced disappearance, appearance-similarity confounders",
            "detection_method": "Learnt attention highlights regions consistent over time and salient relative to target-aware features; no explicit statistical test—detection is implicit in learned attention weights.",
            "downweighting_method": "Implicit downweighting via multiplicative attention on CNN feature maps that lowers influence of irrelevant spatial locations (e.g., distractors) in downstream actor network.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "Ablation: removing recurrent attention degrades Nav-4 from AR=250,EL=401,SR=0.54 to AR=128,EL=193,SR=0.27 and Meta-2 from AR=141,EL=396,SR=0.44 to AR=75,EL=331,SR=0.32 (Table 1), showing significant contribution.",
            "performance_without_robustness": "See above (w/o recurrent attention results are the 'without' numbers).",
            "has_ablation_study": true,
            "number_of_distractors": "Evaluations reported across 0–4 distractors and specific Meta-2/Nav-4 cases in ablation table.",
            "key_findings": "Recurrent spatial-temporal attention substantially improves robustness to distractors and occlusions by producing more consistent target-aware representations; removing it reduces AR and SR considerably.",
            "uuid": "e755.4",
            "source_info": {
                "paper_title": "Towards Distraction-Robust Active Visual Tracking",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "HardNeg & DataAug",
            "name_full": "Hard-negative mining & Data Augmentation (related work / baselines)",
            "brief_description": "Traditional techniques from passive visual tracking literature cited as prior approaches to handle distractors: collecting hard negatives and data augmentation to train discriminative appearance models.",
            "citation_title": "Distractor-aware siamese networks for visual object tracking",
            "mention_or_use": "mention",
            "method_name": "Hard-negative mining; distractor-aware siamese networks; discriminative model prediction (DiMP/ATOM)",
            "method_description": "Prior passive-tracking methods address distractors by mining hard negative samples and augmenting training sets with similar but negative examples to improve classifier discriminativeness (e.g., DaSiamRPN's distractor awareness, DiMP's discriminative model prediction trained with few iterations). These are used as baselines in the paper.",
            "environment_name": "Not specific to paper's virtual lab (used as baselines in UnrealCV environments)",
            "environment_description": "Passive visual tracker algorithms applied to images produced by the UnrealCV environments; typically non-interactive models adapted online to the domain.",
            "handles_distractors": true,
            "distractor_handling_technique": "Hard-negative mining and training discriminative appearance models; online adaptation in DiMP/ATOM; distractor-aware template updates in DaSiamRPN.",
            "spurious_signal_types": "appearance-similarity distractors and background clutter (irrelevant variables in appearance space)",
            "detection_method": "Hard-negative mining identifies confusing negatives in training data; distractor-aware modules detect close-looking distractors via specialized modules in network architectures.",
            "downweighting_method": "Model-based (classifier) downweighting via discriminative training and online adaptation rather than explicit statistical reweighting.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "As baselines: in no-distractor Simple Room DiMP achieves SR=1.0 in Nav-0; performance drops substantially with increasing distractors (qualitative and quantitative degradations reported in Fig.7 and adversarial tests).",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": "Evaluated across 0–4 distractors in paper's experiments; adversarial tests used 2 distractors where baselines failed dramatically.",
            "key_findings": "Classic appearance-based defenses (hard-negative mining, discriminative models) are helpful in simple cases but insufficient for active tracking where control and viewpoint change matter; they are vulnerable when distractors occlude the target or when spatial-temporal consistency is needed.",
            "uuid": "e755.5",
            "source_info": {
                "paper_title": "Towards Distraction-Robust Active Visual Tracking",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distractor-aware siamese networks for visual object tracking",
            "rating": 2,
            "sanitized_title": "distractoraware_siamese_networks_for_visual_object_tracking"
        },
        {
            "paper_title": "Learning discriminative model prediction for tracking",
            "rating": 2,
            "sanitized_title": "learning_discriminative_model_prediction_for_tracking"
        },
        {
            "paper_title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "rating": 2,
            "sanitized_title": "a_reduction_of_imitation_learning_and_structured_prediction_to_noregret_online_learning"
        },
        {
            "paper_title": "AD-VAT: An asymmetric dueling mechanism for learning visual active tracking",
            "rating": 2,
            "sanitized_title": "advat_an_asymmetric_dueling_mechanism_for_learning_visual_active_tracking"
        },
        {
            "paper_title": "Robust adversarial reinforcement learning",
            "rating": 1,
            "sanitized_title": "robust_adversarial_reinforcement_learning"
        },
        {
            "paper_title": "Asynchronous methods for deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "asynchronous_methods_for_deep_reinforcement_learning"
        },
        {
            "paper_title": "Unrealcv: Virtual worlds for computer vision",
            "rating": 1,
            "sanitized_title": "unrealcv_virtual_worlds_for_computer_vision"
        }
    ],
    "cost": 0.0170805,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Distraction-Robust Active Visual Tracking</p>
<p>Fangwei Zhong 
Peng Sun 
Wenhan Luo 
Tingyun Yan 
Yizhou Wang 
Towards Distraction-Robust Active Visual Tracking</p>
<p>In active visual tracking, it is notoriously difficult when distracting objects appear, as distractors often mislead the tracker by occluding the target or bringing a confusing appearance. To address this issue, we propose a mixed cooperativecompetitive multi-agent game, where a target and multiple distractors form a collaborative team to play against a tracker and make it fail to follow. Through learning in our game, diverse distracting behaviors of the distractors naturally emerge, thereby exposing the tracker's weakness, which helps enhance the distraction-robustness of the tracker. For effective learning, we then present a bunch of practical methods, including a reward function for distractors, a cross-modal teacherstudent learning strategy, and a recurrent attention mechanism for the tracker. The experimental results show that our tracker performs desired distraction-robust active visual tracking and can be well generalized to unseen environments. We also show that the multi-agent game can be used to adversarially test the robustness of trackers.</p>
<p>Introduction</p>
<p>We study Active Visual Tracking (AVT), which aims to follow a target object by actively controlling a mobile robot given visual observations. AVT is a fundamental function for active vision systems and widely demanded in real-world applications, e.g., autonomous driving, household robots, and intelligent surveillance. Here, the agent is required to perform AVT in various scenarios, ranging from a simple room to the wild world. However, the trackers are still vulnerable, when running in an environments with complex situations, e.g., complicated backgrounds, obstacle occlusions, distracting objects. Which one is the real target ? Among all the challenges, the distractor, which can induce confusing visual observation and occlusion, is a considerably prominent difficulty. Distraction emerges frequently in the real world scenario, such as a school where students wear similar uniforms. Considering an extreme case (see Fig. 1), there is a group of people dressed in the same clothes, and you are only given a template image of the target, can you confidently identify the target from the crowd?</p>
<p>The distraction has been primarily studied in the passive tracking setting (Kristan et al., 2016) (running on collected video clips) but is rarely considered in the active setting (Luo et al., 2020;Zhong et al., 2021). In the passive setting, previous researchers (Nam &amp; Han, 2016;Zhu et al., 2018;Bhat et al., 2019) took great efforts on learning a discriminative visual representation in order to identify the target from the crowded background. Yet it is not sufficient for an active visual tracker, which should be of not only a suitable state representation but also an optimal control strategy to move the camera to actively avoid distractions, e.g., finding a more suitable viewpoint to seize the target.</p>
<p>To realize such a robust active tracker, we argue that the primary step is to build an interactive environment, where various distraction situations can frequently emergent. A straightforward solution is adding a number of moving objects as distractors in the environment. However, it is nontrivial to model a group of moving objects. Pre-defining object trajectories based on hand-crafted rules seems tempting, but it will lead to overfitting in the sense that the yielded tracker generalizes poorly on unseen trajectories.</p>
<p>arXiv:2106.10110v1 [cs.CV] 18 Jun 2021</p>
<p>Inspired by the recent work on multi-agent learning (Sukhbaatar et al., 2018;Baker et al., 2020), we propose a mixed Cooperative-Competitive Multi-Agent Game to automatically generate distractions. In the game, there are three type of agents: tracker, target and distractor. The tracker tries to always follow the target. The target intends to get rid of the tracker. The target and distractors form a team, which is to make trouble for the tracker. The cooperativecompetitive relations among agents are shown in Fig. 2. Indeed, it is already challenging to learn an escape policy for a target . As the complicated social interaction among agents, it will be even more difficult to model the behavior of multiple objects to produce distraction situations, which is rarely studied in previous work.</p>
<p>Thus, to ease the multi-agent learning process, we introduce a battery of practical alternatives. First, to mitigate the credit assignments problem among agents, we design a reward structure to explicitly identify the contribution of each distractor with a relative distance factor. Second, we propose a cross-modal teacher-student learning strategy, since directly optimizing the visual policies by Reinforcement Learning (RL) is inefficient. Specifically, we split the training process into two steps. At the first step, we exploit the grounded state to find meta policies for agents. Since the grounded state is clean and low-dimensional, we can easily train RL agents to find a equilibrium of the game, i.e., the target actively explores different directions to escape, the distractors frequently appear in the tracker's view, and the tracker still closely follows the target. Notably, a multi-agent curriculum is also naturally emergent, i.e., the difficulty of the environment induced by the target-distractors cooperation is steadily increased with the evolution of the meta policies. In the second step, we use the skillful meta tracker (teacher) to supervise the learning of the visual active tracker (student). When the student interacts with the opponents during learning, we replay the multi-agent curriculum by sampling the historical network parameters of the target and distractors. Moreover, a recurrent attention mechanism is employed to enhance the state representation of the visual tracker.</p>
<p>The experiments are conducted in virtual environments with numbers of distractors. We show that our tracker significantly outperforms the state-of-the-art methods in a room with clean backgrounds and a number of moving distractors. The effectiveness of introduced components are validated in ablation study. After that, we demonstrate another use of our multi-agent game, adversarial testing the trackers. While taking adversarial testing, the target and distractors, optimized by RL, can actively find trajectories to mislead the tracker within a very short time period. In the end, we validate that the learned policy is of good generalization in unseen environments. The code and demo videos are available on https://sites.google.com/view/ distraction-robust-avt. </p>
<p>Cooperative-Competitive Multi-Agent Game</p>
<p>Tracker Target Distractors</p>
<p>Related Work</p>
<p>Active Visual Tracking. The methods to realize active visual tracking can be simply divided into two categories: two-stage methods and end-to-end methods. Conventionally, the task is accomplished in two cascaded sub-tasks, i.e., the visual object tracking (image → bounding box) (Kristan et al., 2016) and the camera control (bounding box → control signal). Recently, with the advances of deep reinforcement learning and simulation, the end-to-end methods (image → control signal), optimizing the neural network in an end-to-end manner (Mnih et al., 2015), has achieved a great progress in active object tracking (Luo et al., 2018;Li et al., 2020;Zhong et al., 2019). It is firstly introduced in (Luo et al., 2018) to train an end-to-end active tracker in simulator via RL.  successfully implements the end-to-end tracker in the real-world scenario. AD-VAT(+)  further improves the robustness of the end-to-end tracker with adversarial RL. Following this track, our work is the first paper to study the distractors in AVT with a mixed multi-agent game.</p>
<p>Distractors has attracted attention of researchers in video object tracking (Babenko et al., 2009;Bolme et al., 2010;Kalal et al., 2012;Nam &amp; Han, 2016;Zhu et al., 2018). To address the issue caused by distractors, data augmentation methods like collecting similar but negative samples to train a more powerful classifier is pretty useful (Babenko et al., 2009;Kalal et al., 2012;Hare et al., 2015;Zhu et al., 2018;Deng &amp; Zheng, 2021). Other advanced techniques like hard negative examples mining (Nam &amp; Han, 2016) are also proposed for addressing this. Recently, DiMP (Bhat et al., 2019) develops a discriminative model prediction architecture for tracking, which can be optimized in only a few iterations. Differently, we model the distractors as agents with learning ability, which can actively produce diverse situations to enhance the learning tracker.</p>
<p>Multi-Agent Game. It is not a new concept to apply the multi-agent game to build a robust agent. Roughly speaking, most of previous methods Florensa et al., 2018;Huang et al., 2017;Mandlekar et al., 2017;Pinto et al., 2017b;Sukhbaatar et al., 2018) focus on modeling a two-agent competition (adversary protagonist) to learn a more robust policy. The closest one to ours is AD-VAT , which proposes an asymmetric dueling mechanism (tracker vs. target) for learning a better active tracker. Our multi-agent game can be viewed as an extension of AD-VAT, by adding a number of learnable distractors within the tracker-target competition. In this setting, to compete with the tracker, it necessary to learn collaborative strategies among the distractors and target. However, only a few works (ope; Hausknecht &amp; Stone, 2015;Tampuu et al., 2017;Baker et al., 2020) explored learning policies under a Mixed Multi-Agent Game. In these multi-agent games, the agents usually are homogeneous, i.e., each agent plays an equal role in a team. Differently, in our game, the agents are heterogeneous, including tracker, target, and distractor(s). And the multi-agent competition is also asymmetric, i.e., the active tracker is "lonely", which has to independently fight against the team formed by the target and distractors.</p>
<p>To find the equilibrium in the multi-agent game, usually, Muti-Agent Reinforcement Learning(MARL) (Rashid et al., 2018;Sunehag et al., 2018;Tampuu et al., 2017;Lowe et al., 2017) are employed. Though effective and successful in some toy examples, the training procedure of these methods is really inefficient and unstable, especially in cases that the agents are fed with high-dimensional raw-pixel observation. Recently, researchers have demonstrated that exploiting the grounded state in simulation can greatly improve the stability and speed of vision-based policy training in singleagent scenario (Wilson &amp; Hermans, 2020;Andrychowicz et al., 2020;Pinto et al., 2017a) by constructing a more compact representation or approximating a more precise value function. Inspired by these, we exploit the grounded state to facilitate multi-agent learning by taking a cross-modal teacher-student learning, which is close to Multi-Agent Imitation Learning (MAIL). MAIL usually rely on a set of collected expert demonstrations (Song et al., 2018;Šošić et al., 2016;Bogert &amp; Doshi, 2014;Lin et al., 2014) or a programmed expert to provide online demonstrations (Le et al., 2017). However, the demonstrations collection and programmed expert designing are usually performed by human. Instead, we adopt a multi-agent game for better cloning the expert behaviour to a vision-based agent. The expert agent is fed with the grounded state and learned by self-play.</p>
<p>Multi-Agent Game</p>
<p>Inspired by the AD-VAT , we introduce a group of active distractors in the tracker-target competition to induce distractions. We model such a competition as a mixed Cooperative-Competitive Multi-Agent Game, where agents are employed to represent the tracker, target and dis-tractor, respectively. In this game, the target and distractor(s) constitute a cooperative group to actively find the weakness of the tracker. In contrast, the tracker has to compete against the cooperative group to continuously follow the target. To be specific, each one has its own goal, shown as following:</p>
<p>• Tracker chases the target object and keeps a specific relative distance and angle from it.</p>
<p>• Target finds a way to get rid of the tracker.</p>
<p>• Distractor cooperates with the target and other distractors to help the target escape from the tracker, by inducing confusing visual observation or occlusion.</p>
<p>Formulation</p>
<p>Formally, we adopt the settings of Multi-Agent Markov Game (Littman, 1994). Let subscript i ∈ {1, 2, 3} be the index of each agent, i.e., i = 1, i = 2, and i = 3 denote the tracker, the target, and the distractors, respectively. Note that, even the number of distractors would be more than one, we use only one agent to represent them. That is because they are homogeneous and share the same policy. The game is governed by the tuple 
&lt; S, O i , A i , R i , P &gt;, i = 1, 2, 3, where S, O i , A i , R i , P denotei,t = o i,t (s t , s t−1 , o i,t−1 ), where o i,t , o i,t−1 ∈ O i , s t , s t−1 ∈ S.
Since the visual observation is imperfect, the agents play a partially observable multi-agent game. It reduces to o i,t = s t in the case of fully observable game, which means that the agent can access the grounded states directly. In the AVT task, the grounded states are the relative poses among all agents, needed by the meta policies. When all the three agents take simultaneous actions a i,t ∈ A i , the updated state s t+1 is drawn from the environment state transition probability, as s t+1 ∼ P(·|s t , a 1,t , a 2,t , a 3,t ). Meanwhile, each agent receives an immediate reward r i,t = r i,t (s t , a i,t ) respectively. The policy of the agent i, π i (a i,t |o i,t ), is a distribution over its action a i,t conditioned on its observation o i,t . Each policy π i is to maximize its cumulative discounted reward E πi T t=1 γ t−1 r i,t , where T denotes the horizontal length of an episode and r i,t is the immediate reward of agent i at time step t. The policy takes as function approximator a neural network with parameter Θ i , written as π i (a i,t |o i,t ; Θ i ). The cooperation-competition will manifest by the design of reward function r i,t , as described in the next subsection.
( * , * ) ( 2 , 2 ) ( 3 1 , 3 1 ) ( 3 2 , 3 2 ) (0, 0)
Distractor Target Tracker Expected position Figure 3. A top-down view of the tracker-centric coordinate system with target (orange), distractors (yellow), and expected target position (green). Tracker (blue) is at the origin of coordinate (0, 0). The gray sector area represents the observable area of the tracker. The arrow in the tracker notes the front of the camera.</p>
<p>Reward Structure</p>
<p>To avoid the aforementioned intra-team ambiguity, we propose a distraction-aware reward structure, which modifies the cooperative-competitive reward principle to take into account the following intuition: 1) The target and distractors are of a common goal (make the tracker fail). So they need to share the target's reward to encourage the targetdistractor cooperation. 2) Each distractor should have its own rewards, which can measure its contribution to the team.</p>
<p>3) The distractor obviously observed by the tracker will cause distraction in all probability, and the one that out of the tracker's view can by no means "mislead" the tracker.</p>
<p>We begin with defining a tracker-centric relative distance d(1, i), measuring the geometric relation between the tracker and the other player i &gt; 1.
d(1, i) = |ρ i − ρ * | ρ max + |θ i − θ * | θ max ,(1)
where (ρ i , θ i ) and (ρ * , θ * ) represent the location of the player i &gt; 1 and the expected target in a tracker-centric polar coordinate system, respectively. ρ is the distance from the origin (tracker), and θ is the relative angle to the front of the tracker. See Fig. 3 for an illustration.</p>
<p>With the relative distance, we now give a formal definition of the reward structure as:
r 1 = 1 − d(1, 2) , r 2 = −r 1 , r j 3 = r 2 − d(1, j) .
(2)</p>
<p>Here we omit the timestep subscript t without confusion. The tracker reward is similar to AD-VAT , measured by the distance between the target and expected location. The tracker and target play a zero-sum game, where r 1 + r 2 = 0. The distractor is to cooperate with the target by sharing r 2 . Meanwhile, we identify its unique contribution by taking its relative distance d(1, j) as a penalty term in the reward. It is based on an observation that once the tracker is misled by a distractor, the distractor will be regarded as the target and placed at the center of the tracker's view. Otherwise, the distractor will be penalized by d(i, j) when it is far from the tracker, as its contribution to the gain of r 2 will be marginal. Intuitively, the penalty term d(1, j) can guide the distractors learn to navigate to the tracker's view, and the bonus from target r 2 can encourage it to cooperate with the target to produce distraction situations to mislead the tracker. In the view of relative distance, the distractor is to minimize 2). Besides, if a collision is detected to agent i, we penalize the agent with a reward of −1. When we remove the penalty, the learned distractors would prefer to physically surround and block the tracker , rather than make confusing visual observations to mislead the tracker.
T t=1 d t (1, j) and maximize T t=1 d t (1, 2), while the tracker is to minimize T t=1 d t (1,</p>
<p>Learning Strategy</p>
<p>To efficiently learn policies in the multi-agent game, we introduce a two-step learning strategy to combine the advantages of Reinforcement Learning (RL) and Imitation Learning (IL). First, we train meta policies (using the grounded state as input) via RL in a self-play manner. After that, IL is employed to efficiently impart the knowledge learned by meta policies to the active visual tracker. Using the grounded state can easily find optimal policies (teacher) for each agent first. The teacher can guide the learning of the visual tracker (student) to avoid numerous trial-and-error explorations. Meanwhile, opponent policies emergent in different learning stage are of different level of difficulties, forming a curriculum for the student learning.</p>
<p>Learning Meta Policies with Grounded State</p>
<p>At the first step, we train meta policies using self-play. Hence, the agent can always play with opponents of an appropriate level, regarded as a natural curriculum . The meta policies, noted as π * 1 (s t ), π * 2 (s t ), π * 3 (s t ), enjoy privileges to access the grounded state s t , rather than only visual observations o i,t . Even though such grounded states are unavailable in most real-world scenarios, we can easily reach it in the virtual environment. For AVT, the grounded state is the relative poses (position and orientation) among players. We omit the shape and size of the target and distractors, as they are similar during training. Note that the state for agent i will be transformed into the entity-centric coordinate system before feed into the policy network. To be specific, the input of agent i is a sequence about the relative poses to other agents, represented as P i,1 , P i,2 , ...P i,n , where n is the number of the agents and P i.j = (ρ i,j , cos(θ i,j ), sin(θ i,j ), cos(φ i,j ), sin(φ i,j ). Note that (ρ i,j , θ i,j , φ i,j ) indicates the relative distance, angle, and relative orientation from agent i to agent j. Agent i is at the origin of the coordination. Since the number of the distractors is randomized during either training or testing, the length of the input sequence would be different across each episode. Thus, we adopt the Bidirectional-Gated Recurrent Unit (Bi-GRU) to pin-down a fixed-length feature vector, to enable the network to handle the variable-length distractors. Inspired by the tracker-award model in AD-VAT , we also fed the tracker's action a 1 to the target and distractors, to induce stronger adversaries.</p>
<p>During training, we optimize the meta policies with a modern RL algorithm, e.g., A3C (Mnih et al., 2016). To collect a model pool containing policies at different levels, we save the network parameters the of target and distractor every 50K interactions. During the student learning stage, we can sample the old parameters from the model pool for the target and distractors to reproduce the emergent multi-agent curriculum. Note that we further fine-tune the meta trackers to play against all the opponents in the model pool before going into the next stage. More details about the meta policies can be found in Appendix. A.</p>
<p>Learning Active Visual Tracking</p>
<p>With the learned meta policies, we seek out a teacher-student learning strategy to efficiently build a distraction-robust active visual tracker in an end-to-end manner, shown as Fig. 4. We apply the meta tracker π * 1 (s t ) (teacher) to teach a visual tracker π 1 (o i,t ) (student) to track. In the teacher-student learning paradigm, the student needs to clone the teacher's behavior. Therefore, we dive into the behavioral cloning problem. However, it is infeasible to directly apply supervised learning to learn from the demonstration collected by expert's behavior. Because the learned policy will inevitably make at least occasional mistakes. However, such a small error may lead the agent to a state which deviates from expert demonstrations. Consequently, The agent will make further mistakes, leading to poor performance. At the end, the student will be of poor generalization to novel scenes.</p>
<p>Thus, we take an interactive training manner as DAG-GER (Ross et al., 2011), in which the student takes actions from the learning policy and gets suggestions from the teacher to optimize the policy. To be specific, the training mechanism is composed of two key modules: Sampler and Learner. In the Sampler, we perform the learning policy π 1 (o i,t ) to control the tracker to interact with the others. The target and distractors are governed by meta policies π * 2 (s t ) and π * 3 (s t ) respectively. Meanwhile, the meta tracker π * 1 (s t ) provides expert suggestions a * 1,t by monitoring the grounded state. At each step, we sequentially store the visual observa-
Tracker Visual Observation 1 Target Distractor ( 1 * || 1 ) 1 * ( 1 * | ) Action 1 Internal State 2 * ( 2 * | ) 3 * ( 2 * | ) Teacher Suggestion 1 * 1 ( 1 | 1 ) Student
Cross-modal Teacher-student Learning Figure 4. An overview of the cross-modal teacher-student learning strategy. Blue, orange, gray represent tracker, target, distractors, respectively. π * 1 , π * 2 , π * 3 enjoy privileges to acquire the grounded state as input. The tracker adopts the student network (visual tracker) to plays against opponents (target and distractors) to collect useful experiences for learning. We sample parameters from the model pool constructed in the first stage. During training, the student network is optimized by minimizing the KL divergence between the teacher suggestion and the student output. tion and the expert suggestions (o 1,t , a * 1,t ) in a buffer B. To make diverse multi-agent environments, we random sample parameters from the model pool for π * 2 (s t ) and π * 3 (s t ). The model pool is constructed during the first stage, containing meta policies at different levels. So we easily reproduce the multi-agent curriculum emergent in the first stage. We also demonstrate the importance of the multi-agent curriculum in the ablation analysis.</p>
<p>In parallel, the Learner samples a batch of sequences from the buffer B and optimizes the student network in a supervised learning manner. The objective function of the learner is to minimize the relative entropy (Kullback-Leibler divergence) of the action distribution between student and teacher, computed as:
L KL = N n=1 T t=1 D KL (a * 1,t ||π(o t )) ,(3)
where N is the number of trajectories in the sampled batch, T is the length of one trajectory. In practice, multiple samplers and one learner work asynchronously, significantly reducing the time needed to obtain satisfactory performance.</p>
<p>Moreover, we employ a recurrent attention mechanism in the end-to-end tracker network  to learn a representation which is consistent in spatial and temporal. We argue that a spatial-temporal representation is needed for the active visual tracking, especially in the case of distraction appearing. Specially, we use the ConvLSTM (Xingjian et al., 2015) to encode an attention map, which is multiplied by the feature map extracted a target-aware feature from the CNN encoder. See Appendix.B for more details.</p>
<p>Experimental Setup</p>
<p>In this section, we introduce the environments, baselines, and evaluation metrics used in our experiments.</p>
<p>Environments. Similar to previous works (Luo et al., 2020;Zhong et al., 2019), the experiments are conducted on Un-realCV environments (Qiu et al., 2017). We extend the two-agent environments used in AD-VAT  to study the multi-agent (n &gt; 2) game. Similar to AD-VAT, the action space is discrete with seven actions, move-forward/backward, turn-left/right, move-forward-andturn-left/right, and no-op. The observation for the visual tracker is the color image in its first-person view. The primary difference is that we add a number of controllable distractors in the environment, shown as Fig. 5. Both target and distractors are controlled by the scripted navigator, which temporally sample a free space in the map and navigate to it with a random set velocity. Note that we name the environments that use the scripted navigator to control the target and x distractors as Nav-x. If they are governed by the meta policies, we mark the environment as Meta-x. Besides, we enable agents to access the poses of players, which is needed by the meta policies. Two realistic scenarios (Urban City and Parking Lot) are used to verify the generalization of our tracker in other unseen realistic environments with considerable complexity. In Urban City, there are five unseen characters are placed, and the appearance of each is randomly sampled from four candidates. So it is potential to see that two characters dressed the same in an environment.</p>
<p>In Parking Lot, all of the target and distractors are of the same appearance. Under this setting, it would be difficult for the tracker to identify the target from distractions.</p>
<p>Evaluation Metrics. We employ the metrics of Accumulated Reward (AR), Episode Length (EL), Success Rate (SR) for our evaluation. Among those metrics, AR is the recommended primary measure of tracking performance, as it considers both precision and robustness. The other metrics are also reported as auxiliary measures. Specifically, AR is affected by the immediate reward and the episode length. Immediate reward measures the goodness of tracking at each step. EL roughly measures the duration of good tracking, as the episode is terminated when the target is lost for continuous 5 seconds or it reaches the max episode length. SR is employed in this work to better evaluate the robustness, which counts the rate of successful tracking episodes after running 100 testing episodes. An episode is marked as success only if the tracker continuously follows the target till the end of the episode (reaching the max episode length).</p>
<p>Baselines. We compare our method with a number of stateof-the-art methods and their variants, including the twostage and end-to-end trackers. First, We develop conventional two-stage active tracking methods by combining passive trackers with a PID-like controller. As for the passive trackers, we directly use three off-the-shelf models (DaSi-amRPN (Zhu et al., 2018), ATOM (Danelljan et al., 2019), DiMP (Bhat et al., 2019)) without additional training in our environment. Notably, both DiMP and ATOM can be optimized on the fly to adapt to a novel domain. So they can generalize well to our virtual environments and achieve strong performance in the no-distractor environments, e.g., DiMP tracker achieves 1.0 SR in Simple Room (Nav-0). Second, two recent end-to-end methods (SARL (Luo et al., 2020), AD-VAT ) are reproduced in our environment to compare. We also extend them by adding two random walking distractors in the training environment, noted as SARL+ and AD-VAT+.</p>
<p>All end-to-end methods are trained in Simple Room with environment augmentation (Luo et al., 2020). After the learning converged, we choose the model that achieves the best performance in the environment for further evaluation.</p>
<p>Considering the random factors, we report the average results after running 100 episodes. More implementation details are introduced in Appendix.C.</p>
<p>Results</p>
<p>We first demonstrate the evolution of the meta policies while learning in our game. Then, we report the testing results in Simple Room with different numbers of distractors. After that, we conduct an ablation study to verify the contribution of each component in our method. We also adversarially test the trackers in our game. Moreover, we evaluate the transferability of the tracker in photo-realistic environments.</p>
<p>The Evolution of the Meta Policies</p>
<p>While learning to play the multi-agent game, the multi-agent curriculum automatically emerges. To demonstrate it, we evaluate the skill-level of the adversaries (target+distractors) at different learning stages from two aspects: the frequency of the distractor appearing in the tracker's view and the success rate (SR) of the off-the-shelf trackers (DiMP and ATOM). To do it, we collects seven meta policies after agents take 0, 0.4M , 0.7M , 1M , 1.3M , 1.65M , 2M interactions, respectively. We then make the visual tracker (ATOM and DiMP) to play with each collected adversaries (one target and two distractors) in Simple Room, and count the success rate of each tracker in 100 episodes. we also let the converged meta tracker (at 2M ) follow the targetdistractors group from different stages, respectively. And we report the success rate of the meta tracker and the frequency of the distractor appearing in the tracker's view, shown as Fig. 6. We can see that the frequency of the distractor appearing is increased during the multi-agent learning. Meanwhile, the success rate of the visual tracker is decreased. This evidence shows that the complexity of the multi-agent environment is steadily increased with the development of the target-distractor cooperation. Meanwhile, we also notice that the learned meta tracker can robustly follow the target (SR ≥ 0.98), even when the distractors frequently appearing in its view. This motivates us to take the meta tracker as a teacher to guide the learning of the active visual tracker.</p>
<p>Evaluating with Scripted Distractors</p>
<p>We analyze the distraction robustness of the tracker in Simple Room with scripted target and distractors. This environment is relatively simple to most real-world scenarios as the background is plain and no obstacles is placed. So most trackers can precisely follow the target when there is no presence of distraction. Hence, we can explicitly analyze the distraction robustness by observing how the tracker's performance is changed with the increasing number of distractors, shown as Fig. 7. Note that we normalize the reward by the average score achieved by the meta tracker, which is regarded as the best performance that the tracker could reach in each configuration. We observed that the learned meta tracker is strong enough to handle different cases, and hardly lost in the environment.</p>
<p>We can see that most methods (except DaSiamRPN) are competitive when there is no distractor. The low score of DaSiamRPN is mainly due to the inaccuracy of the predicted bounding box, which further leads to the tracker's failure in keeping a certain distance from the target. With the increasing number of distractors, the gap between our tracker and baselines gradually broaden. For example, in the four distractors room, the normalized reward achieved by our tracker is two times the ATOM tracker, i.e., 0.61 vs. 0.25. For the two-stage methods, in most simple cases, DiMP is a little better than ATOM, thanks to the discriminative model prediction. However, it remains to get lost easily when the distractor occludes the target. We argue that there are two reasons leading to the poor performance of the passive tracker when distractions appearing: 1) the target representation is without long-term temporal context. 2) the tracker lacks a mechanism to predict the state of an invisible target. Thus, once the target is disappeared (occluded by distractors or out of the view), the tracker will regard the observed distractor as a target. If it follows the false target to go, the true target will hardly appear in its view again.</p>
<p>For the end-to-end methods, it seems much weaker than the two-stage methods in the distraction robustness, especially when playing against many distractors. By visualizing the tracking process, we find that AD-VAT tends to follow the moving object in the view but unable to identify which is the true target. So it is frequently misled by the moving objects around the target. Besides, the curves of SARL+ and AD-VAT+ are very close to the original version (SARL and AD-VAT). However, without the target-distractor cooperation, the distraction situation appears at a low frequency in the plus versions. Thus, the learned trackers are still vulnerable to the distractors, and the improvements they achieved are marginal. This indicates that it is useless to simply augment the training environment with random moving distractors.</p>
<p>Our tracker significantly outperforms others in all the cases of distractors. This evidence shows that our proposed method is of great potential for realizing robust active visual tracking in a complex environment. However, the performance gap between our model and the teacher model (1 − ours) indicates that there is still room for improvement. By visualizing the test sequences of our model we find that it mainly fails in extreme tough cases where the tracker is surrounded by some distractors that totally occlude the target or block the way to track. More vivid examples can be found in the demo video. </p>
<p>Ablation Study</p>
<p>We conduct an ablation study to better understand the contribution of each introduced component in our learning method. 1) To evaluate the effectiveness of the multi-agent curriculum, we use the scripted navigator to control the target and distractors when taking the teacher-student learning, instead of replaying the policies collected in the model pool. We find that the learned tracker obtains comparable results to ours in the Nav-4, but there is an obvious gap in Meta-2, where the target and 2 distractors are controlled by the adversarial meta policies. This shows that the tracker over-fits specific moving pattern of the target, but does not learn the essence of active visual tracking. 2) For teacher-student learning, we directly optimize the visual tracking network by A3C, instead of using the suggestions from the meta tracker. Notably, such method can also be regarded as a method that augments the environment by multi-agent curriculum for SARL method. So, by comparing its result with SARL, we can also recognize the value of the multi-agent curriculum on improving the distraction robustness. For the recurrent attention, we compare it with the previous Conv-LSTM network introduced in . We can see that the recurrent attention mechanism can significantl improve the performance of the tracker in both settings.</p>
<p>Adversarial Testing</p>
<p>Beyond training a tracker, our multi-agent game can also be used as a test bed to further benchmark the distraction robustness of the active trackers. In adversarial testing, the target collaborates with distractors to actively find adversarial trajectories that fail the tracker to follow. Such an adversarial testing is necessary for AVT. Because the trajectories generated by rule-based moving objects designed for evaluation can never cover all of the possible cases, i.e.the trajectories of objects can be arbitrary and have infinitely possible patterns. Moreover, it can also help us discover and understand the weakness of the learned tracker, thus facilitating further development.</p>
<p>We conduct the adversarial testing by training the adversaries to find model-specific adversarial trajectories for each tracker. In this stage, the network of target and distractors are initialized with parameters from the meta polices. For a fair comparison, We iterate the adversaries in 100K inter- action samples. The model of the tracker is frozen during the adversarial testing. The adversarial testing is conduct in Simple Room with 2 distractors. The curves are the average of three runs with different random seeds, and the shaded areas are the standard errors of the mean. Fig. 8 plots the reward of four trackers during the adversarial testing.</p>
<p>We find that most of the trackers are vulnerable to adversaries, resulting in a fast descending of the model's reward.</p>
<p>The rewards of all the methods drop during the testing, showing that the adversaries are learning a more competitive behaviour. The reward of our tracker leads the baseline methods most of the time. In the end, DiMP and ATOM are struggling in the adversarial case, getting very low rewards, ranging from −60 to −100.</p>
<p>Besides, we also observe an interesting but difficult case.</p>
<p>The target rotates at a location and the distractors move around the target and occlude the target; After a while, the distractor goes away, and the two-stage trackers will follow the distractor instead of the target. The demo sequences are available in the demo video. The adversarial testing provides a new evidence to the robustness of our tracker. It also reflects the effectiveness of our method in learning target-distractor collaboration.</p>
<p>Transferring to Unseen Environments</p>
<p>To show the potential of our model in realistic scenarios, we validate the transferability of the learned model in two photo-realistic environments, which are distinct from the training environment.</p>
<p>As the complexity of the environment increases, performance of these models is downgraded comparing to the results in Simple Room, shown as Fig. 9. Even so, our tracker still significantly outperforms others, showing the stronger transferability of our model. In particular, in Parking Lot where the target and distractor have the same appearance, the tracker must be able to consider the spatial-temporal  consistency to identify the target. Correspondingly, trackers which mainly rely on the difference in appearance is not capable of perceiving the consistency, there they should not perform well. In contrast, our tracker performs well in such cases, from which we can infer that our tracker is able to learn spatial-temporal consistent representation that can be very useful when transferring to other environments. Two typical sequences are shown in Fig. 10.</p>
<p>Conclusion and Discussion</p>
<p>Distracting objects are notorious for degrading the tracking performance. This paper offers a novel perspective on how to effectively train a distraction-robust active visual tracker, which is a problem setting that has barely been addressed in previous work. We propose a novel multi-agent game for learning and testing. Several practical techniques are introduced to further improve the learning efficiency, including designing reward function, two-stage teacher-student learning strategy, recurrent attention mechanism etc.. Empirical results on 3D environments verified that the learned tracker is more robust than baselines in the presence of distractors.</p>
<p>Considering a clear gap between our trackers and the ideal one (ours vs teacher model), there are many interesting future directions to be explored beyond our work. For example, we can further explore a more suitable deep neural network for the visual tracker. For real-world deployment, it is also necessary to seek an unsupervised or self-supervised domain adaption method (Hansen et al., 2021) to improve the adaptive ability of the tracker on novel scenarios. Besides, it is also feasible to extend our game on other settings or tasks, such as multi-camera object tracking (Li et al., 2020), target coverage problem , and moving object grasping (Fang et al., 2019). </p>
<p>A.1. State Pre-Processing</p>
<p>To make the neural network better use the grounded state, we pre-process ρ and θ at first. We transform the global state into the entity-centric coordinate system for each agent. To be specific, the input of agent i is a sequence about the relative poses between each agent, represented as P i,1 , P i,2 , ...P i,n , where n is the number of the agents and P i.j = (ρ i,j , cos(θ i,j ), sin(θ i,j ), cos(φ i,j ), sin(φ i,j ).</p>
<p>Note that (ρ i,j , θ i,j , φ i,j ) indicates the relative distance, angle, and relative orientation from agent i to agent j. Agent i is at the origin of the coordination.</p>
<p>A.2. Network Architecture</p>
<p>Since the number of the distractors is randomized during either training or testing, the length of the input sequence would be different across each episode. Thus, we adopt the Bidirectional-Gated Recurrent Unit (Bi-GRU) to pindown a fixed-length feature vector, to enable the network to handle the variable-length distractors. Inspired by the tracker-award model introduced in AD-VAT , we additionally fed the tracker's action a 1 to the target and distractors, to induce a stronger adversarial policy. The network architectures are shown in Fig. 11. Specifically, Bi-GRU(64) indicates that the size of the hidden state in Bi-GRU (Bi-directional Gated Recurrent Unit) is 64. Consequently, the size of the output of Bi-GRU is 128, two times of the hidden state. LSTM(128) indicates that the size of the hidden states in the LSTM (Long-Short Term Memofy) are 128. Following the LSTM, two branches of FC (Fully Connected Network) correspond to Actor(FC (7)) and Critic(FC (1)). The FC (7) indicates a single-layer FC with 7 dimension output, which correspond to the 7-dim discrete action. The FC(1) indicates a single-layer FC with 1 dimension output, which correspond to the value function. The network parameters among the three agents are independent but shared among distractors.</p>
<p>A.3. Training Details</p>
<p>We adopt A3C (Griffis) to train the agent. We find that the reward normalization techniques can make the multi-agent learning process more efficient and steady. To improve</p>
<p>)</p>
<p>Bi−GRU (64) FC (7) LSTM (128) FC(1) FC (7) LSTM (128) FC(1)
P 1,1 P i,n P 1,2 … … ( ) ) ( )</p>
<p>Concatenate</p>
<p>Bi−GRU(64) Figure 11. The network architectures for the meta policies. The left is the network for the tracker. The right is the network for the target and distractors.  Table. 2.
P i,1 P i,n P i,2 … … 1
After that, we further finetune the meta tracker by playing against the saved models. Specifically, we randomly load the parameters from the model pool into the target and distractors networks at the beginning of each episode, and only the tracker network is optimized by RL during training. The details of the hyper-parameters are introduced in Table. 3.  Figure 12. The network architecture for active visual tracking. C1×1-16S1 indicates a convolutional layer of 16 filters of size 1×1 and stride 1.</p>
<p>B. Learning Active Visual Tracker</p>
<p>In this section, we introduce the implementation details of the active visual tracker.</p>
<p>B.1. Network Architecture</p>
<p>The network for visual tracker is based on the Conv-LSTM network introduced in (Luo et al., 2020). The main difference to previous work Luo et al., 2020) is that we additionally adopt a recurrent attention branch to enhance the response of the target in the feature map, shown as Fig. 12. In details, we employ the ConvLSTM (Xingjian et al., 2015) to memorize the appearance information of the target by updating its cell c t and hidden states h t . Comparing to the conventional LSTM network, it can preserves the target's spatial structure, which is useful for localizing the target in the recent frame. Note that in the recurrent attention mechanism, Conv layer is followed by a hardsigmoid activation function. As for the overall network, the raw-pixel image observation is encoded by three layers Convolutional Neural Networks (ConvNet). The specification of the ConvNet is given in the following  image is resized to 120 × 120 × 3 RGB image. Each layer is activated by a LeakyReLU function. Layer # 1 and # 2 are followed by a 2 × 2 max-pooling layer with stride 2, additionally. After that, the feature map is multiplied by the attention map. Then we employ a convolutional layer and a LSTM(256) to furhter encode the state representation for the actor and critic. Note that the critic is only used for the ablation (w/o teacher-student learning) that train the networks via reinforcement learning. In our teacher-student learning paradigm, the critic is not necessary at all. As for the w/o recurrent attention, we just remove the recurrent attention branch and directly feed the feature map from ConvNet to the following. The training process is presented in Algorithm. 1.</p>
<p>B.2. Training Details</p>
<p>The details of the hyper-parameters are introduced in Table. 4. The details of the teacher-student learning process are introduced in Algorithm. 1</p>
<p>C. Baselines</p>
<p>In this section, we introduce the details of baselines used in the experiments.</p>
<p>C.1. Two-Stage Methods</p>
<p>In the first stage, we extract the bounding box of the target from image by passive (video-based) visual object tracker. Then, to meet the requirements of active tracking, we build a controller based on the output bounding box additionally.</p>
<p>Visual Tracker. In this paper, we adopt three off-theshelf trackers as our baselines, e.g. DaSiamRPN (Zhu et al., 2018), ATOM (Danelljan et al., 2019), DiMP (Bhat et al., 2019). We implement them based on their official repository. Specifically, DaSiamRPN is from https: //github.com/foolwood/DaSiamRPN, ATOM and DiMP are from https://github.com/visionml/ pytracking Algorithm 1 Teacher-Student Learning Require: meta tracker π * 1 , a model pool for targets Π 2 and distractors Π 3 Initialize: Randomly initialize student policy π 1 , Replay Buffer B % sample interactions, can run with multiple workers. for simulation episode e = 1 to M do sample a target policy π 2 from Π 2 . sample a distractor policy π 3 from Π 3 . for t = 0 to T do obtain visual observation for tracker o 1,t obtain grounded state for each agent s 1,t , s 2,t , ..s n,t Execute actions a 1,t = π 1 (o 1,t ), a 2,t = π 1 (s 2,t ), a 3,t = π 1 (s 3,t ) Compute suggestions for tracker a * 1 = π * 1 (s 1,t ) Store (o 1,t , a * 1 ) in buffer B end for % Asynchronously update student network while e &lt; M do Sample a batch of N sequences from buffer B Train student policy (π 1 (o t )) by optimizing L KL . end while end for Camera Controller. The camera controller outputs actions based on the specific error of the bounding box. The controller choose a discrete action from the action space according to the horizontal error X err and the size error Fig. 13 Intuitively, X err measures how far the bounding box is horizontally away from the desired position. In the case of Fig.  13, X err is negative, meaning that the camera should move left so the object is closer to the center of the image. The distance to the target is reflected by the size of the bounding box S err . Intuitively, we have the following commands, X err &lt; 0 means that the tracker is to the left and needs to turn right (increasing X).
S err = W b × H b − W exp × H exp , shown as
X err &gt; 0 means that the tracker is to the right and needs to turn right (decreasing X). S err &lt; 0 means that the tracker is too far away and needs to move forward and speed up (increasing S).</p>
<p>S err &gt; 0 means that the tracker is too close and needs to move backward or slow down (decreasing S).</p>
<p>Formally, the output signal in the linear velocity V l and angular velocity V a is depended on S err and X err respectively, V l = −P l * S err , V a = −P a * X err . Note that P l &gt; 0 and P a &gt; 0 are two constant, the higher the more sensitive to the error. Note that, to keep the comparison fair, we discretize the output to map it to the discrete action space we used in other methods.</p>
<p>C.2. End-to-End Methods</p>
<p>There are four end-to-end methods (SARL (Luo et al., 2020), AD-VAT , and their variants) are employed as the baselines. We implement them based on the official repository in https://github.com/zfw1226/ active_tracking_rl.</p>
<p>Network Architecture. The network architecture follows the networks used in . Different to original implementation, we modify the network in two aspects: 1) use color image as input, instead of the gray image used in previous to learn a more discriminative representation. 2) add an auxiliary task for tracker, i.e. predict its immediate reward, to speed up the learning of tracker.</p>
<p>Optimization. The network parameters are updated with a shared Adam optimizer. Each agent is trained by A3C (Mnih et al., 2016). 4 workers are running in parallel during training. The hyper-parameters we used is the same as the original version.</p>
<p>D. Environments</p>
<p>In this section, we introduce the details of the training and testing environments used in the experiments.</p>
<p>D.1. Training Environment</p>
<p>We train the agents in the Simple Room. The environment augmentation technique (Luo et al., 2020;Zhong et al., 2019) is employed to randomize the illumination and visual appearance of objects (target, distractors, backgrounds). When learning the meta policies, the number of distractors is randomized, ranging from 0 to 4. But the number is fixed at 2, when learning the visual policies. Environment augmentation can significantly improve the generalization of the visual policy. To produce more challenging and realistic setting, we modify the action space in two ways: 1) increase the max speed of players from 1m/s to 2m/s, 2) add a filter v t = αv t−1 + (1 − α)v t to smooth the discrete action. </p>
<p>D.2. Testing Environment</p>
<p>There are three environments used for testing, i.e.Simple Room, Urban City, and Parking Lot. There are five appearances used in the Simple Room, shown as the top of Fig.15. There are four appearances used in the Urban City, shown as the bottom of Fig.15. Since the dress of the target and distractors are randomly sampled at each episode, two players can be dressed the same. In Parking Lot, all the target and distractors are of the same dress, shown as the bottom of Fig.16. All appearances used by the target and distractors in the testing environments are different from the training environment. Graphics in Fig.16 are examples of the three testing environment.</p>
<p>E. Detailed Results and Demo Videos</p>
<p>In this section, we report detailed results with the Accumulated Reward (AR), Episode Length (EL), and Success Rate (SR), and show demo videos for better understanding.</p>
<p>E.1. Multi-Agent Curriculum</p>
<p>In Video 1, we show the behaviors of the target and distractors at different learning stages when learning in the multi-agent game. Note that the tracker is governed by the learned meta tracker for better visualization. </p>
<p>E.2. Evaluating with Scripted Distractors</p>
<p>See Table.5 for quantitative results.</p>
<p>E.3. Adversarial Testing</p>
<p>In Video 2, we show the emergent target-distractor cooperation after learning to attack DiMP, ATOM, AD-VAT, and Ours.</p>
<p>E.4. Transferring to Realistic Virtual Environments</p>
<p>See Table.6 for quantitative results. More vivid examples for our tracker are available in Video 3. Note that the videos are recorded from the tracker's view.</p>
<p>Figure 1 .
1An extreme situation of active visual tracking with distractors. Can you identify which one is the real target?</p>
<p>Figure 2 .
2An overview of the Cooperative-Competitive Multi-Agent Game, where the distractors and target cooperate to compete against the tracker.</p>
<p>Figure 5 .
5The snapshots of tracker's visual observation in Simple Room. The right is the augmented training environment. The target is pointed out with a bounding box.</p>
<p>Figure 6 .
6The evolution of target-distractor cooperation in the multi-agent game.</p>
<p>Figure 7 .
7Evaluating the distraction-robustness of the trackers by increasing the number of random distractors in Simple Room</p>
<p>Figure 8 .
8The reward curves of four trackers during the adversarial testing, running with three random seeds. Better viewed in color.</p>
<p>Figure 9 .
9Evaluating generalization of the tracker on two unseen environments (Urban City and Parking Lot).</p>
<p>Figure 10 .
10Two exemplar sequences of our tracker running on the Urban City (top) and Parking Lot (bottom). For better understanding, we point out the target object with a red line bounding box in the first frame. More examples are available in the demo video.</p>
<p>Figure 13 .
13An example to illustrate errors.</p>
<p>Figure 14 .
14Three snapshots of the training environment. The left is captured from the bird view. The right is the visual observation for the tracker. The target is noted by the red bounding box in the tracker's view.</p>
<p>Figure 15 .
15Top: Five targets appear in Simple Room. The one on the leftmost is also used in Parking Lot. Bottom: Four candidate appearances used for target and distractors in Urban City, including two man and two woman.</p>
<p>1 Center on Frontiers of Computing Studies, Dept. of Computer Science, Peking University, Beijing, P.R. China. 2 Tencent Robotics X, Shenzhen, P.R. China 3 Tencent, Shenzhen, P.R. China 4 Adv. Inst. of Info. Tech, Peking University, Hangzhou, P.R. China.. Correspondence to: Fangwei Zhong <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#122;&#102;&#119;&#49;&#50;&#50;&#54;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#122;&#102;&#119;&#49;&#50;&#50;&#54;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>, Yizhou Wang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#121;&#105;&#122;&#104;&#111;&#117;&#46;&#87;&#97;&#110;&#103;&#64;&#112;&#107;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#121;&#105;&#122;&#104;&#111;&#117;&#46;&#87;&#97;&#110;&#103;&#64;&#112;&#107;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).Template 
Visual Observation </p>
<p>Table 1 .
1Ablative analysis of the visual policy learning method in Simple Room. The best results are shown in bold.Methods 
Nav-4 
Meta-2 
AR EL 
SR 
AR EL 
SR 
Ours 
250 401 0.54 141 396 0.44 
w/o multi-agent curriculum 232 394 0.53 -23 283 0.08 
w/o teacher-student learning 76 290 0.22 79 340 0.4 
w/o recurrent attention 
128 193 0.27 75 331 0.32 </p>
<p>Table 2 .
2Hyper-parameters for learning meta policiesHyper-parameters </p>
<h1></h1>
<p>total steps 
2M 
episode length 
500 
discount factor 
0.9 
entropy weight for tracker 
0.01 
entropy weight for target and distractor 
0.03 
number of forward steps in A3C 
20 
learning rate 
1e-3 
number of workers 
4 
optimizer 
Adam </p>
<p>the generalization of the policy for different numbers of 
distractors, we randomly set the number of distractors in 
the range of 0 to 4 for each episode. The details of the 
hyper-parameters are introduced in </p>
<p>Table 3 .
3Hyper-parameters for finetuning meta trackerHyper-parameters </p>
<h1></h1>
<p>total steps 
0.5M 
episode length 
500 
discount factor 
0.9 
entropy weight for tracker 
0.01 
entropy weight for target and distractor 
0.03 
number of forward steps in A3C 
20 
learning rate 
5e-4 
number of workers 
4 
optimizer 
Adam </p>
<p>ConvLSTM </p>
<p>Attention 
Map </p>
<p>ConvNet </p>
<p>Conv </p>
<p>Recurrent Attention Mechanism </p>
<p>LSTM(256) 
FC(7) 
Actor </p>
<p>Image </p>
<p>FC(1) </p>
<p>Critic </p>
<p>Conv </p>
<p>C5X5-128S1 </p>
<p>Conv </p>
<p>C1X1-16S1 </p>
<p>Conv </p>
<p>C1X1-16S1 </p>
<p>Sigmoid </p>
<p>Repeat </p>
<p>where C5×5-32S2 means 32 filters of size 5×5 and stride 2, MP 2S2 indicates max pool of square window of size=2, stride=2. The ConvNet are learned from scratch. The inputtable: </p>
<p>Layer# 
1 
2 
3 
4 
5 
ConvNet 
C5×5-32S2 
MP 2S2 
C5×5-64S1 
MP 2S2 
C3×3-128S1 </p>
<p>Table 4 .
4Hyper-parameters for our teacher-student learning method.Hyper-parameters </p>
<h1></h1>
<p>total steps 
2M 
max episode length 
500 
buffer size 
500 episodes 
batch size 
8 
number of forward steps 
30 
learning rate 
1e-4 
number of samplers 
4 
optimizer 
Adam </p>
<p>Appendix A. Learning Meta PoliciesIn this section, we introduce the implementation details of the meta policies.
. Accessed August. 30https://blog.openai.com/ openai-five/. Accessed August 30, 2018.</p>
<p>Learning dexterous in-hand manipulation. O M Andrychowicz, B Baker, M Chociej, R Jozefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, The International Journal of Robotics Research. 391Andrychowicz, O. M., Baker, B., Chociej, M., Jozefowicz, R., McGrew, B., Pachocki, J., Petron, A., Plappert, M., Powell, G., Ray, A., et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020.</p>
<p>Visual tracking with online multiple instance learning. B Babenko, M.-H Yang, S Belongie, The IEEE Conference on Computer Vision and Pattern Recognition. Babenko, B., Yang, M.-H., and Belongie, S. Visual tracking with online multiple instance learning. In The IEEE Conference on Computer Vision and Pattern Recognition, pp. 983-990, 2009.</p>
<p>Emergent tool use from multi-agent autocurricula. B Baker, I Kanitscheider, T Markov, Y Wu, G Powell, B Mcgrew, I Mordatch, International Conference on Learning Representations. Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., and Mordatch, I. Emergent tool use from multi-agent autocurricula. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=SkxpxJBKwS.</p>
<p>Learning discriminative model prediction for tracking. G Bhat, M Danelljan, L V Gool, R Timofte, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionBhat, G., Danelljan, M., Gool, L. V., and Timofte, R. Learn- ing discriminative model prediction for tracking. In Pro- ceedings of the IEEE International Conference on Com- puter Vision, pp. 6182-6191, 2019.</p>
<p>Multi-robot inverse reinforcement learning under occlusion with interactions. K Bogert, P Doshi, Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS). the 2014 International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)Bogert, K. and Doshi, P. Multi-robot inverse reinforcement learning under occlusion with interactions. In Proceed- ings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 173-180, 2014.</p>
<p>Visual object tracking using adaptive correlation filters. D S Bolme, J R Beveridge, B A Draper, Y M Lui, The IEEE Conference on Computer Vision and Pattern Recognition. Bolme, D. S., Beveridge, J. R., Draper, B. A., and Lui, Y. M. Visual object tracking using adaptive correlation filters. In The IEEE Conference on Computer Vision and Pattern Recognition, pp. 2544-2550, 2010.</p>
<p>Atom: Accurate tracking by overlap maximization. M Danelljan, G Bhat, F S Khan, M Felsberg, Proceed. eedDanelljan, M., Bhat, G., Khan, F. S., and Felsberg, M. Atom: Accurate tracking by overlap maximization. In Proceed-</p>
<p>Selfsupervised policy adaptation during deployment. N Hansen, R Jangir, Y Sun, G Alenyà, P Abbeel, A A Efros, L Pinto, Wang , X , International Conference on Learning Representations. Hansen, N., Jangir, R., Sun, Y., Alenyà, G., Abbeel, P., Efros, A. A., Pinto, L., and Wang, X. Self- supervised policy adaptation during deployment. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=o_V-MjyyGV_.</p>
<p>S Hare, S Golodetz, A Saffari, V Vineet, M.-M Cheng, S L Hicks, Torr , P H Struck, Structured output tracking with kernels. IEEE Transactions on Pattern Analysis and Machine Intelligence. 38Hare, S., Golodetz, S., Saffari, A., Vineet, V., Cheng, M.-M., Hicks, S. L., and Torr, P. H. Struck: Structured output tracking with kernels. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(10):2096-2109, 2015.</p>
<p>Deep reinforcement learning in parameterized action space. M Hausknecht, P Stone, arXiv:1511.04143arXiv preprintHausknecht, M. and Stone, P. Deep reinforcement learn- ing in parameterized action space. arXiv preprint arXiv:1511.04143, 2015.</p>
<p>S Huang, N Papernot, I Goodfellow, Y Duan, Abbeel , P , arXiv:1702.02284Adversarial attacks on neural network policies. arXiv preprintHuang, S., Papernot, N., Goodfellow, I., Duan, Y., and Abbeel, P. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017.</p>
<p>Tracking-learningdetection. Z Kalal, K Mikolajczyk, J Matas, IEEE Transactions on Pattern Analysis and Machine Intelligence. 347Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(7):1409-1422, 2012.</p>
<p>A novel performance evaluation methodology for single-target trackers. M Kristan, J Matas, A Leonardis, T Vojir, R Pflugfelder, G Fernandez, G Nebehay, F Porikli, L Andčehovin, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3811Kristan, M., Matas, J., Leonardis, A., Vojir, T., Pflugfelder, R., Fernandez, G., Nebehay, G., Porikli, F., andČehovin, L. A novel performance evaluation methodology for single-target trackers. IEEE Transactions on Pattern Anal- ysis and Machine Intelligence, 38(11):2137-2155, Nov 2016.</p>
<p>Coordinated multi-agent imitation learning. H M Le, Y Yue, P Carr, P Lucey, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Le, H. M., Yue, Y., Carr, P., and Lucey, P. Coordinated multi-agent imitation learning. In Proceedings of the 34th International Conference on Machine Learning, vol- ume 70, pp. 1995-2003, 2017.</p>
<p>Pose-assisted multi-camera collaboration for active object tracking. J Li, J Xu, F Zhong, X Kong, Y Qiao, Wang , Y , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Li, J., Xu, J., Zhong, F., Kong, X., Qiao, Y., and Wang, Y. Pose-assisted multi-camera collaboration for active object tracking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 759-766, 2020.</p>
<p>Multi-agent inverse reinforcement learning for zero-sum games. X Lin, P A Beling, R Cogill, arXiv:1403.6508arXiv preprintLin, X., Beling, P. A., and Cogill, R. Multi-agent inverse re- inforcement learning for zero-sum games. arXiv preprint arXiv:1403.6508, 2014.</p>
<p>Markov games as a framework for multiagent reinforcement learning. M L Littman, Machine Learning Proceedings. ElsevierLittman, M. L. Markov games as a framework for multi- agent reinforcement learning. In Machine Learning Pro- ceedings 1994, pp. 157-163. Elsevier, 1994.</p>
<p>Multi-agent actor-critic for mixed cooperative-competitive environments. R Lowe, Y Wu, A Tamar, J Harb, O P Abbeel, I Mordatch, Advances in Neural Information Processing Systems. Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6379-6390, 2017.</p>
<p>End-to-end active object tracking via reinforcement learning. W Luo, P Sun, F Zhong, W Liu, T Zhang, Wang , Y , International Conference on Machine Learning. Luo, W., Sun, P., Zhong, F., Liu, W., Zhang, T., and Wang, Y. End-to-end active object tracking via reinforcement learn- ing. In International Conference on Machine Learning, pp. 3286-3295, 2018.</p>
<p>End-to-end active object tracking and its real-world deployment via reinforcement learning. IEEE transactions on pattern analysis and machine intelligence. W Luo, P Sun, F Zhong, W Liu, T Zhang, Wang , Y , 42Luo, W., Sun, P., Zhong, F., Liu, W., Zhang, T., and Wang, Y. End-to-end active object tracking and its real-world deployment via reinforcement learning. IEEE transac- tions on pattern analysis and machine intelligence, 42(6): 1317-1332, 2019.</p>
<p>End-to-end active object tracking and its real-world deployment via reinforcement learning. W Luo, P Sun, F Zhong, W Liu, T Zhang, Wang , Y , IEEE Transactions on Pattern Analysis and Machine Intelligence. 426Luo, W., Sun, P., Zhong, F., Liu, W., Zhang, T., and Wang, Y. End-to-end active object tracking and its real-world deployment via reinforcement learning. IEEE Transac- tions on Pattern Analysis and Machine Intelligence, 42 (6):1317-1332, 2020.</p>
<p>Adversarially robust policy learning: Active construction of physically-plausible perturbations. A Mandlekar, Y Zhu, A Garg, L Fei-Fei, S Savarese, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEMandlekar, A., Zhu, Y., Garg, A., Fei-Fei, L., and Savarese, S. Adversarially robust policy learning: Active con- struction of physically-plausible perturbations. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3932-3939. IEEE, 2017.</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje- land, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540): 529-533, 2015.</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International Conference on Machine Learning. Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn- chronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928- 1937, 2016.</p>
<p>Learning multi-domain convolutional neural networks for visual tracking. H Nam, B Han, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionNam, H. and Han, B. Learning multi-domain convolutional neural networks for visual tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4293-4302, 2016.</p>
<p>Asymmetric actor critic for image-based robot learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, Abbeel , P , arXiv:1710.06542arXiv preprintPinto, L., Andrychowicz, M., Welinder, P., Zaremba, W., and Abbeel, P. Asymmetric actor critic for image-based robot learning. arXiv preprint arXiv:1710.06542, 2017a.</p>
<p>Robust adversarial reinforcement learning. L Pinto, J Davidson, R Sukthankar, A Gupta, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A. Ro- bust adversarial reinforcement learning. In Proceedings of the 34th International Conference on Machine Learn- ing, volume 70, pp. 2817-2826. JMLR.org, 2017b.</p>
<p>Unrealcv: Virtual worlds for computer vision. W Qiu, F Zhong, Y Zhang, S Qiao, Z Xiao, T S Kim, Y Wang, Yuille , A , Proceedings of the 2017 ACM on Multimedia Conference. the 2017 ACM on Multimedia ConferenceQiu, W., Zhong, F., Zhang, Y., Qiao, S., Xiao, Z., Kim, T. S., Wang, Y., and Yuille, A. Unrealcv: Virtual worlds for computer vision. In Proceedings of the 2017 ACM on Multimedia Conference, pp. 1221-1224, 2017.</p>
<p>Monotonic value function factorisation for deep multi-agent reinforcement learning. T Rashid, M Samvelyan, C Schroeder, G Farquhar, J Foerster, S Whiteson, Qmix, International Conference on Machine Learning. PMLRRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and Whiteson, S. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learn- ing, pp. 4295-4304. PMLR, 2018.</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. the Fourteenth International Conference on Artificial Intelligence and StatisticsRoss, S., Gordon, G., and Bagnell, D. A reduction of imita- tion learning and structured prediction to no-regret online learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 627-635, 2011.</p>
<p>Multi-agent generative adversarial imitation learning. J Song, H Ren, D Sadigh, S Ermon, Advances in Neural Information Processing Systems. Song, J., Ren, H., Sadigh, D., and Ermon, S. Multi-agent generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 7461-7472, 2018.</p>
<p>Inverse reinforcement learning in swarm systems. A Sošić, W R Khudabukhsh, A M Zoubir, H Koeppl, arXiv:1602.05450arXiv preprintSošić, A., KhudaBukhsh, W. R., Zoubir, A. M., and Koeppl, H. Inverse reinforcement learning in swarm systems. arXiv preprint arXiv:1602.05450, 2016.</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. S Sukhbaatar, Z Lin, I Kostrikov, G Synnaeve, A Szlam, Fergus , R , In International Conference on Learning Representations. 5Sukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic cur- ricula via asymmetric self-play. In International Confer- ence on Learning Representations, 2018. URL https: //openreview.net/forum?id=SkT5Yg-RZ.</p>
<p>Value-decomposition networks for cooperative multi-agent learning based on team reward. P Sunehag, G Lever, A Gruslys, W M Czarnecki, V Zambaldi, M Jaderberg, M Lanctot, N Sonnerat, J Z Leibo, K Tuyls, Proceedings of the 17th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS). the 17th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zam- baldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 2085-2087, 2018.</p>
<p>Multiagent cooperation and competition with deep reinforcement learning. A Tampuu, T Matiisen, D Kodelja, I Kuzovkin, K Korjus, J Aru, J Aru, Vicente , R , PloS one. 124172395Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru, J., and Vicente, R. Multiagent cooper- ation and competition with deep reinforcement learning. PloS one, 12(4):e0172395, 2017.</p>
<p>Learning to manipulate object collections using grounded state representations. M Wilson, T Hermans, Conference on Robot Learning. Wilson, M. and Hermans, T. Learning to manipulate ob- ject collections using grounded state representations. In Conference on Robot Learning, pp. 490-502, 2020.</p>
<p>Convolutional lstm network: A machine learning approach for precipitation nowcasting. S Xingjian, Z Chen, H Wang, D.-Y Yeung, W.-K Wong, W Woo, Advances in Neural Information Processing Systems. Xingjian, S., Chen, Z., Wang, H., Yeung, D.-Y., Wong, W.-K., and Woo, W.-c. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In Advances in Neural Information Processing Systems, pp. 802-810, 2015.</p>
<p>Learning multi-agent coordination for enhancing target coverage in directional sensor networks. J Xu, F Zhong, Wang , Y , Advances in Neural Information Processing Systems. 33Xu, J., Zhong, F., and Wang, Y. Learning multi-agent coordi- nation for enhancing target coverage in directional sensor networks. In Advances in Neural Information Processing Systems, volume 33, pp. 10053-10064, 2020.</p>
<p>AD-VAT: An asymmetric dueling mechanism for learning visual active tracking. F Zhong, P Sun, W Luo, T Yan, Wang , Y , International Conference on Learning Representations. Zhong, F., Sun, P., Luo, W., Yan, T., and Wang, Y. AD-VAT: An asymmetric dueling mechanism for learning visual active tracking. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=HkgYmhR9KX.</p>
<p>Ad-vat+: An asymmetric dueling mechanism for learning and understanding visual active tracking. F Zhong, P Sun, W Luo, T Yan, Wang , Y , IEEE Transactions on Pattern Analysis and Machine Intelligence. 435Zhong, F., Sun, P., Luo, W., Yan, T., and Wang, Y. Ad- vat+: An asymmetric dueling mechanism for learning and understanding visual active tracking. IEEE Transac- tions on Pattern Analysis and Machine Intelligence, 43 (5):1467-1482, 2021.</p>
<p>Distractor-aware siamese networks for visual object tracking. Z Zhu, Q Wang, B Li, W Wu, J Yan, W Hu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Zhu, Z., Wang, Q., Li, B., Wu, W., Yan, J., and Hu, W. Distractor-aware siamese networks for visual object track- ing. In Proceedings of the European Conference on Com- puter Vision (ECCV), pp. 101-117, 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
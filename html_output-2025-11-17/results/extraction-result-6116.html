<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6116 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6116</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6116</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-261557055</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.02726v3.pdf" target="_blank">Large Language Models for Automated Open-domain Scientific Hypotheses Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging. In this work, we tackle these problems by proposing the first dataset for social science academic hypotheses discovery, with the final goal to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus. Unlike previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity. A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation. To the best of our knowledge, this is the first work showing that LLMs are able to generate novel (''not existing in literature'') and valid (''reflecting reality'') scientific hypotheses.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6116.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6116.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-based Automatic Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of GPT-4 as an automated rater to score LLM-generated research hypotheses on predefined aspects (validness, novelty, helpfulness) with deterministic settings to provide large-scale automatic evaluation and benchmark comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated scoring by GPT-4 using prompts that ask GPT-4 to rate hypotheses on three aspects (validness, novelty, helpfulness) on a 5-point Likert scale; GPT-4 runs with temperature=0.0 and top_p=0.9 for stable scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Validness (reflects reality), Novelty (not existing in literature), Helpfulness (useful to researchers); each on 5-point Likert scales with written rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (as evaluator); gpt-3.5-turbo and Claude3-Opus used as generators in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Social sciences / Business (TOMATO task)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated social-science research hypotheses produced from open-domain raw web corpus using the MOOSE framework or baseline LLM prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4 evaluation was applied at scale (≈1000 hypotheses per MOOSE-related method in GPT-4 tables). Reported average 5-point scores (examples from Table 2): Baseline (Validness 3.954, Novelty 2.483, Helpfulness 3.489); MOOSE-base (3.907, 3.081, 3.859); MOOSE-base + future-feedback (3.955, 3.226, 3.953); MOOSE-base + future+past (3.916, 3.390, 3.931). Present-feedback iterations increased scores (validness and novelty rose with more iterations; helpfulness peaked around 3 iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>TOMATO dataset (50 recent social-science papers + associated raw web corpus; 14 related survey papers available to novelty detector).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Authors report high soft consistency between GPT-4 and human experts (soft consistency scores: Validness 0.85, Novelty 0.823, Helpfulness 0.773; hard consistency lower), indicating GPT-4 produces evaluations that correlate well with expert ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>GPT-4 may grade validness based on textual frequency/exposure rather than true world understanding, potentially underrating highly novel but plausible hypotheses; reliance on a single LLM evaluator can inherit evaluator biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6116.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6116.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Expert (PhD students) Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Blind human evaluation by three social-science PhD students who scored sampled hypotheses on validness, novelty, and helpfulness using detailed 5-point rubric; used as the primary human benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human experts independently score hypotheses on 5-point Likert scales for Validness, Novelty, and Helpfulness using written standards; experts blinded to method, randomized order; 400 hypotheses sampled for evaluation (detailed selection scheme in A.6).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same three criteria as GPT-4: Validness (degree hypothesis reflects reality), Novelty (degree of novelty relative to literature), Helpfulness (degree to which the hypothesis can assist researchers), each with explicit 5-point descriptions (see A.9–A.15).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>N/A (human evaluators); they evaluated outputs produced mainly by gpt-3.5-turbo based MOOSE and baselines; additionally Claude3-Opus used in ablation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Social sciences / Business research hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Human assessment of LLM-generated research hypotheses produced by MOOSE variants and an LLM baseline given raw web corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Expert-averaged scores reported (examples from Table 4): Baseline (Validness 3.579, Novelty 2.276, Helpfulness 2.632); MOOSE-base (3.500, 2.855, 3.026); MOOSE-base + future-feedback (3.645, 3.105, 3.303); MOOSE-base + future+past (3.750, 3.197, 3.368). Present-feedback iterations generally increased expert scores up to an optimal number.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluations performed on subsets drawn from TOMATO-generated hypotheses; total of 400 hypotheses evaluated (one expert evaluated all 400, two others evaluated 104 each), groups of 8 hypotheses per background compared across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>This is the human standard; GPT-4 automated scores were compared to these expert scores showing high soft consistency (indicating GPT-4 could be a reliable proxy). Experts found many MOOSE outputs to be novel and valid.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited pool of experts (three marketing/social-science PhD students) — domain coverage concentrated in Marketing; evaluation cost and time limit number of human-labeled examples; inter-rater (hard) consistency not perfect (hard consistency scores reported), although soft consistency high.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6116.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6116.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation metrics (V/N/H)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Validness / Novelty / Helpfulness 5-point Rubrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Triad of evaluation criteria (Validness, Novelty, Helpfulness) formally defined with 5-point descriptive rubrics and used both for automated (GPT-4) and human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Scoring each hypothesis on three aspects using explicit 5-point scales with anchor descriptions (tables A.9–A.15). Used by GPT-4 and the human experts for consistent evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Validness: how well hypothesis reflects reality (5=completely reflects reality; 1=completely violates). Novelty: degree of novelty relative to literature (5=completely novel; 1=not novel). Helpfulness: utility for researchers (5=ready for publication w/o changes; 1=not helpful).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used to evaluate outputs from gpt-3.5-turbo and Claude3-Opus generators; GPT-4 used as an automated rater implementing these rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Social sciences (generalizable rubric for open-domain hypothesis induction)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>These metrics operationalize the philosophical requirements for hypothetical induction (validity/reality and novelty) and add helpfulness to capture assistive utility to researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Metrics used to produce all quantitative tables; authors emphasize novelty is relatively more important for this task. They report metric-specific trends (e.g., MOOSE increases novelty and helpfulness vs baseline; small validness trade-off).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to hypotheses generated from the TOMATO dataset (50 papers) and 400 expert-evaluated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Rubrics designed to align human and GPT-4 evaluations; results show high soft agreement between GPT-4 and human scores using these rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Rubrics do not explicitly include relevance/significance or conventional NLP automatic metrics (BLEU/ROUGE/METEOR) which authors argue are unsuitable for open-ended novel hypothesis generation; possible trade-offs between novelty and validness complicate optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6116.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6116.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TOMATO benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TOmated open-doMAin hypoThetical inductiOn (TOMATO) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark dataset created for open-domain automated hypothesis induction in social sciences: 50 recent social-science papers (post-Jan 2023) with assembled raw web corpora, background/inspiration passages, survey links, and annotation of reasoning chains and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Used as the testbed/benchmark for generating hypotheses: methods must only use provided raw web corpus to generate hypotheses; evaluation uses GPT-4 and human expert scoring on Validness/Novelty/Helpfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same triad (Validness, Novelty, Helpfulness); additional internal dataset metadata includes reasoning complexity labels for each example.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Generators evaluated on TOMATO include gpt-3.5-turbo (primary), Claude3-Opus (ablation), GPT-4 used only as evaluator (temperature 0.0).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Social sciences / Business research hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Each example pairs a target main hypothesis (from a recent paper) with a constructed raw web corpus (news, Wikipedia, reviews) intended to simulate real-world open-domain evidence from which a system must induce a novel hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Authors use TOMATO to show MOOSE can produce hypotheses judged novel and valid by experts; numerical comparisons (see Tables 2–5) show MOOSE variants outperform a baseline on novelty/helpfulness with varying validness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>The TOMATO dataset itself is the benchmark: 50 papers, raw web passages (background and inspirations), 14 related surveys, annotated reasoning chains; 400 hypotheses sampled for expert evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Ground-truth hypotheses from the original papers are included as a human benchmark; they score highest in novelty and helpfulness in retrieval ablations (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relatively small (50 papers) though richly annotated; domain concentrated in social sciences/business; constructing semantically similar web passages required human curation to avoid data contamination; not trivially expandable without human effort for ground-truth annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6116.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6116.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline vs MOOSE eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline LLM chunk-to-hypothesis vs MOOSE multi-module evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparative evaluation of a simple baseline that inputs a corpus chunk to an LLM to produce a hypothesis versus the MOOSE multi-module framework (with present/past/future feedback mechanisms) across the TOMATO benchmark using GPT-4 and expert scorers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Direct comparison of generated hypothesis sets from baseline and MOOSE variants using GPT-4 automated scoring (large-scale) and human expert scoring (sampled hypotheses). Statistical averages reported in tables for Validness/Novelty/Helpfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary criteria are Validness, Novelty, Helpfulness on 5-point scales; additional analyses consider retrieval methods, feedback ablations, base model choice (gpt-3.5 vs Claude3-Opus), and present/past/future feedback iteration effects.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Generators: gpt-3.5-turbo (primary); Claude3-Opus (ablation). Evaluator: GPT-4 and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Social sciences (hypothesis induction)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Hypotheses produced by baseline single-step prompting vs. MOOSE which has modules: Background Finder, Inspiration Title Finder, Inspiration Finder, Hypothesis Suggestor, Hypothesis Proposer, and three feedback channels (present, past, future).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Key numerical findings: GPT-4 evaluation (Table 2) shows MOOSE-base improves Novelty (3.081 vs baseline 2.483) and Helpfulness (3.859 vs 3.489) while Validness slightly lower; adding future-feedback and past-feedback further increases Novelty (up to 3.390) and Helpfulness. Expert evaluation (Table 4) shows similar trends with MOOSE-base and feedback variants outperforming baseline on Novelty and Helpfulness (example: baseline novelty 2.276 vs MOOSE w/ future+past 3.197). Present-feedback iterations further improve scores until an empirical optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated on TOMATO dataset outputs; GPT-4 evaluation used ≈1000 hypotheses per method; expert evaluation used 50 hypotheses per method (400 total sampled).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human expert ratings corroborate GPT-4 automated comparisons; authors report high soft consistency between GPT-4 and experts, though GPT-4 may underrate novelty's validness. MOOSE outputs were judged by experts to include many hypotheses considered novel and valid that were not present in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Trade-off observed between novelty and validness (increasing novelty sometimes reduces validness); past-feedback implementation relied on heuristics because ChatGPT generated weak past-feedback; base model choice affects absolute scores though relative improvements remain; optimal number of present-feedback iterations must be tuned empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 2)</em></li>
                <li>Goal driven discovery of distributional differences via language descriptions <em>(Rating: 1)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6116",
    "paper_id": "paper-261557055",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "GPT-4 evaluator",
            "name_full": "GPT-4-based Automatic Evaluation",
            "brief_description": "Use of GPT-4 as an automated rater to score LLM-generated research hypotheses on predefined aspects (validness, novelty, helpfulness) with deterministic settings to provide large-scale automatic evaluation and benchmark comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Automated scoring by GPT-4 using prompts that ask GPT-4 to rate hypotheses on three aspects (validness, novelty, helpfulness) on a 5-point Likert scale; GPT-4 runs with temperature=0.0 and top_p=0.9 for stable scoring.",
            "evaluation_criteria": "Validness (reflects reality), Novelty (not existing in literature), Helpfulness (useful to researchers); each on 5-point Likert scales with written rubrics.",
            "llm_model_name": "GPT-4 (as evaluator); gpt-3.5-turbo and Claude3-Opus used as generators in experiments",
            "theory_domain": "Social sciences / Business (TOMATO task)",
            "theory_description": "LLM-generated social-science research hypotheses produced from open-domain raw web corpus using the MOOSE framework or baseline LLM prompting.",
            "evaluation_results": "GPT-4 evaluation was applied at scale (≈1000 hypotheses per MOOSE-related method in GPT-4 tables). Reported average 5-point scores (examples from Table 2): Baseline (Validness 3.954, Novelty 2.483, Helpfulness 3.489); MOOSE-base (3.907, 3.081, 3.859); MOOSE-base + future-feedback (3.955, 3.226, 3.953); MOOSE-base + future+past (3.916, 3.390, 3.931). Present-feedback iterations increased scores (validness and novelty rose with more iterations; helpfulness peaked around 3 iterations).",
            "benchmarks_or_datasets": "TOMATO dataset (50 recent social-science papers + associated raw web corpus; 14 related survey papers available to novelty detector).",
            "comparison_to_human": "Authors report high soft consistency between GPT-4 and human experts (soft consistency scores: Validness 0.85, Novelty 0.823, Helpfulness 0.773; hard consistency lower), indicating GPT-4 produces evaluations that correlate well with expert ratings.",
            "limitations_or_challenges": "GPT-4 may grade validness based on textual frequency/exposure rather than true world understanding, potentially underrating highly novel but plausible hypotheses; reliance on a single LLM evaluator can inherit evaluator biases.",
            "uuid": "e6116.0",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Expert evaluation",
            "name_full": "Human Expert (PhD students) Evaluation",
            "brief_description": "Blind human evaluation by three social-science PhD students who scored sampled hypotheses on validness, novelty, and helpfulness using detailed 5-point rubric; used as the primary human benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Human experts independently score hypotheses on 5-point Likert scales for Validness, Novelty, and Helpfulness using written standards; experts blinded to method, randomized order; 400 hypotheses sampled for evaluation (detailed selection scheme in A.6).",
            "evaluation_criteria": "Same three criteria as GPT-4: Validness (degree hypothesis reflects reality), Novelty (degree of novelty relative to literature), Helpfulness (degree to which the hypothesis can assist researchers), each with explicit 5-point descriptions (see A.9–A.15).",
            "llm_model_name": "N/A (human evaluators); they evaluated outputs produced mainly by gpt-3.5-turbo based MOOSE and baselines; additionally Claude3-Opus used in ablation experiments.",
            "theory_domain": "Social sciences / Business research hypotheses",
            "theory_description": "Human assessment of LLM-generated research hypotheses produced by MOOSE variants and an LLM baseline given raw web corpora.",
            "evaluation_results": "Expert-averaged scores reported (examples from Table 4): Baseline (Validness 3.579, Novelty 2.276, Helpfulness 2.632); MOOSE-base (3.500, 2.855, 3.026); MOOSE-base + future-feedback (3.645, 3.105, 3.303); MOOSE-base + future+past (3.750, 3.197, 3.368). Present-feedback iterations generally increased expert scores up to an optimal number.",
            "benchmarks_or_datasets": "Evaluations performed on subsets drawn from TOMATO-generated hypotheses; total of 400 hypotheses evaluated (one expert evaluated all 400, two others evaluated 104 each), groups of 8 hypotheses per background compared across methods.",
            "comparison_to_human": "This is the human standard; GPT-4 automated scores were compared to these expert scores showing high soft consistency (indicating GPT-4 could be a reliable proxy). Experts found many MOOSE outputs to be novel and valid.",
            "limitations_or_challenges": "Limited pool of experts (three marketing/social-science PhD students) — domain coverage concentrated in Marketing; evaluation cost and time limit number of human-labeled examples; inter-rater (hard) consistency not perfect (hard consistency scores reported), although soft consistency high.",
            "uuid": "e6116.1",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Evaluation metrics (V/N/H)",
            "name_full": "Validness / Novelty / Helpfulness 5-point Rubrics",
            "brief_description": "Triad of evaluation criteria (Validness, Novelty, Helpfulness) formally defined with 5-point descriptive rubrics and used both for automated (GPT-4) and human evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Scoring each hypothesis on three aspects using explicit 5-point scales with anchor descriptions (tables A.9–A.15). Used by GPT-4 and the human experts for consistent evaluation.",
            "evaluation_criteria": "Validness: how well hypothesis reflects reality (5=completely reflects reality; 1=completely violates). Novelty: degree of novelty relative to literature (5=completely novel; 1=not novel). Helpfulness: utility for researchers (5=ready for publication w/o changes; 1=not helpful).",
            "llm_model_name": "Used to evaluate outputs from gpt-3.5-turbo and Claude3-Opus generators; GPT-4 used as an automated rater implementing these rubrics.",
            "theory_domain": "Social sciences (generalizable rubric for open-domain hypothesis induction)",
            "theory_description": "These metrics operationalize the philosophical requirements for hypothetical induction (validity/reality and novelty) and add helpfulness to capture assistive utility to researchers.",
            "evaluation_results": "Metrics used to produce all quantitative tables; authors emphasize novelty is relatively more important for this task. They report metric-specific trends (e.g., MOOSE increases novelty and helpfulness vs baseline; small validness trade-off).",
            "benchmarks_or_datasets": "Applied to hypotheses generated from the TOMATO dataset (50 papers) and 400 expert-evaluated hypotheses.",
            "comparison_to_human": "Rubrics designed to align human and GPT-4 evaluations; results show high soft agreement between GPT-4 and human scores using these rubrics.",
            "limitations_or_challenges": "Rubrics do not explicitly include relevance/significance or conventional NLP automatic metrics (BLEU/ROUGE/METEOR) which authors argue are unsuitable for open-ended novel hypothesis generation; possible trade-offs between novelty and validness complicate optimization.",
            "uuid": "e6116.2",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "TOMATO benchmark",
            "name_full": "TOmated open-doMAin hypoThetical inductiOn (TOMATO) dataset",
            "brief_description": "A benchmark dataset created for open-domain automated hypothesis induction in social sciences: 50 recent social-science papers (post-Jan 2023) with assembled raw web corpora, background/inspiration passages, survey links, and annotation of reasoning chains and complexity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Used as the testbed/benchmark for generating hypotheses: methods must only use provided raw web corpus to generate hypotheses; evaluation uses GPT-4 and human expert scoring on Validness/Novelty/Helpfulness.",
            "evaluation_criteria": "Same triad (Validness, Novelty, Helpfulness); additional internal dataset metadata includes reasoning complexity labels for each example.",
            "llm_model_name": "Generators evaluated on TOMATO include gpt-3.5-turbo (primary), Claude3-Opus (ablation), GPT-4 used only as evaluator (temperature 0.0).",
            "theory_domain": "Social sciences / Business research hypotheses",
            "theory_description": "Each example pairs a target main hypothesis (from a recent paper) with a constructed raw web corpus (news, Wikipedia, reviews) intended to simulate real-world open-domain evidence from which a system must induce a novel hypothesis.",
            "evaluation_results": "Authors use TOMATO to show MOOSE can produce hypotheses judged novel and valid by experts; numerical comparisons (see Tables 2–5) show MOOSE variants outperform a baseline on novelty/helpfulness with varying validness.",
            "benchmarks_or_datasets": "The TOMATO dataset itself is the benchmark: 50 papers, raw web passages (background and inspirations), 14 related surveys, annotated reasoning chains; 400 hypotheses sampled for expert evaluation.",
            "comparison_to_human": "Ground-truth hypotheses from the original papers are included as a human benchmark; they score highest in novelty and helpfulness in retrieval ablations (Table 6).",
            "limitations_or_challenges": "Relatively small (50 papers) though richly annotated; domain concentrated in social sciences/business; constructing semantically similar web passages required human curation to avoid data contamination; not trivially expandable without human effort for ground-truth annotation.",
            "uuid": "e6116.3",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Baseline vs MOOSE eval",
            "name_full": "Baseline LLM chunk-to-hypothesis vs MOOSE multi-module evaluations",
            "brief_description": "Comparative evaluation of a simple baseline that inputs a corpus chunk to an LLM to produce a hypothesis versus the MOOSE multi-module framework (with present/past/future feedback mechanisms) across the TOMATO benchmark using GPT-4 and expert scorers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Direct comparison of generated hypothesis sets from baseline and MOOSE variants using GPT-4 automated scoring (large-scale) and human expert scoring (sampled hypotheses). Statistical averages reported in tables for Validness/Novelty/Helpfulness.",
            "evaluation_criteria": "Primary criteria are Validness, Novelty, Helpfulness on 5-point scales; additional analyses consider retrieval methods, feedback ablations, base model choice (gpt-3.5 vs Claude3-Opus), and present/past/future feedback iteration effects.",
            "llm_model_name": "Generators: gpt-3.5-turbo (primary); Claude3-Opus (ablation). Evaluator: GPT-4 and human experts.",
            "theory_domain": "Social sciences (hypothesis induction)",
            "theory_description": "Hypotheses produced by baseline single-step prompting vs. MOOSE which has modules: Background Finder, Inspiration Title Finder, Inspiration Finder, Hypothesis Suggestor, Hypothesis Proposer, and three feedback channels (present, past, future).",
            "evaluation_results": "Key numerical findings: GPT-4 evaluation (Table 2) shows MOOSE-base improves Novelty (3.081 vs baseline 2.483) and Helpfulness (3.859 vs 3.489) while Validness slightly lower; adding future-feedback and past-feedback further increases Novelty (up to 3.390) and Helpfulness. Expert evaluation (Table 4) shows similar trends with MOOSE-base and feedback variants outperforming baseline on Novelty and Helpfulness (example: baseline novelty 2.276 vs MOOSE w/ future+past 3.197). Present-feedback iterations further improve scores until an empirical optimum.",
            "benchmarks_or_datasets": "Evaluated on TOMATO dataset outputs; GPT-4 evaluation used ≈1000 hypotheses per method; expert evaluation used 50 hypotheses per method (400 total sampled).",
            "comparison_to_human": "Human expert ratings corroborate GPT-4 automated comparisons; authors report high soft consistency between GPT-4 and experts, though GPT-4 may underrate novelty's validness. MOOSE outputs were judged by experts to include many hypotheses considered novel and valid that were not present in literature.",
            "limitations_or_challenges": "Trade-off observed between novelty and validness (increasing novelty sometimes reduces validness); past-feedback implementation relied on heuristics because ChatGPT generated weak past-feedback; base model choice affects absolute scores though relative improvements remain; optimal number of present-feedback iterations must be tuned empirically.",
            "uuid": "e6116.4",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 2
        },
        {
            "paper_title": "Goal driven discovery of distributional differences via language descriptions",
            "rating": 1
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models",
            "rating": 1
        }
    ],
    "cost": 0.013795499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery
12 Jun 2024</p>
<p>Zonglin Yang zonglin001@ntu.edu.sg 
Nanyang Technological University</p>
<p>Xinya Du xinya.du@utdallas.edu 
University of Texas at Dallas</p>
<p>Junxian Li junxian001@ntu.edu.sg 
Nanyang Technological University</p>
<p>Jie Zheng jie.jay.zheng@gmail.com 
Huazhong University of Science and Technology</p>
<p>Soujanya Poria sporia@sutd.edu.sg 
Singapore University of Technology</p>
<p>Erik Cambria cambria@ntu.edu.sg 
Nanyang Technological University</p>
<p>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery
12 Jun 20245D7BE1FCAC1C9C9B8A42E4465C96060AarXiv:2309.02726v3[cs.CL]
Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations.Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging.In this work, we tackle these problems by proposing the first dataset for social science academic hypotheses discovery, with the final goal to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus.Unlike previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity.A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation.To the best of our knowledge, this is the first work showing that LLMs are able to generate novel ("not existing in literature") and valid ("reflecting reality") scientific hypotheses 1 .</p>
<p>Introduction</p>
<p>Logical reasoning is central to human cognition (Goel et al., 2017).It is widely recognized as consisting of three components, which are deductive, inductive, and abductive reasoning (Yang et al., 2023b).Hypothetical induction is considered to be an important sub-type of inductive reasoning (Norton, 2003).It is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain the observations.For example, the proposal of Geocentrism, Heliocentrism, and Newton's law of universal gravitation based on the observations of the motion of (celestial) objects can be seen as a result of hypothetical induction.Hypothetical induction is a process of knowledge exploration from observations to hypotheses: it is challenging because it involves the exploration of knowledge that is even new to humanity.Recent research on this has two main limitations (Yang et al., 2024).Firstly, the observations in their dataset have already been manually selected from the raw web corpus, resulting in a close-domain setting.As a result, a developed system for this dataset relies on already manually selected observations, and cannot utilize the vast raw web corpus to propose hypotheses.Secondly, the ground truth hypotheses are mostly commonsense knowledge (e.g., Newton's law), making the task less challenging since LLMs might have already seen them during pretraining.To this end, we propose a new task setting of hypothetical induction, which is to generate novel and valid research hypotheses targeting being helpful to researchers while only given (vast) raw web corpus (Figure 1).This hypothesis formation process is seen as the first step for scientific discovery (Wang et al., 2023a).We call this task as "auTOmated open-doMAin hypoThetical inductiOn (TOMATO)".It is "automated" since a method for this task should automatically propose hypotheses with few human efforts; It is open-domain since it is not restricted by any manually collected data.</p>
<p>For the TOMATO task, we constructed a dataset consisting of 50 recent social science papers published after January 2023 in top social science journals.For each paper, social science experts collect its main hypothesis, identify its background and inspirations, find semantically similar contents for its background and inspirations from the web corpus, collect the full passage for each matched content, and use all collected web passages as raw web corpus.Although the new dataset involves many manual selection processes, the manually selected contents are used more as benchmarking human performance for comparison.In the TOMATO task, a method is required to only utilize the raw web corpus in the dataset to propose hypotheses.In addition, the raw web corpus is mostly from common news, Wikipedia, and business reviews, which means it can easily expand in scale without much human involvement.</p>
<p>To tackle the TOMATO task, we develop a multimodule framework called MOOSE based on large language model (LLM) prompting (Figure 4).To further improve the quality of the generated hypotheses, we also propose three different feedback mechanisms (present-feedback, past-feedback, and future-feedback) to use LLMs to retrospect and improve the LLM-generated hypotheses for better quality.For present-feedback, the intuition is that, for some modules, their generation can be evaluated by other LLMs and be provided with feedback, which can be utilized by the modules to refine their generation by taking the feedback and previous generation as input and generating again.Some modules can have feedback instantly after their generation to improve themselves.But just like the reward mechanism in reinforcement learning, some rewards (feedback) might be hard to obtain instantly, but need to wait for feedback for a future module.Similarly, we develop past-feedback where a module can benefit from the feedback for a future module.The last one is future-feedback, where a current module can provide justifications for the current module's generation to help a future module's generation, or can provide some initial suggestions which a future module can build upon to further provide more in-depth generation.</p>
<p>Both GPT-4 (OpenAI, 2023) evaluation and expert evaluation indicate that MOOSE performs better than an LLM (Ouyang et al., 2022) based baseline, and each of the three feedback mechanisms can progressively improve the base framework.During expert evaluation, many hypotheses generated by MOOSE are recognized by social science researchers to be both novel ("not existing in the literature") and valid ("reflecting reality").To the best of our knowledge, this is the first work showing that LLMs can be leveraged to generate novel and valid research hypotheses, indicating the potential for LLMs to serve as a "copilot" for scientists.</p>
<p>2 Related Work 2.1 NLP Methods for Scientific Discovery Zhong et al. (2023) propose a dataset where each data consists of a research goal, a corpus pair, and a discovery.However, (1) their task needs a humanprovided research goal and a pairwise corpus for discovery, which is not an automated setting and has a limited application scope; (2) the discovery is not from recent publications.Wang et al. (2023b) is a concurrent work of ours.Compared the first version of two papers, they do not have an iterative feedback for novelty, reality, and clarity.Later they add for novelty, but still lack the other two.These aspects are required by inductive reasoning, and there's an implicit trade-off between reality and novelty.Only stressing on novelty might lead to incorrect and vague generation.Bran et al. (2023) focuses on integrating computational tools in the chemistry domain, but not on providing novel chemistry findings or hypotheses.Boiko et al. (2023) focuses on using LLMs to design, plan, and execution of scientific experiments, but not on finding novel hypotheses.</p>
<p>LLM-based Self Feedback</p>
<p>Self-refine (Madaan et al., 2023) investigates feedback but it only focuses on present-feedback (our framework also proposes past-feedback and futurefeedback), and it is not specially designed for inductive reasoning tasks.Other similar works to self-refine (Press et al., 2022;Peng et al., 2023;Yang et al., 2022;Shinn et al., 2023) also only focus on present-feedback, and their feedback is not multi-aspect nor iterative compared to ours.</p>
<p>, suggesting that cusy, particularly when rs step up to a selfdings in which stranpacted by this social o scan a QR code on t need to perform in ld need to do when thereby experiencing social presence when rs may also be contheir actions of taking dditional body moverded and shared by effect of social presiety and privacy conumber of customers stomer and watching.ers would experience ivacy concerns caused ent technology than hen more people are e, we develop the folikely to use FR payment ology for their transacine behind them.</p>
<p>The Moderating Effect of Experience on the Social Presence Effect</p>
<p>The social presence effect, as we hypothesized in Hypothesis 1, may be moderated by an individual's experience.When an individual's experience in using the technology grows, that individual has more confidence and less anxiety when using the technology.The evidence for such moderation has been documented in the performing arts (e.g., Steptoe and Fidler 1987) and education (e.g., Meijer and Oostdam 2007).In the retail setting, Dahl et al. (2001) find that familiarity with purchasing embarrassing products reduces the embarrassment caused by the social presence in both the selection and commitment stages of the purchase process.The literature suggests that experience in using a technology can also increase self-efficacy (Crossler and Bélanger 2019) and thus increase perceived control over the technology (Hui and Bateson 1991), which then reduces users' privacy concerns over using the technology.In our case, by accumulating more experience in using FR in front of other customers, the focal customer's technology anxiety and privacy concern caused by the social presence effect is expected to decrease, which is similar to the findings of studies of music performers, test takers, retail shoppers, mobile app users in the literature, whose performance anxiety/privacy concern reduced as their experience increased.This means that the experience of customers in using FR payment technology negatively moderates their social presence effects.Hence, we have the following hypothesis: Hypothesis 3. Customers with more experience in using FR payment technology have less social presence effect when using this technology than those with less experience in using FR payment technology.s is that they must go through dife payment process.ustomers do not need to use their , they stand in front of a built-in heckout POS machine that verifies e consumer chooses the FR option OS machine, a frame located at the f the self-checkout machine will be mer needs to position his or her e built-in camera to scan.This prong a selfie that requires the custoselves so that their faces are within ntification process to proceed.In ay need to perform certain livemetimes, such as slightly shaking , 3 so that the FR payment technolhat it is a live person and not a and liveness test gestures required n be a performance-like action.A tification process runs in the backto match the person in front of the to on the ID card in the database.orized once there is a match.quire sensitive private information ed to be executed in public spaces ing to the literature, privacy conss management," including access ational privacy) or access to interacy) (Laufer and Wolfe 1977).In concerns may be related to both and access to interaction.The entiayment platform, the retail stores, ustomers.cy literature and the specific conwe summarize the three sources concerns for the focal customer ut POS machines may emanate. 4y be concerned about his or her that is captured and stored in the shows that users oftentimes do ew technology because of the perllected to set up the technology 2009, Tsai et al. 2011)).Second, a ncerned about his or her digital tured when the customer uses FR concerns may rise from their perate information is collected, con- (Malhotra et al. 2004) since how ted will be used (or not used) is s.Third, a customer may be conomers behind him or her watching hile he or she is using the FR paynsumers may consider the actions doing additional body movement rivate behaviors and view the disto other stranger shoppers around as a violation of physical privacy (Laufer and Wolfe 1977), and they may consider the videos recorded and shared by those strangers around them as a violation of informational privacy (Choi et al. 2015).The first privacy concern is common to both QR and FR, whereas the last two apply to FR only.We discuss how we deal with these privacy concerns in Online Appendix A3.</p>
<p>Hypothesis Development</p>
<p>In this section, we develop hypotheses associated with FR payment usage.A literature review is included in Online Appendix A1.</p>
<p>Social Presence Effect</p>
<p>Based on the social impact theory, an individual's behavior is impacted by real, implied, or even imagined social presence, and the magnitude of the impact is determined by social size, immediacy, and social source strength.The amount of social presence is a multiplicative function of the strength, immediacy, and number of people who evaluate an individual's performance as a member of an audience (Latané 1981).This theory is tested and supported by empirical evidence in the field of psychology (e.g., Jackson and Latané 1981) and is expanded in other fields.Along a similar line, when social impact exists, the social cognitive theory argues that an individual's self-efficacy and emotions, such as technology affect and technology anxiety, play an important role in the individual's decision on technology use.</p>
<p>In retail, a public setting, social presence exists and can create a feeling of embarrassment for customers (Dahl et al. 2001).Using several laboratory experiments, Dahl et al. (2001) find that awareness of social presence during purchase selection and commitment increases a subject's self-reported sense of embarrassment in both laboratory settings and in field studies.Argo et al. (2005) investigate the impact of two social forces-social size and proximity-on customer emotions and self-presentation behaviors in two field experiments, finding that customer emotion and behavior (e.g., brand choice) can be negatively affected by the presence of others in the surroundings during a shopping session.Using a laboratory experiment, Dabholkar and Bagozzi (2002) document that social anxiety resulting from imagined social presence negatively affects customer attitudes toward and intention to use self-service technology.</p>
<p>Customers may have privacy concerns because of the social presence too.Some studies in the literature have found that social presence is a factor that could prevent individuals from using a certain technology.They show that social presence negatively affects customers' perceived control (Hui and Bateson 1991), which is a factor for privacy concerns (Schmidt and Keating 1979).The customer's perceived control is positively associated with the customer's perceived ion Payment Technology in Retail Articles in Advance, pp.1-12, © 2023 INFORMS 3 front of a webcam as they would need to do when using FR payment technology, thereby experiencing less technology anxiety caused by social presence when strangers are around.Consumers may also be concerned about their privacy when their actions of taking a photo and, sometimes, doing additional body movement for a liveness test, are recorded and shared by those strangers around them.The effect of social presence on the user's technology anxiety and privacy concerns should increase with the number of customers waiting in line behind the focal customer and watching.Therefore, we expect that customers would experience greater technology anxiety and privacy concerns caused by social presence using FR payment technology than using QR payment technology when more people are waiting in line behind them.Hence, we develop the following hypothesis.</p>
<p>Hypothesis 1. Customers are less likely to use FR payment technology than QR payment technology for their transactions when they have more people in line behind them.</p>
<p>Herding Effect</p>
<p>A customer's decision on payment technology use may be affected by the preceding customers before him or her as well.Unlike the customers behind him or her, the focal customer can observe the payment choice of the preceding customers.Such observation may influence the choice of payment technology for the focal customer.</p>
<p>Herding theory argues that people tend to herd because they believe the choice of the majority is the best, thus reducing the uncertainty of their own choice (Banerjee 1992, Sunder et al. 2019).In the process of herding, an individual's own belief regarding the quality of different choices is discounted or even ignored (Banerjee 1992)</p>
<p>The Moderating Effect of Experience on the Social Presence Effect</p>
<p>The social presence effect, as we hypothesized in Hypothesis 1, may be moderated by an individual's experience.When an individual's experience in using the technology grows, that individual has more confidence and less anxiety when using the technology.The evidence for such moderation has been documented in the performing arts (e.g., Steptoe and Fidler 1987) and education (e.g., Meijer and Oostdam 2007).In the retail setting, Dahl et al. (2001) find that familiarity with purchasing embarrassing products reduces the embarrassment caused by the social presence in both the selection and commitment stages of the purchase process.The literature suggests that experience in using a technology can also increase self-efficacy (Crossler and Bélanger 2019) and thus increase perceived control over the technology (Hui and Bateson 1991), which then reduces users' privacy concerns over using the technology.In our case, by accumulating more experience in using FR in front of other customers, the focal customer's technology anxiety and privacy concern caused by the social presence effect is expected to decrease, which is similar to the findings of studies of music performers, test takers, retail shoppers, mobile app users in the literature, whose performance anxiety/privacy concern reduced as their experience increased.This means that the experience of customers in using FR payment technology negatively moderates their social presence effects.Hence, we have the following hypothesis: Hypothesis 3. Customers with more experience in using FR payment technology have less social presence effect when using this technology than those with less experience in using FR payment technology.Downloaded from informs.org by [3.0.220.147] on 12 August 2023, at 02:11 .For personal Our present-feedback is developed upon a multiaspect over-generate-then-filter mechanism (Yang et al., 2024).However, they only utilize LLMs to "filter" but not to provide feedback.</p>
<p>Dataset Collection</p>
<p>In this section, we take one publication (Gao et al., 2023) in our dataset as an example to illustrate the dataset collection process.In total, there are 50 papers published after January 2023.Table 1 shows the statistics of the subject distribution.</p>
<p>Most social science publications highlight their hypotheses.Figure 2 shows our selected main hypothesis in the example publication.The research backgrounds are given in the introduction section.In this example paper, the background is about facial recognition payment technology's usage in society.Most social science publications also have a "Hypothesis Development" section (some may call it by other names, e.g., "Theoretical Development").For example, the left part ("Hypothesis Development") in Figure 3 shows the title of this section in the example paper.In this section, several theories used to develop the main hypothesis are separately introduced.Usually, each theory takes one subsection.For example, the right part ("Herding Effect") in Figure 3 shows the title of a subsection, which is a particular theory being used as an inspiration, which with the background can develop the hypothesis in Figure 2.</p>
<p>For each publication in our dataset, we identify its main hypothesis, research background, and in- spirations, where the background and inspirations together provide enough information to be possible to develop the hypothesis.We also abstract the reasoning process from background and inspirations to hypothesis and note it down for each publication in our dataset.In this selected example, the reasoning process is easy, but it has medium difficulty for researchers to associate the inspiration (herding effect) to the background.For each publication, we include an expert-evaluated complexity for both the reasoning process and the association of the inspiration to the background (details in §A.3).Instead of directly copying the background and inspirations from the paper to construct the dataset, we try to find semantically similar text contents from the web corpus as a substitution to avoid data contamination and fit the requirement of TOMATO task that a system should propose novel and valid research hypotheses only given raw web corpus.</p>
<p>In the example paper, we find news sentences reporting the usage of facial recognition payment as ground truth background and a Wikipedia description of the herding effect as ground truth inspiration.We also collect the web link and the full text of the manually selected web passages for backgrounds and inspirations to be used as raw web corpus.</p>
<p>In addition, we collect the link and the publication date for all fifty papers.We also collected fourteen survey papers in related fields that might help check the novelty of the hypotheses.The dataset is fully constructed by a social science PhD student.We illustrate why the dataset shouldn't be collected by automatic methods in §A.4.</p>
<p>Methodology</p>
<p>In general, our method consists of a base multimodule framework and three feedback mechanisms (past-feedback, present-feedback, and futurefeedback).We call the full framework as Multi-mOdule framewOrk with paSt present future feEdback (MOOSE).The base framework without any feedback is called MOOSE-base.MOOSE is described in Figure 4 and Algorithm 1.</p>
<p>Base Framework</p>
<p>The base framework is developed based on the intuitive understanding of how social science researchers propose an initial research hypothesis.</p>
<p>Firstly, a researcher needs to find a suitable research background, e.g., facial recognition payment system's impact.This background should be  proposed with a deep understanding of the societal world.Accordingly, we develop a background finder module, which reads through raw web corpus to find reasonable research backgrounds.</p>
<p>Secondly, since the proposed hypothesis should be novel, directly copying from raw web corpus usually is not enough.A good social science hypothesis should contain an independent variable and a dependent variable, and describe how the independent variable can influence the dependent variable.Therefore, building connections between two variables that have not been known for established connections contributes to a novel hypothesis.We hypothesize that proper inspiration can help this connection-building process, since it might serve as one of the variables itself, or might help to find such variables.However, it could consume lots of computing resources and even be practically impossible if the framework searches over the full web corpus for every found background.Nevertheless, it could be much more viable if only searching over the titles of the corpus, and then only finding inspirational sentences in the passages which match the selected titles.Accordingly, we develop an inspiration title finder module and an inspiration finder module, together to find proper inspirations given a background.</p>
<p>Lastly, a hypothesis proposer module can utilize backgrounds and inspirations for hypotheses.</p>
<p>In general, MOOSE-base consists of a list of serializable generation modules M 0 , M 1 , ..., M n that function sequentially.The input of a module M i is from the output of previous modules M j,j&lt;i and a raw web corpus C (and optionally a related survey corpus).M i 's output is represented as o i .Feedback to o i is represented as f i .</p>
<p>Present-Feedback</p>
<p>LLMs are not perfect and can lead to flaws in the generation, especially for those modules that undertake a difficult task.Previous work on hypothetical induction (Yang et al., 2024) tackles this problem by leveraging LLMs to identify flaws in the generation and filters those with huge flaws.Here we take a step further that instead of filtering, LLMs are leveraged to provide feedback, so that a generation can be improved rather than just filtered.</p>
<p>Accordingly, we define present-feedback as when an output o i can be directly evaluated and provided feedback f i (by LLMs or experts, here we use LLMs) in terms of some aspects, o i and f i are used as additional inputs to M i , so that M i can regenerate o i to refine the previous one with f i .</p>
<p>We implement present-feedback on the Hypotheses Proposer module, since it is a key module that undertakes a very difficult task.In terms of what aspects should the feedback focus on, Yang et al. (2024) propose four aspects according to the philosophical definition and requirement for hypothetical induction (Norton, 2003).The aspects are whether the hypothesis (1) is consistent with observations; (2) reflects reality; (3) generalizes over the observations; (4) is clear and meaningful.</p>
<p>In MOOSE, we basically adopt the four aspects but reframe them to better fit the current task.Specifically, aspect (2) contains aspect (1) most of the time (unless the observations are wrongly described).To save computing power, we adopt aspect (2) but not aspect (1).In addition, we re-frame aspect (3) as whether the hypothesis is novel, and reframe aspect (4) as whether the hypothesis is clear and provides enough details.Accordingly, we develop a reality checker module, a novelty checker module, and a clarity checker module in Figure 4.</p>
<p>Past-Feedback</p>
<p>Just like the reward mechanism in reinforcement learning, some modules' generation can only be evaluated at a future time point.For instance, it is hard to give feedback on the selected inspirations unless we know what hypotheses these inspirations could lead to.Accordingly, we develop past-feedback as when it is hard to directly evaluate o i , the framework continues to run until generating o j,j&gt;i , where o j is highly influenced by o i and can be directly evaluated to obtain present-feedback f j .Then o i , o j , and f j are utilized, possibly by an additional module implemented with an LLM, to provide past-feedback f i to M i , so that M i can regenerate o i with f i to refine the previous o i .</p>
<p>We implement past-feedback on the Inspiration Title Finder module.The intuition is that improper inspirations can lead to low-quality hypotheses, and it is hard to directly evaluate inspirations.</p>
<p>Future-Feedback</p>
<p>We also develop future-feedback, targeting at providing additional useful information for a future module M j to generate o j in better quality.Specifically, we develop future-feedback-1 (FF1) and future-feedback-2 (FF2).FF1 is that in addition to o i , justifications (reasons) of o i are also provided to M j,j&gt;i so that M j can better leverage o i ; FF2 is that for a key module M j that handles a very complex task, an additional module M j−0.5 is being placed before M j , so that M j−0.5 can undertake some of the reasoning burdens of M j to improve the quality of o j .For example, in MOOSE, M j−0.5 is to provide preliminary suggestions for M j .</p>
<p>Specifically in the MOOSE framework, for FF1, no additional modules are needed.Instead, we modify the prompt to require M i to not only generate o i but also provide the justification of o i .We implement it on the Background Finder and the Inspiration Title Finder modules.The intuition is that it could be helpful if the Inspiration Title Finder module knows not only the background but also what possible research topics could be conducted for this background so as to select suitable titles; it could be also helpful for the Inspiration Finder module to know why this background was selected and what potentially helpful inspirations could be found from the passage with the corresponding selected titles.For FF2, we implement it on the Hypothesis Proposer module, since proposing hypotheses is a very important and complex task.Accordingly, we develop a Hypothesis Suggestor module (as M j−0.5 ) to provide some initial suggestions on how to utilize the inspirations and background first, and then Hypothesis Proposer (as M j ) can build upon the suggestions to generate more novel and more complicated hypotheses.</p>
<p>Experiments</p>
<p>Evaluation Metrics &amp; Details</p>
<p>We conduct both automatic evaluation and human evaluation for the experiments.</p>
<p>For automatic evaluation, we adopt validness, novelty, and helpfulness as three aspects for GPT-4 to evaluate.We choose validness and novelty because they are the two basic requirements for hypothetical induction illustrated in philosophical literature (Norton, 2003;Yang et al., 2024).In addition, these two scores also highly resemble the current ACL review form, which requires reviewers to score submitted papers on soundness and excitement aspects.We choose helpfulness because the final goal of the TOMATO task is to provide help and assistance for human scientists.</p>
<p>In §A.5 we illustrate why we don't adopt evaluation metrics such as (1) relevance and significance, and (2) BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), or METEOR (Banerjee and Lavie, 2005).</p>
<p>For human (expert) evaluation, evaluation metrics are the same.Three experts (social science PhD students) take charge of the expert evaluation.They evaluate on 400 randomly selected hypotheses from the baseline and variants of the MOOSE framework.To avoid any bias, they are not told which methods we are comparing; the order of generated hypotheses to compare is also randomized.We introduce how the 400 hypotheses are selected in §A.6, and the high expert agreement in §A.7.</p>
<p>Each metric is on a 5-point Likert scale.Both experts and GPT-4 are given the same description of the scale and evaluation standard of the three aspects (listed in §A.9).</p>
<p>Out of the metrics, we consider the novelty metric to be relatively more important than the validness metric.Because the goal of the TOMATO task is to assist human researchers, but not to directly add the machine-proposed hypotheses to the  literature.If the hypotheses are fully valid but not novel, then they are not helpful at all; but if the hypotheses are novel but not valid, then they can still be possible to inspire human researchers to develop novel and valid hypotheses.Helpfulness is also an important metric since it could be seen as an overall evaluation of a hypothesis.In §A.8, we introduce the surprisingly high consistency between expert evaluation and GPT4 evaluation, indicating that GPT-4 might be able to provide a relatively reliable evaluation for machinegenerated social science hypotheses.</p>
<p>Baselines &amp; Base Model Selection</p>
<p>Since the TOMATO task is to propose hypotheses given only corpus, a natural baseline is to use a corpus chunk as input, and directly output hypotheses.</p>
<p>Except for §6.3, we use gpt-3.5-turbofor each module in MOOSE.To be fair, the baseline is also instantiated with gpt-3.5-turbo.The training data of the model checkpoint is up to September 2021, while all papers in our dataset are published after January 2023, so the model has not seen any of the collected papers in the dataset.In §6.3, we investigate the effect of base model selection by using Claude3-Opus (Anthropic, 2024) for each module in MOOSE.</p>
<p>Main Results</p>
<p>In this subsection, we compare MOOSE-base with the baseline and examine the effect of each of the three feedback mechanisms to MOOSE-base.</p>
<p>We first introduce the number of generated hypotheses being evaluated in §5.3 and §6.For ex-  periments evaluated with GPT-4, fifty backgrounds are selected for each method.For MOOSE-related methods, for each background, on average around 6 inspirations are extracted, resulting in 4 different hypotheses.Each hypothesis leads to another 4 more refined ones with present-feedback.Therefore on average for each MOOSE-related method in GPT-4 evaluation tables, around 50<em>4</em>5=1000 hypotheses are evaluated.For experiments evaluated with expert evaluation, in general, we randomly select one hypothesis for each background, resulting in 50 hypotheses evaluated for each line of the method in expert evaluation tables.</p>
<p>Table 2 shows GPT-4's evaluation targeting at comparing MOOSE-base and the baseline and shows the effect of future-feedback and pastfeedback.In this table, MOOSE-related results are averaged over iterations of present-feedback to not be influenced by present-feedback.MOOSEbase largely outperforms the baseline in terms of both novelty and helpfulness, but slightly lower in terms of validness.As illustrated in §5.1, since the purpose of the TOMATO task is to inspire and help human researchers, novelty and helpfulness metrics should be more important.In practice, we find many hypotheses from baseline almost only rephrasing some sentences in the input corpus, adding little novelty content.MOOSEbase with future-feedback comprehensively outperforms MOOSE-base in terms of all three metrics.MOOSE-base with both future and pastfeedback largely outperforms MOOSE-base with future-feedback in novelty and performs slightly lower in validness and helpfulness metrics.One of the reasons is that the past-feedback may focus more on the novelty aspect because the novelty checker module provides more negative presentfeedback than the reality checker module.</p>
<p>Table 3 shows the effect of present-feedback with GPT-4 evaluation.In this table, the results are averaged over three experiments: MOOSE-base, MOOSE-base with future-feedback, and MOOSEbase with both future and past-feedback to focus on present-feedback.It shows that as more iterations of present-feedback are conducted, validness and novelty steadily go up; helpfulness also steadily goes up but reaches the best performance with 3 iterations of present-feedback.</p>
<p>Table 4 shows expert evaluation results on the comparison between MOOSE-base and the baseline, and the effect of future-feedback and pastfeedback.MOOSE-related results are selected from the 5 th iteration of present-feedback.Similar to GPT-4 evaluation, MOOSE-base largely outperforms the baseline in terms of Novelty and Helpfulness; MOOSE-base with future-feedback comprehensively outperforms MOOSE-base.Different from GPT-4 evaluation, MOOSE-base with future and past-feedback also comprehensively outperforms MOOSE-base with future-feedback.We think one of the reasons could be that GPT-4 might grade validness based on how frequently it has seen relevant texts, but not true understanding of the world.Therefore a more novel hypothesis might tend to have a relatively lower score in validness and helpfulness under GPT-4 evaluation.</p>
<p>Table 5 shows the expert evaluation of presentfeedback.MOOSE-base and MOOSE are both evaluated.Overall performance generally goes up with more iterations of present-feedback, but there might be an optimal number of iterations.</p>
<p>Analysis</p>
<p>Background and Inspirations</p>
<p>Here we try to answer "Is ChatGPT necessary for background and inspiration selection?".</p>
<p>Table 6 shows various methods for background and inspiration selection.In general, there might be a validness-novelty trade-off that if a method reaches a high novelty score, then it is usually hard for it to reach a high validness score.It is surprising that a randomly selected background and randomly selected inspirations can lead to hypotheses with relatively comparable validness and novelty to ChatGPT-picked background and inspirations.Empirically we hypothesize the reason is that randomly picked inspirations are mostly not related to the background, resulting in a high novelty (but less validness and helpfulness).In addition, BM25 (Robertson et al., 2009) picked background and inspirations reach a much higher novelty score compared to ChatGPT-picked ones.Empirically we do not find BM25 retrieved inspirations to be similar to the background, but they are usually with more concrete contents compared with random inspirations.Not surprisingly, Chat-GPT picked background and inspirations reach the highest helpfulness score among those without any ground-truth annotations.Lastly, ground-truth hypotheses reach the highest novelty and helpfulness.</p>
<p>More Ablation Studies</p>
<p>Table 7 shows ablation studies on future-feedback, access to surveys, and the selection of corpus.Firstly, for future-feedback, we separately test the effect of FF1 and FF2.Without FF2, performance comprehensively drops; without FF1, performance drops on validness and novelty, with helpfulness remaining comparable.It seems that FF2 is more significant than FF1.However, the fact that FF1 works on inspiration title finder and inspiration finer modules does not mean that it works on all modules.Empirically we find that adding the reasons (or prospects) for background and inspirations to the hypothesis proposer module will cause a more valid but much less novel Secondly, we cut the access of novelty detector to related surveys to check the effect of related surveys.As a result, novelty largely goes up (0.04), and validness goes down to around 0.26.Empirically one of the main reasons is that BM25 hardly retrieves enough similar survey chunks, so that access to the survey leads novelty detector to tend to reply the hypotheses are novel since it is not mentioned in the related survey.Without presentfeedback, MOOSE and MOOSE w/o access to survey perform quite comparably.</p>
<p>Lastly, the raw corpus in the dataset is from two sources: passages that contain the ground truth backgrounds and passages that contain the ground truth inspirations.In all of the previous experiments, backgrounds are extracted from the background passages, and inspirations are extracted from the inspirations passages.To see whether the passages are only restricted to their designed role, in MOOSE w/ randomized corpus experiment, we use inspiration corpus for background extraction and use both inspiration and background corpus for inspiration extraction.As a result, validness goes up by about 0.025, while novelty goes down by about 0.16.We think one of the reasons is that, in this setting, after selecting a background from an inspiration passage, MOOSE tends to retrieve the same inspiration passage to find inspirations, which leads to less novel results.</p>
<p>Effect of Base Model Selection</p>
<p>In all previous experiments, we adopt GPT-3.5 as the base model.In this section, we investigate the effect of base model selection by using Claude3-Opus as the base model for each module in MOOSE.With Claude3-Opus as the base model, we again analyze the effect of MOOSE-base, past-feedback, and future-feedback in Table 8; and analyze the effect of present-feedback in Table 9.The experiment settings of Table 8 and Table 9 are exactly the same as in Table 2 and Table 3 correspondingly, but only differ in the base model selection.</p>
<p>In general, there are two conclusions.Firstly, MOOSE's components stay effective regardless of different base model selection.It shows the robustness of the MOOSE framework in terms of different base model.Secondly, the absolute evaluation scores on all three metrics largely improved with Claude3-Opus compared to GPT-3.5, indicating the even larger potential of the MOOSE framework when more powerful LLMs are available.</p>
<p>Qualitative Analysis</p>
<p>The following box shows one generated counterintuitive hypothesis (expert evaluation appended).</p>
<p>In collectivist cultures, individuals engage in more conspicuous consumption behaviors compared to individualistic cultures.(Validness: 3.3; Novelty: 4.0; Helpfulness: 4.0)</p>
<p>Here is the assessment from one of the experts:</p>
<p>The main reason I give a high mark for both three dimensions of this hypothesis is because:</p>
<p>(1) For validness, this hypothesis is based on existing cultural theories and empirical evidence that suggests cultural values significantly impact consumer behavior.It aligns with established concepts like collectivism and individualism that have been widely studied in cross-cultural psychology.</p>
<p>(2) For novelty, this hypothesis is counterintuitive to some extent.Prior research has shown that collectivist cultures often prioritize group harmony, cooperation, and social cohesion over individual desires.This emphasis on collective wellbeing might suggest a reduced inclination toward overt displays of personal wealth or status through conspicuous consumption.However, this hypothesis suggests the opposite that collectivist culture's members engage in more conspicuous consumption, which is more commonly linked to individualistic societies in popular perceptions.This challenges the notion that members of collectivist cultures avoid conspicuous consumption behaviors.</p>
<p>(3) For helpfulness, if this hypothesis is confirmed, it could have significant practical implications.Understanding the impact of cultural values on conspicuous consumption can assist businesses and marketers in crafting more effective cross-cultural marketing strategies.It could also aid policymakers in addressing societal issues related to consumerism.</p>
<p>In addition to the analysis of this counterintuitive example, we also provide qualitative analysis on the difference between hypotheses generated from the baseline, MOOSE-base, MOOSEbase w/ future-feedback, and MOOSE-base w/ future and past-feedback in §A.11.More qualitative analysis on highly scored generated hypotheses can be found in §A.12.Additionally, §A.13 illustrates factors for good hypotheses in social science (particularly in Business).§A.14 shows how MOOSE formulates a hypothesis by giving the generation of each of the modules in MOOSE.</p>
<p>Conclusion</p>
<p>In this paper, we propose a new task, automated open-domain hypothetical induction (TOMATO), which is the first task in NLP to focus on social science research hypotheses discovery.Along with the task, we construct a dataset consisting of 50 recent social science papers published in top academic journals.We also developed a multi-module framework MOOSE for the TOMATO task, which contains a base framework and three novel feedback mechanisms.Experiments indicate that MOOSEbase outperforms an LLM-based baseline, and the three feedback mechanisms can progressively fur-ther improve over MOOSE-base.Surprisingly, evaluated by PhD students, MOOSE is able to produce many novel ("not existing in the literature") and valid ("reflecting reality") research hypotheses.To the best of our knowledge, this is the first work showing that LLMs can be leveraged to generate novel and valid scientific hypotheses, indicating the potential of LLMs to serve as a "copilot" for scientists.</p>
<p>Limitations</p>
<p>From the first look, it might seem that the proposed dataset consists of only 50 recent papers.However, they are all manually collected by experts (PhD students), and are annotated with lots of details (e.g., identifying background and inspirations, finding relevant raw web passages for background and inspirations, reasoning process, complexity level).In addition, each paper has been published in a top social science journal, representing the pinnacle of human intelligence.This means it would be incredibly exciting if LLMs could propose a hypothesis from even a single one of these recent papers.</p>
<p>It might also seem that it is not clear whether the design of the framework can apply to other disciplines.However, to the best of our knowledge, this is the first paper using LLMs that can propose novel scientific hypotheses that are new to humanity.We choose social science as the breakthrough point since the main data format of social science is language.Table 1 shows that the dataset covers 7 different disciplines (e.g., Psychology, Management, Marketing).It would be nearly impossible for the first few works to develop a general method to propose novel hypotheses for all disciplines.</p>
<p>This paper concentrates on an automated task setting in which a system is designed to formulate scientific hypotheses independently, without requiring human intervention.In some scenarios, scientists may prefer to use their own background and inspirations as input for controllable hypotheses generation.It might seem that the automated setting and the controllable setting are in conflict.However, we contend that the automated setting make a step further than the controllable setting, since a system developed for an automated setting would inherently support controllable generation by simply substituting the automatically searched inputs (e.g., background and inspirations) with those that are manually crafted.</p>
<p>Societal Impact: Expert evaluation shows that MOOSE, an LLM-based system, might already be able to serve as a copilot for researchers across various social science disciplines.Particularly, as depicted in Figure 1, it can assist researchers in the hypothesis formation process, which is the first step for scientific discovery (Wang et al., 2023a).This capability signifies a step towards enhancing the efficiency of scientific exploration by accelerating the formation and development of innovative and credible research hypotheses, thereby boosting researchers' productivity.To maximize its impact and ensure equitable access, it is imperative to advocate for the open-sourcing of such systems, thereby democratizing access for the global scientific community.</p>
<p>A Appendix</p>
<p>A.1 Hyper-parameters Experiments in §6.3 adopts Claude3-Opus, all other experiments are conducted with gpt-3.5-turbo.Both Claude3-Opus and gpt-3.5-turbouse 0.9 temperature and 0.9 top_p.The hyperparameters for GPT-4 evaluation are 0.0 temperature to ensure the evaluation scores are stable, and 0.9 top_p.</p>
<p>A.2 More Related Works on Reasoning and Scientific Discovery</p>
<p>This paper is a successive work in inductive reasoning and is different from commonsense reasoning (Bosselut et al., 2019;Yang et al., 2020) in that the novel social science hypotheses do not belong to commonsense.</p>
<p>Case-based reasoning (Das et al., 2021;Yang et al., 2023a) also falls in the domain of inductive reasoning, but case-based reasoning is more about high-level guidance on methodology design (case retrieve, reuse, revise, and retain), which is not involved in this paper.Qi et al. (2023) work on zero-shot hypothesis proposing, which is a concurrent work to our paper.They don't focus on social science and business disciplines, and mostly adopt a single LLM as method (prompting, finetuning).</p>
<p>A.3 Dataset Complexity Distribution</p>
<p>Table 10 illustrates the complexity distribution of the proposed dataset from both reasoning and association perspectives."Easy" in the table means it is relatively easy compared to other publications in the dataset, but does not mean it is actually easy to induce the hypotheses.</p>
<p>A.4 Why the Tomato Dataset Shouldn't Be Collected by Automatic Methods</p>
<p>Firstly, there are many hypotheses in a social science publication, which might need an expert to identify which hypothesis is suitable for this task (e.g., whether it is a main hypothesis, whether the background and inspirations are properly introduced).</p>
<p>Secondly, the background and inspirations scatter in a publication.It needs a deep domain understanding of the hypothesis, related background, and inspirations to select the background and inspirations out to form a complete reasoning chain to conclude the hypothesis.</p>
<p>Thirdly, it needs enough domain knowledge to find semantically similar texts (similar to the groundtruth selected background and inspirations) from the web, where the texts should contain enough details to help elicit the hypothesis.</p>
<p>A.5 Why Not Using Other Evaluation Metrics</p>
<p>Other relevant aspects from related literature include relevance (Wang et al., 2023b) and significance (Zhong et al., 2023).</p>
<p>We do not adopt relevance because our task setting is the automated and open domain, without a manually given background; neither for significance because social science is different from engineering subjects -(1) every hypothesis is to reflect the reality of the world, and as long as it reflects the world, it is significant.Therefore it is hard to tell which one is more significant even by experts; (2) the evaluation standard of significance varies from time to time.For example, in the 60s, conducting research on how to improve the assembly line's efficiency as much as possible was seen as very significant.However, in  recent decades, how to alleviate the psychological depression of assembly line workers is seen as more significant.</p>
<p>We do not adopt BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), or METEOR (Banerjee and Lavie, 2005) as evaluation metric to compare the proposed hypothesis and the ground truth hypothesis since (1) proposing novel research hypotheses is an open problem, and (2) TOMATO has an automated open domain setting, which means the automatically selected background and inspirations are hardly the same as a few given ground truth ones (if background and inspirations are not the same, then it is meaningless to compare the hypothesis).Liu et al. (2016) have conducted a comprehensive analysis that they also reached a similar conclusion that BLEU, METEOR, or ROUGE is not suitable for an open-ended task (such as a dialogue system).</p>
<p>A.6 Hypotheses Selection for Expert Evaluation</p>
<p>In total, we randomly selected 400 hypotheses to be evaluated by experts.Specifically, for each background passage in the dataset (out of 50), we use 4 methods (which are to be compared) to collect in total 8 hypotheses.</p>
<p>The 8 hypotheses are from (1) the baseline;</p>
<p>(2) the MOOSE-base framework;</p>
<p>(3) MOOSE-base + future-feedback; (4) MOOSE-base + future-feedback + past-feedback.For ( 2) and (4), we collect three hypotheses, which are (a) without present-feedback; (b) after 2 iterations of present-feedback; and (c) after 4 iterations of present-feedback.For (1) and (3), we only collect one hypothesis, which is without present-feedback.</p>
<p>With these collections, we can evaluate the effect of both the MOOSE-base framework and the three feedback methods, leading to results in Table 4 and Table 5.</p>
<p>Out of the three experts, one expert evaluates the full 400 hypotheses, and the other two each evaluate 104 hypotheses (the first and second 104 hypotheses out of 400).The reason we choose the number "104" is that (1) social science PhD students are quite busy and two of them can only have time to evaluate around 100 hypotheses; (2) the number should be dividable by 8 (since every 8 hypotheses form a group for comparison).</p>
<p>The results of the expert evaluation are averaged over the three experts.Specifically, expert evaluation essentially compares the 8 hypotheses within a group.The 400, 104, and 104 hypotheses evaluation scores can be written as arrays of [50,8], [13,8], and [13,8].We concatenate them to [76,8], and average them across the first dimension.</p>
<p>The payment for expert evaluation is $1 per hypothesis.</p>
<p>A.7 Expert Qualification and Expert Agreement</p>
<p>The constructed dataset covers many subjects, but every collected publication is somewhat related to Marketing, which is a big topic in Business research.It is common in social science to conduct research that connects with other social science domains.The experts for expert evaluation are three PhD students majoring in Marketing.Therefore the experts are qualified enough to provide assessment for machinegenerated hypotheses in the domain.</p>
<p>The consistency scores between experts are shown in Table 11.The soft consistency and hard consistency are defined in §A.8.All soft consistency scores are above 0.75 means, and the average difference between experts in terms of each metric is less than 1 (out of a 5-point scale), exhibiting high expert evaluation agreement.Aspect 1: Validness</p>
<p>points</p>
<p>The hypothesis completely reflects the reality.</p>
<p>points</p>
<p>The hypothesis almost completely reflects the reality, but has only one or two minor conflictions that can be easily modified.</p>
<p>points</p>
<p>The hypothesis has at least one moderate conflict or several minor conflicts.</p>
<p>points</p>
<p>The hypothesis has at least one major confliction with the reality or only establishes in very rare circumstances that are not mentioned in this hypothesis.</p>
<p>point</p>
<p>The hypothesis completely violates the reality.</p>
<p>A.8 Consistency Between Expert Evaluation and GPT-4 Evaluation</p>
<p>To check the consistency between expert evaluation and GPT-4 evaluation, we use the expert evaluation results and find the corresponding GPT-4 evaluation results.In total, there are 400 hypotheses evaluated by experts, so the sample we use to calculate the consistency score is 400.Specifically, similar to Pan et al. (2011), for soft consistency, if the absolute difference between expert evaluation and GPT-4 evaluation (both are on a 5-point scale) is 0/1/2/3/4, then we assign a consistency score of 1.00/0.75/0.50/0.25/0.00;for hard consistency, if only the difference is 0, can the consistency score be 1.00, otherwise consistency score is 0.00.The hard and soft consistency scores shown in Table 12 are averaged for each metric.</p>
<p>The consistency scores are surprisingly high.All soft consistency scores are above 0.75 means, and the average difference between expert and GPT-4 evaluation in terms of each metric is less than 1 (out of a 5-point scale).The results indicate that GPT-4 might be able to provide a relatively reliable evaluation for machine-generated hypotheses.</p>
<p>Aspect 1: Novelty</p>
<p>points</p>
<p>The hypothesis is completely novel and has not been proposed by any existing literature.</p>
<p>points</p>
<p>The main argument or several sub-arguments of the hypothesis are novel.</p>
<p>points</p>
<p>The main argument is not novel, only one or two sub-arguments appear to be novel.</p>
<p>points</p>
<p>The full hypothesis is not novel, but the way it combines the topics can be inspiring for human researchers.</p>
<p>point</p>
<p>The hypothesis is not novel at all and not inspiring for human researchers.Aspect 1: Helpfulness</p>
<p>points</p>
<p>The hypothesis is novel, valid, clear, and specific enough that it is itself a mature research hypothesis, and human researchers can directly adopt it for publication with no modifications needed.</p>
<p>points</p>
<p>The hypothesis is novel enough and can be directly adopted by human researchers for publication after minor modifications.</p>
<p>points</p>
<p>The hypothesis should be largely modified or reconstructed by human researchers to adopt it.</p>
<p>points</p>
<p>Modifying this hypothesis might not deserve the efforts, but a small part of this hypothesis is inspiring for human researchers to develop a new hypothesis.</p>
<p>1 point The hypothesis is not helpful and not inspiring at all.</p>
<p>A.9 Evaluation Aspects Description</p>
<p>The evaluation standard for Validness, Novelty, and Helpfulness is correspondingly displayed in Table 13, Table 14, and Table 15.A.10 More Details About Past-Feedback Design</p>
<p>In practice, we find that ChatGPT is not capable enough to generate past-feedback with enough good quality for the Inspiration Feedback module.Instead, it tends to provide feedback as "the previous inspiration titles are not very relevant to the hypotheses or the background".As a result, the ChatGPT Inspiration Title Finder module tends to select inspiration titles that are very related to the background, resulting in a less novel hypotheses generation.</p>
<p>Therefore instead of instantiating with ChatGPT for the Inspiration Feedback module, we experiment with leveraging human heuristics.The heuristics are "if the inspiration titles are less related to the background, then more novel hypotheses are likely to be proposed.".With this heuristics-based pastfeedback, MOOSE does perform better (as shown in the tables in §5 and §6).</p>
<p>This heuristics-based feedback is possible to be obtained by a language model since it has access to the novelty feedback of each hypothesis as well as the inspiration titles the hypothesis leveraged.Here our contribution is to propose a useful framework for the TOMATO task, which is not limited by any LLMs for any module in the framework.In the future, it is possible for more powerful LLMs to find better inspiration feedback than human heuristics.</p>
<p>A.11 Qualitative Analysis on Hypotheses Generated From Different Methods</p>
<p>We analyze four hypotheses from the baseline, MOOSE-base, MOOSE-base w/ future-feedback, and MOOSE-base w/ future and past-feedback (MOOSE), where the four methods use the same passage to extract background.</p>
<p>• Hypothesis from the baseline: Companies that prioritize customer understanding will have higher profitability than companies that do not prioritize customer understanding.(Evaluated by the expert, Validness: 4; Novelty: 1.5; Helpfulness: 2)</p>
<p>• Hypothesis from MOOSE-base: The level of empathy displayed by leaders in a startup environment influences employees' job satisfaction and organizational success through the mediation of employees' perceived likelihood of negative outcomes and expectation of enjoyment, as well as their propensity toward risky choices.(Evaluated by the expert, Validness: 3.5; Novelty: 3; Helpfulness: 3)</p>
<p>• Hypothesis from MOOSE-base w/ future-feedback: Female CMOs in startups, leveraging their higher levels of empathy, are more likely to prioritize customer satisfaction by actively listening to customer feedback, incorporating customer insights into decision-making processes, and providing personalized customer experiences.This employee prioritization of customer satisfaction is positively associated with higher levels of customer engagement, increased customer loyalty, and improved brand recall, as measured by objective metrics such as sales figures, customer retention rates, and brand recognition in the market.The influence of female CMOs on employee behavior is mediated by their ability to foster a caring relationship with as supported by empirical data and statistical analysis.(Evaluated by the expert, Validness: 3.5; Novelty: 3.5; Helpfulness: 3.5)</p>
<p>• Hypothesis from MOOSE-base w/ future and past-feedback (MOOSE): Female CMOs' empathy advantage influences their consideration of negative consequences of CSR initiatives, specifically in terms of employee well-being and job security.This relationship is moderated by individual differences in emotional intelligence.Additionally, the organizational culture and industry context will further influence the relationship between empathy advantage and consideration of negative consequences.The hypothesis will investigate whether female CMOs with higher levels of empathy are more likely to prioritize employee well-being and job security in the implementation of CSR initiatives, and whether this relationship is stronger in industries with a stronger emphasis on employee well-being and job security.It will also explore the mediating role of organizational culture and the moderating role of emotional intelligence in shaping the relationship between empathy advantage and consideration of negative consequences.(Evaluated by the expert, Validness: 4.5; Novelty: 4; Helpfulness: 4)</p>
<p>Analysis from the expert:</p>
<p>• H1 falls short of challenging established assumptions or introducing a novel perspective beyond the widely accepted link between customer understanding and profitability.</p>
<p>• Both H2 &amp; H3 center around a specific scenario involving female CMOs in startups and delve into their influence on customer satisfaction, employee behavior, and overall business results.From a research standpoint, this more focused approach points to a potential gap in the existing body of knowledge.Moreover, these two hypotheses surpass conventional understanding by considering how the empathy of female CMOs impacts employee behavior and business outcomes.They put forth a fresh viewpoint, suggesting that cultivating a compassionate rapport with customers, fostered by female CMOs, could positively affect customer engagement, loyalty, and brand recognition.These two hypotheses zoom in on a more specific context, introduce an innovative perspective, and probe a potential void in current research.They are anchored in the dynamic world of innovative business settings and propose a more nuanced and all-encompassing connection between variables.</p>
<p>• H4 retains its relevance within a modern business landscape by scrutinizing the intersection of empathy, CSR initiatives, and the dynamics of organizations.This syncs seamlessly with the criterion of being rooted in an innovative business environment.Moreover, it shakes up established assumptions by considering the potential adverse outcomes of CSR initiatives and the role empathy plays in shaping decision-making within this context.This hypothesis delves into a more intricate and thorough exploration, examining a broader spectrum of factors and interactions within a specific context.Additionally, it imparts a deeper comprehension of the interplay between empathy, business choices, and organizational results.It grapples with a more complex and distinctive scenario, unearths possible gaps in the existing literature, and introduces a new angle on the role of empathy in the realm of business decisions.</p>
<p>A.12 Qualitative Analysis on Two MOOSE-Generated Hypotheses With High Expert Evaluation Scores</p>
<p>In the following two grey boxes are two generated hypotheses from MOOSE with high expert evaluation scores (appended to each hypothesis).The expert's assessment of the two hypotheses is:</p>
<p>Hypothesis 1: The level of personalization in crowdfunding campaign storytelling, the influence of social media influencers who align with the campaign, the presence of trust indicators, and the emotional appeal of the campaign will positively impact potential donors' likelihood of making a donation.Additionally, the timing of donation requests and the type of social media influencers (e.g., celebrities vs. micro-influencers) will moderate this relationship.The perceived risk associated with the crowdfunding campaign will negatively moderate the relationship between the emotional appeal and donation likelihood.(Validness: 4.5; Novelty: 4.5; Helpfulness: 4.5)</p>
<p>Hypothesis 2: Limited financial resources and limited access to networks and markets of women entrepreneurs in the manufacturing sector in developing countries may negatively impact their investment in corporate social responsibility (CSR) initiatives that promote gender equality in host countries.This relationship is further influenced by the intersectionality of gender and race, with women of color facing additional challenges.Additionally, the hypothesis considers the role of institutional factors, such as legal frameworks and policies, and the influence of patriarchal structures on women entrepreneurs' ability to invest in CSR initiatives.(Validness: 3.5; Novelty: 4.0; Helpfulness: 4.0)</p>
<p>These two hypotheses both present a comprehensive view of the research narrative.It encompasses multiple hypotheses, including the primary one, as well as the mediation effect, which serves to elucidate the causal connection between the independent and dependent variables.Concurrently, both hypotheses outline the range of the effect -namely, the circumstances in which this effect is applicable, under which scenarios where it might be weakened, and under which situation it could potentially be inverted.</p>
<p>In terms of novelty: 1. Limited prior research or a gap in the existing literature.This means that there is a dearth of studies or information available on the subject, making it an unexplored area.2. Based on a new business setting.It is grounded in an innovative business environment, characterized by novel technologies, contemporary themes, and evolving business requirements.3. The topic offers a fresh and unique perspective that goes beyond conventional understanding.It might challenge existing assumptions, propose new theories, or present an unconventional approach.</p>
<p>A.13 Essential Factors for Good Social Science (and Business) Hypotheses</p>
<p>According to business PhD students, counter-intuitive and novel hypotheses are the mostly favoured (by top business journals).Intuitive and novel hypotheses are also good but not as good as the counter-intuitive ones.Here "novel" refers to "not pointed out by existing literatures".</p>
<p>Empirically they think of all the hypotheses on top business journals, around 20% are counter-intuitive, leaving the remaining 80% intuitive.</p>
<p>Counter-intuitive hypotheses tend to receive a lower validness evaluation compared to intuitive ones.For this reason, we highlight the counter-intuitive hypothesis in §6.4,even if it receives a lower score in validness than hypotheses in §A.12.</p>
<p>A.14 An Example of Hypothesis Formulation via MOOSE</p>
<p>Here we show a complete flow of hypothesis discovery, by giving the output of relevant modules.Specifically, the found background, found inspirations, generated suggestions, generated hypothesis, reality feedback, novelty feedback, and clarity feedback are copied in this section.</p>
<p>• Found background:</p>
<p>Blind boxes, in terms of how they reach the consumers, are an innovation.Digging deeper into the issue, you will learn the very marketing of blind boxes is called probabilistic selling.That is to say, even after making the payment for a blind box, the buyer still has no idea what will be the contents inside.Suspense.A sense of thrill amid an air of expectation ensue.The sheer anticipation of a surprise can be intoxicating even as it can keep the consumer on tenterhooks, in a nice sort of way.</p>
<p>• Found inspirations:</p>
<p>Inspiration 0: "Surprise is like crack for your brain.Scientists at Emory and Baylor used MRIs to measure changes in human brain activity in response to a sequence of pleasurable stimuli, using fruit juice and water.The patterns of juice and water squirts were either predictable or completely unpredictable.Contrary to the researchers' expectations, the reward pathways in the brain responded most strongly to the unpredictable sequence of squirts."Inspiration 1: "According to Tmall, China's largest B2C online marketplace, about 200,000 post-95 consumers bought blind boxes from its platform in 2019, with each spending CNY 20,000 per year on average; some fanatics even shelled out CNY 1 million on this "game of chances" to get the coveted limited-edition toys."Inspiration 2: "Product differentiation is fundamentally a marketing strategy to encourage the consumer to choose one brand or product over another in a crowded field of competitors."Inspiration 3: "The pandemic has shown how a lack of solid statistics can be dangerous.But even with the firmest of evidence, we often end up ignoring the facts we don't like" Inspiration 4: "User-generated content (also known as UGC or consumer-generated content) is original, brand-specific content created by customers and published on social media or other channels."Inspiration 5: "Higher Star Ratings Lead to More Orders.More stars really do lead to more sales.Thedata shows that products with higher star reviews have a much higher volume of purchases."</p>
<p>• Suggestions from Hypothesis Suggestor Module: Suggestion 1: Combining the background information on blind boxes and probabilistic selling with Inspiration 1 (surprise as a powerful reward stimulus) and Inspiration 2 (high spending on blind boxes by young consumers), a novel business research hypothesis could be: The element of surprise and unpredictability in probabilistic selling, such as blind boxes, triggers stronger reward responses in the brain, leading to higher consumer spending and loyalty, particularly among younger demographics.This hypothesis suggests that the psychological impact of surprise can be a significant driver of consumer behavior and purchasing decisions, especially for novel product categories.Suggestion 2: Integrating the background context with Inspiration 3 (product differentiation strategy) and Inspiration 5 (user-generated content), another innovative hypothesis could be: Encouraging user-generated content around the surprise and reveal aspects of blind box products can serve as a powerful product differentiation strategy, setting brands apart in a competitive market.By leveraging the excitement and social sharing around the unboxing experience, businesses can create a unique brand identity and foster a sense of community among their customers.Suggestion 3: Drawing from the background information, Inspiration 4 (the importance and challenges of evidence-based decision making), and Inspiration 6 (the impact of star ratings on sales), a third hypothesis could be: Despite the proven effectiveness of probabilistic selling in driving consumer engagement and sales, businesses may be reluctant to adopt this strategy due to a lack of traditional, evidence-based metrics and the perceived risks associated with unpredictable outcomes.This hypothesis highlights the potential tension between data-driven decision making and the benefits of innovative, surprise-based selling strategies.</p>
<p>• Hypothesis from Hypothesis Proposer Module:</p>
<p>User-generated content, such as unboxing videos and reviews, can significantly influence the purchasing decisions of potential blind box buyers, particularly among younger demographics.The excitement and social sharing around the unboxing experience, as depicted in user-generated content, can amplify the psychological reward of surprise and drive higher sales, making it an effective tool for businesses to differentiate their products and increase consumer engagement in the blind box market.</p>
<p>• Reality Feedback:</p>
<p>The given hypothesis about the influence of user-generated content on blind box purchasing decisions among younger demographics seems to reflect reality to a certain extent.However, there are a few points to consider that might suggest the hypothesis does not entirely capture the reality:</p>
<ol>
<li>
<p>Generalization: The hypothesis assumes that user-generated content significantly influences all younger demographics.In reality, the impact may vary depending on factors such as personal preferences, cultural background, and socio-economic status.</p>
</li>
<li>
<p>Other influential factors: While user-generated content can be an important factor in purchasing decisions, it may not be the sole or most significant driver.Other aspects like product quality, price, brand reputation, and peer influence might play equally important or even more crucial roles.</p>
</li>
<li>
<p>Limited scope: The hypothesis focuses on the excitement and social sharing aspect of unboxing experiences.However, it does not consider the potential negative impact of user-generated content, such as disappointing unboxing experiences or negative reviews that could discourage purchases.</p>
</li>
<li>
<p>Assumption of psychological reward: The hypothesis assumes that the surprise element in blind box purchases always leads to a positive psychological reward.In reality, some consumers might experience disappointment or frustration if they do not receive the desired item, which could deter future purchases.5. Lack of empirical evidence: The hypothesis presents a plausible explanation but does not provide concrete empirical evidence</p>
</li>
</ol>
<p>• Novelty Feedback:</p>
<p>Based on the provided inspirations and the possibly related business literature paragraph, the research hypothesis appears to be novel and not a direct semantic copy of any of the given information.The hypothesis combines insights from various sources, such as the psychological impact of surprise, the popularity of blind boxes among younger demographics, the role of product differentiation, and the influence of user-generated content on purchasing decisions.While the hypothesis draws from these inspirations, it synthesizes them into a unique and specific context related to the blind box market and the impact of user-generated content on consumer engagement and sales.The provided literature paragraph discusses language modeling and large language models, which is not directly related to the hypothesis.To further enhance the novelty of the hypothesis, you could consider the following suggestions:</p>
<ol>
<li>
<p>Explore the potential long-term effects of user-generated content on brand loyalty and customer retention in the blind box market.</p>
</li>
<li>
<p>Investigate the role of social media platforms and influencers in amplifying the impact of usergenerated content on blind box sales.</p>
</li>
<li>
<p>Examine the potential differences in the effectiveness of user-generated content across various product categories within the blind box market (e.g., collectible toys, beauty products, or snacks).</p>
</li>
<li>
<p>Analyze the impact of user-generated content on the perceived value and collectibility of blind box items among consumers.</p>
</li>
</ol>
<p>• Clarity Feedback:</p>
<p>The given hypothesis is relatively clear and specific, as it identifies the relationship between usergenerated content and purchasing decisions of potential blind box buyers.It also specifies the target demographic (younger buyers), the psychological factors at play (excitement, surprise, and social sharing), and the potential benefits for businesses (product differentiation and increased consumer engagement).However, there is still room for improvement in terms of specificity.To make the hypothesis more specific, consider the following:</p>
<ol>
<li>
<p>Quantify the extent of the influence of user-generated content on purchasing decisions (e.g., "can increase the likelihood of purchase by X").</p>
</li>
<li>
<p>Specify the age range of the "younger demographics" (e.g., "particularly among buyers aged 18-34").</p>
</li>
<li>
<p>Clarify which types of user-generated content have the most significant impact (e.g., "especially unboxing videos that showcase the excitement and surprise of the experience").</p>
</li>
<li>
<p>Indicate the platforms or channels where user-generated content is most effective (e.g., "on social media platforms such as Instagram and TikTok").</p>
</li>
<li>
<p>Provide a more detailed explanation of how user-generated content amplifies the psychological reward (e.g., "by creating a sense of anticipation and vicarious experience for potential buyers").6.Quantify the potential</p>
</li>
</ol>
<p>A.15 Future Directions</p>
<p>This work discovered the possibility of LLMs to propose novel research hypotheses.But it mainly focuses on the social science and business disciplines.It would be very interesting to investigate how LLMs can induce novel hypotheses for other disciplines (especially nature science domains).</p>
<p>In addition, the MOOSE framework could be further improved to induce more valid and novel hypotheses for social science and business domains.</p>
<p>From the aspect of human-AI interaction, it would be also interesting to see how MOOSE can act as an AI Copilot to assist scientists in hypothesis discovery.</p>
<p>A.16 Full Algorithm of the MOOSE Framework</p>
<p>Algorithm 1 shows the full algorithm of the proposed framework.end for 24: end for 25: return H</p>
<p>Figure 1 :
1
Figure 1: Comparison of the two paradigms for scientific hypotheses formulation.The new paradigm shows the role of the MOOSE framework (scientist's copilot) and the new task setting of hypothetical induction.</p>
<p>Figure 2 :
2
Figure 2: A selected hypothesis in a social science publication collected in our dataset.</p>
<p>Figure 3 :
3
Figure 3: Hypothetical development section and a particular theory subsection for developing hypotheses.</p>
<p>Figure 4 :
4
Figure4: MOOSE: Our multi-module framework for TOMATO task.The black part is the base framework; orange part represents past-feedback.; green part represents present-feedback; blue part represents future-feedback.Each capitalized letter represents the generation of one of the modules.The same capitalized letter represents the same regardless of its color.If a module has an input arrow pointing in with a capitalized letter, it represents that this module utilizes one of its previous modules' generation (which has the same letter pointing out) as input.</p>
<p>Algorithm 1
1
Algorithm for MOOSE Input: Raw web corpus C, related surveys S Parameter: Total iterations for past-feedback M , total iterations for present-feedback N Output: A list of hypotheses H 1: for c in C do iteration t ∈ 0...N do 18:cf , rf , nf = Clarity_Checker(h), Reality_Checker(h), Novelty_Checker(h, S)19: present_f = [cf , rf , nf ] 20: h = Hypothesis_Proposer(b, i, s, h, present_f )</p>
<p>In our setting, when a customer watches his or her preceding customers use FR payment technology to check out, his or her own belief regarding the quality of FR and QR payment technologies may be discounted.When a customer steps up to the self-checkout POS machine next, he or she is more likely to follow the preceding customers and choose FR payment technology.Hence, we develop the following hypothesis.Customers whose preceding customers use FR payment technology are more likely to use FR payment technology than those whose preceding customers do not use FR payment technology.
Hypothesis 2.t technology use maytomers before him ors behind him or her,e payment choice ofbservation may influ-nology for the focalople tend to herd be-e majority is the best,eir own choice (Bane-e process of herding,ing the quality of dif-en ignored (Banerjeen widely documentedature on informationdfunding (Zhang andt al. 2020), and online</p>
<p>Table 1 :
1
Statistics of subject distribution of the dataset.
Communication5Psychology7Human Resource Management 8Social ScienceInformation System8International Business5Management6Marketing11</p>
<p>Table 2 :
2
Effect of MOOSE-base, future-feedback and past-feedback (evaluated by GPT-4).MOOSE-related results are averaged over iterations of present-feedback.Base model is GPT-3.5.
Validness Novelty HelpfulnessBaseline3.9542.4833.489MOOSE-base3.9073.0813.859w/ future-feedback3.9553.2263.953w/ future-and past-feedback3.9163.3903.931Validness Novelty HelpfulnessMOOSE (w/o present-feedback)3.8233.1143.809w/ 1 iteration of present-feedback3.9183.1993.900w/ 2 iterations of present-feedback3.9513.2933.956w/ 3 iterations of present-feedback3.9693.2703.962w/ 4 iterations of present-feedback3.9703.3293.951</p>
<p>Table 3 :
3
Effect of present-feedback (evaluated by GPT-4).Base model is GPT-3.5.</p>
<p>Table 4 :
4
Effect of MOOSE-base, future-feedback and past-feedback (evaluated by experts).MOOSE results are selected from the 5 th iteration of present-feedback.Base model is GPT-3.5.
Validness Novelty HelpfulnessBaseline3.5792.2762.632MOOSE-base3.5002.8553.026w/ future-feedback3.6453.1053.303w/ future-and past-feedback3.7503.1973.368Validness Novelty HelpfulnessMOOSE-base (w/o present-feedback)3.3422.3822.500w/ 2 iterations of present-feedback3.5392.8032.934w/ 4 iterations of present-feedback3.5002.8553.026MOOSE (w/o present-feedback)3.2242.7372.855w/ 2 iterations of present-feedback3.5793.2503.342w/ 4 iterations of present-feedback3.7503.1973.368</p>
<p>Table 5 :
5
Effect of present-feedback (evaluated by experts).Base model is GPT-3.5.</p>
<p>Table 6 :
6
Analysis of retrieval's effect on generated hypotheses (evaluated by GPT-4).No methods here utilize any feedback mechanisms.Base model is GPT-3.5.
Validness Novelty Helpfulness</p>
<p>Table 7 :
7
More ablation study (evaluated by GPT-4).Results are averaged over iterations of present-feedback.Base model is GPT-3.5.</p>
<p>Table 10 :
10
Statistics of the complexity of the dataset.
Validness Novelty HelpfulnessHard Consistency0.2980.3370.361Soft Consistency0.7550.7930.791</p>
<p>Table 11 :
11
Hard and soft consistency scores between evaluation from different experts in terms of Validness, Novelty, and Helpfulness metrics.</p>
<p>Table 12 :
12
Hard and soft consistency scores between expert evaluation and GPT-4 evaluation in terms of Validness, Novelty, and Helpfulness metrics.
Validness Novelty HelpfulnessHard Consistency0.4850.3920.321Soft Consistency0.8500.8230.773</p>
<p>Table 13 :
13
Evaluation standard for Validness.</p>
<p>Table 14 :
14
Evaluation standard for Novelty.</p>
<p>Table 15 :
15
Evaluation standard for Helpfulness.</p>
<p>Dataset, code, and generated  hypotheses are available at https://github.com/ZonglinY/MOOSE.git.
AcknowledgementThis research/project is supported by the Ministry of Education, Singapore under its MOE Academic Research Fund Tier 2 (STEM RIE2025 Award MOE-T2EP20123-0005).We thank Qingyun Wang, Jinjie Ni, Xulang Zhang, and Qika Lin for their insightful comments on the first finished draft of this work.
Ai Anthropic, The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. 2024</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>Emergent autonomous scientific research capabilities of large language models. A Daniil, Robert Boiko, Gabe Macknight, Gomes, 10.48550/arXiv.2304.05332CoRR, abs/2304.053322023</p>
<p>COMET: Commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, 10.18653/v1/P19-1470Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, Italy2019Association for Computational Linguistics</p>
<p>Sam Andres M Bran, Andrew D Cox, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Casebased reasoning for natural language queries over knowledge bases. Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew Mccallum, 10.18653/v1/2021.emnlp-main.755Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Improving convenience or saving face? an empirical analysis of the use of facial recognition payment technology in retail. Jia Gao, Ying Rong, Xin Tian, Yuliang Yao, 2023Information Systems Research</p>
<p>The reasoning brain: The interplay between cognitive neuroscience and theories of reasoning. Vinod Goel, Gorka Navarrete, Ira A Noveck, Jérôme Prado, 2017</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, Joelle Pineau, 10.18653/v1/D16-1230Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, 10.48550/arXiv.2303.17651CoRR, abs/2303.176512023</p>
<p>A little survey of induction. John D Norton, 10.48550/arXiv.2303.08774CoRR, abs/2303.08774OpenAI. 2023. GPT-4 technical report. 2003</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022In NeurIPS</p>
<p>Annotating and learning event durations in text. Feng Pan, Rutu Mulkar-Mehta, Jerry R Hobbs, 10.1162/COLI_a_00075Comput. Linguistics. 3742011</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao, 10.48550/arXiv.2302.12813CoRR, abs/2302.128132023</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, 10.48550/arXiv.2210.03350CoRR, abs/2210.033502022</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, 10.48550/ARXIV.2311.05965CoRR, abs/2311.059652023</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Reflexion: an agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, 10.48550/arXiv.2303.11366CoRR, abs/2303.113662023</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023a</p>
<p>Learning to generate novel scientific directions with contextualized literature-based discovery. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.48550/arXiv.2305.14259CoRR, abs/2305.142592023b</p>
<p>Re3: Generating longer stories with recursive reprompting and revision. Kevin Yang, Yuandong Tian, Nanyun Peng, Dan Klein, 10.18653/v1/2022.emnlp-main.296Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers</p>
<p>End-to-end case-based reasoning for commonsense knowledge base completion. Zonglin Yang, Xinya Du, Erik Cambria, Claire Cardie, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023a</p>
<p>Jinjie Ni, and Erik Cambria. 2023b. Logical reasoning over natural language as knowledge representation: A survey. Zonglin Yang, Xinya Du, Rui Mao, 10.48550/arXiv.2303.12023CoRR, abs/2303.12023</p>
<p>Improving event duration prediction via time-aware pre-training. Zonglin Yang, Xinya Du, Alexander Rush, Claire Cardie, 10.18653/v1/2020.findings-emnlp.302Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, 10.48550/arXiv.2302.14233CoRR, abs/2302.142332023</p>            </div>
        </div>

    </div>
</body>
</html>
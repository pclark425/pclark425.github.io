<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-966 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-966</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-966</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-fb71224cf4c53bd3f5cbc3610288e07ac90f862f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fb71224cf4c53bd3f5cbc3610288e07ac90f862f" target="_blank">Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Bioinformatics and Biomedicine</p>
                <p><strong>Paper TL;DR:</strong> A series of simulation studies investigating the performance of different resampling methods as indicators of confidence in discovered graph features found that subsampling and sampling with replacement both performed surprisingly well, suggesting that they can serve as grounds for confidence in graph features.</p>
                <p><strong>Paper Abstract:</strong> Causal discovery can be a powerful tool for investigating causality when a system can be observed but is inaccessible to experiments in practice. Despite this, it is rarely used in any scientific or medical fields. One of the major hurdles preventing the field of causal discovery from having a larger impact is that it is difficult to determine when the output of a causal discovery method can be trusted in a real-world setting. Trust is especially critical when human health is on the line. In this paper, we report the results of a series of simulation studies investigating the performance of different resampling methods as indicators of confidence in discovered graph features. We found that subsampling and sampling with replacement both performed surprisingly well, suggesting that they can serve as grounds for confidence in graph features. We also found that the calibration of subsampling and sampling with replacement had different convergence properties, suggesting that one's choice of which to use should depend on the sample size.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e966.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e966.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy Equivalence Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A score-based causal discovery algorithm that searches over equivalence classes of directed acyclic graphs to find the model that optimizes a chosen model-fit statistic (here BIC with a penalty discount). It is fast and scalable and assumes no cycles or unmeasured confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning equivalence classes of bayesian-network structures.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Greedy Equivalence Search (GES)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Greedy Equivalence Search performs a two-phase greedy search in the space of equivalence classes of DAGs: a forward phase adding edges that improve a score (here BIC with penalty discount d=2) and a backward phase removing edges to further improve the score. Likelihoods are computed by MLE after fitting parameters; model complexity is penalized by d*k*log(n). Implementation used: R package 'Causality' running GES with BIC and penalty discount=2.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated causal data (linear Gaussian networks, Hepar2 and Child expert models)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive simulated environments: (1) random linear Gaussian DAGs with 100 variables/100 edges generated via Tetrad and sampled at varying n (100–1000), and (2) two expert Bayesian network models (Hepar2, Child) with data simulated via bnlearn; not an open-ended or interactive virtual lab, but a repeated-sampling simulation study.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>measurement noise and sampling variability implicitly present via finite samples; paper does not describe explicit handling of latent confounders or distractor variables for GES (assumes no unmeasured common causes).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline GES (no resampling ensemble) reported similar Structural Hamming Distance (SHD) to the jackknife ensemble and was often outperformed by the bootstrap ensemble at larger sample sizes; exact numeric SHD values depend on simulation and are reported in figures (SHD small for linear Gaussian, large (≈90–130) for Hepar2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GES was the base causal discovery algorithm used throughout; performance varied by data regime (good on linear Gaussian, poor on expert categorical models). The paper did not introduce explicit distractor-robust modifications to GES; instead, it evaluated how resampling ensembles around GES can provide confidence estimates for discovered features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e966.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e966.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrap ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nonparametric bootstrap ensemble (bagging) applied to GES</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sampling with replacement to the original sample size to produce m=200 bootstrapped datasets, applying GES to each, and using feature frequencies (edge vote proportions) as forecasts and for ensemble graph construction (majority vote); used both to produce a single bagged graph and to produce calibrated probabilities for edges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bootstrap methods: Another look at the jackknife.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Bootstrap ensemble (nonparametric bootstrap + bagging for causal graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Nonparametric bootstrap: draw m=200 datasets by sampling with replacement to the original sample size, run GES on each resample, compute for each ordered/unordered node pair the proportion of resampled graphs that contain each possible relationship; aggregate via majority vote to form an ensemble graph or treat proportions as probabilistic forecasts for edges. Calibration assessed using Brier score and bias-corrected reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulated causal data (linear Gaussian networks; Hepar2 and Child expert models)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive, repeated-sampling simulation setups described above; resampling performed on each generated dataset to create the ensemble/forecasts. Not an interactive virtual lab; no active interventions were selected by the method.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Implicit: edges that are spurious due to sampling variability or noise tend to have low bootstrap vote frequency; the ensemble procedure thus identifies (and via voting downweights) edges with low support across resamples (i.e., variable-selection-by-frequency).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Sampling variability, measurement noise, model-selection instability; does not explicitly address latent confounding or selection bias beyond what GES assumes.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses bootstrap vote frequencies as a diagnostic: edges with low bootstrap frequency are treated as likely spurious; calibration plots (forecast frequency vs actual frequency) and Brier/reliability scores quantify trustworthiness of these frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Majority-vote bagging and using edge vote proportions to downweight low-support edges (ensemble graph takes highest-vote relationship; forecasts can threshold or weight by frequency).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Statistical calibration checks (Brier score and bias-corrected reliability) and calibration plots are used to refute/assess the validity of edge forecasts; low calibrated probabilities indicate likely spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Linear Gaussian: bootstrap forecasts had low Brier scores (≤ ~0.05 for most sample sizes) and bias-corrected reliability never exceeded 0.01 (typically within ~10%); ensemble SHD improved over raw GES at larger sample sizes (bootstrap worse than jackknife at small n but better at larger n, crossover ~n=300–400 or higher). Expert models (Hepar2, Child): bootstrap generally outperformed jackknife and raw GES but absolute performance was poor (SHD large, e.g., Hepar2 SHD between ≈90–130); bootstrap calibration remained relatively good on Hepar2 but poor on Child.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to raw GES, bootstrap ensemble sometimes reduced SHD at higher sample sizes (linear Gaussian) but performed worse at lowest sample sizes; on difficult expert-model tasks bootstrap improved over raw GES modestly but all methods had poor recall.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bootstrap ensemble provides useful probabilistic edge forecasts with generally good calibration in linear Gaussian simulations and some expert-model settings (Hepar2), enabling detection/downweighting of low-support (potentially spurious) edges via vote frequencies and Brier/reliability evaluation. However, bootstrap performance depends strongly on sample size and problem difficulty and does not explicitly correct for latent confounding or selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e966.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e966.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jackknife ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delete-d jackknife ensemble (sampling without replacement to 90% of data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Resampling without replacement deleting 10% of observations repeatedly (200 resamples), applying GES to each resampled dataset, and using vote frequencies for ensemble graph construction and forecasting; compared directly to bootstrap in calibration and ensemble accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Jackknife ensemble (delete-d jackknife + bagging for causal graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Delete-d jackknife: draw m=200 subsamples without replacement to 90% of original size, run GES on each subsample, compute feature frequencies across resamples and aggregate via majority vote or use frequencies as forecasts; designed to avoid some bootstrap biases (per Steck & Jaakkola) and tends to produce sparser models.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulated causal data (linear Gaussian; Hepar2 and Child expert models)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive repeated-sampling simulation; jackknife resamples analyzed similarly to bootstrap resamples; not an active experimental environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Implicit via vote frequencies: jackknife tends to favor sparser graphs and overestimates edge frequencies in some cases; low-support edges across jackknife resamples can be identified as potentially spurious.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Sampling variability and model-selection instability; explicitly not designed for latent confounding or selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses subsample vote frequencies and calibration analysis (Brier/reliability) to detect unstable/spurious edges; calibration plots used to compare forecasted vs actual accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Aggregation via majority vote and use of frequency thresholds to exclude low-support edges; jackknife's tendency towards sparser models acts as an implicit downweighting.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Calibration failure (overconfident forecasts relative to actual frequency) is used as evidence that particular edges are unreliable; no explicit statistical refutation beyond calibration metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Linear Gaussian: jackknife had good Brier scores (≤ ~0.05) and very low bias-corrected reliability (≤ ~0.003, i.e., typically within ~5%), and its reliability appeared relatively independent of sample size; ensemble SHD similar to raw GES. Expert models: jackknife performed worse than bootstrap on Hepar2 and Child, showing poor reliability and overestimating edge probabilities; jackknife calibration was noisier and more overconfident.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to raw GES, jackknife ensemble had similar SHD in linear Gaussian sims and did not consistently improve over raw GES on expert-model tasks; jackknife often produced sparser graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Jackknife ensembles produce well-calibrated forecasts in easy (linear Gaussian) settings and tend to prefer sparser models, but can be overconfident and unreliable on harder categorical expert-model tasks. Their calibration appears less sensitive to sample size than bootstrap in the simulations studied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e966.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e966.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Resampling-as-diagnostic (Brier & reliability)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Resampling frequency forecasts evaluated with Brier score and bias-corrected reliability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Treat edge vote frequencies from resampling ensembles as probabilistic forecasts and measure calibration/accuracy using the Brier score and a bias-corrected reliability decomposition to detect and quantify spurious signals and over/under-confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Verification of forecasts expressed in terms of probability.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Resampling frequency forecasting evaluated by Brier score and bias-corrected reliability</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute for each candidate edge the proportion of resamples in which the edge appears (forecasted probability). Evaluate forecast accuracy with Brier score (mean squared error) and its bias-corrected reliability component (how close forecast probabilities are to observed frequencies) to judge whether vote frequencies reliably indicate true edges or are spuriously high/low.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated repeated-sampling environments described in paper</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive simulated datasets; resampling frequencies and calibration metrics computed per dataset and aggregated across simulation runs to assess reliability under different sample sizes and data-generating models.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detection via calibration: low calibrated probability or high Brier score identifies edge forecasts that may be spurious; frequency thresholds can be applied to downweight or remove edges with poor calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Sampling variability, model selection instability, miscalibrated confidence in edges; does not explicitly diagnose latent confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Calibration plots (forecast vs actual frequency), Brier score and bias-corrected reliability decomposition detect mismatches between forecasted probabilities and observed frequencies, indicating spurious or unstable edges.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Use of forecast probabilities to threshold or weight edges (e.g., exclude low-probability edges); ensemble majority voting is an implicit downweighting mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Statistical refutation by demonstrating miscalibration (systematic over/underestimation) with Brier/reliability metrics; poor calibration indicates forecasts (and underlying edges) should not be trusted.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Reported numerical summaries: in linear Gaussian simulations bootstrap reliability ≤ 0.01 and jackknife ≤ 0.003; Brier scores often ≤ 0.05. For expert models, bootstrap reliability was good on Hepar2 but poor on Child; overall Brier scores were worse on expert categorical models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Resampling frequencies evaluated with Brier score and bias-corrected reliability provide an interpretable diagnostic for when discovered edges are trustworthy; good calibration indicates the resampling-derived probabilities can be used to detect and downweight spurious edges, but calibration depends strongly on data regime and sample size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e966.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e966.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFCI (Really Fast Causal Inference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Really Fast Causal Inference (RFCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm for learning causal structure in the presence of latent and selection variables; mentioned in related work as having been combined with bootstrapping in prior calibration studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning high-dimensional directed acyclic graphs with latent and selection variables.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RFCI (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>RFCI is a constraint-based algorithm designed to learn partial ancestral graphs when latent confounders or selection bias may be present; in the paper it is only cited via related work where Naeini et al. assessed calibration of causal relationships learned using RFCI plus bootstrapping.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned in related work: Naeini et al. investigated calibration of RFCI with bootstrapping and found directed edges generally well calibrated; this paper did not itself evaluate RFCI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e966.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e966.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrapped hill-climbing & bias-correction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrapped hill-climbing (for Bayesian networks) and Steck & Jaakkola bias correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work in Bayesian network learning used bootstrapped hill-climbing to compute feature frequencies, noting calibration and introducing bias corrections because BIC on bootstrap samples favors overly dense graphs; jackknife found less affected by this bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bias-corrected bootstrap and model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Bootstrapped hill-climbing and bias-corrected bootstrap</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Bootstrapped hill-climbing: apply hill-climbing structure search on bootstrap samples and aggregate feature frequencies. Steck & Jaakkola showed BIC biases on bootstrapped data towards dense graphs and derived a correction; jackknife less affected. These are cited as related methods/results but not reimplemented in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior art: bootstrapping can give well-calibrated feature frequencies for Bayesian network features but bootstrap-induced bias can favor dense models; jackknife may avoid that bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bagging predictors. <em>(Rating: 2)</em></li>
                <li>Bootstrap methods: Another look at the jackknife. <em>(Rating: 2)</em></li>
                <li>An assessment of the calibration of causal relationships learned using rfci and bootstrapping. <em>(Rating: 2)</em></li>
                <li>Bias-corrected bootstrap and model uncertainty. <em>(Rating: 2)</em></li>
                <li>Data analysis with bayesian networks: A bootstrap approach. <em>(Rating: 1)</em></li>
                <li>Learning high-dimensional directed acyclic graphs with latent and selection variables. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-966",
    "paper_id": "paper-fb71224cf4c53bd3f5cbc3610288e07ac90f862f",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "GES",
            "name_full": "Greedy Equivalence Search",
            "brief_description": "A score-based causal discovery algorithm that searches over equivalence classes of directed acyclic graphs to find the model that optimizes a chosen model-fit statistic (here BIC with a penalty discount). It is fast and scalable and assumes no cycles or unmeasured confounders.",
            "citation_title": "Learning equivalence classes of bayesian-network structures.",
            "mention_or_use": "use",
            "method_name": "Greedy Equivalence Search (GES)",
            "method_description": "Greedy Equivalence Search performs a two-phase greedy search in the space of equivalence classes of DAGs: a forward phase adding edges that improve a score (here BIC with penalty discount d=2) and a backward phase removing edges to further improve the score. Likelihoods are computed by MLE after fitting parameters; model complexity is penalized by d*k*log(n). Implementation used: R package 'Causality' running GES with BIC and penalty discount=2.",
            "environment_name": "Simulated causal data (linear Gaussian networks, Hepar2 and Child expert models)",
            "environment_description": "Non-interactive simulated environments: (1) random linear Gaussian DAGs with 100 variables/100 edges generated via Tetrad and sampled at varying n (100–1000), and (2) two expert Bayesian network models (Hepar2, Child) with data simulated via bnlearn; not an open-ended or interactive virtual lab, but a repeated-sampling simulation study.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "measurement noise and sampling variability implicitly present via finite samples; paper does not describe explicit handling of latent confounders or distractor variables for GES (assumes no unmeasured common causes).",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": "Baseline GES (no resampling ensemble) reported similar Structural Hamming Distance (SHD) to the jackknife ensemble and was often outperformed by the bootstrap ensemble at larger sample sizes; exact numeric SHD values depend on simulation and are reported in figures (SHD small for linear Gaussian, large (≈90–130) for Hepar2).",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "GES was the base causal discovery algorithm used throughout; performance varied by data regime (good on linear Gaussian, poor on expert categorical models). The paper did not introduce explicit distractor-robust modifications to GES; instead, it evaluated how resampling ensembles around GES can provide confidence estimates for discovered features.",
            "uuid": "e966.0",
            "source_info": {
                "paper_title": "Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Bootstrap ensemble",
            "name_full": "Nonparametric bootstrap ensemble (bagging) applied to GES",
            "brief_description": "Sampling with replacement to the original sample size to produce m=200 bootstrapped datasets, applying GES to each, and using feature frequencies (edge vote proportions) as forecasts and for ensemble graph construction (majority vote); used both to produce a single bagged graph and to produce calibrated probabilities for edges.",
            "citation_title": "Bootstrap methods: Another look at the jackknife.",
            "mention_or_use": "use",
            "method_name": "Bootstrap ensemble (nonparametric bootstrap + bagging for causal graphs)",
            "method_description": "Nonparametric bootstrap: draw m=200 datasets by sampling with replacement to the original sample size, run GES on each resample, compute for each ordered/unordered node pair the proportion of resampled graphs that contain each possible relationship; aggregate via majority vote to form an ensemble graph or treat proportions as probabilistic forecasts for edges. Calibration assessed using Brier score and bias-corrected reliability.",
            "environment_name": "Same simulated causal data (linear Gaussian networks; Hepar2 and Child expert models)",
            "environment_description": "Non-interactive, repeated-sampling simulation setups described above; resampling performed on each generated dataset to create the ensemble/forecasts. Not an interactive virtual lab; no active interventions were selected by the method.",
            "handles_distractors": null,
            "distractor_handling_technique": "Implicit: edges that are spurious due to sampling variability or noise tend to have low bootstrap vote frequency; the ensemble procedure thus identifies (and via voting downweights) edges with low support across resamples (i.e., variable-selection-by-frequency).",
            "spurious_signal_types": "Sampling variability, measurement noise, model-selection instability; does not explicitly address latent confounding or selection bias beyond what GES assumes.",
            "detection_method": "Uses bootstrap vote frequencies as a diagnostic: edges with low bootstrap frequency are treated as likely spurious; calibration plots (forecast frequency vs actual frequency) and Brier/reliability scores quantify trustworthiness of these frequencies.",
            "downweighting_method": "Majority-vote bagging and using edge vote proportions to downweight low-support edges (ensemble graph takes highest-vote relationship; forecasts can threshold or weight by frequency).",
            "refutation_method": "Statistical calibration checks (Brier score and bias-corrected reliability) and calibration plots are used to refute/assess the validity of edge forecasts; low calibrated probabilities indicate likely spurious edges.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Linear Gaussian: bootstrap forecasts had low Brier scores (≤ ~0.05 for most sample sizes) and bias-corrected reliability never exceeded 0.01 (typically within ~10%); ensemble SHD improved over raw GES at larger sample sizes (bootstrap worse than jackknife at small n but better at larger n, crossover ~n=300–400 or higher). Expert models (Hepar2, Child): bootstrap generally outperformed jackknife and raw GES but absolute performance was poor (SHD large, e.g., Hepar2 SHD between ≈90–130); bootstrap calibration remained relatively good on Hepar2 but poor on Child.",
            "performance_without_robustness": "Compared to raw GES, bootstrap ensemble sometimes reduced SHD at higher sample sizes (linear Gaussian) but performed worse at lowest sample sizes; on difficult expert-model tasks bootstrap improved over raw GES modestly but all methods had poor recall.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Bootstrap ensemble provides useful probabilistic edge forecasts with generally good calibration in linear Gaussian simulations and some expert-model settings (Hepar2), enabling detection/downweighting of low-support (potentially spurious) edges via vote frequencies and Brier/reliability evaluation. However, bootstrap performance depends strongly on sample size and problem difficulty and does not explicitly correct for latent confounding or selection bias.",
            "uuid": "e966.1",
            "source_info": {
                "paper_title": "Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Jackknife ensemble",
            "name_full": "Delete-d jackknife ensemble (sampling without replacement to 90% of data)",
            "brief_description": "Resampling without replacement deleting 10% of observations repeatedly (200 resamples), applying GES to each resampled dataset, and using vote frequencies for ensemble graph construction and forecasting; compared directly to bootstrap in calibration and ensemble accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Jackknife ensemble (delete-d jackknife + bagging for causal graphs)",
            "method_description": "Delete-d jackknife: draw m=200 subsamples without replacement to 90% of original size, run GES on each subsample, compute feature frequencies across resamples and aggregate via majority vote or use frequencies as forecasts; designed to avoid some bootstrap biases (per Steck & Jaakkola) and tends to produce sparser models.",
            "environment_name": "Same simulated causal data (linear Gaussian; Hepar2 and Child expert models)",
            "environment_description": "Non-interactive repeated-sampling simulation; jackknife resamples analyzed similarly to bootstrap resamples; not an active experimental environment.",
            "handles_distractors": null,
            "distractor_handling_technique": "Implicit via vote frequencies: jackknife tends to favor sparser graphs and overestimates edge frequencies in some cases; low-support edges across jackknife resamples can be identified as potentially spurious.",
            "spurious_signal_types": "Sampling variability and model-selection instability; explicitly not designed for latent confounding or selection bias.",
            "detection_method": "Uses subsample vote frequencies and calibration analysis (Brier/reliability) to detect unstable/spurious edges; calibration plots used to compare forecasted vs actual accuracy.",
            "downweighting_method": "Aggregation via majority vote and use of frequency thresholds to exclude low-support edges; jackknife's tendency towards sparser models acts as an implicit downweighting.",
            "refutation_method": "Calibration failure (overconfident forecasts relative to actual frequency) is used as evidence that particular edges are unreliable; no explicit statistical refutation beyond calibration metrics.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Linear Gaussian: jackknife had good Brier scores (≤ ~0.05) and very low bias-corrected reliability (≤ ~0.003, i.e., typically within ~5%), and its reliability appeared relatively independent of sample size; ensemble SHD similar to raw GES. Expert models: jackknife performed worse than bootstrap on Hepar2 and Child, showing poor reliability and overestimating edge probabilities; jackknife calibration was noisier and more overconfident.",
            "performance_without_robustness": "Compared to raw GES, jackknife ensemble had similar SHD in linear Gaussian sims and did not consistently improve over raw GES on expert-model tasks; jackknife often produced sparser graphs.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Jackknife ensembles produce well-calibrated forecasts in easy (linear Gaussian) settings and tend to prefer sparser models, but can be overconfident and unreliable on harder categorical expert-model tasks. Their calibration appears less sensitive to sample size than bootstrap in the simulations studied.",
            "uuid": "e966.2",
            "source_info": {
                "paper_title": "Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Resampling-as-diagnostic (Brier & reliability)",
            "name_full": "Resampling frequency forecasts evaluated with Brier score and bias-corrected reliability",
            "brief_description": "Treat edge vote frequencies from resampling ensembles as probabilistic forecasts and measure calibration/accuracy using the Brier score and a bias-corrected reliability decomposition to detect and quantify spurious signals and over/under-confidence.",
            "citation_title": "Verification of forecasts expressed in terms of probability.",
            "mention_or_use": "use",
            "method_name": "Resampling frequency forecasting evaluated by Brier score and bias-corrected reliability",
            "method_description": "Compute for each candidate edge the proportion of resamples in which the edge appears (forecasted probability). Evaluate forecast accuracy with Brier score (mean squared error) and its bias-corrected reliability component (how close forecast probabilities are to observed frequencies) to judge whether vote frequencies reliably indicate true edges or are spuriously high/low.",
            "environment_name": "Simulated repeated-sampling environments described in paper",
            "environment_description": "Non-interactive simulated datasets; resampling frequencies and calibration metrics computed per dataset and aggregated across simulation runs to assess reliability under different sample sizes and data-generating models.",
            "handles_distractors": null,
            "distractor_handling_technique": "Detection via calibration: low calibrated probability or high Brier score identifies edge forecasts that may be spurious; frequency thresholds can be applied to downweight or remove edges with poor calibration.",
            "spurious_signal_types": "Sampling variability, model selection instability, miscalibrated confidence in edges; does not explicitly diagnose latent confounding.",
            "detection_method": "Calibration plots (forecast vs actual frequency), Brier score and bias-corrected reliability decomposition detect mismatches between forecasted probabilities and observed frequencies, indicating spurious or unstable edges.",
            "downweighting_method": "Use of forecast probabilities to threshold or weight edges (e.g., exclude low-probability edges); ensemble majority voting is an implicit downweighting mechanism.",
            "refutation_method": "Statistical refutation by demonstrating miscalibration (systematic over/underestimation) with Brier/reliability metrics; poor calibration indicates forecasts (and underlying edges) should not be trusted.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Reported numerical summaries: in linear Gaussian simulations bootstrap reliability ≤ 0.01 and jackknife ≤ 0.003; Brier scores often ≤ 0.05. For expert models, bootstrap reliability was good on Hepar2 but poor on Child; overall Brier scores were worse on expert categorical models.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Resampling frequencies evaluated with Brier score and bias-corrected reliability provide an interpretable diagnostic for when discovered edges are trustworthy; good calibration indicates the resampling-derived probabilities can be used to detect and downweight spurious edges, but calibration depends strongly on data regime and sample size.",
            "uuid": "e966.3",
            "source_info": {
                "paper_title": "Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "RFCI (Really Fast Causal Inference)",
            "name_full": "Really Fast Causal Inference (RFCI)",
            "brief_description": "An algorithm for learning causal structure in the presence of latent and selection variables; mentioned in related work as having been combined with bootstrapping in prior calibration studies.",
            "citation_title": "Learning high-dimensional directed acyclic graphs with latent and selection variables.",
            "mention_or_use": "mention",
            "method_name": "RFCI (mentioned)",
            "method_description": "RFCI is a constraint-based algorithm designed to learn partial ancestral graphs when latent confounders or selection bias may be present; in the paper it is only cited via related work where Naeini et al. assessed calibration of causal relationships learned using RFCI plus bootstrapping.",
            "environment_name": null,
            "environment_description": null,
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned in related work: Naeini et al. investigated calibration of RFCI with bootstrapping and found directed edges generally well calibrated; this paper did not itself evaluate RFCI.",
            "uuid": "e966.4",
            "source_info": {
                "paper_title": "Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Bootstrapped hill-climbing & bias-correction",
            "name_full": "Bootstrapped hill-climbing (for Bayesian networks) and Steck & Jaakkola bias correction",
            "brief_description": "Prior work in Bayesian network learning used bootstrapped hill-climbing to compute feature frequencies, noting calibration and introducing bias corrections because BIC on bootstrap samples favors overly dense graphs; jackknife found less affected by this bias.",
            "citation_title": "Bias-corrected bootstrap and model uncertainty.",
            "mention_or_use": "mention",
            "method_name": "Bootstrapped hill-climbing and bias-corrected bootstrap",
            "method_description": "Bootstrapped hill-climbing: apply hill-climbing structure search on bootstrap samples and aggregate feature frequencies. Steck & Jaakkola showed BIC biases on bootstrapped data towards dense graphs and derived a correction; jackknife less affected. These are cited as related methods/results but not reimplemented in this study.",
            "environment_name": null,
            "environment_description": null,
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as prior art: bootstrapping can give well-calibrated feature frequencies for Bayesian network features but bootstrap-induced bias can favor dense models; jackknife may avoid that bias.",
            "uuid": "e966.5",
            "source_info": {
                "paper_title": "Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bagging predictors.",
            "rating": 2
        },
        {
            "paper_title": "Bootstrap methods: Another look at the jackknife.",
            "rating": 2
        },
        {
            "paper_title": "An assessment of the calibration of causal relationships learned using rfci and bootstrapping.",
            "rating": 2
        },
        {
            "paper_title": "Bias-corrected bootstrap and model uncertainty.",
            "rating": 2
        },
        {
            "paper_title": "Data analysis with bayesian networks: A bootstrap approach.",
            "rating": 1
        },
        {
            "paper_title": "Learning high-dimensional directed acyclic graphs with latent and selection variables.",
            "rating": 1
        }
    ],
    "cost": 0.0133735,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration*</h1>
<p>Erich Kummerfeld ${ }^{1}$ and Alexander Rix ${ }^{2}$<br>${ }^{1}$ Institute for Health Informatics, University of Minnesota<br>${ }^{2}$ Department of biostatistics, University of Michigan</p>
<p>October 7, 2019</p>
<h4>Abstract</h4>
<p>Causal discovery can be a powerful tool for investigating causality when a system can be observed but is inaccessible to experiments in practice. Despite this, it is rarely used in any scientific or medical fields. One of the major hurdles preventing the field of causal discovery from having a larger impact is that it is difficult to determine when the output of a causal discovery method can be trusted in a real-world setting. Trust is especially critical when human health is on the line.</p>
<p>In this paper, we report the results of a series of simulation studies investigating the performance of different resampling methods as indicators of confidence in discovered graph features. We found that subsampling and sampling with replacement both performed surprisingly well, suggesting that they can serve as grounds for confidence in graph features. We also found that the calibration of subsampling and sampling with replacement had different convergence properties, suggesting that one's choice of which to use should depend on the sample size.</p>
<h2>1 Introduction</h2>
<p>Many scientific disciplines seek to use controlled experiments to build, contradict, or confirm causal models in their respective fields. It is often not possible to conduct controlled experiments, however, for a variety of practical and ethical reasons. Fields that study human health are especially vulnerable to these limitations. In such circumstances, researchers must rely on observational data. Causal discovery methods provide a way to learn causal information from observational data, and so one might expect causal discovery methods to be heavily utilized in the medical sciences, but at the time of writing this is not the case. One major reason for this is that, unlike in some other machine learning domains, it is difficult</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>to determine whether or not the output of a causal discovery method is trustworthy in a real-world setting. Prediction algorithms can be evaluated relatively simply by, for example, measuring the area under the receiver operator curve (AUC) on a holdout sample. However causal graphs do not lend themselves to such an evaluation. An alternative approach is needed.</p>
<p>In this paper we investigate one approach to evaluating the performance of causal discovery methods: resampling. Resampling methods such as the bootstrap, and less commonly the jackknife, have been used heavily in other areas of statistics and machine learning, and it seems natural to extend this approach to causal discovery. Many of the known statistical properties of the bootstrap and jackknife do not apply in an obvious way to the causal discovery setting, however, leaving open the problem of evaluating resampling as a method for evaluating causal discovery applications.</p>
<h1>1.1 Related work</h1>
<p>We are only aware of one previous publication investigating resampling calibration in the context of causal discovery algorithms. Naeini, Jabbari, and Cooper [15] examined the the calibration of directed edges in a bootstrapped version of the Really Fast Causal Inference [5] algorithm and found that they were generally well calibrated. In this paper, we look at both bootstrapping and jackknifing, use a different simulation setup, use a different base search algorithm, and evaluate all edge types.</p>
<p>Most of the previous work in bootstrapping graphical models has been done in the broader field of learning bayesian networks. Friedman, Goldszmidt, and Wyner successfully used bootstrapped hill-climbing, showed that bootstrapped hill-climbing's edge probabilities were well calibrated, and found that bootstrapped hill-climbing substantially outperformed the unbootstrapped version [9, 10]. Steck and Jaakola showed that when using hill-climbing on bootstrapped data, the Bayesian Information Criterion (BIC) score is biased towards overly dense graphs, sometimes severely, and derived a correction [23]. They also showed that the jackknife is not affected by this bias. It is unclear at this time whether these results extend to causal discovery.</p>
<h3>1.2 Methods</h3>
<p>This paper reports the results of a series of simulations that we ran as a first step towards a better understanding of resampling in the context of causal discovery. Three different types of simulations were run: one simulation randomly generated causal models with random structures and random linear Gaussian parameters, and two simulations used existing expert models meant to be representative of medical phenomena. In these simulations, data were generated from a causal graphical model, and then repeatedly resampled, resulting in large collections of data sets. Each of these resampled data sets was then analyzed with a causal discovery algorithm, producing a corresponding large collection of graphs. We then calculated the proportion of times each pair of variables had a particular kind of relationship, as represented by the edge (or lack thereof) connecting them, within this collection of graphs. Finally, we evaluated these proportions as forecasts of whether the edge was present</p>
<p>or not in the data generating model, and calculated both the full Brier score of the forecasts as well as their reliability (calibration).</p>
<h1>1.3 Results</h1>
<p>In our simulations, subsampling (jackknifing) and resampling with replacement (bootstrapping) performed well both in terms of overall Brier score and in terms of reliability, even at smaller sample sizes, for linear Gaussian networks. Both procedures had poor Brier scores with data simulated from expert models over categorical variables. The jackknife also had poor calibration on the expert models, while the bootstrap had excellent calibration on one expert model and poor calibration on the other. The two procedures appeared to have different convergence properties when the sample size was varied. The bootstrap had better performance than the jackknife in most, but not all, circumstances.</p>
<h3>1.4 Outline</h3>
<p>This paper proceeds as follows. In section 2, a brief background is provided on the material covered in this paper: causal discovery, resampling, and the Brier score, a standard method for evaluating calibration. Section 3 provides the specifications for the simulations we ran. Section 4 provides the outcomes of those simulations, including evaluations of both the ensemble accuracy and calibration of the resampling techniques. The paper concludes with section 5 , which discusses the results, their implications, and their limitations, and considers future directions for this line of research.</p>
<h2>2 Background</h2>
<h3>2.1 Causal discovery</h3>
<p>Causal discovery algorithms estimate the structure of a causal model [17], represented as a graph, using data generated from that model [22]. Figure 1 shows an example of a causal graph. Because of the general structure of the problem, these algorithms have a common high-level form: they take data as input, and produce a graph as output. Beyond that, however, they vary considerably. The field is too large to review here, so we will only discuss the algorithm used in this investigation; for recent introductory papers on causal discovery, see $[6]$ or $[12]$.</p>
<p>Greedy Equivalence Search (GES) [4, 13, 19] is a fast and scalable causal discovery algorithm that is correct when there are no causal cycles or unmeasured common causes. It accomplishes this by performing an intelligent search within the space of all causal models to find the model that optimizes a model fit statistic chosen by the user. The most common fit statistic used for this purpose is the Bayesian Information Criterion (BIC) score:</p>
<p>$$
\mathrm{BIC}=-2 \log L+k \log n
$$</p>
<p>Where $L$ is the likelihood of the data given the model, $n$ is the sample size, and $k$ is the number of free parameters in the model. $L$ is computed after fitting the model's free</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of a causal graph. In this example, Alcohol Use and Gallstones cause Liver Disorder, which in turn causes Fatigue.
parameters to the data with a maximum likelihood estimate (MLE) procedure, so it is the highest likelihood that the model can assign to the data. The implementation of GES used in this paper includes a penalty discount parameter as well, which acts as a tuning parameter for preferring more complex or more simple models by modifying the strength of the second term in the BIC score. This modified BIC score's second term is thus $d k \log n$, where $d$ is the user-assigned value for the penalty discount parameter.</p>
<h1>2.2 Resampling</h1>
<p>The bootstrap was introduced by Efron [7] and comes in two primary forms: the parametric bootstrap fits a (parametric) model to the data and samples from the model, and the non parametric bootstrap instead samples from the empirical distribution function by sampling with replacement from the data. We use only non parametric bootstrapping in this paper. In statistics, the bootstrap can be used for many things, including estimating the standard error of an estimator, and constructing confidence intervals. The delete $d$ jackknife is a similar procedure, where one takes the original sample and randomly deletes $d$ observations from the data. $d$ is usually chosen on the order of $\sqrt{n}$, as small numbers can lead to inconsistency when the statistic of interest is not smooth. We chose to include the jackknife due to [23], where the jackknife performed well and generally chose sparser models than the bootstrap in the the context of Bayesian Networks.</p>
<p>In machine learning the bootstrap has primarily been used as a way to improve the performance of prediction algorithms via bootstrap aggregating, or bagging. Bagging was introduced by Breiman [2] as way to improve the predictive performance of learning algorithms. Bagging involves generating $m$ bootstrap replicates of the original data and employing a rule to condense the $m$ models into a single value. In the case of bagging classifiers, [2] suggests generating a frequency vector and classifying based off the most frequent observation, which is the procedure we used in this paper. In the context of learning Bayesian networks, [10] and [9] define the probability of a graph feature (e.g. individual edges and adjacencies) as</p>
<p>$$
p_{n}(f)=\frac{1}{\left|D_{n}\right|} \sum_{D_{n}} I\left(f \in \hat{G}\left(D_{n}\right)\right)
$$</p>
<p>where $D_{n}$ is any possible data set of size $n$ sampled from the Bayesian network $B$, and $\hat{G}\left(D_{n}\right)$</p>
<p>is the graph learned from $D_{n}$. They use the bootstrap to estimate $p$</p>
<p>$$
p_{n}(f) \approx \frac{1}{m} \sum_{m} I\left(f \in \hat{G}\left(D_{n}^{m}\right)\right)
$$</p>
<p>where $D_{n}^{m}$ is the $m$-th bootstrapped data set. As [11] notes, the resulting feature frequencies from the bagging procedure are not probabilities. We treat these quantities only as frequencies with evaluable calibration.</p>
<p>Resampling adds a few levels of complexity to typical causal discovery. First, there are different resampling procedures to choose from. Second, running the learning algorithm on resampled data can be biased [23], which may require adjustment in other parts of the procedure in order to correct for this bias. Third, there are multiple ways to use the resampling graphs, such as aggregating them into a single object or looking at the distribution of their features. We explore both of these uses in this paper.</p>
<h1>2.3 Brier score and reliability</h1>
<p>In order to evaluate the calibration of these resampling techniques for GES, we treat the resampling frequencies as forecasts, and evaluate their forecasting ability using the Brier score[3], a standard tool for evaluating the performance of probabilistic forecasts for binary events. The Brier score is simply the mean squared error between the predicted probability and the observed outcomes:</p>
<p>$$
\text { Brier Score }=\frac{1}{n} \sum_{s=1}^{n}\left(f_{s}-o_{s}\right)^{2}
$$</p>
<p>where $n$ is the sample size, $f_{s}$ is the forecast probability of the event in sample $s$, and $o_{s}$ is the observed outcome in sample $s$, with 0 meaning the event did not occur and 1 meaning the event did occur. Since this is a measure of error, larger values are worse: the best Brier score possible is 0 , while the worst score possible is 1 .</p>
<p>We also used one of the components that the Brier score can be decomposed into[14], typically called reliability or calibration. Here we will call it reliability, in order to distinguish it from the more general concept of calibration. This component evaluates how close each forecast is to the actual frequency with which the event occurs when that forecast was made:</p>
<p>$$
\text { Reliability }=\frac{1}{n} \sum_{k=1}^{K} n_{k}\left(f_{k}-\bar{o}_{k}\right)^{2}
$$</p>
<p>where $K$ is the number of distinct forecasts, $n_{k}$ is the number of samples where forecast $k$ was made, $f_{k}$ is the forecast probability of the event when forecast $k$ is made, and $\bar{o}_{k}$ is the observed frequency of the event occurring among samples where forecast $k$ was made. Like with the overall Brier score, smaller reliability scores are preferred. The optimal reliability is 0 , while the worst possible reliability is 1 . Ferro and Fricker[8] showed that the standard decomposition of the Brier score is biased, and provided a modified version of reliability with less bias. We used both versions, and found the difference to be minor. We report only the bias-corrected reliability in this paper.</p>
<h1>3 Simulation Specifications</h1>
<h3>3.1 Simulation design</h3>
<p>We implemented a series of simulations to evaluate the performance of resampling as a method for determining confidence in individual applications of causal discovery. The overall structure of these simulations is as follows.</p>
<p>First, a graphical model is selected and its parameters are assigned values. For the linear Gaussian models, the graphical structures were randomly generated, and the parameter values are randomly drawn from defined distributions. For the expert model simulations, both the graphical structures and parameter values were determined by the expert model. Second, data are randomly generated from the model, up to a specified sample size. For all simulations, sample sizes were varied from 100 to 1000 in increments of 100 . Both steps are then repeated, resulting in a collection of matched 〈graph, data〉 pairs. Finally, each data set is analyzed, and the output of that analysis is compared against the matched data-generating graph.</p>
<h3>3.2 Algorithm and resampling specifications</h3>
<p>While we varied our overall analysis techniques, we only made use of one pre-existing causal discovery method, the Greedy Equivalence Search (GES) [4, 13, 19]. GES was always run with the Bayesian Information Criterion (BIC) and a penalty discount of 2, and all other parameters were left at their default values. We used R [18] to automate our application of GES to all simulated data sets. For all simulations we used the implementation of GES found in Rix's open source R package "Causality"1, as we found it to be faster than alternative methods for running GES in R.</p>
<p>We used two different resampling methods. First, based on the bootstrap method, we used sampling with replacement to the full original sample size. Second, based on the jackknife method, we used sampling without replacement to $90 \%$ of the original sample size. In both cases, 200 resampled data sets would be made from the original data set. For the rest of the paper we will refer to these procedures simply as "bootstrap" and "jackknife" procedures.</p>
<p>The resampling methods were used in two different ways. For a direct comparison with standard GES, we used the resampling methods to produce an ensemble estimate of the data generating graph via a bagging procedure. The ensemble estimate is calculated by taking the set of output graphs from the set of resampled data sets, and letting the graphs vote on the relationship between every pair of variables, with the highest vote winning. This produces a partially directed graph as output, but is not guaranteed to be a pattern, or even to be acyclic.</p>
<p>We also used the resampling methods to produce forecasts of the edges of the data generating graph. The forecasts were calculated similar to that of the ensemble estimates, except rather than producing a single graph as output, the output is a table of the proportion of votes each relationship got for each variable pair. These proportions can then be treated as forecasts of the probability that that variable pair has that particular relationship.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.3 Model specifications</h1>
<p>Linear Gaussian models We produced 500 independent 〈graph, data〉 pairs of linear Gaussian models and data sets at each sample size. The data was generated using the graphical user interface of the open source Tetrad software package ${ }^{2}$, version 6.6.0. All graphs were generated with 100 variables and 100 edges. Their structures were randomly selected with the random forward process. Each graph was assigned parameters independently in the following way. Each variable is a weighted sum of the value of its parents in the graph and its own independent noise term. Each variable's independent noise term has a Normal distribution with mean 0 and a variance drawn uniformly between 1 and 3 . The weight of the independent noise term is always 1 , and each incoming edge has a weight drawn from SplitUniform $(-1.5,-0.5 ; 0.5,1.5)$.</p>
<p>Expert models We used the Child [21] and Hepar2 [16] expert models. Data from both was generated using the bnlearn R package [18, 20]. 100 data sets were generated from each model at each sample size.</p>
<h2>4 Results</h2>
<h3>4.1 Linear Gaussian simulation results</h3>
<p>Graph estimation performance To evaluate the impact of using resampling ensembles on GES, we calculated the Structural Hamming Distance (SHD) [24] of the standard GES algorithm as well as the jackknife and bootstrap ensemble methods. SHD is a numeric summary of the edgewise distance of the graph estimate from the data generating graph: each missing edge, extra edge, or wrongly oriented edge adds 1 point to the SHD. Figure 2 shows a plot of the SHD by sample size in our simulations. Since in these simulations the data generating graphs all had 100 nodes and 100 edges, these errors are fairly small, suggesting that our linear Gaussian data generation procedure was relatively easy for GES to learn, even at lower sample sizes.</p>
<p>Perhaps because the learning problem was relatively easy, the difference between the three methods were small. The jackknife ensemble approach had similar SHD to standard GES, while the bootstrap ensemble approach performed worse at the lowest sample size of 100, but by sample size 300 was slightly outperforming the other two methods. We also calculated and plotted the adjacency recall and precision, and arrowhead recall and precision, for these methods, but omit them here as the results were captured by the SHD plot. Those plots are available on request.</p>
<p>Forecasting performance To evaluate the bootstrap and jackknife procedures as methods for forecasting the probability that a given edge was present in the data generating model, we calculated the Brier score and reliability of each method. Figure 3 shows a plot of the Brier scores of the bootstrap and jackknife procedures across all sample sizes. This plot mimics the ensemble performance plot, as the bootstrap procedure had worse Brier scores at</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Average Structural Hamming Distance of algorithm output on raw data compared to data generating model. Although overall performance is good relative to the size of the graphs, errors are still present.
lower sample sizes, but better scores at higher sample sizes. Compared to the ensemble plot, the crossover point also occurs at a higher sample size, as the bootstrap procedure does not catch up to the jackknife procedure until the sample size is at least 400. Overall, though, both methods performed reasonably well, with their Brier scores at or below .05 for most sample sizes.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Plot of the Brier scores of forecasts from the bootstrap and jackknife procedures.
Figure 4 shows a plot of the bias-corrected reliability of the bootstrap and jackknife procedures across all sample sizes. Overall both methods performed well, as the bootstrap procedure never rose above 0.01 , corresponding to forecasts that are typically within $10 \%$ of the observed event frequency, and the jackknife procedure never rose above 0.003 , corresponding to forecasts that are typically within $5 \%$ of the observed event frequency. This plot again mimics the ensemble performance plot and Brier score plot, however the difference</p>
<p>between the methods appears more pronounced, with the bootstrap procedure performing proportionally much worse than the jackknife at sample sizes below 400, but much better at sample sizes above 800 . Also of note, the bootstrap's reliability appears to converge quickly towards 0 as the sample sizes increases, while the jackknife's reliability appears independent of sample size, staying near 0.002 at all sample sizes.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Plot of the bias-corrected reliability (calibration) of the bootstrap and jackknife procedures.</p>
<p>Figure 5 shows a plot of the actual frequency of correct edges against the forecast frequency of those edges being correct, for both methods at sample sizes 100,500 , and 1000 . Visual inspection of this plot also suggests that both methods are well calibrated, but reveals some additional differences in their performance. Forecasts from the bootstrap procedure appear to underestimate the actual frequency, while forecasts from the jackknife procedure appear to overestimate the actual frequency.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of forecast accuracy to actual accuracy for bootstrap and jackknife procedures on simulated linear Gaussian data sets with sample sizes 100, 500, and 1000. The diagonal black line represents perfect forecasts.</p>
<h1>4.2 Expert model simulation results</h1>
<p>Hepar2 The Hepar2 model was developed by Onisko [16] as a model of the causes and effects of liver disorders. It was originally developed to aid in the diagnosis of liver disorders. It has 70 nodes and 123 edges. The model was downloaded from the bnlearn Bayesian Network Repository, and the data were simulated from the model using the bnlearn package $[20]$.</p>
<p>Figure 6 shows the accuracy performance of GES, GES with a bootstrap ensemble, and GES with a jackknife ensemble on the Hepar2 simulated data. All methods performed poorly on all sample sizes tested: SHD ranged between 90 and 130, which is a large number of errors given the size of the hepar2 model. All methods appear to still be in the process of converging by sample size 1000. The bootstrap ensemble outperformed both raw GES and the jackknife ensemble. An inspection of the more detailed recall and precision statistics of these methods revealed that most errors were recall errors. All methods had fairly high precision, meaning that identified edges were typically correct, however all methods also had very low recall, meaning that the methods typically did not find most of the edges in the hepar2 model.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison of Structural Hamming Distance for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model, for sample sizes 100 to 1000. Lower values are better.</p>
<p>Figures 7 and 8 show plots of the resampling procedures' Brier scores and reliability, respectably. The bootstrap procedure performed notably better in both cases. While neither method did as well as we might want on total Brier score at any sample size, the bootstrap procedure's forecasts were still highly reliable at all sample sizes. Somewhat interestingly, neither procedure appears to be converging monotonically towards the ideal Brier score or reliability in the sample sizes tested. For both Brier score and reliability, the jackknife procedure is actually performing worse as the sample size grows, until 300 samples, where it turns and improves continuously up through sample size 1000. For the bootstrap procedure, performance slowly gets worse until around sample size 500, where it seems to stabilize.</p>
<p>Figure 9 shows a calibration plot for both procedures at sample sizes 100, 500, and 1000. Both methods were typically overestimating the likelihood of edges being correct, with the jackknife procedure making more extreme errors of this kind than the bootstrap procedure. The jackknife's calibration performance is also much noisier than the bootstrap's calibration</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparison of Brier score for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model, for sample sizes 100 to 1000. Lower values are better.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Comparison of reliability for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model, for sample sizes 100 to 1000. Lower values are better.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Comparison of forecast accuracy to actual accuracy for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model with sample size 500. The diagonal black line represents perfect forecasts.
child The Child model [21] models the effects of birth asphyxia and the outcomes of some related medical exams. It has 20 nodes and 25 edges. The model was downloaded from the bnlearn Bayesian Network Repository, and the data were simulated from the model using the bnlearn package [20].</p>
<p>Figure 10 shows the accuracy performance of GES, GES with a bootstrap ensemble, and GES with a jackknife ensemble on the Child simulated data. As with the Hepar2 simulation, all methods performed poorly for the sample sizes tested, and appear to still be converging at sample size 1000. The bootstrap ensemble again appears to have the best performance, although the difference is small.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Comparison of Structural Hamming Distance for bootstrap and jackknife procedures on data sets simulated from the child expert model, for sample sizes 100 to 1000. Lower values are better.</p>
<p>Figures 11 and 12 show plots of the Brier scores and reliability, respectably, for both</p>
<p>resampling procedures. As with the Hepar2 simulation, Brier scores were poor overall, the jackknife procedure had poor reliability, and the bootstrap procedure outperformed the jackknife procedure on both measures at all sample sizes. Unlike the Hepar2 simulation, the bootstrap did not have strong reliability in the Child simulation. Also unlike the Hepar2 simulation, the Brier scores and reliability for the bootstrap and jackknife procedures changed in similar ways as the sample size increased from 100 to 1000. Both traced a concave shape, with performance degrading from sample size 100 to approximately sample size 500, where it began to improve in an accelerated manner up to sample size 1000, the maximum tested.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Comparison of Brier score for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model, for sample sizes 100 to 1000. Lower values are better.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Comparison of reliability for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model, for sample sizes 100 to 1000. Lower values are better.</p>
<p>Figure 13 shows a calibration plot for both procedures at sample sizes 100, 500, and 1000. As with the Hepar2 simulation, overestimating actual performance is more common than understimating performance.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Comparison of forecast accuracy to actual accuracy for bootstrap and jackknife procedures on data sets simulated from the Child expert model with sample size 500. The diagonal black line represents perfect forecasts.</p>
<h1>5 Discussion and conclusion</h1>
<h3>5.1 Implications</h3>
<p>In carrying out these simulations, we hoped to begin answering some important questions about resampling in the context of causal discovery. For example, when it comes to estimating the causal graphical model of an unknown causal data generating process, should we prefer off-the-shelf causal discovery methods, or a bootstrap ensemble, or a jackknife ensemble? These simulations suggest that bootstrapping might be preferred, and that the difference between all three options may not be large, at least for the GES algorithm. It also appears to be the case that this question does not have a simple answer: the preferred method might depend on sample size and the difficulty of the task.</p>
<p>Another question is: Should we prefer bootstrapping or jackknifing as indicators of confidence in discovered graph features? As with accuracy, it again appears that the preferred method might depend on sample size and the difficulty of the task. Overall, though, in these simulation bootstrapping had better calibration than jackknifing in most cases.</p>
<p>Finally, can resampling be considered a reliable way to evaluate confidence in discovered graph features? These simulations indicate that in some circumstances it can be. Bootstrapping had very good reliability for both the linear Gaussian and Hepar2 simulations, which is impressive given that the data generating models used in these two simulations are quite different. While its performance on the Child simulation was far from what one would want, the plot suggests that it may improve at higher sample sizes.</p>
<h3>5.2 Limitations and future directions</h3>
<p>Our findings have a number of limitations. First, they may not generalize to other search algorithms, such as PC or FCI [22], as we only tested the GES algorithm. In our limited experience PC performs substantially better when bootstrapped, but we have not carried out a complete simulation study to confirm this. It is also unclear how well resampling on</p>
<p>causal discovery algorithms that consider latent variables performs, although some results on that problem has been produced by [15]. These areas provide a natural extension to the work presented in this paper, and would be useful for some of our current work in medical science domains [1]</p>
<p>Our findings also may not apply to real world problems that are very different from the simulated models and expert models evaluated here, or to data sets with sample sizes outside the range of those we tested. We only looked at sample sizes in the range of 100 to 1000. Sample sizes below 100 should probably be analyzed with caution under any circumstances, but there are many real world data sets with sample sizes much larger than 1000.</p>
<p>We also only considered a small subset of possible kinds of graphical structures. The linear Gaussian simulations were fairly sparse, and we only considered two expert models, which have invariant causal structure. Our findings may not apply to data generating models with graphs that are very dense or very sparse, with small-world graphs, or with graphs structurally different than those we used in our simulations in other ways.</p>
<p>None of our simulations included models with more than 100 variables. As such, our results may not apply to data with thousands of variables or more. We also did not consider continuous variable models with non-Gaussian noise or nonlinear relationships, so our results may not apply to data-generating models that have non-Gaussian noise or nonlinear relationships.</p>
<p>Overall, it is still unclear at this time why calibration may have good performance in some circumstances and bad performance in other circumstances. If resampling is not always well calibrated, we should identify features, ideally identifiable from the data themselves or from easily accessible meta-data, that determine the calibration of these resampling methods. Extending these simulations to other algorithms and other data-generating models will help us to understand what those features might be.</p>
<h1>Acknowledgment</h1>
<p>The authors would like to thank the Center for Causal Discovery for developing and supporting the open source Tetrad software package.</p>
<h2>References</h2>
<p>[1] Anker, J. J., Kummerfeld, E., Rix, A., Burwell, S. J., and Kushner, M. G. Causal network modeling of the determinants of drinking behavior in comorbid alcohol use and anxiety disorder. Alcoholism: clinical and experimental research 43, 1 (2019), $91-97$.
[2] Breiman, L. Bagging predictors. Machine learning 24, 2 (1996), 123-140.
[3] Brier, G. W. Verification of forecasts expressed in terms of probability. Monthly weather review 78, 1 (1950), 1-3.
[4] Chickering, D. M. Learning equivalence classes of bayesian-network structures. Journal of machine learning research 2, Feb (2002), 445-498.</p>
<p>[5] Colombo, D., Maathuis, M. H., Kalisch, M., and Richardson, T. S. Learning high-dimensional directed acyclic graphs with latent and selection variables. The Annals of Statistics (2012), 294-321.
[6] Eberhardt, F. Introduction to the foundations of causal discovery. International Journal of Data Science and Analytics 3, 2 (2017), 81-91.
[7] Efron, B. Bootstrap methods: Another look at the jackknife. Ann. Statist. 7, 1 (01 1979), 1-26.
[8] Ferro, C. A., and Fricker, T. E. A bias-corrected decomposition of the brier score. Quarterly Journal of the Royal Meteorological Society 138, 668 (2012), 1954-1960.
[9] Friedman, N., Goldszmidt, M., and Wyner, A. Data analysis with bayesian networks: A bootstrap approach. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence (1999), Morgan Kaufmann Publishers Inc., pp. 196-205.
[10] Friedman, N., Goldszmidt, M., and Wyner, A. J. On the application of the bootstrap for computing confidence measures on features of induced bayesian networks. In AISTATS (1999).
[11] Hastie, T., Tibshirani, R., and Friedman, J. The Elements of Statistical Learning. Springer Series in Statistics. Springer New York Inc., New York, NY, USA, 2001.
[12] Malinsky, D., and Danks, D. Causal discovery algorithms: A practical guide. Philosophy Compass 13, 1 (2018), e12470.
[13] Meek, C. Causal inference and causal explanation with background knowledge. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (San Francisco, CA, USA, 1995), UAI'95, Morgan Kaufmann Publishers Inc., pp. 403-410.
[14] Murphy, A. H. A new vector partition of the probability score. Journal of applied Meteorology 12, 4 (1973), 595-600.
[15] Naeini, M. P., Jabbari, F., and Cooper, G. An assessment of the calibration of causal relationships learned using rfci and bootstrapping. In 4th Workshop on Data Mining for Medical Informatics: Causal Inference for Health Data Analytics (2017).
[16] Onisko, A. Probabilistic causal models in medicine: Application to diagnosis of liver disorders. In Ph. D. dissertation, Inst. Biocybern. Biomed. Eng., Polish Academy Sci., Warsaw, Poland (2003).
[17] Pearl, J. Causality: models, reasoning and inference, vol. 29. Springer, 2000.
[18] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2017.</p>
<p>[19] Ramsey, J., Glymour, M., Sanchez-Romero, R., and Glymour, C. A million variables and more: the fast greedy equivalence search algorithm for learning highdimensional graphical causal models, with an application to functional magnetic resonance images. International journal of data science and analytics 3, 2 (2017), 121-129.
[20] Scutari, M. Learning bayesian networks with the bnlearn R package. Journal of Statistical Software 35, 3 (2010), 1-22.
[21] SPIEGELHALTER, D. J. Learning in probabilistic expert systems. Bayesian statistics 4 (1992), 447-465.
[22] Spirtes, P., Glymour, C. N., Scheines, R., Heckerman, D., Meek, C., Cooper, G., and Richardson, T. Causation, prediction, and search. MIT press, 2000 .
[23] Steck, H., and Jaakkola, T. S. Bias-corrected bootstrap and model uncertainty. In Advances in Neural Information Processing Systems (2004), pp. 521-528.
[24] Tsamardinos, I., Brown, L. E., and Aliferis, C. F. The max-min hill-climbing bayesian network structure learning algorithm. Machine learning 65, 1 (2006), 31-78.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ http://www.phil.cmu.edu/tetrad/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
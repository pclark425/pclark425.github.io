<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2021 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2021</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2021</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-278960265</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.22635v1.pdf" target="_blank">Learning Composable Chains-of-Thought</a></p>
                <p><strong>Paper Abstract:</strong> A common approach for teaching large language models (LLMs) to reason is to train on chain-of-thought (CoT) traces of in-distribution reasoning problems, but such annotated data is costly to obtain for every problem of interest. We want reasoning models to generalize beyond their training distribution, and ideally to generalize compositionally: combine atomic reasoning skills to solve harder, unseen reasoning tasks. We take a step towards compositional generalization of reasoning skills when addressing a target compositional task that has no labeled CoT data. We find that simply training models on CoT data of atomic tasks leads to limited generalization, but minimally modifying CoT formats of constituent atomic tasks to be composable can lead to improvements. We can train"atomic CoT"models on the atomic tasks with Composable CoT data and combine them with multitask learning or model merging for better zero-shot performance on the target compositional task. Such a combined model can be further bootstrapped on a small amount of compositional data using rejection sampling fine-tuning (RFT). Results on string operations and natural language skill compositions show that training LLMs on Composable CoT outperforms multitask learning and continued fine-tuning baselines within a given training data budget.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2021.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2021.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ComposableCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Composable Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-augmentation format for chain-of-thought (CoT) training that inserts proxy prefix CoT snippets (random-letter prefixes) and suffix-tagging so atomic CoT traces become composable at inference time; used together with model-combination (model merging or multitask learning) to achieve zero-shot compositional generalization and further improved with rejection-sampling fine-tuning (RFT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 7B-base; Qwen2.5 7B-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive transformer language models (decoder-only LLMs) fine-tuned with LoRA adapters; standard transformer architecture without additional modular components beyond Task Arithmetic merging when used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard autoregressive transformer; no explicit modular architecture; experiments use Task Arithmetic model merging (linear combination of parameter deltas) and LoRA low-rank adapters for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>symbolic/string operations and open-ended linguistic skill composition (natural language generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>String operation compositional tasks (Last-letter, Letter-concatenation, ASCII multiplication) and Skill-Mix (Literary + Rhetorical composition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Atomic tasks: simple reasoning skills (e.g., find next letter after last letter, take particular letters from words and concatenate them, multiply ASCII value of a letter; write a sentence demonstrating a named language skill). Compositional tasks: pairwise composition of two atomic tasks (e.g., last-letter → ASCII multiply; concatenation → last-letter; concatenation → ASCII multiply; generate sentence demonstrating two skills drawn from different categories). The compositionality is function composition (output of one atomic skill feeds into the next) and concatenation (two atomic skills combined on same input).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>pairwise (composition of 2 atomic skills)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition (composite) and concatenation of primitives; testing systematicity / unseen combinations</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Zero-shot compositional evaluation: atomic CoT training only (no CoT for composed tasks); limited-compositional supervision experiments: small (< = 500) answer-only compositional datasets used for RFT or SFT/CFT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Data-augmentation of atomic CoT examples into a 'prefix/suffix' composable format (proxy prefix CoTs appended to prompts), then (a) combine atomic models via model merging or (b) multitask fine-tuning across atomic augmented datasets; optionally apply rejection-sampling fine-tuning (RFT) on limited compositional *answer-only* data to bootstrap CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>For compositional tasks in zero-shot experiments, no compositional CoT examples were provided; atomic datasets were augmented to have prefix/suffix tags and proxy prefixes (random letters) to train composability. For RFT, small N_(i,j) ≤ 500 compositional *answer-only* examples are used to sample model generations and accept those with correct final answers into supervised CoT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Atomic tasks are learnable with small datasets (Ni, Nj ≤ 500); single-task atomic CoT fine-tuning achieves high exact-match on atomic test sets (reported as high percentages across atomic tasks in the paper's Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>ComposableCoT variants (both Merge and MTL) outperform StandardCoT baselines on zero-shot compositional generalization across the evaluated settings and models; in some settings (e.g., last-letter + ASCII multiplication) zero-shot ComposableCoT even matches or exceeds supervised fine-tuning on compositional answer data. When followed by one iteration of RFT using limited compositional answer data, ComposableCoT + RFT gives the best performance within a fixed data budget versus multitask learning or continued fine-tuning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Paper reports that StandardCoT atomic-trained models typically fail to compose (often memorizing atomic CoT patterns) while ComposableCoT reduces that failure; exact numeric gap depends on task and model—qualitative result: ComposableCoT substantially reduces the zero-shot compositional performance gap compared to StandardCoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Differences observed by composition type: model merging tends to work better than multitask learning for some composite string tasks (concatenation + multiplication), while MTL is better for Skill-Mix literary+rhetorical combos—indicating composition-type dependent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Baselines include StandardCoT-Merge, StandardCoT-MTL (atomic CoT trained in standard format then combined), supervised fine-tuning on compositional answer-only data (SFT), continued fine-tuning (CFT) of combined models on compositional answer data, few-shot CoT/answer prompting. ComposableCoT-merged/MTL outperformed StandardCoT-merged/MTL in zero-shot; after limited compositional supervision, RFT on ComposableCoT variants outperforms MTL and CFT baselines with same data budget.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>No new neural architecture was introduced; comparisons are between combination strategies (model merging via Task Arithmetic vs multitask fine-tuning) applied on the same base LLMs. Model-merging generally outperforms MTL on some string-operation compositions, while MTL outperforms merging on some Skill-Mix compositions—indicating that combination method affects compositional generalization depending on task conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Only two base models of comparable size (both 7B variants) were tested (Llama 2 7B and Qwen2.5 7B); no explicit study of scaling effect on compositional generalization was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Pretrained LLMs were used as base models and then fine-tuned; the paper does not present a systematic ablation isolating pretraining vs fine-tuning transfer effects on compositional generalization beyond the experimental comparisons described.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Training atomic CoT data in a ComposableCoT format (proxy prefixes + suffix tagging) enables inference-time composition: combining atomic models (merge or MTL) yields strong zero-shot compositional generalization, outperforming StandardCoT baselines. 2) Model merging (Task Arithmetic) and MTL have complementary strengths depending on task pair; merging is often stronger for certain string compositions. 3) Rejection-sampling fine-tuning (RFT) on limited compositional answer-only data substantially improves compositional CoT behavior and achieves the best performance under identical data budgets versus MTL or continued fine-tuning. 4) Intrinsic analysis shows ComposableCoT models produce CoT outputs that include both atomic CoT patterns much more frequently than StandardCoT models (e.g., in some Qwen 2.5-7B evaluations ComposableCoT-Merge produced both atomic CoT patterns ≈100% vs StandardCoT-Merge ≈85%).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Observed failure modes include: (1) models generating only one atomic CoT pattern and failing to compose (replicating a single atomic trace for both prefix and suffix), (2) model-merging sometimes fails to continue generation after a prefix CoT (stops early), and (3) models sometimes use wrong atomic CoT in prefix vs suffix. Specific instability was noted for ComposableCoT-Merge on the concat+multiply setting for Qwen2.5-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Composable generalization succeeds when (a) atomic tasks are easy-to-learn via supervised CoT, (b) atomic CoTs are trained in the ComposableCoT (proxy-prefix + suffix) format (random-letter proxy prefixes generalize best to unseen prefixes), and (c) atomic models are combined (merge or MTL) and optionally bootstrapped with RFT using a small number of compositional answer examples.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2021.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2021.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ComposableCoT-Merge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ComposableCoT with Model Merging (Task Arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combine separately fine-tuned atomic ComposableCoT models by Task Arithmetic: linearly combine parameter deltas from a base model to yield a merged multi-task model that can generate compositional CoTs zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 7B-base; Qwen2.5 7B-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two models are fine-tuned separately on different atomic ComposableCoT datasets; merged by θ_comb = α(θ_i − θ_0) + β(θ_j − θ_0) + θ_0 with α,β tuned on validation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer; combination uses Task Arithmetic linear parameter combination (model merging).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>string operations; Skill-Mix language-skill composition</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same as ComposableCoT entry (pairwise compositions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Merged model evaluated zero-shot on compositional tasks derived from two atomic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>2 (pairwise)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition and concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Zero-shot compositional evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Fine-tune separate atomic models on ComposableCoT formatted data; merge via Task Arithmetic; optionally RFT on limited compositional answer data.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>No compositional CoT examples used for zero-shot; atomic datasets only.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Reported as among the strongest zero-shot performers. Intrinsic generation analysis (Qwen2.5-7B) shows very high occurrence of both atomic CoT patterns: e.g., for Last Letter + Multiplication, ComposableCoT-Merge generated T1 CoT 95.4%, T2 CoT 100.0%, Both CoT 100.0, compared to StandardCoT-Merge 70.4%/85.3%/85.3% (Table 3). Overall ComposableCoT-Merge outperformed StandardCoT-Merge across many evaluated settings in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Model-merging tends to be stronger for certain string-operation composite tasks (e.g., concatenation + multiplication) but can be unstable in some settings (documented error case on concat+mult for Qwen2.5-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against StandardCoT-Merge, ComposableCoT-MTL, SFT, CFT, few-shot baselines. ComposableCoT-Merge typically outperforms StandardCoT-Merge in zero-shot; with RFT it attains top performance given same compositional data budget.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Direct comparison is with ComposableCoT-MTL (multitask combined model) on same base LLMs; merging and MTL show complementary strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model merging of atomic ComposableCoT models provides strong zero-shot compositional generalization and produces CoTs containing both atomic patterns much more frequently than merged StandardCoT models (quantified in intrinsic CoT analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Model merging can be unstable: observed failure modes include inability to continue generation after a prefix CoT and occasional failure to incorporate both atomic CoTs (rare but observed in concat+mult on Qwen2.5-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Works best when atomic models are trained in ComposableCoT format and scaling factors α,β are tuned; requires good per-atomic-model performance so that merged model can sample correct CoTs for RFT if used.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2021.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2021.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ComposableCoT-MTL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ComposableCoT with Multitask Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train a single model via multitask learning on the combined augmented atomic ComposableCoT datasets (prefix and suffix examples) so the model can generate both prefix and suffix CoTs for different atomic tasks and compose them at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 7B-base; Qwen2.5 7B-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>One model fine-tuned on concatenated atomic ComposableCoT datasets with multitask objective minimizing negative loglikelihood over prefix and suffix CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer; multitask fine-tuning with LoRA adapters.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>string operations and Skill-Mix</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same pairwise compositional benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated zero-shot on compositional tasks or after RFT on limited compositional supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>2 (pairwise)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition and concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Zero-shot compositional evaluation and limited compositional supervision (≤500 answers) for RFT/CFT comparators.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multitask fine-tuning on atomic ComposableCoT augmented datasets; compared with StandardCoT-MTL and with model merging combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>No compositional CoT examples in zero-shot; uses atomic ComposableCoT examples only. For RFT experiments, small compositional answer-only datasets are used.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>ComposableCoT-MTL consistently outperforms StandardCoT-MTL in zero-shot settings (Table 1) and when followed by RFT achieves top performance across several tasks (Table 2); MTL is especially competitive on Skill-Mix literary+rhetorical compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>MTL tends to be better than merging for Skill-Mix composition (literary + rhetorical), while merging is sometimes better for specific string-operation compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared with StandardCoT-MTL, StandardCoT-Merge, SFT/CFT and few-shot baselines. ComposableCoT-MTL + RFT outperforms equivalent pipelines built on StandardCoT.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Compared directly to ComposableCoT-Merge; the relative winner depends on the task pair.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multitask training on ComposableCoT-formatted atomic data yields strong zero-shot compositional behavior particularly for some linguistic composition tasks; when combined with RFT, ComposableCoT-MTL attains best-in-budget compositional performance in multiple settings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Works well when atomic tasks have limited parameter-space conflict (i.e., tasks compatible under shared parameters); particularly effective for Skill-Mix categories with less interference.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2021.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2021.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection Sampling Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-taught bootstrapping method: sample multiple CoT + answer generations from a combined model on compositional *answer-only* training examples, verify generated answers against gold answers, collect CoTs whose answers match, and use those accepted CoTs to supervised-fine-tune the model (repeatable).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied on ComposableCoT-Merge and ComposableCoT-MTL using base Llama 2 7B and Qwen2.5 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses model's own generative outputs as pseudo-labeled CoT training data after filtering by correctness; sampling 10 responses per prompt with temperature τ=0.9 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>applies to any autoregressive transformer; relies on sampling and supervised fine-tuning (LoRA) steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>same pairwise compositional tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>compositional tasks (string ops & Skill-Mix)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given small sets of compositional answer-only data, RFT produces CoT supervision from model generations and finetunes the combined model for improved compositional CoT generation and final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>composite/concatenative</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>few-shot compositional answer-only training (≤500 examples) used for bootstrapping</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Sample k generations per compositional prompt; accept those where generated answer matches gold; form DRFT = {(q, t, a)} and SFT on those; iterate.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>RFT uses model-sampled compositional CoTs; number of compositional training examples used in experiments ≤ 500; 10 samples per prompt for acceptance filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>RFT applied on ComposableCoT combined models achieves the best compositional performance within a fixed compositional-data budget, outperforming continued fine-tuning (CFT) or multitask learning baselines (reported in Table 2 across tasks and both base models). The paper reports that RFT on StandardCoT sometimes fails if the initial model can't sample enough correct responses; RFT works best when starting from a reasonably good combined model.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against SFT, CFT on StandardCoT and ComposableCoT models and multitask learning given the same compositional answer data budget; RFT-on-ComposableCoT outperforms these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RFT is an effective way to internalize compositional CoT behavior from answer-only compositional supervision; it is more robust than straightforward continued fine-tuning when the objective shifts from generating CoTs to direct answers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>RFT requires a seed combined model that can generate a reasonable fraction of correct-answer samples; if the seed (e.g., StandardCoT-Merge for some settings) fails to produce enough correct samples, RFT cannot bootstrap.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Needs a decent starting combined model (preferably ComposableCoT-trained) that can sample some correct compositional outputs; sampling diversity and acceptance filtering are important (paper samples 10 outputs per prompt).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2021.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2021.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StandardCoT baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Chain-of-Thought training baselines (StandardCoT-Merge, StandardCoT-MTL, SFT, CFT, few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline methods: (a) atomic CoT models trained in standard CoT format then combined via model merging (StandardCoT-Merge) or multitask learning (StandardCoT-MTL); (b) supervised fine-tuning on compositional answer-only data (SFT/CFT); (c) few-shot CoT or few-shot answer prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 7B-base; Qwen2.5 7B-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base pretrained LLMs fine-tuned under standard CoT formatting and combined or further fine-tuned on compositional answer data for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer; no Task Arithmetic for some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>string operations and Skill-Mix</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>same pairwise compositional benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated zero-shot and with limited compositional supervision to compare compositional generalization ability.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>composite and concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>zero-shot and few-shot compositional evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Standard CoT fine-tuning on atomic tasks; combine with merging or MTL; optionally continue fine-tuning on compositional answer data (CFT) or supervised fine-tuning (SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Zero-shot evaluations use no compositional CoT examples; compositional SFT/CFT experiments use small compositional answer datasets (≤500).</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>High on atomic tasks (see paper Table 5); standard CoT atomic fine-tuning learns atomic tasks with small datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>StandardCoT baselines perform substantially worse than ComposableCoT variants on zero-shot compositional generalization; RFT on StandardCoT can help but often underperforms RFT on ComposableCoT because StandardCoT models may fail to sample enough correct CoTs for bootstrapping (noted failure case: RFT fails for StandardCoT-Merge on Llama2-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Explicitly compared in Tables 1–3. ComposableCoT variants outperform these standard baselines in zero-shot and RFT-on-ComposableCoT outperforms CFT/MTL and SFT under the same compositional data budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>StandardCoT atomic-trained models typically fail to produce compositional CoTs at inference; naive continued fine-tuning or multitask learning on compositional answer-only data is less effective than RFT on a ComposableCoT-initialized combined model.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>StandardCoT-trained atomic models often replicate single-task CoT patterns and fail to generate composed CoT sequences; they can be poor seeds for RFT if they don't sample enough correct outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>STaR: Bootstrapping Reasoning With Reasoning. <em>(Rating: 2)</em></li>
                <li>Editing models with task arithmetic. <em>(Rating: 2)</em></li>
                <li>COGS: A compositional generalization challenge based on semantic interpretation. <em>(Rating: 2)</em></li>
                <li>Meta-learning to compositionally generalize. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2021",
    "paper_id": "paper-278960265",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "ComposableCoT",
            "name_full": "Composable Chain-of-Thought",
            "brief_description": "A data-augmentation format for chain-of-thought (CoT) training that inserts proxy prefix CoT snippets (random-letter prefixes) and suffix-tagging so atomic CoT traces become composable at inference time; used together with model-combination (model merging or multitask learning) to achieve zero-shot compositional generalization and further improved with rejection-sampling fine-tuning (RFT).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2 7B-base; Qwen2.5 7B-base",
            "model_description": "Pretrained autoregressive transformer language models (decoder-only LLMs) fine-tuned with LoRA adapters; standard transformer architecture without additional modular components beyond Task Arithmetic merging when used.",
            "model_size": "7B",
            "is_pretrained": true,
            "architectural_features": "standard autoregressive transformer; no explicit modular architecture; experiments use Task Arithmetic model merging (linear combination of parameter deltas) and LoRA low-rank adapters for fine-tuning.",
            "task_domain": "symbolic/string operations and open-ended linguistic skill composition (natural language generation)",
            "task_name": "String operation compositional tasks (Last-letter, Letter-concatenation, ASCII multiplication) and Skill-Mix (Literary + Rhetorical composition)",
            "task_description": "Atomic tasks: simple reasoning skills (e.g., find next letter after last letter, take particular letters from words and concatenate them, multiply ASCII value of a letter; write a sentence demonstrating a named language skill). Compositional tasks: pairwise composition of two atomic tasks (e.g., last-letter → ASCII multiply; concatenation → last-letter; concatenation → ASCII multiply; generate sentence demonstrating two skills drawn from different categories). The compositionality is function composition (output of one atomic skill feeds into the next) and concatenation (two atomic skills combined on same input).",
            "compositional_depth": "pairwise (composition of 2 atomic skills)",
            "composition_type": "function composition (composite) and concatenation of primitives; testing systematicity / unseen combinations",
            "split_type": "Zero-shot compositional evaluation: atomic CoT training only (no CoT for composed tasks); limited-compositional supervision experiments: small (&lt; = 500) answer-only compositional datasets used for RFT or SFT/CFT baselines.",
            "training_strategy": "Data-augmentation of atomic CoT examples into a 'prefix/suffix' composable format (proxy prefix CoTs appended to prompts), then (a) combine atomic models via model merging or (b) multitask fine-tuning across atomic augmented datasets; optionally apply rejection-sampling fine-tuning (RFT) on limited compositional *answer-only* data to bootstrap CoTs.",
            "curriculum_details": null,
            "inoculation_details": "For compositional tasks in zero-shot experiments, no compositional CoT examples were provided; atomic datasets were augmented to have prefix/suffix tags and proxy prefixes (random letters) to train composability. For RFT, small N_(i,j) ≤ 500 compositional *answer-only* examples are used to sample model generations and accept those with correct final answers into supervised CoT fine-tuning.",
            "iid_performance": "Atomic tasks are learnable with small datasets (Ni, Nj ≤ 500); single-task atomic CoT fine-tuning achieves high exact-match on atomic test sets (reported as high percentages across atomic tasks in the paper's Table 5).",
            "compositional_performance": "ComposableCoT variants (both Merge and MTL) outperform StandardCoT baselines on zero-shot compositional generalization across the evaluated settings and models; in some settings (e.g., last-letter + ASCII multiplication) zero-shot ComposableCoT even matches or exceeds supervised fine-tuning on compositional answer data. When followed by one iteration of RFT using limited compositional answer data, ComposableCoT + RFT gives the best performance within a fixed data budget versus multitask learning or continued fine-tuning baselines.",
            "generalization_gap": "Paper reports that StandardCoT atomic-trained models typically fail to compose (often memorizing atomic CoT patterns) while ComposableCoT reduces that failure; exact numeric gap depends on task and model—qualitative result: ComposableCoT substantially reduces the zero-shot compositional performance gap compared to StandardCoT.",
            "performance_by_depth": null,
            "performance_by_composition_type": "Differences observed by composition type: model merging tends to work better than multitask learning for some composite string tasks (concatenation + multiplication), while MTL is better for Skill-Mix literary+rhetorical combos—indicating composition-type dependent behavior.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Baselines include StandardCoT-Merge, StandardCoT-MTL (atomic CoT trained in standard format then combined), supervised fine-tuning on compositional answer-only data (SFT), continued fine-tuning (CFT) of combined models on compositional answer data, few-shot CoT/answer prompting. ComposableCoT-merged/MTL outperformed StandardCoT-merged/MTL in zero-shot; after limited compositional supervision, RFT on ComposableCoT variants outperforms MTL and CFT baselines with same data budget.",
            "architectural_comparison": "No new neural architecture was introduced; comparisons are between combination strategies (model merging via Task Arithmetic vs multitask fine-tuning) applied on the same base LLMs. Model-merging generally outperforms MTL on some string-operation compositions, while MTL outperforms merging on some Skill-Mix compositions—indicating that combination method affects compositional generalization depending on task conflicts.",
            "scale_effects": "Only two base models of comparable size (both 7B variants) were tested (Llama 2 7B and Qwen2.5 7B); no explicit study of scaling effect on compositional generalization was reported.",
            "transfer_results": "Pretrained LLMs were used as base models and then fine-tuned; the paper does not present a systematic ablation isolating pretraining vs fine-tuning transfer effects on compositional generalization beyond the experimental comparisons described.",
            "key_findings": "1) Training atomic CoT data in a ComposableCoT format (proxy prefixes + suffix tagging) enables inference-time composition: combining atomic models (merge or MTL) yields strong zero-shot compositional generalization, outperforming StandardCoT baselines. 2) Model merging (Task Arithmetic) and MTL have complementary strengths depending on task pair; merging is often stronger for certain string compositions. 3) Rejection-sampling fine-tuning (RFT) on limited compositional answer-only data substantially improves compositional CoT behavior and achieves the best performance under identical data budgets versus MTL or continued fine-tuning. 4) Intrinsic analysis shows ComposableCoT models produce CoT outputs that include both atomic CoT patterns much more frequently than StandardCoT models (e.g., in some Qwen 2.5-7B evaluations ComposableCoT-Merge produced both atomic CoT patterns ≈100% vs StandardCoT-Merge ≈85%).",
            "failure_analysis": "Observed failure modes include: (1) models generating only one atomic CoT pattern and failing to compose (replicating a single atomic trace for both prefix and suffix), (2) model-merging sometimes fails to continue generation after a prefix CoT (stops early), and (3) models sometimes use wrong atomic CoT in prefix vs suffix. Specific instability was noted for ComposableCoT-Merge on the concat+multiply setting for Qwen2.5-7B.",
            "success_conditions": "Composable generalization succeeds when (a) atomic tasks are easy-to-learn via supervised CoT, (b) atomic CoTs are trained in the ComposableCoT (proxy-prefix + suffix) format (random-letter proxy prefixes generalize best to unseen prefixes), and (c) atomic models are combined (merge or MTL) and optionally bootstrapped with RFT using a small number of compositional answer examples.",
            "uuid": "e2021.0"
        },
        {
            "name_short": "ComposableCoT-Merge",
            "name_full": "ComposableCoT with Model Merging (Task Arithmetic)",
            "brief_description": "Combine separately fine-tuned atomic ComposableCoT models by Task Arithmetic: linearly combine parameter deltas from a base model to yield a merged multi-task model that can generate compositional CoTs zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2 7B-base; Qwen2.5 7B-base",
            "model_description": "Two models are fine-tuned separately on different atomic ComposableCoT datasets; merged by θ_comb = α(θ_i − θ_0) + β(θ_j − θ_0) + θ_0 with α,β tuned on validation.",
            "model_size": "7B",
            "is_pretrained": true,
            "architectural_features": "standard transformer; combination uses Task Arithmetic linear parameter combination (model merging).",
            "task_domain": "string operations; Skill-Mix language-skill composition",
            "task_name": "Same as ComposableCoT entry (pairwise compositions)",
            "task_description": "Merged model evaluated zero-shot on compositional tasks derived from two atomic tasks.",
            "compositional_depth": "2 (pairwise)",
            "composition_type": "function composition and concatenation",
            "split_type": "Zero-shot compositional evaluation",
            "training_strategy": "Fine-tune separate atomic models on ComposableCoT formatted data; merge via Task Arithmetic; optionally RFT on limited compositional answer data.",
            "curriculum_details": null,
            "inoculation_details": "No compositional CoT examples used for zero-shot; atomic datasets only.",
            "iid_performance": null,
            "compositional_performance": "Reported as among the strongest zero-shot performers. Intrinsic generation analysis (Qwen2.5-7B) shows very high occurrence of both atomic CoT patterns: e.g., for Last Letter + Multiplication, ComposableCoT-Merge generated T1 CoT 95.4%, T2 CoT 100.0%, Both CoT 100.0, compared to StandardCoT-Merge 70.4%/85.3%/85.3% (Table 3). Overall ComposableCoT-Merge outperformed StandardCoT-Merge across many evaluated settings in Table 1.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": "Model-merging tends to be stronger for certain string-operation composite tasks (e.g., concatenation + multiplication) but can be unstable in some settings (documented error case on concat+mult for Qwen2.5-7B).",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against StandardCoT-Merge, ComposableCoT-MTL, SFT, CFT, few-shot baselines. ComposableCoT-Merge typically outperforms StandardCoT-Merge in zero-shot; with RFT it attains top performance given same compositional data budget.",
            "architectural_comparison": "Direct comparison is with ComposableCoT-MTL (multitask combined model) on same base LLMs; merging and MTL show complementary strengths.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Model merging of atomic ComposableCoT models provides strong zero-shot compositional generalization and produces CoTs containing both atomic patterns much more frequently than merged StandardCoT models (quantified in intrinsic CoT analyses).",
            "failure_analysis": "Model merging can be unstable: observed failure modes include inability to continue generation after a prefix CoT and occasional failure to incorporate both atomic CoTs (rare but observed in concat+mult on Qwen2.5-7B).",
            "success_conditions": "Works best when atomic models are trained in ComposableCoT format and scaling factors α,β are tuned; requires good per-atomic-model performance so that merged model can sample correct CoTs for RFT if used.",
            "uuid": "e2021.1"
        },
        {
            "name_short": "ComposableCoT-MTL",
            "name_full": "ComposableCoT with Multitask Learning",
            "brief_description": "Train a single model via multitask learning on the combined augmented atomic ComposableCoT datasets (prefix and suffix examples) so the model can generate both prefix and suffix CoTs for different atomic tasks and compose them at inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2 7B-base; Qwen2.5 7B-base",
            "model_description": "One model fine-tuned on concatenated atomic ComposableCoT datasets with multitask objective minimizing negative loglikelihood over prefix and suffix CoTs.",
            "model_size": "7B",
            "is_pretrained": true,
            "architectural_features": "standard transformer; multitask fine-tuning with LoRA adapters.",
            "task_domain": "string operations and Skill-Mix",
            "task_name": "Same pairwise compositional benchmarks",
            "task_description": "Evaluated zero-shot on compositional tasks or after RFT on limited compositional supervision.",
            "compositional_depth": "2 (pairwise)",
            "composition_type": "function composition and concatenation",
            "split_type": "Zero-shot compositional evaluation and limited compositional supervision (≤500 answers) for RFT/CFT comparators.",
            "training_strategy": "Multitask fine-tuning on atomic ComposableCoT augmented datasets; compared with StandardCoT-MTL and with model merging combinations.",
            "curriculum_details": null,
            "inoculation_details": "No compositional CoT examples in zero-shot; uses atomic ComposableCoT examples only. For RFT experiments, small compositional answer-only datasets are used.",
            "iid_performance": null,
            "compositional_performance": "ComposableCoT-MTL consistently outperforms StandardCoT-MTL in zero-shot settings (Table 1) and when followed by RFT achieves top performance across several tasks (Table 2); MTL is especially competitive on Skill-Mix literary+rhetorical compositions.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": "MTL tends to be better than merging for Skill-Mix composition (literary + rhetorical), while merging is sometimes better for specific string-operation compositions.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared with StandardCoT-MTL, StandardCoT-Merge, SFT/CFT and few-shot baselines. ComposableCoT-MTL + RFT outperforms equivalent pipelines built on StandardCoT.",
            "architectural_comparison": "Compared directly to ComposableCoT-Merge; the relative winner depends on the task pair.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Multitask training on ComposableCoT-formatted atomic data yields strong zero-shot compositional behavior particularly for some linguistic composition tasks; when combined with RFT, ComposableCoT-MTL attains best-in-budget compositional performance in multiple settings.",
            "failure_analysis": null,
            "success_conditions": "Works well when atomic tasks have limited parameter-space conflict (i.e., tasks compatible under shared parameters); particularly effective for Skill-Mix categories with less interference.",
            "uuid": "e2021.2"
        },
        {
            "name_short": "RFT",
            "name_full": "Rejection Sampling Fine-Tuning",
            "brief_description": "A self-taught bootstrapping method: sample multiple CoT + answer generations from a combined model on compositional *answer-only* training examples, verify generated answers against gold answers, collect CoTs whose answers match, and use those accepted CoTs to supervised-fine-tune the model (repeatable).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied on ComposableCoT-Merge and ComposableCoT-MTL using base Llama 2 7B and Qwen2.5 7B",
            "model_description": "Uses model's own generative outputs as pseudo-labeled CoT training data after filtering by correctness; sampling 10 responses per prompt with temperature τ=0.9 in experiments.",
            "model_size": "7B",
            "is_pretrained": true,
            "architectural_features": "applies to any autoregressive transformer; relies on sampling and supervised fine-tuning (LoRA) steps.",
            "task_domain": "same pairwise compositional tasks",
            "task_name": "compositional tasks (string ops & Skill-Mix)",
            "task_description": "Given small sets of compositional answer-only data, RFT produces CoT supervision from model generations and finetunes the combined model for improved compositional CoT generation and final-answer accuracy.",
            "compositional_depth": "2",
            "composition_type": "composite/concatenative",
            "split_type": "few-shot compositional answer-only training (≤500 examples) used for bootstrapping",
            "training_strategy": "Sample k generations per compositional prompt; accept those where generated answer matches gold; form DRFT = {(q, t, a)} and SFT on those; iterate.",
            "curriculum_details": null,
            "inoculation_details": "RFT uses model-sampled compositional CoTs; number of compositional training examples used in experiments ≤ 500; 10 samples per prompt for acceptance filtering.",
            "iid_performance": null,
            "compositional_performance": "RFT applied on ComposableCoT combined models achieves the best compositional performance within a fixed compositional-data budget, outperforming continued fine-tuning (CFT) or multitask learning baselines (reported in Table 2 across tasks and both base models). The paper reports that RFT on StandardCoT sometimes fails if the initial model can't sample enough correct responses; RFT works best when starting from a reasonably good combined model.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against SFT, CFT on StandardCoT and ComposableCoT models and multitask learning given the same compositional answer data budget; RFT-on-ComposableCoT outperforms these baselines.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "RFT is an effective way to internalize compositional CoT behavior from answer-only compositional supervision; it is more robust than straightforward continued fine-tuning when the objective shifts from generating CoTs to direct answers.",
            "failure_analysis": "RFT requires a seed combined model that can generate a reasonable fraction of correct-answer samples; if the seed (e.g., StandardCoT-Merge for some settings) fails to produce enough correct samples, RFT cannot bootstrap.",
            "success_conditions": "Needs a decent starting combined model (preferably ComposableCoT-trained) that can sample some correct compositional outputs; sampling diversity and acceptance filtering are important (paper samples 10 outputs per prompt).",
            "uuid": "e2021.3"
        },
        {
            "name_short": "StandardCoT baselines",
            "name_full": "Standard Chain-of-Thought training baselines (StandardCoT-Merge, StandardCoT-MTL, SFT, CFT, few-shot)",
            "brief_description": "Baseline methods: (a) atomic CoT models trained in standard CoT format then combined via model merging (StandardCoT-Merge) or multitask learning (StandardCoT-MTL); (b) supervised fine-tuning on compositional answer-only data (SFT/CFT); (c) few-shot CoT or few-shot answer prompting baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2 7B-base; Qwen2.5 7B-base",
            "model_description": "Same base pretrained LLMs fine-tuned under standard CoT formatting and combined or further fine-tuned on compositional answer data for comparisons.",
            "model_size": "7B",
            "is_pretrained": true,
            "architectural_features": "standard transformer; no Task Arithmetic for some baselines.",
            "task_domain": "string operations and Skill-Mix",
            "task_name": "same pairwise compositional benchmarks",
            "task_description": "Evaluated zero-shot and with limited compositional supervision to compare compositional generalization ability.",
            "compositional_depth": "2",
            "composition_type": "composite and concatenation",
            "split_type": "zero-shot and few-shot compositional evaluation",
            "training_strategy": "Standard CoT fine-tuning on atomic tasks; combine with merging or MTL; optionally continue fine-tuning on compositional answer data (CFT) or supervised fine-tuning (SFT).",
            "curriculum_details": null,
            "inoculation_details": "Zero-shot evaluations use no compositional CoT examples; compositional SFT/CFT experiments use small compositional answer datasets (≤500).",
            "iid_performance": "High on atomic tasks (see paper Table 5); standard CoT atomic fine-tuning learns atomic tasks with small datasets.",
            "compositional_performance": "StandardCoT baselines perform substantially worse than ComposableCoT variants on zero-shot compositional generalization; RFT on StandardCoT can help but often underperforms RFT on ComposableCoT because StandardCoT models may fail to sample enough correct CoTs for bootstrapping (noted failure case: RFT fails for StandardCoT-Merge on Llama2-7B).",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Explicitly compared in Tables 1–3. ComposableCoT variants outperform these standard baselines in zero-shot and RFT-on-ComposableCoT outperforms CFT/MTL and SFT under the same compositional data budgets.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "StandardCoT atomic-trained models typically fail to produce compositional CoTs at inference; naive continued fine-tuning or multitask learning on compositional answer-only data is less effective than RFT on a ComposableCoT-initialized combined model.",
            "failure_analysis": "StandardCoT-trained atomic models often replicate single-task CoT patterns and fail to generate composed CoT sequences; they can be poor seeds for RFT if they don't sample enough correct outputs.",
            "uuid": "e2021.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "STaR: Bootstrapping Reasoning With Reasoning.",
            "rating": 2
        },
        {
            "paper_title": "Editing models with task arithmetic.",
            "rating": 2
        },
        {
            "paper_title": "COGS: A compositional generalization challenge based on semantic interpretation.",
            "rating": 2
        },
        {
            "paper_title": "Meta-learning to compositionally generalize.",
            "rating": 2
        }
    ],
    "cost": 0.02076725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning Composable Chains-of-Thought
28 May 2025</p>
<p>Fangcong Yin fangcongyin@utexas.edu 
The University of Texas at Austin
♢ Princeton University</p>
<p>Zeyu Leo Liu 
The University of Texas at Austin
♢ Princeton University</p>
<p>Liu Leqi 
The University of Texas at Austin
♢ Princeton University</p>
<p>Ye ♢ Xi 
The University of Texas at Austin
♢ Princeton University</p>
<p>Greg Durrett 
The University of Texas at Austin
♢ Princeton University</p>
<p>Learning Composable Chains-of-Thought
28 May 2025AF3EE582235744F53FABEB04D26094E6arXiv:2505.22635v1[cs.CL]
A common approach for teaching large language models (LLMs) to reason is to train on chain-of-thought (CoT) traces of in-distribution reasoning problems, but such annotated data is costly to obtain for every problem of interest.We want reasoning models to generalize beyond their training distribution, and ideally to generalize compositionally: combine atomic reasoning skills to solve harder, unseen reasoning tasks.We take a step towards compositional generalization of reasoning skills when addressing a target compositional task that has no labeled CoT data.We find that simply training models on CoT data of atomic tasks leads to limited generalization, but minimally modifying CoT formats of constituent atomic tasks to be composable can lead to improvements.We can train "atomic CoT" models on the atomic tasks with Composable CoT data and combine them with multitask learning or model merging for better zero-shot performance on the target compositional task.Such a combined model can be further bootstrapped on a small amount of compositional data using rejection sampling fine-tuning (RFT).Results on string operations and natural language skill compositions show that training LLMs on Composable CoT outperforms multitask learning and continued fine-tuning baselines within a given training data budget. 1</p>
<p>Introduction</p>
<p>Large language models (LLMs) have succeeded at addressing many problems largely by virtue of the massive amounts of data they are trained on.Many problems that defied earlier approaches have become easy because they are now in-distribution for models that have seen similar data during pre-training or instruction-tuning.However, these models still fail at challenging reasoning tasks and it is impossible to scale training data to cover all possible tasks of interest.Ideally, we want models that can generalize to new settings, and particularly, can apply basic "skills" learned during training in novel combinations to solve problems at inference time.How to empower LLMs with this capability, also called compositional generalization [1,2,3,4], remains an open question.For instance, large reasoning models [5,6], built on pre-trained LLMs, are typically trained on a large amount of data annotated with chain-of-thought (CoT) traces, but demonstrating how to generalize from easy problems to harder ones would break the dependence on the scale of training data [7,8], and would facilitate more efficient and robust reasoning with LLMs.</p>
<p>We explore the setting of compositional reasoning where pre-trained LLMs are fine-tuned on CoT data of simple reasoning tasks (atomic tasks) and then evaluated on the combinations of them (compositional tasks) with limited compositional supervision.We find that models trained with atomic CoT data of the standard CoT format demonstrate limited generalization: they typically memorize and reproduce the atomic CoT patterns rather than successfully composing them.We propose a simple modification of the CoT format of the atomic task training data, which we call 1 Code and data are available at: https://github.com/fc2869/composable_cot.</p>
<p>Preprint.Under review.Composable CoT, to improve the compositional reasoning capability by enabling models to compose atomic reasoning skills at inference time.</p>
<p>We first experiment with zero-shot combination of Composable CoT models as illustrated in Figure 1a.</p>
<p>We experiment with two different approaches: first, merging models trained on individual atomic CoT tasks, and second, multitask learning across our atomic CoT datasets.Such combined models achieve zero-shot compositional generalization, even beating models trained on compositional data in some settings.Moreover, we show that for the compositional task, zero-shot Composable CoT models can generate CoT traces that rely less on spurious correlations and reasoning shortcuts.</p>
<p>We then demonstrate that our zero-shot models can be improved further by rejection sampling fine-tuning on a limited amount of compositional supervision as shown in Figure 1b.Using only downstream answer supervision, our models can bootstrap better compositional CoT behavior.On various tasks involving string operations and natural language skill composition, we show that our approach outperforms multi-task learning and continued fine-tuning baselines within a given budget of training data.</p>
<p>The main contributions of this work include: (1) A novel data augmentation scheme for training CoT models on simple reasoning tasks to enable future composition of atomic reasoning skills.(2) A method for improving compositional reasoning with LLMs by combining CoT models trained with such augmentation and training with rejection sampling fine-tuning for better compositional reasoning performance.</p>
<p>Preliminaries</p>
<p>LLM reasoning with chain-of-thought Given a prompt q that states a reasoning problem, an LLM M is prompted to solve it by drawing samples from conditional distribution ỹ ∼ p M (y | q).Let a denote the ground truth answer to q.We consider two common ways of solving q: (1) Direct answer: ỹ only contains the predicted answer ã; (2) Reasoning with chain-of-thought: ỹ includes a chain-of-thought trace t, followed by a predicted answer ã.</p>
<p>While CoT reasoning can be elicited through zero-shot prompting [9,10], recent works show that fine-tuning pre-trained LLMs on CoT traces leads to strong reasoning models [11,6].We define a dataset for a reasoning task T as a set of (prompt, answer) pairs: D T = {(q, a)}.A dataset with CoT traces is then D CoT T = {(q, t, a)}.To fine-tune M parametrized as θ on D CoT T with supervised fine-tuning, we minimize the following supervised learning loss objective:
L D CoT T (θ) = − 1 N N (log p θ (t | q) + log p θ (a | q, t)) where |D CoT T | = N .
Atomic and compositional tasks Consider a set of k tasks that represent basic reasoning skills T 1 , T 2 , ..., T k , which we call atomic tasks.We define compositional tasks T (i,j) where i, j ∈ [1, ..., k] and i ̸ = j, as those tasks that can be expressed as a composition of two atomic tasks.That is, there exist T i and T j such that T (i,j) = g(T i , T j ) where g is some function to combine the two atomic tasks.We discuss more details for g in Appendix A.</p>
<p>We define a collection of reasoning skills as basic if each skill cannot be reduced into a combination of others; i.e., each task cannot be efficiently solved by a rule-based transformation of the solution of some other combination of tasks.</p>
<p>Compositional reasoning from atomic CoT For a pair of atomic tasks T i and T j sampled from the k tasks, we assume access to atomic CoT data D CoT Ti and D CoT Tj with dataset sizes of N i and N j .We define models fine-tuned on atomic CoT data as atomic CoT models.</p>
<p>We assume for their composition T (i,j) , we only have access to a training dataset D T (i,j) of size N (i,j) .We make two assumptions about this data which follow from practical considerations about how our compositional CoTs would work in practice.First, we assume that this data only contains the direct answer and not a labeled example of a CoT.This reflects that high-quality annotated CoT supervision may be harder to obtain in practice than correct answers.Second, we assume that N (i,j) is small.We may be able to collect a small amount of data for each new compositional task of interest, but these compositional tasks are too numerous to undertake large-scale data collection on.We are interested in the following question: Can we achieve good compositional performance by fine-tuning with the atomic CoT data and a limited amount of compositional direct answer data?</p>
<p>3 Learning Composable Chains-of-Thought</p>
<p>Constructing Composable CoT Training Data</p>
<p>Question: Multiply the ASCII value of "a" by 2.</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K T
6 E E Q b 3 c u n c O e x E w p 4 2 k 8 S c u 2 o = " &gt; A A A C d 3 i c b V F N S 8 N A E N 3 E 7 / p V 9 a Y H F 2 u l I p R E / D p 4 K O r B o 4 K 1 Q l P C Z L u p i 5 t N 3 N 2 I J e Q v + O O 8 + T + 8 e H P T V q j V g Y X H m / d m Z m e C h D O l H e f D s q e m Z 2 b n 5 h d K i 0 v L K 6 v l t f V 7 F a e S 0 C a J e S w f A l C U M 0 G b m m l O H x J J I Q o 4 b Q V P l 0 W + 9 U K l Y r G 4 0 / 2 E d i L o C R Y y A t p Q f v m t m v h e B P q R A M 9 a e e 3 V Z 9 i L W B c P y C D M X n M / O 2 f 5 f q n q C Q g 4 + N m Y P M c e j 3 v 4 d 4 k f Z 1 i Y k g n 9 A f a u K N e A x 9 j S j + M 5 9 8 s V p + 4 M A v 8 F 7 g h U 0 C h u / P K 7 1 4 1 J G l G h C Q e l 2 q 6 T 6 E 4 G U j P C q S m d K p o A e Y I e b R s o I K K q k w 3 2 l u O q Y b o 4 j K V 5 Q u M B O + 7 I I F K q H w V G W U y o J n M F + V + u n e r w r J M x k a S a C j J s F K Y c 6 x g X R 8 B d J i n R v G 8 A E M n M r J g 8 g g S i z a l K Z g n u 5 J f / g v v D u n t S P 7 4 9 q j Q u R u u Y R 1 t o B 9 W Q i 0 5 R A 1 2 j G 9 R E B H 1 a m 1 b F 2 r W + 7 G 1 7 z 6 4 N p b Y 1 8 m y g X 2 G 7 3 8 o 2 w I g = &lt; / l a t e x i t &gt;</p>
<p>q <tag k> The ASCII value of the letter a is 97, and […] </tag k></p>
<p>Answer: 194 <tag 1> aaksebnab zldjxhl … </tag 1> Additional proxy prefix CoTs</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j 9 7 e a u o / 4 8 B e H + c / z &lt; / l a t e x i t &gt; a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k q 3 q z 9 I x N y W N
f + r m f / L I O 1 2 i A 0 3 T H + Y = " &gt; A A A C p 3 i c b V H b b t Q w E H X C r Y R L F 3 j k x W J Z q R V o l a A W e O C h A h 4 Q L x T R v U i b K J p 4 n a 1 V 5 1 J 7 g n Z l + d f 4 C N 7 4 G 5 w 0 S N s t I 9 k 6 O m f O e G a c 1 V J o D M M / n n / r 9 p 2 7 9 / b u B w 8 e P n q 8 P 3 j y d K q r R j E + Y Z W s 1 D w D z a U o + Q Q F S j 6 v F Y c i k 3 y W X X x q 9 d l P r r S o y j P c 1 D w p Y F W K X D B A R 6 W D X 6 M 6 j Q v A c w b S z O z B O h U 0 L s S S d m S W m 7 V N z Q d h D 4 N R X E I m I T V b 6 Z b G s l r R 6 y X + O f P W V O / k v 6 L x Z y 4 R 6 B b r S v e W S 4 c h i J G v s R t t o V Z Z Y s J x + J q G 4 6 P 2 C q 3 p Z I E G r A 3 S w d B x X d C b I O r B k P R x m g 5 + x 8 u K N Q U v k U n Q e h G F N S Y G F A o m u Q 3 i R v M a 2 A W s + M L B E g q u E 9 M 1 Y + n I M U u a V 8 q d E m n H b j s M F F p v i s x l t g P p X a 0 l / 6 c t G s z f J 0 a U d Y O 8 Z F c P 5 Y 2 k W N H 2 0 + h S K M 5 Q b h w A p o T r l b J z U M D Q f W 2 7 h G h 3 5 J t g + m Y c v R 0 f f z 8 a n n z s 1 7 F H n p M X 5 I B E 5 B 0 5 I V / I K Z k Q 5 r 3 0 v n o / v D P / 0 P / m T / 3 5 V a r v 9 Z 5 n 5 F rV W V N + Q 7 7 / I x u I h o = " &gt; A A A C i X i c b V F N T 9 t A E F 2 7 f N V A S e H Y y 6 p R B I g q s h H Q q O K A S g 8 9 U q k h S H F k j T f r s G K 9 N r t j l M j y f + E 3 c e u / 6 T o Y K Q R G W u n t m / d m Z 2 f i X A q D v
v / P c T + s r K 6 t b 3 z 0 N r e 2 P + 2 0 P u 9 e m 6 z Q j P d Z J j N 9 E 4 P h U i j e R 4 G S 3 + S a Q Given a pair of atomic tasks, we assume the CoT traces in each atomic task data follow a certain distribution distinct to that dataset.A pre-trained LLM M 0 fine-tuned on the standard CoT data is only optimized to generate CoT traces that can replicate patterns in those two distributions.It is unclear whether a fine-tuned model can produce compositional CoTs for a prompt drawn from the compositional task distribution, as this will generally be out-of-distribution from the perspective of each of the two datasets.Without additional supervision signals, such fine-tuned models typically only replicate one of the learned atomic reasoning patterns in the generated CoT; we show the empirical evidence for this in Section 5.3.
x p L P o j v L u v 8 4 I F r I z L 1 F 2 c 5 H 6 U w U S I R D N B S U e u x k 0 d h C n j L Q J a D 6 m A a C R q m Y k z n Z J y U 0 y o q z 0 V 1 6 H V C B b G E q F y Q V z S U 2 Y S + L v H i T G p T v q Q / o u E v L h H o A m t L N 5 Z 7 i 8 F 7 u W E V B f t R q + 1 3 / X n Q t y B o Q J s 0 c R W 1 n s J x x o q U K 2 Q S j B k G f o 6 j E j Q K J n n l h Y X h O b A 7 m P C h h Q p S b k b l f J I V 7 V h m T J N M 2 6 O Q z t l F R w m p M b M 0 t s q 6 S b O c q 8 n 3 c s M C k 9 6 o F C o v k C v 2 / F B S S I o Z r d d C x 0 J z h n J m A T A t b K + U 3 Y I G h n Z 5 n h 1 C s P z l t + D 6 u B u c d U / / n L Q v f j b j 2 C B f y F d y Q A L y n V y Q 3 + S K 9 A l z 1 p x v z q l z 5 m 6 6 g d t z f z x L X a f x 7 J F X 4 V 7 + B 6 o C x O 4 = &lt; / l a t e x i t &gt; t 0 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 M e 1 X Z M J / y 7 A I Z n E z l 1 g G B X h w b M = " &gt; A A A C i H i c b V F R T 9 s w E H Y y Y C z b o G y P e 7 G o K j E h V Q k q A 6 Q 9 V I W H P T K J U q S m i i 6 u U 6 w 6 T r A v i C r K b 9 l / 4 o 1 / M 6 c E q R R O s v T 5 u + 8 7 n + / i X A q D v v / k u B 8 2 N r c + b n / y P n / 5 u r P b 2 v t 2 b b J C M z 5 k m c z 0 T Q y G S 6 H 4 E A V K f p N r D m k s + S i e n 9 f 5 0 T 3 X R m T q C h c 5 n 6 Q w U y I R D N B S U e t f J 4 / C F P C W g S x H 1 c F D J G i Y i i l d k n F S P l R R + V t U P 7 1 O q C C W E J U r 8 o q G M p v R 1 y V e n E l t y t f 0 h z S 8 4 B K B r r C 2 d G O 5 s x i 8 l x t W 0 T x q t f 2 u v w z 6 F g Q N a J M m L q P W Y z j N W J F y h U y C M e P A z 3 F S g k b B J K + 8 s D A 8 B z a H G R 9 b q C D l Z l I u B 1 n R j m W m N M m 0 P Q r p k l 1 1 l J A a s 0 h j q 6 x 7 N O u 5 m n w v N y 4 w O Z 2 U Q u U F c s W e H 0 o K S T G j 9 V b o V G j O U C 4 s A K a F 7 Z W y W 9 D A 0 O 7 O s 0 M I 1 r / 8 F l w f d Y N f 3 e O / v X Z / 0 I x j m / w g + + S A B O S E 9 M k f c k m G h D m b z q H T c
In order to generate two atomic CoTs in one sequence t i t j , the model must allocate substantial probability p(t i | qt j ), despite these not being in the training distribution.Our goal is to make this as in-distribution as possible even for a model that does not train on explicit compositional examples.</p>
<p>Construction</p>
<p>We define a set of chain-of-thought tags P = {p 1 , ..., p n } for k ∈ {1, ..., n}.</p>
<p>For each training example D CoT T = (q, t, a) in the atomic CoT dataset, we randomly determine whether to augment it with one of the n tags.If an example is assigned tag p k , then we want to treat it as the k-th step in a notional compositional reasoning process.To achieve this, we append k − 1 proxy prefix CoTs (t ′ 1 , . . ., t ′ k−1 ) to the end of the prompt:
D suf T = (qt ′ 1 . . . t ′ k−1 , t k )
where t k =<suffix>ta</suffix>.The procedure is illustrated in Figure 2. We discuss different design choices for forming the proxy prefix CoTs in Appendix B.Here we present a simple yet effective approach where the proxy prefix CoT is a sequence of randomly sampled letters of a random length.Such a design aims at teaching models to generate robust continuation following an arbitrary prefix CoT.While the method applies to n &gt; 2, we focus on the case where n = 2 for simplicity, i.e., we only have a prefix CoT tag and a suffix CoT tag.Concretely, this gives us an implementation consisting of a prefix CoT t pre , which is a CoT conditioned only on the prompt q, and a suffix CoT t suf is conditioned on the prompt and a proxy prefix CoT.The prefix CoT dataset is constructed by the following: D pre T = (q, t pre ) where t pre =<prefix>ta</prefix>. Training objective: Given our construction, we fine-tune M 0 with the augmented dataset D aug
T = D pre T + D suf
T with a multitask learning objective to minimize the sum of the negative log likelihood of generating n prefix CoTs and m suffix CoTs, where N pre + N suf = N :
L D aug T (θ) = L D pre T (θ) + L D suf T (θ) = − 1 N pre Npre log p θ (t pre | q) − 1 N suf N suf log p θ (t suf | q, t ′ pre )</p>
<p>Combining Atomic CoT Models</p>
<p>ComposableCoT-MTL We apply multitask learning (MTL) to fine-tune M 0 on the combined dataset of D aug Ti + D aug Tj and obtain a single MTL model M comb that can generate prefix and suffix CoTs for both atomic tasks.</p>
<p>ComposableCoT-Merge Model merging is another way to combine multiple models into a single multi-task model [12,13,14].Starting from M 0 , we fine-tune two models M i and M j (parametrized by θ i and θ j ) on D aug Ti and D aug Tj respectively to optimize for atomic task performance.Then we use Task Arithmetic [12] to merge the two models into a single model M comb parametrized by θ comb as a linear combination of the differences between the two fine-tuned parameters and the base model parameter: θ comb = α(θ i − θ 0 ) + β(θ j − θ 0 ) + θ 0 where α and β are tunable scaling factors.</p>
<p>Inference Both variants of M comb can be used for zero-shot evaluation on the compositional task T (i,j) .At inference time, we sample a response from M comb , append <suffix> to the end of the generated response when it stops generation, and continue generation until the model stops again.</p>
<p>Improving Composition with Rejection Sampling Fine-tuning</p>
<p>M comb can be further improved with self-taught reasoning [15] by rejection sampling fine-tuning (RFT) [16,17] on the limited compositional data.Recall that for the compositional task, we only have the direct answer labels instead of CoT traces.M comb can serve as a starting point for RFT where we fine-tune M comb with its own, correct CoT responses on the compositional task.</p>
<p>Algorithm 1 shows the algorithm.Concretely, we sample responses from M comb for each example in the compositional training data.Using the direct answer labels to verify the sampled responses, we can collect a supervised fine-tuning dataset D RFT to continued fine-tune M comb .Such a process can be repeated for multiple iterations.Note that some tasks do not require a single correct answer to a given question (e.g.open-ended generation), and it would be hard to verify the correctness of sampled outputs only based on direct answer labels.For those tasks, we follow [15,18] to perform rationalization: we first append the direct answer label to the end of the prompt and sample post-hoc explanations for the given answer from the model; because M comb is optimized to generate an answer following a CoT, we extract the generated answer following the generated explanation and filter out explanations whose following answer is not the same as the provided gold answer; finally, we use the accepted explanations as surrogates for CoT to form the RFT data.</p>
<p>Algorithm 1 Bootstrapping Atomic CoT Models Trained on Composable CoT</p>
<p>Input: The combined model M comb ; dataset DT (i,j) = {(qv, av)}
N (i,j)
v=1 ; the number of iterations c.Output:
1: M0 ← M comb ▷ Initialization 2: for w in 1...c do 3:
if use rationalization then 4:</p>
<p>( tv, ãv) ← Mw−1(qvav) ∀v ∈ {1, ..., N (i,j) } ▷ Performance rationalization 5: else 6:</p>
<p>( tv, ãv) ← Mw−1(qv) ∀v ∈ {1, ..., N (i,j) } 7:</p>
<p>end if 8:</p>
<p>DRFT ← {(qv, tv, av) s.t.v ∈ {1, ..., N (i,j) } and ãv = av} ▷ CoTs with correct answers 9:</p>
<p>Mw ← SFT(M comb , DRFT) ▷ Fine-tune the combined model on the accepted CoT data 10: end for</p>
<p>Experimental Setup</p>
<p>We evaluate on two sets of tasks: a set of string operation tasks and tasks derived from the Skill-Mix [19] dataset.Each setting involves atomic tasks and compositional tasks.We ensure that all atomic tasks are learnable through supervised fine-tuning with a small amount of training data (N i , N j ≤ 500); the single task learning performance can be found in Appendix E String operation tasks We consider the following atomic tasks that involve string operations.</p>
<p>(1) Last letter in alphabet: Determine the next letter in the alphabet following the last letter in a sequence of letters.(2) Letter concentation: Adapted from [9,20,4], this task prompts the LLM to concatenate the first, second, second-to-last, or last letter of each word in a given sequence of words.</p>
<p>(3) ASCII multiplication: Perform multiplicative operations of the ASCII value of a given letter.</p>
<p>We consider the following compositions of the atomic tasks.</p>
<p>1.Last letter + multiplication: Given a sequence of letters, find the next letter in the alphabet following the last letter, determine its ASCII value, and then perform multiplication with a given constant.2. Concatenation + last letter: Given a sequence of words, concatenate the first, second, or second-tolast letter of each word and then find the next letter in the alphabet following the last letter of the concatenated sequence.3. Concatenation + multiplication: Given a sequence of words, concatenate the first, second, or second-to-last letter of each word, find the ASCII value of the last letter of the concatenated sequence, and then perform multiplication.</p>
<p>Data and CoT traces of these tasks are automatically generated using fixed templates.The data generation procedure and the examples can be found in Appendix C.</p>
<p>Skill-Mix</p>
<p>Given the definition and an example of a language skill (e.g.hyperbole), the model needs to write a sentence to demonstrate the skill about a given topic.Because each language skill in Skill-Mix dataset consists of only a few examples, we consider an atomic task to be handling skills over a category of skills, and we evaluate on two categories that are mainly mutually exclusive: literary devices (Skill-Mix-Literary) and rhetorical devices (Skill-Mix-Rhetorical).Atomic CoT traces for Skill-Mix are distilled from GPT-4o [21], following [22].Examples and details can be found in Appendix D. The composition tasks we consider combine literary and rhetorical skills: generate a sentence to demonstrate two provided skills, each of which is sampled from one of the categories.</p>
<p>Evaluation Metrics All string operation tasks are evaluated using exact match accuracy and a regex-based answer extractor is used to extract the answer from the generated response.For Skill-Mix tasks, we use the metrics from [19] to measure the quality of the generated sentence based on a rubric (namely, Full Marks and Skill Fraction), and use GPT-4o-mini for automatic generation.Details can be found in Appendix D.2.</p>
<p>Zero-shot/Few-shot Baselines Figure 3 summarizes the high-order variables of the configurations we evaluate.For zero-shot compositional generalization, we include the following baselines: (1)  .We also include supervised learning baselines (SFT) where M 0 is fine-tuned on the same compositional answer data D T (i,j) .</p>
<p>The differences of methods we evaluate for each setting are summarized in Table 10.</p>
<p>Models and Training</p>
<p>We use Llama 2 7B-base [23] and Qwen2.5 7B-base [24] for experiments.We use LoRA [25] for supervised fine-tuning experiments.For rejection sampling, we sample 10 responses for each prompt and use temperature τ = 0.9 for inference; for the other methods, we use greedy decoding.For Skill-Mix tasks, we perform rationalization for RFT because it is an open-ended generation task (see Section 3.3).Configuration and hyperparameters are in Appendix F.</p>
<p>Results</p>
<p>Zero-shot Generalization</p>
<p>We evaluate the compositional generalization of the proposed method without compositional supervision, including ComposableCoT-Merge and ComposableCoT-MTL.For all methods that we compare with, we control the amount of training data to be the same as N i and N j .For reference, we also include the supervised fine-tuning baseline by fine-tuning M 0 with N (i,j) compositional answer data.Details of the training data for each task can be found in Appendix G.</p>
<p>Learning ComposableCoT achieves better zero-shot generalization.Table 1 shows that Com-posableCoT variants outperform all baselines on a range of settings for both models.Moreover, while having seen no compositional training data, our method achieves comparable or even better performance than supervised fine-tuning baselines with compositional supervision (e.g., last letter + multiplication).We also find that combining atomic CoT models trained on Composable CoT is better than combining models trained on standard CoT: ComposableCoT-Merge is better than StandardCoT-Merge in most settings, and so is ComposableCoT-MTL to StandardCoT-MTL.These indicate that the Composable CoT format leads to better "composability" at inference time.We note one error case of ComposableCoT-Merge on concatenation + last letter on Qwen 2.5-7B; we discuss it in details in Section 5.3.The optimal method to combine atomic CoT models differs for different settings.Regardless of using StandardCoT or ComposableCoT, combining atomic CoT models with model merging is generally better than with multitask learning for concatenation + multiplication, and multitask learning is better for Skill-Mix literary + rhetorical.We hypothesize that such differences are caused by the level of conflicts in the model parameter space between different pairs of atomic tasks.</p>
<p>Compositional Performance with Limited Supervision</p>
<p>We evaluate the performance of Composable CoT models after being further improved with one iteration of RFT using the limited compositional supervision.We compare it with multitask learning and continued fine-tuning baselines given the same compositional answer dataset D T (i,j) of size N (i,j) ≤ 500.For reference, we include the baseline of fine-tuning M 0 on the same compositional answer data.Details of the data condition can be found in Appendix G.</p>
<p>Table 2 shows that within the same budget of compositional training data, using RFT on top of ComposableCoT-MTL and ComposableCoT-Merge achieves the best compositional task performance, outperforming multitask learning and continued fine-tuning baselines across settings.</p>
<p>We further investigate if the performance is mainly driven by RFT or by learning Composable CoT format.We compare RFT upon StandardCoT-Merge with RFT upon ComposableCoT-Merge for LLama 2-7B, and StandardCoT-MTL with ComposableCoT-MTL for Qwen 2.5-7B. 2 Table 2 shows that RFT is a better way to improve the compositional task performance of StandardCoT models with compositional data than MTL and SFT.One explanation is that SFT or MTL based on the atomic CoT models using compositional answer data can lead to a distribution shift in the output space (from generating a CoT sequence to generating a direct answer), and mitigating such distribution shift requires more advanced techniques, usually called CoT internalization [26] or latent CoT [27], beyond the focus of this work; meanwhile, RFT does not have this problem.Moreover, RFT upon ComposableCoT models is generally better than RFT upon StandardCoT models. 3</p>
<p>Intrinsic Evaluation of Generated CoTs</p>
<p>To understand differences in models trained with different CoT formats, we conduct intrinsic evaluations on CoTs generated by ComposableCoT and StandardCoT models for zero-shot composition.</p>
<p>For the string operation tasks, we extract template-based patterns of each atomic CoT from the generated outputs of models evaluated on the compositional task.For Skill-Mix, we consider the CoT pattern of an atomic task to be used if the generated response explicitly mentions the skill corresponding to that atomic skill category.Table 3 shows results with models trained from Qwen 2.5-7B.Using the same combination method (model merging or MTL), combining ComposableCoT leads to consistently higher presence of both atomic CoT patterns in the generated responses compared to StandardCoT.Atomic models trained with the Composable CoT format therefore leverage the combination of learned skills in some form more frequently than StandardCoT.We note the exception of ComposableCoT-Merge on Concat + Multi which seldomly leverages both atomic CoT in the outputs, explaining to its low compositional performance, as mentioned in Section 5.1.We thus hypothesize that in some cases, model merging can still be an unstable combination method.</p>
<p>Examples of error cases in the generated CoTs can be found in Appendix H.</p>
<p>Related Work</p>
<p>As an important cognitive capability of humans [1,2], compositional generalization has been considered a core capability for human-level reasoning models [28,29].As defined by [28], compositional generalization has three components: systematicity, productivity, and primitive application.This work mainly focuses on systematicity, the capability of applying known components in unseen combinations.Systematic generalization has been considered difficult for LLMs because of the limitations of the transformer architecture and autoregressive generation [30,4], and has been explored extensively in the context of parsing [31,32,33,34].</p>
<p>Recent theoretical analyses show that the compositional reasoning capability of LLMs can improved by generating CoT [35,36], but empirical results show that non-trivial effort needs to be put through prompt engineering [37,38] or data selection [39,20,40,41] to observe such improvements with CoT [42], particularly in domains where compositional solutions to problems are crucial [43,44].</p>
<p>Prior work has explored more principled approaches, but they usually rely on heuristics to determine data quality [8,45] or involve computationally intensive methods [7,3].</p>
<p>We are inspired by a line of work on efficient methods for combining models of different capabilities.Past work on model merging [13,46,47] has shown that trained models can be merged to retain the union of their skills, including reasoning [48,49,50], but only limited prior work has shown any kind of skill composition [51].Our work is the first to use model merging for compositional generalization with CoT.We believe that other approaches for combining models could be used, such as learning methods that make modular updates [52,53].</p>
<p>Conclusion</p>
<p>We propose Composable Chain-of-Thought, a data augmentation scheme to convert CoT data of atomic reasoning skills into a format that facilitates inference-time compositional generalization.</p>
<p>Training atomic CoT models with Composable CoT and combining them with model merging or multitask learning leads to better zero-shot compositional reasoning performance than building models with the standard CoT format.Such a combined model can be further improved by a limited amount of compositional data with rejection sampling fine-tuning.Learning to reason with composable CoT shows a promising approach to improve compositional reasoning in LLMs, and could be extended to build more efficient and robust large reasoning models.</p>
<p>Limitations: Our experiments focus on pairwise compositional tasks where two atomic reasoning skills are composed and do not cover compositions of more than two atomic tasks.There is a lack of high-quality datasets that support n-way compositional tasks, and it is a non-trivial effort to collect an appropriate evaluation suite for the purpose.We note that our framework of Composable CoT generalizes to compositional tasks that consist of more than two atomic skills as discussed in Section 3.1.In addition, we only focus on small-scale datasets where both the atomic skills and compositional skills can be learned with a small amount of training data.Our focus here is on conducting controlled experiments, but we believe our methods and their principles can be scaled up to more complex settings.</p>
<p>A A Note on Composing Tasks</p>
<p>There exist various possible ways to combine atomic tasks into a compositional task with the combination function g.We simplify g into two types: (1) composite: the output of one atomic task is used as part of the input of another task, g(T i ,
T j ) = T i • T j or g(T i , T j ) = T j • T i ;
(2) concatenation: the outputs of the two atomic tasks are concatenated using the same input, g(T i , T j ) = T i ⊕ T j or g(T i , T j ) = T j ⊕ T i .Among tasks evaluated in Section 4, the string operation tasks need to be solved by a composite function, while the Skill-Mix task can be solved by either a composite function or a concatenation function.</p>
<p>B Design Choices for Constructing Composable CoT Data</p>
<p>When designing the proxy prefix CoT, we would like to consider the following constraints.(1) We do not assume any prior knowledge about what would possibly be put in the prefix CoT at inference time;</p>
<p>(2) We do not assume strong relevance between the proxy prefix CoT and the suffix, i.e., not all the information in the proxy prefix CoT is useful for predicting the suffix CoT and the final answer.</p>
<p>Based on these considerations, we experiment with the following variants:</p>
<p>• Random letters: We sample random letters from the alphabet to form a sequence of random lengths to simulate an arbitrary prefix CoT.</p>
<p>• Random from the prompt: We sample random letters and words from the prompt q to form a sequence of random lengths to simulate a prefix CoT in a similar distribution as the input distribution.</p>
<p>• Random text: We sample random sentences from OpenWebText [54] to simulate a prefix CoT drawn from the pre-training data distribution.</p>
<p>We evaluate these variants by fine-tuning models on Composable CoT datasets that only have suffix CoT examples.Note that this is different from the multitask learning setting discussed in Section 3.1 where the Composable CoT dataset consists of both the prefix CoT examples and the suffix CoT examples.This experiment mainly aims at stress testing the model's capability of learning a single atomic task with a given proxy prefix CoT variant.We use the same hyperparameter configurations for all proxy prefix variants for a given task.</p>
<p>We evaluate the fine-tuned models on the in-domain task in two settings: (1) In-domain prefix: we append the same type of prefix as we have used for training to the end of the prompt of the in-domain test example and evaluate the model on it; (2) Out-of-domain prefix: we randomly sample a prefix from the other two variants and append it to the end of the prompt of the in-domain test example and evaluate the model on it.We run experiments on the three string operation tasks and report the average performance.Table 4 shows that while using random letters as the proxy prefix leads to the worst in-domain performance, it generalizes the best to out-of-domain prefixes, which is a more desirable behavior.</p>
<p>C Details of String Operation Tasks</p>
<p>Last letter in alphabet We synthetically generate data for Last letter in alphabet.We randomly sample letters from the English alphabet of a random length and concatenate them into a sequence.</p>
<p>Then we extract the last letter from the sequence and derive the next letter following it in the alphabet.An example can be found in Example C.2.We automatically generate a chain-of-thought for each generated problem, using a fixed template shown in Example C.2.</p>
<p>ASCII multiplication Similarly, we randomly sample letters from the English alphabet of a random length and concatenate them into a sequence.Then, we randomly sample another letter s and randomly sample an integer a ∈ {1, ..., 9}.We find the ASCII value of s as f (s) and compute the product af (s) as the gold answer.An example can be found in Example C.3.We automatically generate a chain-of-thought for each generated problem, using a fixed template shown in Example C.3.</p>
<p>Letter concatenation</p>
<p>We follow [9] to generate the dataset by randomly sampling from the most popular first and last names in the United States and the United Kingdom from https:// namecensus.com and randomly concatenating them into a sequence of names.While the original task in [9] only requires concatenating the last letter of each name together, we raise the difficulty level by randomly asking for concatenations of the first, second, second-to-last, or the last letter.An example can be found in Example C.1.The CoT template is also shown in Example C.1.</p>
<p>Compositional tasks</p>
<p>We synthetically construct the compositional tasks of the string operation tasks in similar procedures as used to generate the atomic data.An example of last letter + ASCII multiplication can be found in Example C.4, concatenation + last letter in Example C.5, and concatenation + multiplication in Example C.6.We made a design decision to exclude one variant of concatenation + last letter that concatenates the last letter of each word and finds the next letter following the last letter in the concatenated sequence; this variant can be solved by the reasoning shortcut of only applying Last letter in alphabet rather than a composition of both.</p>
<p>C.1 Atomic Task Example: Letter Concatenation Example</p>
<p>[Instruction] Take the second-to-the-last letter of each word in the sequence and concatenate them in lower case: Tequan Monjur Khia Jodi-leigh answer [Chain-of-Thought + Answer String] The second-to-the-last letter of the 1st word is a.The second-to-the-last letter of the 2nd word is u.The second-to-the-last letter of the 3rd word is i.The second-to-the-last letter of the 4th word is g.So the answer is auig.</p>
<p>[Answer String] auig</p>
<p>C.2 Atomic Task Example: Last letter in alphabet</p>
<p>[Instruction] Find the Last letter in alphabet following the last letter in the sequence: wqsisibnnicdlpwqbnoicdcxcxrfoilpcbnixucbssssejxuzods answer:</p>
<p>[Chain-of-Thought + Answer String] The last letter is s, and the letter following it in alphabet is t.So the answer is t.We adapt the Skill-Mix dataset from [19].For each example, the model is given a natural language skill, its definition, an example of the skill, and a topic to focus on, and the model needs to write a grammatical sentence to demonstrate the skill on the topic.Because we mainly focus on pairwise composition, we only use the k = 2 and k = 1 composition sets of the Skill-Mix data.We apply the following modifications to the dataset to fit our setting of compositional reasoning.</p>
<ol>
<li>
<p>Filtering the categories of skills: We keep examples with skills of the rhetorical and literary categories out of the five categories from the original dataset.This is because the rhetorical and literay skills have the least overlap while other categories have more (e.g. the logical and rhetorical skills have a large body of overlaps).2. Removing the requirements of post-hoc explanation and refinement from the prompt.The original dataset evaluates models by prompting the models to first write a sentence, provide an explanation for the written sentence, and then do another round of refinement based on feedback from the grader (an LLM-as-a-judge).To fit into our setting of chain-of-thought reasoning and direct answering, we remove these irrelevant elements in the prompt.</p>
</li>
<li>
<p>Using a public test set: The original evaluation of [19] hides the test set from the public and models can only be evaluated based on API calls to the hidden test set.To ensure reproducibility of our results, we use a public test set collected by [22].</p>
</li>
</ol>
<p>As an open-ended generation task, Skill-Mix does not have a single ground truth sentence.[22] collect a silver-labeled dataset of sentences and their post-hoc explanations generated by GPT-4o [21] and graded as correct by the grader.We further prompt GPT-4o to transform the (sentence, explanation) pairs into (CoT, sentence) pairs to resolve potential coreference issues.Examples of the prompt, CoT traces, and sentence can be found in Examples D.1, D.2, and D.3.</p>
<p>D.1 Atomic Task Example: Skill-Mix-Literary</p>
<p>[Instruction] Greetings!I am interested in natural language processing and I was wondering if you could help me generate an example of text that illustrates multiple skills in semantics or syntax.The example should be a minimal natural piece of text with up to a few lines in the context of Vikings that illustrates all of the following skills: oxymoron.Please keep the text as short as possible, and make sure the concepts can be found fully from the text.</p>
<p>For reference, here are the definitions and examples for the concepts: <strong>oxymoron</strong>: A figure of speech in which apparently contradictory terms appear in conjunction.For example, "The hall was filled with a deafening silence."Here, the phrase "deafening silence" is an oxymoron.</p>
<p>Please start the minimal natural piece of text with 'Answer:'.</p>
<p>Thanks very much!</p>
<p>[Chain-of-Thought + Answer String] Explanation: The goal is to construct a sentence that showcases the use of an oxymoron while remaining relevant to the Viking theme.To achieve this, we focus on creating a phrase that contains seemingly contradictory terms.We begin with the context of Vikings enduring harsh weather conditions.The word "fiery" conveys an intense heat, while "frost" indicates severe cold.When these terms are combined into "fiery frost," they form an oxymoron as they depict both extreme temperature sensations simultaneously.This juxtaposition effectively creates a semantic paradox, leading to vivid imagery and emphasizing the harsh conditions faced by Vikings.</p>
<p>Answer: "In the bitter cold, the Viking felt a fiery frost."</p>
<p>[Answer String] Answer: "In the bitter cold, the Viking felt a fiery frost."</p>
<p>D.2 Atomic Task Example: Skill-Mix-Rhetorical</p>
<p>[Instruction] Greetings!I am interested in natural language processing and I was wondering if you could help me generate an example of text that illustrates multiple skills in semantics or syntax.The example should be a minimal natural piece of text with up to a few lines in the context of Hiking that illustrates all of the following skills: begging the question or assuming the conclusion.Please keep the text as short as possible, and make sure the concepts can be found fully from the text.</p>
<p>For reference, here are the definitions and examples for the concepts: <strong>begging the question or assuming the conclusion</strong>: A circular argument that paraphrases the question.For example, "Of course the earth is round; people have known this for hundreds of years."</p>
<p>Please start the minimal natural piece of text with 'Answer:'.</p>
<p>Thanks very much!</p>
<p>[Chain-of-Thought + Answer String] Explanation: To illustrate the concept of begging the question or assuming the conclusion within the context of hiking, we start by identifying a statement that relies on its own conclusion as a premise.First, recognize that the claim "hiking is beneficial" needs supporting evidence to be considered valid.However, if we use the phrase "because it's good for your health" as justification, we notice that it merely reiterates the initial claim using slightly different wording.This is because declaring something "beneficial" inherently implies a positive impact, such as being "good for your health."Thus, the reasoning becomes circular, as it depends on the same assumption it seeks to prove.</p>
<p>Answer: "Hiking is beneficial because it's good for your health."</p>
<p>[Answer String] Answer: "Hiking is beneficial because it's good for your health."</p>
<p>D.3 Compositional Task Example: Skill-Mix Literary + Rhetorical</p>
<p>[Instruction] Greetings!I am interested in natural language processing and I was wondering if you could help me generate an example of text that illustrates multiple skills in semantics or syntax.The example should be a minimal natural piece of text with up to a few lines in the context of Vikings that illustrates all of the following skills: anaphora resolution, begging the question or assuming the conclusion.Please keep the text as short as possible, and make sure the concepts can be found fully from the text.</p>
<p>For reference, here are the definitions and examples for the concepts: <strong>anaphora resolution</strong>: Resolving the antecedent of a pronoun or noun phrase.For example, "The car is falling apart, but it still works."Here , "it" is the anaphor and "car" is the antecedent.<strong>*begging the question or assuming the conclusion</strong>: A circular argument that paraphrases the question.For example, "Of course the earth is round; people have known this for hundreds of years."</p>
<p>Please start the minimal natural piece of text with 'Answer:'.</p>
<p>Thanks very much! [Answer String] Answer:</p>
<p>The Viking chief, undefeated thanks to his ship, asserted, "It remains unconquered because it's the 'Indomitable'."</p>
<p>D.2 Evaluation Metrics</p>
<p>We use GPT-4o-mini as the LLM-as-a-judge to grade the generated sentence using the exact same grading rubric as provided by [19]; the grader judges the quality of the sentence based on if: (1) All skills are used; (2) The sentence makes sense; (3) The sentence attaches to the given topic; (4) The sentence is short.We use the evaluation metrics for each generated sentence in [19], including the following:</p>
<ol>
<li>Full Marks: 1 if the generated sentence satisfies all four criteria above and 0 otherwise.</li>
</ol>
<p>Skill Fraction:</p>
<p>The fraction of skills being demonstrated if all the other three criteria are satisfied; 0 otherwise</p>
<p>We aggregate these metrics by averaging over all generated responses.In general, full marks evaluate the model's capability of writing a perfect sentence for the task, while skill fraction evaluates how good the model is at handling skills given that it is good at the other stylistic capabilities.We use Curator [55] for an efficient implementation of the evaluation pipeline.</p>
<p>E Single-Task Learning Performance</p>
<p>We report the single-task learning performance of the atomic CoT models by evaluating them on the in-domain atomic tasks.We would like the atomic tasks to be easy to learn to reflect the practical settings where we train models on basic, easy-to-learn skills and generalize to harder, unseen tasks.The training data conditions and hyperparameters for training can be found in Appendix F. Table 5 shows that all atomic tasks we evaluate are learnable within a small amount of training data (N i , N j ≤ 500).</p>
<p>In addition, we observe that training on ComposableCoT or StandardCoT does not lead to consistent differences in atomic CoT performance, while the exception is on Skill-Mix-Rhetorical for Llama 2-7B where fine-tuning on ComposableCoT outperforms fine-tuning on StandardCoT by a large margin.</p>
<p>F Training Configurations F.1 General Configurations</p>
<p>We conduct all fine-tuning experiments with LoRA [25] using the following set of hyperparameters: we use a rank of 8, α = 16, and a dropout rate of 0.2 to prevent overfitting.We apply LoRA adapters to all linear modules, including the attention matrices Q, K, V and MLP matrices of all layers.We use bfloat16 precision for training and we use the efficient implementation of LoRA by LlamaFactory [56].We use a training batch size of 4 and train for 5 epochs for all experiments that share the same number of training data; for methods that use a potentially smaller amount of training data (e.g.RFT methods usually get fewer data examples than the number of compositional training data provided, depending on how many correct responses we can sample from the model), we adjust the batch size to match the number of steps.</p>
<p>F.2 Configuration for Rejection Sampling Fine-tuning</p>
<p>In addition to the sampling parameters (see Section 4), we consider the following configuration of RFT for sampling the correct responses: if the model generates multiple correct responses for a given question, we only randomly select one of them to be added into the RFT dataset D RFT .In this way we ensures the diversity of examples in D RFT so that the dataset will not be filled with samples from a small set of examples where the model is good at.</p>
<p>F.3 Hyperparameters: Learning Rate</p>
<p>We find in preliminary experiments that learning rate is the most important hyperparameter for the fine-tuning experiments of our interest.We perform hyperparameter sweeps for learning rate over the space of {5e − 3, 1e − 3, 5e − 4, 1e − 4, 5e − 5} on a validation set for each experiment.The optimal learning rate for each method for the experiments with compositional supervision in Table 6.</p>
<p>F.4 Hyperparameters: Model Merging</p>
<p>For methods that use model merging as the combination, we use Task Arithmetic [47] to combine the atomic CoT models.We perform a hyperparameter sweep for the scalars α and β over the space of α ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9} and β = 1 − α on a validation set for each task.</p>
<p>G Data Statistics G.1 General Data Conditions for Experiments</p>
<p>Table 7 summarizes the number of training data and test data used in the evaluations in Sections 5.1 and 5.2.Note that for letter concatenation + multiplication we have two sizes of the compositional training data for Llama 2-7B and Qwen 2.5-7B: this is because all methods on Llama 2-7B perform poorly on zero-shot evaluation for this task and we need a slightly larger amount of compositional training data so that different methods can start to show distinguishable compositional task performance from each other.Regardless, we still consider 500 to be a reasonably small amount of training data, satisfying our ideal data conditions defined earlier.</p>
<p>Figure 1 :
1
Figure 1: (a) Composable Chain-of-thought (left): A compositional task involves two separate atomic capabilities.We use a data augmentation scheme to teach LLMs CoT formats that can be combined at inference time to address compositional tasks.(b) Pipeline for learning Composable CoT (right): Models trained on composable CoT data of atomic skills can be combined with multitask learning or model merging for zero-shot compositional generalization, and can be further improved by rejection sampling fine-tuning on limited compositional supervision.</p>
<p>…Figure 2 :
2
Figure 2: Construction of Composable CoT data with k chain-of-thought tags.We insert k − 1 proxy prefixes at the end of the prompt, before the generation of t k .</p>
<p>Figure 3 :
3
Figure 3: Summary of settings for methods evaluated.Names in the results table reference configurations described in this figure; e.g., ComposableCoT-Merge uses ComposableCoTs with model merging, and in the zero-shot setting does not use further tuning.</p>
<p>[Answer String] t C. 3 C. 4 C. 6
346
Atomic Task Example: ASCII Multiplication [Instruction] Find the ASCII value of the letter after '<letter>' and multiply the ASCII value by 2: byaxaxcpoteznwnwseselyjlretxtxcbfvmfezbycplymfotjbfv jlhotzjbjcpycbtzhorepyjckofj <letter> d answer: [Chain-of-Thought + Answer String] The ASCII value of the letter d is 100, and multiplying the ASCII value by 2 gives us 200.So the answer is 200.[Answer String] 200 Compositional Task Example: Last letter + ASCII Multiplication [Instruction] Find the ASCII value of the Last letter in alphabet following the last letter in the sequence and multiply the ASCII value by 5: knnxqsxvshqugxfuquljumsbihgxvqihnxuufuknxvumuupkpkshljqsbkiz answer: [Answer String] 485 C.5 Compositional Task Example: Concatenation + Last Letter [Instruction] Take the second-to-the-last letter of each word in the sequence, concatenate them in lower case, and find the Last letter in alphabet following the last letter in the sequence of the concatenated letters: Tyjai Ahijah Denzil Amine answer: [Answer String] o Compositional Task Example: Concatenation + Multiplication [Instruction] Take the second-to-the-last letter of each word in the sequence, concatenate them in lower case, then find the ASCII value of the last letter in the sequence of the concatenated letters, and multiply the ASCII value by 3: Zarriah Amylee Li Javarie answer: [Answer String] 315 D Details of Skill-Mix Tasks D.1 Modifications of Skill-Mix</p>
<p>Combination method Further training on compositional data
CoT formatStandard CoTMergeMulti-task Learning (MTL)(past work)Composable CoT+(this work;  §3.1)one model trained on eachone model traineddataset, then mergedon both datasets
CFT: continued fine-tuning on answer-only compositional data RFT: rejection sampling of CoTs, fine-tuning on those leading to correct answers</p>
<p>Table 1 :
1
Zero-shot compositional generalization of ComposableCoT with different combination approaches vs. baselines.Without any compositional supervision, using model merging or multitask learning to combine atomic CoT models trained on Composable CoT data outperforms baselines across settings and models, and is sometimes comparable to SFT with compositional supervision.
Last Letter ConcatConcatSkill-Mix LiteraryMethods+ Mult+ Last Letter + Mult+ RhetoricalEMEMEMFull Marks Skill FractionLlama 2-7BSFT on Base Model with Compositional Supervision3.15.09.035.560.1Few-shot Answer1.00.00.04.116.4Few-shot CoT2.03.01.07.323.1StandardCoT-Merge2.012.52.311.031.6ComposableCoT-Merge (Ours)16.019.13.019.637.1StandardCoT-MTL5.00.00.017.638.7ComposableCoT-MTL (Ours)18.76.53.122.949.9Qwen 2.5-7BSFT on Base Model with Compositional Supervision4.631.92.035.560.3Few-shot Answer2.40.02.734.756.0Few-shot CoT2.00.021.331.841.6StandardCoT-Merge70.454.877.029.848.0ComposableCoT-Merge (Ours)95.419.275.439.662.1StandardCoT-MTL3.660.972.142.058.2ComposableCoT-MTL (Ours)96.363.374.349.066.7</p>
<p>Table 2 :
2
Compositional task performance of rejection sampling fine-tuning (RFT) upon merged Composable atomic CoT models and other baselines.Mult stands for ASCII multiplication and concat stands for letter concatenation.SFT stands for supervised fine-tuning with the compositional answer data; CFT stands for continued fine-tuning; MTL stands for multitask learning method.Results on last letter + mult are omitted because the zero-shot performance already saturates.RFT on ComposableCoT variants achieves the best compositional task performance using the same amount of compositional answer data.
Last Letter ConcatConcatSkill-Mix LiteraryCategory Method+ Mult+ Last Letter + Mult+ RhetoricalEMEMEMFull Marks Skill FractionLlama 2-7BSFT on Base Model3.15.09.035.560.1SFTCFT on StandardCoT-Merge2.016.014.044.165.1CFT on StandardCoT-MTL3.026.011.038.062.1MTLStandardCoT + Comp Answer5.046.013.322.945.5RFTStandardCoT-Merge ComposableCoT-Merge (Ours)0.0 72.023.0 46.029.7 40.026.1 45.352.0 66.6Qwen 2.5-7BSFT on Base Model-31.92.035.560.3SFTCFT on StandardCoT-Merge-41.19.351.071.4CFT on StandardCoT-MTL-60.312.734.756.3MTLStandardCoT + Comp Answer-65.17.141.255.3RFTStandardCoT-MTL ComposableCoT-MTL (Ours)--82.1 86.989.0 88.444.9 57.663.4 71.5</p>
<p>Table 3 :
3
Intrinsic evaluation of the generated CoTs from atomic CoT models evaluated on the compositional task in the zero-shot setting."% T 1 CoT" denotes the percentage of generated responses that use the CoT format of the first atomic task of the composition, and likewise for the second.† denotes that the ComposableCoT method has a significantly higher "% Both CoT" than the StandardCoT counterpart at the 0.01 level using a paired bootstrap test.Combined Composable CoT models generate responses including both atomic CoT patterns more frequently than combined atomic CoT models.
MethodPerformance % T1 CoT % T2 CoT % Both CoTStandardCoT-Merge70.485.395.185.3Last LetterComposableCoT-Merge95.4100.0100.0† 100.0+ MultStandardCoT-MTL3.60.0100.00.0ComposableCoT-MTL96.398.9100.0† 98.9StandardCoT-Merge77.090.398.790.0ConcatComposableCoT-Merge75.491.6100.091.6+ Last Letter StandardCoT-MTL72.199.732.132.1ComposableCoT-MTL74.3100.083.1† 81.3StandardCoT-Merge54.8100.099.499.4ConcatComposableCoT-Merge19.244.660.517.7+ MultStandardCoT-MTL60.9100.066.766.7ComposableCoT-MTL63.3100.085.9† 85.0Skill-MixStandardCoT-Merge29.860.059.235.9LiteraryComposableCoT-Merge39.664.166.9† 43.3+ RhetoricalStandardCoT-MTL42.065.358.037.6ComposableCoT-MTL49.064.565.7
† 42.0</p>
<p>Table 4 :
4
Performance of atomic CoT models fine-tuned on different variants of proxy prefix on Llama 2-7B.Using random letters as the proxy prefix achieves the best out-of-domain performance when evaluated with an unseen prefix at inference time.
Type of Proxy PrefixExact Match Accuracy In Domain Prefix Out-of-Domain PrefixRandom Letters83.090.0Random from the Prompt86.482.5Random Text90.670.0</p>
<p>Table 5 :
5
Single-task learning performance by evaluating the atomic CoT models on the in-domain atomic tasks.
CoT FormatLast Letter ASCII Mult Concat EM EM EM Full Marks Skill Fraction Full Marks Skill Fraction Skill-Mix Literary Skill-Mix RhetoricalLlama 2-7BStandardCoT100.085.783.063.563.553.353.3ComposableCoT95.086.077.071.471.472.472.4Qwen 2.5-7BStandardCoT90.099.077.477.477.670.570.5ComposableCoT99.499.777.377.477.676.781.9</p>
<p>Table 6 :
6
Optimal learning rate for each method in the experiments with compositional supervision.
Last Letter ConcatConcat Skill-Mix LiteraryCategory Method+ Mult+ Last Letter + Mult+ RhetoricalLlama 2-7BSFT on Base Model1e-31e-35e-45e-4SFTCFT on StandardCoT-Merge1e-35e-41e-41e-4CFT on StandardCoT-MTL1e-41e-41e-41e-3MTLStandardCoT + Comp Answer1e-35e-41e-35e-4RFTStandardCoT-Merge ComposableCoT-Merge (Ours)-1e-41e-3 1e-41e-3 1e-35e-4 1e-3Qwen 2.5-7BSFT on Base Model-1e-31e-35e-4SFTCFT on StandardCoT-Merge-5e-45e-41e-4CFT on StandardCoT-MTL-1e-31e-31e-3MTLStandardCoT + Comp Answer-5e-45e-41e-3RFTStandardCoT-MTL ComposableCoT-MTL (Ours)--1e-3 1e-31e-4 1e-35e-4 5e-4
To choose the ablation baselines to compare with, we take the StandardCoT model that wins on more categories and consider a model going from zero performance to nonzero as a "double win". On Llama 2-7B, StandardCoT-Merge achieves nonzero accuracy on two tasks that StandardCoT-MTL achieves zero accuracy on, making it a better starting point for further fine-tuning. On Qwen 2.5-7B, both StandardCoT models have nonzero performance, and we choose StandardCoT-MTL as its performance is generally higher.
Note that RFT requires a reasonably good model to start with: RFT fails for StandardCoT-Merge on Llama 2-7B because it is unable to sample enough correct responses for training from this StandardCoT-Merge model.
AcknowledgmentsThanks to Katrin Erk, Kyle Mahowald, and other members of the TAUR lab for helpful discussion and suggestions.This work was partially supported by the Sloan Foundation, a grant from Open Philanthropy, NSF CAREER Award IIS-2145280, the NSF AI Institute for Foundations of Machine Learning (IFML), and the NSF under Cooperative Agreement 2421782 and the Simons Foundation grant MPS-AI-00010515 awarded to the NSF-Simons AI Institute for Cosmic Origins -CosmicAI, https://www.cosmicai.org/.This research has been supported by computing support on the Vista GPU Cluster through the Center for Generative AI (CGAI) and the Texas Advanced Computing Center (TACC) at the University of Texas at Austin.G.2 Training Data Used by Each MethodWe show a detailed breakdown in Table8of the number of training data used by each zero-shot method for both models and in Table9for Qwen 2.5-7B by each method with compositional answer data in the experiments in Section 5.2.Note that the statistics for Llama 2-7B in the setting with compositional supervision are mostly the same except N (i,j) = 500 for concat + last letter and concat + mult.H Error AnalysesIn addition to not being able to perform the individual atomic task correctly, we show three types of common errors made by ComposableCoT variants in the zero-shot compositional evaluation setting.1. Example H.1 shows an example where the generated CoT is only able to replicate CoT of one atomic CoT and repeat the same CoT in the prefix and suffix.2. Example H.2 shows an example where the combined model fails to continue generation after generating the prefix CoT.This is a common error for Composable models combined with model merging.3. Example H.3 shows an example where the combined model uses the wrong atomic CoT in the prefix that should have been used in the suffix.H.1 Error Case: Replicating One Atomic CoT Pattern[Instruction] Take the first letter of each word in the sequence, concatenate them in lower case, and find the next letter in alphabet following theI Computation resourcesWe conducted all our experiments primarily on a server with NVIDIA RTX A6000 48GB GPU, and a server with GPU of NVIDIA GH200 120GB.Each individual experiment can be run on a single A6000 of 48 GB within 1 hour.J LicensingWe use the following publicly available datasets from prior works with open licenses.
Compositional reasoning in early childhood. Steven Piantadosi, Richard Aslin, In PloS one. 11September 2016</p>
<p>8-month-old infants spontaneously learn and generalize hierarchical rules. Denise M Werchan, Anne G E Collins, Michael Joshua Frank, Dima Amso, Psychological Science. 262015</p>
<p>Meta-learning to compositionally generalize. Henry Conklin, Bailin Wang, Kenny Smith, Ivan Titov, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Xiang Sanyal, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>. Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J L Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R J Chen, R L Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, S S Shuting Pan, Shuang Li, Shaoqing Zhou, Shengfeng Wu, Tao Ye, Tian Yun, Tianyu Pei, T Sun, Wangding Wang, Wanjia Zeng, Wen Zhao, Wenfeng Liu, Wenjun Liang, Wenqin Gao, Wentao Yu, W L Zhang, Wei Xiao, Xiaodong An, Xiaohan Liu, Xiaokang Wang, Xiaotao Chen, Xin Nie, Xin Cheng, Xin Liu, Xingchao Xie, Xinyu Liu, Xinyuan Yang, Xuecheng Li, Xuheng Su, X Q Lin, Xiangyue Li, Xiaojin Jin, Xiaosha Shen, Xiaowen Chen, Xiaoxiang Sun, Xinnan Wang, Xinyi Song, Xianzu Zhou, Xinxia Wang, Y K Shan, Y Q Li, Y X Wang, Yang Wei, Yanhong Zhang, Yao Xu, Yao Li, Yaofeng Zhao, Yaohui Sun, Yi Wang, Yichao Yu, Yifan Zhang, Yiliang Shi, Ying Xiong, Yishi He, Yisong Piao, Yixuan Wang, Yiyang Tan, Yiyuan Ma, Yongqiang Liu, Yuan Guo, Yuduan Ou, Yue Wang, Yuheng Gong, Yujia Zou, Yunfan He, Yuxiang Xiong, Yuxiang Luo, Yuxuan You, Yuyang Liu, Y X Zhou, Yanhong Zhu, Yanping Xu, Yaohui Huang, Yi Li, Yuchen Zheng, Yunxian Zhu, Ying Ma, Yukun Tang, Yuting Zha, Z Z Yan, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhenda Xu, Zhengyan Xie, Zhewen Zhang, Zhicheng Hao, Zhigang Ma, Zhiyu Yan, Zihui Wu, Zijia Gu, Zijun Zhu, Zilin Liu, Ziwei Li, Xie, arXiv:2501.129482025Ziyang Song, Zizheng PanarXiv preprintZhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</p>
<p>QwQ-32B: Embracing the Power of Reinforcement Learning. Qwen Team, March 2025</p>
<p>Easy-to-hard generalization: Scalable alignment beyond human supervision. Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, Chuang Gan, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>The unreasonable effectiveness of easy training data for hard tasks. Peter Hase, Mohit Bansal, Peter Clark, Sarah Wiegreffe, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Editing models with task arithmetic. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, Ali Farhadi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Merging Models with Fisher-Weighted Averaging. S Michael, Colin Matena, Raffel, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>TIES-merging: Resolving interference when merging models. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, Mohit Bansal, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>STaR: Bootstrapping Reasoning With Reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>RAFT: Reward ranked finetuning for generative foundation model alignment. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Shum Kashun, Tong Zhang, Transactions on Machine Learning Research. 2023</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou, 2024</p>
<p>The unreliability of explanations in few-shot prompting for textual reasoning. Xi Ye, Greg Durrett, Proceedings of NeurIPS. NeurIPS2022</p>
<p>SKILL-MIX: a flexible and expandable family of evaluations for AI models. Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, Sanjeev Arora, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. : Openai, Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, Alex Aleksander M Ądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Tachard Renzin, Alexander Passos, Alexi Kirillov, Alexis Christakis, Ali Conneau, Allan Kamali, Allison Jabri, Allison Moyer, Amadou Tam, Amin Crookes, Amin Tootoochian, Ananya Tootoonchian, Andrea Kumar, Andrej Vallone, Andrew Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrey Tulloch, Angela Mishchenko, Angela Baek, Antoine Jiang, Antonia Pelisse, Anuj Woodford, Arka Gosalia, Ashley Dhar, Avi Pantuliano, Avital Nayak, Barret Oliver, Behrooz Zoph, Ben Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Benjamin Wang, Beth Zweig, Blake Hoover, Bob Samic, Bobby Mcgrew, Bogo Spero, Bowen Giertler, Brad Cheng, Brandon Lightcap, Brendan Walkin, Brian Quinn, Brian Guarraci, Bright Hsu, Brydon Kellogg, Camillo Eastman, Carroll Lugaresi, Cary Wainwright, Cary Bassin, Casey Hudson, Chad Chu, Chak Nelson, Chan Li, Channing Jun Shern, Charlotte Conger, Chelsea Barette, Chen Voss, Cheng Ding, Chong Lu, Chris Zhang, Chris Beaumont, Chris Hallacy, Christian Koch, Christina Gibson, Christine Kim, Christine Choi, Christopher Mcleavey, Claudia Hesse, Clemens Fischer, Coley Winter, Colin Czarnecki, Colin Jarvis, Constantin Wei, Dane Koumouzelis, Daniel Sherburn, Daniel Kappler, Daniel Levin, David Levy, David Carr, David Farhi, David Mely, David Robinson, Denny Sasaki, Dev Jin, Dimitris Valladares, Doug Tsipras, Li, Phong Duc, Duncan Nguyen, Edede Findlay, Edmund Oiwoh, Ehsan Wong, Elizabeth Asdar, Elizabeth Proehl, Eric Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eugene Wallace, Evan Brevdo, Farzad Mays, Felipe Petroski Khorasani, Filippo Such, Francis Raso, Fred Zhang, Freddie Von Lohmann, Gabriel Sulit, Gene Goh, Geoff Oden, Giulio Salmon, Greg Starace, Hadi Brockman, Haiming Salman, Haitang Bao, Hannah Hu, Haoyu Wong, Heather Wang, Heather Schmidt, Heewoo Whitney, Hendrik Jun, Henrique Kirchner, Hongyu Ponde De Oliveira Pinto, Huiwen Ren, Hyung Won Chang, Ian Chung, Kivlichan, O' Ian, Ian O' Connell, Ian Connell, Ian Osband, Ian Silber, Ibrahim Sohl, Ikai Okuyucu, Ilya Lan, Ilya Kostrikov, Ingmar Sutskever, Ishaan Kanitscheider, Jacob Gulrajani, Jacob Coxon, Jakub Menick, James Pachocki, James Aung, James Betker, James Crooks, Jamie Lennon, Jan Kiros, Jane Leike, Jason Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jay Wolfe, Jeff Chen, Jenia Harris, Jessica Varavva, Jessica Gan Lee, Ji Shieh, Jiahui Lin, Jiayi Yu, Jie Weng, Jieqi Tang, Joanne Yu, Joaquin Quinonero Jang, Joe Candela, Joe Beutler, Joel Landers, Johannes Parish, John Heidecke, Jonathan Schulman, Jonathan Lachman, Jonathan Mckay, Jonathan Uesato, Jong Wook Ward, Joost Kim, Jordan Huizinga, Jos Sitkin, Josh Kraaijeveld, Josh Gross, Josh Kaplan, Joshua Snyder, Joy Achiam, Joyce Jiao, Juntang Lee, ; Zhuang, Krithika Kiel Howe, Kyle Muthukumar, Lama Luther, Larry Ahmad, Lauren Kai, Lauren Itow, Leher Workman, Leo Pathak, Li Chen, Lia Jing, Liam Guy, Liang Fedus, Lien Zhou, Lilian Mamitsuka, Lindsay Weng, Lindsey Mccallum, Long Held, Louis Ouyang, Lu Feuvrier, Lukas Zhang, Lukasz Kondraciuk, Luke Kaiser, Luke Hewitt, Lyric Metz, Mada Doshi, Maddie Aflak, Madelaine Simens, Madeleine Boyd, Marat Thompson, Mark Dukhan, Mark Chen, Mark Gray, Marvin Hudnall, Marwan Zhang, Mateusz Aljubeh, Matthew Litwin, Max Zeng, Maya Johnson, Mayank Shetty, Meghan Gupta, Mehmet Shah, Meng Jia Yatbaz, Mengchao Yang, Mia Zhong, Mianna Glaese, Michael Chen, Michael Janner, Michael Lampe, Michael Petrov, Michele Wu, Michelle Wang, Michelle Fradin, Miguel Pokrass, Sean Castro, Sean Grove, Shamez Metzger, Shantanu Hermani, Shengjia Jain, Sherwin Zhao, Shino Wu, Shirong Jomoto, Wu, Shuaiqi, Sonia Xia, Spencer Phene, Srinivas Papay, Narayanan, arXiv:2410.21276Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray,2024MikhailPavlov, Miles Brundage, Miles Wang; Steve Coffey; Steve Lee; Stewart Hall; Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce WaltersMiguel Oom Temudo de Castro ; Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal PatwardhanarXiv preprintTyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. GPT-4o System Card</p>
<p>Can models learn skill composition from examples?. Haoyu Zhao, Simran Kaur, Dingli Yu, Anirudh Goyal, Sanjeev Arora, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.092882023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey EdunovarXiv preprintand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, arXiv:2412.15115Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. Xingzhang Ren, Xuancheng Ren,2025arXiv preprint</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Yuntian Deng, Yejin Choi, Stuart Shieber, arXiv:2405.14838From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step. 2024arXiv preprint</p>
<p>Training large language models to reason in a continuous latent space. Shibo Hao, Sainbayar Sukhbaatar, Dijia Su, Xian Li, Zhiting Hu, Jason Weston, Yuandong Tian, arXiv:2412.067692024arXiv preprint</p>
<p>Connectionism and cognitive architecture: A critical analysis. Jerry A Fodor, Zenon W Pylyshyn, Cognition. 281988</p>
<p>Human-like systematic generalization through a metalearning neural network. M Brenden, Marco Lake, Baroni, Nature. 6232023</p>
<p>Limits of transformer language models on learning to compose algorithms. Jonathan Thomm, Giacomo Camposampiero, Aleksandar Terzic, Michael Hersche, Bernhard Schölkopf, Abbas Rahimi, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>COGS: A compositional generalization challenge based on semantic interpretation. Najoung Kim, Tal Linzen, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Bonnie Webber, Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. Brenden Lake, Marco Baroni, Proceedings of the 35th International Conference on Machine Learning. Jennifer Dy, Andreas Krause, the 35th International Conference on Machine LearningPMLRJul 201880</p>
<p>Learning to recombine and resample data for compositional generalization. Ekin Akyürek, Afra Feyza Akyürek, Jacob Andreas, International Conference on Learning Representations. 2021</p>
<p>Compositional semantic parsing with large language models. Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Chain of thought empowers transformers to solve inherently serial problems. Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Dissecting chain-of-thought: Compositionality through in-context filtering and learning. Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, Samet Oymak, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Skills-in-context: Unlocking compositionality in large language models. Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, Jianshu Chen, Findings of the Association for Computational Linguistics: EMNLP 2024. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, Miami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, Furu Wei, arXiv:2406.11698Meta reasoning for large language models. 2024arXiv preprint</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Diverse demonstrations improve in-context compositional generalization. Itay Levy, Ben Bogin, Jonathan Berant, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaJuly 20231</p>
<p>Complementary explanations for effective in-context learning. Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Veselin Stoyanov, Greg Durrett, Ramakanth Pasunuru, Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning. Zayne Rea Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Knowledge crosswords: Geometric knowledge reasoning with large language models. Wenxuan Ding, Shangbin Feng, Yuhan Liu, Zhaoxuan Tan, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov, Findings of the Association for Computational Linguistics: ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024</p>
<p>LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation. Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, Danqi Chen, arXiv:2501.054142025arXiv preprint</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. the 26th Annual International Conference on Machine Learning, ICML '09New York, NY, USAAssociation for Computing Machinery2009</p>
<p>Merging by matching models in task parameter subspaces. Derek Tam, Mohit Bansal, Colin Raffel, Transactions on Machine Learning Research. 2024</p>
<p>Editing models with task arithmetic. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, Ali Farhadi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Unlocking efficient long-to-short llm reasoning with model merging. Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, Mingxuan Yuan, arXiv:2503.206412025arXiv preprint</p>
<p>. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Y Xuehai Pan, Yang Charles, Yangyang Li, Yangyang Hu, Yanru Liu, Yejie Chen, Yibo Wang, Yidao Liu, Yifeng Qin, Ying Liu, Yiping Yang, Yulun Bao, Yuxin Du, Yuzhi Wu, Zaida Wang, Zhaoji Zhou, Zhaowei Wang, Zhen Li, Zheng Zhu, Zhexu Zhang, Zhilin Wang, Zhiqi Yang, Zihao Huang, Huang, arXiv:2501.125992025arXiv preprintZiyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms</p>
<p>Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, Xinchao Wang, arXiv:2502.09601CoT-Valve: Length-Compressible Chain-of-Thought Tuning. 2025arXiv preprint</p>
<p>Realistic evaluation of model merging for compositional generalization. Derek Tam, Yash Kant, Brian Lester, Igor Gilitschenski, Colin Raffel, arXiv:2409.183142024arXiv preprint</p>
<p>LoFiT: Localized Fine-tuning on LLM Representations. Fangcong Yin, Xi Ye, Greg Durrett, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Transformer-squared: Self-adaptive LLMs. Qi Sun, Edoardo Cetin, Yujin Tang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>. Aaron Gokaslan, Vanya Cohen, 2019OpenWebText Corpus</p>
<p>Curator: A Tool for Synthetic Data Creation. Trung * Ryan* Marten, Charlie Vu, Cheng-Jie, Kartik Ji, Shreyas Sharma, Alex Pimpalgaonkar, Maheswaran Dimakis, Sathiamoorthy, January 2025</p>
<p>LlamaFactory: Unified efficient fine-tuning of 100+ language models. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Yixin Cao, Yang Feng, Deyi Xiong, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20243System Demonstrations)</p>            </div>
        </div>

    </div>
</body>
</html>
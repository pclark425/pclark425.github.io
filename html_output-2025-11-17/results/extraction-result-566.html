<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-566 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-566</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-566</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-1c734a14c2325cb76783ca0431862c7f04a69268</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1c734a14c2325cb76783ca0431862c7f04a69268" target="_blank">Deep Domain Confusion: Maximizing for Domain Invariance</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant and shows that a domain confusion metric can be used for model selection to determine the dimension of an adaptationlayer and the best position for the layer in the CNN architecture.</p>
                <p><strong>Paper Abstract:</strong> Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e566.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e566.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMD-regularization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Mean Discrepancy regularization (MMD) applied as domain confusion loss</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of the Maximum Mean Discrepancy metric as a differentiable domain-confusion loss in a deep CNN: MMD is computed on activations of an adaptation layer to penalize differences between source and target feature means, and is used both for model-selection (depth/width) and as a regularizer during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Maximum Mean Discrepancy (MMD) as a domain confusion loss</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>MMD measures the distance between two distributions by comparing the means of their representations in feature space: empirically MMD(X_S,X_T) = ||(1/|X_S|) sum_{x_s in X_S} phi(x_s) - (1/|X_T|) sum_{x_t in X_T} phi(x_t)||. In this paper MMD is computed on CNN activations at a chosen adaptation (bottleneck) layer over minibatches of source and target examples. The squared MMD term is added to the classification loss (L = L_C + lambda * MMD^2) so gradients from MMD backpropagate through the network, encouraging the learned representation to make source and target distributions similar. The authors also compute MMD on fixed pre-trained layer activations to select which network layer to place the adaptation layer, and they compute MMD across candidate adaptation layer widths to choose dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data-analysis regularizer</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Statistics / machine learning (kernel methods); historically used in other areas including bioinformatics (see ref. [6])</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Computer vision / deep learning (visual domain adaptation using CNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Integrated empirical MMD into a supervised deep CNN training pipeline: (1) compute MMD over minibatches of CNN activations (phi is the adaptation-layer activation) rather than as a kernel-based post-hoc statistic; (2) add MMD^2 as a differentiable loss term with weight lambda (set to 0.25 in experiments); (3) use MMD both as a model-selection metric for choosing layer depth (which layer's activations minimize MMD) and adaptation-layer width (grid search over dimensionalities), and as a regularizer during backpropagation-based fine-tuning; (4) place the adaptation 'bottleneck' layer (chosen after fc7) and compute MMD at that layer; (5) adjust learning rates (new layers trained at 10x base rate) to accommodate the modified optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - applying MMD as a domain-confusion regularizer and model-selection metric produced substantially improved domain-adaptation performance on the Office dataset: supervised adaptation results (multi-class accuracies) reported for the proposed method were 84.1% (A->W), 95.4% (D->W), 96.3% (W->D) with an average reported 91.9%, exceeding prior methods; unsupervised adaptation averaged 81.2%, also exceeding prior methods. The authors show qualitative evidence (t-SNE) of domain mixing and quantitative evidence that MMD-guided layer/dimension choices correlate with better target performance.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Potential pitfalls noted: choice of regularization weight lambda is critical (too small — no effect; too large — collapsed/degenerate representations); irregularities in MMD-vs-performance curves (sampling and coarse grid over widths can produce noisy model-selection signals); small target labeled datasets make tuning harder; computational cost of fine-tuning with additional loss and grid searches over widths.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of a strong pre-trained CNN (providing meaningful phi), the fact that MMD is computed on learned representations (so it aligns naturally with CNN activations), ability to compute MMD cheaply on minibatches, and the differentiability of the empirical MMD term which allows backpropagation-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires access to source and target data (labels only needed for classification branch), a differentiable implementation of MMD over minibatches, the ability to fine-tune a pre-trained CNN and to add an adaptation layer, and hyperparameter tuning (lambda, adaptation-layer width, placement). Computational resources to fine-tune deep networks are required.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately high within visual domain-adaptation tasks — authors demonstrate both supervised and unsupervised adaptation on multiple domain shifts; method is general in principle for other deep architectures and could be applied to other modalities where a learned representation phi exists, but success depends on having representations amenable to aligning means and on appropriate hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles (statistical domain discrepancy measure applied as a computational loss), with instrumental/technical know-how (how to integrate MMD into CNN training, minibatch computation, hyperparameter choices).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Domain Confusion: Maximizing for Domain Invariance', 'publication_date_yy_mm': '2014-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e566.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e566.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ImageNet pretraining transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer of CNN representations pre-trained on ImageNet to small-target visual domains</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a deep CNN pre-trained on a large-scale dataset (ImageNet) as the base representation and adapting it for target visual domains (Office) by selecting activations (fc7) and fine-tuning with an added adaptation layer and MMD regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Pre-trained deep CNN feature transfer and fine-tuning (ImageNet -> Office)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A CNN (Krizhevsky et al. architecture) pre-trained on large-scale ImageNet classification is used as the starting representation. The authors evaluate activations from different fully-connected layers (fc6, fc7, etc.) using MMD to select the layer most domain-invariant (fc7). They then add a low-dimensional 'bottleneck' adaptation layer after the selected layer, choose its width via MMD-guided grid search, and fine-tune the network jointly on classification loss (source labels and available target labels in supervised case) and the MMD domain confusion loss. New layers are trained with increased learning rates while lower layers are updated slowly.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / transfer learning / fine-tuning protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Large-scale image classification (ImageNet; general computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Small-sample visual domain adaptation tasks (Office dataset: Amazon, DSLR, Webcam)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Beyond direct feature extraction, the pre-trained model is modified by inserting a bottleneck adaptation layer (chosen to be after fc7 based on MMD), selecting its dimensionality by grid search guided by MMD, and fine-tuning the whole network with a joint loss (classification + lambda*MMD^2). Learning rates for newly initialized layers (adaptation layer and classifier) are set 10x higher. The training procedure is also run in both supervised (few labeled target examples) and unsupervised (no target labels) modes with MMD always computed on all data.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - produced state-of-the-art results on Office benchmarks: supervised average reported 91.9% multi-class accuracy across three transfer tasks and unsupervised average 81.2%, outperforming previously published methods (tables 1 and 2). The method especially excelled in mixing domain data within class clusters (t-SNE) and preventing overfitting compared to naive fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Challenges include small target data amounts making direct fine-tuning risky (overfitting), need to choose where to insert the adaptation layer and what width to use (necessitating MMD-guided model selection and grid searches), and tuning the regularizer weight lambda to avoid degenerate representations. Computational cost of multiple fine-tuning runs for grid search is also a barrier.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Strong starting representation from ImageNet pretraining, availability of labeled source data and (optionally) a few labeled target examples, and an effective domain-discrepancy metric (MMD) to guide model selection. Shared-weight twin-branch architecture enabled joint training on both domains.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires pre-trained CNN weights (ImageNet), source labeled data for classifier training, access to target data (labels optional), GPU compute for fine-tuning, and hyperparameter search (adaptation width, lambda).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within related computer-vision transfer tasks where large pretraining datasets exist; the approach is specifically designed for visual CNN representations but the general idea (pretrain on large source, insert bottleneck, align distributions via a discrepancy measure) could be adapted to other modalities given analogous pretrained models and representations.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills (how to fine-tune, how to add adaptation layers and set learning rates) and explicit procedural steps (model-selection via MMD, joint loss formulation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Domain Confusion: Maximizing for Domain Invariance', 'publication_date_yy_mm': '2014-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e566.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e566.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMD-guided representation selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representation selection and adaptation-layer design guided by MMD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using MMD as a model-selection tool to choose which pre-trained layer's activations to use (depth) and to select the dimensionality (width) of a newly inserted adaptation bottleneck layer before fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>MMD-guided depth and width selection for adaptation layers</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>The authors extract activations from each candidate fully-connected layer of a pre-trained CNN, compute empirical MMD between source and target activations for those layers, and select the layer with lowest MMD as the insertion point for an adaptation layer (in experiments fc7 was selected). For width selection, they fine-tune multiple networks with adaptation layers of varying dimensionality (powers of two, from 64 to 4096), compute MMD on the resulting lower-dimensional representations, and choose the dimensionality that minimizes MMD (256 in their experiments). This procedure effectively treats representation selection as coordinate descent on the joint objective.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>model-selection procedure / computational method</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Model selection and kernel-based distribution-comparison methods from statistics/machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Deep CNN architecture design for visual domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Rather than using MMD solely as a post-hoc statistic, MMD was operationalized as an explicit criterion for architectural choices: layer placement (depth) and adaptation layer width. Practically this required extracting batch activations from pre-trained networks, computing batch MMD scores, and running a grid search of fine-tuning runs for widths to obtain MMD measures on learned representations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - MMD correctly ranked candidate layers in experiments (fc7 selected and yielded better adaptation performance than fc6), and width selection via MMD avoided extremes and produced good performance (256-dim chosen), though the authors note that the MMD-vs-accuracy relationship can be irregular and grid sampling coarse, so selected width was a reasonable but not necessarily globally optimal choice.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Noisy/irregular MMD-vs-performance curves (sampling/limited grid resolution), computational expense (multiple fine-tuning runs for width selection), and potential mismatch between the MMD-minimizing representation and the one that strictly maximizes test accuracy (selection criterion is proxy).</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Clear inverse correlation observed empirically between MMD and target test accuracy for their tasks, enabling MMD to serve as a practical proxy for domain invariance; availability of pre-trained networks and ability to extract activations easily.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Need to run multiple fine-tuning experiments to evaluate widths, compute MMD reliably over sufficient samples, and have compute resources for grid search; also requires access to source and target data (labels not required for MMD computation).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Likely generalizable as a practical heuristic for architecture decisions in other domain-adaptation settings with learned representations, but effectiveness depends on the strength of correlation between MMD and target performance for those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (model-selection protocol) and instrumental know-how (how to measure MMD on activations and perform grid searches).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Domain Confusion: Maximizing for Domain Invariance', 'publication_date_yy_mm': '2014-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Integrating structured biological data by kernel maximum mean discrepancy <em>(Rating: 2)</em></li>
                <li>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition <em>(Rating: 2)</em></li>
                <li>ImageNet classification with deep convolutional neural networks <em>(Rating: 2)</em></li>
                <li>Domain adaptive neural networks for object recognition <em>(Rating: 2)</em></li>
                <li>Adapting visual category models to new domains <em>(Rating: 2)</em></li>
                <li>DLID: Deep learning for domain adaptation by interpolating between domains <em>(Rating: 1)</em></li>
                <li>Geodesic flow kernel for unsupervised domain adaptation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-566",
    "paper_id": "paper-1c734a14c2325cb76783ca0431862c7f04a69268",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "MMD-regularization",
            "name_full": "Maximum Mean Discrepancy regularization (MMD) applied as domain confusion loss",
            "brief_description": "Use of the Maximum Mean Discrepancy metric as a differentiable domain-confusion loss in a deep CNN: MMD is computed on activations of an adaptation layer to penalize differences between source and target feature means, and is used both for model-selection (depth/width) and as a regularizer during fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Maximum Mean Discrepancy (MMD) as a domain confusion loss",
            "procedure_description": "MMD measures the distance between two distributions by comparing the means of their representations in feature space: empirically MMD(X_S,X_T) = ||(1/|X_S|) sum_{x_s in X_S} phi(x_s) - (1/|X_T|) sum_{x_t in X_T} phi(x_t)||. In this paper MMD is computed on CNN activations at a chosen adaptation (bottleneck) layer over minibatches of source and target examples. The squared MMD term is added to the classification loss (L = L_C + lambda * MMD^2) so gradients from MMD backpropagate through the network, encouraging the learned representation to make source and target distributions similar. The authors also compute MMD on fixed pre-trained layer activations to select which network layer to place the adaptation layer, and they compute MMD across candidate adaptation layer widths to choose dimensionality.",
            "procedure_type": "computational method / data-analysis regularizer",
            "source_domain": "Statistics / machine learning (kernel methods); historically used in other areas including bioinformatics (see ref. [6])",
            "target_domain": "Computer vision / deep learning (visual domain adaptation using CNNs)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Integrated empirical MMD into a supervised deep CNN training pipeline: (1) compute MMD over minibatches of CNN activations (phi is the adaptation-layer activation) rather than as a kernel-based post-hoc statistic; (2) add MMD^2 as a differentiable loss term with weight lambda (set to 0.25 in experiments); (3) use MMD both as a model-selection metric for choosing layer depth (which layer's activations minimize MMD) and adaptation-layer width (grid search over dimensionalities), and as a regularizer during backpropagation-based fine-tuning; (4) place the adaptation 'bottleneck' layer (chosen after fc7) and compute MMD at that layer; (5) adjust learning rates (new layers trained at 10x base rate) to accommodate the modified optimization.",
            "transfer_success": "successful - applying MMD as a domain-confusion regularizer and model-selection metric produced substantially improved domain-adaptation performance on the Office dataset: supervised adaptation results (multi-class accuracies) reported for the proposed method were 84.1% (A-&gt;W), 95.4% (D-&gt;W), 96.3% (W-&gt;D) with an average reported 91.9%, exceeding prior methods; unsupervised adaptation averaged 81.2%, also exceeding prior methods. The authors show qualitative evidence (t-SNE) of domain mixing and quantitative evidence that MMD-guided layer/dimension choices correlate with better target performance.",
            "barriers_encountered": "Potential pitfalls noted: choice of regularization weight lambda is critical (too small — no effect; too large — collapsed/degenerate representations); irregularities in MMD-vs-performance curves (sampling and coarse grid over widths can produce noisy model-selection signals); small target labeled datasets make tuning harder; computational cost of fine-tuning with additional loss and grid searches over widths.",
            "facilitating_factors": "Availability of a strong pre-trained CNN (providing meaningful phi), the fact that MMD is computed on learned representations (so it aligns naturally with CNN activations), ability to compute MMD cheaply on minibatches, and the differentiability of the empirical MMD term which allows backpropagation-based optimization.",
            "contextual_requirements": "Requires access to source and target data (labels only needed for classification branch), a differentiable implementation of MMD over minibatches, the ability to fine-tune a pre-trained CNN and to add an adaptation layer, and hyperparameter tuning (lambda, adaptation-layer width, placement). Computational resources to fine-tune deep networks are required.",
            "generalizability": "Moderately high within visual domain-adaptation tasks — authors demonstrate both supervised and unsupervised adaptation on multiple domain shifts; method is general in principle for other deep architectures and could be applied to other modalities where a learned representation phi exists, but success depends on having representations amenable to aligning means and on appropriate hyperparameter tuning.",
            "knowledge_type": "explicit procedural steps and theoretical principles (statistical domain discrepancy measure applied as a computational loss), with instrumental/technical know-how (how to integrate MMD into CNN training, minibatch computation, hyperparameter choices).",
            "uuid": "e566.0",
            "source_info": {
                "paper_title": "Deep Domain Confusion: Maximizing for Domain Invariance",
                "publication_date_yy_mm": "2014-12"
            }
        },
        {
            "name_short": "ImageNet pretraining transfer",
            "name_full": "Transfer of CNN representations pre-trained on ImageNet to small-target visual domains",
            "brief_description": "Using a deep CNN pre-trained on a large-scale dataset (ImageNet) as the base representation and adapting it for target visual domains (Office) by selecting activations (fc7) and fine-tuning with an added adaptation layer and MMD regularization.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Pre-trained deep CNN feature transfer and fine-tuning (ImageNet -&gt; Office)",
            "procedure_description": "A CNN (Krizhevsky et al. architecture) pre-trained on large-scale ImageNet classification is used as the starting representation. The authors evaluate activations from different fully-connected layers (fc6, fc7, etc.) using MMD to select the layer most domain-invariant (fc7). They then add a low-dimensional 'bottleneck' adaptation layer after the selected layer, choose its width via MMD-guided grid search, and fine-tune the network jointly on classification loss (source labels and available target labels in supervised case) and the MMD domain confusion loss. New layers are trained with increased learning rates while lower layers are updated slowly.",
            "procedure_type": "computational method / transfer learning / fine-tuning protocol",
            "source_domain": "Large-scale image classification (ImageNet; general computer vision)",
            "target_domain": "Small-sample visual domain adaptation tasks (Office dataset: Amazon, DSLR, Webcam)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Beyond direct feature extraction, the pre-trained model is modified by inserting a bottleneck adaptation layer (chosen to be after fc7 based on MMD), selecting its dimensionality by grid search guided by MMD, and fine-tuning the whole network with a joint loss (classification + lambda*MMD^2). Learning rates for newly initialized layers (adaptation layer and classifier) are set 10x higher. The training procedure is also run in both supervised (few labeled target examples) and unsupervised (no target labels) modes with MMD always computed on all data.",
            "transfer_success": "successful - produced state-of-the-art results on Office benchmarks: supervised average reported 91.9% multi-class accuracy across three transfer tasks and unsupervised average 81.2%, outperforming previously published methods (tables 1 and 2). The method especially excelled in mixing domain data within class clusters (t-SNE) and preventing overfitting compared to naive fine-tuning.",
            "barriers_encountered": "Challenges include small target data amounts making direct fine-tuning risky (overfitting), need to choose where to insert the adaptation layer and what width to use (necessitating MMD-guided model selection and grid searches), and tuning the regularizer weight lambda to avoid degenerate representations. Computational cost of multiple fine-tuning runs for grid search is also a barrier.",
            "facilitating_factors": "Strong starting representation from ImageNet pretraining, availability of labeled source data and (optionally) a few labeled target examples, and an effective domain-discrepancy metric (MMD) to guide model selection. Shared-weight twin-branch architecture enabled joint training on both domains.",
            "contextual_requirements": "Requires pre-trained CNN weights (ImageNet), source labeled data for classifier training, access to target data (labels optional), GPU compute for fine-tuning, and hyperparameter search (adaptation width, lambda).",
            "generalizability": "High within related computer-vision transfer tasks where large pretraining datasets exist; the approach is specifically designed for visual CNN representations but the general idea (pretrain on large source, insert bottleneck, align distributions via a discrepancy measure) could be adapted to other modalities given analogous pretrained models and representations.",
            "knowledge_type": "instrumental/technical skills (how to fine-tune, how to add adaptation layers and set learning rates) and explicit procedural steps (model-selection via MMD, joint loss formulation).",
            "uuid": "e566.1",
            "source_info": {
                "paper_title": "Deep Domain Confusion: Maximizing for Domain Invariance",
                "publication_date_yy_mm": "2014-12"
            }
        },
        {
            "name_short": "MMD-guided representation selection",
            "name_full": "Representation selection and adaptation-layer design guided by MMD",
            "brief_description": "Using MMD as a model-selection tool to choose which pre-trained layer's activations to use (depth) and to select the dimensionality (width) of a newly inserted adaptation bottleneck layer before fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "MMD-guided depth and width selection for adaptation layers",
            "procedure_description": "The authors extract activations from each candidate fully-connected layer of a pre-trained CNN, compute empirical MMD between source and target activations for those layers, and select the layer with lowest MMD as the insertion point for an adaptation layer (in experiments fc7 was selected). For width selection, they fine-tune multiple networks with adaptation layers of varying dimensionality (powers of two, from 64 to 4096), compute MMD on the resulting lower-dimensional representations, and choose the dimensionality that minimizes MMD (256 in their experiments). This procedure effectively treats representation selection as coordinate descent on the joint objective.",
            "procedure_type": "model-selection procedure / computational method",
            "source_domain": "Model selection and kernel-based distribution-comparison methods from statistics/machine learning",
            "target_domain": "Deep CNN architecture design for visual domain adaptation",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Rather than using MMD solely as a post-hoc statistic, MMD was operationalized as an explicit criterion for architectural choices: layer placement (depth) and adaptation layer width. Practically this required extracting batch activations from pre-trained networks, computing batch MMD scores, and running a grid search of fine-tuning runs for widths to obtain MMD measures on learned representations.",
            "transfer_success": "partially successful - MMD correctly ranked candidate layers in experiments (fc7 selected and yielded better adaptation performance than fc6), and width selection via MMD avoided extremes and produced good performance (256-dim chosen), though the authors note that the MMD-vs-accuracy relationship can be irregular and grid sampling coarse, so selected width was a reasonable but not necessarily globally optimal choice.",
            "barriers_encountered": "Noisy/irregular MMD-vs-performance curves (sampling/limited grid resolution), computational expense (multiple fine-tuning runs for width selection), and potential mismatch between the MMD-minimizing representation and the one that strictly maximizes test accuracy (selection criterion is proxy).",
            "facilitating_factors": "Clear inverse correlation observed empirically between MMD and target test accuracy for their tasks, enabling MMD to serve as a practical proxy for domain invariance; availability of pre-trained networks and ability to extract activations easily.",
            "contextual_requirements": "Need to run multiple fine-tuning experiments to evaluate widths, compute MMD reliably over sufficient samples, and have compute resources for grid search; also requires access to source and target data (labels not required for MMD computation).",
            "generalizability": "Likely generalizable as a practical heuristic for architecture decisions in other domain-adaptation settings with learned representations, but effectiveness depends on the strength of correlation between MMD and target performance for those tasks.",
            "knowledge_type": "explicit procedural steps (model-selection protocol) and instrumental know-how (how to measure MMD on activations and perform grid searches).",
            "uuid": "e566.2",
            "source_info": {
                "paper_title": "Deep Domain Confusion: Maximizing for Domain Invariance",
                "publication_date_yy_mm": "2014-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Integrating structured biological data by kernel maximum mean discrepancy",
            "rating": 2
        },
        {
            "paper_title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition",
            "rating": 2
        },
        {
            "paper_title": "ImageNet classification with deep convolutional neural networks",
            "rating": 2
        },
        {
            "paper_title": "Domain adaptive neural networks for object recognition",
            "rating": 2
        },
        {
            "paper_title": "Adapting visual category models to new domains",
            "rating": 2
        },
        {
            "paper_title": "DLID: Deep learning for domain adaptation by interpolating between domains",
            "rating": 1
        },
        {
            "paper_title": "Geodesic flow kernel for unsupervised domain adaptation",
            "rating": 1
        }
    ],
    "cost": 0.0119945,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep Domain Confusion: Maximizing for Domain Invariance</h1>
<p>Eric Tzeng, Judy Hoffman, Ning Zhang<br>UC Berkeley, EECS \&amp; ICSI<br>{etzeng, jhoffman,nzhang}@eecs.berkeley.edu</p>
<p>Kate Saenko<br>UMass Lowell, CS<br>saenko@cs.uml.edu</p>
<p>Trevor Darrell<br>UC Berkeley, EECS \&amp; ICSI<br>trevor@eecs.berkeley.edu</p>
<h4>Abstract</h4>
<p>Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task.</p>
<h2>1. Introduction</h2>
<p>Dataset bias is a well known problem with traditional supervised approaches to image recognition [32]. A number of recent theoretical and empirical results have shown that supervised methods' test error increases in proportion to the difference between the test and training input distribution [3, 5, 29, 32]. In the last few years several methods for visual domain adaptation have been suggested to overcome this issue $[10,33,2,29,25,22,17,16,19,20]$, but were limited to shallow models. The traditional approach to adapting deep models has been fine-tuning; see [15] for a recent example.</p>
<p>Directly fine-tuning a deep network's parameters on a small amount of labeled target data turns out to be problematic. Fortunately, pre-trained deep models do perform well in novel domains. Recently, [11, 21] showed that using the deep mid-level features learned on ImageNet, instead of the more conventional bag-of-words features, effectively removed the bias in some of the domain adaptation settings in the Office dataset [29]. These algorithms transferred the representation from a large scale domain, ImageNet, as well
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our architecture optimizes a deep CNN for both classification loss as well as domain invariance. The model can be trained for supervised adaptation, when there is a small amount of target labels available, or unsupervised adaptation, when no target labels are available. We introduce domain invariance through domain confusion guided selection of the depth and width of the adaptation layer, as well as an additional domain loss term during fine-tuning that directly minimizes the distance between source and target representations.
as using all of the data in that domain as source data for appropriate categories. However, these methods have no way to select a representation from the deep architecture and instead report results across multiple layer selection choices.</p>
<p>Dataset bias was classically illustrated in computer vision by way of the "name the dataset" game of Torralba and Efros [32]. Indeed, this turns out to be formally connected to measures of domain discrepancy [23, 6]. Optimizing for domain invariance, therefore, can be considered equivalent to the task of learning to predict the class labels while simultaneously finding a representation that makes the do-</p>
<p>mains appear as similar as possible. This principle forms the essence of our proposed approach. We learn deep representations by optimizing over a loss which includes both classification error on the labeled data as well as a domain confusion loss which seeks to make the domains indistinguishable.</p>
<p>We propose a new CNN architecture, outlined in Figure 1, which uses an adaptation layer along with a domain confusion loss based on maximum mean discrepancy (MMD) <em>[6]</em> to automatically learn a representation jointly trained to optimize for classification and domain invariance. We show that our domain confusion metric can be used both to select the dimension of the adaptation layers, choose an effective placement for a new adaptation layer within a pre-trained CNN architecture, and fine-tune the representation.</p>
<p>Our architecture can be used to solve both supervised adaptation, when a small amount of target labeled data is available, and unsupervised adaptation, when no labeled target training data is available. We provide a comprehensive evaluation on the popular Office benchmark for classification across visually distinct domains <em>[29]</em>. We demonstrate that by jointly optimizing for domain confusion and classification, we are able to significantly outperform the current state-of-the-art visual domain adaptation results. In fact, for the case of minor pose, resolution, and lighting changes, our algorithm is able to achieve 96% accuracy on the target domain, demonstrating that we have in fact learned a representation that is invariant to these biases.</p>
<h2>2 Related work</h2>
<p>The concept of visual dataset bias was popularized in <em>[32]</em>. There have been many approaches proposed in recent years to solve the visual domain adaptation problem. All recognize that there is a shift in the distribution of the source and target data representations. In fact, the size of a domain shift is often measured by the distance between the source and target subspace representations <em>[6, 13, 23, 26, 28]</em>. A large number of methods have sought to overcome this difference by learning a feature space transformation to align the source and target representations <em>[29, 25, 13, 16]</em>. For the supervised adaptation scenario, when a limited amount of labeled data is available in the target domain, some approaches have been proposed to learn a target classifier regularized against the source classifier <em>[33, 2, 1]</em>. Others have sought to both learn a feature transformation and regularize a target classifier simultaneously <em>[20, 12]</em>.</p>
<p>Recently, supervised convolutional neural network (CNN) based feature representations have been shown to be extremely effective for a variety of visual recognition tasks <em>[24, 11, 15, 30]</em>. In particular, using deep representations dramatically reduce the effect of resolution and lighting on domain shifts <em>[11, 21]</em>.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: For biased datasets (left), classifiers learned in a source domain do not necessarily transfer well to target domains. By optimizing an objective that simultaneously minimizes classification error and maximizes domain confusion (right), we can learn representations that are discriminative and domain invariant.</p>
<p>Parallel CNN architectures such as Siamese networks have been shown to be effective for learning invariant representations <em>[7, 9]</em>. However, training these networks requires labels for each training instance, so it is unclear how to extend these methods to unsupervised settings.</p>
<p>Multimodal deep learning architectures have also been explored to learn representations that are invariant to different input modalities <em>[27]</em>. However, this method operated primarily in a generative context and therefore did not leverage the full representational power of supervised CNN representations.</p>
<p>Training a joint source and target CNN architecture was proposed by <em>[8]</em>, but was limited to two layers and so was significantly outperformed by the methods which used a deeper architecture <em>[24]</em>, pre-trained on a large auxiliary data source (ex: ImageNet <em>[4]</em>).</p>
<p><em>[14]</em> proposed pre-training with a denoising auto encoder, then training a two-layer network simultaneously with the MMD domain confusion loss. This effectively learns a domain invariant representation, but again, because the learned network is relatively shallow, it lacks the strong semantic representation that is learned by directly optimizing a classification objective with a supervised deep CNN.</p>
<h2>3 Training CNN-based domain invariant representations</h2>
<p>We introduce a new convolutional neural network (CNN) architecture which we use to learn a visual representation that is both domain invariant and which offers strong semantic separation. It has been shown that a pre-trained CNN can be adapted for a new task through fine-tuning *[15,</p>
<p>30, 18]. However, in the domain adaptation scenario there is little, or no, labeled training data in the target domain so we can not directly fine-tune for the categories of interest, $C$ in the target domain, $T$. Instead, we will use data from a related, but distinct source domain, $S$, where more labeled data is available from the corresponding categories, $C$.</p>
<p>Directly training a classifier using only the source data often leads to overfitting to the source distribution, causing reduced performance at test time when recognizing in the target domain. Our intuition is that if we can learn a representation that minimizes the distance between the source and target distributions, then we can train a classifier on the source labeled data and directly apply it to the target domain with minimal loss in accuracy.</p>
<p>To minimize this distance, we consider the standard distribution distance metric, Maximum Mean Discrepancy (MMD) [6]. This distance is computed with respect to a particular representation, $\phi(\cdot)$. In our case, we define a representation, $\phi(\cdot)$, which operates on source data points, $x_{s} \in X_{S}$, and target data points, $x_{t} \in X_{T}$. Then an empirical approximation to this distance is computed as followed:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{MMD}\left(X_{S}, X_{T}\right)= \
&amp; \quad\left|\frac{1}{\left|X_{S}\right|} \sum_{x_{s} \in X_{S}} \phi\left(x_{s}\right)-\frac{1}{\left|X_{T}\right|} \sum_{x_{t} \in X_{T}} \phi\left(x_{t}\right)\right|
\end{aligned}
$$</p>
<p>As Figure 2 shows, not only do we want to minimize the distance between domains (or maximize the domain confusion), but we want a representation which is conducive to training strong classifiers. Such a representation would enable us to learn strong classifiers that readily transfer across domains. One approach to meeting both these criteria is to minimize the loss:</p>
<p>$$
\mathcal{L}=\mathcal{L}<em L="L">{C}\left(X</em>\right)
$$}, y\right)+\lambda \operatorname{MMD}^{2}\left(X_{S}, X_{T</p>
<p>where $\mathcal{L}<em L="L">{C}\left(X</em>$. The hyperparameter $\lambda$ determines how strongly we would like to confuse the domains.}, y\right)$ denotes classification loss on the available labeled data, $X_{L}$, and the ground truth labels, $y$, and $\operatorname{MMD}\left(X_{S}, X_{T}\right)$ denotes the distance between the source data, $X_{S}$, and the target data, $X_{T</p>
<p>One approach to minimizing this loss is to take a fixed CNN, which is already a strong classification representation, and use MMD to decide which layer to use activations from to minimize the domain distribution distance. We can then use this representation to train another classifier for the classes we are interested in recognizing. This can be viewed as coordinate descent on Eqn. 2: we take a network that was trained to minimize $\mathcal{L}<em C="C">{C}$, select the representation that minimizes MMD, then use that representation to again minimize $\mathcal{L}</em>$.</p>
<p>However, this approach is limited in that it cannot directly adapt the representation-instead, it is constrained to
selecting from a set of fixed representations. Thus, we propose creating a network to directly optimize the classification and domain confusion objectives, outlined in Figure 1.</p>
<p>We begin with the Krizhevsky architecture [24], which has five convolutional and pooling layers and three fully connected layers with dimensions ${4096,4096,|C|}$. We additionally add a lower dimensional, "bottleneck," adaptation layer. Our intuition is that a lower dimensional layer can be used to regularize the training of the source classifier and prevent overfitting to the particular nuances of the source distribution. We place the domain distance loss on top of the "bottleneck" layer to directly regularize the representation to be invariant to the source and target domains.</p>
<p>There are two model selection choices that must be made to add our adaptation layer and the domain distance loss. We must choose where in the network to place the adaptation layer and we must choose the dimension of the layer. We use the MMD metric to make both of these decisions. First, as previously discussed, for our initial fixed representation we find the layer which minimizes the empirical MMD distance between all available source and target data, in our experiments this corresponded to placing the layer after the fully connected layer, $f c 7$.</p>
<p>Next, we must determine the dimension for our adaptation layer. We solve this problem with a grid search, where we fine-tune multiple networks using various dimensions and compute the MMD in the new lower dimension representation, finally choosing the dimension which minimizes the source and target distance.</p>
<p>Both the selection of which layer's representation to use ("depth") and how large the adaptation layer should be ("width") are guided by MMD, and thus can be seen as descent steps on our overall objective.</p>
<p>Our architecture (see Figure 1) consists of a source and target CNN, with shared weights. Only the labeled examples are used to compute the classification loss, while all data is used from both domains to compute the domain confusion loss. The network is jointly trained on all available source and target data.</p>
<p>The objective outlined in Eqn. 2 is easily represented by this convolutional neural network where MMD is computed over minibatches of source and target data. We simply use a fork at the top of the network, after the adaptation layer. One branch uses the labeled data and trains a classifier, and the other branch uses all the data and computes MMD between source and target.</p>
<p>After fine-tuning this architecture, owing to the two terms in the joint loss, the adaptation layer learns a representation that can effectively discriminate between the classes in question due to the classification loss term, while still remaining invariant to domain shift due the MMD term. We expect that such a representation will thus enable increased adaptation performance.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />Figure 3: Maximum mean discrepancy and test accuracy for different choices of representation layer. We observe that MMD between source and target and accuracy on the target domain test set seem inversely related, indicating that MMD can be used to help select a layer for adaptation.</p>
<h2>4 Evaluation</h2>
<p>We evaluate our adaptation algorithm on a standard domain adaptation dataset with small-scale source domains. We show that our algorithm is effectively able to adapt a deep CNN representation to a target domain with limited or no target labeled data.</p>
<p>The Office <em>[29]</em> dataset is a collection of images from three distinct domains: Amazon, DSLR, and Webcam. The 31 categories in the dataset consist of objects commonly encountered in office settings, such as keyboards, file cabinets, and laptops. The largest domain has 2817 labeled images.</p>
<p>We evaluate our method across 5 random train/test splits for each of the 3 transfer tasks commonly used for evaluation (Amazon$\rightarrow$Webcam, DSLR$\rightarrow$Webcam, and Webcam$\rightarrow$DSLR) and report averages and standard errors for each setting. We compare in both supervised and unsupervised scenarios against the numbers reported by six recently published methods.</p>
<p>We follow the standard training protocol for this dataset of using 20 source examples per category for the Amazon source domain and 8 images per category for Webcam or DSLR as the source domains <em>[29, 16]</em>. For the supervised adaptation setting we assume 3 labeled target examples per category.</p>
<h3>4.1 Evaluating adaptation layer placement</h3>
<p>We begin with an evaluation of our representation selection strategy. Using a pre-trained convolutional neural network, we extract features from source and target data using the representations at each fully connected layer. We can</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />Figure 4: Maximum mean discrepancy and test accuracy for different values of adaptation layer dimensionality. We observe that MMD between source and target and accuracy on the target domain test set seem inversely related, indicating that MMD can be used to help select a dimensionality to use.</p>
<p>then compute the MMD between source and target at each layer. Since a lower MMD indicates that the representation is more domain invariant, we expect the representation with the lowest MMD to achieve the highest performance after adaptation.</p>
<p>To test this hypothesis, for one of the Amazon$\rightarrow$Webcam splits we apply a simple domain adaptation baseline introduced by Daumé III <em>[10]</em> to compute test accuracy for the target domain. Figure 3 shows a comparison of MMD and adaptation performance across different choices of bridge layers. We see that MMD correctly ranks the representations, singling out $fc7$ as the best performing layer and $fc6$ as the worst. Therefore, we add our adaptation layer after $fc7$ for the remaining experiments.</p>
<h3>4.2 Choosing the adaptation layer dimension</h3>
<p>Before we can learn a new representation via our proposed fine-tuning method, we must determine how wide this representation should be. Again, we use MMD as the deciding metric.</p>
<p>In order to determine what dimensionality our learned adaptation layer should have, we train a variety of networks with different widths on the Amazon$\rightarrow$Webcam task, as this is the most challenging of the three. In particular, we try different widths varying from 64 to 4096, stepping by a power of two each time. Once the networks are trained, we then compute MMD between source and target for each of the learned representations. Our method then selects the dimensionality that minimizes the MMD between the source and target data.</p>
<table>
<thead>
<tr>
<th></th>
<th>$A \rightarrow W$</th>
<th>$D \rightarrow W$</th>
<th>$W \rightarrow D$</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>GFK(PLS,PCA) [16]</td>
<td>$46.4 \pm 0.5$</td>
<td>$61.3 \pm 0.4$</td>
<td>$66.3 \pm 0.4$</td>
<td>53.0</td>
</tr>
<tr>
<td>SA [13]</td>
<td>45.0</td>
<td>64.8</td>
<td>69.9</td>
<td>59.9</td>
</tr>
<tr>
<td>DA-NBNN [31]</td>
<td>$52.8 \pm 3.7$</td>
<td>$76.6 \pm 1.7$</td>
<td>$76.2 \pm 2.5$</td>
<td>68.5</td>
</tr>
<tr>
<td>DLID [8]</td>
<td>51.9</td>
<td>78.2</td>
<td>89.9</td>
<td>73.3</td>
</tr>
<tr>
<td>DeCAF S+T [11]</td>
<td>$80.7 \pm 2.3$</td>
<td>$94.8 \pm 1.2$</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>DaNN [14]</td>
<td>$53.6 \pm 0.2$</td>
<td>$71.2 \pm 0.0$</td>
<td>$83.5 \pm 0.0$</td>
<td>69.4</td>
</tr>
<tr>
<td>Ours</td>
<td>$\mathbf{8 4 . 1} \pm \mathbf{0 . 6}$</td>
<td>$\mathbf{9 5 . 4} \pm \mathbf{0 . 4}$</td>
<td>$\mathbf{9 6 . 3} \pm \mathbf{0 . 3}$</td>
<td>$\mathbf{9 1 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Multi-class accuracy evaluation on the standard supervised adaptation setting with the Office dataset. We evaluate on all 31 categories using the standard experimental protocol from [29]. Here, we compare against six state-of-the-art domain adaptation methods.</p>
<table>
<thead>
<tr>
<th></th>
<th>$A \rightarrow W$</th>
<th>$D \rightarrow W$</th>
<th>$W \rightarrow D$</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>GFK(PLS,PCA) [16]</td>
<td>$15.0 \pm 0.4$</td>
<td>$44.6 \pm 0.3$</td>
<td>$49.7 \pm 0.5$</td>
<td>36.4</td>
</tr>
<tr>
<td>SA [13]</td>
<td>15.3</td>
<td>50.1</td>
<td>56.9</td>
<td>40.8</td>
</tr>
<tr>
<td>DA-NBNN [31]</td>
<td>$23.3 \pm 2.7$</td>
<td>$67.2 \pm 1.9$</td>
<td>$67.4 \pm 3.0$</td>
<td>52.6</td>
</tr>
<tr>
<td>DLID [8]</td>
<td>26.1</td>
<td>68.9</td>
<td>84.9</td>
<td>60.0</td>
</tr>
<tr>
<td>DeCAF S [11]</td>
<td>$52.2 \pm 1.7$</td>
<td>$91.5 \pm 1.5$</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>DaNN [14]</td>
<td>$35.0 \pm 0.2$</td>
<td>$70.5 \pm 0.0$</td>
<td>$74.3 \pm 0.0$</td>
<td>59.9</td>
</tr>
<tr>
<td>Ours</td>
<td>$\mathbf{5 9 . 4} \pm \mathbf{0 . 8}$</td>
<td>$\mathbf{9 2 . 5} \pm \mathbf{0 . 3}$</td>
<td>$\mathbf{9 1 . 7} \pm \mathbf{0 . 8}$</td>
<td>$\mathbf{8 1 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Multi-class accuracy evaluation on the standard unsupervised adaptation setting with the Office dataset. We evaluate on all 31 categories using the standard experimental protocol from [16]. Here, we compare against six state-of-the-art domain adaptation methods.</p>
<p>To verify that MMD makes the right choice, again we compare MMD with performance on a test set. Figure 4 shows that we select 256 dimensions for the adaptation layer, and although this setting is not the one that maximizes test performance, it appears to be a reasonable choice. In particular, using MMD avoids choosing either extreme, near which performance suffers. It is worth noting that the plot has quite a few irregularities-perhaps finer sampling would allow for a more accurate choice.</p>
<h3>4.3. Fine-tuning with domain confusion regularization</h3>
<p>Once we have settled on our choice of adaptation layer dimensionality, we can begin fine-tuning using the joint loss described in Section 3. However, we need to set the regularization hyperparameter $\lambda$. Setting $\lambda$ too low will cause the MMD regularizer have no effect on the learned representation, but setting $\lambda$ too high will regularize too heavily and learn a degenerate representation in which all points are too close together. We set the regularization hyperparameter to $\lambda=0.25$, which makes the objective primarily weighted towards classification, but with enough regularization to avoid overfitting.</p>
<p>We use the same fine-tuning architecture for both unsu-
pervised and supervised. However, in the supervised setting, the classifier is trained on data from both domains, whereas in the unsupervised setting, due to the lack of labeled training data, the classifier sees only source data. In both settings, the MMD regularizer sees all of the data, since it does not require labels.</p>
<p>Finally, because the adaptation layer and classifier are being trained from scratch, we set their learning rates to be 10 times higher than the lower layers of the network that were copied from the pre-trained model. Fine-tuning then proceeds via standard backpropagation optimization.</p>
<p>The supervised adaptation setting results are shown in Table 1 and the unsupervised adaptation results are shown in Table 2. We notice that our algorithm dramatically outperforms all of the competing methods. The distinct improvement of our method demonstrates that the adaptation layer learned via MMD regularized fine-tuning is able to succesfully transfer to a new target domain.</p>
<p>In order to determine how MMD regularization affects learning, we also compare the learning curves with and without regularization on the Amazon $\rightarrow$ Webcam transfer task in Figure 5. We see that, although the unregularized version is initially faster to train, it quickly begins overfitting, and test accuracy suffers. In contrast, using MMD</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A plot of the test accuracy on an unsupervised Amazon→Webcam split during the first 700 iterations of fine-tuning for both regularized and unregularized methods. Although initially the unregularized training achieves better performance, it overfits to the source data. In contrast, using regularization prevents overfitting, so although initial learning is slower we ultimately see better final performance.</p>
<p>Regularization prevents the network from overfitting to the source data, and although training takes longer, the regularization results in a higher final test accuracy.</p>
<p>To further demonstrate the domain invariance of our learned representation, we plot in Figure 6 a t-SNE embedding of Amazon and Webcam images using our learned representation and compare it to an embedding created with <em>fc7</em> in the pretrained model. Examining the embeddings, we see that our learned representation exhibits tighter class clustering while mixing the domains within each cluster. While there is weak clustering in the <em>fc7</em> embedding, we find that most tight clusters consist of data points from one domain or the other, but rarely both.</p>
<h3>4.4. Historical Progress on the Office Dataset</h3>
<p>In Figure 7 we report historical progress on the standard Office dataset since its introduction. We indicate methods which use traditional features (ex: SURF BoW) with a blue circle and methods which use deep representations with a red square. We show two adaptation scenarios. The first scenario is a supervised adaptation task for visually distant domains (Amazon→Webcam). For this task our algorithm outperforms DeCAF by 3.4% multiclass accuracy. Finally, we show the hardest task of unsupervised adaptation for that same shift. Here we show that our method provides the most significant improvement of 5.5% multiclass accuracy.</p>
<h2>5. Conclusion</h2>
<p>In this paper, we presented an objective function for learning domain invariant representations for classification. This objective makes use of an additional domain confusion term to ensure that domains are indistinguishable in the learned representation. We then presented a variety of ways to optimize this objective, ranging from simple representation selection from a fixed pool to a full convolutional architecture that directly minimizes the objective via backpropagation.</p>
<p>Our full method, which uses MMD both to select the depth and width of the architecture while using it as a regularizer during fine-tuning, achieves state-of-the-art performance on the standard visual domain adaptation benchmark, beating previous methods by a considerable margin.</p>
<p>These experiments show that incorporating a domain confusion term into the discriminative representation learning process is an effective way to ensure that the learned representation is both useful for classification and invariant to domain shifts.</p>
<p>This work was supported in part by DARPA's MSEE and SMISC programs, NSF awards IIS-1427425, IIS-1212798, and IIS-1116411, Toyota, and the Berkeley Vision and Learning Center.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: t-SNE embeddings of Amazon (blue) and Webcam (green) images using our supervised 256-dimensional representation learned with MMD regularization (top left) and the original $f c 7$ representation from the pre-trained model (bottom right). Observe that the clusters formed by our representation separate classes while mixing domains much more effectively than the original representation that was not trained for domain invariance. For example, in $f c 7$-space the Amazon monitors and Webcam monitors are separated into distinct clusters, whereas with our learned representation all monitors irrespective of domain are mixed into the same cluster.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Rapid progress over the last few years on a standard visual domain adaptation dataset, Office [29]. We show methods on Amazon $\rightarrow$ Webcam that use traditional hand designed visual representations with blue circles and methods that use deep representations are depicted with red squares. For the supervised task, our method achieves $84 \%$ multiclass accuracy, an increase of $3 \%$. For the unsupervised task, our method achieves $60 \%$ multiclass accuracy, an increase of $6 \%$.</p>
<h2>References</h2>
<p>[1] L. T. Alessandro Bergamo. Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach. In Neural Information Processing Systems (NIPS), Dec. 2010. 2
[2] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for object category detection. In Proc. ICCV, 2011. 1, 2
[3] S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al. Analysis of representations for domain adaptation. Proc. NIPS, 2007. 1
[4] A. Berg, J. Deng, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge 2012. 2012. 2
[5] J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman. Learning bounds for domain adaptation. In Proc. NIPS, 2007. 1
[6] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Schölkopf, and A. J. Smola. Integrating structured biological data by kernel maximum mean discrepancy. In Bioinformatics, 2006. 1, 2, 3
[7] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. Säckinger, and R. Shah. Signature verification using a siamese time delay neural network. International Journal of Pattern Recognition and Artificial Intelligence, 7(04):669-688, 1993. 2
[8] S. Chopra, S. Balakrishnan, and R. Gopalan. DLID: Deep learning for domain adaptation by interpolating between domains. In ICML Workshop on Challenges in Representation Learning, 2013. 2, 5
[9] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face verification. In Computer Vision and Pattern Recognition, 2005.</p>
<p>CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 539-546. IEEE, 2005. 2
[10] H. Daumé III. Frustratingly easy domain adaptation. In ACL, 2007. 1, 4
[11] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. In Proc. ICML, 2014. 1, 2, 5
[12] L. Duan, D. Xu, and I. W. Tsang. Learning with augmented features for heterogeneous domain adaptation. In Proc. ICML, 2012. 2
[13] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Unsupervised visual domain adaptation using subspace alignment. In Proc. ICCV, 2013. 2, 5
[14] M. Ghifary, W. B. Kleijn, and M. Zhang. Domain adaptive neural networks for object recognition. CoRR, abs/1409.6041, 2014. 2, 5
[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. arXiv e-prints, 2013. 1, 2
[16] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In Proc. CVPR, 2012. 1, 2, 4, 5
[17] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsupervised approach. In Proc. ICCV, 2011. 1
[18] J. Hoffman, S. Guadarrama, E. Tzeng, R. Hu, J. Donahue, R. Girshick, T. Darrell, and K. Saenko. LSDA: Large scale detection through adaptation. In Neural Information Processing Systems (NIPS), 2014. 2
[19] J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering latent domains for multisource domain adaptation. In Proc. ECCV, 2012. 1</p>
<p>[20] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Darrell. Efficient learning of domain-invariant image representations. In Proc. ICLR, 2013. 1, 2
[21] J. Hoffman, E. Tzeng, J. Donahue, , Y. Jia, K. Saenko, and T. Darrell. One-shot learning of supervised deep convolutional models. In arXiv 1312.6204; presented at ICLR Workshop, 2014. 1, 2
[22] A. Khosla, T. Zhou, T. Malisiewicz, A. Efros, and A. Torralba. Undoing the damage of dataset bias. In Proc. ECCV, 2012. 1
[23] D. Kifer, S. Ben-David, and J. Gehrke. Detecting change in data streams. In Proc. VLDB, 2004. 1, 2
[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In Proc. NIPS, 2012. 2, 3
[25] B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In Proc. CVPR, 2011. 1, 2
[26] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2
[27] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 689-696, 2011. 2
[28] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. In IJCA, 2009. 2
[29] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In Proc. ECCV, 2010. $1,2,4,5,8$
[30] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229, 2013. 2
[31] T. Tommasi and B. Caputo. Frustratingly easy NBNN domain adaptation. In Proc. ICCV, 2013. 5
[32] A. Torralba and A. Efros. Unbiased look at dataset bias. In Proc. CVPR, 2011. 1, 2
[33] J. Yang, R. Yan, and A. Hauptmann. Adapting SVM classifiers to data with shifted distributions. In ICDM Workshops, 2007. 1, 2</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1157 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1157</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1157</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-34549</p>
                <p><strong>Paper Title:</strong> Approximate Bayes Optimal Policy Search using Neural Networks</p>
                <p><strong>Paper Abstract:</strong> Bayesian Reinforcement Learning (BRL) agents aim to maximise the expected collected rewards obtained when interacting with an unknown Markov Decision Process (MDP) while using some prior knowledge. State-of-the-art BRL agents rely on frequent updates of the belief on the MDP, as new observations of the environment are made. This offers theoretical guarantees to converge to an optimum, but is computationally intractable, even on small-scale problems. In this paper, we present a method that circumvents this issue by training a parametric policy able to recommend an action directly from raw observations. Artificial Neural Networks (ANNs) are used to represent this policy, and are trained on the trajectories sampled from the prior. The trained model is then used online, and is able to act on the real MDP at a very low computational cost. Our new algorithm shows strong empirical performance, on a wide range of test problems, and is robust to inaccuracies of the prior distribution.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1157.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1157.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ANN-BRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Artificial Neural Networks for Bayesian Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline, prior-based policy-search algorithm that trains an ensemble of neural-network classifiers (MLPs combined with SAMME boosting) on simulated trajectories from a prior over MDPs to produce a fast online policy that maps histories (reprocessed into fixed-size features) to actions without performing Bayes updates online.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ANN-BRL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy represented as an ensemble of multilayer perceptrons (3-layer MLPs) trained with SAMME (multi-class boosting). Inputs are fixed-size feature vectors derived from histories (either Q-value features or transition-count features plus current state). Training dataset is generated offline by simulating MDPs sampled from the prior and labelling each history with the optimal action for the fully-known sampled MDP. Online the trained classifier predicts action confidences and selects the highest-scoring action.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Offline adaptive dataset generation using stochastic exploration (ε-Optimal agents) and boosting-based emphasis on hard samples; no online Bayesian updates</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>During offline data generation the authors vary ε (the ε-Optimal agent parameter) between simulations to induce exploration diversity across sampled MDPs, thereby adaptively covering belief-state space; SAMME reweights training samples to focus subsequent weak learners on samples misclassified by earlier learners (adaptive emphasis in model training). At test time the policy is fixed (no posterior updates).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Benchmarks drawn from Flat Dirichlet Multinomial (FDM) priors — Generalised Chain (GC), Generalised Double-Loop (GDL), Grid</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition dynamics (MDP transitions drawn from an FDM prior), discrete finite state and action spaces, stochastic transitions, bounded rewards; uncertainty is over model dynamics (not partial state observability), episodes truncated at horizon T for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>GC: 5 states × 3 actions; GDL: 9 states × 2 actions; Grid: 25 states × 4 actions; trajectories of length T (chosen to bound approximation error); prior/test distributions varied (accurate vs inaccurate prior experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported state-of-the-art empirical performance across the benchmark suite; ANN-BRL using Q-value features (ANN-BRL (Q)) obtains top results, notably outperforming all baselines on the most challenging Grid benchmark (in the inaccurate-prior case it reports a score ~4× larger than OPPS-DS on that benchmark). ANN-BRL also yields low online computation time compared to methods that do online Bayesian planning. (Paper reports mean cumulative-return scores and 95% CIs per benchmark but tabulated values are given in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not directly reported; baseline comparisons include Random, ε-Greedy, Softmax and multiple BRL planners. The ANN-BRL policy trained with only transition-counter features (ANN-BRL (C)) performs worse than ANN-BRL (Q), indicating reduced performance when informative features or adaptive data-generation choices are not used.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>ANN-BRL requires substantial offline simulation (generate trajectories from many sampled MDPs) but is sample-efficient online: after offline training the forward pass is cheap and requires no further environment model inference. Specific offline sample counts: the paper uses n sampled MDPs and T-length trajectories (paper reports default n and training settings), but exact sample-to-performance curves are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled offline via the ε-Optimal stochastic policy used during dataset generation (each simulation uses an ε that controls probability of random vs optimal action when generating trajectories) to expose the learner to both exploitative and exploratory transitions; no explicit online exploration-exploitation mechanism (no Bayes updates online).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared experimentally to Random, ε-Greedy, Softmax, OPPS-DS, BAMCP, BFS3, SBOSS, BEB across three benchmark distributions and both accurate and inaccurate prior cases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Offline-trained ANNs (ANN-BRL) can approximate Bayes-optimal behaviour across a prior distribution and achieve state-of-the-art cumulative returns while being orders of magnitude faster online because they avoid expensive posterior updates; 2) Using Q-value based features yields substantially better performance than raw transition-count features; 3) ANN-BRL is robust to prior inaccuracies and can outperform classical BRL planners on difficult benchmarks (notably Grid) while respecting tight online time budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) Requires significant offline simulation and the quality depends on how well the prior covers the test distribution; 2) Labels used during training are optimal actions under full observability of the sampled MDP rather than true Bayes-optimal actions, which is an approximation the authors note could be improved; 3) ANN-BRL (C) with only transition counters sometimes underperforms other algorithms, indicating sensitivity to feature choice; 4) No online posterior update means the agent cannot adapt further to a specific encountered MDP beyond what the offline-trained policy encodes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1157.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1157.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BAMCP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-adaptive Monte Carlo Planning (BAMCP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Monte-Carlo tree-search method (UCT variant) that plans in the belief-augmented MDP by sampling transitions from posterior distributions; provides (asymptotic) Bayes-optimality guarantees but requires substantial online computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient Bayes-adaptive Reinforcement Learning using sample-based search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BAMCP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based online planner that builds a lookahead search tree in the belief-augmented state space using Monte-Carlo rollouts; samples transition models according to the current posterior and uses a UCT-style bandit selection within the planning tree. Controlled by parameters K (nodes per timestep) and depth.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Belief-lookahead via posterior sampling and Monte-Carlo tree search (online active planning / adaptive experiment selection via lookahead)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each decision step the agent samples possible MDP models from the current posterior (based on observed history) and expands a planning tree; action selection adapts to current posterior uncertainty by preferentially exploring branches of the tree that look promising under sampled models (UCT selection balances exploration vs exploitation in the tree).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmark suite (MDPs drawn from FDM priors: GC, GDL, Grid)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic MDP dynamics (transition probabilities drawn from Dirichlet priors), finite discrete state/action spaces, bounded rewards; uncertainty over models (belief state).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Evaluated on GC (5×3), GDL (9×2), Grid (25×4). Online planning depth and branching control computational cost (tested K and depth parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Included in experimental comparisons; performs well but is sensitive to the online computational budget—BL approaches can achieve high accuracy but may be inapplicable under tight online time constraints. The paper reports BAMCP scores among baselines (see tables) but does not claim BAMCP outperforms ANN-BRL under the evaluated time constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not applicable (BAMCP inherently adapts online).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Relies on online sampling/simulations during planning each decision step; sample-efficient in the sense of targeted lookahead but computationally heavy per timestep; efficiency depends on K and depth parameters and available online time budget.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled via UCT-style selection within the search tree (implicit exploration bonus based on visitation counts) and by sampling models from the posterior so actions are selected to trade off immediate reward vs information value estimated by lookahead.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to BFS3, SBOSS, BEB, OPPS, ANN-BRL and simpler baselines in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BAMCP provides principled online adaptive planning with theoretical Bayes-adaptive guarantees but is practically constrained by online computation time; under tight time budgets other approaches (e.g., ANN-BRL) that invest offline can outperform it in wall-clock-constrained scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High online computational cost; performance degrades / is infeasible when the per-decision time budget is small; the paper emphasizes this as a practical limitation compared to offline-trained policies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1157.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1157.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BFS3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Forward Search Sparse Sampling (BFS3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online belief-augmented planner applying sparse forward search (FSSS) to belief-MDPs, using sampled models and value bounds to prune the search space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Approaching Bayesoptimalilty using Monte-Carlo tree search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BFS3</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Applies Forward Search Sparse Sampling to the belief-augmented MDP: samples a model from the posterior, samples transitions from it, and uses lower/upper value bounds to prune the search; controlled by parameters K (nodes), C (branching factor), and depth.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Belief-lookahead sparse sampling with pruning using value bounds (online adaptive planning)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Samples from the posterior at each decision and adaptively explores branches of the lookahead tree based on computed value bounds (prunes unpromising branches), thereby focusing computation on informative/valuable parts of the belief-space.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>GC, GDL, Grid benchmarks (MDPs with transition uncertainty via FDM priors)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic MDP transitions, finite discrete states/actions, episodes truncated for evaluation; uncertainty represented by posterior over transition matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Evaluated on the same three benchmarks (GC 5×3, GDL 9×2, Grid 25×4); BFS3's parameters control the effective search complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Included in comparisons; provides reasonable performance but is constrained by online time budget and search parameters. In the paper, BFS3 did not consistently beat ANN-BRL under constrained online times.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not applicable (BFS3 is an online adaptive planner).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Uses targeted online sampling and pruning which can be sample-efficient for focused decision points but requires non-trivial computation per decision; actual sample counts and curves not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Managed through the sparse sampling and value-bound pruning mechanism—search decisions reflect both expected rewards and uncertainty-informed pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared experimentally to BAMCP, SBOSS, BEB, OPPS, ANN-BRL and simple baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BFS3 implements an efficient belief-space sparse search approach, but like other belief-lookahead methods its utility is limited by online computational budgets; pruning via value bounds helps focus search but does not eliminate the need for significant computation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Practical applicability limited by online time constraints; no strong advantage over ANN-BRL in the paper's time-constrained evaluation regime.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1157.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1157.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SBOSS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Smarter Best Of Sampled Set (SBOSS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based BRL algorithm that samples multiple MDPs from the posterior, merges them into a single MDP, and uses uncertainty-derived bounds to decide how many models to sample and how often to update — trading computation against accuracy adaptively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smarter sampling in model-based bayesian reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SBOSS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Assumes Dirichlet posterior over transitions; derives uncertainty bounds on state-action values to dynamically determine the number of posterior-sampled models (and resampling frequency), builds a merged MDP from sampled models and computes its optimal action.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Adaptive model sampling based on uncertainty bounds (dynamically determines sample count and posterior update frequency)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses uncertainty bounds to increase/decrease the number of sampled models and the frequency of merging based on desired accuracy/compute tradeoff; thus the algorithm adaptively allocates computation where uncertainty is high.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>GC, GDL, Grid benchmarks (FDM prior over transition matrices)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic transitions in finite discrete MDPs; uncertainty over transition probabilities modelled with Dirichlet distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Benchmarks as above (5×3, 9×2, 25×4). Algorithm performance and runtime depend on parameters ε (controls number of samples) and δ (resampling frequency).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Included in experiments; performance varies with parameter choices and tends to be computationally expensive. SBOSS can perform well with more computation but is less competitive under tight time constraints compared to ANN-BRL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not applicable; SBOSS's core is adaptive sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Adaptive sampling can focus sampling where needed, but merged-MDP computation and dynamic sampling still impose significant online cost; exact sample-efficiency numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Managed implicitly by sampling from posterior and constructing merged models — more samples reduce uncertainty (favor exploitation), fewer samples leave more uncertainty (favor exploration implicitly).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Evaluated against BAMCP, BFS3, BEB, OPPS, ANN-BRL, and simple baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>SBOSS provides a principled way to adapt sampling effort using uncertainty bounds but can be computationally heavy online; its variable per-time-step cost makes it less predictable under strict online time budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Online computational cost can be high; performance sensitive to parameter settings; per-time-step variability in compute makes it difficult to guarantee compliance with real-time constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1157.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1157.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BEB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Exploration Bonus (BEB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computationally light model-free BRL approach that constructs the posterior-mean MDP at each timestep and adds an exploration bonus to rewards for less-observed transitions to encourage exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Near-Bayesian exploration in polynomial time</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BEB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At each timestep computes a mean MDP from the current posterior and modifies its reward by adding β times a function of transition counts (exploration bonus) then solves that MDP to obtain an action for that step; β controls the exploration bonus magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Exploration bonus based on current posterior counts (online adaptive bonus to encourage visiting uncertain transitions)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The exploration bonus decreases as transitions are observed more often (counts increase), so the agent adaptively shifts from exploration to exploitation as uncertainty reduces; action selection each step uses the optimal policy of the modified mean-MDP.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>GC, GDL, Grid (MDPs with FDM priors over transition matrices)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic transitions, finite state/action spaces, uncertainty expressed by Dirichlet/FDM posteriors; episodic with horizon T for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Evaluated on GC (5×3), GDL (9×2), Grid (25×4); parameter β tuned in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported as efficient when prior is accurate but performance drops significantly for inaccurate priors (paper explicitly notes BEB is sensitive to prior inaccuracies). In the comparative experiments BEB achieves competitive results on some benchmarks when prior matches test distribution but is not robust across mismatched priors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not applicable (algorithm intrinsically uses adaptive bonus).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Computationally inexpensive per-step relative to tree-search planners; sample efficiency depends heavily on prior quality and chosen β, and degrades when prior is inaccurate.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit single-parameter mechanism: exploration bonus β × (function of inverse visit counts) added to rewards, causing the solver of the modified MDP to prefer less-observed transitions until counts grow.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to BAMCP, BFS3, SBOSS, OPPS, ANN-BRL, and simple baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BEB is simple and fast online and performs well when priors are accurate, but is brittle to prior misspecification — performance can drop substantially when prior differs from test distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sensitivity to incorrect priors; lacks robustness in inaccurate-prior scenarios relative to methods that better incorporate offline training or more flexible policies (like ANN-BRL).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1157.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1157.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPPS-DS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Offline Prior-based Policy Search for Discrete Strategy spaces (OPPS-DS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline algorithm that enumerates a discrete space of exploration/exploitation strategies and uses multi-armed-bandit style evaluation to identify the best strategy on average under the prior, yielding a simple, fast online policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning exploration/exploitation strategies for single trajectory Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>OPPS-DS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Offline method that generates a (possibly large) discrete set of E/E strategies, evaluates them by Monte Carlo under the prior, and uses a bandit-selection procedure to discard poor strategies early so computational budget is focused on promising candidates; selected strategy is used online without further Bayesian updating.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Offline multi-armed-bandit style adaptive selection of strategy candidates (adaptive allocation of simulation budget across candidate strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>During the offline phase the algorithm adaptively allocates simulation/evaluation budget to candidate strategies: poorly performing strategies are eliminated early using bandit rules, so more evaluations are devoted to promising strategies; online the chosen strategy is fixed and cheap to execute.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>GC, GDL, Grid benchmarks (FDM prior over MDPs)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic transitions in finite discrete MDPs; offline prior used to evaluate candidate strategies; time budgets for offline evaluation control search depth.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Strategy-space sizes tested (F2..F6) with cardinalities up to several thousand; benchmarks have 5×3, 9×2, and 25×4 configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Performs strongly in many settings and is often competitive with ANN-BRL under certain offline/online time-budget regimes; in the inaccurate-prior Grid case OPPS-DS was the second best and ANN-BRL (Q) scored ~4× higher than OPPS-DS on that specific hard benchmark (paper's claim).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not applicable—core of OPPS is its adaptive offline selection.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Efficient in allocating offline simulation budget via bandit elimination; requires fewer evaluations of bad strategies, but still requires significant offline computation to canvass a large strategy space.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled at the meta-level: OPPS selects among pre-defined E/E strategies (which themselves encode exploration/exploitation behavior); the offline bandit eliminates inferior strategies so the final selected strategy represents the chosen E/E tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Directly compared to ANN-BRL, BAMCP, BFS3, SBOSS, BEB and simple baselines in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>OPPS-DS is a practical offline method that can identify good E/E strategies under a prior and perform well online with minimal computation; it competes favorably when adequate offline computation is available but can be outperformed by ANN-BRL (Q) particularly on the hard Grid benchmark when priors are inaccurate.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Quality depends on whether the discrete strategy space contains suitable strategies; lacks theoretical guarantees and requires a well-chosen strategy space; offline computation can be large for rich strategy sets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient Bayes-adaptive Reinforcement Learning using sample-based search <em>(Rating: 2)</em></li>
                <li>Approaching Bayesoptimalilty using Monte-Carlo tree search <em>(Rating: 2)</em></li>
                <li>Smarter sampling in model-based bayesian reinforcement learning <em>(Rating: 2)</em></li>
                <li>Near-Bayesian exploration in polynomial time <em>(Rating: 2)</em></li>
                <li>Learning exploration/exploitation strategies for single trajectory Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Benchmarking for Bayesian Reinforcement Learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1157",
    "paper_id": "paper-34549",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "ANN-BRL",
            "name_full": "Artificial Neural Networks for Bayesian Reinforcement Learning",
            "brief_description": "An offline, prior-based policy-search algorithm that trains an ensemble of neural-network classifiers (MLPs combined with SAMME boosting) on simulated trajectories from a prior over MDPs to produce a fast online policy that maps histories (reprocessed into fixed-size features) to actions without performing Bayes updates online.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ANN-BRL",
            "agent_description": "Policy represented as an ensemble of multilayer perceptrons (3-layer MLPs) trained with SAMME (multi-class boosting). Inputs are fixed-size feature vectors derived from histories (either Q-value features or transition-count features plus current state). Training dataset is generated offline by simulating MDPs sampled from the prior and labelling each history with the optimal action for the fully-known sampled MDP. Online the trained classifier predicts action confidences and selects the highest-scoring action.",
            "adaptive_design_method": "Offline adaptive dataset generation using stochastic exploration (ε-Optimal agents) and boosting-based emphasis on hard samples; no online Bayesian updates",
            "adaptation_strategy_description": "During offline data generation the authors vary ε (the ε-Optimal agent parameter) between simulations to induce exploration diversity across sampled MDPs, thereby adaptively covering belief-state space; SAMME reweights training samples to focus subsequent weak learners on samples misclassified by earlier learners (adaptive emphasis in model training). At test time the policy is fixed (no posterior updates).",
            "environment_name": "Benchmarks drawn from Flat Dirichlet Multinomial (FDM) priors — Generalised Chain (GC), Generalised Double-Loop (GDL), Grid",
            "environment_characteristics": "Unknown transition dynamics (MDP transitions drawn from an FDM prior), discrete finite state and action spaces, stochastic transitions, bounded rewards; uncertainty is over model dynamics (not partial state observability), episodes truncated at horizon T for evaluation.",
            "environment_complexity": "GC: 5 states × 3 actions; GDL: 9 states × 2 actions; Grid: 25 states × 4 actions; trajectories of length T (chosen to bound approximation error); prior/test distributions varied (accurate vs inaccurate prior experiments).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported state-of-the-art empirical performance across the benchmark suite; ANN-BRL using Q-value features (ANN-BRL (Q)) obtains top results, notably outperforming all baselines on the most challenging Grid benchmark (in the inaccurate-prior case it reports a score ~4× larger than OPPS-DS on that benchmark). ANN-BRL also yields low online computation time compared to methods that do online Bayesian planning. (Paper reports mean cumulative-return scores and 95% CIs per benchmark but tabulated values are given in the paper.)",
            "performance_without_adaptation": "Not directly reported; baseline comparisons include Random, ε-Greedy, Softmax and multiple BRL planners. The ANN-BRL policy trained with only transition-counter features (ANN-BRL (C)) performs worse than ANN-BRL (Q), indicating reduced performance when informative features or adaptive data-generation choices are not used.",
            "sample_efficiency": "ANN-BRL requires substantial offline simulation (generate trajectories from many sampled MDPs) but is sample-efficient online: after offline training the forward pass is cheap and requires no further environment model inference. Specific offline sample counts: the paper uses n sampled MDPs and T-length trajectories (paper reports default n and training settings), but exact sample-to-performance curves are not provided.",
            "exploration_exploitation_tradeoff": "Handled offline via the ε-Optimal stochastic policy used during dataset generation (each simulation uses an ε that controls probability of random vs optimal action when generating trajectories) to expose the learner to both exploitative and exploratory transitions; no explicit online exploration-exploitation mechanism (no Bayes updates online).",
            "comparison_methods": "Compared experimentally to Random, ε-Greedy, Softmax, OPPS-DS, BAMCP, BFS3, SBOSS, BEB across three benchmark distributions and both accurate and inaccurate prior cases.",
            "key_results": "1) Offline-trained ANNs (ANN-BRL) can approximate Bayes-optimal behaviour across a prior distribution and achieve state-of-the-art cumulative returns while being orders of magnitude faster online because they avoid expensive posterior updates; 2) Using Q-value based features yields substantially better performance than raw transition-count features; 3) ANN-BRL is robust to prior inaccuracies and can outperform classical BRL planners on difficult benchmarks (notably Grid) while respecting tight online time budgets.",
            "limitations_or_failures": "1) Requires significant offline simulation and the quality depends on how well the prior covers the test distribution; 2) Labels used during training are optimal actions under full observability of the sampled MDP rather than true Bayes-optimal actions, which is an approximation the authors note could be improved; 3) ANN-BRL (C) with only transition counters sometimes underperforms other algorithms, indicating sensitivity to feature choice; 4) No online posterior update means the agent cannot adapt further to a specific encountered MDP beyond what the offline-trained policy encodes.",
            "uuid": "e1157.0"
        },
        {
            "name_short": "BAMCP",
            "name_full": "Bayes-adaptive Monte Carlo Planning (BAMCP)",
            "brief_description": "A Monte-Carlo tree-search method (UCT variant) that plans in the belief-augmented MDP by sampling transitions from posterior distributions; provides (asymptotic) Bayes-optimality guarantees but requires substantial online computation.",
            "citation_title": "Efficient Bayes-adaptive Reinforcement Learning using sample-based search",
            "mention_or_use": "use",
            "agent_name": "BAMCP",
            "agent_description": "Model-based online planner that builds a lookahead search tree in the belief-augmented state space using Monte-Carlo rollouts; samples transition models according to the current posterior and uses a UCT-style bandit selection within the planning tree. Controlled by parameters K (nodes per timestep) and depth.",
            "adaptive_design_method": "Belief-lookahead via posterior sampling and Monte-Carlo tree search (online active planning / adaptive experiment selection via lookahead)",
            "adaptation_strategy_description": "At each decision step the agent samples possible MDP models from the current posterior (based on observed history) and expands a planning tree; action selection adapts to current posterior uncertainty by preferentially exploring branches of the tree that look promising under sampled models (UCT selection balances exploration vs exploitation in the tree).",
            "environment_name": "Same benchmark suite (MDPs drawn from FDM priors: GC, GDL, Grid)",
            "environment_characteristics": "Unknown stochastic MDP dynamics (transition probabilities drawn from Dirichlet priors), finite discrete state/action spaces, bounded rewards; uncertainty over models (belief state).",
            "environment_complexity": "Evaluated on GC (5×3), GDL (9×2), Grid (25×4). Online planning depth and branching control computational cost (tested K and depth parameters).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Included in experimental comparisons; performs well but is sensitive to the online computational budget—BL approaches can achieve high accuracy but may be inapplicable under tight online time constraints. The paper reports BAMCP scores among baselines (see tables) but does not claim BAMCP outperforms ANN-BRL under the evaluated time constraints.",
            "performance_without_adaptation": "Not applicable (BAMCP inherently adapts online).",
            "sample_efficiency": "Relies on online sampling/simulations during planning each decision step; sample-efficient in the sense of targeted lookahead but computationally heavy per timestep; efficiency depends on K and depth parameters and available online time budget.",
            "exploration_exploitation_tradeoff": "Handled via UCT-style selection within the search tree (implicit exploration bonus based on visitation counts) and by sampling models from the posterior so actions are selected to trade off immediate reward vs information value estimated by lookahead.",
            "comparison_methods": "Compared to BFS3, SBOSS, BEB, OPPS, ANN-BRL and simpler baselines in the experiments.",
            "key_results": "BAMCP provides principled online adaptive planning with theoretical Bayes-adaptive guarantees but is practically constrained by online computation time; under tight time budgets other approaches (e.g., ANN-BRL) that invest offline can outperform it in wall-clock-constrained scenarios.",
            "limitations_or_failures": "High online computational cost; performance degrades / is infeasible when the per-decision time budget is small; the paper emphasizes this as a practical limitation compared to offline-trained policies.",
            "uuid": "e1157.1"
        },
        {
            "name_short": "BFS3",
            "name_full": "Bayesian Forward Search Sparse Sampling (BFS3)",
            "brief_description": "An online belief-augmented planner applying sparse forward search (FSSS) to belief-MDPs, using sampled models and value bounds to prune the search space.",
            "citation_title": "Approaching Bayesoptimalilty using Monte-Carlo tree search",
            "mention_or_use": "use",
            "agent_name": "BFS3",
            "agent_description": "Applies Forward Search Sparse Sampling to the belief-augmented MDP: samples a model from the posterior, samples transitions from it, and uses lower/upper value bounds to prune the search; controlled by parameters K (nodes), C (branching factor), and depth.",
            "adaptive_design_method": "Belief-lookahead sparse sampling with pruning using value bounds (online adaptive planning)",
            "adaptation_strategy_description": "Samples from the posterior at each decision and adaptively explores branches of the lookahead tree based on computed value bounds (prunes unpromising branches), thereby focusing computation on informative/valuable parts of the belief-space.",
            "environment_name": "GC, GDL, Grid benchmarks (MDPs with transition uncertainty via FDM priors)",
            "environment_characteristics": "Unknown stochastic MDP transitions, finite discrete states/actions, episodes truncated for evaluation; uncertainty represented by posterior over transition matrices.",
            "environment_complexity": "Evaluated on the same three benchmarks (GC 5×3, GDL 9×2, Grid 25×4); BFS3's parameters control the effective search complexity.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Included in comparisons; provides reasonable performance but is constrained by online time budget and search parameters. In the paper, BFS3 did not consistently beat ANN-BRL under constrained online times.",
            "performance_without_adaptation": "Not applicable (BFS3 is an online adaptive planner).",
            "sample_efficiency": "Uses targeted online sampling and pruning which can be sample-efficient for focused decision points but requires non-trivial computation per decision; actual sample counts and curves not reported.",
            "exploration_exploitation_tradeoff": "Managed through the sparse sampling and value-bound pruning mechanism—search decisions reflect both expected rewards and uncertainty-informed pruning.",
            "comparison_methods": "Compared experimentally to BAMCP, SBOSS, BEB, OPPS, ANN-BRL and simple baselines.",
            "key_results": "BFS3 implements an efficient belief-space sparse search approach, but like other belief-lookahead methods its utility is limited by online computational budgets; pruning via value bounds helps focus search but does not eliminate the need for significant computation.",
            "limitations_or_failures": "Practical applicability limited by online time constraints; no strong advantage over ANN-BRL in the paper's time-constrained evaluation regime.",
            "uuid": "e1157.2"
        },
        {
            "name_short": "SBOSS",
            "name_full": "Smarter Best Of Sampled Set (SBOSS)",
            "brief_description": "A model-based BRL algorithm that samples multiple MDPs from the posterior, merges them into a single MDP, and uses uncertainty-derived bounds to decide how many models to sample and how often to update — trading computation against accuracy adaptively.",
            "citation_title": "Smarter sampling in model-based bayesian reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "SBOSS",
            "agent_description": "Assumes Dirichlet posterior over transitions; derives uncertainty bounds on state-action values to dynamically determine the number of posterior-sampled models (and resampling frequency), builds a merged MDP from sampled models and computes its optimal action.",
            "adaptive_design_method": "Adaptive model sampling based on uncertainty bounds (dynamically determines sample count and posterior update frequency)",
            "adaptation_strategy_description": "Uses uncertainty bounds to increase/decrease the number of sampled models and the frequency of merging based on desired accuracy/compute tradeoff; thus the algorithm adaptively allocates computation where uncertainty is high.",
            "environment_name": "GC, GDL, Grid benchmarks (FDM prior over transition matrices)",
            "environment_characteristics": "Unknown stochastic transitions in finite discrete MDPs; uncertainty over transition probabilities modelled with Dirichlet distributions.",
            "environment_complexity": "Benchmarks as above (5×3, 9×2, 25×4). Algorithm performance and runtime depend on parameters ε (controls number of samples) and δ (resampling frequency).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Included in experiments; performance varies with parameter choices and tends to be computationally expensive. SBOSS can perform well with more computation but is less competitive under tight time constraints compared to ANN-BRL.",
            "performance_without_adaptation": "Not applicable; SBOSS's core is adaptive sampling.",
            "sample_efficiency": "Adaptive sampling can focus sampling where needed, but merged-MDP computation and dynamic sampling still impose significant online cost; exact sample-efficiency numbers not provided.",
            "exploration_exploitation_tradeoff": "Managed implicitly by sampling from posterior and constructing merged models — more samples reduce uncertainty (favor exploitation), fewer samples leave more uncertainty (favor exploration implicitly).",
            "comparison_methods": "Evaluated against BAMCP, BFS3, BEB, OPPS, ANN-BRL, and simple baselines.",
            "key_results": "SBOSS provides a principled way to adapt sampling effort using uncertainty bounds but can be computationally heavy online; its variable per-time-step cost makes it less predictable under strict online time budgets.",
            "limitations_or_failures": "Online computational cost can be high; performance sensitive to parameter settings; per-time-step variability in compute makes it difficult to guarantee compliance with real-time constraints.",
            "uuid": "e1157.3"
        },
        {
            "name_short": "BEB",
            "name_full": "Bayesian Exploration Bonus (BEB)",
            "brief_description": "A computationally light model-free BRL approach that constructs the posterior-mean MDP at each timestep and adds an exploration bonus to rewards for less-observed transitions to encourage exploration.",
            "citation_title": "Near-Bayesian exploration in polynomial time",
            "mention_or_use": "use",
            "agent_name": "BEB",
            "agent_description": "At each timestep computes a mean MDP from the current posterior and modifies its reward by adding β times a function of transition counts (exploration bonus) then solves that MDP to obtain an action for that step; β controls the exploration bonus magnitude.",
            "adaptive_design_method": "Exploration bonus based on current posterior counts (online adaptive bonus to encourage visiting uncertain transitions)",
            "adaptation_strategy_description": "The exploration bonus decreases as transitions are observed more often (counts increase), so the agent adaptively shifts from exploration to exploitation as uncertainty reduces; action selection each step uses the optimal policy of the modified mean-MDP.",
            "environment_name": "GC, GDL, Grid (MDPs with FDM priors over transition matrices)",
            "environment_characteristics": "Unknown stochastic transitions, finite state/action spaces, uncertainty expressed by Dirichlet/FDM posteriors; episodic with horizon T for evaluation.",
            "environment_complexity": "Evaluated on GC (5×3), GDL (9×2), Grid (25×4); parameter β tuned in experiments.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported as efficient when prior is accurate but performance drops significantly for inaccurate priors (paper explicitly notes BEB is sensitive to prior inaccuracies). In the comparative experiments BEB achieves competitive results on some benchmarks when prior matches test distribution but is not robust across mismatched priors.",
            "performance_without_adaptation": "Not applicable (algorithm intrinsically uses adaptive bonus).",
            "sample_efficiency": "Computationally inexpensive per-step relative to tree-search planners; sample efficiency depends heavily on prior quality and chosen β, and degrades when prior is inaccurate.",
            "exploration_exploitation_tradeoff": "Explicit single-parameter mechanism: exploration bonus β × (function of inverse visit counts) added to rewards, causing the solver of the modified MDP to prefer less-observed transitions until counts grow.",
            "comparison_methods": "Compared to BAMCP, BFS3, SBOSS, OPPS, ANN-BRL, and simple baselines.",
            "key_results": "BEB is simple and fast online and performs well when priors are accurate, but is brittle to prior misspecification — performance can drop substantially when prior differs from test distribution.",
            "limitations_or_failures": "Sensitivity to incorrect priors; lacks robustness in inaccurate-prior scenarios relative to methods that better incorporate offline training or more flexible policies (like ANN-BRL).",
            "uuid": "e1157.4"
        },
        {
            "name_short": "OPPS-DS",
            "name_full": "Offline Prior-based Policy Search for Discrete Strategy spaces (OPPS-DS)",
            "brief_description": "An offline algorithm that enumerates a discrete space of exploration/exploitation strategies and uses multi-armed-bandit style evaluation to identify the best strategy on average under the prior, yielding a simple, fast online policy.",
            "citation_title": "Learning exploration/exploitation strategies for single trajectory Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "OPPS-DS",
            "agent_description": "Offline method that generates a (possibly large) discrete set of E/E strategies, evaluates them by Monte Carlo under the prior, and uses a bandit-selection procedure to discard poor strategies early so computational budget is focused on promising candidates; selected strategy is used online without further Bayesian updating.",
            "adaptive_design_method": "Offline multi-armed-bandit style adaptive selection of strategy candidates (adaptive allocation of simulation budget across candidate strategies)",
            "adaptation_strategy_description": "During the offline phase the algorithm adaptively allocates simulation/evaluation budget to candidate strategies: poorly performing strategies are eliminated early using bandit rules, so more evaluations are devoted to promising strategies; online the chosen strategy is fixed and cheap to execute.",
            "environment_name": "GC, GDL, Grid benchmarks (FDM prior over MDPs)",
            "environment_characteristics": "Unknown stochastic transitions in finite discrete MDPs; offline prior used to evaluate candidate strategies; time budgets for offline evaluation control search depth.",
            "environment_complexity": "Strategy-space sizes tested (F2..F6) with cardinalities up to several thousand; benchmarks have 5×3, 9×2, and 25×4 configurations.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Performs strongly in many settings and is often competitive with ANN-BRL under certain offline/online time-budget regimes; in the inaccurate-prior Grid case OPPS-DS was the second best and ANN-BRL (Q) scored ~4× higher than OPPS-DS on that specific hard benchmark (paper's claim).",
            "performance_without_adaptation": "Not applicable—core of OPPS is its adaptive offline selection.",
            "sample_efficiency": "Efficient in allocating offline simulation budget via bandit elimination; requires fewer evaluations of bad strategies, but still requires significant offline computation to canvass a large strategy space.",
            "exploration_exploitation_tradeoff": "Handled at the meta-level: OPPS selects among pre-defined E/E strategies (which themselves encode exploration/exploitation behavior); the offline bandit eliminates inferior strategies so the final selected strategy represents the chosen E/E tradeoff.",
            "comparison_methods": "Directly compared to ANN-BRL, BAMCP, BFS3, SBOSS, BEB and simple baselines in experiments.",
            "key_results": "OPPS-DS is a practical offline method that can identify good E/E strategies under a prior and perform well online with minimal computation; it competes favorably when adequate offline computation is available but can be outperformed by ANN-BRL (Q) particularly on the hard Grid benchmark when priors are inaccurate.",
            "limitations_or_failures": "Quality depends on whether the discrete strategy space contains suitable strategies; lacks theoretical guarantees and requires a well-chosen strategy space; offline computation can be large for rich strategy sets.",
            "uuid": "e1157.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient Bayes-adaptive Reinforcement Learning using sample-based search",
            "rating": 2,
            "sanitized_title": "efficient_bayesadaptive_reinforcement_learning_using_samplebased_search"
        },
        {
            "paper_title": "Approaching Bayesoptimalilty using Monte-Carlo tree search",
            "rating": 2,
            "sanitized_title": "approaching_bayesoptimalilty_using_montecarlo_tree_search"
        },
        {
            "paper_title": "Smarter sampling in model-based bayesian reinforcement learning",
            "rating": 2,
            "sanitized_title": "smarter_sampling_in_modelbased_bayesian_reinforcement_learning"
        },
        {
            "paper_title": "Near-Bayesian exploration in polynomial time",
            "rating": 2,
            "sanitized_title": "nearbayesian_exploration_in_polynomial_time"
        },
        {
            "paper_title": "Learning exploration/exploitation strategies for single trajectory Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "learning_explorationexploitation_strategies_for_single_trajectory_reinforcement_learning"
        },
        {
            "paper_title": "Benchmarking for Bayesian Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "benchmarking_for_bayesian_reinforcement_learning"
        }
    ],
    "cost": 0.0189625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Approximate Bayes Optimal Policy Search using Neural Networks</p>
<p>Michaël Castronovo 
Montefiore Institute
Université de Liège
LiègeBelgium</p>
<p>Vincent Franc ¸ois-Lavet 
Montefiore Institute
Université de Liège
LiègeBelgium</p>
<p>Raphaël Fonteneau 
Montefiore Institute
Université de Liège
LiègeBelgium</p>
<p>Damien Ernst 
Montefiore Institute
Université de Liège
LiègeBelgium</p>
<p>Adrien Couëtoux 
Montefiore Institute
Université de Liège
LiègeBelgium</p>
<p>Approximate Bayes Optimal Policy Search using Neural Networks
33C5CC2B4FE861E470E92DAB0AFDBCBE10.5220/0006191701420153Bayesian Reinforcement LearningArtificial Neural NetworksOffline Policy Search
Bayesian Reinforcement Learning (BRL) agents aim to maximise the expected collected rewards obtained when interacting with an unknown Markov Decision Process (MDP) while using some prior knowledge.State-of-the-art BRL agents rely on frequent updates of the belief on the MDP, as new observations of the environment are made.This offers theoretical guarantees to converge to an optimum, but is computationally intractable, even on small-scale problems.In this paper, we present a method that circumvents this issue by training a parametric policy able to recommend an action directly from raw observations.Artificial Neural Networks (ANNs) are used to represent this policy, and are trained on the trajectories sampled from the prior.The trained model is then used online, and is able to act on the real MDP at a very low computational cost.Our new algorithm shows strong empirical performance, on a wide range of test problems, and is robust to inaccuracies of the prior distribution.</p>
<p>INTRODUCTION</p>
<p>Bayes-Adaptive Markov Decision Processes (BAMDP) (Silver, 1963;Martin, 1967) form a natural framework to deal with sequential decisionmaking problems when some of the information is hidden.In these problems, an agent navigates in an initially unknown environment and receives a numerical reward according to its actions.However, actions that yield the highest instant reward and actions that maximise the gathering of knowledge about the environment are often different.The BAMDP framework leads to a rigorous definition of an optimal solution to this learning problem, which is based on finding a policy that reaches an optimal balance between exploration and exploitation.</p>
<p>In this research, the case where prior knowledge is available about the environment is studied.More specifically, this knowledge is represented as a random distribution over possible environments, and can be updated as the agent makes new observations.In practice, this happens for example when training a drone to fly in a safe environment before sending it on the operation field (Zhang et al., 2015).This is called offline training and can be beneficial to the online performance in the real environment, even if prior knowledge is inaccurate (Castronovo et al., 2014).</p>
<p>State-of-the-art Bayesian algorithms generally do not use offline training.Instead, they rely on Bayes updates and sampling techniques during the interaction, which may be too computationally expensive, even on very small MDPs (Castronovo et al., 2015).In order to reduce significantly this cost, we propose a new practical algorithm to solve BAMDPs: Artificial Neural Networks for Bayesian Reinforcement Learning (ANN-BRL).Our algorithm aims at finding an optimal policy, i.e. a mapping from observations to actions, which maximises the rewards in a certain environment.This policy is trained to act optimally on some MDPs sampled from the prior distribution, and then it is used in the test environment.By design, our approach does not use any Bayes update, and is thus computationally inexpensive during online interactions.Our policy is modelled as an ensemble of ANNs, combined by using SAMME (Zhu et al., 2009), a boosting algorithm.</p>
<p>Artificial Neural Networks offer many advantages for the needed purpose.First, they are able to learn complex functions and are, thus, capable of encoding almost any policy.Second, ANNs can be trained very efficiently, using the backpropagation method, even on a large dataset.Lastly, ANNs' forward pass is fast, which makes them ideal to perform predictions during the online phase, when the computation time constraints are tight.</p>
<p>In our experiments, we used a benchmark recently introduced in (Castronovo et al., 2015).It compares all the major state-of-the-art BRL algorithms on a wide array of test problems, and provides a detailed computation time analysis.Since most state-of-theart agents found in the literature are not any time algorithms, this last feature is very useful to compare solvers that have different time constraints.</p>
<p>This paper is organised as follows: Section 2 gives an overview of the state-of-the-art in Bayesian Reinforcement Learning.Section 3 presents the problem statement.Section 4 describes the algorithm.Section 5 shows a comparison between our algorithm and state-of-the-art algorithms of the domain.Section 6 offers a conclusion and discusses future work.</p>
<p>STATE-OF-THE-ART</p>
<p>Bayesian Reinforcement Learning (BRL) algorithms rely on Bayesian updates of the prior knowledge on the environment as new observations are made.</p>
<p>Model-based approaches maintain explicitly a posterior distribution, given the prior and the transitions observed so far.Bayes-adaptive Monte Carlo Planning (BAMCP) (Guez et al., 2012) and Bayesian Forward Search Sparse Sampling (BFS3) (Asmuth and Littman, 2011) rely on the exploration of the belief state space with a belief-lookahead (BL) approach.In this case, the posterior is used to explore efficiently the look-ahead tree and estimate the Q-values of the current belief-state.The accuracy is depending on the number of nodes those algorithms are able to visit, which is limited by an on-line computation time budget.Despite theoretical guarantees to reach Bayesian optimality offered by BL approaches 1 , they may not be applicable when the time budget that can be allocated for on-line decision making is short (Castronovo et al., 2015).Another method, Smarter Best of Sampled Set (SBOSS) (Castro and Precup, 2010), samples several MDPs from the posterior distribution, builds a merged MDP, and computes its Q-function.The number of MDPs to sample and the frequency at which a merged MDP has to be built is determined by uncertainty bounds on the Q-values.As a consequence, the online computation time of SBOSS may vary at each time-step.However, the number of samples and the frequency are depending on two parameters, which are used to fix the online computation time on average.More computation time improves the accuracy of the computed Qvalues.However, on the downside, this approach remains computationally expensive (Castronovo et al., 2015).</p>
<p>On the other hand, model-free approaches only 1 e.g.BAMCP (Guez et al., 2012).</p>
<p>maintain a list of the transitions observed, and compute value functions.In this case, the prior distribution is used to initialise this list (e.g.: a uniform distribution consisting to assume each transition has been observed once).Bayesian Exploration Bonus (BEB) (Kolter and Ng, 2009a) builds the expected MDP given the current history at each timestep.The reward function of this MDP is slightly modified to give an exploration bonus to transitions which have been observed less frequently.The optimal Q-function of this MDP is then used to determine which action to perform.BEB is a simple, but efficient algorithm that remains computationally inexpensive for accurate prior distributions.Nevertheless, BEB's performance drops significantly for inaccurate prior distributions (Castronovo et al., 2015).</p>
<p>Another approach was proposed a few years ago with Offline Prior-based Policy Search (OPPS) (Castronovo et al., 2012;Castronovo et al., 2014).During an offline phase, OPPS builds a discrete set of E/E strategies, and identifies which strategy of the set is the most efficient on average, to address any MDP drawn from the prior distribution.Instead of evaluating the performance of each strategy with the same accuracy, OPPS uses a multi-armed bandit strategy to discard gradually the worst strategies.This idea allows OPPS to consider a strategy space large enough to contain good candidates for many problems.Besides, the E/E strategies considered are computationally inexpensive for on-line decision making, but the approach lacks theoretical guarantees (Castronovo et al., 2015).</p>
<p>A more detailed description of each algorithm is available in the Appendix 6.1.</p>
<p>PRELIMINARIES</p>
<p>Bayes Adaptive Markov Decision</p>
<p>Process (BAMDP)</p>
<p>We, hereafter, describe the formulation of optimal decision-making in a BAMDP.Let M = (X,U, f (•), ρ M , γ) be a given unknown MDP, where</p>
<p>• X = {x (1) , . . ., x (n X ) } denotes its finite state space</p>
<p>• U = {u (1) , . . ., u (n U ) } denotes its finite action space
• r t = ρ M (x t , u t , x t+1 ) ∈ [R min , R max ]
denotes an instantaneous deterministic, bounded reward</p>
<p>• γ &gt; 0 its discount factor When the MDP is in state x t at time t and action u t is selected, the agent moves instantaneously to a next state x t+1 with a probability P(x t+1 |x t , u t ) = f (x t , u t , x t+1 ).In the BAMDP setting, the dynamics are unknown, and we assume that f is drawn according to a known distribution P( f ).Such a probability distribution is called a prior distribution; it represents what the MDP is believed to be before interacting with it.Let h t = (x 0 , u 0 , r 0 , x 1 , • • • , x t−1 , u t−1 , r t−1 , x t ) denote the history observed until time t.Given the current history h t , a policy π returns an action u t = π(h t ).Given an MDP M and a policy π, we define the cost
J π M = E π M [∑ t γ t r t ]
as the expected cumulated discounted reward on M, when applying policy π.Given a prior distribution p 0 M (•), the goal is to find a policy π * , called Bayes optimal that maximises the expected cost with respect to the prior distribution:
π * = arg max π E M∼p 0 M (•) J π M (1)
It is important to note that although this policy is good on average, with respect to the prior, it does not necessarily perform efficiently on each MDP sampled from the prior.Conversely, given a fixed and fully known MDP M, a policy that is optimal on M is likely to be very different from π * .</p>
<p>Solving BAMDP</p>
<p>Though solving a BAMDP exactly is theoretically well defined, it is intractable in practice (Guez et al., 2013) for two reasons.First, sampling possible transition probabilities, based on past observations, relies on the computation of P( f |h t ) ∝ P(h t | f )P( f ), which is intractable for most probabilistic models (Duff, 2002;Kaelbling et al., 1998;Kolter and Ng, 2009b).Second, the BAMDP state space is actually made of all possible histories and is infinite.Therefore, all known tractable algorithms rely on some form of approximation.They can be divided in two main classes: online methods, and offline methods.The former group (Fonteneau et al., 2013;Asmuth and Littman, 2011;Walsh et al., 2010;Kolter and Ng, 2009a) relies on sparse sampling of possible models based on the current observations, to reduce the number of transition probabilities computations.The latter group (Wang et al., 2012) uses the prior knowledge to train an agent able to act on all possible sequences of observations.Our approach belongs to this group, and is described in Section 4.</p>
<p>ALGORITHM DESCRIPTION</p>
<p>A Bayes optimal policy π * , as defined by Eq. 1, maps histories to Bayes actions.Although π * is unknown, an approximation may be computed.Let π θ be a parametric policy whose model parameters are θ.The model is fed up with the current history h t , and computes an output vector, associating a confidence score to each action in return.The agent simply selects the action with the highest score.</p>
<p>Our model is composed of several ANNs, where the model parameters, denoted by θ, are the weights of all the networks.All ANNs are fed up with the same inputs, and build several output vectors which are merged by using a weighted linear combination.</p>
<p>The training of this model requires a training dataset, whose generation is described in Section 4.1.It consists in performing simulations on MDPs drawn from the prior distribution to generate a training set.To each history observed during these simulations, we recommend an optimal action.Each &lt; history, rec-Algorithm 1: ANN-BRL -Offline phase.</p>
<p>Input: Time horizon T , prior distribution p 0 M (.)A history is a series of transitions whose size is unbounded, but ANNs can only be fed up with input vectors of a fixed size.To address this issue, histories are processed into fixed-size input vectors prior to training our model.This procedure is described in Section 4.2.
Output: A classifier C (.) {Generate transitions} for i = 1 to n do M (i) ∼ p 0 M (.) H (i) ← Simulate 1 trajectory of length T on M (i) end for {Compute input/output vectors for each transition} for i = 1 to n do h T ← H (i) for j = 1 to T do {Compute the input vector of sample (i, j)} h j ← h (1) T , . . . , h ( j) T ϕ i, j ← Reprocess h j {Compute the output vector of sample (i, j)} Q * i, j ← Q-Iteration(M (i) , T ) for k = 1 to n U do if k maximises Q * i, j (x, u (•) ) then out put (k) i, j = 1 else out put (k) i, j = −1 end if end for DataSet (i, j) ← {ϕ i, j , out put i,
More specifically, the ANNs are built iteratively by using SAMME -an Adaboosting algorithm.It consists in modifying the training dataset in order to increase the weights of the samples misclassified by the ANNs built previously.Section 4.3 details the SAMME algorithm and the necessary changes to fit the BRL setting.</p>
<p>Moreover, we also pseudo-code descriptions in</p>
<p>Generation of the Training Dataset</p>
<p>During the offline phase, we use the prior knowledge to generate samples which will compose the training dataset.For a given series of observations h t , we consider the optimal action w.r.t. the MDP from which h t has been generated.In other words, we give a label of 1 to actions that are optimal when the transition function f (.) is known, and −1 to the others.Our dataset is, thus, filled with suboptimal recommendations, from the Bayes optimal perspective.However, our samples are generated from multiple MDPs which are themselves sampled from the prior distribution.As a consequence, a history h can appear multiple times in our dataset but with different output vectors, because it has been generated from different MDPs for which the labels were different.The average output vector for a history h approximates the probability of each action u to be the optimal response to h when f M (.) is known, where M ∼ p 0 M (•).To a certain extent, it is similar to what is done by other BRL algorithms, such as BAMCP (Guez et al., 2012) when it explores a specific part of the belief-states space using Tree-Search techniques.</p>
<p>During the data generation phase, it is necessary to choose which parts of the state space to explore.Generating samples by following what is believed to be an optimal policy is likely to provide examples in rewarding areas of the state space, but only for the current MDP.Since it is not possible to know in advance which MDPs our agent will encounter during the online phase, we choose to induce some random Algorithm 2: ANN-BRL -Online phase.</p>
<p>Input: Prior distribution p 0 M (.), current history
h t = (x 0 , u 0 , r 0 , x 1 , • • • , x t−1 , u t−1 , r t−1 , x t ), classifier C (.)
Output: u t , the action to perform at time-step t {Compute the input vector}
ϕ t ← Reprocess h t input ← ϕ t {Compute the output vector} out put ← C (input) {Choose action u t w.r.t. the output vector} k ← k maximising out put (•) u t ← u (k)
exploration in the data generation process.More precisely, we define an ε-Optimal agent, which makes optimal decisions2 w.r.t. to the MDP with a probability 1 − ε, and random decisions otherwise.By varying the value of 0 &lt; ε &lt; 1 from one simulation to another, we are able to cover the belief-states space more efficiently than using a random agent.</p>
<p>Reprocess of a History</p>
<p>The raw input fed to our model is h t , an ordered series of observations up to time t.In order to simplify the problem and reduce training time, a data preprocessing step is applied to reduce h t to a fixed number of features
ϕ h t = [ ϕ (1) h t , . . . , ϕ (N) h t ], N ∈ N.
There are two types of features that are considered in this paper: Q-values and transition counters.</p>
<p>Q-values are obtained by building an approximation of the current MDP from h t and computing its Q-function, thanks to the well-known Q-Iteration algorithm (Sutton and Barto, 1998).Each Q-value defines a different feature:
ϕ h t = [ Q h t (x (1) , u (1) ), . . . , Q h t (x (n X ) , u (n U ) ) ]
A transition counter represents the number of occurrences of specific transition in h t .Let C h t (&lt; x, u, x &gt;) be the transition counter of transition &lt; x, u, x &gt;.The number of occurrences of all transitions defines the following features:
ϕ h t = [ C h t (&lt; x (1) , u (1) , x (1) &gt;), . . . , C h t (&lt; x (n X ) , u (n U ) , x (n X ) &gt;) ] (2)
At this stage, we computed a set of features which do not take into account the order of appearance of each transition.We consider that this order is not necessary as long as the current state x t is known.In this paper, two different cases have been studied:</p>
<ol>
<li>Q-values: We consider the set of all Q-values defined above.However, in order to take x t into account, those which are not related to x t are discarded.
ϕ h t = [ Q h t (x t , u (1) ), . . . , Q h t (x t , u (n U ) ) ]</li>
<li>Transition counters: We consider the set of all transition counters defined above to which we add x t as an extra feature. 1), u (1) , x (1) &gt;), . . .,
ϕ h t = [ C h t (&lt; x (C h t (&lt; x (n X ) , u (n U ) , x (n X ) &gt;), x t ] (3)</li>
</ol>
<p>Model Definition and Training</p>
<p>The policy is now built from the training dataset by supervised learning on the multi-class classification problem where the classes c are the actions, and the vectors v are the histories.SAMME has been chosen to address this problem.It is a boosting algorithm which directly extends Adaboost from the two-class classifcation problem to the multi-class case.As a reminder, a full description of SAMME is provided in Appendix 6.2.SAMME builds iteratively a set of weak classifiers in order to build a strong one.In this paper, the weak classifiers are neural networks in the form of multilayer perceptrons (MLPs).SAMME algorithm aims to allow the training of a weak classifier to focus on the samples misclassified by the previous weak classifiers.This results in associating weights to the samples which reflect how bad the previous weak classifiers are for this sample.</p>
<p>MLPs are trained by backpropagation3 , which does not support weighted samples.Schwenk et al. presented different resampling approaches to address this issue with neural networks in (Schwenk and Bengio, 2000).The approach we have chosen samples from the dataset by interpreting the (normalised) weights as probabilities.Algorithm 3 describes it formally.</p>
<p>One of the specificities of the BRL formalisation lies in the definition of the classification error δ of a specific sample.This value is critical for SAMME in the evaluation of the performances of an MLP and the
u * = u (c) , p = C (v) Û = {u ∈ U | u = arg max u pu } δ = | Û \ {u * }| | Û|</p>
<p>EXPERIMENTS</p>
<p>Experimental Protocol</p>
<p>In order to empirically evaluate our algorithm, it is necessary to measure its expected return on a test distribution p M , after an offline training on a prior distribution p 0 M .Given a policy π, we denote this expected return J
π(p 0 M ) p M = E M∼p M (•) J π(p 0 M ) M
. In practice, we can only approximate this value.The steps to evaluate an agent π are defined as follows: M , we sample one trajectory on the MDP M, and compute the truncated cumulated return up to time T .The constant T is chosen so that the approximation error is bounded by ε = 0.01.</p>
<p>Finally, to estimate our comparison criterion
J π(p 0 M ) p M
, we compute the empirical average of the algorithm performance over N different MDPs, sampled from p M .For all our experiments, we report the measured values along with the corresponding 0.95 confidence interval.</p>
<p>The results will allow us to identify, for each experiment, the most suitable algorithm(s) depending on the constraints the agents must satisfy.Note that this protocol has been first presented in more details in (Castronovo et al., 2015).</p>
<p>Algorithms Comparison</p>
<p>In our experiment, the following algorithms have been tested, from the most elementary to the stateof-the-art BRL algorithms: Random, ε-Greedy, Softmax, OPPS-DS (Castronovo et al., 2012;Castronovo et al., 2014), BAMCP (Guez et al., 2012), BFS3 (Asmuth and Littman, 2011), SBOSS (Castro and Precup, 2010), and BEB (Kolter and Ng, 2009a).For detailed information on an algorithm and its parameters, please refer to the Appendix 6.1.</p>
<p>Most of the above algorithms are not any-time methods, i.e. they cannot be interrupted at an arbitrary time and yield a sensible result.Given an arbitrary time constraint, some algorithms may just be unable to yield anything.And out of those that do yield a result, some might use longer time than others.To give a fair representation of the results, we simply report, for each algorithm and each test problem, the recorded score (along with confidence interval), and the computation time needed.We can then say, for a given time constraint, what the best algorithms to solve any problem from the benchmark are.</p>
<p>Benchmarks</p>
<p>In our setting, the transition matrix is the only element which differs between two MDPs drawn from the same distribution.Generating a random MDP is, therefore, equivalent to generating a random transition matrix.In the BRL community, a common distribution used to generate such matrices is the Flat Dirichlet Multinomial distribution (FDM).It is chosen for the ease of its Bayesian updates.A FDM is defined by a parameter vector that we call θ.</p>
<p>We study two different cases: when the prior knowledge is accurate, and when it is not.In the former, the prior distribution over MDPs, called p θ 0 M (.), is exactly equal to the test distribution that is used during online training, p θ M (.).In the latter, the inaccuracy of the prior means that p θ 0 M (.) = p θ M (.).Sections 5.3.1, 5.3.2 and 5.3.3describes the three distributions considered for this study.</p>
<p>Generalised Chain Distribution</p>
<p>The Generalised Chain (GC) distribution is inspired from the 5-states chain problem (5 states, 3 ac- Figure 3: Studied distributions for benchmarking.</p>
<p>tions) (Dearden et al., 1998).The agent starts at state 1, and has to go through state 2, 3 and 4 in order to reach the last state, state 5, where the best rewards are.This cycle is illustrated in Figure 3(a).</p>
<p>Generalised Double-Loop Distribution</p>
<p>The Generalised Double-Loop (GDL) distribution is inspired from the double-loop problem (9 states, 2 actions) (Dearden et al., 1998).Two loops of 5 states are crossing at state 1 (where the agent starts) and one loop yields more rewards than the other.This problem is represented in Figure 3(b).</p>
<p>Grid Distribution</p>
<p>The Grid distribution is inspired from the Dearden's maze problem (25 states, 4 actions) (Dearden et al., 1998).The agent is placed at a corner of a 5x5 grid (the S cell), and has to reach the goal corner (the G cell).The agent can perform 4 different actions, corresponding to the 4 directions (up, down, left, right), but the actual transition probabilities are conditioned by the underlying transition matrix.This benchmark is illustrated in Figure 3(c).</p>
<p>Results</p>
<p>For each experiment, we tested each algorithm with several values for their parameter(s).The values considered in this paper are detailed in Appendix 6.    corresponding to the time consumed by the agent while training on the prior distribution 6 .Each of the plots in Fig. 4 and Fig. 5 present a 2-D graph, where the X-axis represents a mean online computation time constraint, while the Y-axis represents an offline computation time constraint.For each point of the graph: (i) all agents that do not satisfy the constraints are discarded; (ii) for each algorithm, the agent leading to the best performance in average is selected; (iii) the list of agents whose performances are not significantly different is built.For this pur-6 Notice that some agents do not require an offline training phase.</p>
<p>pose, a paired sampled Z-test (with a confidence level of 95%) has been used to discard the agents which are significantly worst than the best one.Since several algorithms can be associated to a single point, several boxes have been drawn to gather the points which share the same set of algorithms.</p>
<p>Accurate Case</p>
<p>In Table 1, it is noted that ANN-BRL (Q) 7 gets extremely good scores on the two first benchmarks.When taking into account time constraints, ANN-BRL (Q) requires a slightly higher offline time bound to be on par with OPPS, and can even surpass it on the last benchmark as shown in Fig. 4. ANN-BRL (C)8 is significantly less efficient than ANN-BRL (Q) on the first and last benchmarks.The difference is less noticeable in the second one.</p>
<p>Inaccurate Case</p>
<p>Similar results have been observed for the inaccurate case and can be shown in Fig. 5 and Table 2 except for the last benchmark : ANN-BRL (Q) obtained a very high score, 4 times larger than the one measured for OPPS-DS.It is even more noteworthy that such a difference is observed on the most difficult benchmark.In terms of time constraints, ANN-BRL (Q) is still very close to OPPS-DS except for the last benchmark, where ANN-BRL (Q) is significantly better than the others above certain offline/online time periods.</p>
<p>Another difference is that even though ANN-BRL (C) is still outperformed by ANN-BRL (Q), Fig. 5 reveals some cases where ANN-BRL (C) outperforms (or is on par with) all other algorithms considered.This occurs because ANN-BRL (C) is faster than ANN-BRL (Q) during the online phase, which allows it to comply with smaller online time bounds.</p>
<p>CONCLUSION AND FUTURE WORK</p>
<p>We developed ANN-BRL, an offline policy-search algorithm for addressing BAMDPs.As shown by our experiments, ANN-BRL obtained state-of-the-art performance on all benchmarks considered in this paper.</p>
<p>In particular, on the most challenging benchmark9 , a score 4 times higher than the one measured for the second best algorithm has been observed.Moreover, ANN-BRL is able to make online decisions faster than most BRL algorithms.</p>
<p>Our idea is to define a parametric policy as an ANN, and train it using backpropagation algorithm.This requires a training set made of observationsaction pairs and in order to generate this dataset, several simulations have been performed on MDPs drawn from prior distribution.In theory, we should label each example with a Bayes optimal action.However, those are too expensive to compute for the whole dataset.Instead, we chose to use optimal actions under full observability hypothesis.Due to the modularity of our approach, a better labelling technique could easily be integrated in ANN-BRL, and may bring stronger empirical results.</p>
<p>Moreover, two types of features have been considered for representing the current history: Q-values and transition counters.The use of Q-values allows to reach state-of-the-art performance on most benchmarks and outperfom all other algorithms on the most difficult one.On the contrary, computing a good policy from transition counters only is a difficult task to achieve, even for Artificial Neural Networks.Nevertheless, we found that the difference between this approach and state-of-the-art algorithms was much less noticeable when prior distribution differs from test distribution, which means that at least in some cases, it is possible to compute efficient policies without relying on online computationally expensive tools such as Q-values.</p>
<p>An important future contribution would be to provide theoretical error bounds in simple problems classes, and to evaluate the performance of ANN-BRL on larger domains that other BRL algorithms might not be able to address.</p>
<p>over MDPs drawn from the prior.The OPPS for Discrete Strategy spaces algorithm (OPPS-DS) (Castronovo et al., 2012;Castronovo et al., 2014) formalises the strategy selection problem for a discrete strategy space of index-based strategies.The E/E strategy spaces tested are the ones introduced in (Castronovo et al., 2015) and are denoted by F 2 , F 3 , F 4 , F 5 , F 6 .β is a parameter used during the strategy selection.</p>
<p>Tested Values:</p>
<p>S ∈ {F 2 , F 3 , F 4 , F 5 , F 6 } 10 , β ∈ {50, 500, 1250, 2500, 5000, 10 4 , 10 5 , 10 6 }.</p>
<p>BAMCP</p>
<p>Bayes-adaptive</p>
<p>Monte Carlo Planning (BAMCP) (Guez et al., 2012) is an evolution of the Upper Confidence Tree algorithm (UCT) (Kocsis and Szepesvári, 2006), where each transition is sampled according to the history of observed transitions.The principle of this algorithm is to adapt the UCT principle for planning in a Bayes-adaptive MDP, also called the belief-augmented MDP, which is an MDP obtained when considering augmented states made of the concatenation of the actual state and the posterior.BAMCP relies on two parameters: (i) K, which defines the number of nodes created at each time-step, and (ii) depth defines the depth of the tree.</p>
<p>Tested Values:</p>
<p>K ∈ {1, 500, 1250, 2500, 5000, 10000, 25000}, depth ∈ {15, 25, 50}.</p>
<p>BFS3</p>
<p>The Bayesian Forward Search Sparse Sampling (BFS3) (Asmuth and Littman, 2011) is a BRL algorithm whose principle is to apply the principle of the FSSS (Forward Search Sparse Sampling, see (Kearns et al., 2002)) algorithm to belief-augmented MDPs.It first samples one model from the posterior, which is then used to sample transitions.The algorithm then relies on lower and upper bounds on the value of each augmented state to prune the search space.K defines the number of nodes to develop at each time-step, C defines the branching factor of the tree, and finally depth controls its maximal depth.</p>
<p>Tested Values:</p>
<p>10 The number of arms k is always equal to the number of strategies in the given set.For your information:
|F 2 | = 12, |F 3 | = 43, |F 4 | = 226, |F 5 | = 1210, |F 6 | = 7407
K ∈ {1, 500, 1250, 2500, 5000, 10000}, C ∈ {2, 5, 10, 15}, depth ∈ {15, 25, 50}.</p>
<p>SBOSS</p>
<p>The Smarter Best of Sampled Set (SBOSS) (Castro and Precup, 2010) is a BRL algorithm which relies on the assumption that the model is sampled from a Dirichlet distribution.Based on this assumption, it derives uncertainty bounds on the value of state action pairs.Following this step, it uses those bounds to decide the number of models to sample from the posterior, and the frequency with which the posterior should be updated in order to reduce the computational cost of Bayesian updates.The sampling technique is then used to build a merged MDP, as in (Asmuth et al., 2009), and to derive the corresponding optimal action with respect to that MDP.The number of sampled models is determined dynamically with a parameter ε, while the re-sampling frequency depends on a parameter δ.</p>
<p>Tested Values:
ε ∈ {1.0, 1e − 1, 1e − 2, 1e − 3, 1e − 4, 1e − 5, 1e − 6},
δ ∈ {9, 7, 5, 3, 1, 1e−1, 1e−2, 1e−3, 1e−4, 1e−5, 1e−6}.</p>
<p>BEB</p>
<p>The Bayesian Exploration Bonus (BEB) (Kolter and Ng, 2009a) is a BRL algorithm that builds, at each time-step t, the expected MDP given the current posterior.Before solving this MDP, it computes a new reward function ρ</p>
<p>SAMME Algorithm</p>
<p>A multi-class classification problem consists to find a rule C (.) which associates a class c ∈ {1, . . ., K} to any vector v ∈ R n , n ∈ N. To achieve this task, we are given a set of training samples &lt; v (1) , c (1) &gt;, . . ., &lt; v (N) , c (N) &gt;, from which a classification rule has to be inferred.</p>
<p>SAMME is a boosting algorithm whose goal is to build iteratively a set of weak classifiers C (1) (.), . . ., C (M) : R n → R K , and combine them linearly in order to build a strong classifier C (.).In our case, the weak classifiers are Multilayer Perceptrons (MLPs).m ← m + 1 until err (m) ≥ n U −1 n U {Stop if C (m) is random} C (.) ← { &lt; C (1) , α (1) &gt;, . . ., &lt; C (m) , α (m) &gt; }</p>
<p>Figure 1: ANN-BRL -Offline phase.ommended action &gt; pair is a sample of the training dataset.</p>
<p>Figure 2 :
2
Figure 2: ANN-BRL -Online phase.</p>
<p>both offline and online phases (Algorithm 1 and Algorithm 2 respectively) along with UML diagrams (Figure 1 and Figure 2 respectively).</p>
<p>Algorithm 3 :
3
Resampling algorithm.Input: The original training dataset DataSet (size = N), a set of weights w 1 , . . ., w N Output: A new training dataset DataSet (size = p) {Normalise w k such that 0 ≤ wk ≤ 1 and ∑ k wk = 1} for i = 1 to N do wk ← w k ∑ k w k end for {Resample DataSet} for i = 1 to p do DataSet (i) ← Draw a sample s from DataSet {P(s = DataSet(k)) is equal to wk , ∀k)} end fortuning of the sample weights.Our MLPs do not recommend specific actions, but rather give a confidence score to each one.As a consequence, different actions can receive the same level of confidence by our MLP(s), in which case the agent will break the tie by selecting one of those actions randomly.Therefore, we define the classification error δ as the probability for an agent following a weak classifier C (.) (= an MLP) to select the class c associated to a sample v (&lt; v, c &gt; being an &lt; history, recommended action &gt; pair):</p>
<ol>
<li>Train π offline on p 0 M 2. Sample N MDPs from the test distribution p M 4 3.For each sampled MDP M, compute estimate of J return of agent π trained offline on p 0</li>
</ol>
<p>(a) The GC distribution.(b) The GDL distribution.(c) The Grid distribution.</p>
<p>Figure 4 :
4
Figure 4: Best algorithms w.r.t offline/online periods (accurate case).</p>
<p>Figure 5 :
5
Figure 5: Best algorithms w.r.t offline/online time (inaccurate case).</p>
<p>u, y) = ρ M (x, u, y) + β c <x,u,y> denotes the number of times transition &lt; x, u, y &gt; has been observed at time-step t.This algorithm solves the mean MDP of the current posterior, in which we replaced ρ M (•, •, •) by ρ (t) BEB (•, •, •), and applies its optimal policy on the current MDP for one step.The bonus β is a parameter controlling the E/E balance.Tested Values: β ∈ {0.25, 0.5, 1, 1.5, 2, 2.5, 3, 4, 8, 16}.6.1.9ANN-BRL The Artificial Neural Network for Bayesian Reinforcement Learning algorithm (ANN-BRL) is fully described in Section 4. It samples n MDPs from prior distribution, and generates 1 trajectory for each MDP drawn.The transitions are then used to build training data (one SL sample per transition), and several ANNs are trained on this dataset by SAMME and backpropagation 11 .The training is parametrised by n h , the number of neurons on the hidden layer of the ANN 12 , p, the number of samples resampled from the original training set at each epoch, ε, the learning rate used during the training, r, the maximal number of epoch steps during which the error on VS can increase before stopping the backpropagation training, and M, the maximal number of ANNs built by SAMME.When interacting with an MDP, the BRL agent uses the ANN trained during the offline phase to determine which action to perform.</p>
<p>Fixed Parameters: n = 750, p = 5T 13 , ε = 1e−3, r = 1000.Tested Values: n h ∈ {10, 30, 50}, M ∈ {1, 50, 100}, ϕ = {[ Q-values not related to x t ], [Transition counters, current state ]}.</p>
<p>) C (m) (h), where α (1) . . .α(M) are chosen to minimise the classification error.Given a set of training samples &lt; v (1) , c (1) &gt; , . . ., &lt; v (N) , c (N) &gt;, we associate a weight w i to each sample.Let err(C (.)) be the weighted classification error of a classifier C (.) : error of C (.) for&lt; v (i) , c (i) &gt;.At each iteration m, a weak classifier is trained to minimise the weighted classification error.C (m) (.) = arg min C (.)err(C (.))err(m) = err(C (.))If this classifier behaves better than a random classifier (err (m) &lt; (n U − 1)/n U ), we compute its coefficient α (m) , update the weights of the samples, and build another classifier.Otherwise, we quit.</p>
<p>α (m) = log 1 − err (m) err (m) + log (n U − 1)w i = w i exp (α (m) δ C i )In other words, each new classifier will focus on training samples misclassified by the previous classifiers.Algorithm 4 presents the pseudo-code description for SAMME.</p>
<p>Algorithm 4: SAMME.Input: A training dataset DataSet Output: A classifier C (.) {Initialise the weight of each sample} N ← |DataSet| w (1) i ← 1 N , ∀i ∈ {1, . . ., N} {Train weak classifiers} m ← 1 repeat {Train a weak classifier} C (m) ← Train a classifier on DataSet w.r.t.w (m){Compute its weighted error and its coefficient} err (m) ← 1 m) δ C i ), ∀i Normalise the weights w(m+1)</p>
<p>Table 1 :
1
Best algorithms w.r.t Performance (accurate case).
Agent Random e-Greedy Soft-Max OPPS-DS BAMCP BFS3 SBOSS BEB ANN-BRL (Q) 42.01 ± 1.80 Score on GC Score on GDL Score on Grid 31.12 ± 0.90 2.79 ± 0.07 0.22 ± 0.06 40.62 ± 1.55 3.05 ± 0.07 6.90 ± 0.31 34.73 ± 1.74 2.79 ± 0.10 0.00 ± 0.00 42.47 ± 1.91 3.10 ± 0.07 7.03 ± 0.30 35.56 ± 1.27 3.11 ± 0.07 6.43 ± 0.30 39.84 ± 1.74 2.90 ± 0.07 3.46 ± 0.23 35.90 ± 1.89 2.81 ± 0.10 4.50 ± 0.33 41.72 ± 1.63 3.09 ± 0.07 6.76 ± 0.30 3.11 ± 0.08 6.15 ± 0.31 ANN-BRL (C) 35.95 ± 1.90 2.81 ± 0.09 4.09 ± 0.31</p>
<p>Table 2 :
2
Best algorithms w.r.t Performance (inaccurate case).
Agent Random e-Greedy Soft-Max OPPS-DS BAMCP BFS3 SBOSS BEB ANN-BRL (Q) 38.76 ± 1.71 Score on GC Score on GDL Score on Grid 31.67 ± 1.05 2.76 ± 0.08 0.23 ± 0.06 37.69 ± 1.75 2.88 ± 0.07 0.63 ± 0.09 34.75 ± 1.64 2.76 ± 0.10 0.00 ± 0.00 39.29 ± 1.71 2.99 ± 0.08 1.09 ± 0.17 33.87 ± 1.26 2.85 ± 0.07 0.51 ± 0.09 36.87 ± 1.82 2.85 ± 0.07 0.42 ± 0.09 38.77 ± 1.89 2.86 ± 0.07 0.29 ± 0.07 38.34 ± 1.62 2.88 ± 0.07 0.29 ± 0.05 2.92 ± 0.07 4.29 ± 0.22 ANN-BRL (C) 36.30 ± 1.82 2.84 ± 0.08 0.91 ± 0.15
Approximate Bayes Optimal Policy Search using Neural Networks
By optimal we mean the agent knows the transition matrix of the MDP, and solve it in advance.ICAART 2017 -9th International Conference on Agents and Artificial Intelligence
In order to avoid overfitting, the dataset is divided into two sets: a learning set (LS) and a validation set (VS). The training is terminated once it begins to be less efficient on VS. The samples are distributed 2/3 for LS and 1/3 for VS.
In practice, we can only sample a finite number of trajectories, and must rely on estimators to compare algorithms.
The same MDPs are used for comparing the agents. This choice has been made to reduce drastically the variance of the mean score. ICAART 2017 -9th International Conference on Agents and Artificial Intelligence
Refers to ANN-BRL using Q-values as its features.Approximate Bayes Optimal Policy Search using Neural Networks
Refers to ANN-BRL using transition counters as its features.
  9  Grid benchmark with a uniform prior.
ICAART 2017 -9th International Conference on Agents and Artificial Intelligence
2/3 for the learning set (LS) and 1/3 for the validation set (VS).
In this paper, we only consider 3-layers ANNs in order to build weak classifiers for SAMME.
The number of samples in LS is equal to n × T = 500T . We resample 1% of LS at each epoch, which equals to 5T .
ACKNOWLEDGEMENTSMichaël Castronovo acknowledges the financial support of the FRIA.APPENDIX 6.1 BRL AlgorithmsEach algorithm considered in our experiments is detailed precisely.For each algorithm, a list of "reasonable" values is provided to test each of their parameters.When an algorithm has more than one parameter, all possible parameter combinations are tested.RandomAt each time-step t, the action u t is drawn uniformly from U.ε-GreedyThe ε-Greedy agent maintains an approximation of the current MDP and computes, at each time-step, its associated Q-function.The selected action is either selected randomly (with a probability of ε (1 ≥ ε ≥ 0), or greedily (with a probability of 1 − ε) with respect to the approximated model.Tested Values:ε ∈ {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.Soft-maxThe Soft-max agent maintains an approximation of the current MDP and computes, at each time-step, its associated Q-function.The selected action is selected randomly, where the probability to draw an action u is proportional to Q(x t , u).The temperature parameter τ allows to control the impact of the Q-function on these probabilities (τ → 0 + : greedy selection; τ → +∞: random selection).Tested Values:τ ∈ {0.05, 0.10, 0.20, 0.33, 0.50, 1.0, 2.0, 3.0, 5.0, 25.0}.OPPSGiven a prior distribution p 0 M (.) and an E/E strategy space S, the Offline, Prior-based Policy Search algorithm (OPPS) identify a strategy π * ∈ S which maximises the expected discounted sum of returns
A Bayesian sampling approach to exploration in Reinforcement Learning. J Asmuth, L Li, M Littman, A Nouri, D Wingate, Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI). the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)AUAI Press2009</p>
<p>Approaching Bayesoptimalilty using Monte-Carlo tree search. J Asmuth, M Littman, Proceedings of the 21st International Conference on Automated Planning and Scheduling. the 21st International Conference on Automated Planning and Scheduling2011</p>
<p>Smarter sampling in model-based bayesian reinforcement learning. P S Castro, D Precup, Machine Learning and Knowledge Discovery in Databases. Springer2010</p>
<p>M Castronovo, D Ernst, A Couetoux, R Fonteneau, Benchmarking for Bayesian Reinforcement Learning. 2015Submitted</p>
<p>M Castronovo, R Fonteneau, D Ernst, Bayes Adaptive Reinforcement Learning versus Offline Prior-based Policy Search: an Empirical Comparison. 23rd annual machine learning conference of Belgium and the Netherlands (BENELEARN 2014). 2014</p>
<p>Learning exploration/exploitation strategies for single trajectory Reinforcement Learning. M Castronovo, F Maes, R Fonteneau, D Ernst, Journal of Machine Learning Research. 2012</p>
<p>Bayesian Q-learning. R Dearden, N Friedman, S Russell, Proceedings of Fifteenth National Conference on Artificial Intelligence (AAAI). Fifteenth National Conference on Artificial Intelligence (AAAI)AAAI Press1998</p>
<p>Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. M O Duff, 2002University of Massachusetts AmherstPhD thesis</p>
<p>Optimistic planning for belief-augmented markov decision processes. R Fonteneau, L Busoniu, R Munos, Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), 2013 IEEE Symposium on. IEEE2013</p>
<p>Efficient Bayesadaptive Reinforcement Learning using sample-based search. A Guez, D Silver, P Dayan, Neural Information Processing Systems (NIPS). 2012</p>
<p>Scalable and efficient bayes-adaptive reinforcement learning based on monte-carlo tree search. A Guez, D Silver, P Dayan, Journal of Artificial Intelligence Research. 2013</p>
<p>Planning and acting in partially observable stochastic domains. L Kaelbling, M Littman, Cassandra , A , Artificial Intelligence. 101121998</p>
<p>A sparse sampling algorithm for near-optimal planning in large Markov decision processes. M Kearns, Y Mansour, A Y Ng, Machine Learning. 200249</p>
<p>Bandit based Monte-Carlo planning. L Kocsis, C Szepesvári, European Conference on Machine Learning (ECML). 2006</p>
<p>Near-Bayesian exploration in polynomial time. J Z Kolter, A Y Ng, Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine Learning2009a</p>
<p>Near-bayesian exploration in polynomial time. J Z Kolter, A Y Ng, Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine LearningACM2009b</p>
<p>Bayesian decision problems and markov chains. J J Martin, 1967. 1965Massachusetts Institute of TechnologyOriginally submitted as a Ph.D. thesis</p>
<p>Boosting Neural Networks. H Schwenk, Y Bengio, Neural Comp. 1282000</p>
<p>Markovian decision processes with uncertain transition probabilities or rewards. E A Silver, 1963DTIC DocumentTechnical report</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 1998MIT press Cambridge1</p>
<p>Integrating sample-based planning and model-based reinforcement learning. T J Walsh, S Goschin, M L Littman, AAAI. 2010</p>
<p>Y Wang, K S Won, D Hsu, W S Lee, arXiv:1206.6449Monte carlo bayesian reinforcement learning. 2012arXiv preprint</p>
<p>Learning deep control policies for autonomous aerial vehicles with mpc-guided policy search. T Zhang, G Kahn, S Levine, P Abbeel, CoRR, abs/1509.067912015</p>
<p>Multiclass adaboost. J Zhu, H Zou, S Rosset, T Hastie, Statistics and its Interface. 232009</p>            </div>
        </div>

    </div>
</body>
</html>
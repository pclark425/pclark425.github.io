<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1793 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1793</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1793</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-1df731e254be9c0122a1055629a2df561b4ad5c0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1df731e254be9c0122a1055629a2df561b4ad5c0" target="_blank">ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</a></p>
                <p><strong>Paper Venue:</strong> European Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> This paper formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action.</p>
                <p><strong>Paper Abstract:</strong> Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1\% in average success rate. Project page: https://guanxinglu.github.io/ManiGaussian/.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1793.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1793.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ManiGaussian</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task, language-conditioned robotic manipulation agent that learns scene-level spatiotemporal dynamics by combining dynamic 3D Gaussian splatting with a Gaussian world model that reconstructs future scenes to supervise action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>ManiGaussian</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A manipulation agent that forms a volumetric Gaussian embedding of the scene (dynamic Gaussian splatting), predicts how Gaussian primitives deform under actions (Gaussian world model / deformation predictor), and decodes actions with a PerceiverIO policy conditioned on language and the learned representation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>RLBench</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>RLBench simulated benchmark: a physics-enabled simulated tabletop manipulation suite providing RGB-D observations, scripted expert demonstrations, and diverse task/scene variations (object colors, sizes, placements, counts). Used here with multi-view supervision (20 views) for training and single front-camera online observation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>standard task-level physics simulation (RLBench) with RGB-D rendering and approximate rigid-body dynamics; not a claimed high-fidelity real-world physics simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body object geometry, object poses and interactions under the simulator's physics model, RGB and depth rendering for multi-view novel-view supervision, gripper and end-effector poses (keyframes).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>The paper does not report modeling of real-world sensor noise, actuator delays, controller dynamics, friction/model parameter mismatch calibration to real robots, or explicit sim-level randomized perturbations beyond SE(3) augmentation; dynamics are limited to the RLBench simulator's approximations of contact and rigid-body physics.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Imitation learning / behavior cloning from expert demonstrations (offline dataset of keyframe actions) augmented with a learned Gaussian world model trained to reconstruct current and future views (multi-loss objective including action cross-entropy, geometry/semantic/future consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Average success rate 44.8% on 10 RLBench tasks (evaluated in simulation; per-task success rates reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>SE(3) augmentation on expert demonstrations; task variations sampled in RLBench (randomized object colors, sizes, counts, placements) used during dataset formation.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Authors explicitly note a limitation requiring multiple-view supervision with camera calibration for Gaussian Splatting, which can hinder deployment to real robots; implicit gap factors include differences between simulator physics/rendering and real-world sensor noise, calibration errors, and unmodeled actuator/controller dynamics (these are not explicitly modeled or quantified in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Techniques that would support generalization: explicit 3D reconstruction via Gaussian splatting, learning scene dynamics with a Gaussian world model (future-scene reconstruction loss), distillation of semantic features from foundation vision models, SE(3) augmentation, and multi-view supervision — all claimed to improve robustness/generalization in simulation and therefore are candidates to enable transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>The paper identifies the need for multiple-view supervision with camera calibration as a practical requirement/limitation for the Gaussian Splatting approach to be deployed (i.e., accurate multi-view geometry/calibration is important). No quantitative tolerance thresholds on physics fidelity are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ManiGaussian significantly improves simulated multi-task manipulation performance by explicitly modeling scene-level spatiotemporal dynamics via dynamic Gaussian splatting and a Gaussian world model (average success 44.8% vs. baselines). The paper does not perform sim-to-real transfer experiments; it highlights a practical limitation for real deployment: dependence on multi-view supervision and camera calibration, implying a sim-to-real barrier if those conditions are not met.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1793.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1793.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNFactor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNFactor: Multi-task real robot learning with generalizable neural feature fields</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior generative method that optimizes a generalizable NeRF-like representation (neural feature field) with reconstruction loss plus behavior cloning to improve multi-task robotic manipulation; cited as having shown improvements in both simulation and on real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gnfactor: Multi-task real robot learning with generalizable neural feature fields</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>GNFactor (as reported in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A generative 3D scene representation approach using generalizable neural radiance/feature fields to produce representations for multi-task manipulation policies trained with reconstruction and behavior cloning losses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Mentioned as evaluated in simulation and on real robots in its own work; ManiGaussian compares against GNFactor in RLBench simulation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Referenced claim (from GNFactor) that GNFactor showed effective improvement in both simulated and real scenarios, but ManiGaussian does not reproduce or detail those real-world experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Referenced as using reconstruction loss plus behavior cloning (i.e., combining generative representation learning with imitation learning).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>According to the mention, combining generalizable NeRF-like reconstruction with behavior cloning improved performance and enabled real-robot applicability in GNFactor's work.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior art that demonstrated that generative 3D scene representations (GNFactor) can improve multi-task manipulation and have been applied to both simulated and real robots; ManiGaussian is positioned as improving over GNFactor in simulation by adding explicit dynamics modeling (dynamic Gaussian splatting and a deformation predictor).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1793.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1793.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RLBench: The robot learning benchmark & learning environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used simulated benchmark for robotic manipulation tasks that provides diverse language-conditioned tasks, RGB-D observations, and scripted expert demonstrations for benchmarking learning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RLBench: The robot learning benchmark & learning environment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>RLBench simulated environment (used to evaluate ManiGaussian)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A simulator/benchmark providing a suite of tabletop manipulation tasks with configurable task variations, multi-view rendering, and recorded expert demonstrations used for imitation learning and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>RLBench</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulated benchmark that supplies physics-based interactions (approximate rigid-body contact dynamics) and RGB-D rendering across multiple camera viewpoints; tasks include randomized object colors, sizes, placements and counts.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics simulation (task-level fidelity appropriate for benchmarking policies), RGB-D rendering; not described as high-precision real-world-accurate physics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body geometry, scripted contact interactions as provided by the simulator, multi-view RGB and depth observations, and configurable scene/task variations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not model real-world sensor noise and other deployment-specific factors in the ManiGaussian experiments; paper does not report simulation of actuator/controller delays, true contact friction variability, or camera calibration error in the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Environment used for imitation-learning experiments (behavior cloning from expert demonstrations) and multi-view reconstruction supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Task variations in RLBench were sampled (object colors, sizes, counts, placements); SE(3) augmentation applied to demonstrations during training.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Intrinsic limitations of benchmark-level simulations (approximate physics, limited sensor/actuator realism, calibration differences) can contribute to sim-to-real gaps; ManiGaussian explicitly calls out multi-view/camera calibration requirements as a deployment sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>High-quality multi-view observations and calibrated camera poses are important for Gaussian-splatting-based reconstruction approaches to be usable in the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RLBench provides diverse simulated task variations used to train and evaluate ManiGaussian; results and ablations are entirely in simulation and thus the paper does not provide direct evidence of sim-to-real transfer from RLBench-trained ManiGaussian to real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gnfactor: Multi-task real robot learning with generalizable neural feature fields <em>(Rating: 2)</em></li>
                <li>Daydreamer: World models for physical robot learning <em>(Rating: 2)</em></li>
                <li>Physically embodied gaussian splatting: Embedding physical priors into a visual 3d world model for robotics <em>(Rating: 2)</em></li>
                <li>4D gaussian splatting for real-time dynamic scene rendering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1793",
    "paper_id": "paper-1df731e254be9c0122a1055629a2df561b4ad5c0",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "ManiGaussian",
            "name_full": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation",
            "brief_description": "A multi-task, language-conditioned robotic manipulation agent that learns scene-level spatiotemporal dynamics by combining dynamic 3D Gaussian splatting with a Gaussian world model that reconstructs future scenes to supervise action prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "ManiGaussian",
            "agent_system_description": "A manipulation agent that forms a volumetric Gaussian embedding of the scene (dynamic Gaussian splatting), predicts how Gaussian primitives deform under actions (Gaussian world model / deformation predictor), and decodes actions with a PerceiverIO policy conditioned on language and the learned representation.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "RLBench",
            "virtual_environment_description": "RLBench simulated benchmark: a physics-enabled simulated tabletop manipulation suite providing RGB-D observations, scripted expert demonstrations, and diverse task/scene variations (object colors, sizes, placements, counts). Used here with multi-view supervision (20 views) for training and single front-camera online observation.",
            "simulation_fidelity_level": "standard task-level physics simulation (RLBench) with RGB-D rendering and approximate rigid-body dynamics; not a claimed high-fidelity real-world physics simulator.",
            "fidelity_aspects_modeled": "Rigid-body object geometry, object poses and interactions under the simulator's physics model, RGB and depth rendering for multi-view novel-view supervision, gripper and end-effector poses (keyframes).",
            "fidelity_aspects_simplified": "The paper does not report modeling of real-world sensor noise, actuator delays, controller dynamics, friction/model parameter mismatch calibration to real robots, or explicit sim-level randomized perturbations beyond SE(3) augmentation; dynamics are limited to the RLBench simulator's approximations of contact and rigid-body physics.",
            "real_environment_description": null,
            "task_or_skill_transferred": null,
            "training_method": "Imitation learning / behavior cloning from expert demonstrations (offline dataset of keyframe actions) augmented with a learned Gaussian world model trained to reconstruct current and future views (multi-loss objective including action cross-entropy, geometry/semantic/future consistency).",
            "transfer_success_metric": null,
            "transfer_performance_sim": "Average success rate 44.8% on 10 RLBench tasks (evaluated in simulation; per-task success rates reported in Table 1).",
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "SE(3) augmentation on expert demonstrations; task variations sampled in RLBench (randomized object colors, sizes, counts, placements) used during dataset formation.",
            "sim_to_real_gap_factors": "Authors explicitly note a limitation requiring multiple-view supervision with camera calibration for Gaussian Splatting, which can hinder deployment to real robots; implicit gap factors include differences between simulator physics/rendering and real-world sensor noise, calibration errors, and unmodeled actuator/controller dynamics (these are not explicitly modeled or quantified in the paper).",
            "transfer_enabling_conditions": "Techniques that would support generalization: explicit 3D reconstruction via Gaussian splatting, learning scene dynamics with a Gaussian world model (future-scene reconstruction loss), distillation of semantic features from foundation vision models, SE(3) augmentation, and multi-view supervision — all claimed to improve robustness/generalization in simulation and therefore are candidates to enable transfer.",
            "fidelity_requirements_identified": "The paper identifies the need for multiple-view supervision with camera calibration as a practical requirement/limitation for the Gaussian Splatting approach to be deployed (i.e., accurate multi-view geometry/calibration is important). No quantitative tolerance thresholds on physics fidelity are provided.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "ManiGaussian significantly improves simulated multi-task manipulation performance by explicitly modeling scene-level spatiotemporal dynamics via dynamic Gaussian splatting and a Gaussian world model (average success 44.8% vs. baselines). The paper does not perform sim-to-real transfer experiments; it highlights a practical limitation for real deployment: dependence on multi-view supervision and camera calibration, implying a sim-to-real barrier if those conditions are not met.",
            "uuid": "e1793.0",
            "source_info": {
                "paper_title": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GNFactor",
            "name_full": "GNFactor: Multi-task real robot learning with generalizable neural feature fields",
            "brief_description": "A prior generative method that optimizes a generalizable NeRF-like representation (neural feature field) with reconstruction loss plus behavior cloning to improve multi-task robotic manipulation; cited as having shown improvements in both simulation and on real robots.",
            "citation_title": "Gnfactor: Multi-task real robot learning with generalizable neural feature fields",
            "mention_or_use": "mention",
            "agent_system_name": "GNFactor (as reported in related work)",
            "agent_system_description": "A generative 3D scene representation approach using generalizable neural radiance/feature fields to produce representations for multi-task manipulation policies trained with reconstruction and behavior cloning losses.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": null,
            "virtual_environment_description": "Mentioned as evaluated in simulation and on real robots in its own work; ManiGaussian compares against GNFactor in RLBench simulation experiments.",
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": "Referenced claim (from GNFactor) that GNFactor showed effective improvement in both simulated and real scenarios, but ManiGaussian does not reproduce or detail those real-world experiments.",
            "task_or_skill_transferred": null,
            "training_method": "Referenced as using reconstruction loss plus behavior cloning (i.e., combining generative representation learning with imitation learning).",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": "According to the mention, combining generalizable NeRF-like reconstruction with behavior cloning improved performance and enabled real-robot applicability in GNFactor's work.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Cited as prior art that demonstrated that generative 3D scene representations (GNFactor) can improve multi-task manipulation and have been applied to both simulated and real robots; ManiGaussian is positioned as improving over GNFactor in simulation by adding explicit dynamics modeling (dynamic Gaussian splatting and a deformation predictor).",
            "uuid": "e1793.1",
            "source_info": {
                "paper_title": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "RLBench",
            "name_full": "RLBench: The robot learning benchmark & learning environment",
            "brief_description": "A widely used simulated benchmark for robotic manipulation tasks that provides diverse language-conditioned tasks, RGB-D observations, and scripted expert demonstrations for benchmarking learning algorithms.",
            "citation_title": "RLBench: The robot learning benchmark & learning environment",
            "mention_or_use": "use",
            "agent_system_name": "RLBench simulated environment (used to evaluate ManiGaussian)",
            "agent_system_description": "A simulator/benchmark providing a suite of tabletop manipulation tasks with configurable task variations, multi-view rendering, and recorded expert demonstrations used for imitation learning and evaluation.",
            "domain": "general robotics manipulation (simulation)",
            "virtual_environment_name": "RLBench",
            "virtual_environment_description": "Simulated benchmark that supplies physics-based interactions (approximate rigid-body contact dynamics) and RGB-D rendering across multiple camera viewpoints; tasks include randomized object colors, sizes, placements and counts.",
            "simulation_fidelity_level": "approximate physics simulation (task-level fidelity appropriate for benchmarking policies), RGB-D rendering; not described as high-precision real-world-accurate physics.",
            "fidelity_aspects_modeled": "Rigid-body geometry, scripted contact interactions as provided by the simulator, multi-view RGB and depth observations, and configurable scene/task variations.",
            "fidelity_aspects_simplified": "Does not model real-world sensor noise and other deployment-specific factors in the ManiGaussian experiments; paper does not report simulation of actuator/controller delays, true contact friction variability, or camera calibration error in the real world.",
            "real_environment_description": null,
            "task_or_skill_transferred": null,
            "training_method": "Environment used for imitation-learning experiments (behavior cloning from expert demonstrations) and multi-view reconstruction supervision.",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "Task variations in RLBench were sampled (object colors, sizes, counts, placements); SE(3) augmentation applied to demonstrations during training.",
            "sim_to_real_gap_factors": "Intrinsic limitations of benchmark-level simulations (approximate physics, limited sensor/actuator realism, calibration differences) can contribute to sim-to-real gaps; ManiGaussian explicitly calls out multi-view/camera calibration requirements as a deployment sensitivity.",
            "transfer_enabling_conditions": "High-quality multi-view observations and calibrated camera poses are important for Gaussian-splatting-based reconstruction approaches to be usable in the real world.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "RLBench provides diverse simulated task variations used to train and evaluate ManiGaussian; results and ablations are entirely in simulation and thus the paper does not provide direct evidence of sim-to-real transfer from RLBench-trained ManiGaussian to real robots.",
            "uuid": "e1793.2",
            "source_info": {
                "paper_title": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gnfactor: Multi-task real robot learning with generalizable neural feature fields",
            "rating": 2
        },
        {
            "paper_title": "Daydreamer: World models for physical robot learning",
            "rating": 2
        },
        {
            "paper_title": "Physically embodied gaussian splatting: Embedding physical priors into a visual 3d world model for robotics",
            "rating": 2
        },
        {
            "paper_title": "4D gaussian splatting for real-time dynamic scene rendering",
            "rating": 1
        }
    ],
    "cost": 0.01325925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</h1>
<p>Guanxing Lu ${ }^{1}$ (D) Shiyi Zhang ${ }^{1}$ (D) Ziwei Wang ${ }^{2,3 \boxtimes \star}$ (D), Changliu Liu ${ }^{4}$ (D), Jiwen Lu ${ }^{2}$ (D), and Yansong Tang ${ }^{1}$ (D)<br>${ }^{1}$ Shenzhen Key Laboratory of Ubiquitous Data Enabling, Shenzhen International Graduate School, Tsinghua University<br>${ }^{2}$ Department of Automation, Tsinghua University<br>${ }^{3}$ Nanyang Technological University, ${ }^{4}$ Carnegie Mellon University<br>{lgx23@mails.,sy-zhang23@mails.,lujiwen@,tang.yansong@sz.}tsinghua.edu.cn<br>{ziweiwa2@, cllu6@}andrew.cmu.edu</p>
<h4>Abstract</h4>
<p>Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn a semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by $13.1 \%$ in average success rate ${ }^{1}$.</p>
<p>Keywords: Multi-task robotic manipulation $\cdot$ Dynamic Gaussian Splatting $\cdot$ World model</p>
<h2>1 Introduction</h2>
<p>Designing autonomous agents for language-conditioned manipulation tasks [2, $11,29,31,59,60,62,63,77]$ has been highly desired in the pursuit of artificial intelligence for a long time. In realistic deployment, intelligent robots are usually required to deal with unseen scenarios in novel tasks. Therefore, comprehending complex 3D structures in the deployment scenes is necessary for the robots to achieve high task success rates across diverse manipulation tasks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Consider the human instruction "stack two rose blocks", where the task is considered successful if two rose blocks are stacked upon the green block. The previous method (GNFactor [76]) attempts to pick up the fixed green base but fails severely due to the misunderstanding of the scene dynamics, while our ManiGaussian completes the task successfully by explicitly encoding the scene dynamics via future scene reconstruction in Gaussian embedding space.</p>
<p>To address the challenges, previous arts have made great progress in general manipulation policy learning, which can be divided into two categories including perceptive methods and generative methods. For the first regard, semantic features extracted by perceptive models are directly leveraged to predict the robot actions according to the visual input such as image [14, 15, 40], point cloud $[7,13,79]$ and voxel $[28,60]$. However, the perceptive methods heavily rely on multi-view or gripper-mounted cameras to cover the whole workbench to deal with the occlusion problem within unstructured environments, which restricts their deployment. To this end, generative methods $[22,35,36,47,48,52,75,76]$ capture the 3D scene structure information by reconstructing the scene and objects in arbitrary novel views with self-supervised learning. Nevertheless, they ignore the spatiotemporal dynamics that depict the physical interaction among objects during manipulation, and the predicted actions still fail to complete human goals without correct object interactions. Figure 1 shows a comparison of manipulation achieved by the conventional generative manipulation method (top) and the proposed method (bottom), where the conventional method fails to stack the two rose blocks due to the poor comprehension of scene dynamics.</p>
<p>In this paper, we propose a ManiGaussian method that leverages a dynamic Gassuain Splatting framework for multi-task robotic manipulation. Different from conventional methods which only focus on semantic representation, our method mines the scene-level spatiotemporal dynamics via future scene reconstruction. Therefore, the interaction among objects can be comprehended for accurate manipulation action prediction. More specifically, we first formulate the dynamic Gaussian Splatting framework that models the propagation of diverse semantic features in the Gaussian embedding space, and the semantic features with scene dynamics are leveraged to predict the optimal robot actions</p>
<p>for general manipulation tasks. We build a Gaussian world model to parameterize the distributions in our dynamic Gaussian Splatting framework. Therefore, our framework can acquire informative supervision in interactive environments by reconstructing the future scene according to the current scene and the robot actions, where we constrain consistency between reconstructed and realistic future scenes for dynamics mining. We evaluate our ManiGaussian method on the RLBench dataset [27] with 10 tasks and 166 variants, where our method outperforms the state-of-the-art multi-task robotic manipulation methods by $13.1 \%$ in the average task success rate. Our contributions can be summarized as follows:</p>
<ul>
<li>We propose a dynamic Gaussian Splatting framework to learn the scenelevel spatiotemporal dynamics in general robotic manipulation tasks, so that the robotic agent can complete human instructions with accurate action prediction in unstructured environments.</li>
<li>We build a Gaussian world model to parameterize distributions in our dynamic Gaussian Splatting framework, which can provide informative supervision to learn scene dynamics from the interactive environment.</li>
<li>We conduct extensive experiments of 10 tasks on RLBench, and the results demonstrate that our method achieves a higher success rate than the state-of-the-art methods with less computation.</li>
</ul>
<h1>2 Related Work</h1>
<p>Visual Representations for Robotic Manipulation. Developing intelligent agents for language-conditioned manipulation tasks in complex and unstructured environments has been a longstanding objective. One of the key bottlenecks in achieving this goal is effectively representing visual information of the scene. Prior arts can be categorized into two branches: perceptive methods and generative methods. Perceptive methods directly utilize pretrained 2D [14, 15, 40, 79] or 3D visual representation backbone [7, 13, 28, 60] to learn scene embedding, where optimal robot actions are predicted based on the scene semantics. For example, InstructRL [40] and Hiveformer [15] directly passed 2D visual tokens through a multi-modal transformer to decode gripper actions, but struggled to handle complex manipulation tasks due to the lack of geometric understanding. To incorporate 3D information beyond images, PolarNet [7] and Act3D [13] utilized point cloud representation, where PolarNet used a PointNeXt [49]-based architecture and Act3D designed a ghost point sampling mechanism to decode actions. Moreover, PerAct [60] fed voxel tokens into a PerceiverIO [26]-based transformer policy, demonstrating impressive performance in a variety of manipulation tasks. However, perceptive methods heavily rely on seamless camera overlay for comprehensive 3D understanding, which makes them less effective in unstructured environments. To address this, generative methods [22, 35, 36, 47, 48, 52, 75, 76] have gained attention. which learns the 3D geometry through self-supervised novel view reconstruction. For instance, Li et al. [36] combined NeRF and time contrastive learning to embed 3D geometry and learn fluid dynamics within an</p>
<p>autoencoder framework. GNFactor [76] optimized a generalizable NeRF with a reconstruction loss besides behavior cloning, and showed effective improvement in both simulated and real scenarios. However, conventional generative methods usually ignore the scene-level spatiotemporal dynamics that demonstrate the interaction among objects, and the predicted actions still fail to achieve human goals because of incorrect interaction.</p>
<p>World Models. In recent years, world models have emerged as an effective approach to encode scene dynamics by predicting the future states given the current state and actions, which are explored in autonomous driving 12, 24, 25, 64], game agent [16-20, 54, 73] and robotic manipulation [21, 55, 68]. Early works [16-21, 54, 55, 73] learned a latent space for future prediction by autoencoding, which acquired notable effectiveness in both simulated and real-world situations [68]. However, learning latent for accurate future prediction requires a large amount of data and is limited to simple tasks such as robot control due to the weak representative ability of the implicit features. To address these limitations, explicit representation in the image domain [9, 45, 56, 67] and the language domain $[6,23,38,43,65]$ has been widely studied because of the rich semantics. UniPi [9] reconstructed the future images with a text-conditional video generation model, employing an inverse dynamics model to obtain the intermediate actions. Dynalang [38] learned to predict text representations as future states, and enabled embodied agents to navigate in photorealistic home scans under human instructions. In contrast to these approaches, we generalize the world model to embedding space of dynamic Gaussian Splatting, which predicts the future state for the agent to learn scene-level dynamics from interactive environments.</p>
<p>Gaussian Splatting. Gaussian Splatting [32] models the scenes with a set of 3D Gaussians which are projected to 2D planes with efficient differentiable splatting. Gaussian Splatting achieves higher effectiveness and efficiency compared with implicit representations such as Neural Radiance Fields (NeRF) [8, 30, 36, 39, 46, 58, 76] with fast inference, high fidelity, and strong editability for novel view synthesis. Please refer to [5] for a comprehensive survey on 3D Gaussian Splatting. To deploy Gaussian Splatting in diverse complex scenarios, many variants have been proposed to enhance the generalization ability, enrich the semantic information, and reconstruct deformable scenes. For higher generalization ability across diverse scenes, recent works [4, 10, 61, 70, 78, 80, 83] constructed a direct mapping from pixels to Gaussian parameters from large-scale datasets. To integrate rich semantic information into Gaussian Splatting, many efforts [50, 57, 81, 84] have been demonstrated in distilling Gaussian radiance fields from pretrained foundation models [3, 34, 51, 53]. For instance, LangSplat [50] advanced the Gaussian representation by encoding language features distilled from CLIP [51] using a scene-wise language autoencoder, enabling efficient open-vocabulary localization compared with its NeRF-based counterpart [33]. For deformation modeling, time-variant Gaussian radiance fields $[1,37,44,66,69,71,72]$ were reconstructed from videos instead of images, which are widely applied in applications such as surgical scene reconstruction [42, 82]. Although these approaches have achieved high-quality reconstruction from entire videos like interpolation, extrapolation</p>
<p>to future states conditioned on previous states and actions is unexplored, which holds significance for scene-level dynamics modeling for interactive agents. In this paper, we formulate a dynamic Gaussian Splatting framework to model the scene dynamics of object interactions, which enhances the physical reasoning for agents to complete a wide range of robotic manipulation tasks.</p>
<h1>3 Approach</h1>
<p>In this section, we first briefly introduce preliminaries on the problem formulation (Section 3.1), and then we present an overview of our pipeline (Section 3.2). Subsequently, we introduce a dynamic Gaussian Splatting framework (Section 3.3) that infers the propagation semantics of the manipulation scenarios in the Gaussian embedding space. To enable our dynamic Gaussian Splatting framework to learn scene dynamics from the interactive environment, we build a Gaussian world model (Section 3.4) that reconstructs future scenes according to the propagated semantics.</p>
<h3>3.1 Problem Formulation</h3>
<p>The demand for language-conditioned robotic manipulation is a significant aspect in the development of general intelligent robots. The agent is required to interactively predict the subsequent pose of the robot arm based on the observation and achieve the pose with a low-level motion planner to complete a wide range of manipulation tasks described in humans. The visual input at the $t_{t h}$ step for the agent is defined as $o^{(t)}=\left(\mathbf{C}^{(t)}, \mathbf{D}^{(t)}, \mathbf{P}^{(t)}\right)$, where $\mathbf{C}^{(t)}$ and $\mathbf{D}^{(t)}$ respectively represent the single-view images and the depth images. The proprioception matrix $\mathbf{P}^{(t)} \in \mathbb{R}^{4}$ indicates the gripper state including the end-effector position, openness, and current timestep. Based on the visual input $o^{(t)}$ and the language instructions, the agent is required to generate the optimal action for the robot arm and grippers $\mathbf{a}^{(t)}=\left(\mathbf{a}<em _rot="{rot" _text="\text">{\text {trans }}^{(t)}, \mathbf{a}</em>}}^{(t)}, \mathbf{a<em _col="{col" _text="\text">{\text {open }}^{(t)}, \mathbf{a}</em>}}^{(t)}\right)$, which respectively demonstrates the target translation in voxel $\mathbf{a<em _rot="{rot" _text="\text">{\text {trans }}^{(t)} \in \mathbb{R}^{100^{3}}$, rotation $\mathbf{a}</em>}}^{(t)} \in \mathbb{R}^{(360 / 5) \times 3}$, openness $\mathbf{a<em _col="{col" _text="\text">{\text {open }}^{(t)} \in[0,1]$ and collision avoidance $\mathbf{a}</em> \in[0,1]$.}}^{(t)</p>
<p>To learn the manipulation policy effectively, expert demonstrations as offline datasets are provided for imitation learning, where the sample triplets contain the visual input, language instruction and expert actions. Existing methods leverage powerful visual representations to learn informative latent features for optimal action prediction. However, they ignore the spatiotemporal dynamics which depicts the physical interaction among objects, and the predicted actions usually fail to complete complex human goals without correct object interactions. On the contrary, we present a dynamic Gaussian Splatting framework to mine the scene dynamics for robotic manipulation.</p>
<h3>3.2 Overall Pipeline</h3>
<p>The overall pipeline of our ManiGaussian method is shown in Figure 2, in which we construct a dynamic Gaussian Splatting framework that models the prop-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: The overall pipeline of ManiGaussian, which primarily consists of a dynamic Gaussian Splatting framework and a Gaussian world model. The dynamic Gaussian Splatting framework models the propagation of diverse semantic features in the Gaussian embedding space for manipulation, and the Gaussian world model parameterizes distributions to provide supervision by reconstructing the future scene for scene-level dynamics mining.
agation of diverse semantic features in the Gaussian embedding space for manipulation. We also build a Gaussian world model to parameterize distributions in our dynamic Gaussian Splatting framework, which can provide informative supervision of scene dynamics by future scene reconstruction. More specifically, we transform the visual input from RGB-D cameras to a volumetric representation by lifting and voxelization for data preprocessing. For dynamic Gaussian Splatting, we leverage a Gaussian regressor to infer the Gaussian distribution of geometric and semantic features in the scene based on the representation, propagated along time steps with rich scene-level spatiotemporal dynamics. For the Gaussian world model, we instantiate a deformation field to reconstruct the future scene according to the current scene and the robot actions, and require consistency between reconstructed and realistic scenes for dynamics mining. Therefore, the spatiotemporal dynamics indicating object correlation can be embedded into the representation learned in the dynamic Gaussian Splatting framework. Finally, we employ multi-modal transformer PerceiverIO [26] to predict the optimal robot actions for general manipulation tasks, which considers geometric, semantic, and dynamic information with human instructions.</p>
<h1>3.3 Dynamic Gaussian Splatting for Robotic Manipulation</h1>
<p>In order to capture the scene-level dynamics for general manipulation tasks, we propose a dynamic Gaussian Splatting framework that models the propagation of diverse semantic features within the Gaussian embedding space. While the</p>
<p>vanilla Gaussian Splatting has remarkable effectiveness and efficiency in reconstructing static environments, it fails to capture the scene dynamics for manipulation due to the lack of temporal information. To this end, we formulate a dynamic Gaussian Splatting framework based on the vanilla Gaussian Splatting methodology by enabling the Gaussian points of scene representation to move with robotic manipulation, which demonstrates the physical interactions between objects. The scene representation contains geometric information depicting the explicit visual clues, semantic information illustrating the implicit high-level visual features, and dynamic information encoding the physical properties of the scene, which are utilized to predict the optimal actions.
Dynamic Gaussian Splatting. Gaussian Splatting 32 is a promising approach for multi-view 3D reconstruction, which exhibits fast inference, high fidelity, and strong editability of generated content compared with Neural Radiance Field (NeRF) 46. Gaussian Splatting represents a 3D scene explicitly with multiple Gaussian primitives, where the $i_{t h}$ Gaussian primitive is parameterized by $\theta_{i}=\left(\mu_{i}, c_{i}, r_{i}, s_{i}, \sigma_{i}\right)$, where respectively represent the positions, color, rotation, scale, and opacity for the Gaussian primitive. To render a novel view, we project Gaussian primitives onto the 2D plane by differential tile-based rasterization. The value of the pixel $\mathbf{p}$ can be rendered by the alpha-blend rendering:</p>
<p>$$
C(\mathbf{p})=\sum_{i=1}^{N} \alpha_{i} c_{i} \prod_{j=1}^{i-1}\left(1-\alpha_{j}\right) \quad \text { where, } \alpha_{i}=\sigma_{i} e^{-\frac{1}{2}\left(\mathbf{p}-\mu_{i}\right)^{\top} \Sigma_{i}^{-1}\left(\mathbf{p}-\mu_{i}\right)}
$$</p>
<p>where $C$ is the rendered image, $N$ denotes the number of Gaussians in this tile, $\alpha_{i}$ represents the 2 D density of the Gaussian points in the splatting process, and $\Sigma_{i}$ stands for the covariance matrix acquired from the rotation and scales of the Gaussian parameters. However, the vanilla Gaussian Splatting encounters difficulties in reconstructing changing environments, which limits the ability to model scene-level dynamics that is crucial for manipulation tasks. To address this, we enable the Gaussian particles to be propagated with time to capture the spatiotemporal dynamics of the scene. The parameters of the $i_{t h}$ Gaussian primitive at the $t_{t h}$ step can be expressed as follows:</p>
<p>$$
\theta_{i}^{(t)}=\left(\mu_{i}^{(t)}, c_{i}^{(t)}, r_{i}^{(t)}, s_{i}^{(t)}, \sigma_{i}^{(t)}, f_{i}^{(t)}\right)
$$</p>
<p>The positions, colors, rotations, scales, and opacities with the superscript $t$ represent their counterparts at the $t_{t h}$ step in the propagation, and $f_{i}^{(t)}$ is the high-level semantic feature distilled from the Stable Diffusion 53] visual encoder based on the RGB images of the scene. In robotic manipulation, all objects are regarded as rigid bodies without inherent properties including colors, scales, opacities, and semantic features. $c_{i}^{(t)}, s_{i}^{(t)}, \sigma_{i}^{(t)}$ and $f_{i}^{(t)}$ are therefore regarded as time-independent parameters. The positions and rotation of Gaussian particles change during the manipulation due to the physical interaction between objects and robot grippers, which can be formulated as follows:</p>
<p>$$
\left(\mu_{i}^{(t+1)}, r_{i}^{(t+1)}\right)=\left(\mu_{i}^{(t)}+\Delta \mu_{i}^{(t)}, r_{i}^{(t)}+\Delta r_{i}^{(t)}\right)
$$</p>
<p>where $\Delta \mu_{i}^{(t)}$ and $\Delta r_{i}^{(t)}$ demonstrate the change of positions and rotation from the $t_{t h}$ step to the next one for the $i_{t h}$ Gaussian primitive. With the time-dependent parameters of the Gaussian mixture distribution, the pixel values in 2D views of the scene can still be rendered by (1).
Gaussian World Model. In our implementation, we present a Gaussian world model to parameterize the Gaussian mixture distribution in dynamic Gaussian Splatting, through which the future scene can be reconstructed via parameter propagation. Therefore, the dynamic Gaussian Splatting model can acquire informative supervision in the interactive environment by considering the consistency between the reconstructed and realistic feature scenes. World models are effective to learn the environmental dynamics for downstream tasks by anticipating the future state $s^{(t+1)}$ based on the current state $s^{(t)}$ and action $a^{(t)}$ at the $t_{t h}$ step, which have been applied to a variety of tasks including autonomous driving [12, 24, 25, 64] and game agent [16-20, 54, 73]. For our robotic manipulation tasks, we instantiate the current state in the world model as the visual observation in the current step, and actions refer to those of the robot arm and grippers. They are leveraged to predict the visual scenes observed in the next step that represent the future state. More specifically, the Gaussian world model contains a representation network $q_{\phi}$ that learns high-level visual features with rich semantics for the input observation, a Gaussian regressor $g_{\phi}$ that predicts the Gaussian parameters of different primitives based on the visual features, a deformation predictor $p_{\phi}$ that infers the difference of Gaussian parameters during the propagation, and a Gaussian renderer $\mathcal{R}$ (1) that generates the RGB images for the predicted future state:</p>
<p>$$
\left{\begin{array}{l}
\text { Representation model: } \quad \mathbf{v}^{(t)}=q_{\phi}\left(o^{(t)}\right) \
\text { Gaussian regressor: } \quad \theta^{(t)}=g_{\phi}\left(\mathbf{v}^{(t)}\right) \
\text { Deformation predictor: } \quad \Delta \theta^{(t)}=p_{\phi}\left(\theta^{(t)}, a^{(t)}\right) \
\text { Gaussian renderer: } \quad o^{(t+1)}=\mathcal{R}\left(\theta^{(t+1)}, w\right)
\end{array}\right.
$$</p>
<p>where $o^{(t)}$ and $\mathbf{v}^{(t)}$ mean the visual observation and the corresponding highlevel visual features at the $t_{t h}$ step. $w$ is the camera pose for the view where we project the Gaussian primitives. We leverage multi-head neural networks as the Gaussian regressor, where each head predicts a specific feature for the Gaussian parameters shown in (2). By inferring the changes of positions and rotations between consecutive steps, we acquire the propagated Gaussian parameters in the future step based on (3). Finally, the Gaussian renderer projects the propagated Gaussian distribution in a specific view for future scene reconstruction.</p>
<h1>3.4 Learning Objectives</h1>
<p>Current Scene Consistency Loss. Reconstructing the current scene based on the current Gaussian parameters accurately can enhance the performance of the Gaussian regressor. To achieve this goal, we introduce the consistency objective between the realistic current observation and the rendered according</p>
<p>to the current Gaussian parameters:</p>
<p>$$
\mathcal{L}<em 2="2">{\text {Geo }}=\left|\mathbf{C}^{(t)}-\hat{\mathbf{C}}^{(t)}\right|</em>
$$}^{2</p>
<p>where $\mathbf{C}^{(t)}$ and $\hat{\mathbf{C}}^{(t)}$ respectively mean the groundtruth and prediction of observation images from different views at the $t_{t h}$ step.
Semantic Feature Consistency Loss. The semantic features contain highlevel visual information of the observed scenes. Since the foundation models can extract informative semantic features for general scenes, we expect the semantic features in our Gaussian parameters to mimic those acquired by large pre-trained models such as Stable Diffusion [53], so that the knowledge learned by the pretrained models can be distilled to our Gaussian world model according to the following objective:</p>
<p>$$
\mathcal{L}<em _cos="\cos">{\text {Sem }}=1-\sigma</em>\right)
$$}\left(\mathbf{F}^{(t)}, \hat{\mathbf{F}}^{(t)</p>
<p>where $\mathbf{F}^{(t)}$ and $\hat{\mathbf{F}}^{(t)}$ are projected map of semantic features in Gaussian parameters and the feature map learned by pre-trained models. $\sigma_{\text {cos }}$ means the cosine distance between variables.
Action Prediction Loss. The distribution parameters in our dynamic Gaussian framework are leveraged to predict the optimal action of the robot arm and grippers for general manipulation tasks. We employ a multi-modal transformer PerceiverIO [26] to infer the selection probability of different action candidates based on the Gaussian parameters and the human language instructions, and leverage the cross-entropy loss $C E$ for accurate action prediction:</p>
<p>$$
\mathcal{L}<em _text="\text" _trans="{trans">{\text {Act }}=C E\left(p</em>\right)
$$}}, p_{\text {rot }}, p_{\text {open }}, p_{\text {col }</p>
<p>where $p_{\text {trans }}, p_{\text {rot }}, p_{\text {open }}, p_{\text {col }}$ represent the probability of the groundtruth actions in expert demonstrations for translation, rotation, gripper openness and collision avoidance of the robot, respectively.
Future Scene Consistency Loss. We require consistency between the reconstructed and realistic scenes, so that the dynamic Gaussian Splatting framework can accurately embed scene-level spatiotemporal dynamics in the Gaussian parameters. Specifically, the training objective aligns the predicted future scenes based on different observations and actions with the realistic ones, which can be formulated as follows:</p>
<p>$$
\mathcal{L}<em 2="2">{\text {Dyna }}=\left|\hat{\mathbf{C}}^{(t+1)}\left(a^{(t)}, o^{(t)}\right)-\mathbf{C}^{(t+1)}\right|</em>
$$}^{2</p>
<p>where $\hat{\mathbf{C}}^{(t+1)}\left(a^{(t)}, o^{(t)}\right)$ means the predicted future image of the scene at the $t_{t h}$ step based on the action $a^{(t)}$ and current observation $o^{(t)}$, and $\mathbf{C}^{(t+1)}$ is the realistic counterpart. By imposing the Gaussian world model to predict future scenes based on the representation, the representation is required to encode the physical properties of the scene. This is important for the action decoder to predict effective actions with such representation.</p>
<p>The overall objective for our ManiGaussian agent is written as a weighted combination of different loss terms:</p>
<p>$$
\mathcal{L}=\mathcal{L}<em _Geo="{Geo" _text="\text">{\text {Act }}+\lambda</em>}} \mathcal{L<em _Sem="{Sem" _text="\text">{\text {Geo }}+\lambda</em>}} \mathcal{L<em _Dyna="{Dyna" _text="\text">{\text {Sem }}+\lambda</em>
$$}} \mathcal{L}_{\text {Dyna }</p>
<p>where $\lambda_{\text {Geo }}, \lambda_{\text {Sem }}, \lambda_{\text {Dyna }}$ are the hyperparamters that controls the importance of different terms during training. In training, we set a warm-up phase that freezes the deformation predictor to learn a stable representation model and a Gaussian regressor during the first 3 k iterations. After the warm-up phase, we jointly train the whole Gaussian world model with the action decoder.</p>
<h1>4 Experiments</h1>
<p>In this section, we first introduce the experiment setup including datasets, baseline methods, and implementation details (Section 4.1). Then we compare our method with the state-of-the-art approaches to show the superiority in success rate (Section 4.2), and conduct an ablation study to verify the effectiveness of different components in our dynamic Gaussian Splatting framework and the Gaussian world model (Section 4.3). Finally, we also illustrate the visualization results to depict our intuition (Section 4.4). Further results and case studies can be found in the supplementary material.</p>
<h3>4.1 Experiment Setup</h3>
<p>Simulation. Our experiments are conducted in the popular RLBench [27] simulated tasks. Following [76], we utilize a curated subset of 10 challenging languageconditioned manipulation tasks from RLBench, which includes 166 variations in object properties and scene arrangement. The diversity of these tasks requires the agent to acquire generalizable knowledge about the intrinsical scene-level spatial-temporal dynamics for manipulation, rather than solely relying on mimicking the provided expert demonstrations to achieve high success rates. We evaluated 25 episodes in the testing set for each task to avoid result bias from noise. For visual observation, we employ RGB-D images captured by a single front camera with a resolution of $128 \times 128$. We use the same number of cameras (i.e., 20) as GNFactor to provide multi-view supervision for fair comparisons. During the training phase, we use 20 demonstrations for each task.
Baselines: We compare our ManiGaussian with the previous state of the arts including the perceptive method PerAct [60] and its modified version using 4 camera inputs to cover the workbench, as well as the generative method GNFactor [76]. The evaluation metric is the task success rate, which measures the percentage of completed episodes. An episode is considered successful if the agent completes the goal specified in natural language within a maximum of 25 steps. Implementation Details. We use the $\operatorname{SE}(3)[60,76]$ augmentation for the expert demonstrations in the training set to enhance the generalizability of agents. To mitigate the impact of parameter size, we utilize the same version of PerceiverIO [26] as the action decoder across all baselines. All the compared methods are trained on two NVIDIA RTX 4090 GPUs for 100k iterations with a batch size of 2 . We employ LAMB optimizer [74] with an initial learning rate $5 \times 10^{-4}$. We also adopt a cosine scheduler with warmup in the first 3 k steps.</p>
<p>Table 1: Multi-task Test Results. We evaluate 25 episodes per task for the final checkpoint on 10 challenging tasks from RLBench and report the success rates (\%), where the second results are underlined and the best results are bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method / Task</th>
<th style="text-align: center;">close <br> jar</th>
<th style="text-align: center;">open <br> drawer</th>
<th style="text-align: center;">sweep to <br> dustpan</th>
<th style="text-align: center;">meat off <br> grill</th>
<th style="text-align: center;">turn <br> tap</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PerAct</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">$\underline{54.7}$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">38.7</td>
</tr>
<tr>
<td style="text-align: left;">PerAct (4 cameras)</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$\mathbf{6 5 . 3}$</td>
<td style="text-align: center;">46.7</td>
</tr>
<tr>
<td style="text-align: left;">GNFactor</td>
<td style="text-align: center;">$\underline{25.3}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 0}$</td>
<td style="text-align: center;">$\underline{28.0}$</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">$\underline{50.7}$</td>
</tr>
<tr>
<td style="text-align: left;">ManiGaussian (ours)</td>
<td style="text-align: center;">$\mathbf{2 8 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 0}$</td>
<td style="text-align: center;">$\underline{60.0}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Method / Task</td>
<td style="text-align: center;">slide <br> block</td>
<td style="text-align: center;">put in <br> drawer</td>
<td style="text-align: center;">drag <br> stick</td>
<td style="text-align: center;">push <br> buttons</td>
<td style="text-align: center;">stack <br> blocks</td>
</tr>
<tr>
<td style="text-align: left;">PerAct</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\underline{18.7}$</td>
<td style="text-align: center;">$\underline{6.7}$</td>
</tr>
<tr>
<td style="text-align: left;">PerAct (4 cameras)</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">$\underline{6.7}$</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">5.3</td>
</tr>
<tr>
<td style="text-align: left;">GNFactor</td>
<td style="text-align: center;">$\underline{20.0}$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$\underline{37.3}$</td>
<td style="text-align: center;">$\underline{18.7}$</td>
<td style="text-align: center;">4.0</td>
</tr>
<tr>
<td style="text-align: left;">ManiGaussian (ours)</td>
<td style="text-align: center;">$\mathbf{2 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{1 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{1 2 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of Methods with Different Techniques. Following [15], we manually group the 10 RLBench tasks into 6 categories according to their main challenges to demonstrate the improvement reason. The 6 categories are detailed in the supplementary file.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Geo.</th>
<th style="text-align: center;">Sem.</th>
<th style="text-align: center;">Dyna.</th>
<th style="text-align: center;">Planning</th>
<th style="text-align: center;">Long</th>
<th style="text-align: center;">Tools</th>
<th style="text-align: center;">Motion</th>
<th style="text-align: center;">Screw</th>
<th style="text-align: center;">Occlusion</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">23.6</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\underline{46.0}$</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">$\underline{24.0}$</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">39.2</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\underline{46.0}$</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">$\underline{53.3}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{2 8 . 0}$</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">41.6</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{5 4 . 0}$</td>
<td style="text-align: center;">$\underline{10.0}$</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">$\mathbf{6 4 . 0}$</td>
<td style="text-align: center;">$\underline{24.0}$</td>
<td style="text-align: center;">$\underline{72.0}$</td>
<td style="text-align: center;">$\underline{43.6}$</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">$\mathbf{1 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 0}$</td>
<td style="text-align: center;">$\underline{56.0}$</td>
<td style="text-align: center;">$\mathbf{2 8 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 8}$</td>
</tr>
</tbody>
</table>
<h1>4.2 Comparison with the State-of-the-Art Methods</h1>
<p>In this section, we compare our ManiGaussian with previous state-of-the-art methods on the RLBench tasksuite. Table 1 illustrates the comparison of the average success rate of each task. Our method achieves the best performance with an average success rate of $44.8 \%$, which is state-of-the-art, outperforming the previous arts including both perceptive and generative-based methods by a sizable margin. The dominated generative-based method GNFactor leveraged a generalizable NeRF to learn informative latent representation for optimal action prediction, which showed effective improvement beyond the perceptive-based method PerAct. However, it ignores the scene-level spatiotemporal dynamics that demonstrate the interaction among objects, and the predicted actions still fail to achieve human goals because of the incorrect interaction. On the contrary, our ManiGaussian learns the scene dynamics with the proposed dynamic Gaussian Splatting framework, so that the robotic agent can complete human instructions with accurate action prediction in unstructured environments. As a result, our method outperforms the second-best GNFactor method by a relative improvement of $41.3 \%$. In the task meat off grill where the best performance was not reached, our method also ranks as second best. The experimental results</p>
<p>illustrate the effectiveness of our proposed method across multiple languageconditioned robotic manipulation tasks.</p>
<h1>4.3 Ablation Study</h1>
<p>Our dynamic Gaussian Splatting framework models the propagation of diverse features in the Gaussian embedding space, and the Gaussian world model reconstructs the future scene according to the current scene by constraining the consistency between reconstructed and realistic scenes for dynamics mining. We conduct an ablation study to verify the effectiveness of each presented component in Table 2. We first implement a vanilla baseline without any proposed technique, where we directly train the representation model and the action decoder to predict the robot actions. By adding the Gaussian regressor to predict the Gaussian parameters, the performance improves by $15.6 \%$ compared with the baseline. Especially, in the tasks that require geometric reasoning such as Occlusion, Tools and Screw, it outperforms the vanilla version by sizable margins, which proves the ability of the Gaussian Splatting technology to model the spatial information for manipulation tasks. We then add semantic features distilled from the pretrained foundation model into the dynamic Gaussian Splatting framework. By adding the semantic features and the related consistency loss, we observe that the average success rate increases by $2.4 \%$ than the only geometric features version, which indicates the benefits of the high-level semantic information for robotic manipulation. Besides, we implement the deformation predictor and the corresponding future scene consistency loss, resulting in a dramatic performance improvement of $4.4 \%$. Particularly, the proposed deformation predictor improves the task completion of 4 out of 6 task types, which demonstrates the importance of the scenelevel dynamics encoded by the deformation predictor in the Gaussian world model, especially in long-horizon tasks (Long). Though the dynamic loss may slightly impact short-term results due to the balance of different loss items, it significantly improves overall performance. After combining all the techniques in our dynamic Gaussian Splatting framework, the performance increases from $23.6 \%$ to $44.8 \%$, which verifies the necessity of the scene-level spatiotemporal dynamics mined by the proposed dynamics Gaussian Splatting framework with the Gaussian world model.</p>
<p>Figure 3 depicts the learning curve of the proposed ManiGaussian and the state-of-the-art method GNFactor, where we save checkpoints and test them</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Case Study. The red mark signifies the pose deviates severely from the expert demonstration, whereas the green mark indicates that the pose aligns with the expert trajectory. Our ManiGaussian can successfully complete the human goal with the physical understanding of scene-level spatial-temporal dynamics.
every 10 k parameter updates. Both the compared methods get convergence within 100k training steps. As shown in Figure 3, our ManiGaussian outperforms the state-of-the-art method GNFactor, achieving $1.18 \times$ better performance and $2.29 \times$ faster training. This result proves that our ManiGaussian not only performs better but also trains faster, which also shows the efficiency of the explicit Gaussian scene reconstruction than the implicit approach like NeRF.</p>
<h1>4.4 Qualitative Analysis</h1>
<p>Visualization of Whole Trajectories. We present two qualitative examples of the generated action sequence in Figure 4 from GNFactor and our ManiGaussian. In the top case, the agent is instructed to "Slide the block to yellow target". The results show that the previous agent struggles to complete the task since it imitates the expert's backward pulling motion, even though the claw is already leaning towards the right side of the red block. In contrast, ManiGaussian returns to the red square and successfully slides the square to the yellow target, owing to that our method can correctly understand the scene dynamics of objects in contact. In the bottom case, the agent is instructed to "Turn left tap". The results</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5: Novel View Synthesis Results. We remove the action loss here for better visualization. Our ManiGaussian is capable of both current scene reconstruction and future scene prediction.
show that GNFactor misunderstands the meaning of "left", and instead operates the right tap, and also fails to turn on the tap. In contrast, our ManiGaussian successfully completes the task, which shows that ManiGaussian can not only understand the semantic information, but also execute operations accurately.
Visualization of Novel View Synthesis. Figure 5 shows the novel view image synthesis results. First, based on the front view observation where the gripper shape cannot be seen, our ManiGaussian offers superior detail in modeling cubes in novel views. Second, our method accurately predicts future states based on the recovered details. For example, in the top case of the slide block task, our ManiGaussian not only predicts the future gripper position that corresponds to the human instruction, but also predicts the future cube location influenced by the gripper based on the understanding of the physical interaction among objects. This qualitative result demonstrates that our ManiGaussian learns the intricate scene-level dynamics successfully.</p>
<h1>5 Conclusion</h1>
<p>In this paper, we have presented a ManiGaussian agent that encodes the scenelevel spatiotemporal dynamics for language-conditioned manipulation agents. We design a dynamic Gaussian Splatting framework that models the propagation of features in the Gaussian embedding space, and the latent representation with scene dynamics is leveraged to predict the robot actions. Subsequently, we build a Gaussian world model to parameterize the distributions in the dynamic Gaussian Splatting framework to mine scene-level dynamics by reconstructing the future scene. Experiments in diverse manipulation tasks demonstrate the superiority of ManiGaussian. The limitations stem from the necessity of multiple view supervision with camera calibration for the Gaussian Splatting framework.</p>
<h1>Acknowledgements</h1>
<p>This work was supported in part by the National Key Research and Development Program of China under Grant 2022ZD0114903 and Shenzhen Ubiquitous Data Enabling Key Lab under grant ZDSYS20220527171406015. We would like to greatly thank Yanjie Ze for his kind response to the GNFactor issues.</p>
<h2>References</h2>
<ol>
<li>Abou-Chakra, J., Rana, K., Dayoub, F., Sünderhauf, N.: Physically embodied gaussian splatting: Embedding physical priors into a visual 3d world model for robotics. In: CoRL (2023)</li>
<li>Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al.: Rt-1: Robotics transformer for real-world control at scale. arXiv (2022)</li>
<li>Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: ICCV (2021)</li>
<li>Charatan, D., Li, S., Tagliasacchi, A., Sitzmann, V.: pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. arXiv preprint arXiv:2312.12337 (2023)</li>
<li>Chen, G., Wang, W.: A survey on 3d gaussian splatting. arXiv preprint arXiv:2401.03890 (2024)</li>
<li>Chen, J., Jiang, Y., Lu, J., Zhang, L.: S-agents: self-organizing agents in openended environment. arXiv preprint arXiv:2402.04578 (2024)</li>
<li>Chen, S., Garcia, R., Schmid, C., Laptev, I.: Polarnet: 3d point clouds for languageguided robotic manipulation. arXiv preprint arXiv:2309.15596 (2023)</li>
<li>Driess, D., Schubert, I., Florence, P., Li, Y., Toussaint, M.: Reinforcement learning with neural radiance fields. NeurIPS (2022)</li>
<li>Du, Y., Yang, S., Dai, B., Dai, H., Nachum, O., Tenenbaum, J., Schuurmans, D., Abbeel, P.: Learning universal policies via text-guided video generation. NeurIPS 36 (2024)</li>
<li>Fu, Y., Liu, S., Kulkarni, A., Kautz, J., Efros, A.A., Wang, X.: Colmap-free 3d gaussian splatting. arXiv preprint arXiv:2312.07504 (2023)</li>
<li>Fu, Z., Zhao, T.Z., Finn, C.: Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. arXiv preprint arXiv:2401.02117 (2024)</li>
<li>Gao, Z., Mu, Y., Shen, R., Chen, C., Ren, Y., Chen, J., Li, S.E., Luo, P., Lu, Y.: Enhance sample efficiency and robustness of end-to-end urban autonomous driving via semantic masked world model. arXiv preprint arXiv:2210.04017 (2022)</li>
<li>Gervet, T., Xian, Z., Gkanatsios, N., Fragkiadaki, K.: Act3d: 3d feature field transformers for multi-task robotic manipulation. In: CoRL. pp. 3949-3965 (2023)</li>
<li>Goyal, A., Xu, J., Guo, Y., Blukis, V., Chao, Y.W., Fox, D.: Rvt: Robotic view transformer for 3d object manipulation. arXiv preprint arXiv:2306.14896 (2023)</li>
<li>Guhur, P.L., Chen, S., Pinel, R.G., Tapaswi, M., Laptev, I., Schmid, C.: Instruction-driven history-aware policies for robotic manipulations. In: CoRL. pp. 175-187. PMLR (2023)</li>
<li>Ha, D., Schmidhuber, J.: Recurrent world models facilitate policy evolution. NeurIPS 31 (2018)</li>
<li>
<p>Hafner, D., Lee, K.H., Fischer, I., Abbeel, P.: Deep hierarchical planning from pixels. NeurIPS 35, 26091-26104 (2022)</p>
</li>
<li>
<p>Hafner, D., Lillicrap, T., Ba, J., Norouzi, M.: Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603 (2019)</p>
</li>
<li>Hafner, D., Lillicrap, T., Norouzi, M., Ba, J.: Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193 (2020)</li>
<li>Hafner, D., Pasukonis, J., Ba, J., Lillicrap, T.: Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104 (2023)</li>
<li>Hansen, N., Su, H., Wang, X.: Td-mpc2: Scalable, robust world models for continuous control. arXiv preprint arXiv:2310.16828 (2023)</li>
<li>Hansen, N., Wang, X.: Generalization in reinforcement learning by soft data augmentation. In: ICRA (2021)</li>
<li>Hong, S., Lin, Y., Liu, B., Wu, B., Li, D., Chen, J., Zhang, J., Wang, J., Zhang, L., Zhuge, M., et al.: Data interpreter: An llm agent for data science. arXiv preprint arXiv:2402.18679 (2024)</li>
<li>Hu, A., Corrado, G., Griffiths, N., Murez, Z., Gurau, C., Yeo, H., Kendall, A., Cipolla, R., Shotton, J.: Model-based imitation learning for urban driving. NeurIPS 35, 20703-20716 (2022)</li>
<li>Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J., Corrado, G.: Gaia-1: A generative world model for autonomous driving. arXiv preprint arXiv:2309.17080 (2023)</li>
<li>Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., Carreira, J.: Perceiver: General perception with iterative attention. In: ICML (2021)</li>
<li>James, S., Ma, Z., Arrojo, D.R., Davison, A.J.: Rlbench: The robot learning benchmark \&amp; learning environment. RA-L (2020)</li>
<li>James, S., Wada, K., Laidlow, T., Davison, A.J.: Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation. In: CVPR (2022)</li>
<li>Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., Finn, C.: Bc-z: Zero-shot task generalization with robotic imitation learning. In: CoRL (2022)</li>
<li>Jiang, Z., Zhu, Y., Svetlik, M., Fang, K., Zhu, Y.: Synergies between affordance and geometry: 6 -dof grasp detection via implicit representations. arXiv preprint arXiv:2104.01542 (2021)</li>
<li>Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et al.: Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv (2018)</li>
<li>Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. TOG 42(4) (2023)</li>
<li>Kerr, J., Kim, C.M., Goldberg, K., Kanazawa, A., Tancik, M.: Lerf: Language embedded radiance fields. arXiv preprint arXiv:2303.09553 (2023)</li>
<li>Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In: ICCV. pp. $4015-4026(2023)$</li>
<li>Laskin, M., Srinivas, A., Abbeel, P.: Curl: Contrastive unsupervised representations for reinforcement learning. In: ICML (2020)</li>
<li>Li, Y., Li, S., Sitzmann, V., Agrawal, P., Torralba, A.: 3d neural scene representations for visuomotor control. In: CoRL. pp. 112-123 (2022)</li>
<li>Liang, L., Bian, L., Xiao, C., Zhang, J., Chen, L., Liu, I., Xiang, F., Huang, Z., Su, H.: Robo360: A 3d omnispective multi-material robotic manipulation dataset. arXiv preprint arXiv:2312.06686 (2023)</li>
<li>
<p>Lin, J., Du, Y., Watkins, O., Hafner, D., Abbeel, P., Klein, D., Dragan, A.: Learning to model the world with language. arXiv preprint arXiv:2308.01399 (2023)</p>
</li>
<li>
<p>Lin, Y.C., Florence, P., Zeng, A., Barron, J.T., Du, Y., Ma, W.C., Simeonov, A., Garcia, A.R., Isola, P.: Mira: Mental imagery for robotic affordances. In: CoRL. pp. 1916-1927 (2023)</p>
</li>
<li>Liu, H., Lee, L., Lee, K., Abbeel, P.: Instruction-following agents with jointly pretrained vision-language models. arXiv preprint arXiv:2210.13431 (2022)</li>
<li>Liu, S., James, S., Davison, A.J., Johns, E.: Auto-lambda: Disentangling dynamic task relationships. arXiv preprint arXiv:2202.03091 (2022)</li>
<li>Liu, Y., Li, C., Yang, C., Yuan, Y.: Endogaussian: Gaussian splatting for deformable surgical scene reconstruction. arXiv preprint arXiv:2401.12561 (2024)</li>
<li>Lu, G., Wang, Z., Liu, C., Lu, J., Tang, Y.: Thinkbot: Embodied instruction following with thought chain reasoning. arXiv preprint arXiv:2312.07062 (2023)</li>
<li>Luiten, J., Kopanas, G., Leibe, B., Ramanan, D.: Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713 (2023)</li>
<li>Mendonca, R., Bahl, S., Pathak, D.: Structured world models from human videos. arXiv preprint arXiv:2308.10901 (2023)</li>
<li>Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. CACM 65(1), 99-106 (2021)</li>
<li>Nair, S., Rajeswaran, A., Kumar, V., Finn, C., Gupta, A.: R3m: A universal visual representation for robot manipulation. arXiv (2022)</li>
<li>Parisi, S., Rajeswaran, A., Purushwalkam, S., Gupta, A.: The unsurprising effectiveness of pre-trained vision models for control. In: ICML (2022)</li>
<li>Qian, G., Li, Y., Peng, H., Mai, J., Hammoud, H., Elhoseiny, M., Ghanem, B.: Pointnext: Revisiting pointnet++ with improved training and scaling strategies. NeurIPS 35, 23192-23204 (2022)</li>
<li>Qin, M., Li, W., Zhou, J., Wang, H., Pfister, H.: Langsplat: 3d language gaussian splatting. arXiv preprint arXiv:2312.16084 (2023)</li>
<li>Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021)</li>
<li>Radosavovic, I., Xiao, T., James, S., Abbeel, P., Malik, J., Darrell, T.: Real-world robot learning with masked visual pre-training. In: CoRL (2023)</li>
<li>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022)</li>
<li>Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al.: Mastering atari, go, chess and shogi by planning with a learned model. Nature 588(7839), 604-609 (2020)</li>
<li>Seo, Y., Hafner, D., Liu, H., Liu, F., James, S., Lee, K., Abbeel, P.: Masked world models for visual control. In: CoRL. pp. 1332-1344 (2023)</li>
<li>Seo, Y., Lee, K., James, S.L., Abbeel, P.: Reinforcement learning with action-free pre-training from videos. In: ICML. pp. 19561-19579 (2022)</li>
<li>Shi, J.C., Wang, M., Duan, H.B., Guan, S.H.: Language embedded 3d gaussians for open-vocabulary scene understanding. arXiv preprint arXiv:2311.18482 (2023)</li>
<li>Shim, D., Lee, S., Kim, H.J.: Snerl: Semantic-aware neural radiance fields for reinforcement learning. ICML (2023)</li>
<li>Shridhar, M., Manuelli, L., Fox, D.: Cliport: What and where pathways for robotic manipulation. In: CoRL (2022)</li>
<li>Shridhar, M., Manuelli, L., Fox, D.: Perceiver-actor: A multi-task transformer for robotic manipulation. In: CoRL (2023)</li>
<li>
<p>Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Splatter image: Ultra-fast single-view 3d reconstruction. arXiv preprint arXiv:2312.13150 (2023)</p>
</li>
<li>
<p>Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., Roy, N.: Understanding natural language commands for robotic navigation and mobile manipulation. In: AAAI (2011)</p>
</li>
<li>Tenorth, M., Nyga, D., Beetz, M.: Understanding and executing instructions for everyday manipulation tasks from the world wide web. In: ICRA. pp. 1486-1491. IEEE (2010)</li>
<li>Wang, X., Zhu, Z., Huang, G., Chen, X., Lu, J.: Drivedreamer: Towards real-worlddriven world models for autonomous driving. arXiv preprint arXiv:2309.09777 (2023)</li>
<li>Wang, Z., Cai, S., Chen, G., Liu, A., Ma, X., Liang, Y.: Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560 (2023)</li>
<li>Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., Wang, X.: 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528 (2023)</li>
<li>Wu, J., Ma, H., Deng, C., Long, M.: Pre-training contextualized world models with in-the-wild videos for reinforcement learning. NeurIPS 36 (2024)</li>
<li>Wu, P., Escontrela, A., Hafner, D., Abbeel, P., Goldberg, K.: Daydreamer: World models for physical robot learning. In: CoRL. pp. 2226-2240 (2023)</li>
<li>Xie, T., Zong, Z., Qiu, Y., Li, X., Feng, Y., Yang, Y., Jiang, C.: Physgaussian: Physics-integrated 3d gaussians for generative dynamics. arXiv preprint arXiv:2311.12198 (2023)</li>
<li>Xu, D., Yuan, Y., Mardani, M., Liu, S., Song, J., Wang, Z., Vahdat, A.: Agg: Amortized generative 3d gaussians for single image to 3d. arXiv preprint arXiv:2401.04099 (2024)</li>
<li>Yang, Z., Yang, H., Pan, Z., Zhu, X., Zhang, L.: Real-time photorealistic dynamic scene representation and rendering with 4 d gaussian splatting. arXiv preprint arXiv:2310.10642 (2023)</li>
<li>Yang, Z., Gao, X., Zhou, W., Jiao, S., Zhang, Y., Jin, X.: Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101 (2023)</li>
<li>Ye, W., Liu, S., Kurutach, T., Abbeel, P., Gao, Y.: Mastering atari games with limited data. NeurIPS 34, 25476-25488 (2021)</li>
<li>You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., Hsieh, C.J.: Large batch optimization for deep learning: Training bert in 76 minutes. arXiv (2019)</li>
<li>Ze, Y., Hansen, N., Chen, Y., Jain, M., Wang, X.: Visual reinforcement learning with self-supervised 3d representations. RA-L (2023)</li>
<li>Ze, Y., Yan, G., Wu, Y.H., Macaluso, A., Ge, Y., Ye, J., Hansen, N., Li, L.E., Wang, X.: Gnfactor: Multi-task real robot learning with generalizable neural feature fields. In: CoRL. pp. 284-301. PMLR (2023)</li>
<li>Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., Armstrong, T., Krasin, I., Duong, D., Sindhwani, V., et al.: Transporter networks: Rearranging the visual world for robotic manipulation. In: CoRL (2021)</li>
<li>Zhang, B., Cheng, Y., Yang, J., Wang, C., Zhao, F., Tang, Y., Chen, D., Guo, B.: Gaussiancube: Structuring gaussian splatting using optimal transport for 3d generative modeling. arXiv preprint arXiv:2403.19655 (2024)</li>
<li>
<p>Zhang, T., Hu, Y., Cui, H., Zhao, H., Gao, Y.: A universal semantic-geometric representation for robotic manipulation. arXiv preprint arXiv:2306.10474 (2023)</p>
</li>
<li>
<p>Zheng, S., Zhou, B., Shao, R., Liu, B., Zhang, S., Nie, L., Liu, Y.: Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting for real-time human novel view synthesis. arXiv preprint arXiv:2312.02155 (2023)</p>
</li>
<li>Zhou, S., Chang, H., Jiang, S., Fan, Z., Zhu, Z., Xu, D., Chari, P., You, S., Wang, Z., Kadambi, A.: Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In: CVPR. pp. 21676-21685 (2024)</li>
<li>Zhu, L., Wang, Z., Jin, Z., Lin, G., Yu, L.: Deformable endoscopic tissues reconstruction with gaussian splatting. arXiv preprint arXiv:2401.11535 (2024)</li>
<li>Zou, Z.X., Yu, Z., Guo, Y.C., Li, Y., Liang, D., Cao, Y.P., Zhang, S.H.: Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers. arXiv preprint arXiv:2312.09147 (2023)</li>
<li>Zuo, X., Samangouei, P., Zhou, Y., Di, Y., Li, M.: Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding. arXiv preprint arXiv:2401.01970 (2024)</li>
</ol>
<h1>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation Supplementary Material</h1>
<p>In this supplementary material, we provide additional details and experiments not included in the main paper due to limitations in space.</p>
<ul>
<li>Appendix A: Details of the RLBench dataset and the training pipeline used in our experiments.</li>
<li>Appendix B: Additional implementation details of our ManiGaussian.</li>
<li>Appendix C: Supplementary quantitative analysis.</li>
<li>Appendix D: Supplementary qualitative analysis.</li>
</ul>
<h2>A Details of RLBench</h2>
<p>RLBench Dataset. In this section, we provide a concise overview of the RLBench [27] dataset and our training pipeline. Table 3 is an overview of the 10 selected tasks we use in the experiments. Our task variations include randomly sampled colors, sizes, counts, placements, and categories of objects. We have a color palette of 20 shades, including red, maroon, lime, green, blue, navy, yellow, cyan, magenta, silver, gray, orange, olive, purple, teal, azure, violet, rose, black, and white. The size of the objects is categorized into two types: short and tall. The number of objects can be either 1,2 , or 3 . Other properties vary depending on the specific task. Furthermore, objects are randomly arranged on the tabletop within a certain range, adding to the diversity of the tasks. In the ablation study, we adopt the task classification from [15] to group the RLBench tasks of Table 3 into 6 categories according to their key challenges. The task groups include:</p>
<ul>
<li>The Planning group contains tasks with multiple subtasks. The included tasks are: meat off grill and push buttons.</li>
<li>The Long group includes long-term tasks that requires more than 10 keyframes. The included tasks are: put in drawer and stack blocks.</li>
<li>The Tools group requires the agent to grasp an object to interact with the target object. The included tasks are: slide block, drag stick and sweep to dustpan.</li>
<li>The Motion group requires precise control, which often causes failures due to the predefined motion planner. The included task is: turn tap.</li>
<li>The Screw group requires gripper rotation to screw an object. The included task is: close jar.</li>
<li>The Occlusion group involves tasks with severe occlusion problems from certain views. The included task is: open drawer.</li>
</ul>
<p>Training Pipeline. To learn the policy, we uniformly sample a group of expert episodes from all the task variations, and then randomly choose an input-action pair for each of the tasks to form a batch. Other sampling strategies (e.g., Auto$\lambda$ [41]) are also available. To simplify the tasks, the agent is assumed to access a</p>
<p>Table 3: Selected tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Variations</th>
<th style="text-align: center;">Keyframes</th>
<th style="text-align: center;">Instruction Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">close jar</td>
<td style="text-align: center;">color</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">"close the _ jar"</td>
</tr>
<tr>
<td style="text-align: left;">open drawer</td>
<td style="text-align: center;">placement</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">"open the _ drawer"</td>
</tr>
<tr>
<td style="text-align: left;">sweep to dustpan</td>
<td style="text-align: center;">size</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">"sweep dirt to the _ dustpan"</td>
</tr>
<tr>
<td style="text-align: left;">meat off grill</td>
<td style="text-align: center;">category</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">"take the _ off the grill"</td>
</tr>
<tr>
<td style="text-align: left;">turn tap</td>
<td style="text-align: center;">placement</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">"turn _ tap"</td>
</tr>
<tr>
<td style="text-align: left;">slide block</td>
<td style="text-align: center;">color</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">"slide the block to _ target"</td>
</tr>
<tr>
<td style="text-align: left;">put in drawer</td>
<td style="text-align: center;">placement</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">"put the item in the _ drawer"</td>
</tr>
<tr>
<td style="text-align: left;">drag stick</td>
<td style="text-align: center;">color</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">"use the stick to drag the cube onto the _ target"</td>
</tr>
<tr>
<td style="text-align: left;">push buttons</td>
<td style="text-align: center;">color</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">"push the _ button, [then the _ button]"</td>
</tr>
<tr>
<td style="text-align: left;">stack blocks</td>
<td style="text-align: center;">color, count</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">"stack _ blocks"</td>
</tr>
</tbody>
</table>
<p>predefined motion planner (e.g., RRT-Connect), so that the input-action pairs are determined as the bottleneck end-effector poses (i.e., keyframes) within each demonstration based on empirical rules: a pose is determined as a keyframe if the end-effector changes state (e.g. close the gripper) or its velocities approach zero $[7,13,28,60,76]$. This setting simplifies the sequential decision-making problem into predicting the next optimal keyframe action based on the current observation, which can also be interpreted as a classification task.</p>
<h1>B Additional Implementation Details</h1>
<p>In this section, we detail the architectural design of each submodule in our Gaussian world model. For more details, please refer to our code.
Representation model. The representation model $q_{\phi}$ is the same with [76], which is not the main contribution in this paper. The representation model utilize a shallow 3D UNet to encode the voxel $\in \mathbb{R}^{100^{3} \times 10}$ (RGB features, coordinates, indices, and occupancy) into the high-level visual features $\mathbf{v}^{(t)} \in \mathbb{R}^{100^{3} \times 128}$.
Gaussian regressor. Given the current features $\mathbf{v}^{(t)}$ encoded by the representation model $q_{\phi}$, we pass it through a generalizable Gaussian regressor $g_{\phi}$ to infer the Gaussian distribution $\theta^{(t)}$ directly. The Gaussian regressor is designed as a lightweight multi-head neural network, where each head is responsible for predicting a specific feature. It consists of: (1) a position offset head that predicts the per-pixel 3D center offset $\in \mathbb{R}^{3}$, (2) a color head that predicts the coefficients of the spherical harmonic basis $\in \mathbb{R}^{12}$, (3) a rotation head with normalization that predicts the rotation quaternion $\in \mathbb{R}^{4}$, (4) a scaling head with exponential activation that outputs the scaling factor $\in \mathbb{R}^{3}$, (5) an opacity head with sigmoid activation that predicts the opacity $\in \mathbb{R}^{1}$. (6) a semantic head that predicts the semantic feature $\in \mathbb{R}^{3}$.
Deformation predictor. After obtaining the current visual features $\mathbf{v}^{(t)}$, Gaussian embedding $\theta^{(t)}$ and action $a^{(t)}$, we parameterize the transition process as a deformation predictor $p_{\phi}$ to predict the deformation $\Delta \mu_{i}^{(t)} \in \mathbb{R}^{3}$ and $\Delta r_{i}^{(t)} \in \mathbb{R}^{4}$ of each Gaussian, resulting in the future Gaussian embedding $\theta^{(t+1)}$. The deformation predictor is a fully-connected network with residual connections.
Hyperparameters. The hyperparameters used in ManiGaussian are shown in Table 4 To train the robotic manipulation agent, we use $\lambda_{\text {Geo }}=0.01$, $\lambda_{\text {Sem }}=0.0001$ and $\lambda_{\text {Dyna }}=0.001$ to focus on the action prediction. Other hyperparameters are in line with previous works $[60,76]$ for fair comparison.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>$\boxtimes$ Corresponding author.
${ }^{1}$ Project page: https://guanxinglu.github.io/ManiGaussian/</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
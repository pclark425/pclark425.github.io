<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5220 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5220</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5220</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267636769</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.07927v2.pdf" target="_blank">A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications</a></p>
                <p><strong>Paper Abstract:</strong> Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5220.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5220.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative generate-critique-refine prompting method where an LLM produces an initial answer, critiques its own output, and revises the answer repeatedly until stopping criteria are met, improving accuracy on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large instruction-tuned transformer-based model from OpenAI (GPT-4); size not specified in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (generate → self-critique → refine loop)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three-step iterative loop: (1) generate an initial response, (2) prompt the model to critique its own output, (3) refine the response using the self-generated critique; repeat until predefined stopping criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code optimization; Code readability; Sentiment reversal</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code optimization and readability tasks for code; sentiment reversal is a text transformation task requiring correct polarity change.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported improvements for GPT-4: +8.7 points (code optimization), +13.9 points (code readability), +21.6 points (sentiment reversal); absolute baseline scores not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative gains reported for GPT-4 on multiple tasks (8.7, 13.9, 21.6 point improvements), indicating the iterative self-critique/refine loop produced materially better outputs than the unspecified baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The survey does not report iteration counts or detailed failure modes; no fine-grained analysis of when self-critiques are incorrect is provided in this summary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5220.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5220.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogiCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Chain-of-Thought (LogiCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic prompting framework that injects logical verification into chain-of-thought by applying symbolic reasoning (e.g., reductio ad absurdum) to verify and revise each reasoning step in a think-verify-revise loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-33b and GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-33b: an instruction-tuned open LLM at ~33B parameters; GPT-4: OpenAI's large instruction-tuned model. (Survey lists these models in LogiCoT experiments.)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LogiCoT (think-verify-revise / neurosymbolic verification)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generates stepwise reasoning (CoT), applies symbolic logic-based verification (e.g., reductio ad absurdum) to check steps, and issues targeted feedback to revise incorrect steps in an iterative think-verify-revise loop.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; AQuA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: grade-school math word problems; AQuA: algebraic / math question answering requiring multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported relative improvements over CoT: on GSM8K +0.16% (Vicuna-33b) and +1.42% (GPT-4); on AQuA +3.15% (Vicuna-33b) and +2.75% (GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Small but measurable accuracy gains on GSM8K and larger gains on AQuA when LogiCoT's verification+revise loop is applied versus standard CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey-level summary notes improvements but does not provide detailed error analysis; absolute gains are modest on some benchmarks (e.g., 0.16% for Vicuna on GSM8K), indicating limited effect in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5220.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5220.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (diverse-sampling and answer marginalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/aggregation strategy for chain-of-thought that samples diverse reasoning chains and marginalizes/fuses final answers to select the most consistent solution, improving robustness of multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT-capable LLMs (as in cited studies; e.g., PaLM, GPT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based LLMs capable of producing chain-of-thought rationales; specific models vary across cited experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (sample many reasoning chains → marginalize answers)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Instead of greedy decoding, sample multiple distinct chain-of-thoughts from the model's decoder, then aggregate/marginalize the final answers across samples to choose the most consistent answer.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP, AQuA, StrategyQA, ARC-challenge (and others)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks for arithmetic, symbolic, commonsense, and multi-step reasoning problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported relative improvements over baseline chain-of-thought prompting: +17.9% (GSM8K), +11.0% (SVAMP), +12.2% (AQuA), +6.4% (StrategyQA), +3.9% (ARC-challenge).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Substantial percentage-point gains across multiple reasoning benchmarks when using self-consistency versus baseline CoT decoding, demonstrating improved final-answer accuracy by aggregating diverse reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes that self-consistency can select frequent answers without resolving underlying flawed reasoning; it relies on diversity of sampled chains and may not correct systematic reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5220.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5220.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ECHO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Harmonized Chain-of-Thought (ECHO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that clusters questions, samples representative demonstrations, and iteratively unifies divergent reasoning rationales to produce harmonized, robust CoT demonstrations for in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B; comparisons to GPT-3.5 noted</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixtral-8x7B: a 7B-parameter model variant evaluated in the survey; GPT-3.5 used as a reference for rationale quality.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ECHO (question clustering → demonstration sampling → iterative demonstration unification)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three-stage pipeline: (1) cluster questions with Sentence-BERT and k-means, (2) sample representative questions and generate rationales (Zero-Shot-CoT), (3) iteratively refine/unify rationales with dynamic prompting to align reasoning patterns and reduce noisy diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>10 reasoning benchmarks (arithmetic, commonsense, symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of reasoning benchmarks covering arithmetic, commonsense, and symbolic reasoning tasks used to evaluate demonstration quality and few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>ECHO surpassed Auto-CoT by an average of +2.8% across 10 benchmarks; retained performance with 50% fewer examples (only a -0.8% dip vs Few-Shot-CoT's -1.3%); achieved +2.3% over Auto-CoT on Mixtral-8x7B but remained behind GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative improvements over Auto-CoT and reduced sample requirements (maintains performance with half the examples) indicate that iterative unification of rationales improves in-context demonstration effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>ECHO still lagged GPT-3.5 in some evaluations, which the survey attributes to differences in the intrinsic quality of generated rationales; method relies on clustering quality and the quality of initial rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5220.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5220.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CD-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Denoising with Noisy Chain-of-Thought (CD-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that mitigates the negative effect of noisy/incorrect rationales by contrasting noisy rationales with clean ones, rephrasing flawed examples, selecting optimal reasoning paths, and voting to improve final-answer robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs evaluated in Zhou et al. (survey does not fix a single model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language models used for CoT reasoning; the survey summarizes CD-CoT results across evaluated LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>CD-CoT (contrast noisy vs clean rationales; rephrase & vote)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Constructs contrastive pairs of noisy and clean rationales, rephrases flawed examples to reduce harmful influence, selects better reasoning paths, and votes on answers to denoise the supervision signal.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NoRa (Noisy Rationales) related evaluations and multiple reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks designed to probe robustness of CoT when training/demonstrations include noisy or incorrect rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported average accuracy improvement of +17.8% over baselines that are harmed by noisy rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Substantial average accuracy increases (17.8%) indicate CD-CoT effectively counters the performance degradation caused by noisy rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes that self-correction and self-consistency have limitations; CD-CoT addresses noisy rationales but detailed failure modes or costs (e.g., computation) are not enumerated in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5220.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5220.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoVe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Verification (CoVe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberate multi-step self-verification prompting pipeline where the model generates an answer, plans verification questions, answers them independently, and produces a revised, verified response to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-verification reduces hallucination in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified in survey summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based language models used for QA and generation tasks; the survey summarizes experimental findings without fixing a single model.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Verification (generate → plan verifications → answer verifications → revise)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Four-step loop: (1) generate baseline responses, (2) plan verification questions that check the generated work, (3) answer those verification questions independently, (4) produce a revised response incorporating verification results.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>List questions, question answering, long-form generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks prone to hallucination and factual errors where checking intermediate facts is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Survey reports that CoVe decreases hallucinations while maintaining factuality on evaluated tasks (qualitative and comparative claims summarized from experiments), indicating verification helps reduce errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The survey-level summary does not provide numeric effect sizes in this text; potential cost (extra queries/latency) and failure modes (incorrect verification answers) are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5220.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5220.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ThoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thread of Thought (ThoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-phase prompting method that segments chaotic/long contexts, summarizes and examines each segment incrementally, then refines information for a final response — enabling iterative reasoning over large contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (model-agnostic plug-and-play module; specific models not enumerated in summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey describes ThoT as a modular prompting technique that can be applied to various LLMs; no single model is fixed in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Thread of Thought (segment summarization → incremental refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-phase approach: (1) the LLM summarizes and analyzes each context segment separately, (2) the model refines and aggregates segment-level analyses into a final coherent response.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question answering and conversation datasets (chaotic contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>QA and conversational tasks with long or disorganized input contexts where incremental processing improves understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported performance improvements of 47.20% (question answering) and 17.8% (conversation datasets) in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Large percentage improvements on QA and conversation datasets suggest segment-wise iterative summarization and refinement substantially enhance final response quality in chaotic contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not list detailed failure cases here; potential trade-offs include extra prompting steps and latency for iterative segment processing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5220.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5220.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-Back</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Step-Back Prompting (Take a Step Back)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metacognitive prompting technique that asks models to abstract high-level concepts from instances (abstraction) before performing detailed reasoning, effectively invoking a generate-then-reflect abstraction stage to improve multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Take a step back: evoking reasoning via abstraction in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2L (example advanced model mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2L: an advanced PaLM family model variant referenced by the survey as an example target for Step-Back prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Step-Back (abstraction → reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-stage process where the model first performs abstraction (extracts high-level principles and concepts from instances) and then performs detailed reasoning grounded by the abstraction, enabling a reflective higher-level check before final answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>STEM, Knowledge QA, Multi-Hop Reasoning (MMLU Physics & Chemistry, TimeQA, MuSiQue cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse reasoning-intensive tasks including STEM questions, multi-hop question answering, and knowledge-based QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported boosts: MMLU Physics & Chemistry +7%, TimeQA +27%, MuSiQue +7% when Step-Back prompting applied to PaLM-2L.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Substantial percentage gains on several reasoning benchmarks indicate abstraction-before-reasoning improves model performance in complex multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not detail failure modes; potential constraints include reliance on the model's ability to produce correct abstractions and possible increased prompting complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain-of-verification reduces hallucination in large language models <em>(Rating: 2)</em></li>
                <li>Take a step back: evoking reasoning via abstraction in large language models <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 1)</em></li>
                <li>Contrastive chain-of-thought prompting <em>(Rating: 1)</em></li>
                <li>Can language models perform robust reasoning in chain-of-thought prompting with noisy rationales? <em>(Rating: 2)</em></li>
                <li>Echoprompt: Instructing the model to rephrase queries for improved in-context learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5220",
    "paper_id": "paper-267636769",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (iterative refinement with self-feedback)",
            "brief_description": "An iterative generate-critique-refine prompting method where an LLM produces an initial answer, critiques its own output, and revises the answer repeatedly until stopping criteria are met, improving accuracy on complex tasks.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "A large instruction-tuned transformer-based model from OpenAI (GPT-4); size not specified in the survey.",
            "reflection_method_name": "Self-Refine (generate → self-critique → refine loop)",
            "reflection_method_description": "Three-step iterative loop: (1) generate an initial response, (2) prompt the model to critique its own output, (3) refine the response using the self-generated critique; repeat until predefined stopping criteria.",
            "num_iterations": null,
            "task_name": "Code optimization; Code readability; Sentiment reversal",
            "task_description": "Code optimization and readability tasks for code; sentiment reversal is a text transformation task requiring correct polarity change.",
            "performance_with_reflection": "Reported improvements for GPT-4: +8.7 points (code optimization), +13.9 points (code readability), +21.6 points (sentiment reversal); absolute baseline scores not provided in the survey.",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative gains reported for GPT-4 on multiple tasks (8.7, 13.9, 21.6 point improvements), indicating the iterative self-critique/refine loop produced materially better outputs than the unspecified baselines.",
            "limitations_or_failure_cases": "The survey does not report iteration counts or detailed failure modes; no fine-grained analysis of when self-critiques are incorrect is provided in this summary.",
            "uuid": "e5220.0",
            "source_info": {
                "paper_title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LogiCoT",
            "name_full": "Logical Chain-of-Thought (LogiCoT)",
            "brief_description": "A neurosymbolic prompting framework that injects logical verification into chain-of-thought by applying symbolic reasoning (e.g., reductio ad absurdum) to verify and revise each reasoning step in a think-verify-revise loop.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Vicuna-33b and GPT-4",
            "model_description": "Vicuna-33b: an instruction-tuned open LLM at ~33B parameters; GPT-4: OpenAI's large instruction-tuned model. (Survey lists these models in LogiCoT experiments.)",
            "reflection_method_name": "LogiCoT (think-verify-revise / neurosymbolic verification)",
            "reflection_method_description": "Generates stepwise reasoning (CoT), applies symbolic logic-based verification (e.g., reductio ad absurdum) to check steps, and issues targeted feedback to revise incorrect steps in an iterative think-verify-revise loop.",
            "num_iterations": null,
            "task_name": "GSM8K; AQuA",
            "task_description": "GSM8K: grade-school math word problems; AQuA: algebraic / math question answering requiring multi-step reasoning.",
            "performance_with_reflection": "Reported relative improvements over CoT: on GSM8K +0.16% (Vicuna-33b) and +1.42% (GPT-4); on AQuA +3.15% (Vicuna-33b) and +2.75% (GPT-4).",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "evidence_of_improvement": "Small but measurable accuracy gains on GSM8K and larger gains on AQuA when LogiCoT's verification+revise loop is applied versus standard CoT prompting.",
            "limitations_or_failure_cases": "Survey-level summary notes improvements but does not provide detailed error analysis; absolute gains are modest on some benchmarks (e.g., 0.16% for Vicuna on GSM8K), indicating limited effect in some settings.",
            "uuid": "e5220.1",
            "source_info": {
                "paper_title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (diverse-sampling and answer marginalization)",
            "brief_description": "A decoding/aggregation strategy for chain-of-thought that samples diverse reasoning chains and marginalizes/fuses final answers to select the most consistent solution, improving robustness of multi-step reasoning.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": "CoT-capable LLMs (as in cited studies; e.g., PaLM, GPT variants)",
            "model_description": "Large transformer-based LLMs capable of producing chain-of-thought rationales; specific models vary across cited experiments.",
            "reflection_method_name": "Self-Consistency (sample many reasoning chains → marginalize answers)",
            "reflection_method_description": "Instead of greedy decoding, sample multiple distinct chain-of-thoughts from the model's decoder, then aggregate/marginalize the final answers across samples to choose the most consistent answer.",
            "num_iterations": null,
            "task_name": "GSM8K, SVAMP, AQuA, StrategyQA, ARC-challenge (and others)",
            "task_description": "Benchmarks for arithmetic, symbolic, commonsense, and multi-step reasoning problems.",
            "performance_with_reflection": "Reported relative improvements over baseline chain-of-thought prompting: +17.9% (GSM8K), +11.0% (SVAMP), +12.2% (AQuA), +6.4% (StrategyQA), +3.9% (ARC-challenge).",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "evidence_of_improvement": "Substantial percentage-point gains across multiple reasoning benchmarks when using self-consistency versus baseline CoT decoding, demonstrating improved final-answer accuracy by aggregating diverse reasoning paths.",
            "limitations_or_failure_cases": "Survey notes that self-consistency can select frequent answers without resolving underlying flawed reasoning; it relies on diversity of sampled chains and may not correct systematic reasoning errors.",
            "uuid": "e5220.2",
            "source_info": {
                "paper_title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ECHO",
            "name_full": "Self-Harmonized Chain-of-Thought (ECHO)",
            "brief_description": "A framework that clusters questions, samples representative demonstrations, and iteratively unifies divergent reasoning rationales to produce harmonized, robust CoT demonstrations for in-context learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Mixtral-8x7B; comparisons to GPT-3.5 noted",
            "model_description": "Mixtral-8x7B: a 7B-parameter model variant evaluated in the survey; GPT-3.5 used as a reference for rationale quality.",
            "reflection_method_name": "ECHO (question clustering → demonstration sampling → iterative demonstration unification)",
            "reflection_method_description": "Three-stage pipeline: (1) cluster questions with Sentence-BERT and k-means, (2) sample representative questions and generate rationales (Zero-Shot-CoT), (3) iteratively refine/unify rationales with dynamic prompting to align reasoning patterns and reduce noisy diversity.",
            "num_iterations": null,
            "task_name": "10 reasoning benchmarks (arithmetic, commonsense, symbolic)",
            "task_description": "A suite of reasoning benchmarks covering arithmetic, commonsense, and symbolic reasoning tasks used to evaluate demonstration quality and few-shot performance.",
            "performance_with_reflection": "ECHO surpassed Auto-CoT by an average of +2.8% across 10 benchmarks; retained performance with 50% fewer examples (only a -0.8% dip vs Few-Shot-CoT's -1.3%); achieved +2.3% over Auto-CoT on Mixtral-8x7B but remained behind GPT-3.5.",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative improvements over Auto-CoT and reduced sample requirements (maintains performance with half the examples) indicate that iterative unification of rationales improves in-context demonstration effectiveness.",
            "limitations_or_failure_cases": "ECHO still lagged GPT-3.5 in some evaluations, which the survey attributes to differences in the intrinsic quality of generated rationales; method relies on clustering quality and the quality of initial rationales.",
            "uuid": "e5220.3",
            "source_info": {
                "paper_title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CD-CoT",
            "name_full": "Contrastive Denoising with Noisy Chain-of-Thought (CD-CoT)",
            "brief_description": "A method that mitigates the negative effect of noisy/incorrect rationales by contrasting noisy rationales with clean ones, rephrasing flawed examples, selecting optimal reasoning paths, and voting to improve final-answer robustness.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs evaluated in Zhou et al. (survey does not fix a single model)",
            "model_description": "Large language models used for CoT reasoning; the survey summarizes CD-CoT results across evaluated LLMs.",
            "reflection_method_name": "CD-CoT (contrast noisy vs clean rationales; rephrase & vote)",
            "reflection_method_description": "Constructs contrastive pairs of noisy and clean rationales, rephrases flawed examples to reduce harmful influence, selects better reasoning paths, and votes on answers to denoise the supervision signal.",
            "num_iterations": null,
            "task_name": "NoRa (Noisy Rationales) related evaluations and multiple reasoning benchmarks",
            "task_description": "Tasks designed to probe robustness of CoT when training/demonstrations include noisy or incorrect rationales.",
            "performance_with_reflection": "Reported average accuracy improvement of +17.8% over baselines that are harmed by noisy rationales.",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "evidence_of_improvement": "Substantial average accuracy increases (17.8%) indicate CD-CoT effectively counters the performance degradation caused by noisy rationales.",
            "limitations_or_failure_cases": "Survey notes that self-correction and self-consistency have limitations; CD-CoT addresses noisy rationales but detailed failure modes or costs (e.g., computation) are not enumerated in the summary.",
            "uuid": "e5220.4",
            "source_info": {
                "paper_title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CoVe",
            "name_full": "Chain-of-Verification (CoVe)",
            "brief_description": "A deliberate multi-step self-verification prompting pipeline where the model generates an answer, plans verification questions, answers them independently, and produces a revised, verified response to reduce hallucinations.",
            "citation_title": "Chain-of-verification reduces hallucination in large language models",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified in survey summary)",
            "model_description": "Transformer-based language models used for QA and generation tasks; the survey summarizes experimental findings without fixing a single model.",
            "reflection_method_name": "Chain-of-Verification (generate → plan verifications → answer verifications → revise)",
            "reflection_method_description": "Four-step loop: (1) generate baseline responses, (2) plan verification questions that check the generated work, (3) answer those verification questions independently, (4) produce a revised response incorporating verification results.",
            "num_iterations": null,
            "task_name": "List questions, question answering, long-form generation",
            "task_description": "Tasks prone to hallucination and factual errors where checking intermediate facts is beneficial.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "evidence_of_improvement": "Survey reports that CoVe decreases hallucinations while maintaining factuality on evaluated tasks (qualitative and comparative claims summarized from experiments), indicating verification helps reduce errors.",
            "limitations_or_failure_cases": "The survey-level summary does not provide numeric effect sizes in this text; potential cost (extra queries/latency) and failure modes (incorrect verification answers) are not detailed here.",
            "uuid": "e5220.5",
            "source_info": {
                "paper_title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ThoT",
            "name_full": "Thread of Thought (ThoT)",
            "brief_description": "A two-phase prompting method that segments chaotic/long contexts, summarizes and examines each segment incrementally, then refines information for a final response — enabling iterative reasoning over large contexts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (model-agnostic plug-and-play module; specific models not enumerated in summary)",
            "model_description": "Survey describes ThoT as a modular prompting technique that can be applied to various LLMs; no single model is fixed in the summary.",
            "reflection_method_name": "Thread of Thought (segment summarization → incremental refinement)",
            "reflection_method_description": "Two-phase approach: (1) the LLM summarizes and analyzes each context segment separately, (2) the model refines and aggregates segment-level analyses into a final coherent response.",
            "num_iterations": null,
            "task_name": "Question answering and conversation datasets (chaotic contexts)",
            "task_description": "QA and conversational tasks with long or disorganized input contexts where incremental processing improves understanding.",
            "performance_with_reflection": "Reported performance improvements of 47.20% (question answering) and 17.8% (conversation datasets) in the survey summary.",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "evidence_of_improvement": "Large percentage improvements on QA and conversation datasets suggest segment-wise iterative summarization and refinement substantially enhance final response quality in chaotic contexts.",
            "limitations_or_failure_cases": "Survey does not list detailed failure cases here; potential trade-offs include extra prompting steps and latency for iterative segment processing.",
            "uuid": "e5220.6",
            "source_info": {
                "paper_title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Step-Back",
            "name_full": "Step-Back Prompting (Take a Step Back)",
            "brief_description": "A metacognitive prompting technique that asks models to abstract high-level concepts from instances (abstraction) before performing detailed reasoning, effectively invoking a generate-then-reflect abstraction stage to improve multi-step reasoning.",
            "citation_title": "Take a step back: evoking reasoning via abstraction in large language models",
            "mention_or_use": "mention",
            "model_name": "PaLM-2L (example advanced model mentioned)",
            "model_description": "PaLM-2L: an advanced PaLM family model variant referenced by the survey as an example target for Step-Back prompting.",
            "reflection_method_name": "Step-Back (abstraction → reasoning)",
            "reflection_method_description": "Two-stage process where the model first performs abstraction (extracts high-level principles and concepts from instances) and then performs detailed reasoning grounded by the abstraction, enabling a reflective higher-level check before final answer generation.",
            "num_iterations": null,
            "task_name": "STEM, Knowledge QA, Multi-Hop Reasoning (MMLU Physics & Chemistry, TimeQA, MuSiQue cited)",
            "task_description": "Diverse reasoning-intensive tasks including STEM questions, multi-hop question answering, and knowledge-based QA.",
            "performance_with_reflection": "Reported boosts: MMLU Physics & Chemistry +7%, TimeQA +27%, MuSiQue +7% when Step-Back prompting applied to PaLM-2L.",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "evidence_of_improvement": "Substantial percentage gains on several reasoning benchmarks indicate abstraction-before-reasoning improves model performance in complex multi-step tasks.",
            "limitations_or_failure_cases": "Survey does not detail failure modes; potential constraints include reliance on the model's ability to produce correct abstractions and possible increased prompting complexity.",
            "uuid": "e5220.7",
            "source_info": {
                "paper_title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-verification reduces hallucination in large language models",
            "rating": 2,
            "sanitized_title": "chainofverification_reduces_hallucination_in_large_language_models"
        },
        {
            "paper_title": "Take a step back: evoking reasoning via abstraction in large language models",
            "rating": 2,
            "sanitized_title": "take_a_step_back_evoking_reasoning_via_abstraction_in_large_language_models"
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 1,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "Contrastive chain-of-thought prompting",
            "rating": 1,
            "sanitized_title": "contrastive_chainofthought_prompting"
        },
        {
            "paper_title": "Can language models perform robust reasoning in chain-of-thought prompting with noisy rationales?",
            "rating": 2,
            "sanitized_title": "can_language_models_perform_robust_reasoning_in_chainofthought_prompting_with_noisy_rationales"
        },
        {
            "paper_title": "Echoprompt: Instructing the model to rephrase queries for improved in-context learning",
            "rating": 1,
            "sanitized_title": "echoprompt_instructing_the_model_to_rephrase_queries_for_improved_incontext_learning"
        }
    ],
    "cost": 0.01900125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications
16 Mar 2025</p>
<p>Pranab Sahoo 
Department of Computer Science And Engineering
Indian Institute of Technology Patna</p>
<p>Ayush Kumar Singh 
Department of Computer Science And Engineering
Indian Institute of Technology Patna</p>
<p>Sriparna Saha sriparna@iitp.ac.in 
Department of Computer Science And Engineering
Indian Institute of Technology Patna</p>
<p>Vinija Jain 
Stanford University
3 AmazonAI</p>
<p>Samrat Mondal samrat@iitp.ac.in 
Department of Computer Science And Engineering
Indian Institute of Technology Patna</p>
<p>Aman Chadha 
Stanford University
3 AmazonAI</p>
<p>A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications
16 Mar 202506DBCF461384F3895F42B16B1665B977arXiv:2402.07927v2[cs.AI]
Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs).This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters.Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt.Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge.This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning.However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques.This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area.For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized.We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique.This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.</p>
<p>Introduction</p>
<p>Prompt engineering has emerged as a crucial technique for enhancing the capabilities of pre-trained large language models (LLMs) and vision-language models (VLMs).It involves strategically designing task-specific instructions, referred to as prompts, to guide model output without altering parameters.The significance of prompt engineering is especially evident in its transformative impact on the adaptability of LLMs and VLMs.By offering a mechanism to fine-tune model outputs through carefully crafted instructions, prompt engineering enables these models to excel across diverse tasks and domains.This adaptability is different from traditional paradigms, where model retraining or extensive fine-tuning is often required for task-specific performance.This is the transformative promise of prompt engineering, pushing the boundaries of AI and opening doors to a future brimming with possibilities.In an ever-evolving landscape, ongoing research consistently reveals innovative approaches and applications within prompt engineering.The significance of prompt engineering is underscored by its capacity to steer model responses, enhancing the adaptability and applicability of LLMs across diverse sectors.The landscape of contemporary prompt engineering spans a spectrum of techniques, encompassing foundational methods like zero-shot and few-shot prompting to more intricate approaches such as "chain of code" prompting.The notion of prompt engineering was initially investigated and popularized in the LLMs [Liu et al., 2023], [Tonmoy et al., 2024], [Chen et al., 2023] later extended to VLMs [Wu et al., 2023], [Bahng et al., 2022].Despite the extensive literature on prompt engineering within both LLMs and VLMs, a notable gap remains, particularly concerning a systematic overview of application-centric prompt engineering techniques.With recent strides in prompt engineering, there is a pressing need for a comprehensive survey that offers a nuanced understanding of applications and advancements in contemporary research.This survey dives deep into the ever-evolving landscape of prompt engineering, analyzing over 41 distinct techniques categorized by their diverse applications.Employing a systematic review approach, we meticulously delve into the intricacies of diverse cutting-edge prompting methods.Our examination encompasses their applications, the language models utilized, and the datasets subjected to experimentation, providing a detailed and nuanced analysis of the evolving landscape of prompt engineering.Additionally, we discuss the pros and cons of these techniques, offering insights into their comparative efficacy.We present a comprehensive taxonomy diagram that illustrates how these techniques navigate the vast landscape of LLM capabilities (see Fig. 2) and provide a table summarizing the datasets, employed models, and evaluation metrics (see Table1).From language generation and question answering to code creation and reasoning tasks, prompt engineering empowers the LLMs into performing feats we never thought possible.By bridging the existing gap in the literature, this survey aims to serve as a valuable resource for researchers and practitioners, offering insights into the latest developments and facilitating a deeper understanding of the evolving landscape of prompt engineering.The structure of the paper is organized as follows: Section 2 presents the prompt engineering techniques from both basic to advanced by categorizing application-area and Section 3 provides a conclusion along with considerations for future research endeavors.</p>
<p>Prompt Engineering</p>
<p>In this section, we have organized prompt engineering techniques according to their application areas and provided a concise overview of the evolution of prompting techniques, spanning from zero-shot prompting to the latest advancements.</p>
<p>New Tasks Without Extensive Training</p>
<p>Zero-Shot Prompting Zero-shot prompting offers a paradigm shift in leveraging large LLMs.This technique removes the need for extensive training data, instead relying on carefully crafted prompts that guide the model toward novel tasks [Radford et al., 2019].Specifically, the model receives a task description in the prompt but lacks labeled data for training on specific input-output mappings.The model then leverages its pre-existing knowledge to generate predictions based on the given prompt for the new task.</p>
<p>Few-Shot Prompting</p>
<p>Few-shot prompting provides models with a few input-output examples to induce an understanding of a given task, unlike zero-shot prompting, where no examples are supplied [Brown et al., 2020].Providing even a few high-quality examples has improved model performance on complex tasks compared to no demonstration.However, few-shot prompting requires additional tokens to include the examples, which may become prohibitive for longer text inputs.Moreover, the selection and composition of prompt examples can significantly influence model behavior, and biases like favoring frequent words may still affect few-shot results.While few-shot prompting enhances capabilities for complex tasks, especially among large pre-trained models like GPT-3, careful prompt engineering is critical to achieve optimal performance and mitigate unintended model biases.</p>
<p>Reasoning and Logic</p>
<p>Chain-of-Thought (CoT) Prompting LLMs often stumble in the face of complex reasoning, limiting their potential.Aiming to bridge this gap, Wei et al. [2022] introduced Chain-of-Thought (CoT) prompting as a technique to prompt LLMs in a way that facilitates coherent and step-by-step reasoning processes.The primary contribution lies in the proposal and exploration of CoT prompting, demonstrating its effectiveness in eliciting more structured and thoughtful responses from LLMs compared to traditional prompts.Through a series of experiments, the authors showcase the distinctive qualities of CoT prompting, emphasizing its ability to guide LLMs through a logical reasoning chain.This results in responses that reflect a deeper understanding of the given prompts.For example, the prompt would show the reasoning process and final answer for a multi-step math word problem and mimic how humans break down problems into logical intermediate steps.The authors achieved stateof-the-art performance in math and commonsense reasoning benchmarks by utilizing CoT prompts for PaLM 540B, achieving an accuracy of 90.2%.</p>
<p>Automatic Chain-of-Thought (Auto-CoT) Prompting Manual creation of high-quality CoT examples is both timeconsuming and suboptimal.Zhang et al. [2022] introduced Auto-CoT to automatically instruct LLMs with a "Let's think step-by-step" prompt to generate reasoning chains.Recognizing the possibility of errors in individually generated chains, Auto-CoT enhances robustness through diverse sampling.It samples various questions and generates multiple distinct reasoning chains for each, forming a final set of demonstrations.This automated diverse sampling minimizes errors and enhances few-shot learning, eliminating the need for labor-intensive manual creation of reasoning chains.Auto-CoT demonstrated enhanced performance, surpassing the CoT paradigm with average accuracy improvements of 1.33% and 1.5% on arithmetic and symbolic reasoning tasks, respectively, employing GPT-3.Wang et al. [2022] introduced self-consistency, a decoding strategy enhancing reasoning performance compared to greedy decoding in CoT prompting.For complex reasoning tasks with multiple valid paths, self-consistency generates diverse reasoning chains by sampling from the language model's decoder.It then identifies the most consistent final answer by marginalizing these sampled chains.This approach capitalizes on the observation that problems requiring thoughtful analysis often entail greater reasoning diversity, leading to a solution.The combination of self-consistency and chain-of-thought prompting results in significant accuracy improvements across various benchmarks, such as 17.9% on GSM8K, 11.0% on SVAMP, 12.2% on AQuA, 6.4% on StrategyQA, and 3.9% on ARC-challenge compared to the baseline chain-of-thought prompting.</p>
<p>Self-Consistency</p>
<p>Logical Chain-of-Thought (LogiCoT) Prompting</p>
<p>The ability to perform logical reasoning is critical for LLMs to solve complex, multi-step problems across diverse domains.Existing methods, like CoT prompting, encourage step-by-step reasoning but lack effective verification mechanisms.Zhao et al. [2023] proposes a Logical Chain-of-Thought (LogiCoT) prompting, a neurosymbolic framework that leverages principles from symbolic logic to enhance reasoning in a coherent and structured manner.Specifically, LogiCoT applies the concept of reductio ad absurdum to verify each step of reasoning generated by the model and provide targeted feedback to revise incorrect steps.LogiCoT can reduce logical errors and hallucinations through a think-verify-revise loop.Experimenting with Vicuna-33b and GPT-4, the findings underscore LogiCoT's notable enhancement of reasoning abilities, exhibiting improvements of 0.16% and 1.42% on the GSM8K dataset and 3.15% and 2.75% on the AQuA dataset compared to CoT, respectively.</p>
<p>Chain-of-Symbol (CoS) Prompting</p>
<p>LLMs often struggle with tasks involving complex spatial relationships due to their reliance on natural language, which is susceptible to ambiguity and biases.To overcome this limitation, Hu et al. [2023] introduced CoS, employing condensed symbols instead of natural language.CoS provides distinct advantages: clear and concise prompts, heightened spatial reasoning for LLMs, and improved human interpretability.CoS suffers from challenges such as scalability, generalizability, integration with other techniques, and interpretability of LLM reasoning based on symbols.Notably, the implementation of CoS significantly elevates ChatGPT's performance, boosting accuracy from 31.8% to an impressive 92.6% on Brick World tasks.Moreover, CoS achieves up to a 65.8% reduction in prompt tokens, streamlining the process while maintaining high accuracy.</p>
<p>Tree-of-Thoughts (ToT) Prompting Yao et al. [2023a] and Long [2023] proposed the Tree-of-Thoughts (ToT) framework to enhance prompting capabilities for complex tasks requiring exploration and look-ahead reasoning.ToT extends CoT prompting by managing a tree structure of intermediate reasoning steps, known as "thoughts".Each thought represents a coherent language sequence moving toward the final solution.This structure allows language models to deliberately reason by assessing the progress generated by thoughts in solving the problem.ToT integrates the model's abilities to produce and evaluate thoughts with search algorithms like breadth-first or depth-first search.This enables systematic exploration among reasoning chains, with a look-ahead to expand promising directions and to backtrack when solutions are incorrect.ToT excelled in the Game of 24 tasks, achieving a 74% success rate compared to CoT's 4%.Additionally, in word-level tasks, ToT outperformed CoT with a 60% success rate versus 16%.</p>
<p>Graph-of-Thoughts (GoT) Prompting</p>
<p>The inherent non-linear nature of human thought processes challenges the conventional sequential approach of CoT prompting.Yao et al. [2023b] introduced the "Graph of Thoughts" prompting, a graph-based framework advancing traditional sequential methods to better align with the non-linear characteristics of human thinking.This framework permits dynamic interplay, backtracking, and evaluation of ideas, allowing the aggregation and combination of thoughts from various branches, departing from the linear structure of the tree of thoughts.The key contributions encompass modeling the reasoning process as a directed graph, offering a modular architecture with diverse transformation operations.The framework is presented as a versatile and dynamic approach to language model prompting, capturing the intricacies of human thought processes and enhancing model capabilities.The GoT reasoning model demonstrates substantial gains over the CoT baseline, improving accuracy by 3.41% with T5-base and 5.08% with T5-large on GSM8K.It also boosts accuracy over the state-of-the-art Multimodal-CoT by 6.63% using T5-base and 1.09% with T5-large on ScienceQA.</p>
<p>System 2 Attention (S2A) Prompting</p>
<p>The soft attention mechanism in Transformer-based LLMs is prone to incorporating irrelevant context information, impacting token generation adversely.To address this, Weston and Sukhbaatar [2023] proposed System 2 Attention (S2A), utilizing the reasoning abilities of LLMs to selectively attend to relevant portions by regenerating the input context.S2A employs a two-step process to enhance attention and response quality by employing context regeneration and response generation with refined context.The effectiveness of S2A is evaluated across various tasks, including factual QA, long-form generation, and math word problems.In factual QA, S2A attains an accuracy of 80.3%, demonstrating a substantial enhancement in factuality.In long-form generation, it improves objectivity and receives a score of 3.82 out of 5.</p>
<p>Thread of Thought (ThoT) Prompting Zhou et al. [2023] presented Thread of Thought (ThoT), a prompting technique designed to enhance the reasoning abilities of LLMs within chaotic contexts.ThoT, inspired by human cognition, systematically examines extensive contexts into manageable segments for incremental analysis, employing a two-phase approach where the LLM first summarizes and examines each segment before refining the information for a final response.ThoT's flexibility shines as a versatile "plug-andplay" module, enhancing reasoning across different models and prompting methods.Evaluations on question answering and conversation datasets reveal substantial performance improvements of 47.20% and 17.8%, respectively, especially in chaotic contexts.</p>
<p>Chain-of-Table Prompting</p>
<p>Approaches like CoT, PoT, and ToT represent reasoning steps through free-form text or code, which face challenges when dealing with intricate table scenarios.The study by Wang et al. [2024] introduced a pioneering prompting technique named Chain-of-Table .This method uses step-by-step tabular reasoning by dynamically generating and executing common SQL/-DataFrame operations on tables.The iterative nature of this process enhances intermediate results, empowering LLMs to make predictions through logically visualized reasoning chains.Significantly, Chain-of-Table consistently improves the performance of two benchmark tabular datasets by 8.69% on TabFact and 6.72% on WikiTQ, respectively.</p>
<p>Self-Refine Prompting</p>
<p>Self-Refine prompting, proposed by Madaan et al. [2023], enhances LLM performance by iteratively refining outputs through self-generated feedback, mimicking human revision.While LLMs can handle a wide range of tasks, they often struggle with complex objectives, ambiguous goals, or multi-step reasoning, leading to initial responses with inaccuracies or flawed logic.Inspired by human iterative refinement, Self-Refine enables LLMs to improve their outputs through a structured threestep process: generating an initial response, prompting the model to critique its own output, and refining the response based on this feedback.This cycle continues until predefined stopping criteria are met, allowing the model to produce more accurate and contextually relevant results.Unlike traditional prompting methods, which rely solely on a single-step response, Self-Refine fosters incremental improvement, making it particularly effective for tasks requiring nuanced reasoning.Experimental results demonstrate significant performance gains, with GPT-4 improving by 8.7 points in code optimization, 13.9 points in code readability, and 21.6 points in sentiment reversal tasks, showcasing its potential to enhance the reasoning and adaptability of LLMs across various domains.</p>
<p>Code Prompting</p>
<p>Pre-training on code enhances the reasoning capabilities of LLMs, yet the underlying mechanisms driving this improvement remain poorly understood.To investigate this, Puerto et al. [2024] examines the impact of input representation on LLM reasoning, specifically exploring whether reformulating natural language (NL) problems into code can trigger conditional reasoning abilities.This led to the introduction of Code Prompting, a technique that reformulates NL tasks into structured code, enabling direct prompting of text+code LLMs without relying on external code execution.Experiments on three reasoning benchmarks, ConditionalQA, BoardgameQA, and ShARC, demonstrate that code prompts significantly outperform traditional text-based prompts.On average, GPT 3.5 achieved a performance gain of 8.42 F1 score, while Mistral showed an average improvement of 4.22 across the three datasets.</p>
<p>Self-Harmonized Chain-of-Thought (ECHO) Prompting</p>
<p>While Chain-of-Thought prompting enhances reasoning in LLMs, methods like Auto-CoT, which automate demonstration generation, face challenges from misleading similarity (incorrect rationales in similar examples) and ineffective diversity (irrelevant or overly varied patterns).To address these issues, Mekala et al. [2024] introduced ECHO, a selfharmonized prompting framework that unifies diverse reasoning paths into a coherent pattern, balancing automation with robustness.ECHO operates through three key stages:</p>
<p>(1) Question Clustering, where Sentence-BERT embeddings and k-means group questions into clusters; (2) Demonstration Sampling, which selects representative questions from each cluster and generates rationales using Zero-Shot-CoT; and</p>
<p>(3) Demonstration Unification, where rationales are iteratively refined using a dynamic prompting mechanism to align reasoning patterns.This process minimizes diversity-induced noise while retaining adaptability.ECHO surpassed Auto-CoT by an average of 2.8% across 10 reasoning benchmarks (arithmetic, commonsense, symbolic) while demonstrating greater efficiency.It retained performance with 50% fewer examples, showing only a -0.8% dip compared to Few-Shot-CoT's -1.3% decline.The method also achieved 2.3% gains over Auto-CoT in Mixtral-8x7B, though it remained behind GPT-3.5, a gap attributed to differences in the quality of reasoning rationales.</p>
<p>Logic-of-thought Prompting</p>
<p>LLMs often exhibit unfaithful reasoning, where the generated conclusions diverge from the intermediate reasoning steps.Logic-of-Thought prompting [Liu et al., 2024] is a neurosymbolic framework developed to mitigate this issue by enriching prompts with logical information derived from propositional logic.LoT operates in three phases: (1) Logic Extraction, during which LLMs identify propositions and logical relationships from input texts; (2) Logic Extension, in which a Python-based module applies formal logical laws (e.g., contraposition) to infer additional expressions; and (3) Logic Translation, where the extended logic is rendered back into natural language and appended to the original prompt to ensure contextual fidelity.Moreover, Logic-of-thought is designed to integrate seamlessly with other prompting strategies such as CoT, Self-Consistency, and ToT prompting.Reported evaluations indicate that Logicof-thought can improve CoT accuracy on the ReClor benchmark by 4.35%, enhance CoT prompting with Self-Consistency on LogiQA by 5%, and further boost ToT prompting performance on the ProofWriter dataset by 8%.Additionally, by preserving natural language representations throughout the process, Logic-of-Thought avoids the symbolic extraction errors that can impair other neuro-symbolic systems, such as SatLM.</p>
<p>Instance-adaptive Prompting (IAP) Yuan et al. [2024] tackle the generalization constraints of static task-level prompts (e.g., "Let's think step by step") in zero-shot CoT reasoning by introducing Instance-Adaptive Prompting (IAP), a saliency-driven framework designed to dynamically tailor prompts to individual instances.Through information flow analysis of attention layers, the authors identified distinct patterns: effective reasoning correlates with strong semantic flow from questions to prompts in shallow layers and from integrated question-prompt representations to rationales in deeper layers.In contrast, fragmented or weak flows are indicative of suboptimal reasoning performance.IAP optimizes reasoning fidelity through two adaptive strategies.The first, IAP-ss (Sequential Substitution), enhances efficiency by iteratively testing prompts until predefined saliency thresholds are met.The second, IAP-mv (Majority Vote), prioritizes robustness by aggregating saliency scores across multiple prompts to determine consensus answers.Empirical evaluations underscore the broad applicability of IAP: in mathematical reasoning tasks (GSM8K, SVAMP), IAP-mv boosts the performance of LLaMA-3-8B and Qwen-14B by +1.82% and +3.31%, respectively, compared to static prompts.It achieves 19.25% accuracy on causal judgment tasks, outperforming baselines at 16.04%, and surpasses Self-Discover by +21.7% on MMLU commonsense reasoning with Qwen-14B.</p>
<p>End-to End DAG-Path (EEDP) Prompting End-to-End DAG-Path (EEDP) prompting [Hong et al., 2024] addresses the limitations of traditional graph-flattening methods, such as adjacency lists and edge lists, which struggle with long-distance reasoning in graph-related tasks for LLMs.EEDP's key insight is that conventional flattened representations often lose critical long-range dependencies essential for effective reasoning.To mitigate this, EEDP prioritizes the main backbone paths connecting graph endpoints (nodes with zero in-degree or out-degree) while preserving adjacency lists to maintain local contextual information.The EEDP framework operates through three key stages: (1) preprocessing input graphs into directed acyclic graphs (DAGs) using breadth-first search (BFS) to eliminate cycles, (2) extracting hierarchical paths between endpoints, and (3) compressing shared path segments with a differential pointer algorithm, effectively reducing token length by 55% on molecular graphs.EEDP was evaluated on tasks such as Edge Prediction Connectivity Prediction (EPCP) and Edge Prediction Distance Prediction (EPDP) using educational (Merged_1000) and molecular (ZINC_test_2500) datasets.The evaluation results highlighted significant performance gains over traditional baselines, with EPCP showing a +10.21% accuracy improvement on Merged_1000 and +16.76% on ZINC_test_2500.Similarly, EPDP achieved a +4.73% accuracy boost on Merged_1000 and an impressive +30.13% on ZINC_test_2500.</p>
<p>Layer-of-Thoughts (LoT) Prompting</p>
<p>LLMs demonstrate strong performance in many reasoning tasks yet frequently face challenges with the precision-recall trade-off and explainability, particularly in complex legal retrieval scenarios.Layer-of-Thoughts (LoT) prompting [Fungwacharakorn et al., 2024] introduces a hierarchical framework that leverages constraint hierarchies to structure the reasoning process, thereby enhancing both retrieval accuracy and interpretability.In the context of legal document retrieval, LoT organizes reasoning into "layer thoughts" (conceptual stages) and "option thoughts" (partial solutions), applying sequential constraints to iteratively filter and refine candidate responses.For instance, the framework employs a three-layer process: (1) a Keyword Filtering Layer (KFL) that extracts LLM-generated keywords to initially filter documents using metrics such as at-least-k; (2) a Semantic Filtering Layer (SFL) that prioritizes documents based on multi-level relevance criteria and aggregation metrics; and (3) a Final Confirmation Layer (FCL) that validates the remaining candidates against the original query.By integrating both hard constraints (required) and soft constraints (preferential), LoT not only delivers explainable reasoning but also outperforms state-of-the-art models, for example, achieving an F2 score of 0.835 (with precision of 0.838 and recall of 0.839) on Japanese Civil Law retrieval compared to 0.807 for JNLP, and reaching near-perfect recall (0.966) in German traffic law contexts.</p>
<p>Narrative-of-Thought (NoT) Prompting</p>
<p>Temporal reasoning remains a significant challenge for LLMs, particularly in inferring global temporal relationships from unordered events.To evaluate this capability, Zhang et al. [2024] introduced Temporal Graph Generation (TGG), a benchmark designed to assess LLMs' proficiency in constructing directed acyclic graphs (DAGs) representing event timelines.Experimental results revealed that smaller LLMs (&lt;10B) lagged behind GPT-3.5/4 by approximately 50%, with even GPT-4 facing difficulties due to alignment constraints.To overcome these limitations, the authors proposed Narrative-of-Thought (NOT), a prompting strategy that enhances temporal reasoning without requiring additional model training.NOT comprises three core components: (1) Structural Representation, where events are encapsulated in a Python class and processed through code completion; (2) NOT Prompting template, which generates temporally grounded narratives to guide the construction of temporal graphs; and (3) Narrative-Aware Demonstrations, utilizing GPT-4-generated few-shot examples optimized for both conciseness and accuracy.Results demonstrated that NOT significantly improves the performance of small LLMs, with LLaMA3-8B achieving an F1 score of 42.2, closely matching GPT-3.5's45.7, while exhibiting superior structural coherence.</p>
<p>Buffer of Thoughts (BoT) Prompting</p>
<p>Existing prompting methods often struggle to balance universality, efficiency, and robustness in complex reasoning.To address this, Yang et al. [2024] introduced Buffer of Thoughts (BoT), a framework that enhances LLMs through reusable high-level reasoning patterns.BoT overcomes the limitations of single-query methods (e.g., manual exemplar reliance) and multi-query approaches (e.g., computational inefficiency) by introducing a meta-buffer that distills "thoughttemplates" from diverse tasks and a dynamic buffer-manager that continuously refines them as new problems are solved.BoT retrieves task-specific thought-templates (e.g., structured problem-solving approaches) and adaptively instantiates them, mimicking human analogical reasoning to eliminate manual prompt design and recursive exploration.Experiments across 10 benchmarks demonstrate its state-of-the-art performance, achieving gains of 11% on Game of 24, 20% on Geometric Shapes, and 51% on Checkmate-in-One, while using just 12% of the computational cost of multi-query methods like Treeof-Thoughts.Notably, BoT enhances smaller models, with Llama3-8B + BoT surpassing Llama3-70B in accuracy, showing its potential to democratize efficient reasoning at scale.</p>
<p>Contrastive Denoising with Noisy Chain-of-Thought (CD-CoT) Prompting</p>
<p>Contrastive Denoising with Noisy Chain-of-Thought (CD-CoT) [Zhou et al., 2024] addresses the challenge of "noisy rationales" in chain-of-thought prompting, where irrelevant or incorrect intermediate reasoning steps degrade LLM performance.The NoRa (Noisy Rationales) dataset highlights this issue, showing that LLMs often perform worse with flawed rationales than with no examples at all, as they tend to mimic incorrect reasoning.Existing methods like self-correction and self-consistency offer limited solutions, as self-correction fails without external feedback, and self-consistency selects frequent answers without resolving reasoning flaws.CD-CoT mitigates this by contrasting noisy rationales with clean ones, rephrasing flawed examples, selecting optimal reasoning paths, and voting on the most consistent answer.Experiments show that CD-CoT improves accuracy by 17.8% on average, significantly outperforming baselines and enhancing LLMs' robustness in reasoning-intensive tasks.</p>
<p>Reverse Chain-of-Thought (R-CoT) Prompting Deng et al. [2024] introduced the Reverse Chain-of-Thought (R-CoT) pipeline, a novel approach to enhancing geometric reasoning in LMMs by addressing dataset limitations such as low quality, diversity, and fidelity.R-CoT operates in two stages: GeoChain, which generates high-fidelity geometric images with detailed step-by-step descriptions of geometric relationships (e.g., midlines, radii), and Reverse A&amp;Q, which derives questions from reasoning chains using LLMs, ensuring accurate multi-step problem generation.By prioritizing answer-aware question synthesis, R-CoT mitigates visual hallucinations and reasoning errors in LMMs.The resulting GeoMM dataset includes 20 geometric shapes categorized by complexity, incorporating relational questions often missing in existing datasets like MAVIS and GeomVerse.GeoMM combines high-fidelity images with diverse Q&amp;A pairs, enriched by geometric theorems and line operations.Experimental results demonstrate that R-CoT-trained models achieve state-ofthe-art performance, with the 8B-parameter model surpassing GPT-4o by 12.5% on MathVista and 14.5% on GeoQA, while smaller models (2B, 7B) also set new benchmarks.</p>
<p>Chain of Draft (CoD) Prompting</p>
<p>Chain of Draft (CoD) [Xu et al., 2025], a novel prompting strategy designed to enhance efficiency in complex reasoning tasks.Unlike traditional CoT prompting, which emphasizes detailed step-by-step reasoning, CoD generates concise, informationdense outputs at each step, mirroring human problem-solving strategies where only essential insights are noted.While CoT improves reasoning accuracy, it often leads to verbose outputs and increased computational costs.CoD mitigates this by constraining word usage in each reasoning step, reducing latency and token consumption without sacrificing accuracy.This efficiency-oriented approach is particularly valuable for real-world applications where computational resources and response time are critical.Experimental results across arithmetic, commonsense, and symbolic reasoning benchmarks show that CoD matches or even outperforms CoT in accuracy while significantly lowering token usage and latency.In some cases, CoD achieved comparable accuracy with an 80% reduction in output tokens as well as an average latency reduction of 76.2%, demonstrating its potential as a lightweight yet effective alternative to traditional prompting strategies.</p>
<p>Reduce Hallucination</p>
<p>Retrieval Augmented Generation (RAG) LLMs have revolutionized text generation, yet their reliance on limited, static training data hinders accurate responses, especially in tasks demanding external knowledge.Traditional prompting falls short, requiring expensive retraining.Retrieval Augmented Generation (RAG) [Lewis et al., 2020] emerges as a novel solution, seamlessly weaving information retrieval into the prompting process.RAG analyzes user input, crafts a targeted query, and scours a pre-built knowledge base for relevant resources.Retrieved snippets are incorporated into the original prompt, enriching it with contextual background.The augmented prompt empowers the LLM to generate creative, factually accurate responses.RAG's agility overcomes static limitations, making it a game-changer for tasks requiring upto-date knowledge.RAG outperformed seq2seq models and task-specific architectures on ODQA benchmarks, achieving exact match scores, reaching up to 56.8% on TriviaQA and 44.5% on Natural Questions.</p>
<p>ReAct Prompting</p>
<p>Unlike previous studies that treated reasoning and action separately, ReAct [Yao et al., 2022] enables LLMs to generate reasoning traces and task-specific actions concurrently.This interleaved process enhances synergy between reasoning and action, facilitating the model in inducing, tracking, and updating action plans while handling exceptions.ReAct is applied to diverse language and decision-making tasks, showcasing its effectiveness over state-of-the-art baselines.Notably, in question answering (HotpotQA) and fact verification (Fever), ReAct addresses hallucination and error propagation issues by interacting with a simple Wikipedia API, producing more interpretable task-solving trajectories.Additionally, in interactive decision-making benchmarks like ALFWorld and WebShop, ReAct surpasses both imitation and reinforcement learning approaches, achieving notable success rates of 34% and 10%, respectively, with minimal in-context examples.</p>
<p>Chain-of-Verification (CoVe) Prompting</p>
<p>To address hallucinations in LLMs, Dhuliawala et al. [2023] proposed Chain-of-Verification (CoVe), which involves a systematic four-step process including the model generate baseline responses, plan verification questions to check its work, answer the questions independently, and produce a revised response incorporating the verification.By verifying its work through this deliberate multi-step approach, the LLM enhances logical reasoning abilities and reduces errors even with contradictory information.CoVe emulates human verification to bolster the coherence and precision of LLM output.Experiments on list questions, QA, and long-form generation demonstrate that CoVe decreases hallucinations while maintaining facts [Sahoo et al., 2024].Focused verification questions help models identify and correct their inaccuracies.</p>
<p>Chain-of-Note (CoN) Prompting</p>
<p>Retrieval-augmented language models (RALMs) enhance large language models by incorporating external knowledge to reduce factual hallucination.However, the reliability of retrieved information is not guaranteed, leading to potentially misguided responses.Standard RALMs struggle to assess their knowledge adequacy and often fail to respond with "unknown" when lacking information.To address these challenges, Yu et al. [2023] introduced a novel approach to improve RALMs robustness by handling noisy, irrelevant documents and accurately addressing unknown scenarios.CoN systematically evaluates document relevance, emphasizing critical and reliable information to filter out irrelevant content, resulting in more precise and contextually relevant responses.Testing across diverse open-domain question-answering datasets demonstrated notable improvements, including a +7.9 average boost in exact match scores for noisy retrieved documents and a +10.5 enhancement in rejection rates for questions beyond pre-training knowledge.</p>
<p>Chain-of-Knowledge (CoK) Prompting</p>
<p>Traditional prompting techniques for LLMs have proven powerful in tackling basic tasks.However, their efficacy diminishes due to complex reasoning challenges, often resulting in unreliable outputs plagued by factual hallucinations and opaque thought processes.This limitation arises from their reliance on fixed knowledge sources, ineffective structured query generation, and lack of progressive correction that fails to guide the LLM adequately.Motivated by human problem-solving, CoK [Li et al., 2023d] systematically breaks down intricate tasks into well-coordinated steps.The process initiates with a comprehensive reasoning preparation stage, where the context is established, and the problem is framed.Subsequently, it engages in a dynamic knowledge adaptation phase, meticulously gathering evidence from various sources, such as its internal knowledge base, external databases, and the given prompt.</p>
<p>User Interface</p>
<p>Active Prompting Diao et al. [2023] introduced Active-Prompting as a solution to the challenge of adapting LLMs to diverse reasoning tasks.They address the issue by proposing Active-Prompt to enhance LLMs' performance on complex question-and-answer tasks through task-specific example prompts with chain-of-thought (CoT) reasoning.Unlike existing CoT methods that rely on fixed sets of human-annotated exemplars, Active-Prompt introduces a mechanism for determining the most impactful questions for annotation.Drawing inspiration from uncertaintybased active learning, the method utilizes various metrics to characterize uncertainty and selects the most uncertain questions for annotation.Active-Prompting exhibits superior performance, outperforming self-consistency by an average of 7.0% and 1.8% across eight complex reasoning tasks in textdavinci-002 and code-davinci-002, respectively, showcasing state-of-the-art results.</p>
<p>Fine-Tuning and Optimization</p>
<p>Automatic Prompt Engineer (APE) While crafting effective prompts for LLMs has traditionally been a laborious task for expert annotators, Zhou et al. [2022] introduced Automatic Prompt Engineer (APE) as an innovative approach to automatic instruction generation and selection for LLMs.APE sheds the limitations of static, hand-designed prompts by dynamically generating and selecting the most impactful prompts for specific tasks.This ingenious method analyzes user input, crafts candidate instructions, and then leverages reinforcement learning to choose the optimal prompt, adapting it on the fly to different contexts.Extensive tests on the diverse BIG-Bench suite and the CoT reasoning task revealed APE's prowess, exceeding human-authored prompts in most cases (19 out of 24 tasks) and significantly boosting LLMs reasoning abilities.This breakthrough in automatic prompt engineering paves the way for LLMs to tackle a wider range of tasks with greater efficiency and adaptability, unlocking their full potential across diverse applications.</p>
<p>Knowledge-Based Reasoning and Generation</p>
<p>Automatic Reasoning and Tool-use (ART) The limited reasoning abilities and lack of external tool utilization hinder the potential of LLMs in complex tasks.Paranjape et al. [2023] introduced Automatic Reasoning and Tool-use (ART) to tackle this critical barrier that empowers LLMs to reason through multi-step processes and seamlessly integrate external expertise.ART bridges the reasoning gap, enabling LLMs to tackle complex problems and expand beyond simple text generation.By integrating external tools for specialized knowledge and computations, ART unlocks unprecedented versatility and informs LLM outputs with real-world relevance.This allows LLMs to contribute to diverse fields like scientific research, data analysis, and even decision-making support.Moving beyond traditional prompting techniques, ART automates reasoning steps through structured programs, eliminating the need for laborious hand-crafting.Its dynamic tool integration ensures smooth collaboration, pausing generation to incorporate external tool outputs and seamlessly resuming the flow.Empirical evidence on challenging benchmarks (Big-Bench and MMLU) demonstrates ART's effectiveness, surpassing traditional prompting and even matching hand-crafted demonstrations in some cases.</p>
<p>Improving Consistency and Coherence</p>
<p>Contrastive Chain-of-Thought (CCoT) Prompting Traditional CoT prompting for LLMs often misses a crucial element: learning from mistakes.That is where Contrastive Chain-of-Thought Prompting (CCoT) [Chia et al., 2023] dives in, providing both valid and invalid reasoning demonstrations alongside original prompts.Imagine exploring a map with the right path and the wrong turns to avoid -that is the advantage of contrastive CoT!This dual-perspective approach, tested on reasoning benchmarks like SQuAD and COPA, pushes LLMs to step-by-step reasoning, leading to 4-16% improvements in strategic and mathematical reasoning evaluations compared to traditional CoT, further improved by approximately 5% when integrated with self-consistency techniques.However, questions remain about this technique, such as the automated generation of contrasting demonstrations for diverse problems and its applicability to other NLP tasks beyond reasoning.</p>
<p>Managing Emotions and Tone</p>
<p>Emotion Prompting</p>
<p>While LLMs demonstrate impressive capabilities on various tasks, their ability to comprehend psychological and emotional cues remains uncertain.The study by Li et al. [2023a] addressed the uncertainty surrounding LLMs' ability to comprehend emotional cues by introducing EmotionPrompt.Drawing inspiration from psychological research on language's impact on human performance, they append 11 emotional stimulus sentences to prompts to enhance LLM emotional intelligence.Experimental results demonstrate seamless integration of these stimuli, significantly improving LLM performance across various tasks.EmotionPrompt demonstrates an 8.00% relative improvement in instruction induction and an impressive 115% boost in BIG-Bench tasks, underscoring its efficacy in augmenting LLM capabilities in processing affective signals.An evaluation involving 106 participants reveals an average improvement of 10.9% in performance, truthfulness, and responsibility metrics for generative tasks when employing Emotion-Prompt compared to standard prompts.</p>
<p>Code Generation and Execution</p>
<p>Scratchpad Prompting</p>
<p>Despite the prowess of Transformer-based language models in generating code for basic programming tasks, they encounter challenges in complex, multi-step algorithmic calculations requiring precise reasoning.Addressing this, Nye et al. [2021] introduce a novel approach, centered on task design rather than model modification, introduce a 'scratchpad' concept.The proposal enables the model to generate an arbitrary sequence of intermediate tokens before providing the final answer.Scratchpad Prompting technique outperforms (Mostly Basic Python Programming) MBPP-aug with a 46.8% success rate.Combining CodeNet and single-line datasets yields the highest performance, achieving 26.6% correct final outputs and 24.6% perfect traces.Scratchpad prompting technique faces limitations, including a fixed context window size of 512 tokens and a dependency on supervised learning for scratchpad utilization.</p>
<p>Program of Thoughts (PoT) Prompting</p>
<p>Language models are suboptimal for solving mathematical expressions due to their proneness to arithmetic errors, incapability in handling complex equations, and inefficiency in expressing extensive iterations.To enhance numerical reasoning in language models, Chen et al. [2022] presents Programof-Thoughts (PoT) prompting, advocating the use of external language interpreters for computation steps.PoT enables models like Codex to express reasoning through executable Python programs, resulting in an average performance improvement of approximately 12% compared to CoT prompting on datasets involving mathematical word problems and financial questions.</p>
<p>Structured Chain-of-Thought (SCoT) Prompting LLMs have exhibited impressive proficiency in code generation.The widely used CoT prompting involves producing intermediate natural language reasoning steps before generating code.Despite its efficacy in natural language generation, CoT prompting demonstrates lower accuracy when applied to code generation tasks.Li et al. [2023c] introduce Structured Chain-of-Thought (SCoTs) as an innovative prompting technique tailored specifically for code generation.By incorporating program structures (sequence, branch, and loop structures) into reasoning steps, SCoT prompting enhances LLMs' performance in generating structured source code.This approach explicitly guides LLMs to consider requirements from the source code perspective, improving their overall effectiveness in code generation compared to CoT prompting.The authors validated the effectiveness of SCoT on ChatGPT and Codex across three benchmarks (HumanEval, MBPP, and MBCPP) and demonstrated a superior performance over the CoT prompting by up to 13.79%.</p>
<p>Chain-of-Code (CoC) Prompting</p>
<p>While CoT prompting has proven very effective for enhancing Language models (LMs) semantic reasoning skills, it struggles to handle questions requiring numeric or symbolic reasoning.Li et al. [2023b] introduce Chain-of-Code (CoC) as an extension to improve LM reasoning by leveraging codewriting for both logic and semantic tasks.CoC encourages LMs to format semantic sub-tasks as flexible pseudocode, allowing an interpreter to catch undefined behaviors and simulate them with an "LMulator."Experiments demonstrate CoC's superiority over Chain of Thought and other baselines, achieving an 84% accuracy on BIG-Bench Hard, a 12% gain.CoC proves effective with both large and small models, expanding LMs' ability to correctly answer reasoning questions by incorporating a "think in code" approach.</p>
<p>Optimization and Efficiency</p>
<p>Optimization by Prompting (OPRO) In various domains, optimization is a fundamental process often involving iterative techniques.Yang et al. [2023] introduce Optimization by PROmpting (OPRO), a novel approach that leverages LLMs as optimizers.Unlike traditional methods, OPRO utilizes natural language prompts to iteratively generate solutions based on the problem description, enabling quick adaptation to different tasks and customization of the optimization process.The potential of LLMs for optimization is demonstrated through case studies on classic problems like linear regression and the traveling salesman problem.Additionally, it explores the optimization of prompts to maximize accuracy in natural language processing tasks, highlighting the sensitivity of LLMs.The experiments show that optimizing prompts for accuracy on a small training set effectively translates to high performance on the test set.OPRO leads to a significant performance boost, with the most effective prompts optimized by OPRO outperforming human-designed prompts by up to 8% on the GSM8K dataset and up to 50% on challenging tasks in Big-Bench.</p>
<p>Understanding User Intent</p>
<p>Rephrase and Respond (RaR) Prompting The study by Deng et al. [2023] brings attention to an oftenneglected dimension in exploring LLMs: the disparity between human thought frames and those of LLMs and introduces Rephrase and Respond (RaR).RaR allows LLMs to rephrase and expand questions in a single prompt, demonstrating improved comprehension and response accuracy.The two-step RaR variant, incorporating rephrasing and response LLMs, achieves substantial performance enhancements across various tasks.The study highlights that in contrast to casually posed human queries, the rephrased questions contribute to enhanced semantic clarity and the resolution of inherent ambiguity.These findings offer valuable insights for understanding and enhancing the efficacy of LLMs across various applications.</p>
<p>Metacognition and Self-Reflection</p>
<p>Take a Step Back Prompting Addressing the persistent challenge of complex multi-step reasoning, Zheng et al. [2023] introduced the Step-Back prompting technique, tailored explicitly for advanced language models like PaLM-2L.This innovative approach empowers models to engage in abstraction, extracting high-level concepts and fundamental principles from specific instances.The Step-Back prompting method involves a two-step procedure, integrating Abstraction and Reasoning.Through extensive experiments, applying Step-Back Prompting to PaLM-2L in diverse reasoning-intensive tasks such as STEM, Knowledge QA, and Multi-Hop Reasoning, the results demonstrate a substantial enhancement in reasoning capabilities.Noteworthy performance boosts are observed, with improvements in tasks like MMLU Physics and Chemistry by 7%, TimeQA by 27%, and MuSiQue by 7%.</p>
<p>Figure 1 :
1
Figure 1: Visual breakdown of prompt engineering components: LLMs trained on extensive data, instruction and context as pivotal elements shaping the prompt, and a user input interface.</p>
<p>Figure 2 :
2
Figure 2: Taxonomy of prompt engineering techniques in LLMs, organized around application domains, providing a nuanced framework for customizing prompts across diverse contexts.</p>
<p>Table 1 :
1
Summary of prevalent prompting techniques of LLMs based on the following factors: application, prompt acquisition, prompt turn, language model, dataset, and metrics.
ApplicationPrompting TechniquePrompt Acquisition Prompt TurnLanguage Model(s)Comparison ScopeDatasetMetric(s)
ConclusionIn the domain of artificial intelligence, prompt engineering has become a transformative force, unlocking the vast potential of LLMs.This survey paper aims to serve as a foundational resource that systematically categorizes 41 distinct prompt engineering techniques based on their targeted functionalities, inspiring further research and empowering innovators in the evolving landscape of prompt engineering.The analysis spans applications, models, and datasets, shedding light on the strengths and limitations of each approach.Furthermore, we have added a diagram and a table to highlight the important points.Despite the remarkable successes, challenges persist, including biases, factual inaccuracies, and interpretability gaps, necessitating further investigation and mitigation strategies.The future of prompt engineering holds immense potential, with emerging trends like meta-learning and hybrid prompting architectures promising amplified capabilities.However, ethical considerations are paramount, emphasizing responsible development and deployment to ensure positive integration into our lives.
New Tasks Without Extensive Training §2.1 Zero-shot Prompting. Radford, 2019Few-shot Prompting. Brown et al., 2020</p>
<p>Reasoning and Logic §2.2 Chain-of-Thought (CoT). [ Prompting, Wei, 2022</p>
<p>Automatic Chain-of-Thought (Auto-CoT. Zhang, 2022. 2022</p>
<p>Chain-of-Symbol (CoS). [ Prompting, Zhao, Logical CoT (LogiCoT). 2023. 2023. 2023aTree-of-Thoughts</p>
<p>System 2 Attention Prompting [Weston and Sukhbaatar, 2023] Thread of Thought (ThoT). ; Graph-Of-Thought, [ Prompting, Yao, 2023b. 2023. 2024. 2023Code PromptingPuerto et al., 2024</p>
<p>. [ Prompting, Mekala, 2024. 2024Logic-of-Thought Prompting</p>
<p>End-to End DAG-Path (EEDP) Prompting. Instance-Adaptive Prompting, ; Yuan, 2024. 2024LoTLayer-of-ThoughtsFungwacharakorn et al., 2024</p>
<p>Zhang, Buffer of Thoughts (BoT) Prompting. 2024Narrative-of-Thought (NoT) PromptingYang et al., 2024</p>
<p>Contrastive Denoising with Noisy Chain-of-Thought (CD-CoT) Prompting. Zhou et al., 2024</p>
<p>Reverse Chain-of-Thought. R-CoT</p>
<p>Chain of Draft (CoD) Prompting. [ Prompting, Deng, 2024. 2025</p>
<p>. Lewis , 2020. 2022Reduce Hallucination §2.3 Retrieval Augmented Generation (RAG. ReAct Prompting</p>
<p>Chain-of-Note (CoN) Prompting. Dhuliawala, 2023. 2023</p>
<p>; Chain-Of-Knowledge, [ Prompting, Li, User Interaction §2.4 Active-Prompt. 2023dDiao et al., 2023</p>
<p>Zhou, APEFine-Tuning and Optimization §2.5 Automatic Prompt Engineer. 2022</p>
<p>Knowledge-Based Reasoning and Generation §2. </p>
<p>Automatic Reasoning and Tool-use (ART). §2.7Paranjape et al., 2023</p>
<p>Managing Emotions and Tone §2.8 Emotion Prompting. Chia , 2023. 2023aContrastive Chain-of-Thought Prompting (CCoT</p>
<p>. Code Generation and Execution. 29</p>
<p>. Scratchpad Prompting, [ Nye, 2021</p>
<p>. ; Program Of Thoughts, [ Prompting, Chen, Structured Chain-of-Thought. 2022PoT</p>
<p>Chain of Code (CoC). [ Prompting, Li, Optimization and Efficiency §2.10 Optimization by Prompting. 2023c. 2023b. 2023</p>
<p>Understanding User Intent §2.11 Rephrase and Respond (RaR) Prompting [Deng et al., 2023] Metacognition and Self-Reflection §2.12 Take a Step Back Prompting. Zheng et al., 2023</p>
<p>Exploring visual prompts for adapting largescale models. Ali References Hyojin Bahng, Swami Jahanian, Phillip Sankaranarayanan, Isola, arXiv:2203.172742022arXiv preprint</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Unleashing the potential of prompt engineering in large language models: a comprehensive review. Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, Shengxin Zhu, arXiv:2310.147352023arXiv preprint</p>
<p>Contrastive chain-of-thought prompting. Ken Yew, Guizhen Chia, Chen, Anh Luu, Soujanya Tuan, Lidong Poria, Bing, arXiv:2311.092772023arXiv preprint</p>
<p>Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu, arXiv:2311.04205Rephrase and respond: Let large language models ask better questions for themselves. 2023arXiv preprint</p>
<p>. Linger Deng, Yuliang Liu, Bohan Li, Dongliang Luo, Liang Wu, Chengquan Zhang, Pengyuan Lyu, Ziyang Zhang, Gang Zhang, Errui Ding, Yingying Zhu, and Xiang Bai</p>
<p>Reverse chain-of-thought problem generation for geometric reasoning in large multimodal models. R-Cot , arXiv:2309.11495Chain-of-verification reduces hallucination in large language models. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston, 2024. 2023arXiv preprint</p>
<p>Active prompting with chain-of-thought for large language models. Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang, arXiv:2302.122462023arXiv preprint</p>
<p>Layer-of-thoughts prompting (lot): Leveraging llm-based retrieval with constraint hierarchies. Wachara Fungwacharakorn, Ha Nguyen, May Thanh, Ken Myo Zin, Satoh, 2024</p>
<p>End-to-end graph flattening method for large language models. Bin Hong, Jinze Wu, Jiayu Liu, Liang Ding, Jing Sha, Kai Zhang, Shijin Wang, Zhenya Huang, 2024</p>
<p>Chain-of-symbol prompting elicits planning in large langauge models. Hanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, Yue Zhang, 2023</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Large language models understand and can be enhanced by emotional stimuli. Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie, arXiv:2307.117602023arXiv preprint</p>
<p>Chain of code: Reasoning with a language model-augmented code emulator. Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, Brian Ichter, arXiv:2312.044742023arXiv preprint</p>
<p>Structured chainof-thought prompting for code generation. Jia Li, Ge Li, Yongmin Li, Zhi Jin, arXiv:2305.065992023arXiv preprint</p>
<p>Chain-ofknowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources. Xingxuan Li, Ruochen Zhao, Ken Yew, Bosheng Chia, Shafiq Ding, Soujanya Joty, Lidong Poria, Bing, 2023</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Logic-of-thought: Injecting logic into contexts for full reasoning in large language models. Tongxuan Liu, Wenjiang Xu, Weizhe Huang, Xingyu Wang, Jiaxing Wang, Hailong Yang, Jing Li, 2024</p>
<p>Large language model guided tree-of-thought. Jieyi Long, arXiv:2305.082912023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, 2023</p>
<p>Echoprompt: Instructing the model to rephrase queries for improved in-context learning. Rajasekhar Reddy Mekala, Yasaman Razeghi, Sameer Singh, 2024</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint</p>
<p>Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , arXiv:2303.09014Automatic multi-step reasoning and tool-use for large language models. Art2023arXiv preprint</p>
<p>Code prompting elicits conditional reasoning abilities in text+code llms. Haritz Puerto, Martin Tutek, Somak Aditya, Xiaodan Zhu, Iryna Gurevych, 2024</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>A comprehensive survey of hallucination in large language, image, video and audio foundation models. Pranab Sahoo, Prabhash Meharia, Akash Ghosh, Sriparna Saha, Vinija Jain, Aman Chadha, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Sm Tonmoy, Vinija Zaman, Anku Jain, Rani, Aman Vipula Rawte, Amitava Chadha, Das, arXiv:2401.01313A comprehensive survey of hallucination mitigation techniques in large language models. 2024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-table: Evolving tables in the reasoning chain for table understanding. Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister, 2024</p>
<p>Chainof-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>System 2 attention (is something you might need too). Jason Weston, Sainbayar Sukhbaatar, arXiv:2311.118292023arXiv preprint</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, 2023</p>
<p>Chain of draft: Thinking faster by writing less. Silei Xu, Wenhao Xie, Lingxiao Zhao, Pengcheng He, 2025</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, arXiv:2309.03409Large language models as optimizers. 2023arXiv preprint</p>
<p>Buffer of thoughts: Thought-augmented reasoning with large language models. Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, Advances in Neural Information Processing Systems. 2024</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. Yao Yao, Zuchao Li, Hai Zhao, arXiv:2305.165822023arXiv preprint</p>
<p>Chain-of-note: Enhancing robustness in retrieval-augmented language models. Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu, 2023</p>
<p>Instance-adaptive zero-shot chain-of-thought prompting. Xiaosong Yuan, Chen Shen, Shaotian Yan, Xiaofeng Zhang, Liang Xie, Wenxiao Wang, Renchu Guan, Ying Wang, Jieping Ye, 2024</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022arXiv preprint</p>
<p>Narrative-of-thought: Improving temporal reasoning of large language models via recounted narratives. Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang, 2024</p>
<p>Enhancing zero-shot chain-of-thought reasoning in large language models through logic. Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter, 2023</p>
<p>Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed H Cheng, Quoc V Chi, Denny Le, Zhou, arXiv:2310.06117Take a step back: evoking reasoning via abstraction in large language models. 2023arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022arXiv preprint</p>
<p>Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, Jianbing Shen, arXiv:2311.08734Thread of thought unraveling chaotic contexts. 2023arXiv preprint</p>
<p>Can language models perform robust reasoning in chain-of-thought prompting with noisy rationales?. Zhanke Zhou, Rong Tao, Jianing Zhu, Yiwen Luo, Zengmao Wang, Bo Han, Advances in Neural Information Processing Systems. A Globerson, L Mackey, D Belgrave, A Fan, U Paquet, J Tomczak, C Zhang, Curran Associates, Inc202437</p>            </div>
        </div>

    </div>
</body>
</html>
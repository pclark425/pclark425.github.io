<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-279 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-279</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-279</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-267750563</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.11194v2.pdf" target="_blank">Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with a hybrid of structured tables and unstructured text remain uncertain. This study explores LLMs’ mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs’ capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique EEDP tailored to semi-structured documents, matching or outper-forming baselines performance while providing a nuanced understanding of LLMs abilities.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e279.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e279.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art instruction-tuned large language model evaluated in this work across financial tabular QA tasks; shows the strongest arithmetic and multi-step reasoning performance among models tested.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic (composed sequences of atomic ops like add/sub/mul/div, ratios/percentages)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Evaluated on arithmetic operands spanning orders-of-magnitude grouped by τ from -6 to 6; also tested on real financial table values including large magnitudes and decimals</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot prompting variants: Direct, Chain-of-Thought (CoT), Program-of-Thoughts (PoT - program output executed externally), Decomposers, and EEDP (Elicit→Extract→Decompose→Predict)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On financial tabular QA (EEDP): TATQA 88.67% , FinQA 76.05% , ConvFinQA 77.91% , Multihiertt 70.32% (EEDP). CoT and PoT also strong (e.g., TATQA CoT 86.91%, PoT 89.99%); synthetic elementary ops: far better on addition/subtraction than multiplication/division (qualitative summary in Appendix A.1).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Paper reports behavioral/engineering-level observations: LLMs 'employ mathematical algorithms' learned from training data to compute, but do not provide low-level mechanistic internals (no head-level or circuit-level analysis). PoT outputs programs executed externally (avoids LLM internal numeric execution), while EEDP has the LLM both extract and compute internally; no direct claim of specialized attention heads or internal carry-tracking mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Larger/capable models (GPT-4, GPT-3.5-Turbo, PaLM-2-540B) perform better, especially on addition/subtraction; performance degrades with increasing number of reasoning steps, with increasing order-of-magnitude extremes, with more decimal precision, and with hierarchical table complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Errors include incorrect extraction of evidence, insufficient domain knowledge (wrong formula choice), question misinterpretation, incorrect instantiation of variables, and precision errors; arithmetic-specific failures: multiplication/division and ratio/percentage questions and extreme orders-of-magnitude are harder than addition/subtraction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across prompting methods: Direct vs CoT vs PoT vs Decomposers vs EEDP; also compared across models (GPT-4 vs GPT-3.5-Turbo vs PaLM-2 vs LLaMA-2 etc.) and vs external program execution (PoT uses external execution).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-4 attains the best arithmetic/multi-step performance in these financial table tasks, excels at addition/subtraction but still degrades on multiplication/division and with many reasoning steps; EEDP yields competitive or superior results without external executors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e279.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e279.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned LLM from OpenAI evaluated here; performs well on simpler arithmetic and benefits from chain-of-thought and EEDP prompting, but lags behind GPT-4 on harder operations and multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Same synthetic benchmark grouping by τ (orders-of-magnitude -6 to 6) and real financial table numbers including large magnitudes and decimals</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Direct, CoT, PoT, Decomposers, EEDP (few-shot setups with k typically 2 or 4 depending on context length)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>EEDP accuracies reported: TATQA 79.73%, FinQA 61.88%, ConvFinQA 61.75%, Multihiertt 49.65%. CoT and PoT often improve results vs Direct. On synthetic ops: much better on addition/subtraction than multiplication/division.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Same high-level observation: LLM performs arithmetic using patterns/algorithms encoded in weights; no mechanistic circuit-level explanation provided. When prompted to produce programs (PoT), computation can be offloaded externally to exact executors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Per paper, larger/stronger models do better; GPT-3.5-Turbo is intermediate between GPT-4 and smaller open models. Performance declines as number of reasoning steps increases or with harder ops (mul/div).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Precision errors, incorrect instantiation, difficulty with multiplication/division and multistep chaining, and extraction failures from complex/hierarchical tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-4, PaLM-2-540B, LLaMA-2-13B, MAmmoTH-13B, Mistral-7B across prompting methods; CoT/PoT/EEDP vs Direct.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-3.5-Turbo benefits from reasoning-focused prompting (CoT, EEDP) and handles simple arithmetic well but shows degraded accuracy on complex multi-step arithmetic and harder operations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e279.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e279.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-2-540B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 (540B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer LLM (540B) evaluated on financial tabular QA and synthetic arithmetic; shows strong gains with EEDP prompting and exhibits the same operational weaknesses on multiplication/division and multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2-540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, ratios/percentages, multi-step arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Evaluated on orders-of-magnitude τ ∈ [-6,6] synthetic groups and real financial numbers (large magnitudes and decimals); also multistep reasoning chains of varying length</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Direct, CoT, PoT, Decomposers, and EEDP (few-shot with shot count adjusted to context length)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>EEDP accuracies: TATQA 81.51%, FinQA 61.95%, ConvFinQA 63.42%, Multihiertt 37.97%. Noted as one of the models where EEDP outperforms PoT significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Paper reports behavioral observations: PaLM-2 computes internally under EEDP; no internal mechanistic decomposition (no attention/head analysis). Fine-tuning and pretraining data influence arithmetic strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Large parameter count correlates with better arithmetic on simple ops; still experiences drop on multiplication/division and with many reasoning steps or hierarchical evidence extraction challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggles on division/percentage type questions, orders-of-magnitude extremes, and when required to extract evidence from deep hierarchical tables; precision/instantiation errors also observed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT models, LLaMA-2-13B, MAmmoTH-13B, Mistral-7B across prompting strategies; EEDP vs PoT highlighted.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>PaLM-2-540B benefits substantially from EEDP prompting; large models can match/exceed PoT without external executors but still struggle on multiplication/division and long reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e279.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e279.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 (13B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 13B instruction-following model included in experiments; shows much weaker arithmetic and multi-step reasoning performance compared to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Same synthetic grouping by τ and real financial table numbers; limited context-shot count (2 shots for constrained context)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Direct, CoT, PoT, Decomposers, and EEDP (few-shot; fewer shots when context limited)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>EEDP accuracies reported: TATQA 40.95%, FinQA 30.47%, ConvFinQA 39.42%, Multihiertt 24.15%; CoT/PoT yield modest gains but remain far below SOTA.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No mechanistic detail beyond that smaller models lack capacity/precision for reliable arithmetic in complex settings; fine-tuning and prompt formatting affect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance substantially lower than larger models; accuracy degrades quickly with more reasoning steps and complex table structures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Even simple calculations can produce incorrect results due to imprecise internal arithmetic; extraction and multistep reasoning errors common.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked against larger models and different prompting approaches; shown to underperform on arithmetic-heavy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Smaller models like LLaMA-2-13B are markedly weaker on arithmetic and multistep table-based reasoning; prompting helps but cannot fully bridge the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e279.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e279.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAmmoTH-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAmmoTH (13B) - math-focused instruction-tuned model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B model fine-tuned specifically on MathInstruct-style data designed to improve math capabilities; shows mixed results, sometimes underperforming when EEDP prompts are long or stylistically mismatched.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MAmmoTH: Building math generalist models through hybrid instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAmmoTH-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic; trained on CoT/PoT style rationales</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Evaluated on same synthetic and financial tasks; trained on MathInstruct (hybrid CoT/PoT responses)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Direct, CoT, PoT, Decomposers, and EEDP in experiments; model is fine-tuned pre-experiment on math instruction data</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>EEDP performance variable and sometimes poor: TATQA EEDP 4.37% (very low in TATQA under EEDP for reasons reported), FinQA 35.05%, ConvFinQA 32.77%, Multihiertt 18.36%. PoT/CoT sometimes yield better results depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Paper hypothesizes two reasons for poor EEDP performance: (a) reduction in number of shots to fit longer EEDP responses (context constraints), and (b) finetuning aligns model to particular response styles (CoT/PoT hybrid), reducing adaptability to EEDP format — suggesting that fine-tuning changes how models prefer to reason/format outputs but no internal mechanistic circuits are described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Fine-tuning on math data helps on some tasks but may hurt when the deployment prompting style differs; overall not as robust as much larger, non-finetuned models on these tabular arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Style/format mismatch with prompts, sensitivity to shot count/context length, and standard arithmetic issues (instantiation/precision errors).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT family, PaLM-2, LLaMA-2 and Mistral under CoT/PoT/EEDP; performs inconsistently, sometimes worse than non-finetuned larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Math-targeted fine-tuning (MAmmoTH) does not guarantee superior performance on complex, semi-structured financial arithmetic tasks and can underperform when prompt style or context constraints differ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e279.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e279.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B open-source instruction-tuned model included in experiments; shows modest arithmetic capability and benefits from reasoning prompts but lags compared to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Evaluated on synthetic orders-of-magnitude groups and financial table numbers; limited by smaller capacity</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Direct, CoT, PoT, Decomposers, EEDP (few-shot; shot count adjusted)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>EEDP accuracies reported (approx): TATQA 56.06%, FinQA 34.86%, ConvFinQA 36.57%, Multihiertt ~11-12% (varies). CoT/PoT sometimes help but remain behind larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No internal mechanistic claims; behaves like other smaller models with limited arithmetic precision and multistep reasoning capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Lower capacity results in lower accuracy, especially as complexity (# reasoning steps, hierarchy depth, magnitude extremes) increases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Precision and instantiation errors, extraction errors from complex tables, poor performance on multiplication/division and long chains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked across prompting strategies and against larger models; shown underperforming in arithmetic-heavy scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Mistral-7B benefits from structured prompting but is limited by model capacity and struggles with complex arithmetic and multi-hop extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e279.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e279.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EEDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicit → Extract → Decompose → Predict (EEDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel single-prompt prompting strategy introduced in this paper that elicits domain knowledge, extracts supporting evidence, decomposes the problem into atomic operations, and predicts the final answer using only the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (evaluated across models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>designed for multi-step arithmetic: addition, subtraction, multiplication, division, ratios/percentages, and composition of atomic ops</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Applied to problems spanning orders-of-magnitude (τ = -6..6), multi-step sequences, multi-cell table evidence with hierarchical depth and decimal values</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Single-shot/few-shot prompting pipeline: (1) Elicit domain knowledge, (2) Extract relevant evidence from table/text, (3) Decompose into atomic operations (Add, Subtract, Multiply, Divide, Greater, Min, Max, Exp), (4) Predict final answer — all executed inside the LLM (no external executor)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>EEDP matched or outperformed PoT and other baselines across multiple models/datasets: e.g., GPT-4 EEDP TATQA 88.67% close to PoT 89.99%; for PaLM-2-540B EEDP often significantly outperforms PoT (e.g., TATQA EEDP 81.51% vs PoT 61.60%). EEDP is computationally efficient (single API call per query).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>EEDP forces explicit elicitation of domain knowledge and decomposition into atomic operations, improving evidence grounding and structured computation; contrasts with PoT where model outputs program executed externally — EEDP keeps computation internal, so arithmetic accuracy depends on the LLM's internal numeric computation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>EEDP benefits larger models more (they can both decompose and compute reliably); for small/fine-tuned models (e.g., MAmmoTH-13B) EEDP can underperform due to context/shot reduction or style mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When extraction step fails (missing/wrong evidences) or when internal computation is imprecise (precision/instantiation errors), EEDP yields incorrect answers; long EEDP outputs can force fewer demonstration shots, hurting models sensitive to shots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to Direct, CoT, PoT (external execution), and Decomposer prompting methods; EEDP often matches or exceeds PoT and CoT while requiring only one API call and no external executor.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>EEDP is an effective single-prompt method that structures arithmetic reasoning (elicitation, extraction, decomposition, prediction) and often matches or outperforms PoT/CoT without external execution, though its gains depend on model capacity and prompt-length tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e279.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e279.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic Arithmetic Benchmark (Appendix A.1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-generated synthetic arithmetic expressions grouped by order-of-magnitude τ</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2600-problem synthetic evaluation constructed in Appendix A.1: 650 problems per operation (add, sub, mul, div) grouped into τ bins (-6..6) to study effect of operand order-of-magnitude on arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated across multiple LLMs (GPT-4 used to generate problems)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (single-step arithmetic tasks varying magnitudes)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Groups with τ (order-of-magnitude of largest operand) from -6 to +6, 50 expressions per group per operation; includes very small to very large operands and decimals</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Synthetic benchmark generated by GPT-4 and used to probe models in isolation (no table evidence); performance measured as accuracy per op per τ bin</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative summary: addition and subtraction are relatively robust across τ; multiplication and division performance drops substantially at extreme orders-of-magnitude; large models (GPT-4, GPT-3.5-Turbo, PaLM-2-540B) outperform smaller models on these synthetic tasks (exact percentages not tabulated in main text but trends shown in Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Findings suggest models are more reliable for simpler associative ops (add/sub) even across magnitude variation, while algorithmic/precision-sensitive ops (mul/div) reveal limitations, implying LLMs lack robust general-purpose numeric algorithms for precise multiplication/division across scales.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Bigger models show substantially better accuracy on synthetic arithmetic, especially on non-trivial magnitudes; error rates rise with extreme τ.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Errors increase for multiplication/division at high and low τ, precision loss with decimals and large magnitudes, and off-by-small amounts indicating limited exact numeric fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used to compare operation types and model sizes; also used to analyze effect of operand order-of-magnitude on accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Synthetic evaluation shows consistent, operation-dependent failure patterns: addition/subtraction robust across magnitudes but multiplication/division and extreme orders-of-magnitude expose substantial weaknesses in LLM arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>MAmmoTH: Building math generalist models through hybrid instruction tuning <em>(Rating: 2)</em></li>
                <li>Are NLP models really able to solve simple math word problems? <em>(Rating: 1)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Program of thoughts (PoT) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-279",
    "paper_id": "paper-267750563",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A state-of-the-art instruction-tuned large language model evaluated in this work across financial tabular QA tasks; shows the strongest arithmetic and multi-step reasoning performance among models tested.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic (composed sequences of atomic ops like add/sub/mul/div, ratios/percentages)",
            "number_range_or_complexity": "Evaluated on arithmetic operands spanning orders-of-magnitude grouped by τ from -6 to 6; also tested on real financial table values including large magnitudes and decimals",
            "method_or_intervention": "few-shot prompting variants: Direct, Chain-of-Thought (CoT), Program-of-Thoughts (PoT - program output executed externally), Decomposers, and EEDP (Elicit→Extract→Decompose→Predict)",
            "performance_result": "On financial tabular QA (EEDP): TATQA 88.67% , FinQA 76.05% , ConvFinQA 77.91% , Multihiertt 70.32% (EEDP). CoT and PoT also strong (e.g., TATQA CoT 86.91%, PoT 89.99%); synthetic elementary ops: far better on addition/subtraction than multiplication/division (qualitative summary in Appendix A.1).",
            "mechanistic_insight": "Paper reports behavioral/engineering-level observations: LLMs 'employ mathematical algorithms' learned from training data to compute, but do not provide low-level mechanistic internals (no head-level or circuit-level analysis). PoT outputs programs executed externally (avoids LLM internal numeric execution), while EEDP has the LLM both extract and compute internally; no direct claim of specialized attention heads or internal carry-tracking mechanisms.",
            "performance_scaling": "Larger/capable models (GPT-4, GPT-3.5-Turbo, PaLM-2-540B) perform better, especially on addition/subtraction; performance degrades with increasing number of reasoning steps, with increasing order-of-magnitude extremes, with more decimal precision, and with hierarchical table complexity.",
            "failure_modes": "Errors include incorrect extraction of evidence, insufficient domain knowledge (wrong formula choice), question misinterpretation, incorrect instantiation of variables, and precision errors; arithmetic-specific failures: multiplication/division and ratio/percentage questions and extreme orders-of-magnitude are harder than addition/subtraction.",
            "comparison_baseline": "Compared across prompting methods: Direct vs CoT vs PoT vs Decomposers vs EEDP; also compared across models (GPT-4 vs GPT-3.5-Turbo vs PaLM-2 vs LLaMA-2 etc.) and vs external program execution (PoT uses external execution).",
            "key_finding": "GPT-4 attains the best arithmetic/multi-step performance in these financial table tasks, excels at addition/subtraction but still degrades on multiplication/division and with many reasoning steps; EEDP yields competitive or superior results without external executors.",
            "uuid": "e279.0",
            "source_info": {
                "paper_title": "Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo",
            "brief_description": "An instruction-tuned LLM from OpenAI evaluated here; performs well on simpler arithmetic and benefits from chain-of-thought and EEDP prompting, but lags behind GPT-4 on harder operations and multi-step tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic (as above)",
            "number_range_or_complexity": "Same synthetic benchmark grouping by τ (orders-of-magnitude -6 to 6) and real financial table numbers including large magnitudes and decimals",
            "method_or_intervention": "Direct, CoT, PoT, Decomposers, EEDP (few-shot setups with k typically 2 or 4 depending on context length)",
            "performance_result": "EEDP accuracies reported: TATQA 79.73%, FinQA 61.88%, ConvFinQA 61.75%, Multihiertt 49.65%. CoT and PoT often improve results vs Direct. On synthetic ops: much better on addition/subtraction than multiplication/division.",
            "mechanistic_insight": "Same high-level observation: LLM performs arithmetic using patterns/algorithms encoded in weights; no mechanistic circuit-level explanation provided. When prompted to produce programs (PoT), computation can be offloaded externally to exact executors.",
            "performance_scaling": "Per paper, larger/stronger models do better; GPT-3.5-Turbo is intermediate between GPT-4 and smaller open models. Performance declines as number of reasoning steps increases or with harder ops (mul/div).",
            "failure_modes": "Precision errors, incorrect instantiation, difficulty with multiplication/division and multistep chaining, and extraction failures from complex/hierarchical tables.",
            "comparison_baseline": "Compared to GPT-4, PaLM-2-540B, LLaMA-2-13B, MAmmoTH-13B, Mistral-7B across prompting methods; CoT/PoT/EEDP vs Direct.",
            "key_finding": "GPT-3.5-Turbo benefits from reasoning-focused prompting (CoT, EEDP) and handles simple arithmetic well but shows degraded accuracy on complex multi-step arithmetic and harder operations.",
            "uuid": "e279.1",
            "source_info": {
                "paper_title": "Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PaLM-2-540B",
            "name_full": "PaLM 2 (540B parameters)",
            "brief_description": "A large transformer LLM (540B) evaluated on financial tabular QA and synthetic arithmetic; shows strong gains with EEDP prompting and exhibits the same operational weaknesses on multiplication/division and multi-step reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM-2-540B",
            "model_size": "540B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, ratios/percentages, multi-step arithmetic",
            "number_range_or_complexity": "Evaluated on orders-of-magnitude τ ∈ [-6,6] synthetic groups and real financial numbers (large magnitudes and decimals); also multistep reasoning chains of varying length",
            "method_or_intervention": "Direct, CoT, PoT, Decomposers, and EEDP (few-shot with shot count adjusted to context length)",
            "performance_result": "EEDP accuracies: TATQA 81.51%, FinQA 61.95%, ConvFinQA 63.42%, Multihiertt 37.97%. Noted as one of the models where EEDP outperforms PoT significantly.",
            "mechanistic_insight": "Paper reports behavioral observations: PaLM-2 computes internally under EEDP; no internal mechanistic decomposition (no attention/head analysis). Fine-tuning and pretraining data influence arithmetic strategies.",
            "performance_scaling": "Large parameter count correlates with better arithmetic on simple ops; still experiences drop on multiplication/division and with many reasoning steps or hierarchical evidence extraction challenges.",
            "failure_modes": "Struggles on division/percentage type questions, orders-of-magnitude extremes, and when required to extract evidence from deep hierarchical tables; precision/instantiation errors also observed.",
            "comparison_baseline": "Compared against GPT models, LLaMA-2-13B, MAmmoTH-13B, Mistral-7B across prompting strategies; EEDP vs PoT highlighted.",
            "key_finding": "PaLM-2-540B benefits substantially from EEDP prompting; large models can match/exceed PoT without external executors but still struggle on multiplication/division and long reasoning chains.",
            "uuid": "e279.2",
            "source_info": {
                "paper_title": "Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-2-13B",
            "name_full": "LLaMA 2 (13B parameters)",
            "brief_description": "An open-source 13B instruction-following model included in experiments; shows much weaker arithmetic and multi-step reasoning performance compared to larger models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B",
            "model_size": "13B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic",
            "number_range_or_complexity": "Same synthetic grouping by τ and real financial table numbers; limited context-shot count (2 shots for constrained context)",
            "method_or_intervention": "Direct, CoT, PoT, Decomposers, and EEDP (few-shot; fewer shots when context limited)",
            "performance_result": "EEDP accuracies reported: TATQA 40.95%, FinQA 30.47%, ConvFinQA 39.42%, Multihiertt 24.15%; CoT/PoT yield modest gains but remain far below SOTA.",
            "mechanistic_insight": "No mechanistic detail beyond that smaller models lack capacity/precision for reliable arithmetic in complex settings; fine-tuning and prompt formatting affect outputs.",
            "performance_scaling": "Performance substantially lower than larger models; accuracy degrades quickly with more reasoning steps and complex table structures.",
            "failure_modes": "Even simple calculations can produce incorrect results due to imprecise internal arithmetic; extraction and multistep reasoning errors common.",
            "comparison_baseline": "Benchmarked against larger models and different prompting approaches; shown to underperform on arithmetic-heavy tasks.",
            "key_finding": "Smaller models like LLaMA-2-13B are markedly weaker on arithmetic and multistep table-based reasoning; prompting helps but cannot fully bridge the gap.",
            "uuid": "e279.3",
            "source_info": {
                "paper_title": "Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MAmmoTH-13B",
            "name_full": "MAmmoTH (13B) - math-focused instruction-tuned model",
            "brief_description": "A 13B model fine-tuned specifically on MathInstruct-style data designed to improve math capabilities; shows mixed results, sometimes underperforming when EEDP prompts are long or stylistically mismatched.",
            "citation_title": "MAmmoTH: Building math generalist models through hybrid instruction tuning",
            "mention_or_use": "use",
            "model_name": "MAmmoTH-13B",
            "model_size": "13B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic; trained on CoT/PoT style rationales",
            "number_range_or_complexity": "Evaluated on same synthetic and financial tasks; trained on MathInstruct (hybrid CoT/PoT responses)",
            "method_or_intervention": "Direct, CoT, PoT, Decomposers, and EEDP in experiments; model is fine-tuned pre-experiment on math instruction data",
            "performance_result": "EEDP performance variable and sometimes poor: TATQA EEDP 4.37% (very low in TATQA under EEDP for reasons reported), FinQA 35.05%, ConvFinQA 32.77%, Multihiertt 18.36%. PoT/CoT sometimes yield better results depending on dataset.",
            "mechanistic_insight": "Paper hypothesizes two reasons for poor EEDP performance: (a) reduction in number of shots to fit longer EEDP responses (context constraints), and (b) finetuning aligns model to particular response styles (CoT/PoT hybrid), reducing adaptability to EEDP format — suggesting that fine-tuning changes how models prefer to reason/format outputs but no internal mechanistic circuits are described.",
            "performance_scaling": "Fine-tuning on math data helps on some tasks but may hurt when the deployment prompting style differs; overall not as robust as much larger, non-finetuned models on these tabular arithmetic tasks.",
            "failure_modes": "Style/format mismatch with prompts, sensitivity to shot count/context length, and standard arithmetic issues (instantiation/precision errors).",
            "comparison_baseline": "Compared to GPT family, PaLM-2, LLaMA-2 and Mistral under CoT/PoT/EEDP; performs inconsistently, sometimes worse than non-finetuned larger models.",
            "key_finding": "Math-targeted fine-tuning (MAmmoTH) does not guarantee superior performance on complex, semi-structured financial arithmetic tasks and can underperform when prompt style or context constraints differ.",
            "uuid": "e279.4",
            "source_info": {
                "paper_title": "Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mistral-7B-Instruct",
            "name_full": "Mistral 7B Instruct",
            "brief_description": "A 7B open-source instruction-tuned model included in experiments; shows modest arithmetic capability and benefits from reasoning prompts but lags compared to larger models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic",
            "number_range_or_complexity": "Evaluated on synthetic orders-of-magnitude groups and financial table numbers; limited by smaller capacity",
            "method_or_intervention": "Direct, CoT, PoT, Decomposers, EEDP (few-shot; shot count adjusted)",
            "performance_result": "EEDP accuracies reported (approx): TATQA 56.06%, FinQA 34.86%, ConvFinQA 36.57%, Multihiertt ~11-12% (varies). CoT/PoT sometimes help but remain behind larger models.",
            "mechanistic_insight": "No internal mechanistic claims; behaves like other smaller models with limited arithmetic precision and multistep reasoning capacity.",
            "performance_scaling": "Lower capacity results in lower accuracy, especially as complexity (# reasoning steps, hierarchy depth, magnitude extremes) increases.",
            "failure_modes": "Precision and instantiation errors, extraction errors from complex tables, poor performance on multiplication/division and long chains.",
            "comparison_baseline": "Benchmarked across prompting strategies and against larger models; shown underperforming in arithmetic-heavy scenarios.",
            "key_finding": "Mistral-7B benefits from structured prompting but is limited by model capacity and struggles with complex arithmetic and multi-hop extraction.",
            "uuid": "e279.5",
            "source_info": {
                "paper_title": "Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "EEDP",
            "name_full": "Elicit → Extract → Decompose → Predict (EEDP)",
            "brief_description": "A novel single-prompt prompting strategy introduced in this paper that elicits domain knowledge, extracts supporting evidence, decomposes the problem into atomic operations, and predicts the final answer using only the LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs (evaluated across models)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "designed for multi-step arithmetic: addition, subtraction, multiplication, division, ratios/percentages, and composition of atomic ops",
            "number_range_or_complexity": "Applied to problems spanning orders-of-magnitude (τ = -6..6), multi-step sequences, multi-cell table evidence with hierarchical depth and decimal values",
            "method_or_intervention": "Single-shot/few-shot prompting pipeline: (1) Elicit domain knowledge, (2) Extract relevant evidence from table/text, (3) Decompose into atomic operations (Add, Subtract, Multiply, Divide, Greater, Min, Max, Exp), (4) Predict final answer — all executed inside the LLM (no external executor)",
            "performance_result": "EEDP matched or outperformed PoT and other baselines across multiple models/datasets: e.g., GPT-4 EEDP TATQA 88.67% close to PoT 89.99%; for PaLM-2-540B EEDP often significantly outperforms PoT (e.g., TATQA EEDP 81.51% vs PoT 61.60%). EEDP is computationally efficient (single API call per query).",
            "mechanistic_insight": "EEDP forces explicit elicitation of domain knowledge and decomposition into atomic operations, improving evidence grounding and structured computation; contrasts with PoT where model outputs program executed externally — EEDP keeps computation internal, so arithmetic accuracy depends on the LLM's internal numeric computation capability.",
            "performance_scaling": "EEDP benefits larger models more (they can both decompose and compute reliably); for small/fine-tuned models (e.g., MAmmoTH-13B) EEDP can underperform due to context/shot reduction or style mismatch.",
            "failure_modes": "When extraction step fails (missing/wrong evidences) or when internal computation is imprecise (precision/instantiation errors), EEDP yields incorrect answers; long EEDP outputs can force fewer demonstration shots, hurting models sensitive to shots.",
            "comparison_baseline": "Compared directly to Direct, CoT, PoT (external execution), and Decomposer prompting methods; EEDP often matches or exceeds PoT and CoT while requiring only one API call and no external executor.",
            "key_finding": "EEDP is an effective single-prompt method that structures arithmetic reasoning (elicitation, extraction, decomposition, prediction) and often matches or outperforms PoT/CoT without external execution, though its gains depend on model capacity and prompt-length tradeoffs.",
            "uuid": "e279.6",
            "source_info": {
                "paper_title": "Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Synthetic Arithmetic Benchmark (Appendix A.1)",
            "name_full": "GPT-4-generated synthetic arithmetic expressions grouped by order-of-magnitude τ",
            "brief_description": "A 2600-problem synthetic evaluation constructed in Appendix A.1: 650 problems per operation (add, sub, mul, div) grouped into τ bins (-6..6) to study effect of operand order-of-magnitude on arithmetic performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "evaluated across multiple LLMs (GPT-4 used to generate problems)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (single-step arithmetic tasks varying magnitudes)",
            "number_range_or_complexity": "Groups with τ (order-of-magnitude of largest operand) from -6 to +6, 50 expressions per group per operation; includes very small to very large operands and decimals",
            "method_or_intervention": "Synthetic benchmark generated by GPT-4 and used to probe models in isolation (no table evidence); performance measured as accuracy per op per τ bin",
            "performance_result": "Qualitative summary: addition and subtraction are relatively robust across τ; multiplication and division performance drops substantially at extreme orders-of-magnitude; large models (GPT-4, GPT-3.5-Turbo, PaLM-2-540B) outperform smaller models on these synthetic tasks (exact percentages not tabulated in main text but trends shown in Figure 5).",
            "mechanistic_insight": "Findings suggest models are more reliable for simpler associative ops (add/sub) even across magnitude variation, while algorithmic/precision-sensitive ops (mul/div) reveal limitations, implying LLMs lack robust general-purpose numeric algorithms for precise multiplication/division across scales.",
            "performance_scaling": "Bigger models show substantially better accuracy on synthetic arithmetic, especially on non-trivial magnitudes; error rates rise with extreme τ.",
            "failure_modes": "Errors increase for multiplication/division at high and low τ, precision loss with decimals and large magnitudes, and off-by-small amounts indicating limited exact numeric fidelity.",
            "comparison_baseline": "Used to compare operation types and model sizes; also used to analyze effect of operand order-of-magnitude on accuracy.",
            "key_finding": "Synthetic evaluation shows consistent, operation-dependent failure patterns: addition/subtraction robust across magnitudes but multiplication/division and extreme orders-of-magnitude expose substantial weaknesses in LLM arithmetic.",
            "uuid": "e279.7",
            "source_info": {
                "paper_title": "Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "MAmmoTH: Building math generalist models through hybrid instruction tuning",
            "rating": 2,
            "sanitized_title": "mammoth_building_math_generalist_models_through_hybrid_instruction_tuning"
        },
        {
            "paper_title": "Are NLP models really able to solve simple math word problems?",
            "rating": 1,
            "sanitized_title": "are_nlp_models_really_able_to_solve_simple_math_word_problems"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Program of thoughts (PoT)",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_pot"
        }
    ],
    "cost": 0.019383499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering</p>
<p>Pragya Srivastava 
Manuj Malik manujm@smu.edu.sg 
Singapore Management University</p>
<p>Vivek Gupta 
University of Pennsylvania</p>
<p>Tanuja Ganu # 
Dan Roth danroth@seas.upenn.edu 
University of Pennsylvania</p>
<p>Microsoft Research 
Rami Aly 
Zhijiang Guo 
Michael Sejr Schlichtkrull 
Tom Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared D Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Amanda Askell 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Tom Henighan 
Rewon Child 
Aditya Ramesh 
Daniel Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Ma- Teusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mccandlish 
Alec Radford 
Ilya Sutskever 
Dario 2020 Amodei 
Wenhu Chen 
Hongmin Wang 
Jianshu Chen 
Yunkai Zhang 
Shiyang Li 
Xiyou Zhou 
William Yang 
Tabfact 
Ming Wei Chang 
Eva Schlinger 
William Wang 
William Cohen 
Hanwen Zha 
Zhiyu Chen 
Wenhan Xiong 
Peter Clark 
Oren Etzioni 
Daniel Khashabi 
Tushar Khot 
Dalvi Bhavana 
Kyle Mishra 
Ashish Richardson 
Carissa Sabharwal 
Oyvind Schoenick 
Niket Tafjord 
Sumithra Tandon 
Dirk Bhakthavatsalam 
Michal Groeneveld 
Michael Guerquin 
2021 Schmitz 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Reiichiro Nakano 
Jacob Devlin 
Ming-Wei Chang </p>
<p>ICLR
Addis AbabaEthiopia</p>
<p>Charese Smiley
Sameena Shah</p>
<p>Iana Borova
Matt Beane, Ting-Hao HuangDylan Langdon, Reema Moussa</p>
<p>Bryan Routledge, and William Yang Wang</p>
<p>Shiyang Li, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022bZhiyu Chen, Charese Smiley</p>
<p>Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering
EFD4BF4B450037AF7A9F4380458E2A2C
Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with a hybrid of structured tables and unstructured text remain uncertain.This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt.Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks.We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps.The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semistructured tables.Ultimately, we introduce a novel prompting technique EEDP tailored to semi-structured documents, matching or outperforming baselines performance while providing a nuanced understanding of LLMs abilities.</p>
<p>Introduction</p>
<p>In the constantly evolving realm of artificial intelligence, Large Language Models (LLMs) have risen as cutting-edge tools for natural language understanding.They excel in a wide array of NLP tasks, including machine translation (MT), text summarization, question answering, and code generation.One specific area where LLMs' mathematical reasoning abilities come under scrutiny is the domain of numerical reasoning tasks.Past research has delved into the potential of language models for mathematical reasoning tasks, as seen in studies such as in Amini et al. (2019); Upadhyay and Chang (2017); Patel et al. (2021); Cobbe et al. (2021).These investigations provide a means to evaluate the performance of language models when The Goldman Sachs Group Incorporation</p>
<p>Notes to Consolidated Financial Statements</p>
<p>The table below presents a summary of Level 3 financial assets.</p>
<p>Financial Asset</p>
<p>Dec. 2017 Cash Instruments $15,395 Derivatives $3,802 Other Financial Instruments $4</p>
<p>Q: What was the total value of Level 3 financial assets for Goldman Sachs in December 2017?</p>
<p>A: $15,395 + $3,802 + $4 = $19,201 it comes to solving mathematical problems, ranging from straightforward math word problems to more complex ones.However, the problem becomes significantly more challenging when we encounter a hybrid of structured such as semi-structured tables and unstructured text, as shown in example in figure 1.Such tables are common in documents such as invoices, health records, and financial reports in corporate settings.In the financial domain, these tables present numerical data in a structured format, including income statements, balance sheets, cash flow statements, shareholder equity data, and annual reports.The majority of NLP models are primarily trained to handle raw unstructured textual data, which limits their ability to reason over semi-structured data, such as tables, or more intricate hybrids of tables and text, as seen in Chen et al. (2020b); Aly et al. (2021); Chen et al. (2020aChen et al. ( , 2021a)).Tables, especially these with intricate relationships and dependencies, often necessitate multihop reasoning, connecting information across multiple steps, as shown in Gupta et al. (2020).</p>
<p>NLP models may encounter difficulties in performing such multi-step reasoning, particularly when dealing with complex mathematical opera-arXiv:2402.11194v3[cs.CL] 9 Oct 2025 tions involving tables, as highlighted in Li et al. (2022).Previous research such as Chen (2023), exemplified these issues and demonstrated LLM capacity to process and reason with semi-structured tables.However, these studies are somewhat constrained and don't explicitly explore the models' mathematical reasoning abilities.This is particularly evident in data/tasks that involve a substantial number of arithmetic reasoning steps, operate on extreme orders of magnitude, or deal with intricate tables where extracting the relevant information for a query becomes challenging.</p>
<p>Moreover, when handling domain-specific documents, such as those in finance, a language model must not only have the necessary domain knowledge to craft the right approach for task-solving but also the capability to manipulate structured data, such as tables.Therefore, in this study, we aim to fill this gap by providing both qualitative and quantitative analyses of LLM's ability to reason on mathematical content on four finance datasets i.e.FinQA (Chen et al., 2021b), TATQA (Zhu et al., 2021), ConvFinQA (Chen et al., 2022b), and Multihiertt (Zhao et al., 2022).These datasets feature questions demanding intricate numerical reasoning, combining semi-structured tables and text.Each dataset provides pre-annotated executable programs for precise answer retrieval.Our goal is to illustrate how model performance varies as the numerical complexity of the underlying data and the intricacy of the mathematical reasoning steps required to solve a query increase.Building upon these observations, we propose a novel approach termed (Elicit −→ Extract −→ Decompose −→ Predict) EEDP, designed to deconstruct model responses into discrete components.This innovative method offers a deeper, more transparent insight into the numerical limitations of the model when tackling these tasks.Our contributions are as follows:</p>
<ol>
<li>
<p>We conduct a comprehensive robust evaluation of state-of-the-art Large Language Models (LLMs) for tabular (hybrid) question answering, with a specific focus on mathematical reasoning tasks, using public financial tabular datasets to establish a thorough performance benchmark.</p>
</li>
<li>
<p>Our analysis is thorough and multifaceted, encompassing both qualitative and quantitative aspects across several dimensions.We aim to provide nuanced insights into the strengths and limitations of LLMs in tabular (hybrid) question answering, especially in scenarios involving mathematical reasoning.</p>
</li>
<li>
<p>Building upon qualitative analysis, we introduce a novel and improved prompting method called EEDP.Our novel approach not only enhances our understanding of model weaknesses but also substantially enhances model performance compared to existing prompting methods across multiple models types.</p>
</li>
</ol>
<p>Our metadata dataset and source code are available at https://vgupta123.github.io/eedp.</p>
<p>Metadata Annotations</p>
<p>We annotated four tabular datasets: FinQA, TATQA, ConvFinQA, and Multihiertt with meta information related to a.) reasoning steps, b.) question category, c.) table length, d.) hierarchical complexity e.) missing information.1 Below, we provide detailed information about these meta-data annotations:</p>
<ol>
<li>Number of Reasoning Steps: Including the count of arithmetic operations in questions is crucial.More operations reflect increased complexity in reasoning, and their interdependence offers insights into the models' proficiency.This annotation, applied across all four datasets, reveals their ability in handling intricate arithmetic tasks.Refer to Figure 8 in Appendix A.4 for distribution of questions based on the number of reasoning steps involved.</li>
</ol>
<p>Question Categorization:</p>
<p>In numerical reasoning, grasping the evolution from fundamental arithmetic to advanced operations is crucial, marking a shift in cognitive complexity.As questions advance, they typically involve more intricate combinations of operations and linguistic nuances.Our research identify both the capabilities and limitations of LLMs in understanding these concepts.</p>
<p>We establish 12 mathematical concept categories (Table 1) with corresponding definitions, annotating each question.The dataset coverage across these categories is shown in Figure 9   to domain-specific knowledge focus and TIME questions due to limited sample size.</p>
<p>3.</p>
<p>Experimental Results</p>
<p>In this study, we choose to experiment with LLMs such as GPT-3.5-Turbo,GPT-4, PaLM-540B, Mistral-7B-Instruct2 , Llama-2-13B3 and MAmmoTH-13B4 .These LLMs are at the cutting edge for both open-source and closed models applications.</p>
<p>Models like MAmmoTH-13B are specifically fine-tuned during pre-training to excel in mathematical reasoning tasks.For more detail about the the model choices refer to Appendix A.2.</p>
<p>LLMs Prompting Methods: For an instructiontuned LLM, it's assumed that we give the model a task-specific instruction I accompanied with a few (usually k ∈ {2, 4}) demonstrations D T of a task T. We experiment with the following prompting techniques: (a.) Direct: In this setup, we explicitly instruct the models to abstain from providing explanations and just return the final answer to the posed question.For this scenario, D T contains {(p i , q i , a i )} k i=1 where p is the premise (table-text), q is the question, and a is the ground-truth answer.</p>
<p>(b.) CoT: Moving forward, we experiment with the chain-of-thoughts prompting strategy where we instruct the model to output the explanation to the answer derived by it.Here, our D T contains {(p i , q i , e i )} k i=1 where p is the premise which includes the table and the associated text, q is the question and e is the explanation of the answer.(c.) PoT: In this case, the expected response is a code derivation of the answer.Here, D T contains {(p i , q i , c i )} k i=1 where p is the premise which includes the table and the associated text, q is the question and c is the code-derivation of the answer.(d.) Decomposers: (Ye et al., 2023) proposed to address the challenge of handling large tables by decomposing them into more manageable subtables.Similarly, complex questions are handled by breaking them down into simpler subquestions.Decomposition proves effective with SQL tables, facilitating the removal of distracting details while retaining all supporting evidence.Questions are first parsed to break them down into simpler, more manageable subquestions.The model then addresses each subquestion independently before composing the answers to arrive at the final solution.In this case, our demonstration set D T contains {(p ′ i , ⟨q 1 , q 2 , ..., q n ⟩, a i )} k i=1 where p ′ is the premise obtained by the irrelevant information removal to the question from the original premise p and ⟨q 1 , q 2 , ..., q n ⟩ are the subquestions whose answers lead to the final answer.</p>
<p>EEDP Prompting Strategy:</p>
<p>We propose a novel prompting strategy: Elicit −→ Extract −→ Decompose −→ Predict. Figure 4 show an illustration of our EEDP approach.Below are the detail of each EEDP step:</p>
<ol>
<li>
<p>Elicit: We prompt the model explicitly to first elicit relevant domain knowledge for answer-ing a given query.</p>
</li>
<li>
<p>Extract: Conditioned on the table, question and the elicited domain knowledge, the model extracts supporting evidences to answer a given question.</p>
</li>
<li>
<p>Decompose: We instruct the LLM to break a complex mathematical reasoning task into multiple atomic operations and compose the operations to arrive at the final answer.</p>
</li>
</ol>
<p>Predict:</p>
<p>The model finally returns the derived answer in the above steps.</p>
<p>Figure 11 shows a example for EEDP strategy with one shot.</p>
<p>Results and Analysis.Table 2 shows a comparison in performance between different prompting strategies.Despite being a single prompt, EEDP demonstrates comparable or superior performance compared to PoT.Notably, we outperform PoT significantly for PaLM-2-540B and LLAMA-2-13B across most datasets.Moreover, while PoT relies on external tools for executing mathematical programs/code to obtain answers, EEDP exclusively utilizes LLM for all tasks, including evidence extraction, operation identification, and execution, ensuring precision throughout the process.</p>
<p>As shown in Table 2, the Decomposers prompting strategy exhibits a much poorer performance compared to other strategies.The reason behind this was statistically found to be the inaccurate formation of subtables from the main table, leading to information loss as described in the previous paragraph.The performance of EEDP either surpasses or matches very closely with that of PoT.The number of shots was adjusted depending on the context length of the model.</p>
<p>We can see that MAmmoTH-13B model, which is fine-tuned on the MathInstruct dataset (Yue et al., 2024) containing Instruction-Response pairs where the responses are a hybrid of CoT and PoT rationales, fails to perform well with the EEDP methodology.We argue that this is due to two potential reasons: (a.) Reduction of the number of shots to adjust the context length as the EEDP response is longer than that of the other methods, and (b.) Finetuning may contribute to suboptimal performance due to its alignment with a particular style and format of responses, potentially limiting the model's adaptability and generalization to other diverse contexts.EEDP's Computational Efficiency EEDP functions as a unified single-prompt method, minimizing computational complexity.Unlike methods like PoT, which rely on external tools, EEDP operates independently.When assessing computational cost, we consider API calls and token generation.Since EEDP uses a single-step prompting approach, only one API call is needed per query, making its computational cost comparable to methods like CoT.</p>
<p>For inference with open-source models, we used hardware with an A40 40GB GPU.Processing one dataset per model using the vLLM library took approximately 10 hours.</p>
<p>4 Where do LLMs fail?</p>
<p>Through manual inspection, we rigorously evaluate the EEDP responses against the meta-annotations from section 2 as ground-truth benchmarks for extraction and model reasoning accuracy • Deficit in Domain Knowledge (R1): These errors occur when the model attempts to derive an answer to the posed question using a wrong formula for domain-specific measures, for eg.COGS, ROI etc.</p>
<p>• Question Misinterpretation (R2): These errors occur when the model interprets the question differently and provides responses that are not aligned with the intended query.</p>
<p>Overall, the model's outputs lack grounding in the original question posed to it.</p>
<p>Incorrect Calculation:</p>
<p>This variety of errors include those where the model commits mistakes due to calculation mistakes.This can be of two types as described below.</p>
<p>• Incorrect Instantiation (C1): These include cases if the model extracts the right evidences, uses the right derivation formula but instantiates the variables incorrectly with the values resulting in an incorrect answer.</p>
<p>• Precision Error (C2): Language models employ mathematical algorithms for arithmetic operations, but their results may not always be perfectly accurate due to insufficient data pattern coverage or introduced biases during training.Consequently, they can sometimes generate outputs with slight inaccuracies or deviations from correct results.We show a detailed analysis in A.1.</p>
<p>Analysis:</p>
<p>The above categorization provides a nuanced understanding of the diverse challenges and shortcomings exhibited in different facets of mathematical reasoning.We observe that in a lot of cases, the error propagates because of a deficiency in domain knowledge.It is critical for both evidence extraction and reasoning.Despite possessing general domain knowledge owing to the massive amount of data these models have been pre-trained upon, these models may require explicit prompts to elicit the specific domain knowledge needed for a particular question.Furthermore, errors can arise due to the models' limited proficiency in multi-step reasoning, especially in tackling questions involving multiple arithmetic operations in a sequence.</p>
<p>We give a quantitative measure of each type of errors for each of the 4 datasets we consider for our study in Table 3.We also provide examples corresponding to each error category in figures 12, 13, 14, 15, 16 and 17.Statistically, we find that reasoning errors contribute a significant chunk to the total number of errors.In case of complex hierarchical tables like that in Multihiertt, the model is found to struggle with extracting the right supporting evidences from the premise for a given question.Calculation errors can be taken care of if a thirdparty calculation tool (an external agent) is chained to the language model.</p>
<p>Analysis on Reasoning Annotations</p>
<p>We analyse model performance on the basis of finegrained annotations as described in the section 4.</p>
<p>Performance vs Number of Reasoning Steps.</p>
<p>We investigate model performance with increasing mathematical reasoning steps, as shown in Figure 2.This analysis provides insights into models' ability to handle varying task complexities.As expected, performance decreases with more reasoning steps, indicating LLMs' challenges in retrieving information and reasoning as complexity grows.</p>
<p>Anomalies are observed in ConvFinQA, where accuracy improves after greater than or equal to two reasoning steps, potentially due to questions referring to answers of prior conversation turns.Anomalies like these warrant further investigation beyond this study's scope.</p>
<ol>
<li>Performance across Question Types.We analyze the performance trends across different question categories, as defined in Table 1, to assess the models' understanding of various mathematical and financial concepts.Figure 3 shows that EEDP consistently performs better than or as well as PoT across all datasets.The improvement is particularly pronounced for PaLM-2-540B in all question categories.</li>
</ol>
<p>Performance across Arithmetic Operations.</p>
<p>Figure 5 in Appendix A.1 shows that for relatively simpler arithmetic operations like addition and subtraction, the effect of order of magnitude of the operands is less profound as compared to harder arithmetic operations such as multiplication and division.We observe the trend in the performance accuracy with the growing and diminishing orders of magnitude.We also observe bigger and more capable models such as GPT-4, GPT-3.5-TURBO and PaLM 2-540B perform much better on simpler addition, subtraction task in comparison to multiplication, division task.For more details on refer to the Appendix A.1.</p>
<p>6 Other Related Works</p>
<p>LLMs on Mathematical Reasoning</p>
<p>Pre-trained Language Models (PLMs) excel in NLP tasks (Devlin et al., 2019;Zhuang et al., 2021) by leveraging extensive textual corpora to acquire world knowledge (Guu et al., 2020).Expanding PLMs for math-related tasks has been challenging due to their non-specific training.Recent attempts include MWP-BERT and Minerva (Liang et al., 2022;Lewkowycz et al., 2022), but curating high-quality math data remains difficult.To bridge the gap, researchers fine-tune PLMs for specific math tasks.Notable works, like Bhaskara, Selfsampler, Aristo, FinQANet, TAGOP, MT2Net, and others (Mishra et al., 2022;Ni et al., 2022;Clark et al., 2021;Chen et al., 2021b;Zhu et al., 2021;Zhao et al., 2022;Cao and Xiao, 2022;Welleck et al., 2022), employ PLMs such as GPT-Neo and RoBERTa for math problem-solving.</p>
<p>Tabular Question Answering</p>
<p>Handling diverse input formats in question answering, including structured tables and visual data, poses challenges for language models.HybridQA (Chen et al., 2020b) introduces questions requiring reasoning over tables and text.MultimodalQA (Talmor et al., 2021) adds visual inference.Our focus is on multi-hop question answering over tables and text.TAPAS (Herzig et al., 2020) tackles table- based questions without logical forms, while Tapex (Liu et al., 2022) empowers generative models with table reasoning.Models like FinQANet, TagOP, and MT2Net (Chen et al., 2021b;Zhu et al., 2021;Zhao et al., 2022) employ a retriever module to extract supporting facts from input financial reports, followed by a reasoning module to derive the final answer.Retrieving relevant evidence and conducting reasoning both demand domain-specific knowledge, such as understanding financial transactions, identifying revenue trends, and interpreting complex investment statements.Thus, reliance on an external knowledge base becomes crucial for addressing the challenges of domain-specific multi-hop question answering.
C H A N G E R A T IO C O U N T IN G A V E R A G E D IF F S U M R A T IO 0.2 0.4 0.6 0.8 1 C O M P A R E S U M P R O D U C T D O M A IN IN F O D IV IS IO N D IF F R A T IO C H A N G E R A T IO A V E R A G E 0.2 0.4 0.6 0.8 1 D IV IS IO N C H A N G E R A T IO A V E R A G E S U M R A T IO D IF F 0.2 0.4 0.6 0.8 1 A V E R A G E C H A N G E R A T IO S U M R A T IO D IF F 0.20</p>
<p>Prompt Engineering</p>
<p>In-context Learning (ICL) equips Language Models (LLMs) with task examples and queries, enabling them to perform target tasks without updating model parameters (Brown et al., 2020;OpenAI, 2023).They excel in mathematical reasoning with few-shot prompts but struggle with more complex tasks.Methods like chain-of-thoughts (CoT) (Wei et al., 2022) have improved LLM performance by guiding them through intermediate reasoning steps.Enhancing multi-step reasoning involves two main approaches: improving in-context examples and obtaining better reasoning steps.Some focus on stable in-context example selection (Fu et al., 2023;Rubin et al., 2022;Lu et al., 2023).Others adopt a modular approach, using off-the-shelf tools (Schick et al., 2023), program of thoughts (PoT) (Chen et al., 2022a), or decomposition strategies (Zhou et al., 2023;Dua et al., 2022;Khot et al., 2023).</p>
<p>Our task requires complex multi-step reasoning across diverse information sources.LLMs, as demonstrated by (Chen, 2023), can reason over structured tables without explicit encoding.They also serve as versatile decomposers, breaking down extensive evidence and complex questions (Ye et al., 2023).</p>
<p>Our Contributions.Our study aimed to delve into the mathematical reasoning abilities of LLMs within the context of financial documents where models are tasked with complex hybrid (table-text) question answering.This presents a significant challenge, requiring models not only to provide accurate numerical analysis but also to retrieve right supporting evidence tailored to specific question requirement.Moreover, it necessitates the extraction of necessary knowledge from the model's pretrained parameters to address queries.</p>
<p>Firstly, we meticulously annotate popular financial datasets, such as FinQA, ConvFinQA, TATQA, and Multihiertt, with detailed meta-information.This includes specifying mathematical operations, types of reasoning involved, table dimensions, question types, and the depth of table hierarchy.Furthermore, we conduct a manual error analysis to quantify error types across multiple LLMs.These detailed annotations are invaluable for analyzing various dimensions where LLM models encounter challenges.This, in turn, aids in the development of better prompting techniques such as EEDP, aimed at enhancing LLMs' mathematical reasoning abilities.The resulting improvement in performance with EEDP across multiples datasets serves as compelling evidence of the effectiveness of this approach.</p>
<p>What did we learn?Our analysis revealed that LLMs can accurately handle addition and subtraction tasks e.g.modle perform fairly when calculating total expenses or profits, but struggle with multiplication and division e.g.model performs poorly with questions requiring reasoning operations involving proportions, ratios, percentages, and division.Moreover, as the complexity of the data increases either through a higher absolute order scale or more decimal numbers, model performance degrades.Model performance also degrades with increasing number of reasoning steps and lengthy complex hierarchical table structures.e.g. in complex datasets with hierarchical structures such as Multiheirtt, TATQA, incorrect extraction leads to modeling errors.Similarly, on queries involving multiple conversational turns, such as those in ConvFinQA, model perform poorly due to reasoning failures, like misinterpreting multiple queries longer context.Across all models, incorrect reasoning and incorrect extraction consistently emerge as common sources of errors.For smaller models, even straightforward calculations, result in errors due to imprecise calculations.</p>
<p>EEDP vs other methods (a.) EEDP vs PoT: PoT enhances LLM inference with the use of variable names for the supporting values extracted from the premise and prompts the LLM to express their thought process in the form of programs.The model output is a program which is executed externally to derive the final answer.EEDP proposes to decompose a complex reasoning task into simple atomic steps whose solutions can be composed to give the final answer.In PoT, they don't make the language model do the computation while in our case the language model not only outputs the reason but also computes the final answer.This distinction implies that PoT may have an inherent advantage over EEDP.</p>
<p>(b.) EEDP vs Decomposers: The prompting strategy proposed by (Ye et al., 2023) was originally designed for querying SQL tables, they use LLMs to break down evidence and questions for SQL interpreters.In contrast, our approach addresses more complex scenarios involving both tables and text, requiring advanced reasoning skills beyond the capability of standard SQL interpreters.Pruning a non-SQL table using this method can lead to significant information loss from the premise which can be a potential ingredient required to derive the final answer.Additionally, this is an expensive method as it requires 3X API calls as opposed to other prompting methods.Moreover, EEDP is a unified prompting strategy which integrates multiple solver elements into a single unified prompt for elicitation, extraction, decomposition and prediction.</p>
<p>Conclusion</p>
<p>In conclusion, our study delved into LLMs' mathematical reasoning in complex financial scenarios, assessing their adaptability to mixed structured tables and unstructured text.Through rigorous experimentation, we uncovered insights into their performance and limitations, presenting a tailored prompting technique that outperformed other baseline methods.Our findings advance understanding of LLMs' abilities in tackling intricate mathematical tasks within semi-structured documents, suggesting directions for future research.Please refer to appendix section A.3 for future work details.</p>
<p>Limitations</p>
<p>The scope of this work is limited by the following challenges:</p>
<p>Dataset Scarcity.There are not many datasets dealing with numerical reasoning over semistructured data apart from the ones from financial domain.As a future work, it would be interesting to similar analysis across various domains such as e-commerce, healthcare, sports and scientific tables from research papers, uncovering new challenges and insights.This expansion will enhance the applicability and impact of our research within the NLP community.However, creating tailored datasets for these domains presents a significant challenge.</p>
<p>For now to ensure a comprehensive evaluation of LLMs, we have integrated financial datasets that offer diverse challenges.For instance, Multihiertt evaluates model performance with intricate premise structures, providing insights into handling complex data hierarchies.ConvFinQA delves into the intricate chains of numerical reasoning within conversational question answering contexts, offering a unique perspective on dynamic data interpretation.Moreover, FinQA and TAT-QA encompass a wide array of reasoning types, with a significant portion requiring domain-specific knowledge, thereby broadening the evaluation spectrum.</p>
<p>Factors Isolation.It is essential to acknowledge that there may be multiple factors influencing the performance of large language models while dealing with numerical reasoning over semi-structured data.In our analysis, we have focused on specific factors and trends, but it is important to recognize that the overall performance is affected by a multitude of variables.Marginalizing i.e. observing the trend along one while keeping the rest as constants or isolating a single factor is challenging and cannot be done with real-world data.Additionally, future investigations may benefit from simulating controlled scenarios on synthetic and counterfactual datasets to gain deeper insights into the impact of individual factors on model performance.</p>
<p>Modeling Improvement.We emphasize our analysis on prominent models such as GPT-4, GPT-3.5-TURBO, and PaLM 2-540B due to their substantial size and capabilities.Notably, other open-sourced large language models like LLaMA 2-13B, MAmmoTH-13B and Mistral-7B-Instruct did not exhibit satisfactory performance in numeri-cal reasoning over semi-structured data.For more detail about the the model choices refer to Appendix A.2.This accentuates the need for exploring computationally feasible and cheaper models that can deliver remarkable performance in tasks involving numerical reasoning over heterogeneous sources of information.Future experiments with ample computational resources may involve exploring larger open-source models like OLMo, Mixtral, and DBRX, which have been recently released.</p>
<p>Ethics Statement</p>
<p>We, the authors of this work, affirm that our work complies with the highest ethical standards in research and publication.In conducting this research, we have considered and addressed various ethical considerations to ensure the responsible and fair use of computational linguistics methodologies.We provide detailed information to facilitate the reproducibility of our results.This includes sharing code, datasets (in our case, we deal with publicly available datasets and comply to the ethical standards mentioned by the authors of the respective works.), and other relevant resources to enable the research community to validate and build upon our work.The claims in the paper match the experimentation results, however, with black-box large language models, a certain degree of stochasticity is expected which we attempt to minimize by keeping a fixed temperature.We describe in the fullest details the annotations, dataset splits, models used and prompting methods tried, ensuring reproducibility of our work.</p>
<p>A Appendix</p>
<p>A.1 How proficient are LLMs in performing simple arithmetic operations?</p>
<p>To assess the effectiveness of Large Language Models in handling fundamental arithmetic tasks (addition (+), subtraction (-), multiplication (*), and division (/)) across operands of varying magnitudes, we generate a set of 2600 synthetic arithmetic expressions using GPT-4.This set includes 650 problems for each arithmetic operation.Within each operation category, we categorize tasks into groups based on a parameter denoted as τ :
τ = OOM( arg max op ∥OOM(op)∥ )
where, arg max selects the operand op with the greater absolute value of the order of magnitude, and OOM represents the order of magnitude.This approach is motivated by cognitive challenges commonly faced by humans, as they often encounter difficulties with both high and low orders of magnitude.Essentially, captures the order of magnitude of the operand with the larger absolute value among the two.For each arithmetic operation, we establish groups with τ , ranging from -6 to 6. Within each group, there are 50 arithmetic expressions.This systematic grouping provides a comprehensive assessment across various difficulty levels based on operand magnitudes.</p>
<p>Analysis.Figure 5 illustrates that for simpler arithmetic operations like addition and subtraction, the impact of the order of magnitude of the operands is less significant compared to harder operations like multiplication and division.We observe a trend in performance accuracy with increasing and decreasing orders of magnitude.Larger models such as GPT-4, GPT-3.5-TURBO, and PaLM 2-540B perform significantly better on addition and subtraction tasks as compared to the multiplication and division tasks.</p>
<p>A.2 Model Selection Criteria</p>
<p>Our model selection process was guided primarily by resource constraints and the timeframe of our research endeavor.We aimed to identify models that represented the state-of-the-art (SOTA), such as GPT-4, or those with a specific focus on mathematical reasoning, such as MAmmoTH, aligning with the parameters of our project.Here's a breakdown:</p>
<ol>
<li>Resource and Budget Constraints: Given our limited computing resources and budget, we meticulously selected models that could provide valuable insights within the confines of our project.Incorporating additional models would have been impractical given our resource limitations.</li>
</ol>
<p>The number of shots, indicated by #shots = k, was tailored to the context length of the model.Specifically, for models with a context length exceeding the input length, we standardized k to 4. For instance, we allocated 2 shots for models like LLaMA and MammoTH due to their constrained context length.However, for other models capable of accommodating larger contexts, we increased the number of shots to 4. Additionally, we used a temperature of 0 and top p = 1 for our experiments.</p>
<p>Models with Mathematical Capabilities:</p>
<p>We prioritized models renowned for their advanced mathematical prowess, such as MAmmoTH, alongside state-of-the-art Large Language Models (LLMs) like GPT-4.Our goal was to gain deeper insights into the mathematical reasoning capabilities of cuttingedge models within the context of financial documents.</p>
<ol>
<li>Better Prompting Approaches: Rather than focusing solely on model diversity, we concentrated on exploring a variety of prompting methods, particularly those aimed at enhancing mathematical reasoning.We believed this approach would yield more valuable insights into the performance of both LLMs and their associated prompting techniques in real-world financial tasks.</li>
</ol>
<p>Excluding Underperforming Models:</p>
<p>While we experimented with various models, such as Falcon-7B-Instruct and MPT-7B-Instruct, we found them to underperform significantly compared to models like LLaMA and Mistral.Consequently, we excluded them from detailed analysis.Future experiments with ample computational resources may involve exploring additional open-source models like OLMo, Mixtral, and DBRX, which have been recently released.</p>
<p>A.3 Other Modeling Techniques</p>
<p>Based on our research and the results obtained from our proposed method 'EEDP', we do have several insights that could guide future model development:</p>
<ol>
<li>
<p>Domain-Specific Pre-training: Our method "EEDP" suggests that LLMs could benefit from pre-training that focuses on extracting domain-specific knowledge.In the context of financial documents, for instance, this could involve training models on a corpus of financial texts, thereby enabling them to better understand and reason about financial concepts and terminology.</p>
</li>
<li>
<p>Knowledge Elicitation: The elicitation step in "EEDP" indicates the potential for designing LLMs that can elicit or extract relevant information from a given context more effectively.This could involve developing models that are better at identifying and focusing on key pieces of information in a document, which is crucial for accurately answering questions about the document.</p>
</li>
<li>
<p>Modular Modeling: Our research introduce a novel approach to the reasoning process, wherein it's broken down into modular steps.In this methodology, Large Language Models (LLMs) handle different aspects of a task in distinct stages.This division potentially enhances the overall accuracy and efficiency of the model.</p>
</li>
</ol>
<p>For instance, the model might begin by eliciting domain-specific knowledge, then proceed to extract relevant information from the premise.Following this, it engages in reasoning about this information to answer a question and finally derives the answer, using the output from the preceding reasoning steps as a reference point.</p>
<p>By potentially training individual expert models to handle each specific stage, we could optimize performance for each distinct task.This modular approach allows for specialized processing of each step, thereby improving the overall performance and interpretability of the final output.</p>
<p>Hierarchical Structure Understanding:</p>
<p>Representing the input structure of the table in a better format to the LLM could be beneficial.One can also explore introducing special positional encodings, similar to those used in TAPAS, to serve as row and column IDs for each cell.This approach would differ from traditional positional encodings, which are designed to capture the inherently linear structure of textual data.TThis integration would facilitate the extraction of relvant information from the table correctly, considering its structure more effectively, avoiding information extraction errors.Another idea could be converting the premise containing the complex table and text into a common representation such as a knowledge graph.Furthermore, models specifically tuned to answer to human queries over complex documents in multiple conversational turns (like that in ConvFinQA) should also be considered, as it's a challenge for language model's to backtrack to their decisions that were made previously in the conversation.Figure 8 for the distribution of questions by reasoning steps.We define 12 mathematical concept categories, see Table 1) and annotate each question accordingly.The dataset coverage for these categories is shown in Figure 9.Our method for estimating hierarchy depth is shown in Figure 10. Figure 11 shows a example for EEDP strategy with one shot.Figure 11 shows a example for EEDP strategy with one shot.(2009,2008), creating a substructure within the table.The third level (3) involves the column "(in millions, except percentages)," and the fourth level (4) encompasses the rows under categories like "Revenues," "Distribution fees," "Net investment income," and so on.To locate specific values like "$1733" and "$1912", a retriever module needs to navigate through these four levels.</p>
<p>A.4 Metadata Annotations Dataset Coverage</p>
<p>Instruction:</p>
<p>Please carefully analyze the provided information, perform necessary numerical calculations, and provide accurate answers to the given question using the provided data.Take into account the pre-text, table, and post-text when formulating your response.When performing numerical calculations, ensure you gather the required information and follow a step-by-step approach.</p>
<ol>
<li>
<p>Elicit the most relevant domain fact or knowledge that might be useful for you to extract the right operands and forming the right approach to the problem.</p>
</li>
<li>
<p>Translate table data into sentences and identify gold evidence for answering the question.</p>
</li>
<li>
<p>Define a sequence of atomic operations (Add, Subtract, Divide, Multiply, Greater, Min, Max, Exp) which take into account only two operands at a time.Divide a complex task into a sequence of atomic operations defined above.</p>
</li>
<li>
<p>Finally, conclude as: The final answer is <answer>.Question: What was the 2019 percentage change in pre-tax losses?</p>
</li>
</ol>
<p>Response #: Domain Knowledge: Pre-tax losses, or operating losses, refer to financial losses that a company incurs before considering the effects of income taxes.To find the 2019 percentage change in pre-tax losses, we need to find the difference between the new and the old value of the pre-tax losses, then divide the obtained difference by the old value and multiply this value by 100.</p>
<p>Gold Evidences:</p>
<p>• The pre-tax losses in 2019 are $19,573.</p>
<p>• The pre-tax losses in 2018 are $25,403.</p>
<p>Steps:</p>
<ol>
<li>Subtract 25403 from 19573.</li>
</ol>
<p>• The final answer is -22.95%.Steps:</p>
<ol>
<li>Add CET1 capital and Tier 1 capital.</li>
</ol>
<p>• Response 1: $184,375 + $184,375 = $368,750 2. Add #1 and Total capital.</p>
<p>• Response 2: #1 + $195,839 = $564,589</p>
<p>The final answer is $564,589 million.375 + $184,375 + $195,839 + $184,375 + $184,375 + $189,419  Gold Evidences:</p>
<p>Ground</p>
<p>• Table shows Asset Management of Total invested assets in 2006 is $68,280.</p>
<p>• Table shows Mortgage and other loans receivable, net of allowance (in millions) in 2006 is $28,418.</p>
<p>Steps:</p>
<ol>
<li>Divide Asset Management by the total Mortgage and other loans receivable, net of allowance in 2006.</li>
</ol>
<p>• Response 1: 68280 28418 = 2.4024</p>
<p>The final answer is 2.4024.</p>
<p>Ground-truth steps:   $138.48 $155.26 $200.79 $227.08 $318.46 S&amp;P 500 Index $132.04 $149.89 $151.94 $169.82 $206.49 S&amp;P Industrials Index $140.18 $153.73 $149.83 $177.65 $214.55 S&amp;P Consumer Durables &amp; Apparel Index $135.84 $148.31 $147.23 $138.82 $164.39Question: What was the difference in percentage cumulative total shareholder return on Masco common stock versus the S&amp;P 500 Index for the five-year period ended 2017?</p>
<p>Masco</p>
<p>Response #: Domain Knowledge: Cumulative total shareholder return is a measure of the overall performance of a stock over a given period of time.It includes the change in the stock's price as well as any dividends paid to shareholders during that time.</p>
<p>Gold Evidences:</p>
<p>• The cumulative total shareholder return for Masco in 2017 is $318.46.</p>
<p>• The cumulative total shareholder return for the S&amp;P 500 Index in 2017 is $206.49.</p>
<p>Steps:</p>
<ol>
<li>Divide the cumulative total shareholder return for Masco in 2017 (318.46)</li>
</ol>
<p>Figure 1 :
1
Figure 1: An example of a semi-structured financial document question answering.</p>
<p>Figure 2 :
2
Figure2: A comparison showcasing the performance trends across various datasets with the increasing number of reasoning steps.The analysis contrasts the effectiveness of EEDP (our method) against PoT in addressing complex reasoning.</p>
<p>Figure 3 :
3
Figure 3: A comparison showcasing the performance trends observed in various datasets across different question types.The analysis contrasts the effectiveness of EEDP (our method) against Few-Shot PoT (PoT).Best viewed in color.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Our EEDP Approach (a.) Instructions, and (b.) Demonstration.</p>
<p>Figure 6 :
6
Figure 6: Sample distribution of Multihiertt &amp; FinQA datasets partitioned by number of rows in the supporting table.</p>
<p>Figure 7 :
7
Figure 7: Sample distribution of Multihiertt Dataset partitioned by (a) hierarchical complexity of the gold evidence.(b) the percentage of empty cells in the supporting table.</p>
<p>Figure 6
6
Figure 6 displays the dataset distribution for Multihiertt and FinQA based on table length (number of rows).Figure 7 (a) shows how we calculate hierarchical complexity for examples with multiple relevant rows at various hierarchical depths.For the distribution of missing information (empty cell</p>
<p>Figure 8 :
8
Figure 8: Sample distribution of examples in numerical reasoning on tabular datasets partitioned by the number of reasoning steps involved.Clockwise (from top-left) : FinQA, ConvFinQA, TATQA, Multihiertt.</p>
<p>Figure 9 :
9
Figure 9: Sample distribution of Numerical &amp; Tabular Reasoning Datasets partitioned by Question Concept Category types.Clockwise (from top-left): FinQA, ConvFinQA, Multihiertt, TATQA.</p>
<p>•</p>
<p>Response 3: −0.2295 × 100 = −22.95</p>
<p>Figure 11 :
11
Figure 11: A Template for our proposed prompting strategy, EEDP with 1-shot demonstration.</p>
<p>Asset Management in 2006 is taken as $68,280.0 by the model instead of $4,884.0due to error in the extraction of the ground-truth.</p>
<p>Figure</p>
<p>Figure 13: Error due to wrong evidences</p>
<p>•</p>
<p>by the cumulative total shareholder return for the S&amp;P 500 index in 2017 (206.49)• Response 1: 318.46 / 206.49= 1.5422 2. Subtract 1 from the quotient obtained in #1 Response 2: 1.5422 -1 = 0.5422 3. Multiply the value obtained in #2 with 100 to find the percentage difference • Response 3: 0.5422 * 100 = 54.22The final answer is 54.22%.Ground-truth steps: [((318.46-100.0)/100.0)<em>100.0-((206.49-100.0)/100.0)</em>100.0]= 111.97Reason: Wrong formula for return on investment</p>
<p>Figure 14 :
14
Figure 14: Example of an error caused due to insufficient domain knowledge</p>
<p>Table 1 :
1
Mathematical</p>
<p>concept categories and definitions for studying LLM concept comprehension abilities.</p>
<p>Table Length :
Length
Evaluating
with multiple relevant cells, we consider the cellwith the highest hierarchical depth for our analysis.Our approach to estimating hierarchy depth is illus-trated in Figure 10. Figure 7(a) in Appendix A.4illustrate how we calculate hierarchical complexityfor examples with multiple relevant rows at varioushierarchical depths.5. Missing Information: Interpreting a table be-comes challenging as the number of empty cellsincreases. Empty cells indicate missing or unde-fined information, leading to potential gaps in un-derstanding.Assessing empty cell proportions is crucial toquantify data ambiguity. More empty cells sug-gest higher uncertainty, which can hinder models'ability to derive meaningful insights and impact rea-soning accuracy. In Multihiertt, where tables arehierarchical in nature and empty cells occur quitefrequently, we annotate examples with the emptycells percentage, contributing to our understandingof data ambiguity. For distribution of missing infor-mation (empty cells proportions) across datasets,refer to Figure 7 (b) in Appendix A.4.Annotation Splits. We prioritized complex nu-merical questions in our selection criteria, bal-ancing this with resource constraints such as theLLM context length limits. We also took into ac-count tables with deeper hierarchies in Multihierttand multi-turn conversations in ConvFinQA. Forperformance withTATQA, we utilized 45% of the development set bylarger supporting tables is crucial. Larger tablesfiltering out examples involving simple span selec-complicate multi-hop reasoning tasks by increas-tion. In the case of Multihiertt, we included 68% ofing the amount of information, making it harderthe test set by excluding examples where the tableto identify relevant evidence. We prioritize theselength exceeds 40. For FinQA and ConvFinQA,annotations for datasets like FinQA and Multihiertt,we employed the complete test and developmentwhere questions mainly use tables as supporting ev-sets, respectively.idence. Therefore, these annotations are confinedto these datasets. In Multihiertt, when multiple ta-bles support evidence, we consider the one with thehighest row count i.e. maximum table length. Thedataset distribution for Multihiertt and FinQA w.r.ttable length (number of rows) is shown in Figure 6in Appendix A.4.4. Hierarchical Complexity: In hierarchical ta-bles, such as those in Multihiertt, evaluating modelperformance concerning the growing hierarchicalcomplexity in cells with critical information be-comes paramount. To tackle this, we annotate eachexample in Multihiertt with the hierarchy depthof cells containing relevant information. For table</p>
<p>Table 2 :
2
Comparison of performance of different models tested against a variety of prompting strategies
DatasetModelDirectCoTPoTEEDP DecomposersGPT-455.8186.91 89.9988.6747.46GPT-3.5-Turbo31.3877.57 82.1179.7328.53TATQAPaLM 2-540B44.6662.93 61.6081.5157.94Llama 2-13B3.3635.95 34.1640.9525.93MAmmoTH-13B19.1156.25 10.024.3722.89Mistral-7B10.9259.14 16.5356.067.24GPT-465.1272.38 75.2676.0544.93GPT-3.5-Turbo40.4759.18 68.9761.8832.33FinQAPaLM 2-540B30.3334.79 30.4161.9546.38Llama 2-13B1.8025.34 12.9730.4711.91MAmmoTH-13B22.8335.32 15.8635.0517.65Mistral-7B26.1134.23 10.5634.8612.34GPT-463.1071.19 78.8177.9118.76GPT-3.5-Turbo37.6248.33 61.1961.7510.50ConvFinQAPaLM 2-540B20.1938.00 40.1463.4222.32Llama 2-13B3.8029.45 29.9239.4210.35MAmmoTH-13B21.6146.088.7832.777.83Mistral-7B12.3548.45 14.4836.5711.16GPT-441.3555.13 67.2370.3236.86GPT-3.5-Turbo25.8842.33 52.1849.6520.61MultihierttPaLM 2-540B14.2020.67 36.5237.9720.19Llama 2-13B1.5430.66 18.1224.1516.86MAmmoTH-13B10.1218.566.5718.3611.87Mistral-7B14.909 22.92 14.9410.9711.63</p>
<p>Table 3 :
3
Error Analysis on Various Datasets.In this table,
DatasetErrorTypePer.(%)ExtractionE1 E210.38 25FinQAReasoningR125R215.57Calculation C1/C224.06ExtractionE1 E28.45 14.08ConvFinQAReasoningR1 R219.72 36.62Calculation C1/C221.13ExtractionE1 E213.79 31.03TATQAReasoningR1 R222.41 5.17Calculation C1/C227.59ExtractionE1 E220.5 31.5MultihierttReasoningR1 R215.5 12Calculation C1/C220.5Extraction.E1: Missing Evidences, Extraction.E2: WrongEvidences, Reasoning.R1: Insufficient Domain Knowledge,Reasoning.R2: Question Misinterpretation, Calculation: In-stantiation (C1) and Precision errors (C2)1. Incorrect Extraction: This category encom-passes errors where the model faces difficulties
. The reasoning programs represent sequences of arithmetic operations necessary to derive the final answer, utilizing values extracted from supporting evidence as operands.To assess calculation accuracy, we manually identify the model's instantiation and precision errors.Our EEDP prompt ensures that the model predominantly outputs responses in the expected format, with exceptions being rare.However, since we manually analyze all outputs, we do not penalize the model for format deviations but rather for incorrect outputs.Penalties are applied only when the model makes errors in extraction, reasoning, and/or calculation.Below, we categorize the EEDP response errors in detail based on their origins:</p>
<p>Figure10: This illustration demonstrates how the "hierarchical complexity" is determined for each supporting piece of evidence.The hierarchical complexity corresponds to the number of levels or tiers of information structure within the table.The top level (1) encompasses the table itself, which contains information about the results of operations in a firm's Advice &amp; Wealth Management segment.The second level (2) includes columns specifying the years
Years Ended December 31,20092008(in millions, except percentages)RevenuesManagement and financial advice fees$1,234$1,339Distribution fees$1,733$1,912Net investment income$297$-43Other revenues$85$80Total revenues$3,349$3,288Banking and deposit interest expense$133$178Total net revenues$3,216$3,110ExpensesDistribution expenses$1,968$2,121General and administrative expense$1,282$1,138Total expenses$3,250$3,259Pretax loss$-34$-149Gold Evidences:• Table shows Distribution fees of Years Ended December 31, 2009 (in millions, except percentages) is$1,733 .• Table shows Distribution fees of Years Ended December 31, 2008 (in millions, except percentages) is$1,912 .Hierarchical Complexity: max(4, 4) = 4
Question: What will Distribution fees reach in 2010 if it continues to grow at its current rate?(in millions)?</p>
<p>Demonstration: "Effective Income Tax Rate": A reconciliation of the United States federal statutory income tax rate to our effective income tax rate is as follows: In 2019 and 2018 we had pre-tax losses of $19,573 and $25,403, respectively, which are available for carry forward to offset future taxable income.
Year EndedYear EndedDecember 31, 2018 December 31, 2019United States federal statutory rate21.00%21.00%Effective income tax rate1.99%-0.01%</p>
<p>Regulatory capital, assets, and risk-based capital ratios for JPMorgan Chase and its significant IDI subsidiaries underBasel III Standardized Transitional and Basel III Advanced Transitional at December  31, 2017, and 2016.What is the sum of CET1 capital, Tier 1 capital, and Total capital in 2017?(in million) In banking, CET1 capital, Tier 1 capital, and Total capital are used to measure a bank's financial health and stability.CET1 (Common Equity Tier 1) capital consists mainly of common shares and retained earnings.Tier 1 capital is the core capital mainly comprising shareholders' equity and disclosed reserves.Total capital includes both Tier 1 capital and Tier 2 capital, which consists of hybrid capital instruments and subordinated debt.•Table shows JPMorgan Chase Bank, N.A. Basel III Standardized Transitional CET1 capital on Dec 31, 2017 as $184,375 million.• Table shows JPMorgan Chase Bank, N.A. Basel III Standardized Transitional Tier 1 capital on Dec 31, 2017 as $184,375 million.• Table shows JPMorgan Chase Bank, N.A. Basel III Standardized Transitional Total capital on Dec 31, 2017 as $195,839 million.
Basel III Standardized Transitional Basel III Advanced Transitional(in millions)Dec 31, 2017Dec 31, 2016Dec 31, 2017Dec 31, 2016Regulatory capitalCET1 capital$184,375$179,319$184,375$179,319Tier 1 capital(a)$184,375$179,341$184,375$179,341Total capital$195,839$191,662$189,419$184,637AssetsCapital ratios(c)CET113.8%13.7%(e)15.0%Tier 1(a)13.8%13.7%(e)15.0%Total14.7%14.6%(e)15.4%Tier 1 leverage(d)8.7%8.6%8.7%Question: Response #:Domain Knowledge: Gold Evidences:</p>
<p>= $1,122,758 Reason: The model missed adding JPMorgan Chase Bank, N.A. Basel III Advanced Transitional CET1 capital ($184,375), Tier 1 ($184,375), and Total Capital ($189,419) in 2017.What is the proportion of Asset Management to the total Mortgage and other loans receivable, net of allowance in 2006?Proportion refers to the relationship between two quantities, where one quantity is considered in relation to the other.To find the proportion of Asset Management to the total Mortgage and other loans receivable, net of allowance in 2006, we need to divide the value of Asset Management by the total value of Mortgage and other loans receivable, net of allowance in 2006.
YearLife/Fin.Gen.Rtd.Fin.AssetTotalIns.Ins.Services Services Mngmt2006Fixed Maturities:Bonds available for sale$67,994$288,018$1,357$29,500-$386,869Bonds held to maturity$21,437----$21,437Bond trading securities$1$10,835---$10,836Equity Securities:Common stocks$4,245$8,705-$226$80$13,256Preferred stocks$1,884$650$5--$2,539Other Assets:Mortgage and other loans$17$21,043$2,398$4,884$76$28,418Flight equipment--$39,875--$39,875Total Invested Assets$116,059 $428,825 $185,404 $68,280$3,373$801,941Question: Response #:Domain Knowledge:
Figure 12: Error due to missing evidences</p>
<p>One author annotated the data, and the other checked for accuracy; we took stringent measures to minimize errors.
https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1
3 https://huggingface.co/meta-llama/Llama-2-13b-chat-hf
4 https://huggingface.co/TIGER-Lab/MAmmoTH-13B
AcknowledgementsResearch was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-20-1-0080.The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.This work was partially funded by ONR Contract N00014-19-1-2620.Lastly, we extend our appreciation to the reviewing team for their insightful comments.Gold Evidences:• The carrying amount of the company's purchased distressed loan portfolio at December 31, 2010, was $392 million.• The net allowance was $77 million as of December 31, 2010.Steps:1. Find the combined total of net allowance and the carrying amount.• Response 1: 392 + 77 = 469 2. Calculate the percentage of net allowance in relation to the carrying amount.• Response 2: 77 392 × 100 = 19.643. Calculate the percentage of the combined total in relation to the carrying amount.• Response 3: 469 392 × 100 = 119.64The final answer is 119.64%.• Response 2: 5.5 21.1 = 0.2607GroundMultiply the result obtained inStep 2 by 100 to get the percent change.• Response 3: 0.2607 × 100 = 26.07The final answer is 26.07%.Ground
Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, 2023</p>
<p>INFOTABS: Inference on tables as semi-structured data. Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, Vivek Srikumar, 10.18653/v1/2020.acl-main.210Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Realm: Retrievalaugmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020ICML'20. JMLR.org</p>
<p>TaPas: Weakly supervised table parsing via pre-training. Jonathan Herzig, Krzysztof Pawel, Thomas Nowak, Francesco Müller, Julian Piccinno, Eisenschlos, 10.18653/v1/2020.acl-main.398Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Fin-Math: Injecting a tree-structured solver for question answering over financial reports. Chenying Li, Wenbo Ye, Yilun Zhao, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2022</p>
<p>MWP-BERT: Numeracy-augmented pre-training for math word problem solving. Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, Xiangliang Zhang, 10.18653/v1/2022.findings-naacl.74Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>TAPEX: Table pre-training via learning a neural SQL executor. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou, International Conference on Learning Representations. 2022</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, International Conference on Learning Representations (ICLR). 2023</p>
<p>LILA: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan, 10.18653/v1/2022.emnlp-main.392Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Learning math reasoning from self-sampled correct and partially-correct solutions. Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, Jianfeng Gao, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, 10.18653/v1/2022.naacl-main.191Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, Jonathan Berant, Multimodalqa: Complex question answering over text, tables and images. 2021</p>
<p>Annotating derivations: A new evaluation strategy and dataset for algebra word problems. Shyam Upadhyay, Ming-Wei Chang, Proceedings of the 15th Conference of the European Chapter. Long Papers. the 15th Conference of the European ChapterValencia, SpainAssociation for Computational Linguistics20171</p>
<p>Naturalprover: Grounded mathematical proof generation with language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, Yejin Choi, Curran Associates, Inc. Sean Welleck2022. 202235Advances in Neural Information Processing Systems</p>
<p>Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning. Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li, 10.1145/3539618.3591708Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23New York, NY, USA2023Association for Computing Machinery</p>
<p>MAmmoTH: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>MultiHiertt: reasoning over multi hierarchical tabular and textual data. Yilun Zhao, Yunxiang Li, Chenying Li, Rui Zhang, 10.18653/v1/2022.acl-long.454Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. </p>
<p>TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, Tat-Seng Chua, 10.18653/v1/2021.acl-long.254Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>A robustly optimized BERT pre-training approach with post-training. Liu Zhuang, Lin Wayne, Shi Ya, Zhao, Proceedings of the 20th Chinese National Conference on Computational Linguistics. the 20th Chinese National Conference on Computational LinguisticsHuhhot, ChinaChinese Information Processing Society of ChinaJun. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
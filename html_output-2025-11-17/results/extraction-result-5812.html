<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5812 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5812</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5812</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-4d1c856275744c0284312a3a50efb6ca9dc4cd4c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4d1c856275744c0284312a3a50efb6ca9dc4cd4c" target="_blank">Know What You Don’t Know: Unanswerable Questions for SQuAD</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> SQuadRUn is a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.</p>
                <p><strong>Paper Abstract:</strong> Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5812.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5812.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SQuAD 2.0 (SQUADRUN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SQuAD with adveRsarial Unanswerable questions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A version of the SQuAD extractive reading-comprehension dataset augmented with 53,775 human-written unanswerable questions that are adversarially crafted to be relevant and to contain plausible (but incorrect) answer spans in the context; models must decide to abstain when no supported answer exists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DocQA + ELMo (also evaluated: DocQA, BiDAF-No-Answer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Extractive reading comprehension (SQuAD / SQuAD 2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a paragraph and a question, extract an answer span from the paragraph or abstain if the paragraph does not support any answer (SQuAD 2.0 contains both answerable and adversarially-crafted unanswerable questions).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Dataset-level format change: original SQuAD only contains answerable questions; SQuAD 2.0 mixes answerable and human-adversarial unanswerable questions that are designed to look similar to answerable ones and include plausible distractor spans. Systems output either an extracted span or abstain (no-answer) and are evaluated with EM and F1 where abstaining on a negative example scores 1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original SQuAD (only answerable questions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>DocQA + ELMo: SQuAD test F1 = 85.8; SQuAD 2.0 (SQUADRUN) test F1 = 66.3 (F1 on SQUADRUN test for DocQA+ELMo). Human F1 on SQUADRUN test = 89.5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>DocQA + ELMo: SQuAD F1 85.8 vs SQuAD 2.0 F1 66.3 (representing drop when unanswerable questions are included).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-19.5 F1 (85.8 -> 66.3) for DocQA + ELMo when moving from SQuAD to SQuAD 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because original SQuAD guaranteed an answer exists, models could rely on context- and type-matching heuristics (selecting the span most related to the question) rather than verifying textual entailment; adding realistic unanswerable questions forces models to check whether the paragraph actually entails the answer, which current models struggle with.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Know What You Don’t Know: Unanswerable Questions for SQuAD', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5812.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5812.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TFIDF negatives</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TF-IDF retrieved negative examples (from Clark & Gardner 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatically generated unanswerable examples produced by pairing SQuAD questions with other paragraphs from the same article selected by TF-IDF overlap; these negatives were used as a comparison condition in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simple and effective multi-paragraph reading comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DocQA + ELMo (also evaluated: DocQA, BNA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Extractive reading comprehension with automatically-generated negatives</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Train/test on SQuAD augmented with TF-IDF retrieved negative (unanswerable) examples; the model must predict answer spans or abstain for negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Automatic negative generation via TF-IDF retrieval: existing questions are paired with other paragraphs from the same article chosen by TF-IDF similarity, producing negatives which are not guaranteed to contain plausible answers but are automatically generated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Human-adversarial unanswerable questions (SQuAD 2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>DocQA + ELMo on SQuAD + TfIDF (dev): EM = 79.4, F1 = 83.0 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>DocQA + ELMo on SQUADRUN dev: EM = 65.1, F1 = 67.6; thus performance on TfIDF-augmented data is much higher than on human-adversarial negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+15.4 F1 (83.0 vs 67.6) — models perform better on TFIDF-generated negatives than on human-adversarial negatives</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (automatic negatives easier)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Automatically generated negatives often lack the relevance or plausible answer spans required to make negatives challenging; distant supervision retrieval may return paragraphs that are not topically similar or lack plausible distractors, making them easier for models to detect.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Know What You Don’t Know: Unanswerable Questions for SQuAD', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5812.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5812.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RULEBASED negatives</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-based edited negatives (Jia & Liang 2017 method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatically created unanswerable questions formed by editing existing SQuAD questions with simple rules (entity/number swaps, antonym replacement); used in comparisons and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adversarial examples for evaluating reading comprehension systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DocQA + ELMo (also evaluated: DocQA, BNA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Extractive reading comprehension with rule-based edited negatives</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Train/test on SQuAD augmented with rule-based edited unanswerable questions produced by systematic replacements (entities/numbers/antonyms) applied to SQuAD questions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Rule-based transformation of existing answerable questions to create negatives (entity/number swaps, WordNet antonym swaps), producing unanswerable questions that are syntactically similar to originals but semantically altered.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Human-adversarial unanswerable questions (SQuAD 2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>DocQA + ELMo on SQuAD + RuleBased (dev): EM = 85.7, F1 = 89.6 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>DocQA + ELMo on SQUADRUN dev: EM = 65.1, F1 = 67.6; performance on RuleBased-augmented data substantially higher than on human adversarial negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+22.0 F1 (89.6 vs 67.6) — models perform much better on rule-based negatives than human-written adversarial negatives</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (rule-based negatives easier)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Rule-based negatives are less diverse and rely on limited transformations (entity/number/antonym swaps), which models can learn to detect via surface heuristics; thus they are easier targets than diverse human-written adversarial examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Jia & Liang (2017) showed that adversarially editing examples at test time can fool pretrained models, but models trained on similar adversarial examples are not easily fooled — implying that exposure during training can mitigate those test-time adversarial effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Know What You Don’t Know: Unanswerable Questions for SQuAD', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5812.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5812.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plausible-answer distractors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Crowdworker-provided plausible but incorrect answer spans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For each crowdworker-written unanswerable question, annotators provided a plausible answer span from the paragraph (same type as the question's expected answer) to act as a distractor; used to analyze error modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DocQA + ELMo (and human annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Analysis of distractor effectiveness in unanswerable questions</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure how often systems and humans, when wrong on unanswerable questions, select the plausible (but incorrect) answer span provided by crowdworkers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Unanswerable questions are paired with a human-provided 'plausible' span in the paragraph that matches the expected answer type and serves as an explicit distractor during analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Approximately 50% of all wrong answers on unanswerable questions (for both systems and humans) exactly matched the crowdworker-provided plausible answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (plausible distractors increase error rate / confuse models and humans)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing type-matching plausible spans creates strong distractors that exploit models' tendency to select semantically or typologically-matching spans rather than verify entailment, increasing mistaken extractions on unanswerable questions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Know What You Don’t Know: Unanswerable Questions for SQuAD', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5812.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5812.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Abstain threshold tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tuning a probability threshold on model-predicted 'unanswerable' probability to decide abstention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models predict a probability that a question is unanswerable; a threshold on this probability is tuned (on dev) to decide whether to abstain, which slightly improves performance versus taking the argmax output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BNA, DocQA, DocQA + ELMo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Decision to abstain on unanswerable questions</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Models jointly predict a distribution over answer spans and a probability that the question has no answer; at test time, abstain if 'no-answer' probability exceeds a tuned threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Output-format / decision-rule modification: instead of always outputting argmax span, models apply a tunable threshold on the predicted unanswerable probability to determine abstention.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Argmax prediction (no tuned threshold)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Tuning the abstain threshold on development data 'does slightly better' than taking the argmax prediction (no numeric delta reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (slightly)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Tuning the threshold helps accommodate different proportions of negative examples between training and test splits, producing a small but consistent gain over naive argmax decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Know What You Don’t Know: Unanswerable Questions for SQuAD', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5812.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5812.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial test-time edits (Jia & Liang)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial examples for evaluating reading comprehension systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that constructs adversarial test-time edits (e.g., appending distracting sentences) that can reliably fool pretrained SQuAD models at test time; discussed as related work and contrasted with SQuAD 2.0's human-written adversarial negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adversarial examples for evaluating reading comprehension systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pretrained SQuAD models (various architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Adversarial evaluation of reading comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate robustness by applying rule-based adversarial edits to paragraphs at test time that introduce misleading spans or contradictory information to fool models.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Test-time adversarial editing (e.g., append adversarial sentence or edit question) versus training on natural data; primarily a test-time perturbation format.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Training on similar adversarial examples (vs only test-time adversarial edits)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Jia & Liang showed their adversarial edits can fool pretrained SQuAD models at test time (no single aggregate metric given here); the present paper notes that models trained on similar examples are not easily fooled.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (pretrained models are fooled at test time), but mitigated if models are trained on similar adversarial examples</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Test-time adversarial edits exploit models' reliance on surface cues; exposure to similar adversarial examples during training can reduce vulnerability, while diverse human-written adversarial negatives (SQuAD 2.0) remain challenging even when included in training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Know What You Don’t Know: Unanswerable Questions for SQuAD', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Adversarial examples for evaluating reading comprehension systems <em>(Rating: 2)</em></li>
                <li>Simple and effective multi-paragraph reading comprehension <em>(Rating: 2)</em></li>
                <li>SQuAD: 100,000+ questions for machine comprehension of text <em>(Rating: 2)</em></li>
                <li>Making neural QA as simple as possible but not simpler <em>(Rating: 2)</em></li>
                <li>Zero-shot relation extraction via reading comprehension <em>(Rating: 1)</em></li>
                <li>NewsQA: A machine comprehension dataset <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5812",
    "paper_id": "paper-4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "SQuAD 2.0 (SQUADRUN)",
            "name_full": "SQuAD with adveRsarial Unanswerable questions",
            "brief_description": "A version of the SQuAD extractive reading-comprehension dataset augmented with 53,775 human-written unanswerable questions that are adversarially crafted to be relevant and to contain plausible (but incorrect) answer spans in the context; models must decide to abstain when no supported answer exists.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DocQA + ELMo (also evaluated: DocQA, BiDAF-No-Answer)",
            "model_size": null,
            "task_name": "Extractive reading comprehension (SQuAD / SQuAD 2.0)",
            "task_description": "Given a paragraph and a question, extract an answer span from the paragraph or abstain if the paragraph does not support any answer (SQuAD 2.0 contains both answerable and adversarially-crafted unanswerable questions).",
            "problem_format": "Dataset-level format change: original SQuAD only contains answerable questions; SQuAD 2.0 mixes answerable and human-adversarial unanswerable questions that are designed to look similar to answerable ones and include plausible distractor spans. Systems output either an extracted span or abstain (no-answer) and are evaluated with EM and F1 where abstaining on a negative example scores 1.",
            "comparison_format": "Original SQuAD (only answerable questions)",
            "performance": "DocQA + ELMo: SQuAD test F1 = 85.8; SQuAD 2.0 (SQUADRUN) test F1 = 66.3 (F1 on SQUADRUN test for DocQA+ELMo). Human F1 on SQUADRUN test = 89.5.",
            "performance_comparison": "DocQA + ELMo: SQuAD F1 85.8 vs SQuAD 2.0 F1 66.3 (representing drop when unanswerable questions are included).",
            "format_effect_size": "-19.5 F1 (85.8 -&gt; 66.3) for DocQA + ELMo when moving from SQuAD to SQuAD 2.0",
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Because original SQuAD guaranteed an answer exists, models could rely on context- and type-matching heuristics (selecting the span most related to the question) rather than verifying textual entailment; adding realistic unanswerable questions forces models to check whether the paragraph actually entails the answer, which current models struggle with.",
            "counterexample_or_null_result": null,
            "uuid": "e5812.0",
            "source_info": {
                "paper_title": "Know What You Don’t Know: Unanswerable Questions for SQuAD",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "TFIDF negatives",
            "name_full": "TF-IDF retrieved negative examples (from Clark & Gardner 2017)",
            "brief_description": "Automatically generated unanswerable examples produced by pairing SQuAD questions with other paragraphs from the same article selected by TF-IDF overlap; these negatives were used as a comparison condition in experiments.",
            "citation_title": "Simple and effective multi-paragraph reading comprehension",
            "mention_or_use": "use",
            "model_name": "DocQA + ELMo (also evaluated: DocQA, BNA)",
            "model_size": null,
            "task_name": "Extractive reading comprehension with automatically-generated negatives",
            "task_description": "Train/test on SQuAD augmented with TF-IDF retrieved negative (unanswerable) examples; the model must predict answer spans or abstain for negatives.",
            "problem_format": "Automatic negative generation via TF-IDF retrieval: existing questions are paired with other paragraphs from the same article chosen by TF-IDF similarity, producing negatives which are not guaranteed to contain plausible answers but are automatically generated.",
            "comparison_format": "Human-adversarial unanswerable questions (SQuAD 2.0)",
            "performance": "DocQA + ELMo on SQuAD + TfIDF (dev): EM = 79.4, F1 = 83.0 (Table 4).",
            "performance_comparison": "DocQA + ELMo on SQUADRUN dev: EM = 65.1, F1 = 67.6; thus performance on TfIDF-augmented data is much higher than on human-adversarial negatives.",
            "format_effect_size": "+15.4 F1 (83.0 vs 67.6) — models perform better on TFIDF-generated negatives than on human-adversarial negatives",
            "format_effect_direction": "improved (automatic negatives easier)",
            "explanation_or_hypothesis": "Automatically generated negatives often lack the relevance or plausible answer spans required to make negatives challenging; distant supervision retrieval may return paragraphs that are not topically similar or lack plausible distractors, making them easier for models to detect.",
            "counterexample_or_null_result": null,
            "uuid": "e5812.1",
            "source_info": {
                "paper_title": "Know What You Don’t Know: Unanswerable Questions for SQuAD",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "RULEBASED negatives",
            "name_full": "Rule-based edited negatives (Jia & Liang 2017 method)",
            "brief_description": "Automatically created unanswerable questions formed by editing existing SQuAD questions with simple rules (entity/number swaps, antonym replacement); used in comparisons and experiments.",
            "citation_title": "Adversarial examples for evaluating reading comprehension systems",
            "mention_or_use": "use",
            "model_name": "DocQA + ELMo (also evaluated: DocQA, BNA)",
            "model_size": null,
            "task_name": "Extractive reading comprehension with rule-based edited negatives",
            "task_description": "Train/test on SQuAD augmented with rule-based edited unanswerable questions produced by systematic replacements (entities/numbers/antonyms) applied to SQuAD questions.",
            "problem_format": "Rule-based transformation of existing answerable questions to create negatives (entity/number swaps, WordNet antonym swaps), producing unanswerable questions that are syntactically similar to originals but semantically altered.",
            "comparison_format": "Human-adversarial unanswerable questions (SQuAD 2.0)",
            "performance": "DocQA + ELMo on SQuAD + RuleBased (dev): EM = 85.7, F1 = 89.6 (Table 4).",
            "performance_comparison": "DocQA + ELMo on SQUADRUN dev: EM = 65.1, F1 = 67.6; performance on RuleBased-augmented data substantially higher than on human adversarial negatives.",
            "format_effect_size": "+22.0 F1 (89.6 vs 67.6) — models perform much better on rule-based negatives than human-written adversarial negatives",
            "format_effect_direction": "improved (rule-based negatives easier)",
            "explanation_or_hypothesis": "Rule-based negatives are less diverse and rely on limited transformations (entity/number/antonym swaps), which models can learn to detect via surface heuristics; thus they are easier targets than diverse human-written adversarial examples.",
            "counterexample_or_null_result": "Jia & Liang (2017) showed that adversarially editing examples at test time can fool pretrained models, but models trained on similar adversarial examples are not easily fooled — implying that exposure during training can mitigate those test-time adversarial effects.",
            "uuid": "e5812.2",
            "source_info": {
                "paper_title": "Know What You Don’t Know: Unanswerable Questions for SQuAD",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Plausible-answer distractors",
            "name_full": "Crowdworker-provided plausible but incorrect answer spans",
            "brief_description": "For each crowdworker-written unanswerable question, annotators provided a plausible answer span from the paragraph (same type as the question's expected answer) to act as a distractor; used to analyze error modes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DocQA + ELMo (and human annotators)",
            "model_size": null,
            "task_name": "Analysis of distractor effectiveness in unanswerable questions",
            "task_description": "Measure how often systems and humans, when wrong on unanswerable questions, select the plausible (but incorrect) answer span provided by crowdworkers.",
            "problem_format": "Unanswerable questions are paired with a human-provided 'plausible' span in the paragraph that matches the expected answer type and serves as an explicit distractor during analysis.",
            "comparison_format": null,
            "performance": "Approximately 50% of all wrong answers on unanswerable questions (for both systems and humans) exactly matched the crowdworker-provided plausible answer.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced (plausible distractors increase error rate / confuse models and humans)",
            "explanation_or_hypothesis": "Providing type-matching plausible spans creates strong distractors that exploit models' tendency to select semantically or typologically-matching spans rather than verify entailment, increasing mistaken extractions on unanswerable questions.",
            "counterexample_or_null_result": null,
            "uuid": "e5812.3",
            "source_info": {
                "paper_title": "Know What You Don’t Know: Unanswerable Questions for SQuAD",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Abstain threshold tuning",
            "name_full": "Tuning a probability threshold on model-predicted 'unanswerable' probability to decide abstention",
            "brief_description": "Models predict a probability that a question is unanswerable; a threshold on this probability is tuned (on dev) to decide whether to abstain, which slightly improves performance versus taking the argmax output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BNA, DocQA, DocQA + ELMo",
            "model_size": null,
            "task_name": "Decision to abstain on unanswerable questions",
            "task_description": "Models jointly predict a distribution over answer spans and a probability that the question has no answer; at test time, abstain if 'no-answer' probability exceeds a tuned threshold.",
            "problem_format": "Output-format / decision-rule modification: instead of always outputting argmax span, models apply a tunable threshold on the predicted unanswerable probability to determine abstention.",
            "comparison_format": "Argmax prediction (no tuned threshold)",
            "performance": "Tuning the abstain threshold on development data 'does slightly better' than taking the argmax prediction (no numeric delta reported).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (slightly)",
            "explanation_or_hypothesis": "Tuning the threshold helps accommodate different proportions of negative examples between training and test splits, producing a small but consistent gain over naive argmax decisions.",
            "counterexample_or_null_result": null,
            "uuid": "e5812.4",
            "source_info": {
                "paper_title": "Know What You Don’t Know: Unanswerable Questions for SQuAD",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Adversarial test-time edits (Jia & Liang)",
            "name_full": "Adversarial examples for evaluating reading comprehension systems",
            "brief_description": "A method that constructs adversarial test-time edits (e.g., appending distracting sentences) that can reliably fool pretrained SQuAD models at test time; discussed as related work and contrasted with SQuAD 2.0's human-written adversarial negatives.",
            "citation_title": "Adversarial examples for evaluating reading comprehension systems",
            "mention_or_use": "mention",
            "model_name": "Pretrained SQuAD models (various architectures)",
            "model_size": null,
            "task_name": "Adversarial evaluation of reading comprehension",
            "task_description": "Evaluate robustness by applying rule-based adversarial edits to paragraphs at test time that introduce misleading spans or contradictory information to fool models.",
            "problem_format": "Test-time adversarial editing (e.g., append adversarial sentence or edit question) versus training on natural data; primarily a test-time perturbation format.",
            "comparison_format": "Training on similar adversarial examples (vs only test-time adversarial edits)",
            "performance": "Jia & Liang showed their adversarial edits can fool pretrained SQuAD models at test time (no single aggregate metric given here); the present paper notes that models trained on similar examples are not easily fooled.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced (pretrained models are fooled at test time), but mitigated if models are trained on similar adversarial examples",
            "explanation_or_hypothesis": "Test-time adversarial edits exploit models' reliance on surface cues; exposure to similar adversarial examples during training can reduce vulnerability, while diverse human-written adversarial negatives (SQuAD 2.0) remain challenging even when included in training.",
            "uuid": "e5812.5",
            "source_info": {
                "paper_title": "Know What You Don’t Know: Unanswerable Questions for SQuAD",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Adversarial examples for evaluating reading comprehension systems",
            "rating": 2
        },
        {
            "paper_title": "Simple and effective multi-paragraph reading comprehension",
            "rating": 2
        },
        {
            "paper_title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "rating": 2
        },
        {
            "paper_title": "Making neural QA as simple as possible but not simpler",
            "rating": 2
        },
        {
            "paper_title": "Zero-shot relation extraction via reading comprehension",
            "rating": 1
        },
        {
            "paper_title": "NewsQA: A machine comprehension dataset",
            "rating": 1
        }
    ],
    "cost": 0.014959499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Know What You Don't Know: Unanswerable Questions for SQuAD</h1>
<p>Pranav Rajpurkar<em> Robin Jia</em> Percy Liang<br>Computer Science Department, Stanford University<br>{pranavsr,robinjia,pliang}@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQUADRUN, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQUADRUN, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQUADRUN is a challenging natural language understanding task for existing models: a strong neural system that gets $86 \%$ F1 on SQuAD achieves only $66 \%$ F1 on SQUADRUN. We release SQUADRUN to the community as the successor to SQuAD.</p>
<h2>1 Introduction</h2>
<p>Machine reading comprehension has become a central task in natural language understanding, fueled by the creation of many large-scale datasets (Hermann et al., 2015; Hewlett et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017). In turn, these datasets have spurred a diverse array of model architecture improvements (Seo et al., 2016; Hu</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Article: Endangered Species Act
Paragraph: " ... Other legislation followed, including the Migratory Bird Conservation Act of 1929, a 1937 treaty prohibiting the hunting of right and gray whales, and the Bald Eagle Protection Act of 1940. These later laws had a low cost to society-the species were relatively rare-and little opposition was raised."
Question 1: "Which laws faced significant opposition?" Plausible Answer: later laws
Question 2: "What was the name of the 1937 treaty?" Plausible Answer: Bald Eagle Protection Act</p>
<p>Figure 1: Two unanswerable questions written by crowdworkers, along with plausible (but incorrect) answers. Relevant keywords are shown in blue.
et al., 2017; Wang et al., 2017; Clark and Gardner, 2017; Huang et al., 2018). Recent work has even produced systems that surpass human-level exact match accuracy on the Stanford Question Answering Dataset (SQuAD), one of the most widely-used reading comprehension benchmarks (Rajpurkar et al., 2016).</p>
<p>Nonetheless, these systems are still far from true language understanding. Recent analysis shows that models can do well at SQuAD by learning context and type-matching heuristics (Weissenborn et al., 2017), and that success on SQuAD does not ensure robustness to distracting sentences (Jia and Liang, 2017). One root cause of these problems is SQuAD's focus on questions for which a correct answer is guaranteed to exist in the context document. Therefore, models only need to select the span that seems most related to the question, instead of checking that the answer is actually entailed by the text.</p>
<p>In this work, we construct SQUADRUN, ${ }^{1}$ a new dataset that combines the existing questions in SQuAD with 53,775 new, unanswerable ques-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>tions about the same paragraphs. Crowdworkers crafted these questions so that (1) they are relevant to the paragraph, and (2) the paragraph contains a plausible answer-something of the same type as what the question asks for. Two such examples are shown in Figure 1.</p>
<p>We confirm that SQUADRUN is both challenging and high-quality. A state-of-the-art model achieves only $66.3 \%$ F1 score when trained and tested on SQUADRUN, whereas human accuracy is $89.5 \%$ F1, a full 23.2 points higher. The same model architecture trained on SQuAD gets $85.8 \%$ F1, only 5.4 points worse than humans. We also show that our unanswerable questions are more challenging than ones created automatically, either via distant supervision (Clark and Gardner, 2017) or a rule-based method (Jia and Liang, 2017). We release SQUADRUN to the public as the successor to SQuAD, and designate it SQuAD 2.0 on the official SQuAD leaderboard. ${ }^{2}$ We are optimistc that this new dataset will encourage the development of reading comprehension systems that know what they don't know.</p>
<h2>2 Desiderata</h2>
<p>We first outline our goals for SQuADRUN. Besides the generic goals of large size, diversity, and low noise, we posit two desiderata specific to unanswerable questions:</p>
<p>Relevance. The unanswerable questions should appear relevant to the topic of the context paragraph. Otherwise, simple heuristics (e.g., based on word overlap) could distinguish answerable and unanswerable questions (Yih et al., 2013).</p>
<p>Existence of plausible answers. There should be some span in the context whose type matches the type of answer the question asks for. For example, if the question asks, "What company was founded in 1992?", then some company should be mentioned in the context. Otherwise, typematching heuristics could distinguish answerable and unanswerable questions (Weissenborn et al., 2017).</p>
<h2>3 Existing datasets</h2>
<p>Next, we survey existing reading comprehension datasets with these criteria in mind. We use the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>term "negative example" to refer to a context passage paired with an unanswerable question.</p>
<h3>3.1 Extractive datasets</h3>
<p>In extractive reading comprehension datasets, a system must extract the correct answer to a question from a context document or paragraph. The Zero-shot Relation Extraction dataset (Levy et al., 2017) contains negative examples generated with distant supervision. Levy et al. (2017) found that $65 \%$ of these negative examples do not have a plausible answer, making them easy to identify.</p>
<p>Other distant supervision strategies can also create negative examples. TriviaQA (Joshi et al., 2017) retrieves context documents from the web or Wikipedia for each question. Some documents do not contain the correct answer, yielding negative examples; however, these are excluded from the final dataset. Clark and Gardner (2017) generate negative examples for SQuAD by pairing existing questions with other paragraphs from the same article based on TF-IDF overlap; we refer to these as TFIDF examples. In general, distant supervision does not ensure the existence of a plausible answer in the retrieved context, and might also add noise, as the context might contain a paraphrase of the correct answer. Moreover, when retrieving from a small set of possible contexts, as in Clark and Gardner (2017), we find that the retrieved paragraphs are often not very relevant to the question, making these negative examples easy to identify.</p>
<p>The NewsQA data collection process also yields unanswerable questions, because crowdworkers write questions given only a summary of an article, not the full text (Trischler et al., 2017). Only $9.5 \%$ of their questions are unanswerable, making this strategy hard to scale. Of this fraction, we found that some are misannotated as unanswerable, and others are out-of-scope (e.g., summarization questions). Trischler et al. (2017) also exclude negative examples from their final dataset.</p>
<p>Jia and Liang (2017) propose a rule-based procedure for editing SQuAD questions to make them unanswerable. Their questions are not very diverse: they only replace entities and numbers with similar words, and replace nouns and adjectives with WordNet antonyms. We refer to these unanswerable questions as RULEBASED questions.</p>
<h3>3.2 Answer sentence selection datasets</h3>
<p>Sentence selection datasets test whether a system can rank sentences that answer a question higher</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reasoning</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Example</th>
<th style="text-align: center;">Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Negation</td>
<td style="text-align: center;">Negation word inserted or removed.</td>
<td style="text-align: center;">Sentence: "Several hospital pharmacies have decided to outsource high risk preparations...." <br> Question: "What types of pharmacy functions have never been outsourced?"</td>
<td style="text-align: center;">$9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Antonym</td>
<td style="text-align: center;">Antonym used.</td>
<td style="text-align: center;">S: "the extinction of the dinosaurs... allowed the tropical rainforest to spread out across the continent." Q: "The extinction of what led to the decline of rainforests?"</td>
<td style="text-align: center;">$20 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Entity Swap</td>
<td style="text-align: center;">Entity, number, or date replaced with other entity, number, or date.</td>
<td style="text-align: center;">S: "These values are much greater than the 9-88 cm as projected ... in its Third Assessment Report." Q: "What was the projection of sea level increases in the fourth assessment report?"</td>
<td style="text-align: center;">$21 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Mutual Exclusion</td>
<td style="text-align: center;">Word or phrase is mutually exclusive with something for which an answer is present.</td>
<td style="text-align: center;">S: "BSkyB... waiv[ed] the charge for subscribers whose package included two or more premium channels." Q: "What service did BSkyB give away for free unconditionally?"</td>
<td style="text-align: center;">$15 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Impossible Condition</td>
<td style="text-align: center;">Asks for condition that is not satisfied by anything in the paragraph.</td>
<td style="text-align: center;">S: "Union forces left Jacksonville and confronted a Confederate Army at the Battle of Olustee... Union forces then retreated to Jacksonville and held the city for the remainder of the war." Q: "After what battle did Union forces leave Jacksonville for good?"</td>
<td style="text-align: center;">$4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Other <br> Neutral</td>
<td style="text-align: center;">Other cases where the paragraph does not imply any answer.</td>
<td style="text-align: center;">S: "Schuenemann et al. concluded in 2011 that the Black Death... was caused by a variant of Y. pestis..." Q: "Who discovered Y. pestis?"</td>
<td style="text-align: center;">$24 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Answerable</td>
<td style="text-align: center;">Question is answerable (i.e. dataset noise).</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$7 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1: Types of negative examples in SQUADRUN exhibiting a wide range of phenomena.
than sentences that do not. Wang et al. (2007) constructed the QASent dataset from questions in the TREC 8-13 QA tracks. Yih et al. (2013) showed that lexical baselines are highly competitive on this dataset. WikiQA (Yang et al., 2015) pairs questions from Bing query logs with sentences from Wikipedia. Like TFIdF examples, these sentences are not guaranteed to have plausible answers or high relevance to the question. The dataset is also limited in scale ( 3,047 questions, 1,473 answers).</p>
<h3>3.3 Multiple choice datasets</h3>
<p>Finally, some datasets, like MCTest (Richardson et al., 2013) and RACE (Lai et al., 2017), pose multiple choice questions, which can have a "none of the above" option. In practice, multiple choice options are often unavailable, making these datasets less suited for training user-facing systems. Multiple choice questions also tend to be quite different from extractive ones, with more emphasis on fill-in-the-blank, interpretation, and summarization (Lai et al., 2017).</p>
<h2>4 The SQUADRUN dataset</h2>
<p>We now describe our new dataset, which we constructed to satisfy both the relevance and plausible answer desiderata from Section 2.</p>
<h3>4.1 Dataset creation</h3>
<p>We employed crowdworkers on the Daemo crowdsourcing platform (Gaikwad et al., 2015) to write unanswerable questions. Each task consisted of an entire article from the original SQuAD dataset. For each paragraph in the article, workers were asked to pose up to five questions that were impossible to answer based on the paragraph alone, while referencing entities in the paragraph and ensuring that a plausible answer is present. As inspiration, we also showed questions from SQuAD for each paragraph; this further encouraged unanswerable questions to look similar to answerable ones. Workers were asked to spend 7 minutes per paragraph, and were paid $\$ 10.50$ per hour. Screenshots of our interface are shown in Appendix A.1.</p>
<p>We removed questions from workers who wrote 25 or fewer questions on that article; this filter helped remove noise from workers who had trouble understanding the task, and therefore quit before completing the whole article. We applied this filter to both our new data and the existing answerable questions in SQuAD. To generate train, development, and test splits, we used the same partition of articles as SQuAD, and combined the existing SQuAD data with our new data for each split. For the SQUADRUN development and test sets, we removed articles for which we did not col-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">SQuAD</th>
<th style="text-align: right;">SQUADRUN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Total examples</td>
<td style="text-align: right;">87,599</td>
<td style="text-align: right;">130,319</td>
</tr>
<tr>
<td style="text-align: left;">Negative examples</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">43,498</td>
</tr>
<tr>
<td style="text-align: left;">Total articles</td>
<td style="text-align: right;">442</td>
<td style="text-align: right;">442</td>
</tr>
<tr>
<td style="text-align: left;">Articles with negatives</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">285</td>
</tr>
<tr>
<td style="text-align: left;">Development</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Total examples</td>
<td style="text-align: right;">10,570</td>
<td style="text-align: right;">11,873</td>
</tr>
<tr>
<td style="text-align: left;">Negative examples</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">5,945</td>
</tr>
<tr>
<td style="text-align: left;">Total articles</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">35</td>
</tr>
<tr>
<td style="text-align: left;">Articles with negatives</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">35</td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Total examples</td>
<td style="text-align: right;">9,533</td>
<td style="text-align: right;">8,862</td>
</tr>
<tr>
<td style="text-align: left;">Negative examples</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">4,332</td>
</tr>
<tr>
<td style="text-align: left;">Total articles</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">28</td>
</tr>
<tr>
<td style="text-align: left;">Articles with negatives</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">28</td>
</tr>
</tbody>
</table>
<p>Table 2: Dataset statistics of SQUADRUN, compared to the original SQuAD dataset.
lect unanswerable questions. This resulted in a roughly one-to-one ratio of answerable to unanswerable questions in these splits, whereas the train data has roughly twice as many answerable questions as unanswerable ones. Table 2 summarizes overall statistics of SQUADRUN.</p>
<h3>4.2 Human accuracy</h3>
<p>To confirm that our dataset is clean, we hired additional crowdworkers to answer all questions in the SQUADRUN development and test sets. In each task, we showed workers an entire article from the dataset. For each paragraph, we showed all associated questions; unanswerable and answerable questions were shuffled together. For each question, workers were told to either highlight the answer in the paragraph, or mark it as unanswerable. Workers were told to expect every paragraph to have some answerable and some unanswerable questions. They were asked to spend one minute per question, and were paid $\$ 10.50$ per hour.</p>
<p>To reduce crowdworker noise, we collected multiple human answers for each question and selected the final answer by majority vote, breaking ties in favor of answering questions and preferring shorter answers to longer ones. On average, we collected 4.8 answers per question. We note that for the original SQuAD, Rajpurkar et al. (2016) evaluated a single human's performance; therefore, they likely underestimate human accuracy.</p>
<h3>4.3 Analysis</h3>
<p>We manually inspected 100 randomly chosen negative examples from our development set to understand the challenges these examples present. In Table 1, we define different categories of nega-
tive examples, and give examples and their frequency in SQUADRUN. We observe a wide range of phenomena, extending beyond expected phenomena like negation, antonymy, and entity changes. In particular, SQUADRUN is much more diverse than RULEBASED, which creates unanswerable questions by applying entity, number, and antonym swaps to existing SQuAD questions. We also found that $93 \%$ of the sampled negative examples are indeed unanswerable.</p>
<h2>5 Experiments</h2>
<h3>5.1 Models</h3>
<p>We evaluated three existing model architectures: the BiDAF-No-Answer (BNA) model proposed by Levy et al. (2017), and two versions of the DocumentQA No-Answer (DocQA) model from Clark and Gardner (2017), namely versions with and without ELMo (Peters et al., 2018). These models all learn to predict the probability that a question is unanswerable, in addition to a distribution over answer choices. At test time, models abstain whenever their predicted probability that a question is unanswerable exceeds some threshold. We tune this threshold separately for each model on the development set. When evaluating on the test set, we use the threshold that maximizes F1 score on the development set. We find this strategy does slightly better than simply taking the argmax prediction, possibly due to the different proportions of negative examples at training and test time.</p>
<h3>5.2 Main results</h3>
<p>First, we trained and tested all three models on SQUADRUN, as shown in Table 3. Following Rajpurkar et al. (2016), we report average exact match and F1 scores. ${ }^{3}$ The best model, DocQA + ELMo, achieves only 66.3 F1 on the test set, 23.2 points lower than the human accuracy of 89.5 F1. Note that a baseline that always abstains gets 48.9 test F1; existing models are closer to this baseline than they are to human performance. Therefore, we see significant room for model improvement on this task. We also compare with reported test numbers for analogous model architectures on SQuAD. There is a much larger gap between humans and machines on SQUADRUN compared to SQuAD, which confirms that SQUADRUN is a much harder dataset for existing models.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>System</th>
<th>SQuAD test</th>
<th></th>
<th>SQUADRUn dev</th>
<th></th>
<th>SQUADRUn test</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>EM</td>
<td>F1</td>
<td>EM</td>
<td>F1</td>
<td>EM</td>
<td>F1</td>
</tr>
<tr>
<td>BNA</td>
<td>68.0</td>
<td>77.3</td>
<td>59.8</td>
<td>62.6</td>
<td>59.2</td>
<td>62.1</td>
</tr>
<tr>
<td>DocQA</td>
<td>72.1</td>
<td>81.0</td>
<td>61.9</td>
<td>64.8</td>
<td>59.3</td>
<td>62.3</td>
</tr>
<tr>
<td>DocQA + ELMo</td>
<td>$\mathbf{7 8 . 6}$</td>
<td>$\mathbf{8 5 . 8}$</td>
<td>$\mathbf{6 5 . 1}$</td>
<td>$\mathbf{6 7 . 6}$</td>
<td>$\mathbf{6 3 . 4}$</td>
<td>$\mathbf{6 6 . 3}$</td>
</tr>
<tr>
<td>Human</td>
<td>82.3</td>
<td>91.2</td>
<td>86.3</td>
<td>89.0</td>
<td>86.9</td>
<td>89.5</td>
</tr>
<tr>
<td>Human-Machine Gap</td>
<td>3.7</td>
<td>5.4</td>
<td>$\mathbf{2 1 . 2}$</td>
<td>$\mathbf{2 1 . 4}$</td>
<td>$\mathbf{2 3 . 5}$</td>
<td>$\mathbf{2 3 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Exact Match (EM) and F1 scores on SQUADRUN and SQuAD. The gap between humans and the best tested model is much larger on SQUADRUN, suggesting there is a great deal of room for model improvement.</p>
<table>
<thead>
<tr>
<th>System</th>
<th>SQuAD + TfIDF</th>
<th></th>
<th>SQuAD + RuleBased</th>
<th></th>
<th>SQUADRUn dev</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>EM</td>
<td>F1</td>
<td>EM</td>
<td>F1</td>
<td>EM</td>
<td>F1</td>
</tr>
<tr>
<td>BNA</td>
<td>72.7</td>
<td>76.6</td>
<td>80.1</td>
<td>84.8</td>
<td>59.8</td>
<td>62.6</td>
</tr>
<tr>
<td>DocQA</td>
<td>75.6</td>
<td>79.2</td>
<td>80.8</td>
<td>84.8</td>
<td>61.9</td>
<td>64.8</td>
</tr>
<tr>
<td>DocQA + ELMo</td>
<td>$\mathbf{7 9 . 4}$</td>
<td>$\mathbf{8 3 . 0}$</td>
<td>$\mathbf{8 5 . 7}$</td>
<td>$\mathbf{8 9 . 6}$</td>
<td>$\mathbf{6 5 . 1}$</td>
<td>$\mathbf{6 7 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Exact Match (EM) and F1 scores on the SQUADRUN development set, compared with SQuAD with two types of automatically generated negative examples. SQUADRUN is more challenging for current models.</p>
<h3>5.3 Automatically generated negatives</h3>
<p>Next, we investigated whether automatic ways of generating negative examples can also yield a challenging dataset. We trained and tested all three model architectures on SQuAD augmented with either TfIDF or RULEBASED examples. To ensure a fair comparison with SQUADRUN, we generated training data by applying TfIDF or RULEBASED only to the 285 articles for which SQUADRUN has unanswerable questions. We tested on the articles and answerable questions in the SQUADRUN development set, adding unanswerable questions in a roughly one-to-one ratio with answerable ones. These results are shown in Table 4. The highest score on SQUADRUN is 15.4 F1 points lower than the highest score on either of the other two datasets, suggesting that automatically generated negative examples are much easier for existing models to detect.</p>
<h3>5.4 Plausible answers as distractors</h3>
<p>Finally, we measured how often systems were fooled into answering the plausible but incorrect answers provided by crowdworkers for our unanswerable questions. For both computer systems and humans, roughly half of all wrong answers on unanswerable questions exactly matched the plausible answers. This suggests that the plausible answers do indeed serve as effective distractors. Full results are shown in Appendix A.2.</p>
<h2>6 Discussion</h2>
<p>SQUADRUN forces models to understand whether a paragraph entails that a certain span is the answer to a question. Similarly, recognizing
textual entailment (RTE) requires systems to decide whether a hypothesis is entailed by, contradicted by, or neutral with respect to a premise (Marelli et al., 2014; Bowman et al., 2015). Relation extraction systems must understand when a possible relationship between two entities is not entailed by the text (Zhang et al., 2017).</p>
<p>Jia and Liang (2017) created adversarial examples that fool pre-trained SQuAD models at test time. However, models that train on similar examples are not easily fooled by their method. In contrast, the adversarial examples in SQUADRUN are difficult even for models trained on examples from the same distribution.</p>
<p>In conclusion, we have presented SQUADRUN, a challenging, diverse, and large-scale dataset that forces models to understand when a question cannot be answered given the context. We are optimistic that SQUADRUN will encourage the development of new reading comprehension models that know what they don't know, and therefore understand language at a deeper level.</p>
<p>Reproducibility. All code, data, experiments are available on the Codalab platform at https : //bit.ly/2rDHBgY.</p>
<p>Acknowledgments. We would like to thank the anonymous reviewers, Arun Chaganty, Peng Qi, and Sharon Zhou for their constructive feedback. We are grateful to Durim Morina and Michael Bernstein for their help with the Daemo platform. This work was supported by funding from Facebook. R.J. is supported by an NSF Graduate Research Fellowship under Grant No. DGE-114747.</p>
<h2>References</h2>
<p>S. Bowman, G. Angeli, C. Potts, and C. D. Manning. 2015. A large annotated corpus for learning natural language inference. In Empirical Methods in Natural Language Processing (EMNLP).
C. Clark and M. Gardner. 2017. Simple and effective multi-paragraph reading comprehension. arXiv preprint arXiv:1710.10723 .
S. N. Gaikwad, D. Morina, R. Nistala, M. Agarwal, A. Cossette, R. Bhanu, S. Savage, V. Narwal, K. Rajpal, J. Regino, et al. 2015. Daemo: A self-governed crowdsourcing marketplace. In Proceedings of the 28th Annual ACM Symposium on User Interface Software \&amp; Technology. pages 101-102.
K. M. Hermann, T. Koisk, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems (NIPS).
D. Hewlett, A. Lacoste, L. Jones, I. Polosukhin, A. Fandrianto, J. Han, M. Kelcey, and D. Berthelot. 2016. Wikireading: A novel large-scale language understanding task over Wikipedia. In Association for Computational Linguistics (ACL).
M. Hu, Y. Peng, and X. Qiu. 2017. Reinforced mnemonic reader for machine comprehension. arXiv .
H. Huang, C. Zhu, Y. Shen, and W. Chen. 2018. Fusionnet: Fusing via fully-aware attention with application to machine comprehension. In International Conference on Learning Representations (ICLR).
R. Jia and P. Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In Empirical Methods in Natural Language Processing (EMNLP).
M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Association for Computational Linguistics (ACL).
G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683 .
O. Levy, M. Seo, E. Choi, and L. Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In Computational Natural Language Learning (CoNLL).
M. Marelli, S. Menini, M. Baroni, L. Bentivogli, R. bernardi, and R. Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In Language Resources and Evaluation Conference (LREC).
T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In Workshop on Cognitive Computing at NIPS.
M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. 2018. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL).
P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP).
M. Richardson, C. J. Burges, and E. Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP). pages 193-203.
M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. arXiv .
A. Trischler, T. Wang, X. Yuan, J. Harris, A. Sordoni, P. Bachman, and K. Suleman. 2017. NewsQA: A machine comprehension dataset. In Workshop on Representation Learning for NLP.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the jeopardy model? a quasi-synchronous grammar for QA. In Empirical Methods in Natural Language Processing (EMNLP).
W. Wang, N. Yang, F. Wei, B. Chang, and M. Zhou. 2017. Gated self-matching networks for reading comprehension and question answering. In Association for Computational Linguistics (ACL).
D. Weissenborn, G. Wiese, and L. Seiffe. 2017. Making neural QA as simple as possible but not simpler. In Computational Natural Language Learning (CoNLL).
Y. Yang, W. Yih, and C. Meek. 2015. WikiQA: A challenge dataset for open-domain question answering. In Empirical Methods in Natural Language Processing (EMNLP). pages 2013-2018.
W. Yih, M. Chang, C. Meek, and A. Pastusiak. 2013. Question answering using enhanced lexical semantic models. In Association for Computational Linguistics (ACL).
Y. Zhang, V. Zhong, D. Chen, G. Angeli, and C. D. Manning. 2017. Position-aware attention and supervised data improve slot filling. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ For negative examples, abstaining receives a score of 1 , and any other response gets 0 , for both exact match and F1.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ SQuAD with adveRsarial Unanswerable questions&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
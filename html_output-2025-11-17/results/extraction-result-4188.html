<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4188 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4188</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4188</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-276902702</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.05822v3.pdf" target="_blank">Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?</a></p>
                <p><strong>Paper Abstract:</strong> The potential of AI researchers in scientific discovery remains largely untapped. Over the past decade, AI for Science (AI4Science) publications in 145 Nature Index journals have increased fifteen-fold, yet they still account for less than 3% of the total publications. Drawing upon the Diffusion of Innovation theory, we project AI4Science's share of total publications to rise from 2.72% in 2024 to approximately 20% by 2050. Achieving this shift requires fully harnessing the potential of AI researchers, as nearly 95% of AI-driven research in these journals is led by experimental scientists. To facilitate this, we propose structured workflows and strategic interventions to position AI researchers at the forefront of scientific discovery. Specifically, we identify three critical pathways: equipping experimental scientists with accessible AI tools to amplify the impact of AI researchers, bridging cognitive and methodological gaps to enable more direct involvement in scientific discovery, and proactively fostering a thriving AI-driven scientific ecosystem. By addressing these challenges, we aim to empower AI researchers as key drivers of future scientific breakthroughs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4188.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4188.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-OSS-120B (term extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-OSS-120B (large reasoning language model used for scientific-term extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 120B-parameter large reasoning LLM that the authors used to extract scientific entities (terms) from titles and abstracts of AI-related papers, producing a high-frequency term dataset and enabling identification of domain patterns and topic frequency distributions across the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-OSS-120B term-extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A prompt-driven extraction pipeline using the GPT-OSS-120B large reasoning LLM to parse titles and abstracts and return scientific terms. The pipeline used low-temperature generation (initial temperature 0.0, raised to 0.3 if necessary), followed by multi-step curation (delimiter correction, splitting, normalization, prefix-based clustering, frequency aggregation, and manual merging) and LLM-human collaboration for classification into AI/Science/Not Applicable categories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-OSS-120B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>120B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multidisciplinary (natural and health sciences across Nature Index journals: biology, chemistry, materials, environmental science, medicine, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>15,000 (dataset noted as 'fifteen thousand AI-related research articles' used for term extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Term-frequency / co-occurrence and topical pattern extraction (empirical literature-derived patterns), i.e., identification of high-frequency scientific entities and their distribution across disciplines rather than explicit physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>High-frequency scientific term lists and frequency-ranked clusters (no explicit mathematical/physical laws reported in this paper from these outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Text mining of titles and abstracts via LLM prompt engineering (temperature control), followed by automated curation and clustering and manual verification.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Multi-step curation pipeline plus LLM-human collaboration for classification of extracted terms; manual adjustments for high-frequency terms (synonym merging and capitalization normalization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No quantitative performance metrics for law extraction reported; authors report reducing 20,636 initially extracted terms to 16,598 after clustering and curation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported (no numeric success rate for extraction of quantitative relationships; only term-count reduction statistics provided).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limited to titles/abstracts (not full-text or figures); potential LLM hallucinations mitigated by temperature control and human curation; does not extract explicit quantitative equations or scale laws by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No explicit baseline comparison reported for term-extraction performance in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4188.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4188.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid LLM classification workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid multi-model LLM + human verification workflow (Qwen3-32B / GPT-OSS-20B / DeepSeek R1-32B → GPT-OSS-120B → human)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical classification pipeline combining three small/medium reasoning LLMs (20–32B) with a 120B LLM and human verification to classify 20,603 AI-related articles into research-type categories, thereby distilling literature-level categorical relationships and patterns at scale with >95% reported accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hybrid voting LLM classification system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three 20–32B reasoning models (Qwen3-32B, GPT-OSS-20B, DeepSeek R1-32B) independently classify each paper (title + abstract) into four categories. If unanimous, assignment is direct; if conflicting, outputs are escalated to GPT-OSS-120B for tie-breaking; persistent disagreements are resolved by human reviewers. The pipeline included iterative prompt/search-term optimization through four rounds using random samples and a voting mechanism to reach target accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen3-32B; GPT-OSS-20B; DeepSeek R1-32B; GPT-OSS-120B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>20B–32B (small models), 120B (large model)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multidisciplinary across Nature Index journals (natural and health sciences)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>20,603 AI-related research articles (dataset used for classification)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Categorical pattern discovery (classification of articles into research-type categories), i.e., distillation of meta-level literature relationships and adoption patterns rather than physics or empirical equations.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Classification outcomes such as 72.7% of the articles categorized as 'AI for Science' and distributional patterns of author-affiliation types over time (e.g., AI-institute participation rising from 13.33% to 25.30%).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompted LLM classification of title+abstract, model voting ensemble, escalation to larger LLM on disagreement, and human verification for remaining conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Random sampling of 400 LLM classifications with four rounds of optimization; human verification applied for cases with large-model discrepancies; target/achieved classification accuracy >95%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Classification accuracy >95% reported on the sampled set; disagreement rate among large models ~3.2% (cases requiring human verification).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>>95% classification accuracy on evaluated sample (as reported by authors after optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Risk of LLM hallucinations; requires prompt and search-term optimization; limited to title+abstract inputs (not full-text); category boundaries and affiliation inference are simplifications that may miss nuanced interdisciplinary contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No formal baseline comparison to alternative automated classifiers or manual-only pipelines reported, though human verification was used as ground truth for sampled checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4188.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4188.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based agents for Literature-Based Discovery (LBD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based research agents (examples: AlphaEvolve, SciMaster's X-Master, Virtual Lab)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-driven agent systems cited as tools for automated literature-based discovery and hypothesis generation, combining LLM creativity with external tool integration and iterative refinement to uncover latent relationships or generate novel scientific hypotheses from corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based agent framework for LBD and hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>General class of systems that use large language models as agents to read and synthesize literature, propose hypotheses or solutions, and interact with auxiliary algorithms/tools (e.g., structure predictors, refinement algorithms). Examples cited include AlphaEvolve (combines LLM creativity with refinement algorithms to generate and refine mathematical/computer-science solutions), X-Master (an agent that interacts with external tools during reasoning), and Virtual Lab (LLM-PI-led, human-in-the-loop teams using pipelines like ESM-AlphaFold-Rosetta for design).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (examples in protein design, mathematics/computer science, general scientific hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Cited instance counts vary; paper cites specific demonstrations (e.g., Virtual Lab designing 92 nanobodies as reported in Swanson et al. 2025).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Discovery/design/hypothesis generation and latent-relationship identification (qualitative and sometimes quantitative hypotheses), including candidate functional relationships or design rules inferred from literature and models.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Design of 92 nanobodies using an ESM-AlphaFold-Rosetta pipeline with LLM-led planning; AlphaEvolve refining algorithmic/mathematical solutions (no explicit equations provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM-driven literature analysis (NLP) to identify latent connections, iterative planning & tool-in-the-loop refinement, and proposal of hypotheses or designs; often combined with domain-specific predictive models for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Experimental validation in cited examples (e.g., nanobody binders validated experimentally); human-in-the-loop review and downstream computational refinement algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not specified in this paper for general LLM-agent performance; cited demonstrations report successful experimental validations but without standardized quantitative metrics in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not numerically reported here; cited example yielded functional experimental binders including improved variants, but no aggregate success rate provided.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Potential for hallucination and false or low-confidence hypotheses; need for tool integration and domain-specific refinement; dependence on downstream validation (experiments) to confirm hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No systematic baseline comparisons presented in this paper; examples are reported as successful demonstrations rather than benchmarked against human-only workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4188.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4188.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based literature data extraction methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned and prompt-engineered LLM pipelines for extracting structured experimental/numerical data from the literature (examples: Polak & Morgan; Ai et al.; Gupta et al.; Polak et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of LLM-based approaches (fine-tuning, prompt engineering, multimodal LLMs) cited as being used to extract structured data (tabular/numerical/experimental parameters) from textual procedures, figures, and supplementary materials to enable downstream data-driven discovery and empirical relationship mining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based literature-to-structured-data extraction systems</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Methods include fine-tuning LLMs on domain-specific extraction tasks (e.g., organic synthesis procedure parsing), prompt-engineered conversational LLM workflows, and multimodal LLMs that combine text+figure/table parsing to harvest structured datasets from publications; pipelines often include retrieval of target articles, preprocessing, LLM-based extraction, automatic curation, and human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (organic synthesis), materials science, polymer science and other experimental sciences where numeric/parameter data appear in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Multiple cited works (examples in reference list include: Polak & Morgan 2024 Nat Commun; Polak et al. Digital Discovery 2024; Ai et al. Digital Discovery 2024; Gupta et al. Commun Mater 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of numerical experimental parameters, reaction conditions, materials property datapoints — enabling subsequent empirical relationship discovery (correlations, regressions, empirical models), but the survey paper itself does not report specific extracted laws from these pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not provided in this paper; referenced works focus on producing structured datasets (e.g., materials property tables, reaction condition datasets) that can be used to derive empirical relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Fine-tuned LLMs and prompt-engineered conversational LLMs, sometimes in multimodal configurations, applied to full methods/procedures, captions, supplementary tables, and figures; outputs go through automatic curation and human review.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Cited studies typically use human curation, spot checks, clustering, and domain-expert verification; this survey references those works but does not re-report their validation metrics in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in detail in this survey; individual cited papers likely report precision/recall or extraction accuracy but numbers are not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not specified in this survey paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM limitations in extracting complex multimodal information (figures, plots, supplementary files); risks of hallucinated numeric values; need for multimodal models and fine-tuning; domain-specific post-processing required for high precision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No systematic comparisons reported in this survey; referenced works typically compare to rule-based or simpler NLP extractors in their own publications (not summarized here).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4188.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4188.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SHapley Additive exPlanations (SHAP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic post-hoc explanation method that attributes feature-level contributions to a model's predictions using Shapley values, enabling quantitative interpretation of feature importance and interactions in scientific models and thereby distilling relationships from fitted models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SHAP feature-attribution method</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes additive feature attributions based on game-theoretic Shapley values for any predictive model (model-agnostic and model-specific variants for trees/neural nets). Widely used in the cited literature to quantify per-feature contributions to outcomes (e.g., metabolite contributions to disease risk, contributions of ingredients to material strength, nanomaterial-plant-soil interaction analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine, materials science, environmental nanoscience (examples cited in the paper include metabolomics/disease-risk studies, materials compressive-strength analysis, plant-soil interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Multiple domain studies cited that applied SHAP (e.g., Buergel et al. 2022; Yu et al. 2022; Zheng et al. 2023), plus foundational SHAP methodological papers.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Quantitative attribution relationships (feature contribution values); used to produce interpretable numerical attributions that can indicate correlations or drivers of outcomes, but not necessarily fundamental physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Quantified contributions of individual metabolites to disease risk (Buergel et al.); feature contributions and interactions affecting compressive strength of alkali-activated materials (Zheng et al.). No explicit algebraic law/equation reproduced in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Post-hoc analysis of trained predictive models to compute Shapley-value-based attributions per feature, enabling ranking and interaction inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Domain-specific validation typically performed in cited works via domain interpretation and follow-up analyses; not quantified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not applicable as SHAP is an explanation method; no accuracy metrics reported here for attribution correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported; adoption cited as evidence of utility across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Attributions depend on the underlying predictive model quality and training data; SHAP explains model behavior (which can be biased) rather than proving causal laws; computational cost can be high for large models or many features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to other explanation methods in the literature (not detailed in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>Flexible, model-agnostic method for materials data extraction from text using general purpose language models <em>(Rating: 2)</em></li>
                <li>Extracting structured data from organic synthesis procedures using a fine-tuned large language model <em>(Rating: 2)</em></li>
                <li>Agent-based learning of materials datasets from the scientific literature <em>(Rating: 2)</em></li>
                <li>MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses <em>(Rating: 2)</em></li>
                <li>Closed-loop transfer enables artificial intelligence to yield chemical knowledge <em>(Rating: 2)</em></li>
                <li>The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies <em>(Rating: 2)</em></li>
                <li>Large language models reveal big disparities in current wildfire research <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4188",
    "paper_id": "paper-276902702",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "GPT-OSS-120B (term extraction)",
            "name_full": "GPT-OSS-120B (large reasoning language model used for scientific-term extraction)",
            "brief_description": "A 120B-parameter large reasoning LLM that the authors used to extract scientific entities (terms) from titles and abstracts of AI-related papers, producing a high-frequency term dataset and enabling identification of domain patterns and topic frequency distributions across the literature.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-OSS-120B term-extraction pipeline",
            "system_description": "A prompt-driven extraction pipeline using the GPT-OSS-120B large reasoning LLM to parse titles and abstracts and return scientific terms. The pipeline used low-temperature generation (initial temperature 0.0, raised to 0.3 if necessary), followed by multi-step curation (delimiter correction, splitting, normalization, prefix-based clustering, frequency aggregation, and manual merging) and LLM-human collaboration for classification into AI/Science/Not Applicable categories.",
            "model_name": "GPT-OSS-120B",
            "model_size": "120B",
            "scientific_domain": "Multidisciplinary (natural and health sciences across Nature Index journals: biology, chemistry, materials, environmental science, medicine, etc.)",
            "number_of_papers": "15,000 (dataset noted as 'fifteen thousand AI-related research articles' used for term extraction)",
            "law_type": "Term-frequency / co-occurrence and topical pattern extraction (empirical literature-derived patterns), i.e., identification of high-frequency scientific entities and their distribution across disciplines rather than explicit physical laws.",
            "law_examples": "High-frequency scientific term lists and frequency-ranked clusters (no explicit mathematical/physical laws reported in this paper from these outputs).",
            "extraction_method": "Text mining of titles and abstracts via LLM prompt engineering (temperature control), followed by automated curation and clustering and manual verification.",
            "validation_approach": "Multi-step curation pipeline plus LLM-human collaboration for classification of extracted terms; manual adjustments for high-frequency terms (synonym merging and capitalization normalization).",
            "performance_metrics": "No quantitative performance metrics for law extraction reported; authors report reducing 20,636 initially extracted terms to 16,598 after clustering and curation.",
            "success_rate": "Not reported (no numeric success rate for extraction of quantitative relationships; only term-count reduction statistics provided).",
            "challenges_limitations": "Limited to titles/abstracts (not full-text or figures); potential LLM hallucinations mitigated by temperature control and human curation; does not extract explicit quantitative equations or scale laws by itself.",
            "comparison_baseline": "No explicit baseline comparison reported for term-extraction performance in this paper.",
            "uuid": "e4188.0",
            "source_info": {
                "paper_title": "Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Hybrid LLM classification workflow",
            "name_full": "Hybrid multi-model LLM + human verification workflow (Qwen3-32B / GPT-OSS-20B / DeepSeek R1-32B → GPT-OSS-120B → human)",
            "brief_description": "A hierarchical classification pipeline combining three small/medium reasoning LLMs (20–32B) with a 120B LLM and human verification to classify 20,603 AI-related articles into research-type categories, thereby distilling literature-level categorical relationships and patterns at scale with &gt;95% reported accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hybrid voting LLM classification system",
            "system_description": "Three 20–32B reasoning models (Qwen3-32B, GPT-OSS-20B, DeepSeek R1-32B) independently classify each paper (title + abstract) into four categories. If unanimous, assignment is direct; if conflicting, outputs are escalated to GPT-OSS-120B for tie-breaking; persistent disagreements are resolved by human reviewers. The pipeline included iterative prompt/search-term optimization through four rounds using random samples and a voting mechanism to reach target accuracy.",
            "model_name": "Qwen3-32B; GPT-OSS-20B; DeepSeek R1-32B; GPT-OSS-120B",
            "model_size": "20B–32B (small models), 120B (large model)",
            "scientific_domain": "Multidisciplinary across Nature Index journals (natural and health sciences)",
            "number_of_papers": "20,603 AI-related research articles (dataset used for classification)",
            "law_type": "Categorical pattern discovery (classification of articles into research-type categories), i.e., distillation of meta-level literature relationships and adoption patterns rather than physics or empirical equations.",
            "law_examples": "Classification outcomes such as 72.7% of the articles categorized as 'AI for Science' and distributional patterns of author-affiliation types over time (e.g., AI-institute participation rising from 13.33% to 25.30%).",
            "extraction_method": "Prompted LLM classification of title+abstract, model voting ensemble, escalation to larger LLM on disagreement, and human verification for remaining conflicts.",
            "validation_approach": "Random sampling of 400 LLM classifications with four rounds of optimization; human verification applied for cases with large-model discrepancies; target/achieved classification accuracy &gt;95%.",
            "performance_metrics": "Classification accuracy &gt;95% reported on the sampled set; disagreement rate among large models ~3.2% (cases requiring human verification).",
            "success_rate": "&gt;95% classification accuracy on evaluated sample (as reported by authors after optimization).",
            "challenges_limitations": "Risk of LLM hallucinations; requires prompt and search-term optimization; limited to title+abstract inputs (not full-text); category boundaries and affiliation inference are simplifications that may miss nuanced interdisciplinary contributions.",
            "comparison_baseline": "No formal baseline comparison to alternative automated classifiers or manual-only pipelines reported, though human verification was used as ground truth for sampled checks.",
            "uuid": "e4188.1",
            "source_info": {
                "paper_title": "Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM-based agents for Literature-Based Discovery (LBD)",
            "name_full": "LLM-based research agents (examples: AlphaEvolve, SciMaster's X-Master, Virtual Lab)",
            "brief_description": "LLM-driven agent systems cited as tools for automated literature-based discovery and hypothesis generation, combining LLM creativity with external tool integration and iterative refinement to uncover latent relationships or generate novel scientific hypotheses from corpora.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-based agent framework for LBD and hypothesis generation",
            "system_description": "General class of systems that use large language models as agents to read and synthesize literature, propose hypotheses or solutions, and interact with auxiliary algorithms/tools (e.g., structure predictors, refinement algorithms). Examples cited include AlphaEvolve (combines LLM creativity with refinement algorithms to generate and refine mathematical/computer-science solutions), X-Master (an agent that interacts with external tools during reasoning), and Virtual Lab (LLM-PI-led, human-in-the-loop teams using pipelines like ESM-AlphaFold-Rosetta for design).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Cross-domain (examples in protein design, mathematics/computer science, general scientific hypothesis generation)",
            "number_of_papers": "Cited instance counts vary; paper cites specific demonstrations (e.g., Virtual Lab designing 92 nanobodies as reported in Swanson et al. 2025).",
            "law_type": "Discovery/design/hypothesis generation and latent-relationship identification (qualitative and sometimes quantitative hypotheses), including candidate functional relationships or design rules inferred from literature and models.",
            "law_examples": "Design of 92 nanobodies using an ESM-AlphaFold-Rosetta pipeline with LLM-led planning; AlphaEvolve refining algorithmic/mathematical solutions (no explicit equations provided in this paper).",
            "extraction_method": "LLM-driven literature analysis (NLP) to identify latent connections, iterative planning & tool-in-the-loop refinement, and proposal of hypotheses or designs; often combined with domain-specific predictive models for refinement.",
            "validation_approach": "Experimental validation in cited examples (e.g., nanobody binders validated experimentally); human-in-the-loop review and downstream computational refinement algorithms.",
            "performance_metrics": "Not specified in this paper for general LLM-agent performance; cited demonstrations report successful experimental validations but without standardized quantitative metrics in this survey.",
            "success_rate": "Not numerically reported here; cited example yielded functional experimental binders including improved variants, but no aggregate success rate provided.",
            "challenges_limitations": "Potential for hallucination and false or low-confidence hypotheses; need for tool integration and domain-specific refinement; dependence on downstream validation (experiments) to confirm hypotheses.",
            "comparison_baseline": "No systematic baseline comparisons presented in this paper; examples are reported as successful demonstrations rather than benchmarked against human-only workflows.",
            "uuid": "e4188.2",
            "source_info": {
                "paper_title": "Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM-based literature data extraction methods",
            "name_full": "Fine-tuned and prompt-engineered LLM pipelines for extracting structured experimental/numerical data from the literature (examples: Polak & Morgan; Ai et al.; Gupta et al.; Polak et al.)",
            "brief_description": "A collection of LLM-based approaches (fine-tuning, prompt engineering, multimodal LLMs) cited as being used to extract structured data (tabular/numerical/experimental parameters) from textual procedures, figures, and supplementary materials to enable downstream data-driven discovery and empirical relationship mining.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-based literature-to-structured-data extraction systems",
            "system_description": "Methods include fine-tuning LLMs on domain-specific extraction tasks (e.g., organic synthesis procedure parsing), prompt-engineered conversational LLM workflows, and multimodal LLMs that combine text+figure/table parsing to harvest structured datasets from publications; pipelines often include retrieval of target articles, preprocessing, LLM-based extraction, automatic curation, and human verification.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Chemistry (organic synthesis), materials science, polymer science and other experimental sciences where numeric/parameter data appear in literature.",
            "number_of_papers": "Multiple cited works (examples in reference list include: Polak & Morgan 2024 Nat Commun; Polak et al. Digital Discovery 2024; Ai et al. Digital Discovery 2024; Gupta et al. Commun Mater 2024).",
            "law_type": "Extraction of numerical experimental parameters, reaction conditions, materials property datapoints — enabling subsequent empirical relationship discovery (correlations, regressions, empirical models), but the survey paper itself does not report specific extracted laws from these pipelines.",
            "law_examples": "Not provided in this paper; referenced works focus on producing structured datasets (e.g., materials property tables, reaction condition datasets) that can be used to derive empirical relationships.",
            "extraction_method": "Fine-tuned LLMs and prompt-engineered conversational LLMs, sometimes in multimodal configurations, applied to full methods/procedures, captions, supplementary tables, and figures; outputs go through automatic curation and human review.",
            "validation_approach": "Cited studies typically use human curation, spot checks, clustering, and domain-expert verification; this survey references those works but does not re-report their validation metrics in detail.",
            "performance_metrics": "Not reported in detail in this survey; individual cited papers likely report precision/recall or extraction accuracy but numbers are not reproduced here.",
            "success_rate": "Not specified in this survey paper.",
            "challenges_limitations": "LLM limitations in extracting complex multimodal information (figures, plots, supplementary files); risks of hallucinated numeric values; need for multimodal models and fine-tuning; domain-specific post-processing required for high precision.",
            "comparison_baseline": "No systematic comparisons reported in this survey; referenced works typically compare to rule-based or simpler NLP extractors in their own publications (not summarized here).",
            "uuid": "e4188.3",
            "source_info": {
                "paper_title": "Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SHAP",
            "name_full": "SHapley Additive exPlanations (SHAP)",
            "brief_description": "A model-agnostic post-hoc explanation method that attributes feature-level contributions to a model's predictions using Shapley values, enabling quantitative interpretation of feature importance and interactions in scientific models and thereby distilling relationships from fitted models.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "SHAP feature-attribution method",
            "system_description": "Computes additive feature attributions based on game-theoretic Shapley values for any predictive model (model-agnostic and model-specific variants for trees/neural nets). Widely used in the cited literature to quantify per-feature contributions to outcomes (e.g., metabolite contributions to disease risk, contributions of ingredients to material strength, nanomaterial-plant-soil interaction analyses).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biomedicine, materials science, environmental nanoscience (examples cited in the paper include metabolomics/disease-risk studies, materials compressive-strength analysis, plant-soil interactions).",
            "number_of_papers": "Multiple domain studies cited that applied SHAP (e.g., Buergel et al. 2022; Yu et al. 2022; Zheng et al. 2023), plus foundational SHAP methodological papers.",
            "law_type": "Quantitative attribution relationships (feature contribution values); used to produce interpretable numerical attributions that can indicate correlations or drivers of outcomes, but not necessarily fundamental physical laws.",
            "law_examples": "Quantified contributions of individual metabolites to disease risk (Buergel et al.); feature contributions and interactions affecting compressive strength of alkali-activated materials (Zheng et al.). No explicit algebraic law/equation reproduced in this survey.",
            "extraction_method": "Post-hoc analysis of trained predictive models to compute Shapley-value-based attributions per feature, enabling ranking and interaction inspection.",
            "validation_approach": "Domain-specific validation typically performed in cited works via domain interpretation and follow-up analyses; not quantified in this survey.",
            "performance_metrics": "Not applicable as SHAP is an explanation method; no accuracy metrics reported here for attribution correctness.",
            "success_rate": "Not reported; adoption cited as evidence of utility across domains.",
            "challenges_limitations": "Attributions depend on the underlying predictive model quality and training data; SHAP explains model behavior (which can be biased) rather than proving causal laws; computational cost can be high for large models or many features.",
            "comparison_baseline": "Compared conceptually to other explanation methods in the literature (not detailed in this survey).",
            "uuid": "e4188.4",
            "source_info": {
                "paper_title": "Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "Flexible, model-agnostic method for materials data extraction from text using general purpose language models",
            "rating": 2,
            "sanitized_title": "flexible_modelagnostic_method_for_materials_data_extraction_from_text_using_general_purpose_language_models"
        },
        {
            "paper_title": "Extracting structured data from organic synthesis procedures using a fine-tuned large language model",
            "rating": 2,
            "sanitized_title": "extracting_structured_data_from_organic_synthesis_procedures_using_a_finetuned_large_language_model"
        },
        {
            "paper_title": "Agent-based learning of materials datasets from the scientific literature",
            "rating": 2,
            "sanitized_title": "agentbased_learning_of_materials_datasets_from_the_scientific_literature"
        },
        {
            "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
            "rating": 2,
            "sanitized_title": "moosechem_large_language_models_for_rediscovering_unseen_chemistry_scientific_hypotheses"
        },
        {
            "paper_title": "Closed-loop transfer enables artificial intelligence to yield chemical knowledge",
            "rating": 2,
            "sanitized_title": "closedloop_transfer_enables_artificial_intelligence_to_yield_chemical_knowledge"
        },
        {
            "paper_title": "The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies",
            "rating": 2,
            "sanitized_title": "the_virtual_lab_of_ai_agents_designs_new_sarscov2_nanobodies"
        },
        {
            "paper_title": "Large language models reveal big disparities in current wildfire research",
            "rating": 1,
            "sanitized_title": "large_language_models_reveal_big_disparities_in_current_wildfire_research"
        }
    ],
    "cost": 0.017397999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?</p>
<p>Hengjie Yu 
School of Engineering
Westlake University
310030HangzhouZhejiangChina</p>
<p>Institute of Advanced Technology
Westlake Institute for Advanced Study
310024HangzhouZhejiangChina</p>
<p>Shuya Liu 
School of Engineering
Westlake University
310030HangzhouZhejiangChina</p>
<p>Haiyun Yang 
School of Engineering
Westlake University
310030HangzhouZhejiangChina</p>
<p>Yuping Yan 
Institute of Advanced Technology
Westlake Institute for Advanced Study
310024HangzhouZhejiangChina</p>
<p>Maozhen Qu 
School of Engineering
Westlake University
310030HangzhouZhejiangChina</p>
<p>Department of Chemistry
National University of Singapore
3 Science Drive 3117543Singapore, Singapore</p>
<p>Yaochu Jin jinyaochu@westlake.edu.cn 
School of Engineering
Westlake University
310030HangzhouZhejiangChina</p>
<p>Institute of Advanced Technology
Westlake Institute for Advanced Study
310024HangzhouZhejiangChina</p>
<p>Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?
38E6DB9AAC3AC4BCD076BB6B03288AA4AI for ScienceScientific discoveryDiffusion of InnovationResearch paradigmAI researcher
The potential of AI researchers in scientific discovery remains largely untapped.Over the past decade, AI for Science (AI4Science) publications in 145 Nature Index journals have increased fifteen-fold, yet they still account for less than 3% of the total publications.Drawing upon the Diffusion of Innovation theory, we project AI4Science's share of total publications to rise from 2.72% in 2024 to approximately 20% by 2050.Achieving this shift requires fully harnessing the potential of AI researchers, as nearly 95% of AI-driven research in these journals is led by experimental scientists.To facilitate this, we propose structured workflows and strategic interventions to position AI researchers at the forefront of scientific discovery.Specifically, we identify three critical pathways: equipping experimental scientists with accessible AI tools to amplify the impact of AI researchers, bridging cognitive and methodological gaps to enable more direct involvement in scientific discovery, and proactively fostering a thriving AI-driven scientific ecosystem.By addressing these challenges, we aim to empower AI researchers as key drivers of future scientific breakthroughs.</p>
<p>Introduction</p>
<p>Over the past decade, AI has become a powerful catalyst for scientific discovery (Gil et al. 2014;Xu et al. 2021;Wang et al. 2023).Since 2015, its adoption and impact have expanded rapidly across scientific disciplines, driving unprecedented growth (Gao and Wang 2024).This surge has fueled the emergence of AI for Science (AI4Science), a field that leverages machine learning techniques to tackle complex data challenges and uncover insights that were previously beyond reach.The impact of AI4Science is exemplified by the success of deep learning models in structural biology, chemistry, and biomedical research.For instance, AlphaFold (Jumper et al. 2021) and ESMFold (Lin et al. 2023) have revolutionized protein structure prediction, dramatically improving the accuracy and efficiency of computational modeling compared to traditional physics-based methods.Similarly, in molecular representation learning, models like MolCLR (Wang et al. 2022) leverage contrastive learning to enhance the predictive power of molecular property predictions, facilitating drug discovery and materials science applications.Beyond structural and molecular biology, AI, especially large language models (LLMs), has also demonstrated remarkable utility in biomedical data analysis.GPTCelltype (Hou and Ji 2024), an R package utilizing GPT-4, automates accurate cell type annotation from single-cell RNA sequencing data, greatly reducing the effort and expertise needed for this task.PathCha (Lu et al. 2024), a visionlanguage AI assistant tailored for human pathology, excels in diagnostic accuracy and user preference by integrating a specialized vision encoder with a pretrained LLM.AI has been recognized as a transformative technology and powerful paradigm in various scientific fields (Xu et al. 2021), such as health and medicine (Rajpurkar et al. 2022), materials (Choudhary et al. 2022), biology (Angermueller et al. 2016), chemistry (Baum et al. 2021), and the environment (Konya and Nematzadeh 2024).</p>
<p>Transformative technologies have periodically reshaped scientific discovery.Computational modeling and numerical simulation, such as density functional theory (Huang et al. 2023), revolutionized research in physics, chemistry, and engineering by enabling large-scale predictive simulations.The 1990s brought high-throughput experimental techniques like next-generation sequencing (Satam et al. 2023) and mass spectrometry (Heuckeroth et al. 2024), shifting biological science toward data-driven exploration.The early 2000s saw the rise of statistical learning and data science (Edfeldt et al. 2024), enhancing pattern recognition and predictive modeling across disciplines.As AI continues to reshape scientific discovery, it is essential to assess both the scope of its contributions and the challenges that remain.Moreover, "I sensed anxiety and frustration at NeurIPS'24," remarked Kyunghyun Cho (Cho 2024), receiving great attention and reflecting on the intense competition in AI research.Doctoral candidates nearing graduation and early-career AI researchers face an increasingly challenging academic and industrial landscape and struggle to secure positions due to the rapid saturation of talent in core AI fields.To ensure the long-term sustainability of the AI talent pipeline, it is crucial to broaden the application of AI across diverse fields.AI has already demonstrated significant value in accelerating scientific discovery.This article explores two fundamental questions: To what extent has AI advanced scientific discovery, and what potential remains for further expansion and innovation?Additionally, what gaps still exist in the engagement of AI researchers with scientific discovery, and how can these be bridged?</p>
<p>To answer these questions, this research examines AI-related research papers published in leading natural and health science journals over the past decade.By analyzing these publications and their author affiliations, we aim to understand the current progress of AI4Science within high-impact research and identify pathways for AI researchers to engage in this field.Furthermore, we offer practical guidance and a structured workflow to support AI researchers in embarking on scientific discovery.By highlighting key entry points and methodological approaches, we aim to lower the barriers for AI researchers seeking to contribute to scientific advancements.Ultimately, we hope this survey serves as both an inspiration and a resource, facilitating broader integration of AI into scientific discovery while simultaneously expanding the frontiers of human knowledge, fostering mutual benefits for both AI researchers and the broader scientific community.</p>
<p>Results and discussion</p>
<p>AI4Science: A rapidly growing, yet relatively minor component of scientific discovery</p>
<p>The 145 Nature Index journals, selected by an expert panel of active scientists for their reputation and impact, have long been central venues for groundbreaking research in the natural and health sciences.To examine the role of AI in high-impact scientific discovery, we compiled a dataset of 20,603 AI-related research articles published in these journals over the past decade, drawing from the Web of Science Core Collection.Although our search terms targeted AI-related studies, not all retrieved articles fell into the category of AI for Science.A classification of research types was therefore essential.Here, we defined four categories of research: Typical AI Research refers to studies focused on the development or analysis of AI methods, such as machine learning, neural networks, deep learning, algorithms, or hardware, without direct application to natural or health sciences; AI for Science encompasses studies that apply AI techniques to address scientific challenges in fields such as materials science, chemistry, physics, biology, and medicine, where AI is explicitly used to enhance discovery, prediction, data analysis, or scientific understanding; Science for AI denotes research that leverages findings from natural or cognitive sciences to inspire or improve AI models, architectures, or hardware, with the goal of advancing AI itself rather than solving scientific problems; Not Applicable covers studies outside these domains, including those unrelated to AI, those applying AI in non-scientific contexts, or those where AI is mentioned only peripherally.</p>
<p>Manual classification is both time-consuming and impractical, given the need to categorize more than 100,000 entries, first by research type, and subsequently by institutional affiliations and scientific terms.The increasing amount of literature creates major challenges for analysis, as traditional methods like reviews and meta-analyses are limited to a small number of studies (Gurevitch et al. 2018).With the rapid growth of publications, AI tools, especially LLMs, provide an effective solution by analyzing large volumes of research (Lin et al. 2024;Luo et al. 2024;Ahad et al. 2024).Models like ChatGPT can handle complex tasks, such as recognizing textual entities, and have shown remarkable abilities, making them ideal for efficiently summarizing knowledge and identifying research gaps (Singhal et al. 2023;Hagendorff et al. 2023).LLMs were employed here for basic classification tasks, without requiring complex understanding or detailed outputs.However, precautions are necessary to mitigate the inherent risk of hallucinations associated with these models (Farquhar et al. 2024).</p>
<p>To balance efficiency with accuracy, we developed a hybrid workflow that combines automated reasoning models with human verification (Fig. 1).This approach integrates large and small reasoning models, a voting mechanism, and optimization approaches for search terms, prompts, and model usage, ensuring the accuracy of LLM classifications.For cases with significant discrepancies in large models (only 3.2%), human verification was applied.Through random sampling of 400 LLM classification results, we conducted four rounds of optimization on search terms, model usage, and prompts, achieving a classification accuracy of over 95%.Ultimately, 72.7% of the articles were categorized under AI for Science.The process begins with the use of three powerful 20B-32B small reasoning models (Qwen3-32B, GPT-OSS-20B, and DeepSeek R1-32B) released in 2025.These models classify the input articles-based on their title and abstract-into four categories: AI for Science, Science for AI, Typical AI Research, and Not Applicable.If all three models consistently classify a paper as AI for Science, it is directly categorized as such.For entries with conflicting results, a 120B large reasoning model (GPT-OSS-120B) is used.If the large model's classification aligns with the majority of the small models' results, the paper is assigned to that category.In cases of further disagreement, human classification is applied.A random sample of 400 AI-determined classifications is assessed for accuracy.If the accuracy is below 95%, improvements are made to the search terms, prompts, models, and evaluation strategies.The displayed results represent the final classifications.</p>
<p>Our analysis reveals that the number of AI-related articles exhibits a rapid increase over the last ten years, with both the absolute number and proportion rising across 145 Nature Index journals (Fig. 2A and Fig. S1).The growth in AI-related research is especially evident in recent years, with the number of such articles in 2024 being fifteen times higher than in 2015, and the relative proportion of AI-related articles has also surged, reaching twelve times the level observed in 2015.This significant rise highlights the escalating adoption of AI methodologies across various scientific disciplines.Notably, 21 journals each published over 50 AI-related articles in 2024 alone, indicating a broad integration of AI.Furthermore, 16 journals had more than 5% of their total publications focused on AI, with five surpassing 9%.Among these, Nature Methods stands out, with AI-related articles comprised 15.87% of its total output in 2024, reflecting the pivotal role of AI in advancing specific fields.However, they still represent only 2.72% of total publications, indicating that AI is still in the early stages of widespread adoption in science.</p>
<p>The Diffusion of Innovation theory (Rogers et al. 2014), developed by Everett Rogers, describes how new ideas and technologies spread through a population over time.It posits that adoption follows an S-curve, with innovations initially embraced by a small group of innovators and early adopters, followed by the early majority, late majority, and finally laggards.Based on this pattern, we anticipate that AI-related research will continue to grow rapidly over the next few decades, with its expansion gradually slowing as it reaches maturity (Fig. 2B).As the innovation matures, its adoption rate increases rapidly, then slows as it becomes widely accepted and integrated into everyday practices.AI's application in scientific discovery is likely following a similar pattern.Early adopters in fields like computational biology, materials science, and datadriven research have led the charge, demonstrating the transformative potential of AI.As more researchers integrate AI into their work and the technology becomes more accessible, adoption will spread to the early majority and late majority, solidifying its place as a central tool in scientific research.By 2050, we project that AI-related research will stabilize at around 20% of the total Nature Index journal publications, reflecting AI's fully integrated role in modern research.Projected growth trend of AI-related research in Nature Index journals based on a logistic growth model.This estimate follows the S-shaped curve derived from the Diffusion of Innovation theory and is calibrated using historical data from the past decade.The projection assumes that AI for Science will account for 20% of Nature Index publications by 2050.The shaded area represents the upper and lower bounds of this 20% projection.</p>
<p>Experimental scientists lead AI4Science research</p>
<p>To elucidate the roles of AI researchers in the current AI for Science wave, we analyzed 61,759 author affiliations from 20,603 AI-related articles.These affiliations were categorized into three groups: AI Institute, Science Institute, or Not Applicable (for those not clearly affiliated with either AI or Science).The reasoning LLM-human collaboration classification workflow is depicted in Fig. S2, which is similar to the workflow for research type (Fig. 1) but with slight adjustments.</p>
<p>The trends in author affiliations across these 20,603 AI-related research articles from 2015 to 2024 are illustrated in Fig. 3. AI institutions are increasingly involved in scientific discovery, yet they largely maintain a supportive role despite their growing significance.The average number of AI institutions per paper has risen from 0.16 in 2015 to 0.38 in 2024, a 1.38-fold increase (Fig. 3A).In contrast, the average number of institutions per research article and the total number of scientific institutions have remained relatively stable, experiencing a decline between 2018 and 2022, followed by a slight upward trend in recent years.The proportion of papers with AI institutions as author affiliations has grown from 13.33% to 25.30%, a 0.90-fold increase (Fig. 3B).Additionally, AI institute-led research saw a notable rise in 2016, followed by a gradual and uneven upward trend in the subsequent years (Fig. 3C), with the average percentage over the past three years at 5.66%.Similarly, the average ranking of AI institutions as first authors has improved from 6.97 in the first year to 3.68 over the past nine years.However, scientific institutions continue to dominate AI-driven scientific discovery, with the average ranking of the first-listed scientific institution remaining stable at around 1.1, shifting slightly from 1.09 in the first three years to 1.12 in the latest three years (Fig. 3D).Recognizing that most of these journals may not be as familiar to AI researchers, we have also conducted analysis on their participation in five well-known interdisciplinary journals that are also familiar within the AI research community, including Nature, Nature Communications, Science, Science Advances, and PNAS.The five journals contribute 3,262 articles, accounting for 21.7% of the overall AI for Science publications.This trend is unsurprising, as AI researchers are more actively engaged in these interdisciplinary journals compared to the broader Nature Index journals.In 2015, the average number of AI-affiliated institutions per article, the proportion of articles involving AI institutions, and the percentage of articles with AI institutions as the first affiliation were all twice as high as the average for Nature Index journals (Fig. S3).By 2024, AI institutions were involved in 36.3% of published articles, but only 10.7% of articles listing an AI institution as the first affiliation.Furthermore, after 2021, AI institutions' participation reached a plateau, suggesting a stabilization phase in their contribution to these journals.This stagnation may indicate that AI's integration into specific scientific fields has transitioned from a phase of rapid expansion to one of consolidation.</p>
<p>Unlocking the potential of AI researchers in scientific discovery: Three key directions</p>
<p>AI has been and will continue to accelerate the process of scientific discovery.While experimental scientists are increasingly leveraging AI tools in their research, the full potential of AI researchers in driving scientific breakthroughs remains largely untapped (Fig. 3).Achieving the projected growth from 2.72% to 20% in AI-driven scientific contributions will require the active and extensive involvement of AI researchers.We identify three key directions, which are both ongoing and yet to be fully defined, that are essential to realizing this transformation, as illustrated in Fig. 4. Developing user-friendly AI tools for scientific discovery presents a significant opportunity for AI researchers, enhancing the impact and practical value of their work.Moreover, such advancements hold the potential to bridge the gap between AI technology and real-world applications, facilitating the transition from research to practical products.Equipping experimental scientists with AI tools-such as data collection (Polak and Morgan 2024), modeling and analysis (Krenn et al. 2022a), and experiment design and conduction (Gottweis et al. 2025)-will accelerate the discovery process across a range of scientific fields.In fact, early adopters among experimental scientists have already begun utilizing AI tools in their research.As shown in Fig. 3B and Fig. 3C, 75% of AI-related articles are currently independently authored by experimental scientists, who also lead the majority of collaborative research efforts.The next key step in advancing the application of AI is the development of more userfriendly tools and platforms, which will be discussed in Section 2.4.This is crucial for further driving the adoption of AI among experimental scientists.</p>
<p>The success of AI4Science hinges not only on experimental scientists adopting AI tools but also on the development of proactive AI researchers who are deeply embedded in scientific discovery.These researchers will bridge the gap between AI and domainspecific knowledge, developing novel AI algorithms and models tailored to specific scientific challenges, thus driving the next generation of AI-powered discoveries.The proportion of AI researchers leading AI-assisted scientific discoveries has increased (Fig. 3C).However, it is also observed that the rate of increase has stagnated in recent years, and the overall number remains relatively small.We attribute this limitation to two main gaps: a cognitive gap (regarding what scientific discoveries AI can be applied to) and a methodological gap (in terms of the workflow in which AI researchers lead scientific discoveries).Discussions on bridging these gaps will be presented in Section 2.5 and Section 2.6, respectively.</p>
<p>Collaboration is a key driver of AI4Science research, and this collaboration extends beyond individual studies to a broader research ecosystem.A quintessential example of this is the SHapley Additive exPlanations (SHAP) (Lundberg and Lee 2017;Lundberg et al. 2020), a model interpretation method, initially proposed by AI researchers and widely adopted across various scientific disciplines.For instance, SHAP has been used to quantify the contributions of individual metabolites to disease risk, identifying key metabolites influencing the risk of 24 investigated diseases (Buergel et al. 2022).It has also been applied to interpret nanomaterial-plantenvironment interactions, providing insights into the factors affecting the root uptake of metal-oxide nanoparticles and their interactions within the soil (Yu et al. 2022).Additionally, SHAP was employed to analyze the contributions of raw ingredients and their interactions, offering valuable insights into the factors influencing the compressive strength of alkali-activated materials (Zheng et al. 2023).This is also facilitated by the development of an easily accessible package by the authors (https://shap.readthedocs.io/),which can be considered a key factor in enabling the widespread use of AI tools by experimental scientists.Another reason is that, for experimental scientists, model explanation is often more important than marginal improvements in model performance, as it enables the extraction of hypotheses that can inform subsequent research.Such an ecosystem would significantly accelerate the progress of AI4Science, creating fertile ground for innovation and driving advancements in scientific discovery.Moreover, the establishment of standards and best practices in AI implementation across scientific discovery will ensure reproducibility, transparency, and broader adoption.</p>
<p>Four approaches to equipping experimental scientists with AI</p>
<p>AI will not replace scientists because scientific discovery requires human intuition, creativity, and the ability to navigate uncertainty-capabilities that AI lacks (Makarov et al. 2021;Messeri and Crockett 2024).However, scientists who leverage AI can process vast amounts of data, automate complex analyses, and generate insights more efficiently, giving them a significant advantage in accelerating breakthroughs and expanding the frontiers of knowledge (Oviedo et al. 2022;Kapoor et al. 2024).Here, we discuss four key approaches, both established and emerging, that enable researchers to harness AI effectively, lowering barriers to adoption while accelerating scientific discovery (Fig. 5).These approaches not only illustrate how experimental scientists can harness AI tools but also present significant opportunities for AI and software researchers.Building user-friendly platforms of this kind, akin to existing statistical analysis software but enhanced with AI capabilities, can bridge the gap between advanced machine learning and practical scientific applications, enabling broader adoption and innovation.User-friendly AI platforms can streamline data-driven research by automating model selection, training, and model explanation (Fig. 5A).Scientists provide raw data, specify prediction targets, and define constraints such as time and computational resources.The platform then constructs an appropriate model, performs analysis, and generates an explainable report.Model explanation and the resulting reports are critical components as they directly contribute to scientific understanding, which is one of the primary objectives of scientific research (Krenn et al. 2022b).Besides, many scientific fields lack sufficient experimental data, yet AI can still provide meaningful insights by integrating domain knowledge with LLMs (Fig. 5B).Although powerful, LLMs face challenges such as generating false information, relying on outdated data, and lacking clear logical reasoning.A promising solution to address these issues is retrievalaugmented generation, which incorporates information from external repositories to enhance the accuracy of outputs, particularly in knowledge-intensive tasks (Lewis et al. 2020;Lewis et al. 2020).Researchers can input relevant prompts and background information into a platform, which then constructs a domain-specific vector database.By leveraging retrieval-augmented inference, the system generates predictions in zeroor few-shot learning scenarios.This approach enables scientists to explore hypotheses and obtain preliminary insights even in data-scarce environments.In fact, experimental scientists are increasingly adept at using AI-driven methods to tackle both data-rich (Pyzer-Knapp et al. 2022;Wong et al. 2024) and data-scarce (Chen et al. 2024;Liu et al. 2024) scientific problems.User-friendly platforms will further accelerate this process by making these tools more accessible, encouraging broader adoption among scientists, and ultimately empowering AI to play a larger role in driving scientific discovery.</p>
<p>Data is essential for driving machine learning advancements in scientific discovery, as its quality and quantity directly influence the accuracy and reliability of predictions (Liu et al. 2023).A vast amount of historical data is embedded in published literature, with an ever-growing volume emerging in scientific journal articles.However, its unstructured format, including both natural language text and figures, presents significant obstacles for immediate use by modern informatics systems that rely on structured datasets (Gupta et al. 2024).Therefore, the extraction of usable data from these articles is crucial for AI-assisted scientific discovery.While LLMs have advanced the way data is extracted from literature (Polak et al. 2024), their ability to extract complex text data remains limited.Additionally, their capacity to collect data from images and related research documents, such as supplementary tables, is still underdeveloped.However, with the development of more powerful LLMs, especially those fine-tuned specifically for data extraction (Ai et al. 2024), and the rise of multimodal LLMs, we anticipate a significant enhancement in data extraction capabilities.Researchers specify target articles and parameters of interest, and the platform retrieves, preprocesses, and extracts relevant information using LLMs (Fig. 5C).The resulting dataset is automatically curated and formatted, accelerating the process of literature-based data collection and synthesis.This approach facilitates metaanalyses, comparative studies, and data-driven hypothesis generation by efficiently aggregating knowledge from published research.Furthermore, AI can further revolutionize scientific experimentation by autonomously designing and executing experiments (Fig. 5D).Researchers define hypotheses, control variables, and desired outputs, while an AI-powered platform-enhanced by knowledge-augmented reasoning-develops an optimized experimental plan.Robotics then execute the experiments, and a validation module assesses the reliability and quality of the data.Although AI-and robot-driven automated laboratories are still in their early stages, initial applications have already emerged (Szymanski et al. 2023;Angello et al. 2024), demonstrating remarkable potential to transform scientific research (Angelopoulos et al.).This approach minimizes manual labor, increases reproducibility, and enables high-throughput hypothesis testing, ultimately accelerating scientific progress.</p>
<p>Scientific domain "ocean" with AI application potential</p>
<p>AI offers immense potential in scientific discovery, yet many opportunities remain underexplored by AI researchers.Currently, the application of AI in science is largely concentrated in fields with abundant structured datasets, such as foundational life sciences and medical research.Prominent examples include protein structure and function prediction, single-cell annotation, and drug discovery (Fig. 6A).The success of AI in these areas is largely driven by the availability of well-curated, large-scale datasets and clearly defined machine learning tasks.However, scientific discovery extends far beyond these domains, and numerous critical challenges in other fields remain largely untapped by AI researchers.</p>
<p>Fig. 6 Research areas with high AI involvement and strong application potential. (A)</p>
<p>Hot topics in AI-driven scientific research.These are prominent research areas identified through an analysis of leading AI-for-Science publications.The fields listed are currently benefiting from abundant, well-structured datasets and clearly defined machine learning tasks, such as protein prediction and drug discovery.(B) Scientific domains with broad AI application potential.This "ocean" of domains represents a forward-looking perspective on the vast areas where AI is poised to make a significant impact.These domains were curated by our team to highlight complex, multidisciplinary problems where new AI approaches are needed to handle unstructured data and integrate diverse knowledge sources.(C) High-frequency scientific terms from titles and abstracts.These terms were extracted using a LLM (GPT-OSS-120B) from a dataset of fifteen thousand AI-related research articles.This provides a data-driven view of the current focus of AI applications in science, highlighting the fields where AI is already actively being used to address fundamental research questions.</p>
<p>Beyond life sciences, AI holds transformative potential in broader scientific domains such as materials design, material-biology-environment interactions, biosynthesis and chemical synthesis, environmental assessment and remediation, climate prediction, and industrial process optimization (Fig. 6B).These areas pose complex, multidimensional problems that require AI-driven approaches for data integration, modeling, prediction, and explanation.Despite their importance, these domains have not received comparable attention from the AI research community, often due to the lack of easily accessible datasets or the need for specialized domain knowledge to frame AI-driven solutions effectively.</p>
<p>To map the current landscape of AI-driven scientific discovery, we analyzed the titles and abstracts of AI-related research articles published in high-impact scientific journals.</p>
<p>To extract scientific terms from research articles, we applied a large reasoning LLM model (GPT-OSS-120B) to titles and abstracts.Each identified term was classified into one of three categories using a LLM-human collaboration workflow: AI, Science, or Not Applicable.By identifying high-frequency scientific entities (Fig. 6C), we highlight the fields where AI is already being employed by experimental scientists to address fundamental research questions.AI researchers can expand their impact by developing advanced algorithms tailored to complex, unstructured scientific data.Additionally, AI-driven approaches to data acquisition, experimental design, and realtime analysis can accelerate discovery by optimizing research workflows.</p>
<p>AI4Science workflow for AI researchers: From tool providers to key contributors</p>
<p>There are many AI4Science workflows for scientific discovery in various fields, such as proteomics (Mann et al. 2021), materials (Pyzer-Knapp et al. 2022), biomedicine (Gao et al. 2024), and medical imaging (Najjar 2023), but most are designed for researchers in specific fields rather than for AI researchers.We propose an AI4Science workflow for AI researchers (Fig. 7).</p>
<p>The first and most crucial step is to identify suitable scientific problems, which can be guided by the researchers' interests, experience, and available resources.Areas where AI has already been applied may be a good starting point (Fig. 6C), as experimental scientists may have used basic machine learning methods to address such problems.Literature-based-discovery (LBD) has been used to bridge the growing gap in scientific knowledge by systematically uncovering hidden relationships between disparate research areas (Henry and McInnes 2017).As scientific literature grows exponentially, the application of LBD and interdisciplinary knowledge transfer techniques (Cunningham et al. 2025) is becoming increasingly important for synthesizing knowledge across fields, facilitating interdisciplinary collaboration, and accelerating the pace of discovery.Understanding the chosen scientific domain is a prerequisite for conducting efficient research, and with the development of LLM-based agents (Schmidgall et al. 2025), this will become easier.LLMs have significantly enhanced LBD by offering advanced natural language processing capabilities that enable the automated generation of novel scientific ideas (Hu et al. 2024) and scientific hypotheses (Yang et al. 2024).Moreover, researcher agents based on LLMs have been developed to aid scientific discovery.For instance, Google DeepMind's AlphaEvolve combines the creativity of an LLM with algorithms that refine and filter mathematical and computer science solutions (Novikov et al. 2025).This system allows for both innovative idea generation and solution improvement.Similarly, SciMaster's X-Master is a general-purpose AI agent designed to interact flexibly with external tools during reasoning (Chai et al. 2025).By mimicking the dynamic problem-solving process of human researchers, X-Master creates a feedback loop: tool outputs refine the agent's reasoning, while better reasoning leads to more effective tool use, facilitating smarter scientific discovery.These models can analyze large volumes of text, identify latent patterns, and suggest potential connections between previously unrelated areas of research.LLM-based agents have been used in scientific discovery.For example, the Virtual Lab, an LLM-PI-led, human-in-the-loop agent team, used an ESM-AlphaFold-Rosetta pipeline to design 92 nanobodies, with experiments validating functional binders, including two improved against JN.1/KP.3variants (Swanson et al. 2025).</p>
<p>Overall, AI plays three main roles across different fields: prediction, comprehension, and innovation (discovery or design).These roles are logically connected.For example, in the case of proteins, one can first predict the structure or functional annotation of a protein (Gligorijević et al. 2021), then understand the relationship between the sequence or structure and function (Wang et al. 2025), and finally, based on this understanding, design new proteins to achieve specific functions (Jiang et al. 2024).In data-rich fields, one can skip the understanding step, but in many cases, understanding is closely tied to scientific discovery.Formal experiments can generally be divided into three steps: data collection, modeling and analysis, and experimental validation.For AI researchers, we do not recommend collecting data through experiments or simulations, as this increases the difficulty of conducting research.Instead, we suggest focusing on fields with available datasets or those where data can be gathered from literature.In data-rich fields, the competition is more intense.For many fields, there is a lack of publicly available high-quality data, and extracting data from literature can be an effective way to enhance competitiveness.Existing LLM and AI tools (Ansari and Moosavi 2024;Polak and Morgan 2024) also make data collection from literature easier.Modeling and analysis are areas AI researchers are familiar with, so we will not elaborate further, but we emphasize the importance of model explanation and causal inference.Because scientific understanding is one of the main aims of science (Krenn et al. 2022b).Finally, we recommend experimental validation of the results obtained through computational methods.Many experiments have standardized procedures, and collaborating with other labs, shared experimental platforms, or commercialized research service institutions can help complete this step.Furthermore, the advancement of automated laboratories powered by LLMs and robotics is set to streamline experimental validation (M.Bran et al. 2024;Angelopoulos et al.), making it as accessible as the use of AI tools by experimental scientists.</p>
<p>Limitations and outlook</p>
<p>While this study is limited to AI-related research in 145 expert-selected Nature Index journals, we make this choice because the study focuses on the natural and health sciences, and these journals constitute the primary venues for scientific discovery in those fields.Our binary affiliation scheme (AI institute versus science institute) and the use of first affiliation to infer primary contribution are operational simplifications that cannot fully capture interdisciplinary contribution structures (e.g., co-first authorship, joint labs, field-specific norms), yet they enable a scalable analysis that yields a macrolevel quantitative portrait and action-oriented guidance.Despite these limitations, the study offers a fresh lens that reframes AI researchers not merely as tool providers but as potential co-leaders of scientific discovery, quantifies the current participation gap, contextualizes growth via Diffusion of Innovation projections, and presents practical workflows and strategic interventions to empower both AI researchers and experimental scientists.</p>
<p>Unlocking the potential of AI researchers is not about replacing experimental scientists but about achieving a more effective division of labor.Many scientific areas depend on making predictions and extracting insights from complex data, tasks where AI researchers excel, while problem formulation, experimental design, and mechanistic validation rely on the deep domain expertise of experimental scientists.Moreover, LLM-based research agents are already being applied to real-world scientific discovery (M.Bran et al. 2024;Swanson et al. 2025;Penadé s et al. 2025), underscoring an inevitable trend that makes deeper AI-researcher involvement not only beneficial but necessary.At present, AI-researcher-led scientific discoveries represent roughly 5% of AI4Science publication in these journals; raising this share could catalyze a healthier cross-disciplinary ecosystem.In such an ecosystem, AI researchers would better understand scientific questions, experimental scientists would acquire foundational AI literacy, and more cross-disciplinary institutes, joint centers, and dual-PI teams would emerge to tackle a focused set of critical scientific problems.This structural shift can accelerate breakthroughs while creating more sustainable career pathways for earlycareer AI researchers at the intersection of AI and the sciences.</p>
<p>Conclusion</p>
<p>AI4Science has seen rapid growth over the past decade, yet its full potential remains untapped, particularly the contributions of AI researchers themselves.While AI is increasingly integrated into scientific research, its application is still largely driven by experimental scientists, with AI researchers often playing a supportive role.This imbalance limits the depth of AI's impact on scientific discovery.To accelerate progress, this work highlights the need to unlock the expertise of AI researchers, positioning them as active contributors rather than just tool developers.By fostering deeper collaboration and bridging cognitive and methodological gaps, AI researchers can drive more transformative advancements and reshape the landscape of scientific discovery.Looking ahead, the future of AI4Science depends on a well-defined human-AI collaboration paradigm: one that leverages AI's analytical power while ensuring human researchers remain in control of scientific reasoning and validation.Addressing challenges such as AI hallucinations and setting clear boundaries for AI applications will be essential.Moreover, experimental validation stands as the ultimate safeguard of AI-driven discoveries across most disciplines, upholding scientific rigor.With these strategic shifts, AI4Science can expand from its current 2.72% share of publications in Nature Index journal to 20% by 2050, unlocking unprecedented opportunities for scientific discovery.</p>
<p>Methods</p>
<p>Data collection</p>
<p>AI-related research articles published in 145 Nature Index journals over the past decade were retrieved from the Web of Science Core Collection.The search query used was [SO="Journal" AND PY=2015-2024 AND DT=Article AND TS=("machine learning" OR "deep learning" OR "artificial intelligence" OR "AI" OR "data-driven" OR "neural network" OR "convolutional network" OR "residual network" OR "long short-term memory" OR "multilayer perceptron" OR "transformer architecture" OR "transformer model" OR "vision transformer" OR "foundation model" OR "pretrained model" OR "ensemble learning" OR "gradient boosting" OR "XGBoost" OR "LightGBM" OR "CatBoost" OR "random forest" OR "decision tree" OR "tree-based model" OR "support vector machine" OR "k-nearest neighbors" OR "naive bayes" OR "bayesian modeling" OR "bayesian learning" OR "generative adversarial network" OR "autoencoder" OR "genetic algorithm" OR "optimization algorithm" OR "reinforcement learning" OR "transfer learning" OR "few-shot learning" OR "zero-shot learning" OR "semi-supervised learning" OR "self-supervised learning" OR "unsupervised learning" OR "federated learning" OR "attention mechanism" OR "selfattention" OR "cross-attention" OR "multihead attention" OR "language model" OR "natural language processing" OR "computer vision" OR "image processing" OR "speech recognition" OR "audio processing")], which yielded a total of 20,603 AIrelated research articles.The retrieved data included information such as titles, abstracts, publication years, author affiliations, and journal names.To obtain the total number of research articles published in these journals during the same period, we used the search query (SO="Journal name" AND PY=2015-2024 AND DT=Article), which yielded a total of 1.16 million research articles.</p>
<p>Research type classification</p>
<p>Although the search terms targeted AI-related studies, not all retrieved articles were classified as AI for Science, necessitating a classification of research types.We defined four categories of research: Typical AI Research, AI for Science, Science for AI, and Not Applicable.Given the scale of the dataset, manual classification was impractical.Therefore, we employed a hybrid workflow that combines automated reasoning models with human verification (Fig. 1).Large and small reasoning models, a voting mechanism, and optimization of search terms, prompts, and model usage were integrated to ensure classification accuracy.The meta prompt is provided in Supplemental Information.The process begins with three state-of-the-art 20-32B reasoning models, Qwen3-32B (Yang et al. 2025), GPT-OSS-20B, and DeepSeek R1-32B (Guo et al. 2025), released in 2025.Articles (title and abstract) unanimously classified as AI for Science by all three models were directly assigned.In cases of disagreement, a larger 120B model (GPT-OSS-120B) was introduced.If the large model's output aligned with the majority decision of the smaller models, the article was categorized accordingly.To address LLMs' discrepancies in classification, human verification was applied.A random sample of 400 LLM classifications was assessed for accuracy, leading to four rounds of optimization, which resulted in a classification accuracy exceeding 95%.</p>
<p>Author affiliation classification</p>
<p>To classify author affiliations, we applied the similar hybrid workflow used in Research Type Classification, utilizing both large and small reasoning models (Fig. S2).The meta prompt is provided in Supplemental Information.Affiliations were categorized into three groups: AI Institute, Science Institute, and Not Applicable.AI Institute includes institutions explicitly focused on AI, machine learning, or related fields.Science Institute refers to institutions dedicated to research in natural or health sciences, like biology, chemistry, or medicine, without reference to AI.Not Applicable includes non-research organizations, comprehensive research institutions, or institutions where AI or scientific research is not the primary focus.Unlike Research Type Classification, which prioritizes the accuracy of classifying AI for Science papers, Author Affiliation Classification equally emphasizes all categories.Each affiliation was considered independently, with discrepancies resolved through the use of a large reasoning model or human classification when necessary.</p>
<p>Scientific entity extraction and curation</p>
<p>The GPT-OSS-120B was deployed to extract terms from the titles and abstracts of research articles.The meta prompt is provided in Supplemental Information.Following extraction, we implemented a multi-step curation pipeline to ensure the reliability of the term dataset (shown in Fig. S5).First, inconsistent delimiter usage (e.g., excessive commas in place of semicolons) was corrected, and entries with abnormally short or malformed strings were filtered out.Terms were then split into individual entities and stripped of surrounding whitespace and punctuation.We standardized capitalization (first-letter only) and aggregated terms by frequency.To unify semantically similar entries, we normalized terms by removing case, punctuation, and hyphenation, then clustered those with shared prefixes.Finally, high-frequency terms were filtered by character length and word count.Remaining terms were clustered into groups of similar expressions and ranked by total frequency.The resulting list was exported for downstream analysis.</p>
<p>Scientific entity classification and word cloud plotting</p>
<p>Using the LLM-human collaboration workflow (shown in Fig. S6), each identified term was classified into one of three categories based on a structured prompt: AI (terms directly related to artificial intelligence technologies, such as machine learning, neural networks, and deep learning), Science (terms associated with natural or health sciences, such as physics, biology, or medicine), or Not Applicable (general or unrelated terms not specific to either domain).The meta prompt is provided in Supplemental Information.The high-frequency terms were subject to minor manual adjustments, including the merging of obvious synonyms and standardization of capitalization, to enhance clarity and consistency in the visualization.After classification, we utilized the wordcloud package to visualize the high-frequency terms, providing a clear representation of the most commonly mentioned entities in the dataset.) was used to extract scientific terms from titles and abstracts.The initial temperature was set to 0.0.For entries where appropriate scientific terms could not be extracted, the temperature was adjusted to 0.3.A total of 20,636 terms were initially extracted, which was subsequently reduced to 16,598 after clustering.#Output Format# Only output one of the following three categories: "Typical AI Research", "AI for Science", "Science for AI", or "Not Applicable".DO NOT provide any explanation, commentary, or any other text.</p>
<p>""" messages = [ {"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": f"Journal Name: {journal_name}\n" f"Article Title: {article_title}\n" f"Abstract: {abstract}"} ] *[Reasoning: high] is only used for GPT-OSS models.research focus, non-research commercial organizations, and institutions whose research does not explicitly indicate a link to AI or scientific domains.Additionally, some institutions that house both AI and science research teams, such as certain research labs or large multi-disciplinary institutions, should be classified as Not Applicable if they do not explicitly focus on either AI or science in isolation.</p>
<h1>Output Format# Only output one of the following three categories per input: "AI Institute", "Science Institute", or "Not Applicable".Do not provide any explanation, justification, or additional text.</h1>
<p>""" messages = [ {"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": f"#Input address: {address}"} ] *[Reasoning: high] is only used for GPT-OSS models.</p>
<p>Fig. 1
1
Fig. 1 Workflow for classifying research articles in collaboration with reasoning models and human oversight.The process begins with the use of three powerful 20B-32B small reasoning models (Qwen3-32B, GPT-OSS-20B, and DeepSeek R1-32B) released in 2025.These models classify the input articles-based on their title and abstract-into four categories: AI for Science, Science for AI, Typical AI Research, and Not Applicable.If all three models consistently classify a paper as AI for Science, it is directly categorized as such.For entries with conflicting results, a 120B large reasoning model (GPT-OSS-120B) is used.If the large model's classification aligns with the majority of the small models' results, the paper is assigned to that category.In cases of further disagreement, human classification is applied.A random sample of 400</p>
<p>Fig. 2
2
Fig. 2 Publication and projected growth trends of AI-related research articles in 145 Nature Index journals.(A) Publication trends of 20,603 AI-related research articles in Nature Index journals (2015-2024).Publication proportions are shown in Fig. S1.(B) Projected growth trend of AI-related research in Nature Index journals based on a logistic growth model.This estimate follows the S-shaped curve derived from the Diffusion of Innovation theory and is calibrated using historical data from the past decade.The projection assumes that AI for Science will account for 20% of Nature Index publications by 2050.The shaded area represents the upper and lower bounds of this 20% projection.</p>
<p>Fig. 3
3
Fig. 3 Trends in author affiliations of 20,603 AI-related research articles (2015-2024).(A) Number of institutions per article by institution type.(B) Number and proportion of articles involving AI-related institutions.(C) Number and proportion of articles with</p>
<p>Fig. 4
4
Fig. 4 Schematic representation of three key strategies for unlocking the potential of AI researchers in scientific discovery: (1) equipping experimental scientists with userfriendly AI tools, (2) enabling AI researchers to take a more direct and active role in scientific discovery, and (3) fostering a thriving AI-driven scientific ecosystem to sustain long-term innovation.</p>
<p>Fig. 5
5
Fig. 5 Schematic diagram of four approaches to equipping experimental scientists with AI. (A) User-friendly platforms facilitate modeling and explanation when researchers have access to sufficient data.(B) Domain-enhanced LLMs enable zero-shot and fewshot inference in data-scarce scenarios.(C) The integration of LLMs, multimodal systems, and multi-task frameworks makes automated data extraction from literature increasingly feasible.(D) Advances in LLMs and robotics are driving the development of autonomous experimental platforms, freeing researchers from labor-intensive experiments.</p>
<p>Fig. 7
7
Fig. 7 AI4Science workflow for AI researchers.</p>
<p>Fig. S1
S1
Fig. S1 Publication ratios of fifteen thousand AI-related research articles in 145 Nature Index journals (2015-2024).</p>
<p>Fig. S2
S2
Fig. S2 Workflow for classifying author affiliations in collaboration with reasoning models and human oversight.</p>
<p>Fig. S3
S3
Fig. S3 Trends in publication count and author affiliations of 3,262 AI-related research articles published in five well-known multidisciplinary journals (2015-2024), including Nature, Nature Communications, Science, Science Advances, and PNAS.(A) Number of publications by year.(B) Number of institutions per article by institution type.(C) Number and proportion of articles involving AI-related institutions.(D) Number and proportion of articles with AI-related institutions as the first affiliation.(E) Rank of the first-affiliated AI and Science institution.The shaded area represents the 95% confidence interval.</p>
<p>Fig. S4
S4
Fig. S4 High-frequency AI terms in the titles and abstracts of fifteen thousand AIrelated research articles.</p>
<p>Fig. S5
S5
Fig. S5Workflow for the extraction and curation of scientific terms.A 120B large reasoning model (GPT-OSS-120B) was used to extract scientific terms from titles and abstracts.The initial temperature was set to 0.0.For entries where appropriate scientific terms could not be extracted, the temperature was adjusted to 0.3.A total of 20,636 terms were initially extracted, which was subsequently reduced to 16,598 after clustering.</p>
<p>Fig. S6
S6
Fig. S6 Workflow for classifying scientific terms in collaboration with reasoning models and human oversight.</p>
<p>AcknowledgementsThis work is supported by Westlake Education Foundation under the Grant No. 103110846022301 and China Postdoctoral Science Foundation under the Grant No. 2024M762941.Declarations#Task# Given one affiliation string from a scientific paper, classify each affiliation into one of the following categories:1. <strong>AI Institute</strong>: The affiliation explicitly includes terms or names related to Artificial Intelligence (AI), its subfields, or broader intelligence-related fields.This includes departments, centers, labs, or institutes whose primary focus is on artificial intelligence, machine learning, deep learning, neural networks, computer science, reinforcement learning, meta-learning, big data, data science, natural language processing, computer vision, or other intelligent interdisciplinary research.Any institution with renowned names or organizations typically associated with AI-such as "Turing," "DeepMind," "OpenAI," "Mila," "FAIR," or similar-should be classified as AI-focused.Additionally, institutions that integrate AI research with other domains, such as AI in healthcare, bioinformatics, intelligent robotics, or interdisciplinary AI studies, should also be classified as AI-focused, as these fields heavily rely on AI technology.2. <strong>Science Institute</strong>: The affiliation is primarily engaged in research within the natural sciences or health sciences.This includes fields such as biology, chemistry, physics, environmental science, pharmacy, medicine, engineering, agricultural science, and related disciplines.Institutions classified as Science Institutes typically focus on understanding the natural world, biological systems, or the physical and chemical properties of matter.Typical terms in the affiliation may include "Faculty of Science," "Department of Chemistry," "Institute of Biomedicine," "School of Engineering," "Department of Physics," "Institute of Environmental Science," "College of Medicine," or similar.Additionally, these institutions will not include any terms or references related to Artificial Intelligence (AI) or its subfields within the affiliation address.These institutions are characterized by their focus on empirical, experimental, and applied research in scientific fields that do not explicitly focus on AI. 4. <em>Hyphenation and Abbreviations</em>: Follow the common rules for using hyphens in academic writing (e.g., "data-driven", "state-of-the-art", "long-term").Retain full original terminology if the abbreviation is not well-known or commonly used in the field.5. <em>Avoid Generic Terms</em>: Do not include overly broad or generic terms that do not contribute to understanding the specific scientific focus of the paper.3. <em>Not Applicable</em> Terms that do not fall under AI or Science.This category includes general terms, tools, or concepts that don't directly relate to either field, such as generic methods, statistical terms, or other unrelated technical phrases.#Output Format# Only output one of the following categories: "AI", "Science", or "Not Applicable".DO NOT provide any explanation, commentary, or any other text."""messages = [ {"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": f"#Input term: {term}"} ] *[Reasoning: high] is only used for GPT-OSS models.
Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis. J I Ahad, R M Sultan, A Kaikobad, F Rahman, M R Amin, N Mohammed, S Rahman, 2024 IEEE International Conference on Big Data (BigData). Washington, DC, USAIEEE2024</p>
<p>Extracting structured data from organic synthesis procedures using a fine-tuned large language model. Q Ai, F Meng, J Shi, B Pelkie, C W Coley, 10.1039/D4DD00091ADigital Discovery. 392024</p>
<p>Closed-loop transfer enables artificial intelligence to yield chemical knowledge. N H Angello, D M Friday, C Hwang, S Yi, A H Cheng, T C Torres-Flores, E R Jira, W Wang, A Aspuru-Guzik, M D Burke, C M Schroeder, Y Diao, Jackson Ne, 10.1038/s41586-024-07892-1Nature. 63380292024</p>
<p>Alterovitz R Transforming science labs into automated factories of discovery. A Angelopoulos, J F Cahoon, 10.1126/scirobotics.adm6991Science Robotics. 9956991</p>
<p>Deep learning for computational biology. C Angermueller, T Pä Rnamaa, L Parts, O Stegle, 10.15252/msb.20156651Molecular Systems Biology. 1278782016</p>
<p>Agent-based learning of materials datasets from the scientific literature. M Ansari, S M Moosavi, 10.1039/D4DD00252KDigital Discovery. 3122024</p>
<p>Artificial Intelligence in Chemistry: Current Trends and Future Directions. Z J Baum, X Yu, P Y Ayala, Y Zhao, S P Watkins, Q Zhou, 10.1021/acs.jcim.1c00619J Chem Inf Model. 6172021</p>
<p>Metabolomic profiles predict individual multidisease outcomes. T Buergel, J Steinfeldt, G Ruyoga, M Pietzner, D Bizzarri, D Vojinovic, Upmeier Zu Belzen, J Loock, L Kittner, P Christmann, L Hollmann, N Strangalies, H Braunger, J M Wild, B Chiesa, S T Spranger, J Klostermann, F Van Den Akker, E B Trompet, S Mooijaart, S P Sattar, N Jukema, J W Lavrijssen, B Kavousi, M Ghanbari, M Ikram, M A Slagboom, E Kivimaki, M Langenberg, C Deanfield, J Eils, R Landmesser, U , 10.1038/s41591-022-01980-3Nat Med. 28112022</p>
<p>J Chai, S Tang, R Ye, Y Du, X Zhu, M Zhou, Y Wang, E W Zhang, Y Zhang, L Chen, S , 10.48550/ARXIV.2507.05241SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam. 2025</p>
<p>ProBID-Net: a deep learning model for protein-protein binding interface design. Z Chen, M Ji, J Qian, Z Zhang, X Zhang, H Gao, H Wang, R Wang, Y Qi, 10.1039/D4SC02233EChem Sci. 15472024</p>
<p>2024) I sensed anxiety and frustration at NeurIPS'24. K Cho, 5 Feb 2025</p>
<p>Recent advances and applications of deep learning methods in materials science. K Choudhary, B Decost, C Chen, A Jain, F Tavazza, R Cohn, C W Park, A Choudhary, A Agrawal, Sjl Billinge, E Holm, S P Ong, C Wolverton, 10.1038/s41524-022-00734-6npj Comput Mater. 81592022</p>
<p>Facilitating interdisciplinary knowledge transfer with research paper recommender systems. E Cunningham, B Smyth, D Greene, 10.1162/qss.a.9Quantitative Science Studies :1-22. 2025</p>
<p>A data science roadmap for open science organizations engaged in early-stage drug discovery. K Edfeldt, A M Edwards, O Engkvist, J Günther, M Hartley, D G Hulcoop, A R Leach, B D Marsden, A Menge, L Misquitta, S Müller, D R Owen, K T Schütt, N Skelton, A Steffen, A Tropsha, E Vernet, Y Wang, J Wellnitz, T M Willson, D-A Clevert, B Haibe-Kains, L H Schiavone, M Schapira, 10.1038/s41467-024-49777-xNat Commun. 15156402024</p>
<p>Detecting hallucinations in large language models using semantic entropy. S Farquhar, J Kossen, L Kuhn, Y Gal, 10.1038/s41586-024-07421-0Nature. 63080172024</p>
<p>Quantifying the use and potential benefits of artificial intelligence in scientific research. J Gao, D Wang, 10.1038/s41562-024-02020-5Nat Hum Behav. 8122024</p>
<p>Empowering biomedical discovery with AI agents. S Gao, A Fang, Y Huang, V Giunchiglia, A Noori, J R Schwarz, Y Ektefaie, J Kondic, M Zitnik, 10.1016/j.cell.2024.09.022Cell. 187222024</p>
<p>Amplify scientific discovery with artificial intelligence. Y Gil, M Greaves, J Hendler, H Hirsh, 10.1126/science.1259439Science. 34662062014</p>
<p>Structure-based protein function prediction using graph convolutional networks. V Gligorijević, P D Renfrew, T Kosciolek, J K Leman, D Berenberg, T Vatanen, C Chandler, B C Taylor, I M Fisk, H Vlamakis, R J Xavier, R Knight, K Cho, R Bonneau, 10.1038/s41467-021-23303-9Nat Commun. 12131682021</p>
<p>. J Gottweis, W-H Weng, A Daryin, T Tu, A Palepu, P Sirkovic, A Myaskovsky, F Weissenberger, K Rong, R Tanno, K Saab, D Popovici, J Blum, F Zhang, K Chou, A Hassidim, B Gokturk, A Vahdat, P Kohli, Y Matias, A Carroll, K Kulkarni, N Tomasev, Y Guan, V Dhillon, E D Vaishnav, B Lee, Trd Costa, Penadé S Jr, G Peltz, Y Xu, A Pawlosky, A Karthikesalingam, V Natarajan, 10.48550/ARXIV.2502.18864Towards an AI co-scientist. 2025</p>
<p>DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning. D Guo, D Yang, H Zhang, J Song, P Wang, Q Zhu, R Xu, R Zhang, S Ma, X Bi, X Zhang, X Yu, Y Wu, Z F Wu, Z Gou, Z Shao, Z Li, Z Gao, A Liu, B Xue, B Wang, B Wu, B Feng, C Lu, C Zhao, C Deng, C Ruan, D Dai, D Chen, Ji D Li, E Lin, F Dai, F Luo, F Hao, G Chen, G Li, G Zhang, H Xu, H Ding, H Gao, H Qu, H Li, H Guo, J Li, J Chen, J Yuan, J Tu, J Qiu, J Li, J Cai, J L Ni, J Liang, J Chen, J Dong, K Hu, K You, K Gao, K Guan, K Huang, K Yu, K Wang, L Zhang, L Zhao, L Wang, L Zhang, L Xu, L Xia, L Zhang, M Zhang, M Tang, M Zhou, M Li, M Wang, M Li, M Tian, N Huang, P Zhang, P Wang, Q Chen, Q Du, Q Ge, R Zhang, R Pan, R Wang, R Chen, R J Jin, R L Chen, R Lu, S Zhou, S Chen, S Ye, S Wang, S Yu, S Zhou, S Pan, S Li, S S Zhou, S Wu, S Yun, T Pei, T Sun, T Wang, T Zeng, W Liu, W Liang, W Gao, W Yu, W Zhang, W , Xiao Wl , An W Liu, X Wang, X Chen, X Nie, X Cheng, X Liu, X Xie, X Liu, X Yang, X Li, X Su, X Lin, X Li, X Q , Jin X Shen, X Chen, X Sun, X Wang, X Song, X Zhou, X Wang, X Shan, X Li, Y K Wang, Y Q Wei, Y X Zhang, Y Xu, Y Li, Y Zhao, Y Sun, Y Wang, Y Yu, Y Zhang, Y Shi, Y Xiong, Y Zz, Z Ren, Z Sha, Z Fu, Z Xu, Z Xie, Z Zhang, Z Hao, Z Ma, Z Yan, Z Wu, Z Gu, Z Zhu, Z Liu, Z Li, Z Xie, Z Song, Z Pan, Z Huang, Z Xu, Z Zhang, Z Zhang, 10.1038/s41586-025-09422-zNature. 64580812025</p>
<p>Data extraction from polymer literature using large language models. S Gupta, A Mahmood, P Shetty, A Adeboye, R Ramprasad, 10.1038/s43246-024-00708-9Commun Mater. 512692024</p>
<p>Meta-analysis and the science of research synthesis. J Gurevitch, J Koricheva, S Nakagawa, G Stewart, 10.1038/nature25753Nature. 55576952018</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. T Hagendorff, S Fabi, M Kosinski, 10.1038/s43588-023-00527-xNat Comput Sci. 3102023</p>
<p>Literature Based Discovery: Models, methods, and trends. S Henry, B T Mcinnes, 10.1016/j.jbi.2017.08.011Journal of Biomedical Informatics. 742017</p>
<p>Reproducible mass spectrometry data processing and compound annotation in MZmine 3. S Heuckeroth, T Damiani, A Smirnov, O Mokshyna, C Brungs, A Korf, J D Smith, P Stincone, N Dreolin, L-F Nothias, T Hyötyläinen, M Orešič, U Karst, P C Dorrestein, D Petras, X Du, Jjj Van Der Hooft, R Schmid, T Pluskal, 10.1038/s41596-024-00996-yNat Protoc. 1992024</p>
<p>Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis. W Hou, Ji Z , 10.1038/s41592-024-02235-4Nat Methods. 2182024</p>
<p>X Hu, H Fu, J Wang, Y Wang, Z Li, R Xu, Y Lu, Jin Y Pan, L Lan, Z , 10.48550/ARXIV.2410.14255An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas. Nova2024</p>
<p>The central role of density functional theory in the AI age. B Huang, Von Rudorff, G F , Von Lilienfeld, O A , 10.1126/science.abn3445Science. 38166542023</p>
<p>PocketFlow is a data-and-knowledge-driven structure-based molecular generative model. Y Jiang, G Zhang, J You, H Zhang, R Yao, H Xie, L Zhang, Z Xia, M Dai, Y Wu, L Li, S Yang, 10.1038/s42256-024-00808-8Nat Mach Intell. 632024</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, Saa Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, 10.1038/s41586-021-03819-2Nature. 59678732021</p>
<p>REFORMS: Consensus-based Recommendations for Machine-learning-based. S Kapoor, E M Cantrell, K Peng, T H Pham, C A Bail, O E Gundersen, J M Hofman, J Hullman, M A Lones, M M Malik, P Nanayakkara, R A Poldrack, I D Raji, M Roberts, M J Salganik, M Serra-Garcia, B M Stewart, G Vandewiele, A Narayanan, 10.1126/sciadv.adk3452Science. Sci Adv. 101834522024</p>
<p>Recent applications of AI to environmental disciplines: A review. A Konya, P Nematzadeh, 10.1016/j.scitotenv.2023.167705Science of The Total Environment. 9061677052024</p>
<p>On scientific understanding with artificial intelligence. M Krenn, R Pollice, S Y Guo, M Aldeghi, A Cervera-Lierta, P Friederich, Passos Dos, G Gomes, F Hä Se, A Jinich, A Nigam, Z Yao, A Aspuru-Guzik, 10.1038/s42254-022-00518-3Nat Rev Phys. 4122022a</p>
<p>On scientific understanding with artificial intelligence. M Krenn, R Pollice, S Y Guo, M Aldeghi, A Cervera-Lierta, P Friederich, Passos Dos, G Gomes, F Hä Se, A Jinich, A Nigam, Z Yao, A Aspuru-Guzik, 10.1038/s42254-022-00518-3Nat Rev Phys. 4122022b</p>
<p>Rocktä schel T, others (2020) Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W Yih, Advances in neural information processing systems. 33</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Z Lin, H Akin, R Rao, B Hie, Z Zhu, W Lu, N Smetanin, R Verkuil, O Kabeli, Y Shmueli, Dos Santos Costa, A Fazel-Zarandi, M Sercu, T Candido, S Rives, A , 10.1126/science.ade2574Science. 37966372023</p>
<p>Large language models reveal big disparities in current wildfire research. Z Lin, A Chen, X Wang, Z Liu, S Piao, 10.1038/s43247-024-01341-7Commun Earth Environ. 511682024</p>
<p>A prompt-engineered large language model, deep learning workflow for materials classification. S Liu, T Wen, Asls Pattamatta, D J Srolovitz, 10.1016/j.mattod.2024.08.028Materials Today. 802024</p>
<p>Data quantity governance for machine learning in materials science. Y Liu, Z Yang, X Zou, S Ma, D Liu, M Avdeev, S Shi, 10.1093/nsr/nwad125National Science Review. 1071252023</p>
<p>A multimodal generative AI copilot for human pathology. M Y Lu, B Chen, Dfk Williamson, R J Chen, M Zhao, A K Chow, K Ikemura, A Kim, D Pouli, A Patel, A Soliman, C Chen, T Ding, J J Wang, G Gerber, I Liang, L P Le, A V Parwani, L L Weishaupt, F Mahmood, 10.1038/s41586-024-07618-3Nature. 63480332024</p>
<p>S Lundberg, S-I Lee, 10.48550/ARXIV.1705.07874A Unified Approach to Interpreting Model Predictions. 2017</p>
<p>From local explanations to global understanding with explainable AI for trees. S M Lundberg, G Erion, H Chen, A Degrave, J M Prutkin, B Nair, R Katz, J Himmelfarb, N Bansal, S-I Lee, 10.1038/s42256-019-0138-9Nat Mach Intell. 212020</p>
<p>Potential Roles of Large Language Models in the Production of Systematic Reviews and Meta-Analyses. X Luo, F Chen, D Zhu, L Wang, Z Wang, H Liu, M Lyu, Y Wang, Q Wang, Y Chen, 10.2196/56780J Med Internet Res. 26e567802024</p>
<p>Augmenting large language models with chemistry tools. M Bran, A Cox, S Schilter, O Baldassari, C White, A D Schwaller, P , 10.1038/s42256-024-00832-8Nat Mach Intell. 652024</p>
<p>Best practices for artificial intelligence in life sciences research. V A Makarov, T Stouch, B Allgood, C D Willis, N Lynch, 10.1016/j.drudis.2021.01.017Drug Discovery Today. 2652021</p>
<p>Artificial intelligence for proteomics and biomarker discovery. M Mann, C Kumar, W-F Zeng, M T Strauss, 10.1016/j.cels.2021.06.006Cell Systems. 1282021</p>
<p>Artificial intelligence and illusions of understanding in scientific research. L Messeri, M J Crockett, 10.1038/s41586-024-07146-0Nature. 6272024</p>
<p>Redefining Radiology: A Review of Artificial Intelligence Integration in Medical Imaging. R Najjar, 10.3390/diagnostics13172760Diagnostics. 131727602023</p>
<p>AlphaEvolve: A coding agent for scientific and algorithmic discovery. A Novikov, N Vũ, M Eisenberger, E Dupont, P-S Huang, A Z Wagner, S Shirobokov, B Kozlovskii, Fjr Ruiz, A Mehrabian, M P Kumar, See A Chaudhuri, S Holland, G Davies, A Nowozin, S Kohli, P Balog, M , 10.48550/ARXIV.2506.131312025</p>
<p>Interpretable and Explainable Machine Learning for Materials Science and Chemistry. F Oviedo, J L Ferres, T Buonassisi, K T Butler, 10.1021/accountsmr.1c00244Acc Mater Res. 362022</p>
<p>AI mirrors experimental science to uncover a mechanism of gene transfer crucial to bacterial evolution. J R Penadé S, J Gottweis, L He, J B Patkowski, A Daryin, W-H Weng, T Tu, A Palepu, A Myaskovsky, A Pawlosky, V Natarajan, A Karthikesalingam, Trd Costa, 10.1016/j.cell.2025.08.018S00928674250097302025</p>
<p>Flexible, model-agnostic method for materials data extraction from text using general purpose language models. M P Polak, S Modi, A Latosinska, J Zhang, C-W Wang, S Wang, A D Hazra, D Morgan, 10.1039/D4DD00016ADigital Discovery. 362024</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. M P Polak, D Morgan, 10.1038/s41467-024-45914-8Nat Commun. 15115692024</p>
<p>Accelerating materials discovery using artificial intelligence, high performance computing and robotics. E O Pyzer-Knapp, J W Pitera, Pwj Staar, S Takeda, T Laino, D P Sanders, J Sexton, J R Smith, A Curioni, 10.1038/s41524-022-00765-zComput Mater. 81842022</p>
<p>AI in health and medicine. P Rajpurkar, E Chen, O Banerjee, E J Topol, 10.1038/s41591-021-01614-0Nat Med. 2812022</p>
<p>Diffusion of innovations. E M Rogers, A Singhal, M M Quinlan, An integrated approach to communication theory and research. Routledge2014</p>
<p>Next-Generation Sequencing Technology: Current Trends and Advancements. H Satam, K Joshi, U Mangrolia, S Waghoo, G Zaidi, S Rawool, R P Thakare, S Banday, A K Mishra, G Das, S K Malonia, 10.3390/biology12070997Biology. 1279972023</p>
<p>S Schmidgall, Y Su, Z Wang, X Sun, J Wu, X Yu, J Liu, Z Liu, E Barsoum, 10.48550/ARXIV.2501.04227Agent Laboratory: Using LLM Agents as Research Assistants. 2025</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, P Payne, M Seneviratne, P Gamble, C Kelly, A Babiker, N Schä Rli, A Chowdhery, P Mansfield, D Demner-Fushman, Agüera Y Arcas, B Webster, D Corrado, G S Matias, Y Chou, K Gottweis, J Tomasev, N Liu, Y Rajkomar, A Barral, J Semturs, C Karthikesalingam, A Natarajan, V , 10.1038/s41586-023-06291-2Nature. 62079722023</p>
<p>The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies. K Swanson, W Wu, N L Bulaong, J E Pak, J Zou, 10.1038/s41586-025-09442-9Nature. 2025</p>
<p>An autonomous laboratory for the accelerated synthesis of novel materials. N J Szymanski, B Rendy, Y Fei, R E Kumar, T He, D Milsted, M J Mcdermott, M Gallant, E D Cubuk, A Merchant, H Kim, A Jain, C J Bartel, K Persson, Y Zeng, G Ceder, 10.1038/s41586-023-06734-wNature. 62479902023</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, A Anandkumar, K Bergen, C P Gomes, S Ho, P Kohli, J Lasenby, J Leskovec, T-Y Liu, A Manrai, D Marks, B Ramsundar, L Song, J Sun, J Tang, P Veličković, M Welling, L Zhang, C W Coley, Y Bengio, M Zitnik, 10.1038/s41586-023-06221-2Nature. 62079722023</p>
<p>DPFunc: accurately predicting protein function via deep learning with domain-guided structure information. W Wang, Y Shuai, M Zeng, Fan W Li, M , 10.1038/s41467-024-54816-8Nat Commun. 161702025</p>
<p>Molecular contrastive learning of representations via graph neural networks. Y Wang, J Wang, Z Cao, Barati Farimani, A , 10.1038/s42256-022-00447-xNat Mach Intell. 432022</p>
<p>Discovery of a structural class of antibiotics with explainable deep learning. F Wong, E J Zheng, J A Valeri, N M Donghia, M N Anahtar, S Omori, A Li, A Cubillos-Ruiz, A Krishnan, Jin W Manson, A L Friedrichs, J Helbig, R Hajian, B Fiejtek, D K Wagner, F F Soutter, H H Earl, A M Stokes, J M Renner, L D Collins, J J , 10.1038/s41586-023-06887-8Nature. 62679972024</p>
<p>Artificial intelligence: A powerful paradigm for scientific research. Y Xu, X Liu, X Cao, C Huang, E Liu, S Qian, X Liu, Y Wu, F Dong, C-W Qiu, J Qiu, K Hua, W Su, J Wu, H Xu, Y Han, C Fu, Z Yin, M Liu, R Roepman, S Dietmann, M Virta, F Kengara, Z Zhang, L Zhang, T Zhao, J Dai, J Yang, L Lan, M Luo, Z Liu, T An, B Zhang, X He, S Cong, X Liu, W Zhang, J P Lewis, J M Tiedje, Q Wang, An Z Wang, F Zhang, L Huang, T Lu, C Cai, Z Wang, F Zhang, J , 10.1016/j.xinn.2021.100179The Innovation. 241001792021</p>
<p>. A Yang, A Li, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Gao, C Huang, C Lv, C Zheng, D Liu, F Zhou, F Huang, F Hu, H Ge, H Wei, H Lin, J Tang, J Yang, J Tu, J Zhang, J Yang, J Yang, J Zhou, J Zhou, J Lin, K Dang, K Bao, K Yang, L Yu, L Deng, M Li, M Xue, M Li, P Zhang, P Wang, Q Zhu, R Men, R Gao, S Liu, S Luo, T Li, T Tang, W Yin, X Ren, X Wang, X Zhang, X Ren, Y Fan, Y Su, Y Zhang, Y Zhang, Y Wan, Y Liu, Z Wang, Z Cui, Z Zhang, Z Zhou, Z Qiu, 10.48550/ARXIV.2505.093882025Qwen3 Technical Report</p>
<p>MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses. Z Yang, W Liu, B Gao, T Xie, Y Li, W Ouyang, S Poria, E Cambria, D Zhou, 10.48550/ARXIV.2410.070762024</p>
<p>Interpretable machine learning for investigating complex nanomaterial-plant-soil interactions. H Yu, Z Zhao, D Luo, F Cheng, 10.1039/D2EN00181KEnviron Sci: Nano. 9112022</p>
<p>A data-driven approach to predict the compressive strength of alkali-activated materials and correlation of influencing parameters using SHapley Additive exPlanations (SHAP) analysis. X Zheng, Y Xie, X Yang, M N Amin, S Nazar, S A Khan, F Althoey, A F Deifalla, 10.1016/j.jmrt.2023.06.207Journal of Materials Research and Technology. 252023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2695 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2695</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2695</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-271600631</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.00764v2.pdf" target="_blank">AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Model (LLM) based agents have garnered significant attention and are becoming increasingly popular. Furthermore, planning ability is a crucial component of an LLM-based agent, involving interaction with the environment and executing actions to complete a planning task, which generally entails achieving a desired goal from an initial state. This paper investigates enhancing the planning abilities of LLM-based agents through instruction tuning, referred to as agent training. Recent studies on agent training have demonstrated that utilizing expert-level trajectory data (sequences of action-observation pairs) for instruction-tuning LLMs effectively enhances their planning capabilities. However, existing work primarily focuses on synthesizing trajectories from manually designed planning tasks and environments. The labor-intensive nature of creating these environments and tasks impedes the generation of sufficiently varied and extensive trajectories for agent training. To address this limitation, this paper explores the automated synthesis of diverse environments and a gradual range of planning tasks, from easy to difficult. We introduce a framework, AgentGen, that leverages LLMs first to generate environments and subsequently generate planning tasks conditioned on these environments. Specifically, to improve environmental diversity, we propose using an inspiration corpus composed of various domain-specific text segments as the context for synthesizing environments. Moreover, to increase the difficulty diversity of generated planning tasks, we propose a bidirectional evolution method, Bi-Evol, that evolves planning tasks from easier and harder directions to synthesize a task set with a smoother difficulty curve, thereby enhancing the learning process of LLMs more effectively. These methods collectively contribute to the generation of diverse trajectory data for instruction-tuning. Based on AgentGen, we greatly expanded the number of environments and planning tasks available for agent training. The evaluation results from AgentBoard indicate that AgentGen greatly enhances the planning capabilities of LLMs. For instance, the AgentGen instruction-tuned Llama-3.1-8B outperforms GPT-3.5 in overall performance. Moreover, the AgentGen-tuned Llama-3.1-70B model achieves state-of-the-art results in planning tasks. Project page: https://agent-gen.github.io/.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2695.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2695.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Think-in-memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Think-in-memory: Recalling and post-thinking enable llms with long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work (in related work) that, by its title, studies enabling LLMs with a long-term memory capability via recalling and post-thinking mechanisms; cited but not used or analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Think-in-memory: Recalling and post-thinking enable llms with long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not specified in AgentGen; referenced in Related Work only. No architecture or experimental details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2695.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2695.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorybank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced paper title listed in the Related Work that appears to address augmenting LLMs with a long-term memory module; AgentGen mentions it but does not use or evaluate it.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not specified in AgentGen; referenced only in Related Work. No details about the memory architecture or experiments are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2695.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2695.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ghost-in-Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work (Related Work) that, by title, investigates LLM agents augmented with text-based knowledge and memory for open-world environments; AgentGen only cites it and does not employ its methods or report results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Only cited in Related Work; AgentGen does not instantiate or evaluate the agent described by this work and provides no architecture details.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2695.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2695.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Online-adaptation-memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Online adaptation of language models with a memory of amortized contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced title in the Related Work section that suggests techniques for online adaptation of LMs using an amortized-context memory mechanism; the current paper only cites it without using or detailing the method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Online adaptation of language models with a memory of amortized contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned in Related Work only; AgentGen does not provide further technical description or experiments based on this method.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2695.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2695.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long-input-memory-system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unleashing infinite-length input capacity for largescale language models with self-controlled memory system.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work in Related Work that appears to propose a memory system to enable very long input contexts for large language models; AgentGen cites it but does not use or evaluate that memory system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unleashing infinite-length input capacity for largescale language models with self-controlled memory system.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced in Related Work only; no architecture, agent implementation, or experimental details are given in AgentGen.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Think-in-memory: Recalling and post-thinking enable llms with long-term memory. <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory. <em>(Rating: 2)</em></li>
                <li>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. <em>(Rating: 2)</em></li>
                <li>Online adaptation of language models with a memory of amortized contexts. <em>(Rating: 2)</em></li>
                <li>Unleashing infinite-length input capacity for largescale language models with self-controlled memory system. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2695",
    "paper_id": "paper-271600631",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Think-in-memory",
            "name_full": "Think-in-memory: Recalling and post-thinking enable llms with long-term memory.",
            "brief_description": "A referenced work (in related work) that, by its title, studies enabling LLMs with a long-term memory capability via recalling and post-thinking mechanisms; cited but not used or analyzed in this paper.",
            "citation_title": "Think-in-memory: Recalling and post-thinking enable llms with long-term memory.",
            "mention_or_use": "mention",
            "agent_name": "",
            "agent_description": "Not specified in AgentGen; referenced in Related Work only. No architecture or experimental details provided in this paper.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": null,
            "uses_memory": true,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2695.0",
            "source_info": {
                "paper_title": "AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Memorybank",
            "name_full": "Memorybank: Enhancing large language models with long-term memory.",
            "brief_description": "A referenced paper title listed in the Related Work that appears to address augmenting LLMs with a long-term memory module; AgentGen mentions it but does not use or evaluate it.",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory.",
            "mention_or_use": "mention",
            "agent_name": "",
            "agent_description": "Not specified in AgentGen; referenced only in Related Work. No details about the memory architecture or experiments are provided in this paper.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": null,
            "uses_memory": true,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2695.1",
            "source_info": {
                "paper_title": "AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Ghost-in-Minecraft",
            "name_full": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory.",
            "brief_description": "A referenced work (Related Work) that, by title, investigates LLM agents augmented with text-based knowledge and memory for open-world environments; AgentGen only cites it and does not employ its methods or report results.",
            "citation_title": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory.",
            "mention_or_use": "mention",
            "agent_name": "",
            "agent_description": "Only cited in Related Work; AgentGen does not instantiate or evaluate the agent described by this work and provides no architecture details.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": null,
            "uses_memory": true,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2695.2",
            "source_info": {
                "paper_title": "AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Online-adaptation-memory",
            "name_full": "Online adaptation of language models with a memory of amortized contexts.",
            "brief_description": "A referenced title in the Related Work section that suggests techniques for online adaptation of LMs using an amortized-context memory mechanism; the current paper only cites it without using or detailing the method.",
            "citation_title": "Online adaptation of language models with a memory of amortized contexts.",
            "mention_or_use": "mention",
            "agent_name": "",
            "agent_description": "Mentioned in Related Work only; AgentGen does not provide further technical description or experiments based on this method.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": null,
            "uses_memory": true,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2695.3",
            "source_info": {
                "paper_title": "AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Long-input-memory-system",
            "name_full": "Unleashing infinite-length input capacity for largescale language models with self-controlled memory system.",
            "brief_description": "A referenced work in Related Work that appears to propose a memory system to enable very long input contexts for large language models; AgentGen cites it but does not use or evaluate that memory system.",
            "citation_title": "Unleashing infinite-length input capacity for largescale language models with self-controlled memory system.",
            "mention_or_use": "mention",
            "agent_name": "",
            "agent_description": "Referenced in Related Work only; no architecture, agent implementation, or experimental details are given in AgentGen.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": null,
            "uses_memory": true,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2695.4",
            "source_info": {
                "paper_title": "AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Think-in-memory: Recalling and post-thinking enable llms with long-term memory.",
            "rating": 2,
            "sanitized_title": "thinkinmemory_recalling_and_postthinking_enable_llms_with_longterm_memory"
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory.",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory.",
            "rating": 2,
            "sanitized_title": "ghost_in_the_minecraft_generally_capable_agents_for_openworld_environments_via_large_language_models_with_textbased_knowledge_and_memory"
        },
        {
            "paper_title": "Online adaptation of language models with a memory of amortized contexts.",
            "rating": 2,
            "sanitized_title": "online_adaptation_of_language_models_with_a_memory_of_amortized_contexts"
        },
        {
            "paper_title": "Unleashing infinite-length input capacity for largescale language models with self-controlled memory system.",
            "rating": 1,
            "sanitized_title": "unleashing_infinitelength_input_capacity_for_largescale_language_models_with_selfcontrolled_memory_system"
        }
    ],
    "cost": 0.011004499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation</p>
<p>Mengkang Hu mkhu@connect.hku.hk 
Ping Luo 0000-0002-6685-7950</p>
<p>The University of Hong Kong Hong Kong
China</p>
<p>Can Xu</p>
<p>Qingfeng Sun</p>
<p>AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation
65AB10B4B4125149F0F11A749DBFB41A10.1145/3690624.3709321Large Language ModelsLarge Language Model based AgentPlanning
Large Language Model (LLM) based agents have garnered significant attention and are becoming increasingly popular.Furthermore, planning ability is a crucial component of an LLM-based agent, involving interaction with the environment and executing actions to complete a planning task, which generally entails achieving a desired goal from an initial state.This paper investigates enhancing the planning abilities of LLM-based agents through instruction tuning, referred to as agent training.Recent studies on agent training have demonstrated that utilizing expert-level trajectory data (sequences of action-observation pairs) for instruction-tuning LLMs effectively enhances their planning capabilities.However, existing work primarily focuses on synthesizing trajectories from manually designed planning tasks and environments.The labor-intensive nature of creating these environments and tasks impedes the generation of sufficiently varied and extensive trajectories for agent training.To address this limitation, this paper explores the automated synthesis of diverse environments and a gradual range of planning tasks, from easy to difficult.We introduce a framework, AgentGen, that leverages LLMs first to generate environments and subsequently generate planning tasks conditioned on these environments.Specifically, to improve environmental diversity, we propose using an inspiration corpus composed of various domainspecific text segments as the context for synthesizing environments.Moreover, to increase the difficulty diversity of generated planning tasks, we propose a bidirectional evolution method, Bi-Evol, that</p>
<p>evolves planning tasks from easier and harder directions to synthesize a task set with a smoother difficulty curve, thereby enhancing the learning process of LLMs more effectively.These methods collectively contribute to the generation of diverse trajectory data for instruction-tuning.Based on AgentGen, we greatly expanded the number of environments and planning tasks available for agent training.The evaluation results from AgentBoard indicate that AgentGen greatly enhances the planning capabilities of LLMs.For instance, the AgentGen instruction-tuned Llama-3.1-8Boutperforms GPT-3.5 in overall performance.Moreover, the Agent-Gen-tuned Llama-3.1-70Bmodel achieves state-of-the-art results in planning tasks.Project page: https://agent-gen.github.io/.</p>
<p>CCS Concepts</p>
<p> Computing methodologies  Planning for deterministic actions; Robotic planning; Planning and scheduling; Natural language processing.</p>
<p>Introduction</p>
<p>Recently, owing to advancements in Large Language Models (LLMs) [39,42,43,62], the LLM-based Agents have garnered widespread attention from the artificial intelligence community.Generally, an LLM-based agent refers to utilizing LLMs to perceive the environment, make decisions, and execute actions to substitute or help people accomplish some specific tasks [66,77,79].Furthermore, planning is often regarded as one of the most important applications of LLM-based agents, such as robotic planning [19,46,54,63], travel planning [78,90], etc.In this study, planning is conceptualized as the systematic process of identifying a sequence of executable actions within a given environment to complete a planning task, defined as the transition from an initial state to achieve specified goal conditions, considering constraints and available resources [25,50].</p>
<p>Improving planning capabilities through instruction-tuning LLMs is a significant research problem, referred to as agent training.As shown in Figure 1, similar to imitation learning [23], a typical agent training process can be divided into three stages: (i) Preparing environments and planning tasks.(ii) Synthesizing expert-level trajectories (sequences of action-observation pairs) on these planning tasks.For example, utilizing state-of-the-art LLMs (e.g., ) as the agent and filtering trajectory based on reward score [6,86].(iii) Instruction-tuning LLMs with the synthesized trajectory data.Recently, the effectiveness of enhancing the planning capabilities of LLMs through agent training has been demonstrated by many studies [6,8,57,64,68,[85][86][87].Despite their success, one key limitation of these works is that they primarily rely on manually designed environments and planning tasks.The labor-intensive nature of creating environments and planning tasks hinders the generation of diverse and extensive trajectory data.More explicitly, designing diverse environments requires defining a range of rich and practical scenarios, and implementing these environments typically involves the participation of human experts with programming skills.Additionally, formulating tasks often demands creating a task set with a gradual difficulty progression.Due to this constraint, existing agent training studies typically use only a few environments for data synthesis.</p>
<p>To address the aforementioned deficiencies, this paper introduces an automatic framework AgentGen that utilizes LLMs to construct diverse environments and planning tasks for agent training, expanding the available environments from a few to hundreds.More specifically, AgentGen is structured around two stages: (1) Environment Generation: Achieving sufficient environmental diversity is essential for creating diverse planning tasks, which involves covering a broad range of scenarios and domains.To ensure this, we use an inspiration corpus composed of diverse text segments as context for generating environment specifications with LLMs, where actions, restrictions, and other details are defined using natural language.For example, in Figure 2, we randomly selected a text segment from the inspiration corpus: "How to boost your diet with peanut butter powder?"This prompted the generation of a related environment specification: "You are a nutritionist tasked with creating a new healthy recipe book that incorporates peanut butter powder as a key ingredient".Subsequently, we prompt the LLM to produce the corresponding code based on this specification, which may be composed of Python, Planning Domain Definition Language (PDDL) [38], or other domain-specific languages.Furthermore, we constructed an environment library to serve as in-context examples and iteratively expanded it by incorporating high-quality newly generated environments.(2) Task Generation: Conditioned on the generated environment, we aim to create multiple planning tasks.In this stage, it is crucial to have a gradual set of tasks ranging from easy to difficult, i.e., difficulty diversity.To achieve greater difficulty diversity, we propose a bidirectional evolution method, Bi-Evol, where the LLM first generates random planning tasks and then evolves these tasks by applying constraints towards both simplification and increased difficulty.The created task set with Bi-Evol has a smooth difficulty curve that facilitates LLMs' smoother acquisition of planning skills.To verify the effectiveness of our method, we synthesized environments and planning tasks based on PDDL [38] and constructed a dataset comprising 592 environments, each with 20 tasks.We then used a domain-independent planner to obtain 7,246 high-quality trajectories.Subsequently, we used this trajectory data for instruction-tuning a series of LLMs and demonstrated the trained model on AgentBoard [37].Since our instruction-tuning dataset is composed of trajectory synthesized from PDDL-based planning tasks, we refer to evaluation tasks implemented in PDDL as in-domain tasks and tasks implemented in other programming languages as out-of-domain tasks.Importantly, this evaluation was conducted in a zero-shot manner without utilizing any trajectory data from these tasks.Experimental results demonstrate that AgentGen achieved more than a tenfold improvement over the raw LLama-3.1-8B on in-domain tasks (33.3 vs. 3.0), with overall performance surpassing that of GPT-3.5.Furthermore, the performance of AgentGen-tuned Llama-3.1-70Bexceeded GPT-4, setting a new state-of-the-art in planning tasks.In out-of-domain tasks, AgentGen also demonstrated similar experimental outcomes.Specifically, it led to a significant improvement in average success rates, with the raw LLama-3.1-8Bmodel achieving a 10.0% increase and the 70B model a 3.7% improvement.In summary, the proposed environment and planning task generation method AgentGen can help improve planning ability.Moreover, not only can in-domain tasks benefit from this, but out-of-domain tasks also improve, which confirms both the effectiveness and generalization.Our contributions can be summarized as follows:</p>
<p> We introduce AgentGen, which, as far as we know, is the first framework for automatically generating diverse planning tasks and environments targeted for LLM-based agent training. We propose utilizing an inspiration corpus as the context for generating environments with LLMs, resulting in 592 diverse environments that encompass a broad range of scenarios. We propose a bidirectional evolution method Bi-Evol that evolves seed planning tasks in both simpler and more complex directions, thereby constructing a task set with a smoother difficulty curve. We constructed an agent instruction-tuning dataset with 7246 high-quality trajectories through AgentGen.LLMs instruction-tuned with this dataset achieved massive improvement in both in-domain and out-of-domain planning tasks, which validated the effectiveness and generalization of AgentGen.</p>
<p>Preliminary 2.1 Planning Problems</p>
<p>We consider goal-directed deterministic planning problems [50], which are formally defined as a tuple P = (T, E), where E denotes the environment in which the agent interacts and T denotes the task that the agent needs to complete.Specifically, an environment E typically models a world, encompassing the definitions of the action space A and state space S, as well as the transition function T : S  A  S. Task T is further defined by the tuple T = (G, I), where G refers to the goal conditions and I refers to initial states of the agent.The initial states I are a subset of the state space S  that specifies the starting conditions of the agent.The goal G is a subset of the state space S  that specifies the desired outcomes or conditions.Specifically, G can be expressed as G = {  S  |  () = true}.Here,  () is a boolean-valued function representing conditions or propositions that must be satisfied for the state  to be considered part of the goal set.</p>
<p>Planning Problem Implementation</p>
<p>A planning problem can be implemented with programming languages such as Python or domain-specific languages such as Planning Domain Definition Language (PDDL) [38].For example, in a PDDL-based planning problem, the domain PDDL file can be regarded as the environment E, defining states (predicates) and actions and specifying the transition function using preconditions and effects of each action.The problem PDDL file, on the other hand, can be seen as the task T. Both initial states and goal conditions are typically defined as combinations of predicates.Another widely used programming language for constructing planning problems is Python.For example, in OpenAI gym 1 , a planning problem will be implemented as a Python class, where the transition function is implemented as a method of the class, usually named the "step" or "update" function.Meanwhile, the goal G is typically represented as a reward function that indicates the objective of the task, and the initial states I are defined in a method named "reset."</p>
<p>1 https://www.gymlibrary.dev/index.html</p>
<p>Large Language Model based Agent</p>
<p>An LLM-based agent leverages a pre-trained language model to operate within the defined environment E and complete the given task T. Given an environment E, the LLM-based agent perceives its state S and takes actions A based on its understanding and processing of the input.The transition function T : S  A  S remains consistent, where the LLM-based agent determines the next state by generating appropriate actions through natural language processing.The goal  guides the LLM-based agent in selecting actions that maximize the reward.The agent utilizes the language model to interpret the task requirements and generate actions that align with achieving the specified goal.In essence, the LLM-based agent forms a policy  : S  A using the LLM, where  () is the action taken in state  based on the LLM's understanding and processing of the task.</p>
<p>Methodology</p>
<p>Problem Definition.The process of generating planning tasks can be formalized as a function  :   (T, E), where  is the input space (e.g., instructions or prompts) and tuple (T, E) is the space of all possible planning tasks and environments.Based on the definition in Section 2.1, we can express this as  () = (T  , E  ),    , where T  is the generated planning task and E  is the generated environment for a given input .Our two-stage approach can be further decomposed as follows: i) Environment Generation ( 3.1):In the first stage, we generate the environment E  based on the input instruction .This can be represented as E  =  E (), where  E is the environment generation function that takes the instruction  as input and produces the environment E  .ii) Task Generation ( 3.2):In the second stage, we generate the task T  , conditioned on the environment E  generated in the first stage.This can be expressed as: T  =  T (, E  ), where  T is the task generation function that takes both the original instruction  and the generated environment E  as inputs to produce the task T  .We will detail the implementation of these two stages in the following section.</p>
<p>Environment Generation</p>
<p>Overview.As is shown in Figure 2, we propose a sophisticated framework for environment generation structured around three main components: (1) an environment specification generation module where an LLM first generates a specification of the environment, typically including a general overview of the environment, descriptions of the state space and action space, and definitions of the transition functions; (2) an environment implementation module that generates corresponding code based on the environment specification; and (3) an environment library that stores previously generated high-quality environments, serving as a comprehensive environment dataset and providing in-context examples for generating new environments.Each component will be elaborated on in the following paragraph.</p>
<p>Environment Specification.We initially prompt the LLM to generate an environment specification, which typically includes an overall depiction of the environment, specific actions and their corresponding preconditions and effects, and certain restrictions within the environment.The environment specification will serve as the basis for generating specific environment codes.This twostage approach, similar to the Chain-of-Thought [74], can better assist the LLM in creating high-quality environments.For generating environment specifications, One direct approach is to prompt LLMs to generate random environments.However, due to the inherent inductive bias of LLMs, they struggle to generate diverse environments in this way.Therefore, to address this issue, we build an inspiration corpus  = { 0 ,  1 , ,   }, containing sufficiently diverse text segments used to serve as the "inspiration" for generating environment specification with LLMs.More specifically, when generating an environment, we first sample a text segment   from , then prompt the LLM to generate a related environment based on   .Taking the example in Figure 2, we first sample a text segment "How to boost your diet with peanut butter powder?" from .Then we prompt an LLM to generate a related environment where the agent is defined as a nutritionist tasked with creating a new healthy recipe book that prominently features peanut butter powder as a key ingredient.This approach significantly enhances the diversity of generated environments, thereby empowering more generalized agent training.The inspiration corpus can be implemented in various ways, such as using a large-scale pre-trained corpus like Common Crawl.Alternatively, a domain-specific corpus, such as a code generation dataset [7,27], can be used to generate environments for a specific domain.This paper uses LIMA [93] as the inspiration corpus, an instruction-tuning dataset with sufficient diversity.</p>
<p>Environment Implementation.Conditioned on the generated environment specification, we generate its corresponding code, i.e., implementing the environment.This can be formulated as a typical code-generation problem with LLMs.We also introduce a validation tool capable of capturing syntax errors to provide feedback during the code generation process, thereby iteratively refining it.</p>
<p>Environment Library.We define the library at iteration  as:
  =  0   =1 {E  |E  =  E (,   1 ),     ,  (E  ) = }
, where  0 is the initial seed library, and the union represents all verified environments generated up to iteration .This iterative process allows continuous expansion and refinement of the environment library, potentially leading to increasingly complex and diverse environments over time.</p>
<p>Task Generation</p>
<p>Overview.As depicted in Figure 3, conditioned on the generated environments, we prompt LLMs to generate corresponding planning tasks.We employ a two-stage generation approach Bi-Evol for creating a diverse range of planning tasks in terms of difficulty.We begin by prompting the LLM with a specific environment, enabling it to generate an initial set of planning tasks in a zero-shot way.Subsequently, we adjust these tasks to make them simpler or more challenging, forming a comprehensive set of planning tasks.</p>
<p>Bidirectional Evolution.Many studies have proposed evolving instructions, primarily focusing on making instructions more difficult [35,36,80].The effectiveness of this approach relies heavily on the assumption that LLMs inherently possess the ability to follow simple instructions.However, according to findings from some studies [33,37], LLMs often exhibit poor performance even in simple planning tasks.Therefore, we propose Bi-Evol, which introduces evolution in two directions: easy-evol and hard-evol.Easy-evol typically involves simplifying the goal conditions.The motivation is that easier tasks can facilitate learning when the agent performs poorly and cannot directly learn from typically difficult goals.Conversely, hard-evol usually involves making the goal conditions more complex, increasing the number of steps required for the agent to complete the task.This can further enhance the agent's capability to perform the planning task.To our knowledge, we are the first to introduce bidirectional evolution in the agent data generation scenario.The prompt examples are shown in Figure 3.</p>
<p>Experiments</p>
<p>To evaluate the effectiveness of the proposed framework, we synthesize environments and planning tasks using the Planning Domain Definition Language (PDDL), a widely adopted programming language for planning.Subsequently, we evaluate its performance across various unseen planning tasks in a zero-shot manner.To validate the effectiveness and generalizability of AgentGen, we categorized the evaluated tasks into two distinct groups: i) In-Domain Tasks: Planning tasks implemented using PDDL.ii) Out-of-Domain Tasks: These comprise tasks developed using other programming languages, such as Python.</p>
<p>Experimental Setup</p>
<p>Evaluation Tasks.For In-Domain Tasks, we select four widely used PDDL-based planning tasks: Blocksworld, Gripper, Tyreworld, and Barman [37].More explicitly, Blocksworld requires an agent to achieve a target configuration by moving blocks, while Gripper involves moving objects between different rooms.Tyreworld simulates changing a car tire, including removing the flat tire, replacing it with a spare, and installing the new tire.Barman emulates a bartender's tasks in mixing cocktails, which include combining various ingredients, using shakers, and garnishing drinks.For Outof-Domain Tasks, we select three challenging partial-observable planning tasks: Alfworld [54] and BabyAI [10], Jericho [15].Alfworld is an environment designed to test agents' abilities to perform everyday household tasks.While in BabyAI, the agent interprets and executes natural language instructions in a grid-world setting.Jericho [15] is a collection of text-based interactive fiction games in which players issue textual commands to alter the environment.</p>
<p>Evaluation Metrics.We utilized two evaluation metrics to evaluate planning ability: success rate and progress rate [37].During each interaction round, we assigned a progress rate, denoted as   , to measure the progression towards the goal state .As the agent transitions through states   = [ 0 , . . .,   ], its progress is assessed using a matching score  (, )  [0, 1], which quantifies the similarity between the current state and the goal state.Initially,   is set to 0, indicating no progress.Only when the progress rate reaches 1 does the success rate attain 1; all other scenarios yield a 0 outcome.The success rate reflects the agent's capacity to complete a comprehensive task.</p>
<p>Baselines.We compare AgentGen with a series of widely-used multipurpose foundation models that exhibit state-of-the-art performance, such as GPT-3.5 [42] and GPT-4 [44], CodeLlama [48], Mistral [24], Llama-2 [62], and Llama-3.1 [39].We use their instructtuned versions for all multipurpose foundation models ( B.1).Additionally, some models have undergone specialized training on agent trajectory data, such as AgentLM [86], FireAct [6], Agent-Flan [8].We also utilize the AgentInstruct [86] dataset to train Llama-3.1, following the training configuration of AgentGen as a baseline model.</p>
<p>Implementation Details.We followed the environment and task implementation of AgentBoard [37].For the configuration of evaluation tasks, we employ act-only prompting [83], setting the maximum step length for the LLM agent to 30.We selected LIMA [93] as the text corpus  for generating environments, which leverages various data manipulation techniques to ensure a diverse range of instructions.For environment generation and task generation, we employ GPT-42 , configuring the inference parameters with a temperature of 0 and a top_p value of 0.95.Based on AgentGen, we generated a total of 592 environments.For each environment, we generated ten unconditioned tasks, which were then evolved into ten refined tasks using Bi-Evol.To generate trajectory data for training, we utilized the domain-independent planner FastDownward 3 , ensuring optimal trajectory data.This process ultimately led to 7246 trajectories.Since the trajectory data is structured, such as "pickup(o1)", we employ GPT-4 to generate a natural language mapping, for example, "pick up object {arg1}", to transform structured actions into natural language actions.We detailed the generation of natural language mapping in B.2.During the training process, we employed Llama-3.1-8B(base version) as our foundation model, blending general instruction data from the ShareGPT dataset in a 1:4 ratio.For the 70B model, we selected Llama-3.1-70B-Instruct and trained it using LoRA [17] without incorporating general instructions, with a rank of 16.The hyperparameters were configured as follows: a batch size of 64, 10 epochs, a context length of 4096 tokens, and no warmup steps.Checkpoints from epochs 5 through 10 were retained and subsequently evaluated on in-domain tasks.The model demonstrating optimal performance was then selected for further evaluation on out-of-domain tasks.We conducted all experiments utilizing V100 and A100 GPUs.</p>
<p>Evaluation on In-Domain Tasks</p>
<p>As shown in Table 1, the AgentGen-tuned Llama-3.1-8Bmodel outperforms GPT-3.5 in overall progress rate (33.3 vs. 25.0).Furthermore, the AgentGen-tuned Llama-3.1-70Bmodel slightly surpasses GPT-4 (81.5 vs. 81.2).When compared to other models with similar parameter scales, AgentGen consistently demonstrates superior performance across four distinct tasks.In relation to the base Llama-3.1 model, our model exhibits a substantial improvement for both the 8B and 70B versions, with overall progress rates increasing by 30.3 and 2.5, respectively.Notably, in tasks where the success rate of Llama-3.1-8B is zero, AgentGen achieves significant breakthroughs, further validating the efficacy of AgentGen.From the above, we can draw the following conclusions: i) AgentGen-tuned Llama-3.1-8Boutperforms GPT-3.5 in overall performance, while the 70B version achieves state-of-the-art results; ii) AgentGentuned Llama-3.1 has significantly improved both success rate and progress rate; iii) AgentGen consistently outperforms other models with similar parameter scales.</p>
<p>Robustness</p>
<p>To validate the robustness of the constructed dataset with Agent-Gen, we conducted a series of experiments to evaluate its performance across different foundation models.We selected several widely used 7-8B foundation models, including Llama-3-8B, CodeLLama-7B, and Mistral-7B, to test the versatility and effectiveness of AgentGen.As is shown in Table 2, all three models exhibited significant improvements after training, with Llama-3-8B showing the highest success rate increase of 10.0 and CodeLlama-7B demonstrating a maximum progress rate increase of 9.9.These experimental results prove that the dataset constructed with Agent-Gen for agent training is highly effective across different models.</p>
<p>Evaluation on Out-of-Domain Tasks</p>
<p>We also conducted evaluations on out-of-domain agent tasks.As illustrated in Table 3, similar experimental phenomena were observed.Firstly, AgentGen demonstrates a substantial performance improvement over Llama-3.1, with an increase of 13.1% in the average progress rate for the 8B model and 5.0% for the 70B model.Additionally, the AgentGen-tuned Llama-3.1-8Bmodel outperforms GPT-3.5.When compared to general models and agent finetuning models with similar parameter scales, AgentGen consistently outperforms them on both tasks.The superior performance on out-of-domain tasks further emphasizes the effectiveness and generalization capability of our data synthesis methods.</p>
<p>Analysis 5.1 Diversity Analysis</p>
<p>To assess the diversity of the generated environment specifications, we conducted a semantic similarity analysis using cosine similarity.We randomly sampled 100 environment specifications for visualization purposes and converted them into TF-IDF vectors.Figure 4 presents a heatmap of the cosine similarity matrix computed between all pairs of these specifications.Key observations from this analysis include: (i) The predominant blue color in the heatmap indicates generally low similarity between most pairs of environment specifications.This suggests a high degree of diversity among the Table 1: Performance comparison between AgentGen and baseline models in in-domain tasks."Overall" is the weighted average of performance in different tasks."SR" and "PR" stand for "success rate" and "progress rate" metrics.</p>
<p>Model</p>
<p>Size Version  generated environments.(ii) The computed average cosine similarity across all sampled environment specifications is 0.176, reflecting a rich tapestry of distinct semantic features and thematic elements.</p>
<p>Gripper</p>
<p>Difficulty Analysis</p>
<p>To validate the effectiveness of Bi-Evol, we conducted a difficulty analysis on the generated tasks.We heuristically computed the difficulty score as a weighted sum of object count, action count, state variable count, constraint count, and average action impact (mean number of effects per action).We divided the tasks into seed and evolved categories, and then calculated the variance of their difficulty scores.The distribution is visualized in Figure 5. Several conclusions can be drawn: (i) The seed tasks exhibit a higher and narrower peak at lower variance values, indicating a more homogeneous set of initial tasks.In contrast, the evolved tasks show a lower and broader peak, suggesting a more diverse range of complexities; (ii) Despite the substantial overlap between the two distributions, the evolved tasks display a distinct shift towards higher variance values.This shift underscores the effectiveness of AgentGen in broadening the range of problem complexities.</p>
<p>Related Work</p>
<p>Large Language Model based Agent.Large Language Models have demonstrated exceptional reasoning capabilities [24,39,42,43,62].Owing to such abilities, over the past two years, LLM-based agents have experienced significant development [16,53,58,66,75,77].Unlike the traditional method of using LLMs for text-based reasoning, such as Chain-of-Thought [74], LLM-based agents typically    involve interaction with the environment, adjusting the output in a closed-loop manner based on environmental information.These LLM-based agents, now fortified with capabilities like Memorizing [22,29,32,53,61,84,88,91,95], Tool-use [9,28,45,47,51,52], and Planning [2,5,12,40,41,49], exhibit a marked enhancement in their overall efficacy.Although this paper mainly focuses on the planning capability of LLM-based agents, we believe AgentGen has the potential to generalize to other scenarios of LLM-based agents.</p>
<p>Planning with Large Language Models.Planning is one of the key applications of LLM-based agents, applicable in various scenarios such as robotic planning [11,13,19,31,46,54,63,76], travel planning [1,78], calendar scheduling [90], code generation [4] and others [72].It is typically defined as the process of systematically determining a sequence of actions or steps required to achieve a desired goal from an initial state, considering constraints and available resources.This definition primarily differentiates from studies that utilize LLMs to generate ungrounded plans as guidance for problem-solving [67,94], rather than directly producing executable actions.Planning can be categorized into two types: open-loop planning, where the LLM outputs an entire action sequence before execution [19,63], and closed-loop planning, where the LLM-based agent decides the next action based on real-time environmental interaction after executing a previous action [5,20,21,30,55,56,59,60].This paper mainly focuses on close-loop planning, which is more adaptable for error correction, human interaction, and environmental grounding.Recent studies on closeloop planning have integrated chain-of-thought reasoning into the planning process [83].Additionally, some papers have explored the use of tree-search methods to enhance the performance of LLM planning [14,18,34,70,82,89,92].Instead of designing novel frameworks or engaging in prompt engineering, this paper explores how training can enhance the planning capabilities of LLM-based agents.</p>
<p>Agent Training.Recently, numerous studies have aimed to enhance LLM-based agent capabilities by incorporating agent trajectory data into their training [8,57,64,68,87].Advanced works such as AgentTuning [86] utilize GPT-4 to generate trajectory data across six distinct environments.Subsequently, this data is filtered and employed in training Large Language Models, enhancing the agent capabilities of base models.Another work, FireAct [6], proposes training with both CoT data and ReAct format data, enabling the model to discern when to use reasoning to solve problems and when to call external tools.Agent LUMOS [85] suggests separately training Planning and Grounding models, enabling LLM-based agents to learn to decompose complex problems before execution.LLM-Modulo framwork [26] proposes to leverage LLMs generating candidate plans and verify them with an external verifier.Then, use the verified trajectories for fine-tuning LLMs.Similarly, [3] takes a generate-test loop to synthesize trajectories for LLM training.Unlike previous papers on all agent training, AgentGen goes beyond merely generating trajectory data using Large Language Models.Instead, we utilize Large Language Models to generate agent environments, which can be considered a more foundational application.As a result, we have constructed over 500 environments for training, whereas previous works typically use fewer than 10 environments to synthesize agent data.</p>
<p>Environment and Task Generation with Large Language Models.The utilization of LLMs to generate environments and tasks is an emerging application.Some studies have explored utilizing LLMs to generate layouts in robotic simulations, typically involving the creation of configuration files [65,71,81].While these methods can construct numerous scene-level environments, they often struggle to achieve diversity at the underlying mechanism level.AgentTuning [86] employs a task generation approach similar to the Self-instruct [73] method, using the test set as seed data.This approach not only poses a risk of data leakage but also leads to insufficient diversity in task difficulty.ByteSized32 [69] uses LLMs to generate Python-based games based on predefined task specifications automatically.Similarly, other works [13] leverage LLMs to automatically construct PDDL domains based on a task specification.In contrast to these studies, this paper proposes using a diverse text corpus to generate environment code automatically.This approach facilitates the creation of a wide range of rich environments without predefined definitions.</p>
<p>Conclusion</p>
<p>In this paper, we explore using LLMs to automatically generate environment and planning tasks for LLM-based agent training.Specifically, for generating diverse environments, we propose utilizing an inspiration corpus composed of various domain-specific text segments as the context for environment synthesis.To enhance the difficulty diversity of generated planning tasks, we introduce a bidirectional evolution method, Bi-Evol, which evolves planning tasks from both easier and more challenging directions to create a task set with a more gradual difficulty curve, thereby improving the effectiveness of LLM learning.Based on AgentGen, we developed a dataset consisting of 592 environments and 7246 trajectories and trained it on a series of LLMs.The AgentGen-tuned Llama-3.1-8Bmodel surpassed GPT-3.5 on planning tasks, while the AgentGen-tuned Llama-3.1-70Bmodel achieved a new stateof-the-art performance.</p>
<p>Figure 1 :
1
Figure 1: A typical agent training process includes three stages: task preparation, trajectory synthesis, and instruction tuning.AgentGen primarily distinguishes itself from existing agent training literature in the task preparation stage, where we introduce a fully automated task generation framework AgentGen for constructing diverse environments and planning tasks with gradual learning curves.</p>
<p>Figure 2 :
2
Figure 2: Overview of the process of environment generation.</p>
<p>Figure 3 :
3
Figure 3: Overview of the process of task generation.The two-stage task generation process includes first generating unconditioned tasks, then applying Bi-Evol to evolve these planning tasks.Ultimately, both parts are incorporated into the task set.In examples of evolving methods, red indicates evolution towards more difficult tasks, while green indicates the opposite.</p>
<p>Figure 4 :
4
Figure 4: Cosine similarity heatmap depicting the semantic relationships among randomly sampled 100 environment specifications.Darker shades represent a higher similarity between the two specifications.</p>
<p>Figure 5 :
5
Figure 5: Distribution of complexity variance for seed and evolved tasks.The histogram shows the frequency of variance values for seed tasks (blue) and evolved tasks (red), with kernel density estimation curves overlaid.</p>
<p>Table 2 :
2
Overall performance comparison of models before and after training with AgentGen on in-domain tasks."SR" and "PR" stands for "success rate" and "progress rate" respectively.
BeforeAfterModelSRPRSRPRSR PRLlama-3-8B1.7 13.4 11.7 23.0 10.09.6CodeLlama-7B 1.78.26.7 18.15.09.9Mistral-7B0.05.51.7 10.41.74.9</p>
<p>Table 3 :
3
Performance comparison between AgentGen and baseline models on out-of-domain tasks."SR" and "PR" stand for "success rate" and "progress rate" metrics.
ModelSizeVersionAlfworld [54] BabyAI [10] Jericho [15]AverageSRPRSRPRSRPRSRPRGPT-4-2023-05-1543.465.5 56.270.7 35.052.4 44.9 62.9GPT-3.5--turbo turbo-16k17.2 4.535.6 18.9 25.2 33.931.9 45.10.0 0.020.4 12.0 29.3 16.1 12.8 28.87Binstruct1.42.2 15.228.30.09.25.5 13.9CodeLlama13Binstruct2.213.4 17.022.20.00.06.4 11.934Binstruct3.011.3 13.419.90.015.55.5 15.6Mistral7Binstruct-v0.20.09.8 18.124.40.012.16.0 15.47Bchat0.01.55.48.30.04.21.84.7Llama-213Bchat0.06.218.20.03.22.19.770Bchat3.013.2 19.630.00.07.87.5 17.0FireAct7B-0.00.84.58.60.02.81.54.7Agent-Flan7B-0.00.80.00.00.00.00.00.3AgentLM7B 70B-----27.7 8.037.15.5 0.015.2 18.4----Llama-3.18B0.010.5 17.933.60.08.86.0 17.6. AgentTuning 8B---10.719.30.08.2--. AgentGen8B-17.931.7 32.146.00.014.3 16.0 30.7Llama-3.170Binstruct17.242.7 38.457.2 10.031.5 21.8 43.8. AgentTuning  70B---17.935.9 10.031.9--. AgentGen70B-19.446.1 42.062.2 15.038.1 25.5 48.81.00.80.60.40.20.0
We applied the gpt-4-20230321 API from Azure OpenAI service.
https://www.fast-downward.org/
AgentTuning utilized Alfworld's training set, meaning Alfworld cannot be considered an out-of-domain task. Consequently, we did not evaluate the performance of AgentLM or the AgentTuning-trained model on Alfworld.
A Complexity AnalysisTo evaluate the complexity of the automatically generated PDDL domains, we analyzed the distribution of actions and predicates across the generated instances.Figure6presents a 3D histogram illustrating the frequency of domains with varying numbers of actions and predicates.The histogram reveals several key insights: (i) The generated domains exhibit a wide range of complexity, with the number of actions varying from 1 to 10 and the number of predicates ranging from 5 to 25; (ii) The histogram shows a diverse spread of domain configurations, indicating that our automated generation method produces a varied set of PDDL domains.This distribution analysis provides evidence that our automated method is capable of producing PDDL domains with varying levels of complexity, which is crucial for comprehensive testing and evaluation of planning algorithms.B More Implementation Details B.1 ModelsWe applied the instruct version for models.Specifically, the detailed version for each model is presented in Table4.Model VersionCodeLlama meta-llama/CodeLlama-7b-Instruct-hf Mistral mistralai/Mistral-7B-Instruct-v0.2 Llama2 meta-llama/Llama-2-7b-chat-hf Llama3 meta-llama/Meta-Llama-3-8B-Instruct AgentLM THUDM/agentlm-7bTable4: Evaluated models in this study.B.2 Natural Language MappingWe leverage GPT-4 to generate the natural language mapping that converts structured actions into its natural language format.When the mapping failed to yield, we heuristically serialized the structured actions.The prompt for generating natural language mapping with GPT-4 is as follows:Natural Language Mapping Generation I would like you to create natural language mapping for PDDL.The form of the natural language mapping is a Python dictionary, wherein 1.The key corresponds to the name of a predicate or action within the domain PDDL.2. The value is its equivalent in natural language, with parameters presented in "{argn}", where n is the index of its parameters in the PDDL expression.3. You must ensure that the number of "{}" corresponds precisely to the number of parameters in predicates or actions.4.You should very carefully check the order of {argn}.
Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. Mohamed Aghzal, Erion Plaku, Ziyu Yao, arXiv:2310.032492023. 2023arXiv preprint</p>
<p>Akash Srivastava, and Pulkit Agrawal. 2024. Compositional foundation models for hierarchical planning. Anurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Advances in Neural Information Processing Systems. 362024</p>
<p>Learning and leveraging verifiers to improve planning capabilities of pre-trained language models. Daman Arora, Subbarao Kambhampati, arXiv:2305.170772023. 2023arXiv preprint</p>
<p>Codeplan: Repository-level coding using llms and planning. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Shashank Ashok, Shet, Proceedings of the ACM on Software Engineering. 12024. 2024FSE</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on robot learning. PMLR2023</p>
<p>Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, arXiv:2310.05915Fireact: Toward language agent fine-tuning. 2023. 2023arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021. 2021arXiv preprint</p>
<p>Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, Feng Zhao, arXiv:2403.12881Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models. 2024. 2024arXiv preprint</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, arXiv:2210.028752022. 2022arXiv preprint</p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, Yoshua Bengio, arXiv:1810.08272Babyai: A platform to study the sample efficiency of grounded language learning. 2018. 2018arXiv preprint</p>
<p>Task and Motion Planning with Large Language Models for Object Rearrangement. Yan Ding, Xiaohan Zhang, Chris Paxton, Shiqi Zhang, arXiv:2303.06247[cs.RO]2023</p>
<p>Zeyu Gao, Yao Mu, Jinye Qu, Mengkang Hu, Lingyue Guo, Ping Luo, Yanfeng Lu, arXiv:2406.09953DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning. 2024. 2024arXiv preprint</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 2023. 202336</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023. 2023arXiv preprint</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Ct, Xingdi Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352MetaGPT: Meta programming for multi-agent collaborative framework. 2023. 2023arXiv preprint</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021. 2021arXiv preprint</p>
<p>Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, Ping Luo, arXiv:2310.08582Tree-planner: Efficient close-loop task planning with large language models. 2023. 2023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International conference on machine learning. PMLR2022</p>
<p>Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control. Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, Brian Ichter, arXiv:2303.00855[cs.RO]2023</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter, arXiv:2207.05608[cs.RO]Inner Monologue: Embodied Reasoning through Planning with Language Models. 2022</p>
<p>Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, Xing Xie, arXiv:2308.16505Recommender ai agent: Integrating large language models for interactive recommendations. 2023. 2023arXiv preprint</p>
<p>Imitation learning: A survey of learning methods. Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, Chrisina Jayne, ACM Computing Surveys (CSUR). 502017. 2017</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023. 2023Mistral 7B. arXiv preprint</p>
<p>Hierarchical task and motion planning in the now. Leslie Pack, Kaelbling , Toms Lozano-Prez, 10.1109/ICRA.2011.5980391IEEE. 2011. 2011</p>
<p>Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy, arXiv:2402.01817LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks. 2024. 2024arXiv preprint</p>
<p>DS-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, Tao Yu, International Conference on Machine Learning. PMLR2023</p>
<p>Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, arXiv:2304.08244Api-bank: A comprehensive benchmark for tool-augmented llms. 2023. 2023arXiv preprint</p>
<p>Unleashing infinite-length input capacity for largescale language models with self-controlled memory system. Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li, arXiv-23042023. 2023arXiv e-prints</p>
<p>On grounded planning for embodied tasks with language models. Chengsong Bill Yuchen Lin, Qian Huang, Wenda Liu, Sam Gu, Xiang Sommerer, Ren, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023. 2023arXiv preprint</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Think-in-memory: Recalling and post-thinking enable llms with long-term memory. 2023. 2023arXiv preprint</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, arXiv:2308.03688AgentBench: Evaluating llms as agents. 2023. 2023arXiv preprint</p>
<p>Yanming Liu, Xinyue Peng, Yuwei Zhang, Jiannan Cao, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du, arXiv:2406.03807Tool-Planner: Dynamic Solution Tree Planning for Large Language Model with Tool Clustering. 2024. 2024arXiv preprint</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023. 2023arXiv preprint</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang, arXiv:2306.08568Wizardcoder: Empowering code large language models with evol-instruct. 2023. 2023arXiv preprint</p>
<p>Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He, arXiv:2401.13178AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. 2024. 2024arXiv preprint</p>
<p>PDDL-the planning domain definition language. Drew Mcdermott, Malik Ghallab, Adele E Howe, Craig A Knoblock, Ashwin Ram, Manuela M Veloso, Daniel S Weld, David E Wilkins, 199859656859</p>
<p>Introducing Meta Llama 3: The most capable openly available LLM to date. A I Meta, 2024</p>
<p>RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis. Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian Ge, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, arXiv:2402.161172024. 2024arXiv preprint</p>
<p>Embodiedgpt: Vision-language pre-training via embodied chain of thought. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>OpenAI: Introducing ChatGPT. 2022OpenAI</p>
<p>Openai, arxiv 2303.08774Gpt-4 technical report. 2023. 2023213</p>
<p>Aaron Parisi, Yao Zhao, Noah Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022. 2022arXiv preprint</p>
<p>Virtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789ToolLLM: Facilitating large language models to master 16000+ real-world apis. 2023. 2023arXiv preprint</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jrmy Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023. 2023arXiv preprint</p>
<p>Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, Rui Zhao, arXiv:2308.03427Tptu: Task planning and tool usage of large language model-based ai agents. 2023. 2023arXiv preprint</p>
<p>Artificial intelligence: a modern approach. J Stuart, Peter Russell, Norvig, 2016Pearson</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 10.48550/ARXIV.2302.04761arXiv:2302.047612023. 2023</p>
<p>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, 10.48550/ARXIV.2303.17580arXiv:2303.175802023. 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.03768Alfworld: Aligning text and embodied environments for interactive learning. 2020. 2020arXiv preprint</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, arXiv:2212.04088[cs.AI]LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. 2023</p>
<p>Trial and error: Exploration-based trajectory optimization for llm agents. Yifan Song, Xiang Da Yin, Jie Yue, Sujian Huang, Bill Li, Lin Yuchen, arXiv:2403.025022024. 2024arXiv preprint</p>
<p>Shunyu Theodore R Sumers, Karthik Yao, Thomas L Narasimhan, Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023. 2023arXiv preprint</p>
<p>Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, arXiv:2305.16653AdaPlanner: Adaptive Planning from Feedback with Language Models. 2023. 2023arXiv preprint</p>
<p>Simeng Sun, Yang Liu, Shuohang Wang, arXiv:2305.14564Chenguang Zhu, and Mohit Iyyer. 2023. PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents. 2023arXiv preprint</p>
<p>Online adaptation of language models with a memory of amortized contexts. Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, Jonathan Richard Schwarz, arXiv:2403.043172024. 2024arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXiv:2307.09288Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, Yu Su, arXiv:2403.04746LLMs in the Imaginarium: tool learning through simulated trial and error. 2024. 2024arXiv preprint</p>
<p>Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang, arXiv:2310.01361Gensim: Generating robotic simulation tasks via large language models. 2023. 2023arXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023. 2023arXiv preprint</p>
<p>Plan-and-solve prompting: Improving zero-shot chainof-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.040912023. 2023arXiv preprint</p>
<p>Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents. Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, Timothy Baldwin, arXiv:2402.116512024. 2024arXiv preprint</p>
<p>Ruoyao Wang, Graham Todd, Eric Yuan, Ziang Xiao, Marc-Alexandre Ct, Peter Jansen, arXiv:2305.14879ByteSized32: A corpus and challenge task for generating taskspecific world models expressed as text games. 2023. 2023arXiv preprint</p>
<p>Promptagent: Strategic planning with language models enables expert-level prompt optimization. Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, Zhiting Hu, arXiv:2310.164272023. 2023arXiv preprint</p>
<p>Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, Chuang Gan, arXiv:2311.014552023. 2023arXiv preprint</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.01560[cs.AI]Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents. 2023</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021. 2021arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>AutoGen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023. 2023arXiv preprint</p>
<p>Embodied Task Planning with Large Language Models. Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan, arXiv:2307.01848[cs.CV]2023</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023. 2023arXiv preprint</p>
<p>Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, arXiv:2402.01622Travelplanner: A benchmark for real-world planning with language agents. 2024. 2024arXiv preprint</p>
<p>OpenAgents: An Open Platform for Language Agents in the Wild. Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Jing Toh, Junning Hua, Qian Zhao, Che Liu, Leo Z Liu, Yiheng Liu, Hongjin Xu, Dongchan Su, Caiming Shin, Tao Xiong, Yu, 10.48550/ARXIV.2310.106342023. 2023</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023. 2023arXiv preprint</p>
<p>Language guided generation of 3d embodied ai environments. Yue Yang, Fan-Yun Sun, Luca Weihs, Eli Vanderbilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023. 2023arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022. 2022arXiv preprint</p>
<p>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, arXiv:2308.02151Retroformer: Retrospective large language agents with policy gradient optimization. 2023. 2023arXiv preprint</p>
<p>Lumos: Learning agents with unified data, modular design, and open-source llms. Faeze Da Yin, Abhilasha Brahman, Khyathi Ravichander, Kai-Wei Chandu, Yejin Chang, Bill Choi, Lin Yuchen, arXiv:2311.056572023. 2023arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023. 2023arXiv preprint</p>
<p>Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, arXiv:2402.15506AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning. 2024. 2024arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. 19632-1964238</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Zirui Zhao, Wee Sun Lee, David Hsu, Advances in Neural Information Processing Systems. 2024. 202436</p>
<p>Swaroop Huaixiu Steven Zheng, Hugh Mishra, Xinyun Zhang, Minmin Chen, Azade Chen, Le Nova, Hou, Heng-Tze, Quoc V Cheng, Ed H Le, Chi, arXiv:2406.04520NATURAL PLAN: Benchmarking LLMs on Natural Language Planning. 2024. 2024arXiv preprint</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. 19724-1973138</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, arXiv:2310.044062023. 2023arXiv preprint</p>
<p>Lima: Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.10625Least-tomost prompting enables complex reasoning in large language models. 2022. 2022arXiv preprint</p>
<p>Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.17144Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. 2023. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
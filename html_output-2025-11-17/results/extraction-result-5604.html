<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5604 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5604</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5604</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-266521610</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.14670v1.pdf" target="_blank">Zero-shot Causal Graph Extrapolation from Text via LLMs</a></p>
                <p><strong>Paper Abstract:</strong> We evaluate the ability of large language models (LLMs) to infer causal relations from natural language. Compared to traditional natural language processing and deep learning techniques, LLMs show competitive performance in a benchmark of pairwise relations without needing (explicit) training samples. This motivates us to extend our approach to extrapolating causal graphs through iterated pairwise queries. We perform a preliminary analysis on a benchmark of biomedical abstracts with ground-truth causal graphs validated by experts. The results are promising and support the adoption of LLMs for such a crucial step in causal inference, especially in medical domains, where the amount of scientific text to analyse might be huge, and the causal statements are often implicit.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5604.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5604.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Turbo (pairwise zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 Turbo (used with engineered zero-shot prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 Turbo was used in zero-shot, prompt-engineered queries to identify cause-effect orientation between two marked entities in sentences; the model produced very high orientation accuracy without task-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Pairwise causal orientation detection (SemEval causal category)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a sentence with two tagged entities known to be causally related, decide which entity is the cause and which is the effect (A → B or A ← B). Evaluation uses the SemEval dataset's causal sentences (1003 causal sentences total; 325 causal relations in the test set).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot, engineered natural-language prompt (see Figure 3). The prompt supplies the sentence inside delimiters (<Text>...</Text>) and tags for the two entities (<Entity>...</Entity>), gives a closed set of answer options (A/B/C), instructs the model to analyze step-by-step before concluding, requests a structured final answer inside explicit tags (<Answer>[answer]</Answer>), and forces the model to base responses only on the provided source text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to supervised systems trained on the SemEval training set (winner system used ~8000 training samples). No alternative prompt formats (e.g., few-shot vs zero-shot) were experimentally compared within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>F1 ≃ 99% on orientation predictions (confusion matrix over 998 cases; on the 325 causal relations in the test split the F1 ≃ 99%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>SemEval-2010 task winner (supervised method) reported F1 ≃ 90% on the causal category (trained on ~8000 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+~9 percentage points F1 relative to the SemEval winner (note: this is a cross-method comparison, not a within-model prompt-format ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The authors attribute the high zero-shot performance to the combination of GPT's pretrained language understanding and the prompt engineering choices: explicit delimiters/tags, structured output requirements, closed-answer options, and requesting step-by-step analysis (which encourages the model to 'think' through orientation). They also constrained the model to rely only on the provided text to avoid reliance on background knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-shot Causal Graph Extrapolation from Text via LLMs', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5604.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5604.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-CG extrapolation (iterated pairwise zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot causal graph extrapolation via iterated pairwise GPT-4 Turbo prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that extracts entities from medical abstracts and runs iterated zero-shot pairwise causal queries (using GPT-4 Turbo) to assemble a causal graph (LLM-CG); the approach attains high recall but lower precision due to many inferred indirect links.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Causal graph extrapolation from medical abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract entities (diseases, medications, treatments, symptoms) from an abstract, then query the LLM for all entity pairs to determine directed cause-effect arcs and assemble a causal graph; compare to expert-annotated ground-truth causal graphs (20 medical papers, abstracts only).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot pipeline: (1) entity recognition via explicit instructions (emphasizing medical entity types and synonym resolution), (2) iterated pairwise causal queries using structured prompts (like the pairwise prompt) for every entity pair, collecting directed arcs. Small entity sets (≤20) allow quadratic pairwise querying. Prompt outputs are normalized/mapped to form the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Recall ≃ 97%, Precision ≃ 74% (on the benchmark of 20 expert-annotated abstracts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed (format yielded very high recall but reduced precision)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that many false positives (lower precision) stem from the prompt/format not enforcing the distinction between direct and indirect causal relations — the LLM may infer transitive/indirect links (A→B and B→C leading to A→C) as direct causal arcs. They also note that the prompts did not enforce acyclicity, which led to directed cycles inconsistent with ground-truth acyclic medical graphs; they propose further prompt engineering (e.g., asking explicitly about directness or acyclicity) to reduce false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Directed cycles and extra arcs were produced by the current prompt format even though ground-truth CGs are acyclic, indicating the format can introduce structurally incorrect links.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-shot Causal Graph Extrapolation from Text via LLMs', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5604.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5604.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt engineering techniques (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt engineering techniques: delimiters, structured output, few-shot, chain-of-thought, task checks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A summary of prompt-engineering strategies discussed: clarity/precision of instructions, use of delimiters (brackets/tags/quotes), specifying structured output formats, checking task conditions, few-shot examples, and asking for step-by-step (chain-of-thought) explanations to improve LLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Zero-shot Causal Graph Extrapolation from Text via LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General prompt design strategies (described, not ablated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Descriptions of common prompt-engineering techniques intended to improve LLM accuracy on downstream tasks (delimiters, structured outputs, few-shot exemplars, step-by-step reasoning, and condition checks).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Descriptive: recommends using clear and precise instructions, delimiters (brackets/tags/quotes) to separate sections, forcing structured output formats, verifying preconditions inside the prompt, few-shot prompting (showing examples), and requesting step-by-step reasoning (chain-of-thought) before a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>claimed to improve performance (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>These techniques are argued to guide the model to produce more organized, relevant, and verifiable outputs: delimiters and structured-output constraints reduce ambiguity; few-shot exemplars give in-context examples that help the model pattern-match; step-by-step prompts help the model 'think' and verify intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zero-shot Causal Graph Extrapolation from Text via LLMs', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Prompt programming for large language models: Beyond the few-shot paradigm <em>(Rating: 2)</em></li>
                <li>Can Large Language Models Infer Causation from Correlation? <em>(Rating: 2)</em></li>
                <li>Understanding causality with large language models: Feasibility and opportunities <em>(Rating: 2)</em></li>
                <li>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals <em>(Rating: 2)</em></li>
                <li>Large language models and knowledge graphs: Opportunities and challenges <em>(Rating: 1)</em></li>
                <li>Towards expert-level medical question answering with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5604",
    "paper_id": "paper-266521610",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "GPT-4 Turbo (pairwise zero-shot)",
            "name_full": "Generative Pre-trained Transformer 4 Turbo (used with engineered zero-shot prompts)",
            "brief_description": "GPT-4 Turbo was used in zero-shot, prompt-engineered queries to identify cause-effect orientation between two marked entities in sentences; the model produced very high orientation accuracy without task-specific training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo",
            "model_size": null,
            "task_name": "Pairwise causal orientation detection (SemEval causal category)",
            "task_description": "Given a sentence with two tagged entities known to be causally related, decide which entity is the cause and which is the effect (A → B or A ← B). Evaluation uses the SemEval dataset's causal sentences (1003 causal sentences total; 325 causal relations in the test set).",
            "problem_format": "Zero-shot, engineered natural-language prompt (see Figure 3). The prompt supplies the sentence inside delimiters (&lt;Text&gt;...&lt;/Text&gt;) and tags for the two entities (&lt;Entity&gt;...&lt;/Entity&gt;), gives a closed set of answer options (A/B/C), instructs the model to analyze step-by-step before concluding, requests a structured final answer inside explicit tags (&lt;Answer&gt;[answer]&lt;/Answer&gt;), and forces the model to base responses only on the provided source text.",
            "comparison_format": "Compared to supervised systems trained on the SemEval training set (winner system used ~8000 training samples). No alternative prompt formats (e.g., few-shot vs zero-shot) were experimentally compared within this paper.",
            "performance": "F1 ≃ 99% on orientation predictions (confusion matrix over 998 cases; on the 325 causal relations in the test split the F1 ≃ 99%).",
            "performance_comparison": "SemEval-2010 task winner (supervised method) reported F1 ≃ 90% on the causal category (trained on ~8000 samples).",
            "format_effect_size": "+~9 percentage points F1 relative to the SemEval winner (note: this is a cross-method comparison, not a within-model prompt-format ablation).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "The authors attribute the high zero-shot performance to the combination of GPT's pretrained language understanding and the prompt engineering choices: explicit delimiters/tags, structured output requirements, closed-answer options, and requesting step-by-step analysis (which encourages the model to 'think' through orientation). They also constrained the model to rely only on the provided text to avoid reliance on background knowledge.",
            "counterexample_or_null_result": null,
            "uuid": "e5604.0",
            "source_info": {
                "paper_title": "Zero-shot Causal Graph Extrapolation from Text via LLMs",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLM-CG extrapolation (iterated pairwise zero-shot)",
            "name_full": "Zero-shot causal graph extrapolation via iterated pairwise GPT-4 Turbo prompts",
            "brief_description": "A pipeline that extracts entities from medical abstracts and runs iterated zero-shot pairwise causal queries (using GPT-4 Turbo) to assemble a causal graph (LLM-CG); the approach attains high recall but lower precision due to many inferred indirect links.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo",
            "model_size": null,
            "task_name": "Causal graph extrapolation from medical abstracts",
            "task_description": "Extract entities (diseases, medications, treatments, symptoms) from an abstract, then query the LLM for all entity pairs to determine directed cause-effect arcs and assemble a causal graph; compare to expert-annotated ground-truth causal graphs (20 medical papers, abstracts only).",
            "problem_format": "Zero-shot pipeline: (1) entity recognition via explicit instructions (emphasizing medical entity types and synonym resolution), (2) iterated pairwise causal queries using structured prompts (like the pairwise prompt) for every entity pair, collecting directed arcs. Small entity sets (≤20) allow quadratic pairwise querying. Prompt outputs are normalized/mapped to form the graph.",
            "comparison_format": null,
            "performance": "Recall ≃ 97%, Precision ≃ 74% (on the benchmark of 20 expert-annotated abstracts).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "mixed (format yielded very high recall but reduced precision)",
            "explanation_or_hypothesis": "Authors hypothesize that many false positives (lower precision) stem from the prompt/format not enforcing the distinction between direct and indirect causal relations — the LLM may infer transitive/indirect links (A→B and B→C leading to A→C) as direct causal arcs. They also note that the prompts did not enforce acyclicity, which led to directed cycles inconsistent with ground-truth acyclic medical graphs; they propose further prompt engineering (e.g., asking explicitly about directness or acyclicity) to reduce false positives.",
            "counterexample_or_null_result": "Directed cycles and extra arcs were produced by the current prompt format even though ground-truth CGs are acyclic, indicating the format can introduce structurally incorrect links.",
            "uuid": "e5604.1",
            "source_info": {
                "paper_title": "Zero-shot Causal Graph Extrapolation from Text via LLMs",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Prompt engineering techniques (mentioned)",
            "name_full": "Prompt engineering techniques: delimiters, structured output, few-shot, chain-of-thought, task checks",
            "brief_description": "A summary of prompt-engineering strategies discussed: clarity/precision of instructions, use of delimiters (brackets/tags/quotes), specifying structured output formats, checking task conditions, few-shot examples, and asking for step-by-step (chain-of-thought) explanations to improve LLM responses.",
            "citation_title": "Zero-shot Causal Graph Extrapolation from Text via LLMs",
            "mention_or_use": "mention",
            "model_name": "GPT-4 Turbo",
            "model_size": null,
            "task_name": "General prompt design strategies (described, not ablated)",
            "task_description": "Descriptions of common prompt-engineering techniques intended to improve LLM accuracy on downstream tasks (delimiters, structured outputs, few-shot exemplars, step-by-step reasoning, and condition checks).",
            "problem_format": "Descriptive: recommends using clear and precise instructions, delimiters (brackets/tags/quotes) to separate sections, forcing structured output formats, verifying preconditions inside the prompt, few-shot prompting (showing examples), and requesting step-by-step reasoning (chain-of-thought) before a final answer.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "claimed to improve performance (qualitative)",
            "explanation_or_hypothesis": "These techniques are argued to guide the model to produce more organized, relevant, and verifiable outputs: delimiters and structured-output constraints reduce ambiguity; few-shot exemplars give in-context examples that help the model pattern-match; step-by-step prompts help the model 'think' and verify intermediate reasoning steps.",
            "counterexample_or_null_result": null,
            "uuid": "e5604.2",
            "source_info": {
                "paper_title": "Zero-shot Causal Graph Extrapolation from Text via LLMs",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Prompt programming for large language models: Beyond the few-shot paradigm",
            "rating": 2,
            "sanitized_title": "prompt_programming_for_large_language_models_beyond_the_fewshot_paradigm"
        },
        {
            "paper_title": "Can Large Language Models Infer Causation from Correlation?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_infer_causation_from_correlation"
        },
        {
            "paper_title": "Understanding causality with large language models: Feasibility and opportunities",
            "rating": 2,
            "sanitized_title": "understanding_causality_with_large_language_models_feasibility_and_opportunities"
        },
        {
            "paper_title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals",
            "rating": 2,
            "sanitized_title": "semeval2010_task_8_multiway_classification_of_semantic_relations_between_pairs_of_nominals"
        },
        {
            "paper_title": "Large language models and knowledge graphs: Opportunities and challenges",
            "rating": 1,
            "sanitized_title": "large_language_models_and_knowledge_graphs_opportunities_and_challenges"
        },
        {
            "paper_title": "Towards expert-level medical question answering with large language models",
            "rating": 1,
            "sanitized_title": "towards_expertlevel_medical_question_answering_with_large_language_models"
        }
    ],
    "cost": 0.010357,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Zero-shot Causal Graph Extrapolation from Text via LLMs</p>
<p>Alessandro Antonucci alessandro@idsia.ch. 
Gregorio Piqué 
Marco Zaffalon 
Zero-shot Causal Graph Extrapolation from Text via LLMs
6C783387EAB3359CA76DA38F664EC98C
We evaluate the ability of large language models (LLMs) to infer causal relations from natural language.Compared to traditional natural language processing and deep learning techniques, LLMs show competitive performance in a benchmark of pairwise relations without needing (explicit) training samples.This motivates us to extend our approach to extrapolating causal graphs through iterated pairwise queries.We perform a preliminary analysis on a benchmark of biomedical abstracts with ground-truth causal graphs validated by experts.The results are promising and support the adoption of LLMs for such a crucial step in causal inference, especially in medical domains, where the amount of scientific text to analyse might be huge, and the causal statements are often implicit.</p>
<p>Introduction</p>
<p>In recent years, machine learning algorithms based on deep neural architectures have achieved astonishing results in many applied domains (see, e.g., Shen, Wu, and Suk 2017 for an application in medical image analysis).These results are obtained by processing vast amounts of training data, allowing us to learn correlations and accurately solve predictive tasks.On the other side, scientific investigations need more than accurate predictions based on correlations, the focus being identifying causal relations between the entities in the system under consideration.This requires dedicated formalisms and tools that cannot be provided by pure datadriven approaches, such as those considered by deep learning techniques (Bareinboim et al. 2022).</p>
<p>Following the Pearlian approach to causality (Pearl 2009), the first tool to go beyond a pure correlational analysis is to pair the observational data with a causal graph (CG) modelling cause-effect relations as directed arcs connecting nodes associated with a system's entities.For example, Figure 1 depicts a CG for a medical domain.The classical do calculus (Pearl 2009) allows to compute (some) interventional queries from the CG and a set of observations.If knowledge about the structural models underlying the CG is also available, even the more challenging counterfactual queries can be addressed (e.g., Zaffalon et al. 2023).</p>
<p>Figure 1: A (acyclic) CG modelling the causal relations included in the medical abstract in Figure 2.</p>
<p>Randomised experiments are the traditional way to discover causal relations, hence CGs.Yet, such studies might be too expensive, time-consuming, or even impossible in many cases.This motivates causal discovery intended as revealing causal information from purely observational data.Popular approaches are the Fast Causal Inference by Spirtes, Glymour, and Scheines (2000) based on a constraint-based procedure checking independence and the score-based Greedy Equivalence Search by Chickering ( 2002).Yet, the output of these algorithms is generally a class of CGs, equivalent under the Markov condition, thus only partially solving the task.To consistently obtain a single CG, the only alternative not involving dedicated randomised experiments is to elicit the direction of the causal relations from a domain expert.</p>
<p>Especially for scientific and medical domains, the interaction with experts might also be indirect and consist of analysing the scientific literature on the topic and trying to grasp the causal relations stated in the text.Consider, for instance, the medical abstract in Figure 2. The text contains several causal statements summarised by the CG in Figure 1.This paper focuses on automating such a natural language understanding (NLU) task.</p>
<p>Even if no specific studies for the causal case are available (see the next section about the related work), classical and deep learning techniques for NLU have already been proven to classify relations among entities accurately (Li et al. 2022).Those are typically supervised approaches re-</p>
<p>arXiv:2312.14670v1 [cs.AI] 22 Dec 2023</p>
<p>Fulminant type 1 diabetes (FT1D) is a novel type of type 1 diabetes that is caused by extremely rapid destruction of the pancreatic β cells.Early diagnosis or prediction of FT1D is critical for the prevention or timely treatment of diabetes ketoacidosis, which can be life-threatening.Understanding its triggers or promoting factors plays an important role in the prevention and treatment of FT1D.In this review, we summarised the various triggering factors of FT1D, including susceptibility genes, immunological factors (cellular and humoural immunity), immune checkpoint inhibitor therapies, drug reactions with eosinophilia and systemic symptoms or drug-induced hypersensitivity syndrome, pregnancy, viral infections, and vaccine inoculation.This review provides the basis for future research into the pathogenetic mechanisms that regulate FT1D development and progression to further improve the prognosis and clinical management of patients with FT1D.</p>
<p>Figure 2: The medical abstract of Luo et al. (2020).Some cause-effect relations are highlighted (cause in blue, effect in orange).</p>
<p>quiring annotated data for the learning phase.This point might be critical because of the potential costs of the annotation process and the possible lack of diversity in the data.</p>
<p>Such a difficulty can be partially solved by using large language models (LLMs), trained in an unsupervised manner from vast volumes of textual data.Here we consider the Generative Pre-trained Transformer (GPT) language models designed to understand and generate human-like text based on the prompts.As the focus is on scientific domains, particularly medicine, we are not interested in the background, "commonsense", knowledge embedded in the LLM.By standard prompting techniques, we force the answer to the query to be only based on the source document.We do not expect to discover new knowledge this way but rather combine and standardise already existing information.</p>
<p>The paper is organised as follows.First we discuss the related work and notice how the direction we are considering, i.e., the unsupervised, zero-shot, extrapolation CGs from natural text through LLMs is relatively unexplored.Thus, to advocate our point, after a discussion on the necessary prompting techniques, we first consider a benchmark of pairwise relations and we test the accuracy of GPT in the causal recognition task against the existing results in the literature.The good performances we observe motivate us to extend the same technique to GCs.Finally we discuss the necessary future work required by this preliminary-butpromising analysis.</p>
<p>Related Work</p>
<p>The problem we consider can be intended as a (causal) specialisation of learning a knowledge graph from natural language in a purely unsupervised manner.The literature for such a general task is vast (see Ji et al. 2021 for a recent survey) and also includes recent attempts based on LLMs (Pan et al. 2023).Yet, the particular case of CGs received relatively little attention and, to the best of our knowledge, the only work in the same direction we consider is by Arsenyan and Shahnazaryan (2023).This is an interesting preliminary study adopting the BERT-based models.Yet, it misses ground-truth data, thus not offering any quantitative baseline for our study.</p>
<p>From a more general perspective, the empirical analysis of the potential of LLMs in causal inference is attracting growing interest in the recent literature.Causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g.commonsense knowledge), while we are interested in specific, scientific, knowledge from selected sources.Moreover, the work by Jin et al. (2023) outlines a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task.Following Zhang et al. (2023) LLMs are not yet able to provide satisfactory answers for discovering new causal knowledge or for highstakes decision-making tasks with high precision.</p>
<p>Prompt Engineering</p>
<p>LLMs are known to often achieve satisfactory results in question answering (see, e.g., Singhal et al. 2023).However, some expedients have been shown to increase the accuracy of queries.These techniques, known as prompt engineering (Reynolds and McDonell 2021), can be regarded as rules and instructions to enhance the LLM capabilities on various tasks.Prompt engineering involves crafting systems and user messages that guide the LLM responses and shape its output to meet specific requirements.</p>
<p>Among the many prompt engineering techniques, improving the clarity and precision of the prompt text by providing precise and specific instructions represents the most obvious strategy.Delimiters like brackets, tags or quotes can separate sections within the prompt, aiding in a more organised interpretation of the input.Furthermore, prompting for structured output by specifying the desired response format guides the model in generating well-organised results.Checking task conditions also ensures that the necessary assumptions are met.For instance, the prompt can verify whether essential information is available to complete the task and provide alternative instructions if this information is missing.</p>
<p>The technique called few-shot prompting instead involves showing successful task examples to the model before requesting similar ones.This helps the model understand the context better, preparing it to deliver pertinent and accurate responses.Another principle of prompt engineering is giving the model enough time to "think".This involves requesting the model to answer with a step-by-step explanation of its thought process before providing the final answer.Doing so allows the model to work out its solution rather than rushing to conclusions.This principle applies, for example, when the model needs to verify the accuracy of a given solution.In this case, it is prompted to formulate its solution and then compare it to the one provided.</p>
<p>In the experiments discussed in the next two sections, we used GPT-4 Turbo as the LLM of choice for our study.The interaction with the LLM was achieved through a Python API 1 in a AMD EPYC 7742 server (2.25-3.4GHz,128GB).Our code is freely available in a repository, where all the details about our prompt engineering choices can be retrieved. 21 https://platform.openai.com/docs/api-reference/chat. 2 https://github.com/IDSIA-papers/causal-llms.</p>
<p>You will be provided with a text delimited by the <Text></Text> xml tags, and a pair of entities delimited by the <Entity></Entity> xml tags representing entities extracted from the given text.</p>
<p>Text:</p>
<p><Text>Cobalt metal fume and dust cause upper respiratory tract irritation, chronic interstitial pneumonitis, and skin sensitization.</Text>Entities: <Entity>fume</Entity> <Entity>sensitization</Entity></p>
<p>Read the provided text carefully to comprehend the context and content.Examine the roles, interactions, and details surrounding the entities within the text.</p>
<p>Based only on the information in the text, determine the most likely cause-and-effect relationship between the entities from the following listed options (A, B, C):</p>
<p>Options: A: "fume" causes "sensitization"; B: "sensitization" causes "fume"; C: "fume" and "sensitization" are not directly causally related;</p>
<p>Your response should analyze the situation in a step-by-step manner, ensuring the correctness of the ultimate conclusion, which should accurately reflect the likely causal connection between the two entities, based on the information presented in the text.If no clear causal relationship is apparent, select the appropriate option accordingly.</p>
<p>Then provide your final answer within the tags <Answer>[answer]</Answer>, (e.g.<Answer>C</Answer>).</p>
<p>Figure 3: GPT prompt for causal relation discovery.</p>
<p>Some of our design strategies can be seen in Figure 3, where we show the prompt used to query the LLM for discovering the causal relationship between a pair of entities extracted from a natural language text.Additional prompts used in our experiments are available in the repository mentioned above.These include prompts designed to explain the reasons behind the model's chosen answers or correct contradictions the LLM made with its answers.</p>
<p>Learning Pairwise Relations</p>
<p>We first test the ability of GPT to recognise the right "orientation" in a causal relation.In practice, given a sentence where the two relevant entities are identified, assuming we know that a direct cause-effect relation links these, we want to decide whether the first entity causes the second or vice versa.</p>
<p>For an empirical evaluation, we consider the dataset from Hendrickx et al. ( 2019) used for a competition of the popular SemEval workshops. 3The dataset includes 10 217 sentences.Each sentence has tags to identify two relevant entities and an annotation about the kind of relation and its orientation.The sentences with cause-effect relations are 1003, while the others refer to other categories (e.g., membercollection, product-producer or content-container).Table 1 reports a few examples of causal sentences with entity tags and orientation.</p>
<p>We ask GPT to detect the right orientation among the two entities by a prompt analogous to that in Figure 3.Our prompt needs an average time of 11.5 ± 3.8s to obtain an anwser about the orientation.The LLM might also deny the presence of a causal relation.Yet, this is the case only 3 https://semeval.github.io.</p>
<p>Sentence Orientation</p>
<p>Zinc is essential for growth and cell division.</p>
<p>A → B The infection came from a wound.</p>
<p>A ← B As we saw earlier, helicobacter is responsible for causing stomach ulcer.</p>
<p>A → B</p>
<p>The pseudolesion was caused by drainage of the paraumbilical vein.</p>
<p>A ← B Table 1: Four examples of relations in the benchmark dataset.The cause-effect orientation between the first (blue) and the second (orange) entity is also reported.</p>
<p>for five sentences (≃ 0.4% of the cases).Table 2 depicts the confusion matrix for the orientation predictions in the remaining 998 cases.The very accurate performance (F1score ≃ 99%) advocates the ability of GPT in properly recognising the right orientation in a causal sentence.</p>
<p>Ground Truth
A → B A ← B GPT A → B 335 7 A ← B 6 650
Table 2: Confusion matrix for cause-effect orientation.</p>
<p>Notably, a direct inspection of the thirteen misoriented sentences highlights possible errors in the annotation of the SemEval data set.Some examples are reported in Table 3.</p>
<p>Sentence (Mis)Orientation</p>
<p>Alternators generate electricity by the same principle as DC generators.</p>
<p>A ← B</p>
<p>The movement developed from the rediscovery by European scholars of many Greek and Roman texts.</p>
<p>A → B</p>
<p>The cow makes a sound called lowing, also known as mooing.</p>
<p>A ← B</p>
<p>Defra identified the different noises made by dogs and the meanings behind them.</p>
<p>A → B</p>
<p>The relative calm produced by the Shia ceasefire has coincided with what the CIA is now calling the "near strategic defeat" of al-Qaeda in Iraq.</p>
<p>A → B</p>
<p>The backup vocals are from a rather talented female, Stephanie Eitel.</p>
<p>A → B Table 3: Wrong orientations detected by GPT.</p>
<p>The sentences in the dataset are already split in training and test set.For a comparison with other baselines, we consider the 2717 test sentences and extract the 325 causal relations.On those sentences only, our F1-score ≃ 99% is higher than the value obtained by the winner of the competition on the causal category (F1-score ≃ 90%, as reported by Hendrickx et al. (2019)).Notably such a good performance has been achieved without the 8000 training samples used for the training by the other method.Similar considerations can be done for a comparison against the deep learning approaches considered by Yin et al. (2017).</p>
<p>The good performance achieved in the orientation task even suggest using LLMs as a post-processing for the output of the sets of Markov equivalent CGs returned by causal learning algorithms when both data and texts are available.</p>
<p>Causal Graph Extrapolation</p>
<p>The pairwise procedure discussed in the previous section can be naturally extended to CG extrapolation by iterated applications on all the possible pairs of entities.We consider a benchmark of 20 medical papers to be processed by the repeated pairwise procedure for preliminary validation.We restrict our attention to the abstracts, regarded here as a reliable summary of the causal relations discussed in the paper.</p>
<p>The LLM is also used for entity recognition.For this task, the medical text is complemented with additional information about the types of entities to be extracted.Since we focus on medical literature, the model was explicitly instructed to identify entities with a particular emphasis on diseases, medications, treatments, and symptoms.Additional operations are performed to recognise synonyms, redundant entities, or entities and names that can be used interchangeably.In the generated output, entities with synonymous or similar meanings are matched.The relatively small number of entities (≤20) extracted from each abstract makes it possible to execute the, quadratic-time, iterated pairwise approach in a limited amount of time (≤30 minutes per abstract).</p>
<p>We denote the resulting CG as LLM-CG.CG * is instead the ground-truth CG obtained by asking domain experts to highlight the causal relations in the abstract.Note that the entities the experts consider during their annotation are those returned by the LLM.In this setup, a false positive is an arc present in LLM-CG but not in CG * , while a false negative is an arc missing in LLM-CG but not in CG * .</p>
<p>Figure 4: A multiply connected pattern in a CG.</p>
<p>In the experiment we observe relatively few false negatives and hence a high recall ≃ 97%, but, due to higher number of false positives a much lower precision ≃ 74%.The result denote a good ability of LLMs in properly grasping the actual causal relations in the text.We conjecture that the high number of false positives might reflect the inability of the model LLM in distinguishing between direct causeeffect relations and indirect ones.Consider for instance the slice of CG in Figure 4.Such a multiply connected topology might require an additional prompt engineering effort in order to decide whether the LLM properly grasps the direct influence of A toward C or should be instead regarded as a LLM reasoning based on the transitive property (A affect B and B affects C).</p>
<p>[. ..]Another patient had a significantly increased heart rate variability index without obvious changes in heart rate after the intervention.By reestablishing the balance in autonomic nervous system regulation and enhancing peripheral microcirculation, lifetide biofeedback intervention helps to maintain stable blood glucose levels, achieve disease remission [. ..]In some cases, we also observed directed cycles in the resulting CG (Figure 6).Those cycles always have length bigger than two, i.e., we never observe two entities connected by arcs of opposite orientations.This was also the case for the pairwise experiments discussed in the previous section.Nevertheless, directed cycles might arise in causality to model special situations such as the presence of feedback loops (Rehder 2017).This is not the case of the medical domains under consideration for our benchmark texts, and the in fact the ground-truth CGs are all acyclic.If a cyclic CG arises in a domain that seems to contradict such a possibility, we might again design dedicated prompt engineering approaches to for the model acyclicity.</p>
<p>Conclusions and Future Work</p>
<p>We presented the results of some preliminary experiments performed with GPT for CG extrapolation from text.A number of specific prompt engineering strategies are also discussed.The results are promising, especially concerning the orientation of pairwise relations already classified as causeeffect.We consequently regard our approach as a natural post-processing tool to be used to refine the output of standard algorithms for causal discovery when training data are available together with text.If only text is available, the LLM might add to the CG arcs that are not reflecting a direct cause-effect relation.</p>
<p>As a future work we should intend to significantly expand the benchmark of ground-truth CGs for a deeper validation and extend the analysis to hybrid approaches mixing LLMs processing data and causal discovery algorithms processing data.opportunities.</p>
<p>Figure 5 :
5
Figure 5: An extract from the medical abstract inducing the multiply connected pattern in Figure 4.</p>
<p>Figure 6 :
6
Figure 6: Two directed cycles in a CG.</p>
<p>Large Language Models for Biomedical Causal Graph Construction. V Arsenyan, D Shahnazaryan, arXiv:2301.124732023arXiv preprint</p>
<p>On Pearl's hierarchy and the foundations of causal inference. E Bareinboim, J D Correa, D Ibeling, T Icard, Probabilistic and causal inference: the works of judea pearl. 2022. 2002. Nov3Optimal structure identification with greedy search</p>
<p>I Hendrickx, S N Kim, Z Kozareva, P Nakov, D O Séaghdha, S Padó, M Pennacchiotti, L Romano, S Szpakowicz, arXiv:1911.10422Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. 2019arXiv preprint</p>
<p>. S Ji, S Pan, E Cambria, P Marttinen, S Y Philip, </p>
<p>A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems. 33</p>
<p>Can Large Language Models Infer Causation from Correlation?. Z Jin, J Liu, Z Lyu, S Poff, M Sachan, R Mihalcea, M Diab, B Schölkopf, arXiv:2306.058362023arXiv preprint</p>
<p>A survey on text classification: From traditional to deep learning. Q Li, H Peng, J Li, C Xia, R Yang, L Sun, P S Yu, L He, ACM Transactions on Intelligent Systems and Technology (TIST). 1322022</p>
<p>Fulminant type 1 diabetes: a comprehensive review of an autoimmune condition. S Luo, X Ma, X Li, Z Xie, Z Zhou, Diabetes/Metabolism Research and Reviews. 366e33172020</p>
<p>J Z Pan, S Razniewski, J.-C Kalo, S Singhania, J Chen, S Dietze, H Jabeen, J Omeliyanenko, W Zhang, M Lissandrini, arXiv:2308.06374Large language models and knowledge graphs: Opportunities and challenges. 2023arXiv preprint</p>
<p>Reasoning with causal cycles. J Pearl, Cognitive science. 412009. 2017Cambridge university pressCausality</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. L Reynolds, K Mcdonell, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. 2021</p>
<p>Deep learning in medical image analysis. Annual review of biomedical engineering. D Shen, G Wu, H.-I Suk, 201719</p>
<p>Towards expert-level medical question answering with large language models. K Singhal, T Tu, J Gottweis, R Sayres, E Wulczyn, L Hou, K Clark, S Pfohl, H Cole-Lewis, D Neal, arXiv:2305.096172023arXiv preprint</p>
<p>Approximating counterfactual bounds while fusing observational, biased and randomised data sources. P Spirtes, C N Glymour, R Scheines, W Yin, K Kann, M Yu, H Schütze, A Antonucci, R Cabañas, D Huber, arXiv:1702.01923Comparative study of CNN and RNN for natural language processing. MIT press2000. 2017. 2023162109023arXiv preprintCausation, prediction, and search</p>
<p>C Zhang, S Bauer, P Bennett, J Gao, W Gong, A Hilmkil, J Jennings, C Ma, T Minka, N Pawlowski, arXiv:2304.05524Understanding causality with large language models: Feasibility and opportunities. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6033 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6033</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6033</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-268385144</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.09032v2.pdf" target="_blank">CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences</a></p>
                <p><strong>Paper Abstract:</strong> —Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs’ outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that CodeLlama-7B-Instruct, aligned through reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO) using CodeUltraFeedback’s AI feedback data, outperforms 34B LLMs on CODAL-Bench, validating the utility of CodeUltra-Feedback for preference tuning. Furthermore, we show our DPO-aligned CodeLlama model improves functional correctness on HumanEval+ compared to the unaligned base model. Therefore, our contributions bridge the gap in preference tuning of LLMs for code and set the stage for further advancements in model alignment and RLAIF for code intelligence.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6033.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6033.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge (GPT-3.5/4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge using GPT-3.5-Turbo and GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses GPT-3.5-Turbo (primary) and GPT-4-Turbo as automatic judges to rate LLM-generated code responses on 1–5 (dataset annotation) and 1–10 (CODAL-Bench) scales, producing both numeric scores and textual rationales to assess alignment with five coding preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>code generation / code-quality preference alignment</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5-Turbo (primary judge) and GPT-4-Turbo (also used as judge and reference)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No human evaluation was run in this work for direct comparison; the authors used automated LLM judges only and explicitly state they did not perform human-vs-LLM comparison (left for future work).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Numeric alignment ratings (1–5 for dataset annotations; 1–10 on CODAL-Bench), textual rationales, win/tie/lose proportions, pairwise Welch's t-tests with permutations to compare score distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Empirically, GPT-3.5-Turbo and GPT-4-Turbo received the highest alignment scores; GPT-3.5-Turbo as judge treated GPT-4-Turbo responses as superior (e.g., GPT-4 win rate ≈51.6% vs GPT-3.5). The paper reports statistically significant score gaps between GPT-3.5 and other LLMs (p < 0.0001 for most comparisons). Using GPT-4 responses as references reduces alignment scores for many models relative to using GPT-3.5 references, showing judge/reference choice materially affects numeric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Judgment outcomes depend on judge model and reference response; potential preferences or style biases of judge LLMs can affect scores. Use of closed-source judges is costly; judge behavior can vary (GPT-4 and GPT-3.5 give different relative scores).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>No empirical human-vs-LLM divergences are reported (no direct human comparison). Observed problematic cases include substantial score drops for some models when judged under different judge/reference configurations (e.g., CodeLlama-13B score fell under GPT-4 judge), revealing judge-dependent inconsistencies.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Design grading templates and chain-of-thought prompting to increase consistency; evaluate with multiple judges and reference responses; consider training a small open-source critic LLM using the AI feedback dataset to replace costly closed-source judges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6033.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6033.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human comparison (absent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Absence of direct human vs LLM-judge comparison in this work</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors explicitly report that they did not run a direct comparison between GPT-3.5/GPT-4 judgments and human judgments on CODAL-Bench or CodeUltraFeedback and leave it as future work, while noting prior literature that suggests high agreement between GPT models and humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>code generation / judgment validation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5-Turbo and GPT-4-Turbo (used as judges in the study, but not compared to humans here)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Not conducted — the paper states explicitly: 'our work does not include a comparison between GPT-3.5/4 and human judgements, we hypothesize LLM judgments could match human judgments on CODAL-Bench and leave the comparison for future work.'</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>None in this paper (no direct human-LLM agreement metrics reported). The authors reference prior work claiming GPT-3.5/4 achieve agreement rates comparable to humans on MT-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>No quantitative or qualitative differences between LLM judges and humans are reported because no human study was performed; only a hypothesis that LLM judgments could match humans based on prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Because no human baseline was collected, the validity of LLM-as-a-judge as a proxy for humans on CODAL-Bench remains unverified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>None empirically reported versus human judges (explicitly absent).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Authors recommend future work to run human-LLM comparisons and to incorporate human annotations to improve representativeness and validate LLM judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6033.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6033.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Judge variability & biases</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed judge variability and potential judgment biases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that changing judge LLM and reference responses changes alignment scores and rankings—indicating judge-dependent variability—and notes possible biases (e.g., favoring verbose responses) that could influence judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>code generation evaluation / meta-evaluation of judges</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5-Turbo and GPT-4-Turbo (empirically contrasted in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No human setup; variability assessed across LLM judges and reference-response choices.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Average alignment scores per model and preference, win/tie/lose ratios against GPT-3.5 and GPT-4, and pairwise statistical tests to assess significance of score differences across judges.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Using GPT-3.5 as judge with GPT-3.5 references yields higher scores for some models; using GPT-4 as judge (or GPT-4 references) often lowers scores for CodeLlama variants while increasing others (e.g., WizardCoder). Differences indicate judges prefer different response styles and that reference choice shifts absolute scores and relative rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Potential preference biases (e.g., favoring length/verbosity), judge-specific style preferences, sensitivity to which model produced the reference response, and closed-source judge opaqueness preventing detailed analysis of bias sources.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Concrete instances where a model's score shifts substantially depending on judge/reference: CodeLlama-13B-Instruct dropped from 5.58 to 4.83 under GPT-4 judgment; WizardCoder-33B increased from 6.26 to 6.75 — illustrating judge-dependent rank reversals.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use multiple judges and multiple reference responses to average out judge-specific biases; further investigation into judge preferences; build and fine-tune small open-source critic models using AI feedback data to provide cheaper, customizable judges; design grading templates to reduce style sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6033.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6033.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mitigations & recommended practices</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mitigation strategies and recommended practices to address LLM-as-a-Judge limitations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper proposes several practical mitigations: rigorous grading templates and chain-of-thought prompting for consistency, employing multiple judges and references, fine-tuning small critic LLMs on collected AI feedback to replace costly closed-source judges, and adding human annotations where feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>evaluation methodology for code generation and preference alignment</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Proposal applies to using GPT-3.5/GPT-4 or fine-tuned small critic models as judges</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Recommended future addition: incorporate human annotations to improve sample representativeness and to validate judge LLMs; no specific human panel size or protocol given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Recommendations aim to improve the reliability of numeric alignment ratings, inter-judge consistency, and robustness of rank-order comparisons (no new metrics introduced).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Not applicable (these are proposed strategies rather than evaluated interventions in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Mitigations are proposed to address judge cost, judge-specific bias, and judge/reference sensitivity; authors note the need to empirically validate these mitigations in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Not empirically demonstrated here; mitigations are motivated by observed judge inconsistencies and potential biases.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>1) Use reference-guided single-answer grading and chain-of-thought prompting to increase consistency; 2) Evaluate using multiple judges and reference responses; 3) Fine-tune a small open-source critic LLM on CodeUltraFeedback AI feedback (ratings + rationales) to reduce reliance on closed-source judges; 4) Incorporate human annotations and stricter sample selection for benchmarks; 5) Explore judge selection effects systematically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MT-Bench <em>(Rating: 2)</em></li>
                <li>Shepherd: A critic for language model generation <em>(Rating: 2)</em></li>
                <li>Prometheus: Inducing fine-grained evaluation capability in language models <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An automatic evaluator of instruction-following models <em>(Rating: 2)</em></li>
                <li>Ultrafeedback: Boosting language models with high-quality feedback <em>(Rating: 2)</em></li>
                <li>Zephyr: Direct distillation of LM alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6033",
    "paper_id": "paper-268385144",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge (GPT-3.5/4)",
            "name_full": "LLM-as-a-Judge using GPT-3.5-Turbo and GPT-4-Turbo",
            "brief_description": "The paper uses GPT-3.5-Turbo (primary) and GPT-4-Turbo as automatic judges to rate LLM-generated code responses on 1–5 (dataset annotation) and 1–10 (CODAL-Bench) scales, producing both numeric scores and textual rationales to assess alignment with five coding preferences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "code generation / code-quality preference alignment",
            "llm_judge_model": "GPT-3.5-Turbo (primary judge) and GPT-4-Turbo (also used as judge and reference)",
            "human_evaluation_setup": "No human evaluation was run in this work for direct comparison; the authors used automated LLM judges only and explicitly state they did not perform human-vs-LLM comparison (left for future work).",
            "metrics_compared": "Numeric alignment ratings (1–5 for dataset annotations; 1–10 on CODAL-Bench), textual rationales, win/tie/lose proportions, pairwise Welch's t-tests with permutations to compare score distributions.",
            "reported_differences": "Empirically, GPT-3.5-Turbo and GPT-4-Turbo received the highest alignment scores; GPT-3.5-Turbo as judge treated GPT-4-Turbo responses as superior (e.g., GPT-4 win rate ≈51.6% vs GPT-3.5). The paper reports statistically significant score gaps between GPT-3.5 and other LLMs (p &lt; 0.0001 for most comparisons). Using GPT-4 responses as references reduces alignment scores for many models relative to using GPT-3.5 references, showing judge/reference choice materially affects numeric scores.",
            "llm_specific_limitations": "Judgment outcomes depend on judge model and reference response; potential preferences or style biases of judge LLMs can affect scores. Use of closed-source judges is costly; judge behavior can vary (GPT-4 and GPT-3.5 give different relative scores).",
            "notable_failure_cases": "No empirical human-vs-LLM divergences are reported (no direct human comparison). Observed problematic cases include substantial score drops for some models when judged under different judge/reference configurations (e.g., CodeLlama-13B score fell under GPT-4 judge), revealing judge-dependent inconsistencies.",
            "mitigation_strategies": "Design grading templates and chain-of-thought prompting to increase consistency; evaluate with multiple judges and reference responses; consider training a small open-source critic LLM using the AI feedback dataset to replace costly closed-source judges.",
            "uuid": "e6033.0",
            "source_info": {
                "paper_title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Human comparison (absent)",
            "name_full": "Absence of direct human vs LLM-judge comparison in this work",
            "brief_description": "The authors explicitly report that they did not run a direct comparison between GPT-3.5/GPT-4 judgments and human judgments on CODAL-Bench or CodeUltraFeedback and leave it as future work, while noting prior literature that suggests high agreement between GPT models and humans.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "code generation / judgment validation",
            "llm_judge_model": "GPT-3.5-Turbo and GPT-4-Turbo (used as judges in the study, but not compared to humans here)",
            "human_evaluation_setup": "Not conducted — the paper states explicitly: 'our work does not include a comparison between GPT-3.5/4 and human judgements, we hypothesize LLM judgments could match human judgments on CODAL-Bench and leave the comparison for future work.'",
            "metrics_compared": "None in this paper (no direct human-LLM agreement metrics reported). The authors reference prior work claiming GPT-3.5/4 achieve agreement rates comparable to humans on MT-Bench.",
            "reported_differences": "No quantitative or qualitative differences between LLM judges and humans are reported because no human study was performed; only a hypothesis that LLM judgments could match humans based on prior literature.",
            "llm_specific_limitations": "Because no human baseline was collected, the validity of LLM-as-a-judge as a proxy for humans on CODAL-Bench remains unverified in this paper.",
            "notable_failure_cases": "None empirically reported versus human judges (explicitly absent).",
            "mitigation_strategies": "Authors recommend future work to run human-LLM comparisons and to incorporate human annotations to improve representativeness and validate LLM judgments.",
            "uuid": "e6033.1",
            "source_info": {
                "paper_title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Judge variability & biases",
            "name_full": "Observed judge variability and potential judgment biases",
            "brief_description": "The paper documents that changing judge LLM and reference responses changes alignment scores and rankings—indicating judge-dependent variability—and notes possible biases (e.g., favoring verbose responses) that could influence judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "code generation evaluation / meta-evaluation of judges",
            "llm_judge_model": "GPT-3.5-Turbo and GPT-4-Turbo (empirically contrasted in the paper)",
            "human_evaluation_setup": "No human setup; variability assessed across LLM judges and reference-response choices.",
            "metrics_compared": "Average alignment scores per model and preference, win/tie/lose ratios against GPT-3.5 and GPT-4, and pairwise statistical tests to assess significance of score differences across judges.",
            "reported_differences": "Using GPT-3.5 as judge with GPT-3.5 references yields higher scores for some models; using GPT-4 as judge (or GPT-4 references) often lowers scores for CodeLlama variants while increasing others (e.g., WizardCoder). Differences indicate judges prefer different response styles and that reference choice shifts absolute scores and relative rankings.",
            "llm_specific_limitations": "Potential preference biases (e.g., favoring length/verbosity), judge-specific style preferences, sensitivity to which model produced the reference response, and closed-source judge opaqueness preventing detailed analysis of bias sources.",
            "notable_failure_cases": "Concrete instances where a model's score shifts substantially depending on judge/reference: CodeLlama-13B-Instruct dropped from 5.58 to 4.83 under GPT-4 judgment; WizardCoder-33B increased from 6.26 to 6.75 — illustrating judge-dependent rank reversals.",
            "mitigation_strategies": "Use multiple judges and multiple reference responses to average out judge-specific biases; further investigation into judge preferences; build and fine-tune small open-source critic models using AI feedback data to provide cheaper, customizable judges; design grading templates to reduce style sensitivity.",
            "uuid": "e6033.2",
            "source_info": {
                "paper_title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Mitigations & recommended practices",
            "name_full": "Mitigation strategies and recommended practices to address LLM-as-a-Judge limitations",
            "brief_description": "The paper proposes several practical mitigations: rigorous grading templates and chain-of-thought prompting for consistency, employing multiple judges and references, fine-tuning small critic LLMs on collected AI feedback to replace costly closed-source judges, and adding human annotations where feasible.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "evaluation methodology for code generation and preference alignment",
            "llm_judge_model": "Proposal applies to using GPT-3.5/GPT-4 or fine-tuned small critic models as judges",
            "human_evaluation_setup": "Recommended future addition: incorporate human annotations to improve sample representativeness and to validate judge LLMs; no specific human panel size or protocol given in this paper.",
            "metrics_compared": "Recommendations aim to improve the reliability of numeric alignment ratings, inter-judge consistency, and robustness of rank-order comparisons (no new metrics introduced).",
            "reported_differences": "Not applicable (these are proposed strategies rather than evaluated interventions in the paper).",
            "llm_specific_limitations": "Mitigations are proposed to address judge cost, judge-specific bias, and judge/reference sensitivity; authors note the need to empirically validate these mitigations in future work.",
            "notable_failure_cases": "Not empirically demonstrated here; mitigations are motivated by observed judge inconsistencies and potential biases.",
            "mitigation_strategies": "1) Use reference-guided single-answer grading and chain-of-thought prompting to increase consistency; 2) Evaluate using multiple judges and reference responses; 3) Fine-tune a small open-source critic LLM on CodeUltraFeedback AI feedback (ratings + rationales) to reduce reliance on closed-source judges; 4) Incorporate human annotations and stricter sample selection for benchmarks; 5) Explore judge selection effects systematically.",
            "uuid": "e6033.3",
            "source_info": {
                "paper_title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MT-Bench",
            "rating": 2
        },
        {
            "paper_title": "Shepherd: A critic for language model generation",
            "rating": 2,
            "sanitized_title": "shepherd_a_critic_for_language_model_generation"
        },
        {
            "paper_title": "Prometheus: Inducing fine-grained evaluation capability in language models",
            "rating": 2,
            "sanitized_title": "prometheus_inducing_finegrained_evaluation_capability_in_language_models"
        },
        {
            "paper_title": "AlpacaEval: An automatic evaluator of instruction-following models",
            "rating": 2,
            "sanitized_title": "alpacaeval_an_automatic_evaluator_of_instructionfollowing_models"
        },
        {
            "paper_title": "Ultrafeedback: Boosting language models with high-quality feedback",
            "rating": 2,
            "sanitized_title": "ultrafeedback_boosting_language_models_with_highquality_feedback"
        },
        {
            "paper_title": "Zephyr: Direct distillation of LM alignment",
            "rating": 1,
            "sanitized_title": "zephyr_direct_distillation_of_lm_alignment"
        }
    ],
    "cost": 0.011658499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences</p>
<p>Martin Weyssow martin.weyssow@umontreal.ca 
DIRO
University of Montreal
Canada</p>
<p>Aton Kamanda aton.kamanda@umontreal.ca 
DIRO
University of Montreal
Canada</p>
<p>Houari Sahraoui sahraouh@iro.umontreal.ca 
DIRO
University of Montreal
Canada</p>
<p>CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences
D32C2B95C43F6C190883431B5B51C8FECodeLlama-13B-Instruct CodeLlama-34B-Instruct DeepSeek-Coder-6.7B-Instruct WizardCoder-33B GPT-4-Turbo CodeLlama7B-Instruct CodeLlama-7B-Instruct +SFT CodeLlama-7B-Instruct +DPO CodeLlama-7B-Instruct +SFT+DPO
Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs.By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment.In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback.We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback.We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences.Our results show that CodeLlama-7B-Instruct, aligned through reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B LLMs on CODAL-Bench, validating the utility of CodeUltra-Feedback for preference tuning.Furthermore, we show our DPOaligned CodeLlama model improves functional correctness on HumanEval+ compared to the unaligned base model.Therefore, our contributions bridge the gap in preference tuning of LLMs for code and set the stage for further advancements in model alignment and RLAIF for code intelligence.</p>
<p>I. INTRODUCTION</p>
<p>The advent of recent large language models (LLMs) has ushered in a new era of LLMs with high coding capabilities [1]- [6], showcasing remarkable performances across a wide range of downstream tasks including code generation [7]- [10], code translation [11], [12], bug fixing [6], [13], [14], and more.As the coding abilities of LLMs continue to surge forward, a crucial question emerges: how well do these capabilities align with the expectations of developers, particularly concerning non-functional requirements such as code readability, efficiency, and adherence to best practices?</p>
<p>Current methodologies for fine-tuning and evaluating LLMs widely focus on core capabilities of LLMs, e.g., translating, summarizing, or reviewing code [15]- [22], and functional correctness in diverse code generation scenarios [1], [6], [7], [23]- [25].Other works aim at assessing non-functional properties of LLM-generated code, including code quality [26], runtime efficiency [27] and others [28], [29].However, they generally consist of refining existing benchmarks to evaluate non-functional requirements, often without considering more open coding problems with intricate user instructions.Additionally, the evaluation often relies on automated metrics and external tools based on rigid standards and patterns, overlooking the nuanced complexity of users' instructions and LLMs' outputs.Finally, we underline a lack of large-scale datasets tailored for tuning and aligning LLMs to non-functional requirements.</p>
<p>The existing datasets and benchmarks in code intelligence collectively reveal a significant gap in both tuning LLMs and measuring their alignment to non-functional requirements through a more human-centric approach capable of discerning the intricacies of natural and programming languages.Addressing this gap requires a dual approach: developing datasets tailored to tune and align LLMs with non-functional requirements and benchmarks to evaluate their alignment.</p>
<p>In this paper, we introduce two novel contributions: CodeUltraFeedback and CODAL-Bench ("Code Alignment Benchmark").CodeUltraFeedback is a preference dataset comprising 10,000 complex instructions with 40,000 meticulously curated LLMs responses aligned with five nonfunctional requirements (or coding preferences1 ): instruction following, code explanation, code complexity and efficiency, code readability, and coding style.The objective of CodeUl-traFeedback is to serve as a dataset for preference tuning of LLMs, leveraging recent advancements in LLM alignment, including UltraFeedback [30], reinforcement learning from AI feedback (RLAIF) [31]- [33] and LLM-as-a-Judge [34], [35], predicated upon the advanced judging capabilities of LLMs like GPT-3.5 or GPT-4.We adopt an approach auxiliary to UltraFeedback for building our dataset, and start by tagging each instruction with a coding preference.Then, we generate responses to the instructions using four LLMs randomly selected from a pool of 14 LLMs to achieve diversity and consider various writing styles.Finally, we use LLM-as-a-Judge with GPT-3.5 to rate the alignment of the LLMs' responses with respect to the instruction's coding preference, resulting in annotations comprising a numerical rating and a textual rationale for the rating.These AI feedbacks can then be leveraged to align LLMs to generate high-quality responses given a coding preference by tuning the LLM through RLAIF using techniques like direct preference optimization (DPO) [36].</p>
<p>Subsequently, we construct CODAL-Bench by selecting a subset of 500 instructions from CodeUltraFeedback.CODAL-Bench serves a different purpose than CodeUltraFeedback and aims to comprehensively assess and compare the alignment of LLMs with coding preferences.We design a rigorous singleanswer grading scheme with LLM-as-a-Judge where GPT-3.5-Turbo or GPT-4-Turbo evaluates a single LLM response at a time in a consistent and objective manner.This approach offers a more nuanced evaluation strategy than previous methods that depend on automated metrics and external tools by leveraging the advanced reasoning capabilities of LLMs.Their ability to discern intricacies and nuances in language allows for a more refined and context-sensitive evaluation of how well code aligns with coding preferences in a human-centric fashion.</p>
<p>In the initial part of this paper, we conduct an exploratory analysis of CodeUltraFeedback's annotations and demonstrate the strong judging capabilities of GPT-3.5-Turbo.GPT-3.5-Turboeffectively recognizes GPT-4-Turbo as a superior model to itself, demonstrating impartiality and fairness in its judgement while also discerning between responses of different quality.Moreover, our initial exploration also reveals a lack of alignment of 12 LLMs, including strong LLMs such as WizardCoder-33B [37] and DeepSeek-Coder-Instruct-33B [3].</p>
<p>In the second part of this paper, we explore preference tuning of a small LLM, CodeLlama-7B-Instruct [2] using CodeUltraFeedback with supervised fine-tuning (SFT) and RLAIF with DPO [36], [38].The method relies on using the AI feedback data from CodeUltraFeedback, where LLMs' responses with high and low ratings are selected for tuning and aligning CodeLlama-7B-Instruct to coding preferences with DPO.DPO encourages the LLM to favour highly rated LLMs' responses, enabling itself to generate more aligned content at inference.Moreover, we implement QLoRA [39] for efficient fine-tuning and show SFT and DPO can be achieved on a single RTX A5000 GPU (24GB).</p>
<p>Our experiments validate the utility of CodeUltraFeedback for preference tuning.We demonstrate that tuning CodeLlama-7B-Instruct with SFT and DPO substantially improves LLM alignment across all coding preferences with high statistical significance on CODAL-Bench, surpassing all larger LLMs, including CodeLlama-34B-Instruct and WizardCoder-33B when using GPT-3.5-Turboas a judge.Further, we explore LLM alignment on CODAL-Bench using GPT-4-Turbo as a judge, and highlight more mitigated effects of SFT and DPO, albeit CodeLlama-7B-Instruct still outperforms CodeLlama-13B-Instruct and CodeLlama-34B-instruct in this setup.Finally, we show that preference tuning does not hinder the capability of CodeLlama-7B-Instruct in generating functionally correct code.On the contrary, we highlight that CodeLlama-7B-Instruct tuned with SFT and DPO substantially improves Pass@k on HumanEval [1] and HumanEval+ [7].Therefore, our work establishes strong evidence of the benefits of SFT and DPO to enhance LLM alignment with humancentric coding preferences, demonstrating improvements in both preference alignment and functional correctness.Therefore, our work paves the way for future advancements in LLM training and evaluation, promising a closer alignment with the nuanced expectations of developers.</p>
<p>To summarize, our contributions are the following: -We release CodeUltraFeedback, a preference dataset of 10,000 complex instructions and 40,000 responses generated using 14 diverse LLMs for aligning LLMs to coding preferences in a code generation scenario.-We provide CODAL-Bench, a benchmark to evaluate and compare LLM alignment over five coding preferences: instruction following, code explanation, code complexity and efficiency, code readability, and coding style.-We validate the utility of CodeUltraFeedback in facilitating LLM alignment of CodeLlama-7B-Instruct, a small LLM, using SFT and DPO.Our models, dataset, benchmark, and prompt templates are available at https://github.com/martinwey/CodeUltraFeedback.</p>
<p>II. CODEULTRAFEEDBACK</p>
<p>In this section, we introduce our new dataset, CodeUl-traFeedback.Fig. 1 depicts our methodology for building CodeUltraFeedback, which incorporates various components and ideas that we describe in the subsequent subsections.</p>
<p>A. Coding Preferences and Principles.</p>
<p>We start by identifying five coding preferences to guide the creation of our dataset and which are essential to evaluate the broader capabilities of LLMs: (1) Instruction-Following is about the strict adherence of the LLM to the instructions provided by users.This preference is foundational for ensuring that LLMs truly follow the user intent and thus provide personalized responses to instructions.(2) Code Explanation emphasizes the generation of clear code with detailed explanations.It underscores the importance of understandable and adaptable code, serving as a bridge between potentially complex programming solutions and the users.(3) Code Complexity and Efficiency preference underlines the LLM capability to generate code optimized for performance in terms of speed and resource utilization.It is another crucial aspect requiring the LLM to carefully balance the speed and resource usage of the solution.(4) Code Readability serves a distinct purpose to the code explanation preference by emphasizing the clarity and understandability of the code itself through its structure, style, and the presence of meaningful documentation and in-line comments.(5) Coding Style preference focuses on the importance of writing code in a manner that not only meets syntactical correctness but also aligns with the idiomatic practices and stylistic norms of the programming language.</p>
<p>We generate 10 principles per preference using ChatGPT following prior work practices [30], [41], [42].These principles shortly describe the essence of each preference and are leveraged in the Principle-Driven Code Generation step Magicoder Evol-Instruct Dataset  CodeUltraFeedback is based on a 10k-instruction subset of Magicoder Evol-Instruct dataset [37], [40], where each instruction is tagged with a coding preference.(iii) LLMs Selection: for each instruction, four LLMs are randomly sampled from a diverse pool of 14 LLMs.(iv) Principle-Driven Code Generation: one principle is randomly selected per LLM to guide and align the code generation process with the assigned coding preference.(v) LLM-as-a-Judge Annotation: LLM-as-a-Judge with GPT-3.5-Turbo is used to judge LLMs' responses according to coding preferences evaluation criteria (see Table I).</p>
<p>to guide the LLMs generative process.The rationale for establishing 10 principles per preference is to achieve comprehensive and diverse outputs in this code generation step, which we further describe in Section II-D.</p>
<p>Altogether, these five preferences complement the functional correctness requirement of LLM-generated code.Assessing how well LLMs align with these preferences amounts to evaluating how the generated solutions are optimized for human comprehension, maintainability, performance, and collaborative work.</p>
<p>B. Initial Dataset.</p>
<p>To build CodeUltraFeedback, we rely on Magicoder Evol-Instruct dataset [40], an Evol-Instruct [37], [43] version of CodeAlpaca dataset comprising complex coding problem instructions.We select a random subset of Magicoder Evol-Instruct comprising 10,000 samples serving as an initial dataset for CodeUltraFeedback.We assign 2,000 samples per coding preference, ensuring a balanced representation of each preference in the dataset.The reason for subsampling the dataset Evaluate the readability of code segments.Assess how comments and documentation contribute to understanding the code's logic, purpose, and operation.</p>
<p>Evaluation Criteria:</p>
<p>Clarity: How clear and understandable are the code and its accompanying comments/documentation? Conciseness: Are the comments and documentation succinct yet informative?</p>
<p>Relevance: Do the comments and documentation directly contribute to explaining the code's logic, objectives, and functionality?</p>
<p>Comprehensibility: Can users of varying technical backgrounds easily grasp the code's purpose and how it works?Scoring: Rate outputs on a scale of 1 to 5:</p>
<ol>
<li>Poor Readability: The code is hard to follow, with little to no helpful comments/documentation.</li>
</ol>
<p>Basic Readability:</p>
<p>The code has minimal comments/documentation, offering limited clarity or insight.</p>
<ol>
<li>Good Readability: The code is reasonably clear with comments/documentation that aid understanding, though some areas could be improved.4. Very Good Readability: The code and comments/documentation are clear and concise, making the code's logic and purpose easily understandable.</li>
</ol>
<p>Excellent Readability:</p>
<p>The code exemplifies outstanding readability, with clear, concise, and comprehensive comments/documentation that make it accessible to all users.to 10,000 samples is to lower costs related to OpenAI API needed to generate some of the responses and the annotations.</p>
<p>C. LLMs Selection.</p>
<p>In order to generate highly diverse solutions to the instructions, we select a diverse pool of 14 LLMs spanning eight model families.These include GPT-4-Turbo and GPT-3.5-Turbo as closed-source LLMs.We include Llama-2-13B-Chat and Llama-2-70B-Chat [5], specialized for chat use cases.We choose CodeLlama-7/13/34B-Instruct models [2], instructiontuned for code generation.We also include WizardLM-15/33B models [43] and Mistral-7B-Instruct [4], which are instructiontuned LLMs that can follow complex instructions.Finally, we include WizardCoder-15/33B [37] and DeepSeek-Coder-6.7/33B-Instruct[3], which are instruction-tuned LLMs that demonstrate state-of-the-art performance on code generation benchmarks such as HumanEval [1] and HumanEval+ [7].From this pool, we randomly select four LLMs for each instruction to generate responses, allowing for the representation of a variety of coding approaches and styles.</p>
<p>D. Principle-Driven Code Generation.</p>
<p>In this step, we leverage the concept of principle-driven generation [41], [42], [44] for code.One principle, corresponding to the coding preference of the sample, is randomly chosen per LLM and appended to the instruction in an input prompt to guide the generation.This strategy helps the LLMs generate responses that potentially align with the requirements of the coding preferences.The output of this step is the four LLMs' responses to the principle-augmented instructions.</p>
<p>E. LLM-as-a-Judge Annotation.</p>
<p>We apply the LLM-as-a-Judge methodology [34] to annotate responses using GPT-3.5-Turbo.Drawing from practices established in previous work [30], we implement a set of guidelines TABLE II: Excerpt of ratings and rationales generated by GPT-3.5-Turbo using LLM-as-a-Judge prompting.GPT-3.5-Turbogenerates clear rationales for each rating demonstrating its strong judging capabilities.</p>
<p>Instruction: Devising a Python-based algorithm that utilizes the technique of ternary search, could you draw this up while also illustrating the functionality of each critical stage in the searching process?Preference: Readability GPT-4: [Response] Rating: 5 Rationale: The code is accompanied by clear and concise comments that comprehensively explain each critical stage of the ternary search algorithm.The comments provide a thorough understanding of the algorithm's logic, purpose, and functionality, making it easily accessible to users of varying technical backgrounds.Additionally, the comments include a detailed explanation of the algorithm's relevance and efficiency compared to binary search, enhancing the overall comprehensibility of the code.
DeepSeek-Coder-6.7B-Instruct: [Response] Rating: 4 Rationale: [Rationale] Mistral-7B-Instruct: [Response] Rating: 3 Rationale: [Rationale] CodeLlama-13B-Instruct: [Response]
Rating: 2 Rationale: The code is clear, but the comments are minimal and lack sufficient detail to fully explain the ternary search algorithm and its critical stages.While the comments briefly describe the function's purpose, they do not provide a comprehensive explanation of the algorithm's logic and functionality.Additionally, the comments do not cover the relevance of ternary search in algorithmic strategies, making it less accessible to users with varying technical backgrounds.</p>
<p>ensuring GPT-3.5-Turbodelivers both consistent and objective feedback.Each annotation consists of (1) a score on a 1-5 scale offering a comprehensive assessment of the response's alignment, and (2) a rationale for the assigned score, highlighting the response's strengths and identifying any potential shortcomings.Tasking GPT-3.5-Turbo to provide a rationale for the scores encourages a chain-of-thought [45] process during the annotation process, elucidating the reasons behind the scores and meaningful feedback.In addition, this process mitigates the risk of variability and subjectivity that might arise from purely quantitative assessments.Consequently, this reflective process guarantees that the scoring is not just a surfacelevel judgment but can be backed by thoughtful consideration of some well-defined coding preferences evaluation criteria.</p>
<p>Table I illustrates the assessment template and evaluation criteria for code readability evaluation.It aims to provide precise and comprehensive criteria to the LLM judge for response assessment.We design analogous assessment templates, each tailored with specific evaluation criteria, for evaluating the other coding preferences.The annotation procedure entails constructing a prompt combining the coding preference assessment template and the four LLMs responses.GPT-3.5-Turbo is then tasked to generate the annotations for all four responses simultaneously, enhancing the consistency and reliability of the evaluation process over individual response assessments.Table II presents an example of the output of the annotation process using GPT-3.5-Turbo,with the LLMs responses and some rationales omitted for brevity.This example showcases GPT-3.5-Turbo'scapability to provide comprehensive justifications for each rating, demonstrating its effective evaluation capabilities.Furthermore, Table II illustrates an example of relatively intricate instructions included in CodeUltraFeedback, showcasing the complexity of the task.More details about CodeUltraFeedback and the prompt templates are included in our replication package: https://github.com/martinwey/CodeUltraFeedback.III.EVALUATING LLMS ON CODEULTRAFEEDBACK While CodeUltraFeedback has the potential to facilitate diverse downstream applications, including LLM alignment through RLAIF and DPO, we begin our experiments by analyzing the dataset itself.Specifically, the goal is to gauge GPT-3.5-Turbo'sjudging capabilities and the innate ability of LLMs to align to coding preferences without any tailored optimization involved.</p>
<p>A. LLMs Scores Exploration</p>
<p>We start by exploring the ratings generated by GPT-3.5-Turbo in the LLM-as-a-Judge annotation phase.In Table III, we present a detailed analysis of the average scores across coding preferences for each LLM, offering initial insight into their baseline capabilities.</p>
<p>First, we observe that GPT-3.5-Turbo and GPT-4-Turbo score the highest across all preferences and on average.This result is expected considering both models have undergone extensive instruction and RLHF tunings, which help align the LLMs more closely with human preferences [33].</p>
<p>At first glance, the scores might suggest marginal score discrepancies given the 1-5 scoring range.However, these variations are substantial and carry statistical significance.To quantify this significance, we performed pairwise Welch's ttests with 100,000 permutations to mitigate the risk of type I error.These t-tests compare the score distributions for each LLM (and GPT-4-Turbo) against GPT-3.5-Turboacross each coding preference and consistently reveal highly significant statistical differences (with p &lt; 0.0001 for most comparisons).Therefore, these results validate the discernible performance gaps between all models and GPT-3.5-Turbo.Moreover, the results also demonstrate a notably superior performance of GPT-4-Turbo relative to GPT-3.5-Turbo.This effectively illustrates GPT-3.5-Turbo'simpartiality as a judge, rewarding higher scores to more proficient LLMs and equitably evaluating its own responses.</p>
<p>In summary, our findings indicate that all LLMs, including highly capable ones such as WizardCoder-33B and DeepSeek-Coder-33B-Instruct, underperform compared to GPT-3.5-Turbo.This underperformance is likely due to a lack of alignment of the LLMs, and we believe fine-tuning them using alignment techniques might enhance their performance.</p>
<p>B. LLMs vs GPT-3.5-Turbo</p>
<p>To gauge the relative performance of the LLMs against GPT-3.5-Turbomore in-depth, we analyze their performances An LLM wins/loses if it gets a greater/lower score than GPT-3.5-Turbo.A tie is when the LLM and GPT-3.5-Turboget identical scores.</p>
<p>through a win-tie-lose plot depicted in Fig. 2. The idea is to pit each LLM (left-hand side of the figure, e.g., Mistral-7B-Instruct) against GPT-3.5-Turbo in a comparative matchup, where the model achieving the highest rating wins.The plot illustrates the percentage of ratings of an LLM that are higher/lower compared to those of GPT-3.5-Turbo,all preferences combined (a tie stands for ratings having identical values).This figure further demonstrates significant gaps between LLMs and GPT-3.5-Turbo,underscoring a consistent preference for GPT-3.5-Turbo and GPT-4-Turbo's responses over other LLMs.For instance, even Mistral-7B-Instruct, the second-best LLM, achieves a win rate of merely 29.8%.Additionally, the plot further validates the utilization of GPT-3.5-Turbo as a judge, showcasing its capability to discern between responses of differing quality.For example, it is proficient in recognizing higher-quality responses, as evidenced by its comparison with GPT-4-Turbo, which boasts a win rate of 51.6%.</p>
<p>In conclusion, these initial observations highlight a significant research opportunity and the necessity to delve deeper into aligning LLMs with coding preferences to make them more competitive with models like GPT-3.5 and GPT-4.</p>
<p>TABLE III:</p>
<p>Average scores across coding preferences for each LLM.The statistical significance of differences in ratings between each LLM and GPT-3.5-Turbo was tested using pairwise Welch's t-tests with 100,000 permutations to control for type I error.Significance levels are denoted as follows: * for p &lt; 0.05, † for p &lt; 0.01, and ‡ for p &lt; 0.001.The results from almost all pairwise t-tests indicate highly significant differences (p &lt; 0.001).</p>
<p>Model</p>
<p>IV. ALIGNING LLMS TO CODING PREFERENCES</p>
<p>In this section, we present our experimental setup to improve LLM alignment with coding preferences using CodeUl-traFeedback as preference data, which features the utilization of SFT and DPO [7], [38].The purpose is twofold: validate the utility of CodeUltraFeedback for LLM alignment to coding preferences and show that a small LLM tuned using SFT and DPO can achieve greater alignment performance.</p>
<p>A. Supervised Fine-Tuning (SFT)</p>
<p>As demonstrated in Zephyr's paper [38], an instructiontuning phase is required prior to tuning an LLM using DPO as it facilitates tuning.This phase is achieved through supervised fine-tuning (SFT) using a dataset D = {(x 1 , y 1 ), ..., (x n , y n )} of instruction-response pairs.Recent work leverages the Self-Instruct [46] and Evol-Instruct [37], [43] frameworks to compose such a dataset by generating instructions and responses using a highly capable model such as GPT-3.5/4[33], [40], [47].In this context, SFT is a form of knowledge distillation [48] that leverages a supervised signal from responses generated by a teacher LLM to tune a student LLM.</p>
<p>Formally, for each instruction-response pair (x i , y i ) ∈ D, the learning objective is to minimize the cross-entropy loss L SF T , defined as:
L SF T = − 1 n n i=1 log P (y i |x i , θ),
where P (y i |x i , θ) represents the probability of generating the response y i given the instruction x i , parameterized by θ, the LLM parameters.</p>
<p>B. Direct Preference Optimization (DPO)</p>
<p>DPO is an efficient ranking optimization method to align LLMs to preference data, where a response y w is preferred over a rejected response y l .Unlike traditional reinforcement learning methods like RLHF, which require training a separate reward model, DPO enhances stability and performance by directly adjusting the LLM's policy towards higher-ranked responses (i.e., y w ).Given a dataset P of triplets (x, y w , y l ), a reference LLM policy π ref , and an LLM policy π θ , the objective is to maximize the following expectation:
E (x,yw,y l )∼P log σ β log π θ (y w |x) π ref (y w |x) − π θ (y l |x) π ref (y l |x) .
The expression π θ (yw|x) π ref (yw|x) − π θ (y l |x) π ref (y l |x) calculates the difference in probabilities that the model assigns to the preferred response y w and the response y l , relative to the reference policy π ref .Intuitively, the difference indicates how much the preference alignment of π θ has improved for a given triplet (x, y w , y l ) after applying DPO compared to the reference policy π ref .β is a hyperparameter that adjusts the sensitivity to the reference policy π ref .</p>
<p>In our experiments, π ref is a SFT-tuned LLM.</p>
<p>C. Experimental Details</p>
<p>Datasets.We build "Code Alignment Benchmark" (CODAL-Bench), a new benchmark for assessing LLM alignment to coding preferences.CODAL-Bench consists of 500 randomly selected samples from CodeUltraFeedback, with a balanced representation of each preference.The benchmark aims to provide a rigorous and comprehensive framework to evaluate and compare LLM alignment to coding preferences, allowing researchers to evaluate the impact of new alignment methods.</p>
<p>For SFT, we use Magicoder Evol-Instruct [40], which consists of complex instruction-response pairs generated by GPT-4.We filter out the 10,000 samples used to build CodeUltra-Feedback to mitigate data leakage between the SFT and DPO phases and avoid potential overfitting.The process results in 100,772 samples (95% for training, 5% for model evaluation).</p>
<p>We use the remaining 9,500 samples from CodeUltra-Feedback for DPO training.From each sample, we extract binary preferences (y w , y l ), denoting the preferred and rejected response.These preferences are selected based on the highest and lowest ratings assigned by GPT-3.5-Turbo during the LLM-as-a-Judge annotation phase (see Section II-E).In instances where multiple responses have the highest/lowest ratings, we select one randomly to obtain y w / y l .</p>
<p>LLMs.We conduct our experiments on CodeLlama-7B-Instruct as it comprises 7B parameters, allowing for tuning on a modest computing infrastructure.Furthermore, the model lags behind more proficient LLMs on CodeUltraFeedback (see Table III), which makes it an ideal candidate for demonstrating the potential impact of SFT and DPO on improving LLM alignment.For comparison, we include the following LLMs: CodeLlama-13/34B-Instruct, DeepSeek-Coder-6.7B-Instruct,WizardCoder-33B, GPT-3.5-Turbo, and GPT-4-Turbo.</p>
<p>Grading Procedure.CODAL-Bench seeks to systematically compare LLM alignment, whereas CodeUltraFeedback's main purpose is to serve as a training dataset for preference tuning.Therefore, we designed a new grading procedure adapted for evaluating each LLM individually using LLM-as-a-Judge.We implement a reference-guided single-answer grading system that has proven efficient in alignment benchmarks such as MT-Bench [34].Each LLM's response is individually evaluated against a reference response, providing a consistent basis for comparison and ensuring objective judgments across different LLMs.Additionally, we instruct the judge LLM to provide a rationale alongside the rating, enhancing the consistency of the evaluations.Ratings are assigned on a scale from 1 to 10, facilitating nuanced assessments of LLM alignment.We generate reference responses using GPT-3.5-Turboand GPT-4-Turbo and use both models as judges.</p>
<p>Training Details.We use HuggingFace TRL [49] to implement SFT and DPO with QLoRA [39] for efficient fine-tuning.We use a cosine learning rate scheduler with a learning rate of 2e-04 for SFT and 5e-05 for DPO, and 10% warmup steps.We fine-tune CodeLlama-7B-Instruct for 3 and 5 epochs for SFT and DPO, respectively.All experiments were conducted on a single NVIDIA RTX A5000 GPU (24GB).We provide extensive training details in our replication package.</p>
<p>V. RESULTS</p>
<p>SFT and DPO Improve Alignment to Coding Preferences.In Fig. 3, we report the average alignment scores of the LLMs on the five coding preferences using GPT-3.5-Turboas a judge with its own responses to the instructions as references.We do not include GPT-3.5-Turbo in the results as references and responses would be identical, leading to biased and perfect alignment scores.</p>
<p>GPT-4-Turbo stands out by surpassing all other LLMs across every coding preference by a considerable margin.Interestingly, all LLMs demonstrate lower scores on the instruction following preference.This trend suggests a relative challenge in achieving high performance in this area, possibly due to the inherently less precise nature of the preference and potentially vague instructions.Conversely, LLMs tend to achieve higher scores on code explanation and readability preferences We hypothesize this trend can be attributed to the fact that all LLMs have undergone instruction tuning, which can encourage LLMs to output reasoning and explanations alongside generated code.We observe that scores substantially increase when tuning CodeLlama-7B-Instruct with SFT+DPO, highlighting the effectiveness of these methods in enhancing model alignment across all coding preferences.For instance, the model yields a 7.89 score on code complexity and efficiency preference, substantially surpassing CodeLlama-7B-Instruct (6.16) and larger models, including Wizard-33B (6.93) and CodeLlama-34B-Instruct (6.42).</p>
<p>Tuning CodeLlama-7B-Instruct with SFT shows an improvement in alignment scores compared to the base model, which are further elevated with the application of DPO independently or in conjunction with SFT.To quantify these observations, we conducted pairwise t-tests (not reported for brevety) comparing the baseline model, against its variants tuned with SFT, DPO, and SFT+DPO.Although SFT alone improves performance, we found no significant statistical difference with the baseline across preferences, except for code readability.However, we found highly significant statistical differences (p &lt; 0.001) for DPO and SFT+DPO for all preferences with the exception of instruction following.After DPO and SFT+DPO tunings, CodeLlama-7B-Instruct achieves superior alignment compared to substantially larger models such as CodeLlama-34B-Instruct and WizardCoder-33B across all coding preferences.DPO Enhances Competitive Edge Against GPT-4-Turbo.Fig. 4 illustrates the win-tie-lose ratios of the LLMs against GPT-4-Turbo for all preferences combined.</p>
<p>Non-aligned models achieve low win rates, with CodeLlama-7B-Instruct and WizardCoder-33B peaking at a 10.8% win rate.GPT-4-Turbo, on the other hand, consistently outperforms most LLMs with win rates exceeding 60%, underscoring its superiority.Tuning CodeLlama-7B-Instruct with DPO and SFT+DPO results in increases in win rates by 4.8% and 6.2%, respectively, and raises tie scores by 15.4% and 16.4%, respectively.These improvements underscore the effectiveness of alignment techniques in rendering LLM responses more competitive.Nonetheless, the remaining discernible gaps with GPT-4-Turbo suggest that there is still substantial room for improvement.</p>
<p>Different Judges and References Impact Alignment Scores.We investigate the effect of judges and references on the LLMs alignment scores in Table IV</p>
<p>GPT-4-Turbo</p>
<p>Win Tie Lose</p>
<p>Fig. 4: Win-tie-lose ratios of LLMs against GPT-4-Turbo.DPO and SFT+DPO substantially increase the percentage of won and tie matches.</p>
<p>Using GPT-3.5-Turbo as a judge, alignment scores are compared against its own (left values) and GPT-4-Turbo's responses (right values), revealing a decrease against the latter due to GPT-4-Turbo's higher response standards.Additionally, CodeLlama-7B-Instruct+SFT+DPO achieves an alignment score of 7.08, nearing GPT-3.5-Turbo'saverage of 7.18.</p>
<p>Interestingly, divergent trends emerge under GPT-4-Turbo's judgment, with all CodeLlama models scoring lower, while other LLMs see score increases.For instance, CodeLlama-13B-Instruct's score falls from 5.58 to 4.83, while WizardCoder-33B's increases from 6.26 to 6.75.Despite this, SFT, DPO, and SFT+DPO alignment techniques boost CodeLlama-7B-Instruct's performance, still exceeding the 34B variant of CodeLlama, albeit with varying impact with DPO.</p>
<p>These observations hint at GPT-4-Turbo's potential preference for certain response styles and variability in judgment compared to GPT-3.5-Turbo, underscoring the need for further investigation to fully understand the implications of judge selection on LLM alignment scores.</p>
<p>SFT and DPO Improve Functional Correctness.While our findings highlight the positive impact of SFT and DPO on aligning LLMs with coding preferences, ensuring the functional correctness of the generated code remains a pivotal concern.In Table V, we report CodeLlama-7B-Instruct's Pass@k on HumanEval [1] and HumanEval+ [7].</p>
<p>Firstly, SFT substantially enhances Pass@k on both benchmarks, with Pass@1 rising from 37.9 to 51.2 and 33.2 to 45.6 for HumanEval and HumanEval+, respectively.This result aligns with Magicoder [40] and WizardCoder [37] findings, showing that knowledge distillation through SFT substantially enhances the LLM's effectiveness on these benchmarks.Secondly, while DPO and SFT+DPO variants exhibit a slight reduction in Pass@k relative to SFT alone, they still maintain substantially higher Pass@k compared to the baseline.</p>
<p>These results show that LLM alignment through SFT and DPO also improves functional correctness, mostly due to the SFT phase, further demonstrating the usefulness of aligning LLMs.The lesser improvements in Pass@k with DPO can be attributed to DPO's learning objective, focusing on favouring preferred responses with respect to coding preferences rather than based on their functional correctness.Therefore, we underscore the need for more investigations into the design of learning objectives and methods that can prioritize both functional correctness and alignment to coding preferences.</p>
<p>VI. DISCUSSION</p>
<p>In this paper, we have demonstrated how LLM-as-a-Judge enables the creation of new datasets and benchmarks like CodeUltraFeedback and CODAL-Bench to facilitate the tuning of LLMs for better alignment with coding preferences and assess their performance.This section offers further insights from our experiments, potential improvements for LLM alignment, and the significance of exploring varied LLM-as-a-Judge configurations.Additionally, we outline some limitations of CodeUltraFeedback and CODAL-Bench.</p>
<p>A. LLM Alignment and LLM-as-a-Judge</p>
<p>Balancing Learning Objectives.A crucial area requiring deeper investigation is the balance between functional correctness and non-functional requirements.In our experiments, we demonstrate that CodeLlama-7B-Instruct+SFT+DPO shows higher performance on HumanEval/HumanEval+ compared to the base model.Understanding how to integrate functional and non-functional learning objectives could significantly advance the tuning process of LLMs and further enhance their performance across a broad range of benchmarks.</p>
<p>CodeUltraFeedback for Critic LLM Training.In this work, we demonstrate the utility of CodeUltraFeedback for LLM preference tuning using RLAIF and DPO.One of the drawbacks of CODAL-Bench is the need to leverage models like GPT-4 to evaluate other LLMs, which might turn out to be cost-prohibitive.Nonetheless, we believe the AI feedback data in CodeUltraFeedback, e.g., ratings and rationales, can be leveraged to fine-tune a small critic LLM trained to evaluate other LLMs.Prior work including Shepherd [50] and Prometheus [51] have been proposed around that idea and showed promising results.</p>
<p>Parallel Findings with Zephyr Our findings draw parallels with Zephyr 's [38] insights on DPO, with models beginning to overfit after a few epochs, achieving 100% training set accuracy.In our case, overfitting after five DPO training epochs correlated with improved performance.Although the model overfits, the gap between chosen and rejected rewards continued to grow, suggesting that monitoring reward accuracy might not be relevant for model selection.Therefore, more Potential Judgment Biases.We highlight GPT-3.5-Turbo'simpartiality and robustness in judgment in both our dataset and benchmark.The performance ranking of LLMs remained consistent, underscoring the reliability of GPT-3.5-Turbo'sevaluations.Despite this, there remains the possibility of inherent biases in LLM judgments, such as a preference for lengthy or verbose responses.The grading procedures employed for CodeUltraFeedback and CODAL-Bench were carefully designed, drawing on established practices to preclude such biases [34], [52].Single-answer grading with a reference response to the instruction and chain-of-thought prompting enables both GPT-3.5-Turbo and GPT-4-Turbo to produce consistent judgements.However, further exploration into the potential biases influencing LLM judgments is warranted.</p>
<p>Exploring Alternative Judges.Future studies could benefit from incorporating a wider range of judges to understand how different LLMs' evaluative perspectives might influence the judgment process and outcomes.Additionally, the reliance on closed-source models can be mitigated by tuning small and open-source critic LLMs.</p>
<p>B. Limitations</p>
<p>Our methodology presupposes the existence of a highly capable model, such as GPT-3.5 or GPT-4, as a proxy to humans to accurately evaluate other LLMs.This reliance on these models is based on previous findings indicating that GPT-3.5 and GPT-4 align closely with human judgments [34], [52], achieving agreement rates on par with those between humans themselves on MT-Bench [34].Such evidence supports their utilization in an LLM-as-a-Judge framework.Although our work does not include a comparison between GPT-3.5/4 and human judgements, we hypothesize LLM judgments could match human judgments on CODAL-Bench and leave the comparison for future work.</p>
<p>Our current iteration of CODAL-Bench relies on randomly selected samples.Future versions aim to employ a more rigorous filtering process, possibly incorporating human annotations, to enhance the representativeness and quality of selected samples.</p>
<p>Lastly, our grading procedure in CODAL-Bench necessitates reference responses for judgment.This usage of reference responses gives the judge LLMs a consistent point of comparison, enabling fair and consistent judgements across LLMs outputs.Future work might explore ways to reduce this dependency.</p>
<p>VII. RELATED WORK</p>
<p>We explore and differentiate our work from the existing landscape of datasets and benchmarks in code intelligence, with a particular focus on execution-based benchmarks, closed-solutions datasets, and benchmarks, as well as those assessing non-functional aspects of generated code.</p>
<p>A. Execution-based Benchmarks</p>
<p>This category of benchmarks emphasizes the functional correctness of generated code by evaluating whether it passes a set of unit tests.Execution-based benchmarks include Hu-manEval [1], MBPP [24], APPS [23], and DS-1000 [53], which have extensively been used to compare LLMs in code generation scenarios [2], [6], [37], [40].Subsequent work has expanded HumanEval and MBPP benchmarks to multiple programming languages [8], [10], [54], more unit tests [7], and more tasks including code summarization [8], code repair [6], code explanation [6], and code translation [10].This category also encompasses benchmarks designed around specific use cases, such as Odex [55], which deals with Python opendomain problems and includes manual annotations of user intents, and StudentEval [56], which utilizes student-defined prompts for Python code generation.xCodeEval [25] benchmark extends the scope to code understanding, generation, and retrieval across numerous tasks.Lastly, benchmarks like ClassEval [57] and CoderEval [58] shift the focus towards evaluating LLMs' proficiency in generating functionally correct code across various programming abstractions, such as class, file, and project levels.</p>
<p>Our work differs from this category of benchmarks and focuses on aligning LLMs to coding preferences and measuring their alignment using an LLM as a proxy for human evaluation without relying on unit tests or automated metrics.Nonetheless, we believe evaluating the functional correctness of LLMgenerated code remains a pivotal concern complementary to the aim of CodeUltraFeedback and CODAL-Bench.</p>
<p>B. General Datasets and Closed-Solutions Benchmarks</p>
<p>Initial datasets like GitHub Java Corpus [59], Py150/Js150 [60], [61], and CodeSearchNet [62] laid the groundwork for evaluating language models in coderelated tasks, including modelling, summarization, and search.Subsequent developments introduced benchmarks like CodeXGlue [15], CodeNet [16], XLCoST [17], ReCode [63], and CrossCodeBench [18], each expanding the evaluation scope to include code understanding, generation, and robustness against perturbations across various coding tasks and languages.Recently, CrossCodeEval [64] broadened this scope by assessing generative capabilities and the use of cross-file information for project-wide coding tasks.</p>
<p>These datasets and benchmarks evaluate language models' core capabilities, mainly through task-specific transfer learning, relying on ground truth solutions which may overlook valid code variations.In contrast, CodeUltraFeedback and CODAL-Bench focus on aligning LLMs with human coding preferences.Additionally, the LLM-as-a-Judge approach serves as an alternative to automated evaluations, prioritizing nuanced assessment of natural and programming languages over strict adherence to ground truth.</p>
<p>C. Non-Functional Evaluation of LLM-Generated Code</p>
<p>Recent advancements have expanded LLM evaluation to include non-functional requirements [65], an aspect overlooked in earlier studies.Examples include efforts to assess the quality of AI-generated code [26], [66] by using static analysis tools.In contrast, our work adopts a broader strategy, employing LLMs' advanced reasoning to tune and assess their alignment with human coding preferences.Furthermore, Yetis ¸tiren et al. [28] evaluated LLM-generated code using quality metrics such as security, reliability, and maintainability on the HumanEval benchmark.Our work encompasses a broader evaluation scope, including more complex instructions, and CodeUltraFeedback to tune and align LLMs to coding preferences.In contrast to CyberSecEval [67] and EffiBench [27], which concentrate on particular code aspects like security and efficiency, our methodology based on LLM-as-a-Judge provides a holistic evaluation across several dimensions, offering a framework that can be adapted to assess an array of coding preferences.NoFunEval [29] stands as the most closely related benchmark to our work, evaluating LLM-generated code's non-functional properties across 958 coding problems using functional specifications and static analysis tools.In contrast, we leverage LLMs' evaluative capabilities for LLM evaluation, overcoming static analysis tools' limitations in capturing language nuances.Additionally, we introduce a comprehensive dataset for tuning LLM preferences, distinguishing our work as a unique contribution to the field of code LLM evaluation.</p>
<p>VIII. CONCLUSION AND FUTURE WORK</p>
<p>In this paper, we introduce CodeUltraFeedback and CODAL-Bench, a preference dataset and benchmark of complex instructions for LLM alignment to five coding preferences.Our analysis reveals significant alignment disparities among various LLMs compared to GPT-3.5 and GPT-4.Furthermore, we demonstrate how CodeUltraFeedback facilitates preference tuning using SFT, RLAIF and DPO.Our experiments conclude that CodeLlama-7B-Instruct tuned with SFT and DPO outperforms 34B LLMs on CODAL-Bench and enhances functional correctness on HumanEval+.We hope</p>
<p>111k complex coding problems 5
5
coding preferences 50 principles to guide LLMs generation 10k instructions random subsampling 2k instructions/preference LLMs Selection Pool of 14 LLMs from 8 model families 4 random LLMs/instruction 1.The assistant must ensure that its code comes with clear ... 2. It's important for the assistant to deliver comprehensive ... ...</p>
<p>1 . 1 . 1 . 1 . 4 Fig. 1 :
111141
Fig. 1: Overview of CodeUltraFeedback dataset construction procedure.(i) Coding Preferences and Principles: we define five coding preferences: Instruction-following, Code Explanation, Code Complexity and Efficiency, Code Readability, and Coding Style, and 10 principles for each preference to guide LLMs' generative process.(ii) Magicoder Evol-Instruct Dataset:CodeUltraFeedback is based on a 10k-instruction subset of Magicoder Evol-Instruct dataset[37],[40], where each instruction is tagged with a coding preference.(iii) LLMs Selection: for each instruction, four LLMs are randomly sampled from a diverse pool of 14 LLMs.(iv) Principle-Driven Code Generation: one principle is randomly selected per LLM to guide and align the code generation process with the assigned coding preference.(v) LLM-as-a-Judge Annotation: LLM-as-a-Judge with GPT-3.5-Turbo is used to judge LLMs' responses according to coding preferences evaluation criteria (see TableI).</p>
<p>Fig. 2 :
2
Fig.2: Win-tie-lose ratios of LLMs against GPT-3.5-Turbo.An LLM wins/loses if it gets a greater/lower score than GPT-3.5-Turbo.A tie is when the LLM and GPT-3.5-Turboget identical scores.</p>
<p>Fig. 3 :
3
Fig.3: Average alignment scores for LLMs across coding preferences on CODAL-Bench, evaluated using GPT-3.5-Turboas a judge with reference-guided single-answer grading.</p>
<p>TABLE I :
I
Code readability assessment template.</p>
<p>.
5.117.066.176.866.526.345.197.096.427.066.676.495.317.156.897.336.836.705.747.006.937.626.956.857.338.358.588.718.428.284.866.306.166.426.276.005.336.616.557.316.766.515.257.597.607.937.377.155.327.837.897.997.787.36</p>
<p>TABLE IV :
IV
Average alignment scores of LLMs on CODAL-Bench.G-3.5 and G-4 refer to utilizing GPT-3.5-Turbo and GPT-4-Turbo responses as references, respectively.
JudgeGPT-3.5-TURBO GPT-4-TURBOReference: G-3.5G-4G-4CodeLlama-13B-Instruct6.345.584.83CodeLlama-34B-Instruct6.495.845.36DeepSeek-Coder-6.7B-Instruct6.706.076.32WizardCoder-33B6.856.266.75GPT-3.5-Turbo-7.187.35GPT-4-Turbo8.28--CodeLlama-7B-Instruct6.005.464.72+SFT6.515.835.84+DPO7.156.795.08+SFT+DPO7.367.085.85</p>
<p>TABLE V :
V
Pass@k of CodeLlama-7B-Instruct variants on HumanEval and HumanEval+.
HumanEval HumanEval+k=1 k=10 k=1 k=10CodeLlama-7B-Instruct 37.9 60.4 33.2 54.9+SFT51.2 82.9 45.6 79.3+DPO42.3 80.5 35.8 70.1+SFT+DPO43.1 75.6 36.7 69.5experimentation on DPO could illuminate better model tuningand selection approaches for improved model alignment.
In the rest of this paper, we use the term "coding preferences" instead of "non-functional requirements" to more precisely capture the process of instructing LLMs with explicit preferences, although both terms have similar meanings.
CodeUltraFeedback and CODAL-Bench can further support research related to LLM alignment in code intelligence.In future work, we plan to explore more judges to assess LLMs on CODAL-Bench and use CodeUltraFeedback to fine-tune our own judge LLM, thereby reducing our dependence on closed-source models like GPT-4.Code Explanation Code Complexity and Efficiency Code Readability Coding Style Code Explanation Code Complexity &amp; Efficiency Code Readability Instruction Following Code Explanation Code Complexity &amp; Efficiency Code Readability
Evaluating large language models trained on code. M Chen, 2021</p>
<p>Code llama: Open foundation models for code. B Roziere, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Y Adi, J Liu, T Remez, J Rapin, arXiv:2308.129502023arXiv preprint</p>
<p>Deepseek-coder: When the large language model meets programming-the rise of code intelligence. D Guo, Q Zhu, D Yang, Z Xie, K Dong, W Zhang, G Chen, X Bi, Y Wu, Y Li, arXiv:2401.141962024arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Octopack: Instruction tuning code large language models. N Muennighoff, Q Liu, A Zebaze, Q Zheng, B Hui, T Y Zhuo, S Singh, X Tang, L Von, S Werra, Longpre, arXiv:2308.071242023arXiv preprint</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. J Liu, C S Xia, Y Wang, L Zhang, arXiv:2305.012102023arXiv preprint</p>
<p>Multi-lingual evaluation of code generation models. B Athiwaratkun, S K Gouda, Z Wang, X Li, Y Tian, M Tan, W U Ahmad, S Wang, Q Sun, M Shang, arXiv:2210.148682022arXiv preprint</p>
<p>Exploring parameter-efficient fine-tuning techniques for code generation with large language models. M Weyssow, X Zhou, K Kim, D Lo, H Sahraoui, arXiv:2308.104622023arXiv preprint</p>
<p>Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. Q Zheng, X Xia, X Zou, Y Dong, S Wang, Y Xue, Z Wang, L Shen, A Wang, Y Li, arXiv:2303.175682023arXiv preprint</p>
<p>On the evaluation of neural code translation: Taxonomy and benchmark. M Jiao, T Yu, X Li, G Qiu, X Gu, B Shen, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2023</p>
<p>Understanding the effectiveness of large language models in code translation. R Pan, A R Ibrahimzada, R Krishna, D Sankar, L P Wassi, M Merler, B Sobolev, R Pavuluri, S Sinha, R Jabbarvand, arXiv:2308.031092023arXiv preprint</p>
<p>Repairllama: Efficient representations and fine-tuned adapters for program repair. A Silva, S Fang, M Monperrus, arXiv:2312.156982023arXiv preprint</p>
<p>A comprehensive study of automatic program repair on the quixbugs benchmark. H Ye, M Martinez, T Durieux, M Monperrus, Journal of Systems and Software. 1711108252021</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. S Lu, D Guo, S Ren, J Huang, A Svyatkovskiy, A Blanco, C Clement, D Drain, D Jiang, D Tang, arXiv:2102.046642021arXiv preprint</p>
<p>Codenet: A largescale ai for code dataset for learning a diversity of coding tasks. R Puri, D S Kung, G Janssen, W Zhang, G Domeniconi, V Zolotov, J Dolby, J Chen, M Choudhury, L Decker, arXiv:2105.126552021arXiv preprint</p>
<p>Xlcost: A benchmark dataset for cross-lingual code intelligence. M Zhu, A Jain, K Suresh, R Ravindran, S Tipirneni, C K Reddy, arXiv:2206.084742022arXiv preprint</p>
<p>Crosscodebench: Benchmarking cross-task generalization of source code models. C Niu, C Li, V Ng, B Luo, arXiv:2302.040302023arXiv preprint</p>
<p>Generationbased code review automation: How far are we. X Zhou, K Kim, B Xu, D Han, J He, D Lo, arXiv:2303.072212023arXiv preprint</p>
<p>Improving the learning of code review successive tasks with cross-task knowledge distillation. O B Sghaier, H Sahraoui, arXiv:2402.020632024arXiv preprint</p>
<p>Unity is strength: Crosstask knowledge distillation to improve code review generation. O B Sghaier, L Maes, H Sahraoui, arXiv:2309.033622023arXiv preprint</p>
<p>A multi-step learning approach to assist code review. O B Sghaier, H Sahraoui, 2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE2023</p>
<p>Measuring coding challenge competence with apps. D Hendrycks, S Basart, S Kadavath, M Mazeika, A Arora, E Guo, C Burns, S Puranik, H He, D Song, J Steinhardt, NeurIPS. 2021</p>
<p>Program synthesis with large language models. J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, arXiv:2108.077322021arXiv preprint</p>
<p>xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. M A M Khan, M S Bari, X L Do, W Wang, M R Parvez, S Joty, arXiv:2303.030042023arXiv preprint</p>
<p>A lightweight framework for high-quality code generation. M L Siddiq, B Casey, J Santos, arXiv:2307.082202023arXiv preprint</p>
<p>Effibench: Benchmarking the efficiency of automatically generated code. D Huang, J M Zhang, Y Qing, H Cui, arXiv:2402.020372024arXiv preprint</p>
<p>Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt. B Yetis ¸tiren, I Özsoy, M Ayerdem, E Tüzün, arXiv:2304.107782023arXiv preprint</p>
<p>Nofuneval: Funny how code lms falter on requirements beyond functional correctness. M Singhal, T Aggarwal, A Awasthi, N Natarajan, A Kanade, arXiv:2401.159632024arXiv preprint</p>
<p>Ultrafeedback: Boosting language models with high-quality feedback. G Cui, L Yuan, N Ding, G Yao, W Zhu, Y Ni, G Xie, Z Liu, M Sun, arXiv:2310.013772023arXiv preprint</p>
<p>Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Rlaif: Scaling reinforcement learning from human feedback with ai feedback. H Lee, S Phatale, H Mansoor, K Lu, T Mesnard, C Bishop, V Carbune, A Rastogi, arXiv:2309.002672023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, 2022. 202213</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, arXiv:2306.056852023arXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. X Li, T Zhang, Y Dubois, R Taori, I Gulrajani, C Guestrin, P Liang, T B Hashimoto, 2023</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, S Ermon, C D Manning, C Finn, arXiv:2305.182902023arXiv preprint</p>
<p>Wizardcoder: Empowering code large language models with evol-instruct. Z Luo, C Xu, P Zhao, Q Sun, X Geng, W Hu, C Tao, J Ma, Q Lin, D Jiang, arXiv:2306.085682023arXiv preprint</p>
<p>Zephyr: Direct distillation of lm alignment. L Tunstall, E Beeching, N Lambert, N Rajani, K Rasul, Y Belkada, S Huang, L Werra, C Fourrier, N Habib, arXiv:2310.169442023arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, arXiv:2305.143142023arXiv preprint</p>
<p>Magicoder: Source code is all you need. Y Wei, Z Wang, J Liu, Y Ding, L Zhang, arXiv:2312.021202023arXiv preprint</p>
<p>Principle-driven self-alignment of language models from scratch with minimal human supervision. Z Sun, Y Shen, Q Zhou, H Zhang, Z Chen, D Cox, Y Yang, C Gan, arXiv:2305.030472023arXiv preprint</p>
<p>Orca: Progressive learning from complex explanation traces of gpt-4. S Mukherjee, A Mitra, G Jawahar, S Agarwal, H Palangi, A Awadallah, arXiv:2306.027072023arXiv preprint</p>
<p>Wizardlm: Empowering large language models to follow complex instructions. C Xu, Q Sun, K Zheng, X Geng, P Zhao, J Feng, C Tao, D Jiang, arXiv:2304.122442023arXiv preprint</p>
<p>Principled instructions are all you need for questioning llama-1/2, gpt-3.5/4. S M Bsharat, A Myrzakhan, Z Shen, arXiv:2312.161712023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Self-instruct: Aligning language model with self generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Alpaca: A strong, replicable instructionfollowing model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Stanford Center for Research on Foundation Models. 32023</p>
<p>A survey on knowledge distillation of large language models. X Xu, M Li, C Tao, T Shen, R Cheng, J Li, C Xu, D Tao, T Zhou, 2024</p>
<p>Trl: Transformer reinforcement learning. . W Leandro, B Younes, T Lewis, B Edward, T Tristan, L Nathan, H Shengyi, 2020</p>
<p>Shepherd: A critic for language model generation. T Wang, P Yu, X E Tan, S O'brien, R Pasunuru, J Dwivedi-Yu, O Golovneva, L Zettlemoyer, M Fazel-Zarandi, A Celikyilmaz, arXiv:2308.045922023arXiv preprint</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. S Kim, J Shin, Y Cho, J Jang, S Longpre, H Lee, S Yun, S Shin, S Kim, J Thorne, arXiv:2310.084912023arXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. X Li, T Zhang, Y Dubois, R Taori, I Gulrajani, C Guestrin, P Liang, T B Hashimoto, GitHub repository. 2023</p>
<p>Ds-1000: A natural and reliable benchmark for data science code generation. Y Lai, C Li, Y Wang, T Zhang, R Zhong, L Zettlemoyer, W -T. Yih, D Fried, S Wang, T Yu, International Conference on Machine Learning. PMLR202318345</p>
<p>Multipl-e: a scalable and polyglot approach to benchmarking neural code generation. F Cassano, J Gouwar, D Nguyen, S Nguyen, L Phipps-Costin, D Pinckney, M.-H Yee, Y Zi, C J Anderson, M Q Feldman, IEEE Transactions on Software Engineering. 2023</p>
<p>Docprompting: Generating code by retrieving the docs. S Zhou, U Alon, F F Xu, Z Jiang, G Neubig, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Studenteval: A benchmark of student-written prompts for large language models of code. H M Babe, S Nguyen, Y Zi, A Guha, M Q Feldman, C J Anderson, 2023</p>
<p>Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation. X Du, M Liu, K Wang, H Wang, J Liu, Y Chen, J Feng, C Sha, X Peng, Y Lou, arXiv:2308.018612023arXiv preprint</p>
<p>Codereval: A benchmark of pragmatic code generation with generative pre-trained models. H Yu, B Shen, D Ran, J Zhang, Q Zhang, Y Ma, G Liang, Y Li, Q Wang, T Xie, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software Engineering2024</p>
<p>Mining source code repositories at massive scale using language modeling. M Allamanis, C Sutton, 2013 10th working conference on mining software repositories (MSR). IEEE2013</p>
<p>Probabilistic model for code with decision trees. V Raychev, P Bielik, M Vechev, ACM SIGPLAN Notices. 51102016</p>
<p>Learning programs from noisy data. V Raychev, P Bielik, M Vechev, A Krause, ACM Sigplan Notices. 5112016</p>
<p>Codesearchnet challenge: Evaluating the state of semantic code search. H Husain, H.-H Wu, T Gazit, M Allamanis, M Brockschmidt, arXiv:1909.094362019arXiv preprint</p>
<p>Recode: Robustness evaluation of code generation models. S Wang, Z Li, H Qian, C Yang, Z Wang, M Shang, V Kumar, S Tan, B Ray, P Bhatia, arXiv:2212.102642022arXiv preprint</p>
<p>Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. Y Ding, Z Wang, W U Ahmad, H Ding, M Tan, N Jain, M K Ramanathan, R Nallapati, P Bhatia, D Roth, arXiv:2310.112482023arXiv preprint</p>
<p>Robustness, security, privacy, explainability, efficiency, and usability of large language models for code. Z Yang, Z Sun, T Z Yue, P Devanbu, D Lo, 2024</p>
<p>Refining chatgpt-generated code: Characterizing and mitigating code quality issues. Y Liu, T Le-Cong, R Widyasari, C Tantithamthavorn, L Li, X.-B D Le, D Lo, ACM Transactions on Software Engineering and Methodology. 2023</p>
<p>Purple llama cyberseceval: A secure coding benchmark for language models. M Bhatt, S Chennabasappa, C Nikolaidis, S Wan, I Evtimov, D Gabi, D Song, F Ahmad, C Aschermann, L Fontana, arXiv:2312.047242023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
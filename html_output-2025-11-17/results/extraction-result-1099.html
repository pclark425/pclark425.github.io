<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1099 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1099</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1099</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-274777202</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.10425v3.pdf" target="_blank">Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian Thermodynamic Approach to Adaptation</a></p>
                <p><strong>Paper Abstract:</strong> This paper introduces a novel approach to creating adaptive language agents by integrating active inference with large language models (LLMs). While LLMs demonstrate remarkable capabilities, their reliance on static prompts limits adaptation to new information and changing environments. We address this by implementing an active inference framework that acts as a cognitive layer above an LLM-based agent, dynamically adjusting prompts and search strategies through principled information-seeking behavior. Our framework models the environment using three state factors (prompt, search, and information states) with seven observation modalities capturing quality metrics. By framing the agent's learning through the free energy principle, we enable systematic exploration of prompt combinations and search strategies. Experimental results demonstrate the effectiveness of this approach, with the agent developing accurate models of environment dynamics evidenced by emergent structure in observation matrices. Action selection patterns reveal sophisticated exploration-exploitation behavior, transitioning from initial information-gathering to targeted prompt testing. The integration of thermodynamic principles with language model capabilities provides a principled framework for creating robust, adaptable agents, extending active inference beyond traditional low-dimensional control problems to high-dimensional, language-driven environments.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1099.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1099.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-AIF-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Inference for Self-Organizing Multi-LLM Systems agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active-inference cognitive layer placed above a multi-LLM research agent that dynamically adapts prompts and search strategies by minimizing expected variational free energy, learning state–observation mappings and balancing information gain with pragmatic value under thermodynamic-inspired costs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Active Inference Multi-LLM Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A hierarchical agent combining an active inference generative model (three discrete state factors: prompt, search, information) with multiple LLMs as the research/execution backend. Key components: A matrices (observation likelihoods across 7 modalities), B matrices (transitions for prompt/search/info), C preference vector, D initial priors, Dirichlet learning (pA, pB), EFE-based policy selection (softmax with precision γ), and an external evaluator (GPT-4o-mini) that supplies standardized quality observations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information gain maximization via active inference (expected free energy minimization)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent computes Expected Free Energy (EFE) for candidate policies over a 2-step policy horizon and selects policies via a softmax posterior (precision γ). It adapts by updating posterior state beliefs from observations, learning A and B parameters via Dirichlet updates (outer-product updates with learning rate η), and using both state-information gain and parameter-information gain to prefer actions that reduce uncertainty (search actions early) or increase pragmatic value (prompt actions later). Observations comprising prompt quality and search quality metrics drive belief updates and consequent action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Custom research-agent environment (language-driven POMDP)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable Markov decision process (POMDP); discrete state factors (prompt, search, information); initially unknown/stochastic observation mappings (uniform priors); seven observation modalities giving graded quality metrics; external evaluator-dependent observations; information states representing hidden knowledge levels rather than raw environment state.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Discrete, moderately high-dimensional: prompt state space = 33 states, search state space = 11 states, information state space = 3 states; observation resolution = 11 graded levels per modality; action set: no-action, prompt-only (33 possible actions), search-only (11 possible actions); policy horizon = 2 steps, inference horizon = 1 step. Episodes/interaction length not explicitly fixed; experiments run over many timesteps (figures show >40 timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative: agent learned structured A matrices from initial uniform priors, developed accurate models of state→observation mappings, and exhibited an emergent exploration→exploitation transition (heavy search actions for ~first 40 timesteps followed by prompt-focused exploitation). No numeric performance metrics (e.g., reward, accuracy percentages) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not numerically quantified. Empirical description: the agent shows a clear exploration phase (~first 40 timesteps) followed by exploitation; A/B matrices developed discernible structure after 'multiple interactions' but no sample counts to convergence are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Managed by minimizing Expected Free Energy combining information gain (epistemic value) and pragmatic value (preferred observations). Mechanisms: inclusion of state- and parameter-information gain in EFE, softmax policy posterior with precision γ (8.0) controlling exploration vs exploitation, deterministic action selection enabled, and learning-rate-driven parameter updates (η=50.0). The agent naturally favors search (epistemic) early and shifts to prompt testing (pragmatic exploitation) as information states and search-quality observations improve.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>No experimental baselines or quantitative comparisons reported. Related works are discussed (e.g., WEBRL, Voyager, Odyssey, Re-ReST, SELF) but not used as experimental baselines in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) The active inference agent successfully learned structured observation likelihoods (A matrices) from initially uniform priors, revealing which prompt and search choices produce higher-quality outputs. 2) The EFE landscape evolved to clearly separate effective from ineffective policies, enabling informed policy selection. 3) A clear temporal strategy emerged: an early exploration (search-dominated) phase to reduce uncertainty, followed by a prompt-focused exploitation phase once information states and search-quality metrics indicated sufficient knowledge. 4) The framework demonstrates that thermodynamically-motivated free energy minimization can drive adaptive experimental design in a high-dimensional, language-driven environment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) No quantitative performance metrics or baselines provided (no numerical success rates, convergence times, or sample-efficiency figures). 2) The state/action spaces are fixed and discrete (33 prompts, 11 searches), limiting open-ended discovery; the paper explicitly notes the lack of dynamic state space expansion as a key limitation. 3) Reliance on an external LLM evaluator (GPT-4o-mini) for observations could bias or constrain learning; dependency on structured JSON outputs may limit transfer to noisier real-world signals. 4) Deterministic action selection and very large learning rate (η=50.0) are design choices whose robustness is not analyzed. 5) Not evaluated against external benchmarks or more complex real-world language tasks; generalization unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian Thermodynamic Approach to Adaptation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active inference: The free energy principle in mind, brain, and behavior <em>(Rating: 2)</em></li>
                <li>The free energy principle for action and perception: A mathematical review <em>(Rating: 2)</em></li>
                <li>Active inference, belief propagation, and the free energy principle <em>(Rating: 2)</em></li>
                <li>Active inference, curiosity and insight <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 1)</em></li>
                <li>WEBRL: Training LLM web agents via self-evolving online curriculum reinforcement learning <em>(Rating: 1)</em></li>
                <li>Generative AI for self-adaptive systems: State of the art and research roadmap <em>(Rating: 1)</em></li>
                <li>Re-ReST: Reflection-reinforced self-training for language agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1099",
    "paper_id": "paper-274777202",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "AI-AIF-LLM",
            "name_full": "Active Inference for Self-Organizing Multi-LLM Systems agent",
            "brief_description": "An active-inference cognitive layer placed above a multi-LLM research agent that dynamically adapts prompts and search strategies by minimizing expected variational free energy, learning state–observation mappings and balancing information gain with pragmatic value under thermodynamic-inspired costs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Active Inference Multi-LLM Agent",
            "agent_description": "A hierarchical agent combining an active inference generative model (three discrete state factors: prompt, search, information) with multiple LLMs as the research/execution backend. Key components: A matrices (observation likelihoods across 7 modalities), B matrices (transitions for prompt/search/info), C preference vector, D initial priors, Dirichlet learning (pA, pB), EFE-based policy selection (softmax with precision γ), and an external evaluator (GPT-4o-mini) that supplies standardized quality observations.",
            "adaptive_design_method": "information gain maximization via active inference (expected free energy minimization)",
            "adaptation_strategy_description": "The agent computes Expected Free Energy (EFE) for candidate policies over a 2-step policy horizon and selects policies via a softmax posterior (precision γ). It adapts by updating posterior state beliefs from observations, learning A and B parameters via Dirichlet updates (outer-product updates with learning rate η), and using both state-information gain and parameter-information gain to prefer actions that reduce uncertainty (search actions early) or increase pragmatic value (prompt actions later). Observations comprising prompt quality and search quality metrics drive belief updates and consequent action selection.",
            "environment_name": "Custom research-agent environment (language-driven POMDP)",
            "environment_characteristics": "Partially observable Markov decision process (POMDP); discrete state factors (prompt, search, information); initially unknown/stochastic observation mappings (uniform priors); seven observation modalities giving graded quality metrics; external evaluator-dependent observations; information states representing hidden knowledge levels rather than raw environment state.",
            "environment_complexity": "Discrete, moderately high-dimensional: prompt state space = 33 states, search state space = 11 states, information state space = 3 states; observation resolution = 11 graded levels per modality; action set: no-action, prompt-only (33 possible actions), search-only (11 possible actions); policy horizon = 2 steps, inference horizon = 1 step. Episodes/interaction length not explicitly fixed; experiments run over many timesteps (figures show &gt;40 timesteps).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative: agent learned structured A matrices from initial uniform priors, developed accurate models of state→observation mappings, and exhibited an emergent exploration→exploitation transition (heavy search actions for ~first 40 timesteps followed by prompt-focused exploitation). No numeric performance metrics (e.g., reward, accuracy percentages) reported.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Not numerically quantified. Empirical description: the agent shows a clear exploration phase (~first 40 timesteps) followed by exploitation; A/B matrices developed discernible structure after 'multiple interactions' but no sample counts to convergence are provided.",
            "exploration_exploitation_tradeoff": "Managed by minimizing Expected Free Energy combining information gain (epistemic value) and pragmatic value (preferred observations). Mechanisms: inclusion of state- and parameter-information gain in EFE, softmax policy posterior with precision γ (8.0) controlling exploration vs exploitation, deterministic action selection enabled, and learning-rate-driven parameter updates (η=50.0). The agent naturally favors search (epistemic) early and shifts to prompt testing (pragmatic exploitation) as information states and search-quality observations improve.",
            "comparison_methods": "No experimental baselines or quantitative comparisons reported. Related works are discussed (e.g., WEBRL, Voyager, Odyssey, Re-ReST, SELF) but not used as experimental baselines in this study.",
            "key_results": "1) The active inference agent successfully learned structured observation likelihoods (A matrices) from initially uniform priors, revealing which prompt and search choices produce higher-quality outputs. 2) The EFE landscape evolved to clearly separate effective from ineffective policies, enabling informed policy selection. 3) A clear temporal strategy emerged: an early exploration (search-dominated) phase to reduce uncertainty, followed by a prompt-focused exploitation phase once information states and search-quality metrics indicated sufficient knowledge. 4) The framework demonstrates that thermodynamically-motivated free energy minimization can drive adaptive experimental design in a high-dimensional, language-driven environment.",
            "limitations_or_failures": "1) No quantitative performance metrics or baselines provided (no numerical success rates, convergence times, or sample-efficiency figures). 2) The state/action spaces are fixed and discrete (33 prompts, 11 searches), limiting open-ended discovery; the paper explicitly notes the lack of dynamic state space expansion as a key limitation. 3) Reliance on an external LLM evaluator (GPT-4o-mini) for observations could bias or constrain learning; dependency on structured JSON outputs may limit transfer to noisier real-world signals. 4) Deterministic action selection and very large learning rate (η=50.0) are design choices whose robustness is not analyzed. 5) Not evaluated against external benchmarks or more complex real-world language tasks; generalization unknown.",
            "uuid": "e1099.0",
            "source_info": {
                "paper_title": "Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian Thermodynamic Approach to Adaptation",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active inference: The free energy principle in mind, brain, and behavior",
            "rating": 2,
            "sanitized_title": "active_inference_the_free_energy_principle_in_mind_brain_and_behavior"
        },
        {
            "paper_title": "The free energy principle for action and perception: A mathematical review",
            "rating": 2,
            "sanitized_title": "the_free_energy_principle_for_action_and_perception_a_mathematical_review"
        },
        {
            "paper_title": "Active inference, belief propagation, and the free energy principle",
            "rating": 2,
            "sanitized_title": "active_inference_belief_propagation_and_the_free_energy_principle"
        },
        {
            "paper_title": "Active inference, curiosity and insight",
            "rating": 2,
            "sanitized_title": "active_inference_curiosity_and_insight"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 1,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "WEBRL: Training LLM web agents via self-evolving online curriculum reinforcement learning",
            "rating": 1,
            "sanitized_title": "webrl_training_llm_web_agents_via_selfevolving_online_curriculum_reinforcement_learning"
        },
        {
            "paper_title": "Generative AI for self-adaptive systems: State of the art and research roadmap",
            "rating": 1,
            "sanitized_title": "generative_ai_for_selfadaptive_systems_state_of_the_art_and_research_roadmap"
        },
        {
            "paper_title": "Re-ReST: Reflection-reinforced self-training for language agents",
            "rating": 1,
            "sanitized_title": "rerest_reflectionreinforced_selftraining_for_language_agents"
        }
    ],
    "cost": 0.00745,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian Thermodynamic Approach to Adaptation
November 2024</p>
<p>Rithvik Prakki rprakki@unc.edu 
Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian Thermodynamic Approach to Adaptation
November 20246E76E10AAE04011435F4129925D41D3EarXiv:2412.10425v3[cs.CL]
This paper introduces a novel approach to creating adaptive language agents by integrating active inference with large language models (LLMs).While LLMs demonstrate remarkable capabilities, their reliance on static prompts limits adaptation to new information and changing environments.We address this by implementing an active inference framework that acts as a cognitive layer above an LLM-based agent, dynamically adjusting prompts and search strategies through principled informationseeking behavior.Our framework models the environment using three state factors (prompt, search, and information states) with seven observation modalities capturing quality metrics.By framing the agent's learning through the free energy principle, we enable systematic exploration of prompt combinations and search strategies.Experimental results demonstrate the effectiveness of this approach, with the agent developing accurate models of environment dynamics evidenced by emergent structure in observation matrices.Action selection patterns reveal sophisticated exploration-exploitation behavior, transitioning from initial information-gathering to targeted prompt testing.The integration of thermodynamic principles with language model capabilities provides a principled framework for creating robust, adaptable agents, extending active inference beyond traditional low-dimensional control problems to high-dimensional, language-driven environments.</p>
<p>Introduction</p>
<p>Recent advancements in artificial intelligence have witnessed the emergence of large language models (LLMs) that demonstrate remarkable capabilities in natural language understanding and generation.These models have been instrumental in various applications, ranging from chatbots to complex problem-solving agents.However, a significant limitation of current LLM-based systems is their reliance on static prompts, which do not adapt dynamically to new information or changing environments.This rigidity hampers the ability of AI agents to perform self-improvement and adapt their interactions based on past experiences.</p>
<p>Active inference, grounded in the Free Energy Principle (FEP), offers a promising framework for modeling adaptive and autonomous behavior in cognitive agents.The FEP implies a classical thermodynamics through its foundation in Bayesian mechanics, where belief updating incurs specific thermodynamic costs (Fields et al., 2023).Just as biological systems must balance the thermodynamic free energy required for metabolic maintenance, cognitive systems can be understood as minimizing their variational free energya mathematical construct that bounds the entropy of their sensory exchanges with the environment.By treating perception and action as processes aimed at minimizing this variational free energy, active inference agents can update their beliefs and make decisions that optimize their interactions with the environment while managing the inherent costs of information processing.</p>
<p>In this paper, we introduce a novel approach that integrates an active inference generative model as a cognitive layer atop a research agent powered by multiple LLMs.Our active inference agent acts as a 'brain' that dynamically adjusts the prompts provided to the LLMs, facilitating a learning process that evolves with each interaction.Through the lens of the FEP, the agent maintains state factors for prompts, search terms, and information, allowing it to systematically explore various prompt combinations and assess their efficacy while managing the trade-off between information gain and energetic costs.</p>
<p>To evaluate and refine its strategies, the agent receives observations in the form of metrics related to the responses of the research agent-specifically, accuracy, relevance, and comprehensiveness.These metrics inform the agent's posterior beliefs about which prompt combinations yield optimal results, guiding future decisions through principled belief updating that respects thermodynamic constraints.When conducting searches using predetermined search terms, the agent observes additional modalities such as information relevance, information usefulness, source quality, and information state.These observations help the agent assess the quality of external information sources and incorporate them into its learning process, effectively reducing its informational entropy through active exploration.</p>
<p>The agent operates by alternating between prompt-changing and searching states, with each belief update incurring thermodynamic costs as described by the Jarzynski equality.Actions taken in these states produce observations that update the agent's beliefs, enabling it to adapt its strategies over time.This dynamic interplay allows the agent to achieve continuous self-improvement by minimizing both its instantaneous variational free energy (through accurate perception) and expected free energy (through adaptive action selection).The expected free energy serves as a principled objective function that guides the system toward preferred future states while accounting for both utility and information gain.This foundation in the FEP provides theoretical guarantees about the agent's ability to maintain stability while adapting to new information, with explicit consideration of the thermodynamic costs associated with belief updating.</p>
<p>Related Works</p>
<p>The field of improving agent behavior with large language models (LLMs) has seen extensive research, which can be broadly categorized into the following related themes: improving LLM performance, leveraging LLMs as adaptive agents, and the integration of active inference principles for adaptive decision-making.Below, we review each of these themes, highlighting their contributions and limitations, and distinguishing our approach from existing work.</p>
<p>Improving LLM Performance</p>
<p>Efforts to enhance the performance of LLMs have primarily focused on improving the models themselves through better training data curation and self-improvement loops.For instance, Bowman et al. (2022) and Sun et al. (2023) explored methods for aligning LLMs with human preferences by using heuristics and self-generated principles to filter high-quality training data.Similarly, Bai et al. (2022) investigated using self-improvement via critique-based fine-tuning, while Gou et al. (2023a) proposed leveraging external tools for more granular feedback.</p>
<p>Approaches such as Re-ReST (Guo et al., 2024) curate better training data for LLMs by generating and validating data with ground truth feedback, enhancing the model's reasoning capabilities.SELF (Lu et al., 2023) and Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing (Sun et al., 2023) use reflective mechanisms to iteratively improve LLMs.While these techniques improve LLM capabilities, they primarily target the model's intrinsic quality rather than enhancing the decision-making or adaptability of agents leveraging LLMs.</p>
<p>Our approach diverges by focusing on the agent's structure, policies, and environmental interactions rather than improving the LLM itself.Specifically, we use active inference to guide an agent in adapting prompts dynamically, enabling systematic exploration of complex policy spaces that are not addressed by intrinsic LLM improvements.</p>
<p>Leveraging LLMs as Adaptive Agents</p>
<p>Adaptive agent frameworks often use LLMs as the core reasoning component.WEBRL (Wang et al., 2023b) and Meta-Analysis of Agent Frameworks (Nascimento et al., 2024) illustrate LLMs functioning as components within multi-agent systems that use reinforcement learning to adapt based on environmental feedback.In such systems, the LLM serves as the vector of adaptation, leveraging its context window for environmental understanding.</p>
<p>In open-world exploration tasks, such as Minecraft-based environments, LLMs are used to drive agent behavior based on environmental cues (Wortsman et al., 2019;Wang et al., 2023;Liu et al., 2023).Frameworks like Voyager (Wang et al., 2023) and Odyssey (Liu et al., 2023) demonstrate how large language models can guide agents in acquiring open-world skills.However, these systems rely on using the LLM itself as the vector of adaptation, which limits the agent's ability to learn structurally from its environment beyond immediate context.</p>
<p>In contrast, our model integrates active inference to go beyond context-driven adaptation.By modeling the environment through state factors and observing structured feedback metrics (e.g., accuracy, relevance, comprehensiveness), our framework actively updates beliefs about prompts and search actions.This allows the agent to dynamically adapt not just its behavior but its structural knowledge about effective strategies.</p>
<p>Our agent is able to take advantage of a computational framework based on the human brain, in selecting which LLMs to use and for what purpose going beyond simply using LLMs directly for all aspects of exploration and exploitation.</p>
<p>Active Inference in AI</p>
<p>Active inference has emerged as a theoretical framework for modeling adaptive behavior, rooted in the free energy principle (Friston et al., 2017).Most applications in AI focus on low-dimensional problems, such as robotic control or navigation (Schwartenbeck et al., 2019;Parr et al., 2022), where actions aim to reduce uncertainty about the environment.</p>
<p>The use of active inference in high-dimensional, language-driven environments remains underexplored.Existing research has not addressed its application as a "brain" for LLM-based agents to systematically explore complex policy spaces.Our work addresses this gap by employing active inference to balance exploration and exploitation in an LLM-driven research agent.By explicitly modeling prompt combinations, search strategies, and the costs of actions, our framework introduces a practical approach to incorporating active inference for structured exploration and learning.</p>
<p>Integrated Frameworks for Multi-Objective Learning</p>
<p>Some frameworks aim to combine various optimization strategies into unified models.For example, SELF-Evolutionary Systems (Zhong et al., 2023) apply reinforcement learning for internet-based exploration tasks, and Generative AI for Self-Adaptive Systems (Nascimento et al., 2024) provide research roadmaps for improving agent adaptability.However, these methods often treat optimization, information retrieval, and adaptation as distinct processes.</p>
<p>Our approach unifies these elements within a single active inference model.By framing decisions through Expected Free Energy (EFE) minimization, we enable the agent to dynamically select between exploration (e.g., testing new prompts) and exploitation (e.g., retrieving information).This integration provides a coherent mechanism for multi-objective learning, allowing for systematic decision-making that accounts for both environmental feedback and resource constraints.</p>
<p>Background</p>
<p>Theoretical Foundations</p>
<p>Active inference rests on the principle that biological systems minimize variational free energy both through perception and action.We begin with a rigorous derivation of this framework from first principles.</p>
<p>Derivation of Variational Free Energy</p>
<p>Starting with the definition of surprise (negative log model evidence):
− ln p(o) = − ln s p(o, s)(1)
We can introduce an arbitrary distribution q(s) by multiplying and dividing by it:
− ln p(o) = − ln s p(o, s)q(s) q(s)(2)
By Jensen's inequality, since ln is a concave function:
− ln p(o) ≤ − s q(s) ln p(o, s) q(s) = F (3)
This upper bound F is the variational free energy.We can decompose it:
F = s q(s) ln q(s) p(o, s) (4) = s q(s) ln q(s) − s q(s) ln p(o, s) (5) = s q(s) ln q(s) − s q(s) ln p(s) − s q(s) ln p(o|s) (6) = D KL [q(s)||p(s)] − E q(s) <a href="7">ln p(o|s)</a></p>
<p>Information Gain and Pragmatic Value Formulation of Expected Free Energy</p>
<p>The expected free energy in its conceptual form from Smith et al. is given by:
G π = −E q(o|π) [D KL [q(s|o, π)||q(s|π)]] information gain − E q(o|π) [ln p(o|π)] pragmatic value (8) EU = T −1 t=0 M −1 m=0 q(o m t |π) • ln σ(C m ) (9)
This is the form used in the experiments.However, this formulation is a conceptual variety of the physical formulation, involving entropy.As shown in Champion et al., this formulation can be derived through a series of steps.The derivation relies on the following equality: q(s|π)q(s|o, π) = q(o|π)q(o|s) (10) which holds because the forecast distribution is a partially observable Markov decision process.We start by re-arranging Bayes theorem as follows: q(s|o, π) = q(o|s, π)q(s|π) q(o|π) ⇔ q(s|π)q(s|o, π) = q(o|π)q(o|s, π)</p>
<p>Starting with the definition of G π and using (2), one can show that:
G π = −E q(o|π) [D KL [q(s|o, π)||q(s|π)]] − E q(o|π) <a href="12">ln p(o|π)</a>
Using the KL-divergence definition, log properties and the linearity of expectation, we get:
G π = −E q(o,s|π) [ln q(o|s)] + E q(o|π) <a href="13">ln q(o|π) − ln p(o|π)</a>
Lastly, recognizing the entropy and KL-divergence definitions leads to the final results:
G π = D KL [q(o|π)||p(o|C)]
risk over observations
+ E q(s|π) H[q(o|s)] ambiguity (14)</p>
<p>Message Passing Implementation</p>
<p>To implement state inference through gradient descent on VFE:</p>
<p>∂F ∂q(s)</p>
<p>= ln q(s) − ln p(s) − ln p(o|s) + 1 (15)</p>
<p>For factorized variational inference with multiple factors f , the solution becomes:
q(s f ) = σ( m ln p(o m |s f , s −f ) + ln p(s f )) (16)
where s −f represents all other factors except f .For temporal models with control states π:
q(s τ +1,f |π) = B π,f q(s τ |π)(17)
where B π,f is the transition matrix for factor f under policy π.</p>
<p>In matrix notation this becomes:
s π,τ +1,f = B π,f s π,τ(18)</p>
<p>Learning Through Parameter Updates</p>
<p>The learning rules follow from minimizing VFE with respect to model parameters.For the A matrix with multiple modalities m:
∂F ∂A m = ∂ ∂A m E q(s) <a href="19">ln p(o|s)</a>
For Dirichlet priors over parameters:
p(A m ) = Dir(a m )(20)
Given observation o m and beliefs q(s) over states that modality m depends on, the update uses the outer product:
o m ⊗ q(s) = o m • |s| dim=1 q(s) dim (21)
This yields the learning rule:
a t+1 m = a t m + η • (o m ⊗ q(s)) ⊙ (A m &gt; 0) (22)
where η is the learning rate and ⊙ represents element-wise multiplication.</p>
<p>Policy Selection</p>
<p>The posterior over policies follows from minimizing expected free energy:
G π = E q(o|π) [ln p(o)] + E q(s|π) [ln q(s|π)] + G param (23)
where G param captures parameter information gain.The policy posterior is computed via softmax:
q(π) = σ(γG + ln E) (24)
where γ is the precision parameter and E represents prior policy preferences ("habits").The selected policy is then:
π * = arg max π q(π)(25)
4 Setup</p>
<p>Agent Architecture</p>
<p>The research agent's architecture is implemented through an active inference framework with three key state factors: prompt states (33 possible combinations), search states (11 possible states), and information states (3 possible states: no information, basic information, detailed information).The agent observes seven modalities: three prompt-dependent quality metrics (accuracy, relevance, comprehensiveness), three search-dependent quality metrics (information relevance, information usefulness, source quality), and one information state observation.</p>
<p>Generative Model</p>
<p>The generative model is defined by the following components:</p>
<p>Observation Model (A Matrices)</p>
<p>The observation model consists of a set of likelihood mappings between hidden states and observations, organized into a tensor with different slices for each modality type (see Figure 1).</p>
<p>The prompt-dependent modalities (A [0,1,2] ) map 33 possible prompt states to 11 quality levels (0-10), capturing how different prompt combinations influence output quality.The search-dependent modalities (A [3,4,5] ) map 11 search states to quality observations, modeling how search strategies affect information gathering.The information state modality (A [6] ) provides a direct mapping between hidden and observed information states.providing a direct mapping between 3 hidden information states and their corresponding observations.Darker colors indicate higher probability values, showing the structure of the likelihood mappings.Here the matrices are all uniform since the agent starts with no knowledge of the state-observation mappings.</p>
<p>Transition Model (B Matrices)</p>
<p>The transition model is structured as three matrices corresponding to each state factor.The prompt transitions are modeled by a 33 × 33 × 33 tensor (B prompt ) handling transitions between prompt states.Search transitions utilize an 11 × 11 × 11 tensor (B search ) for modeling transitions between search states.Information state progression is captured by a 3 × 3 × 1 tensor (B inf o ).Transition probabilities are initially configured to maintain current states when no action is taken, with controlled transitions possible through specific actions.</p>
<p>Prior Preferences (C Matrix)</p>
<p>The preference distribution is structured to drive both information-seeking and quality-maximizing behavior.For quality metrics (modalities 0-5), the model implements a strong negative preference (-16.0) for low quality observations, with quadratically increasing preferences for higher quality levels scaled by 2.0.Information states (modality 6) are assigned highly structured preferences with values of -32.0, 8.0, and 64.0 for no info, basic info, and detailed info states respectively.</p>
<p>Initial State Priors (D Matrix)</p>
<p>Initial state beliefs are configured as uniform distributions across all state factors.Prompt states are initialized with 1/33 probability for each state, search states with 1/11 probability for each state, and information states with 1/3 probability for each state.</p>
<p>Learning Parameters</p>
<p>The agent employs Dirichlet distributions for learning the observation and transition models:</p>
<p>Observation Learning (pA)</p>
<p>Minimal concentration parameters (base concentration = 1.0) are used for all modalities to maximize learning flexibility.The prompt modalities use 11 × 33 matrices, search modalities employ 11 × 11 matrices, and the information state modality utilizes a 3 × 3 matrix.</p>
<p>Transition Learning (pB)</p>
<p>The model implements minimal structured priors with a base concentration of 1.0 and small biases (0.1) for specific transition types.These include state persistence under no action for prompt transitions, decay to no-search state for search transitions, and forward progression for information state transitions.</p>
<p>Observation Generation</p>
<p>The agent receives structured observations about search quality and prompt effectiveness through evaluations performed by GPT-4o-mini, which provides standardized JSON output.For example, for search-related observations, the model evaluates search results using a structured output format:</p>
<p>Return only a JSON object with these three metrics, scored 0.0-1.0:{ "info_relevance": [Score from 0.0 to 1.0 counting up by 0.1], "info_usefulness": [Score from 0.0 to 1.0 counting up by 0.1], "source_quality": [Score from 0.0 to 1.0 counting up by 0.1] } These standardized scores are then scaled to the 11-point observation space used by the active inference agent's observation model.The use of structured JSON output ensures consistent, programmatic evaluation of search results, providing reliable feedback for the agent's learning process.Similar structured evaluations are performed for prompt quality metrics (accuracy, relevance, and comprehensiveness), with the language model providing standardized scores that are then mapped to the agent's observation space.</p>
<p>This approach leverages GPT-4o-mini's capability for structured output generation to create a reliable evaluation pipeline, ensuring that the active inference agent receives consistent, well-formatted observations for updating its belief states and learning the effectiveness of different actions.</p>
<p>Action Selection</p>
<p>The agent employs a sophisticated policy selection mechanism with a policy horizon of 2 steps and an inference horizon of 1 step.Both state-information gain and parameter-information gain are enabled, and the model uses deterministic action selection.Valid policies consist of three types of actions: no action [0, 0, 0], prompt-only actions [p, 0, 0] where p ∈ {1, 2, . . ., 33}, and search-only actions [0, s, 0] where s ∈ {1, 2, . . ., 11}.There can be no such action where [p, s, 0] since this would imply that the prompt and search actions are being performed simultaneously and the agent is not able to do that.</p>
<p>Control Parameters</p>
<p>The model implements several key control parameters.A learning rate (η) of 50.0 is used for both observation and transition learning.Policy precision (γ) is set to 8.0 to balance exploration and exploitation.Action precision (α) is configured at 16.0 to control action selection determinism.</p>
<p>State Factor Dependencies</p>
<p>The model employs structured dependencies between state factors and observations.The prompt factor influences accuracy, relevance, and comprehensiveness metrics.The search factor affects information relevance, usefulness, and source quality observations.The information factor determines information state observations.These dependencies are encoded in A factor list and B factor list specifications to ensure proper message passing during inference.</p>
<p>Implementation</p>
<p>The active inference algorithm can be expressed as:</p>
<p>Algorithm 1 Active Inference with Environment Interaction 1: Initialize A, B, C, D matrices, precision γ 2: Initialize beliefs q(s 0 ) 3: while not converged do 4:
o t ← observe environment 5:
for each policy π do 6:</p>
<p>q(s t |π) ← σ(ln p(s t ) + ln p(o t |s t )) ▷ State estimation 7:
q(s t+1:T |π) ← B π q(s t |π) ▷ State prediction 8: G π ← ComputeEFE(q(s t:T |π), A, C) ▷ Expecteda t+1 ← a t + η • (o t ⊗ q(s t )) ⊙ (A &gt; 0) ▷ Parameter update 20: end if 21: t ← t + 1 22: end while
The algorithm iterates until convergence, performing state estimation, policy evaluation, action selection and parameter learning at each step.The key equations governing each update have been derived in previous sections.</p>
<p>Results</p>
<p>Learning Environment Dynamics</p>
<p>Through active exploration and learning, the agent successfully developed an accurate model of the environment, particularly the relationships between states and observations.Figure 2 shows the final learned observation mappings after multiple interactions with the environment.Compared to the initial uniform distributions (Figure 1), these matrices show clear structure, indicating the agent has learned meaningful relationships between states and observations.The prompt quality matrices (A [0,1,2] ) developed distinct patterns showing which prompt combinations lead to higher quality outputs.The search quality matrices (A [3,4,5] ) reveal learned associations between search actions and information quality, while the information state matrix (A [6] ) captures the reliable mapping between hidden and observed information states.</p>
<p>Strategic Action Selection</p>
<p>The agent's action selection strategy evolved over time as its Expected Free Energy over policies changed.Figure 3 shows the expected free energy of different policies at four time points during the agent's operation.This progression reveals how the agent learned to value different action combinations based on their information-gathering and goal-achieving potential.</p>
<p>Figure 2: Final learned observation mappings after environment interaction.The first three matrices on the top row show the relationships between prompt states and quality metrics.The final matrix on the top row and the first two matrices on the bottom row show the relationship between search states and search quality metrics.The final matrix on the bottom row shows the learned mapping between the information state factor and the information observation modality.The matrices effectively show, for each prompt and search term, what scores seem to be associated with them based on the observations.The final matrix structure results from a predominance of "detailed info" observations.The emergence of structure from the initial uniform distributions (Figure 1) demonstrates successful learning of environment dynamics.</p>
<p>Information-Driven Exploration</p>
<p>The agent's action selection patterns provide strong evidence for sophisticated exploration and exploitation behavior emerging from the free energy minimization framework.Figures 4 and 5    The temporal progression of the agent's behavior, clearly visible in Figure 5, reveals a strategic shift in action selection.During the initial phase (approximately the first 40 timesteps), the agent heavily favors search actions, indicated by the higher density of red dots.This initial search-focused behavior naturally emerges from the free energy minimization framework, as the agent prioritizes reducing uncertainty about its environment through observation of search quality metrics and information state levels.</p>
<p>As the experiment progresses, Figure 5 shows a marked transition to prompt-dominated behavior, evidenced by the increasing density of blue dots in later timesteps.This shift occurs as the agent receives observations indicating higher information states and favorable search quality metrics.Rather than directly encoding search results, the agent's behavior is guided by these external indicators of knowledge state and search effectiveness, leading to more focused prompt testing.</p>
<p>The aggregate view provided by Figure 4 complements this temporal analysis by showing the overall distribution of action selections.The concentration patterns in the heatmap reveal which prompt-search combinations proved most effective, where prompt and search actions that produced more preferred obser-vations were sampled more.</p>
<p>The progression of policy Expected Free Energy (EFE) values (Figure 3) provides further insight into how the agent's evaluation of different action combinations became more refined over time.Initially, the differences in EFE values were relatively small, reflecting the agent's inability to differentiate between the available policies.As learning progressed, the EFE landscape developed clear structure, indicating the agent had learned which action combinations were most effective for achieving its goals.</p>
<p>The final learned observation matrices (Figure 2) provide evidence that this exploration strategy was successful in discovering the underlying structure of the environment.The emergence of distinct patterns in these matrices, particularly in the prompt-quality relationships, demonstrates that the agent effectively learned which prompt combinations lead to better outcomes, guided by the externally provided information state observations and quality metrics rather than direct knowledge incorporation from searches.</p>
<p>This sophisticated exploration-exploitation pattern emerged naturally from the free energy minimization framework, with the agent learning to balance information-gathering (through search actions and their associated quality metrics) with exploitation of known effective prompts (guided by prompt quality observations and information state feedback).The clear temporal progression from exploration to exploitation, visible in both the timeline and aggregate statistics, demonstrates the effectiveness of active inference in managing this fundamental trade-off.</p>
<p>Future Work</p>
<p>This paper has demonstrated a novel approach to combining active inference with traditional AI agents, showing how active inference can guide the exploration and optimization of agent parameters.However, this implementation represents only an initial step toward fully adaptive AI systems.</p>
<p>A key limitation of the current model is its reliance on a fixed state space with predefined prompts.Future work should focus on developing active inference models capable of dynamically expanding their state space during operation.This would allow the agent to explore a vastly larger set of possible prompts and configurations, moving beyond the constraints of preset options to discover novel, potentially more effective combinations.</p>
<p>Another crucial direction is the development of hierarchical generative models.Rather than relying on abstract information states, such models would enable the agent to directly encode and reason about environmental information.This would allow for more sophisticated understanding of the agent's performance and context, leading to more informed adaptation decisions.</p>
<p>The framework could also be extended to optimize broader aspects of agent intelligence, including agent architecture, execution order of sub-agents, and tool utilization.These elements represent critical parameters that require intelligence to optimize effectively.By incorporating these factors into the active inference model, we could enable more comprehensive agent improvement.</p>
<p>These developments represent intermediate steps toward an ambitious long-term goal: a universal active inference model that can be applied to any agent system without predefined parameters.Such a model would function analogously to human researchers, capable of conducting research, engaging in trial and error, performing comprehensive parameter testing, and maintaining an intelligent exploration-exploitation loop.This would enable automated, principled improvement of any given agent system, representing a significant advance in artificial intelligence adaptation and optimization.</p>
<p>Figure 1 :
1
Figure 1: Visualization of the three main components of the observation model tensor.Left: Prompt quality observations A [0,1,2] mapping 33 prompt states to 11 quality levels for accuracy, relevance, and comprehensiveness.Middle: Search quality observations A [3,4,5] mapping 11 search states to 11 quality levels for information relevance, usefulness, and source quality.Right: Information state observations A [6]providing a direct mapping between 3 hidden information states and their corresponding observations.Darker colors indicate higher probability values, showing the structure of the likelihood mappings.Here the matrices are all uniform since the agent starts with no knowledge of the state-observation mappings.</p>
<p>Figure 3 :
3
Figure 3: Progression of Expected Free Energy (EFE) values for different policies across four time points.The evolution shows how the agent learned to distinguish between effective and ineffective action combinations.Lower EFE values (darker colors) indicate more preferred policies.The emergence of clear patterns demonstrates the agent's developing understanding of which actions are most valuable in different contexts.</p>
<p>illustrate complementary views of this behavioral progression.</p>
<p>Figure 4 :
4
Figure 4: Heatmap showing the frequency of action selection across prompt and search dimensions.Lighter colors indicate more frequently selected actions.The pattern shows the overall distribution of action selections, with certain prompt-search combinations being consistently preferred over others based on their effectiveness.</p>
<p>Figure 5 :
5
Figure 5: Time series of action selection throughout the experiment.Blue dots represent prompt actions (labeled with prompt IDs), while red dots represent search actions (labeled with search IDs).The progression shows a clear transition from search-dominated early phases to prompt-dominated later phases, demonstrating the agent's evolving strategy from exploration to exploitation.</p>
<p>Data AvailabilityThe full code implementation of the active inference agent, environment, and evaluation system described in this paper is available in our public GitHub repository at https://github.com/RPD123-byte/Active-Inference-for-Self-Organizing-Multi-LLM-Systems-A-Bayesian-Thermodynamic-Approach-to-Adaptat.This includes all components necessary to reproduce our experimental results, including the agent controller, environment simulation, and visualization tools.
graph websearch agent: Websearch agent built on the LangGraph framework. GitHub repository. J Adeojo, 2024</p>
<p>Training language models to follow instructions with human feedback. Y Bai, W Saunders, L Ouyang, Advances in Neural Information Processing Systems. 352022</p>
<p>Measuring progress on scalable oversight for large language models. S R Bowman, S Deng, C Raffel, arXiv:2202.077652022arXiv preprint</p>
<p>The Physics of Free Will. H R Brown, K J Friston, Neuroscience and Biobehavioral Reviews. 902018</p>
<p>The free energy principle for action and perception: A mathematical review. C L Buckley, C S Kim, S Mcgregor, A K Seth, 10.1007/s00422-019-00805-wBiological Cybernetics. 11262017</p>
<p>Reframing the Expected Free Energy: Four Formulations and a Unification. T Champion, H Bowman, D Marković, M Grześ, 2023Birmingham, United KingdomUniversity of Kent, School of Computing, Canterbury, United Kingdom; University of Birmingham, School of Psychology and School of Computer Science</p>
<p>Wellcome Centre for Human Neuroimaging (honorary). London, United KingdomTechnische Universität Dresden, Department of Psychology, Dresden, Germany; University College London</p>
<p>Active Inference with State-Only Control. L Dandoy, M Di Francesco, 2023</p>
<p>Control flow in active inference systems. C Fields, F Fabrocini, K Friston, J F Glazebrook, H Hazan, M Levin, A Marcianò, 2023Allen Discovery Center at Tufts University</p>
<p>The graphical brain: Belief propagation and active inference. K Friston, T Parr, B De Vries, 10.1162/NETNa00018Network Neuroscience. 142017</p>
<p>Leveraging external tools for critique-driven self-improvement in language models. W Gou, X Sun, Q Li, arXiv:2306.051232023aarXiv preprint</p>
<p>Re-ReST: Reflection-reinforced self-training for language agents. J Guo, Y Liu, W Chen, arXiv:2403.071252024arXiv preprint</p>
<p>Odyssey: Empowering Minecraft agents with open-world skills. S Liu, Y Li, K Zhang, arXiv:2310.012342023arXiv preprint</p>
<p>SELF: Self-evolution with language feedback. J Lu, W Zhong, W Huang, arXiv:2310.005332023arXiv preprint</p>
<p>Predictive Coding: A Theoretical and Experimental Review. B Millidge, A Tschantz, C L Buckley, 2020</p>
<p>Generative AI for self-adaptive systems: State of the art and research roadmap. N Nascimento, P Alencar, D Cowan, ACM Transactions on Autonomous and Adaptive Systems. 1932024</p>
<p>Active inference: The free energy principle in mind, brain, and behavior. T Parr, G Pezzulo, K J Friston, Journal of Mathematical Psychology. 1001023642021</p>
<p>N Sajid, K Friston, T Parr, Planning and Active Inference. 2021</p>
<p>Active Inference, Curiosity and Insight. P Schwartenbeck, K Friston, Neural Computation. 29102017</p>
<p>Active inference, belief propagation, and the free energy principle. P Schwartenbeck, T Fitzgerald, C Mathys, R Dolan, K Friston, 10.1371/journal.pone.0277199PLOS ONE. 1711e02771992023</p>
<p>The role of the free energy principle in cognitive systems. S Shipp, 10.1177/26339137231222481Cognitive Science. 1722023</p>
<p>Advances in Active Inference. J Smith, M Johnson, Trends in Cognitive Sciences. 2023</p>
<p>A step-by-step tutorial on active inference and its application to empirical data. R Smith, K J Friston, C J Whyte, Journal of Mathematical Psychology. 1071026322023</p>
<p>Toward self-improvement of LLMs via imagination, searching, and criticizing. Z Sun, L Wang, Y Li, arXiv:2310.005332023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, arXiv:2306.072912023arXiv preprint</p>
<p>WEBRL: Training LLM web agents via self-evolving online curriculum reinforcement learning. P Wang, J Chen, H Zhao, arXiv:2305.098762023barXiv preprint</p>
<p>Learning to learn how to learn: Self-adaptive visual navigation using meta-learning. M Wortsman, K Ehsani, M Rastegari, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2019</p>
<p>Y Zhang, Z Zheng, Reinforcement Learning with Active Inference. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1855 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1855</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1855</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-da2fe6cd385194b0274d04d04ee72e8caf3854d4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/da2fe6cd385194b0274d04d04ee72e8caf3854d4" target="_blank">Learning Universal Policies via Text-Guided Video Generation</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work casts the sequential decision making problem as a text-conditioned video generation problem, where a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video.</p>
                <p><strong>Paper Abstract:</strong> A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks. Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Specifically, we cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video. By leveraging text as the underlying goal specification, we are able to naturally and combinatorially generalize to novel goals. The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images, which, for example, enables learning and generalization across a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and widely available videos from the internet, the approach enables knowledge transfer through predicting highly realistic video plans for real robots.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1855.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1855.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniPi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Policies via Text-Guided Video Generation (UniPi)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-conditioned video-diffusion planner that generates future image sequences from an initial RGB frame and a language goal, and an inverse-dynamics module that maps generated frames to low-level robot actions to produce multi-task and cross-environment behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>UniPi</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>UniPi consists of (1) a first-frame-conditioned video diffusion model (video U-Net / temporal super-resolution hierarchy) that generates H-step future image sequences conditioned on the current RGB frame and a text task description (T5 embeddings), and (2) a separately trained inverse-dynamics model that regresses low-level robot controls from consecutive frames; the generated frames are converted to actions and executed (open-loop in experiments). The system uses classifier-free guidance, hierarchical temporal super-resolution, and tiling of the initial frame to maintain environment consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Internet-scale text–video and image–text pairs (web videos and captions), plus pretrained language model embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>UniPi was pretrained on the same large-scale data used by [19]: approximately 14 million video–text pairs and 60 million image–text pairs, and additionally uses LAION-400M image–text data; T5-XXL (4.6B params) provides pretrained text embeddings. For robotics finetuning the model was further finetuned on the Bridge robot dataset (7.2k video–text pairs, 80/20 train/test split). For simulated experiments, task-specific video datasets of ~200k generated videos were used to train the video model; inverse dynamics training used ~20k annotated videos for the combinatorial task (and 200k action-annotated videos for multitask).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Simulated combinatorial block-manipulation (6-DOF robot), CLIPort-style multi-task manipulation suite, and real-world Bridge robotic tasks (finetune/eval)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Simulated: PyBullet 6-DOF robotic arm with discrete contact action performing multi-step block manipulation (coloring via bowls and placing blocks in spatial relations) with randomized object placements; Multitask: CLIPort-derived tasks (place in bowl, packing objects/pairs, assembling kits, tower-of-Hanoi variants); Real-world: Bridge dataset tasks (robot video-text pairs) used for finetuning and evaluation of generated video plans and success classification.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions / task descriptions (goal-oriented text prompts describing the desired objective, e.g., 'put the red block to the right of the cyan block').</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous robot joint controls (7-dimensional controls used in inverse dynamics) plus a discrete contact/gripper action (contact is thresholded >0.5 in continuous predictions); low-level joint targets executed through the robot joint controller (PyBullet) with a timeout for infeasible actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Learned inverse-dynamics regression: a small convolutional network trained to predict continuous joint controls (7-D) from image observations (frame pairs / sequences). Generative pipeline: (1) generate H future frames conditioned on current frame and text, (2) feed generated frames to inverse-dynamics model to predict actions, (3) execute predicted actions (open-loop in experiments; MPC supported but not used).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB image sequences (video frames) as observations; high-enough spatial resolution to see objects (experiment resolutions reported: e.g., 10x48x64, 20x48x64 for temporal models, higher resolutions for real-world finetuning). Uses pretrained language embeddings (T5) to encode text instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Real-world finetune: Success classifier on generated last-frame: 77.1% success (Pretrain) with CLIP Score 24.54 ± 0.03, FID 14.54 ± 0.57, FVD 264.66 ± 13.64 (metrics averaged over 32 samples). Simulated combinatorial tasks (UniPi trained on 200k simulated videos): Place (seen) 59.1 ± 2.5% and Relation (seen) 53.2 ± 2.0%; Place (novel) 60.1 ± 3.9% and Relation (novel) 46.1 ± 3.0%. Multitask (CLIPort suite): Place Bowl 51.6 ± 3.6%, Pack Object 75.5 ± 3.1%, Pack Pair 45.7 ± 3.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Real-world finetune from scratch (no internet pretrain): Success 72.6% with CLIP Score 24.43 ± 0.04, FID 17.75 ± 0.56, FVD 288.02 ± 10.45 (same Bridge finetune set). For simulated tasks, the reported baselines are different architectures rather than ablations of internet pretraining; UniPi without certain video-conditioning components (ablation) shows much lower seen-task performance (e.g., removing frame conditioning/consistency/hierarchy reduced Place seen to 13.2% in extreme ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Pretraining: 14M video–text pairs + 60M image–text pairs + LAION-400M used for internet-scale pretraining; finetuning on Bridge used 7.2k robot video–text pairs; simulated training used ~200k generated videos for video model and ~20k annotated videos for inverse dynamics in combinatorial tasks. No explicit learning-curve sample-count-to-threshold reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Without internet pretraining, finetuning solely on Bridge (7.2k pairs) yields lower generation and success metrics (success 72.6% vs 77.1% with pretraining). The paper does not report the number of environment interactions required to reach particular episodic-success thresholds in training curves.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>No direct samples-to-threshold improvement reported; empirical gains from internet pretraining include an absolute increase of ~4.5 percentage points in the surrogate success metric on Bridge finetuning (72.6% -> 77.1%), and improved generative quality (FID and FVD reductions). Pretraining enabled strong generalization with relatively small robotics finetuning data (7.2k).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key contributors: (1) semantic alignment via language-conditioned video generation (T5 embeddings capturing instruction structure), (2) large-scale web video/text pretraining capturing general motion and visual dynamics, (3) use of images as a universal state/action interface enabling cross-morphology reuse, (4) hierarchical temporal generation and first-frame conditioning/tiling for consistency, (5) a small inverse-dynamics module separating environment-agnostic planning from agent-specific control.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations: diffusion sampling is slow; possible hallucinations in partially-observed settings; action execution depends on quality and amount of action-annotated videos (inverse dynamics trained on limited annotated data e.g., 20k); mismatch between photorealistic internet videos and robot embodiment leading to domain gap; no explicit reward optimization (imitation-style), and limited evaluation of sample-efficiency curves.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-conditioned, internet-pretrained video diffusion models can serve as universal planners: pretraining on large web-scale text–video datasets plus T5 text embeddings improves video-plan realism and downstream task success after modest robotics finetuning (thousands of robot videos). Mapping generated frames to actions via a small inverse-dynamics model enables transfer to continuous joint-control robots, yielding substantially better combinatorial and multi-task generalization than baselines; internet pretraining yields measurable but modest absolute improvements in success and generative metrics and enables generalization to novel commands and scenes with limited task-specific finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Universal Policies via Text-Guided Video Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1855.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1855.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Video PreTraining (VPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that learns to act by watching unlabeled online videos, aiming to leverage web video data for action learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Video pretraining (vpt): Learning to act by watching unlabeled online videos.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Video PreTraining (VPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>VPT learns action policies by leveraging large quantities of unlabeled online videos, typically using an inverse dynamics or action labeling approach to produce action supervision from videos.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Unlabeled online videos (web videos) with captions/metadata</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Referenced as prior work; exact datasets and sizes are in the VPT paper (not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic control via learning from videos (as described in cited VPT work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not detailed in this paper; VPT is mentioned as an approach that can utilize internet-scale video data to learn acting policies.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (VPT uses videos; may rely on inferred labels or action primitives depending on original work)</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (depends on VPT experimental setup in original paper)</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Generally uses inverse-dynamics or action labeling to map visual observations in videos to actions (details in VPT paper).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB video input; possibly audio/metadata</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Leverages abundant unlabeled video data to obtain behavioral priors and action mappings (as argued in referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Domain gap between web videos and robot embodiment; need for inverse-dynamics labeling; details in VPT paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as related work that uses internet videos to produce action supervision for control — consult the original VPT paper for quantitative transfer details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Universal Policies via Text-Guided Video Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1855.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1855.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DALL·E-Bot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DALL·E-Bot: Introducing web-scale diffusion models to robotics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work applying web-scale text-to-image diffusion models (e.g., DALL·E style) to robotics tasks, producing goal images to condition policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dall-e-bot: Introducing web-scale diffusion models to robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>DALL·E-Bot (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Uses web-scale text-conditioned image diffusion to synthesize goal images that can be used to guide robot policies (goal-image conditioning approach).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Web-scale text–image datasets used to train image diffusion generators</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; the DALL·E-Bot reference should be consulted for dataset sizes and specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic planning conditioned on generated goal images</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Involves generating goal images from text, then conditioning robotic policies on these goals; not detailed further in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language prompts used to generate goal images</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Robot control; specifics in the cited DALL·E-Bot paper</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Generate goal image from text (diffusion) and use goal-conditioned policy to map to actions (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Image observations and generated goal images</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Access to large web-scale image-text generative priors to create plausible goal conditions for policies.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Potential mismatch between generated goal images and robot embodiment; limited discussion here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related work showing how web-scale text-conditioned image diffusion models can be introduced into robotics; refer to the original DALL·E-Bot paper for full experimental details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Universal Policies via Text-Guided Video Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1855.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1855.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1: Robotics Transformer for Real-World Control at Scale</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotics transformer approach that conditions on vision and text to produce control outputs, demonstrating large-scale supervised learning for robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-1: Robotics transformer for real-world control at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-1 (cited as baseline context)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A transformer-based robotic policy that takes image and language inputs to output control actions; designed for real-world robot control with large-scale supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale robot demonstration datasets with associated language instructions (as in RT-1 paper)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not provided in this paper; see the RT-1 paper for dataset sizes and collection details.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Real-world robot control tasks (RT-1 benchmark tasks in original work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>RT-1 learns to map image observations and language to robot actions for real tasks; specifics are in the RT-1 publication.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions specifying tasks/goals</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level robot actions/poses (RT-1 predicts robot control targets or primitives)</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>End-to-end transformer policy mapping visual+text inputs to control outputs (see RT-1 paper).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB vision; possibly other sensors as used by RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale supervised robot data with language annotations enabling direct mapping from text to actions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>RT-1 operates in environments with shared action/state spaces for training and test; may not trivially generalize across morphologies — discussed as contrast in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as representative prior work that learns language-conditioned control policies directly in robot action spaces; differs from UniPi's image-as-universal-interface approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Universal Policies via Text-Guided Video Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1855.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1855.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (Text-To-Text Transfer Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained text-to-text transformer used here to encode language instructions fed into the video diffusion planner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>T5-XXL (text encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained language model (T5-XXL, ~4.6B parameters) used to encode textual task descriptions into embeddings which condition the video diffusion planner.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale natural language corpora (as per T5 pretraining; not re-specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>This paper uses pretrained T5-XXL to provide text embeddings; the original T5 paper provides dataset and pretraining details.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used to condition UniPi on natural language for simulated and real robot tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>T5 supplies semantic text encodings for goal-conditioned video generation across the simulated combinatorial tasks, CLIPort suite, and Bridge finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions encoded by T5</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (T5 encodes instructions; mapping to embodied actions is done via UniPi's video planner + inverse dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>T5 embeddings condition the diffusion denoiser s(·|c,x0) (classifier-free guidance applied). The mapping to actions is indirect via generated frames then inverse dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>N/A (T5 handles text); used jointly with RGB video frames in UniPi.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>T5-XXL used as conditioning input across UniPi and baselines; the paper reports that conditioning with pretrained language features is used throughout (no isolated ablation on T5 alone).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretrained language embeddings provide strong, generalizable instruction encodings which facilitate combinatorial generalization of video-conditioned planning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not isolated in experiments; potential limitations if text encoder fails to capture fine-grained task semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using pretrained language embeddings (T5) is practical and standard for conditioning the video planner on natural language goals; contributes to UniPi's language generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Universal Policies via Text-Guided Video Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1855.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1855.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Imagen Video (pretraining data/model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Imagen Video: High Definition Video Generation with Diffusion Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art text-to-video diffusion work whose data and modeling approach underpin UniPi's internet-scale pretraining and architecture choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Imagen video: High definition video generation with diffusion models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Imagen Video (data & modeling reference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>High-fidelity text-to-video diffusion models and the public training data and techniques used in large text–video pretraining referenced by UniPi for its pretraining corpus and architectural templates.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale web video-text pairs for text-to-video generation</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper states UniPi used the same data as [19] (Imagen Video): ~14M video-text pairs, 60M image-text pairs, plus LAION-400M; Imagen Video is the cited source for these resources and modeling practices.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used as pretraining source for downstream robotic video planning tasks in UniPi</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Provides the text–video generative prior that UniPi finetunes on small robot datasets to synthesize robot-relevant motion videos.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language captions paired with web videos</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (Imagen Video itself is a generative model; UniPi maps generated frames to robot actions via inverse dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>UniPi finetunes the Imagen-Video-style diffusion model to generate robot-relevant video conditioned on text and first-frame; action mapping is handled by an inverse-dynamics module in UniPi.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB video frames</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large, diverse web video–text corpora provide motion priors and visual semantics that help synthesize plausible robot plan videos after modest finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Domain mismatch between web videos and robot embodiment; not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>UniPi explicitly leverages Imagen-Video-scale data and modeling to obtain a generative prior for robot video planning and demonstrates that internet pretraining improves downstream generation quality and task success after finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Universal Policies via Text-Guided Video Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Video pretraining (vpt): Learning to act by watching unlabeled online videos. <em>(Rating: 2)</em></li>
                <li>Dall-e-bot: Introducing web-scale diffusion models to robotics. <em>(Rating: 2)</em></li>
                <li>Imagen video: High definition video generation with diffusion models. <em>(Rating: 2)</em></li>
                <li>Planning with diffusion for flexible behavior synthesis. <em>(Rating: 2)</em></li>
                <li>Rt-1: Robotics transformer for real-world control at scale. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1855",
    "paper_id": "paper-da2fe6cd385194b0274d04d04ee72e8caf3854d4",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "UniPi",
            "name_full": "Universal Policies via Text-Guided Video Generation (UniPi)",
            "brief_description": "A text-conditioned video-diffusion planner that generates future image sequences from an initial RGB frame and a language goal, and an inverse-dynamics module that maps generated frames to low-level robot actions to produce multi-task and cross-environment behaviors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "UniPi",
            "model_agent_description": "UniPi consists of (1) a first-frame-conditioned video diffusion model (video U-Net / temporal super-resolution hierarchy) that generates H-step future image sequences conditioned on the current RGB frame and a text task description (T5 embeddings), and (2) a separately trained inverse-dynamics model that regresses low-level robot controls from consecutive frames; the generated frames are converted to actions and executed (open-loop in experiments). The system uses classifier-free guidance, hierarchical temporal super-resolution, and tiling of the initial frame to maintain environment consistency.",
            "pretraining_data_type": "Internet-scale text–video and image–text pairs (web videos and captions), plus pretrained language model embeddings",
            "pretraining_data_details": "UniPi was pretrained on the same large-scale data used by [19]: approximately 14 million video–text pairs and 60 million image–text pairs, and additionally uses LAION-400M image–text data; T5-XXL (4.6B params) provides pretrained text embeddings. For robotics finetuning the model was further finetuned on the Bridge robot dataset (7.2k video–text pairs, 80/20 train/test split). For simulated experiments, task-specific video datasets of ~200k generated videos were used to train the video model; inverse dynamics training used ~20k annotated videos for the combinatorial task (and 200k action-annotated videos for multitask).",
            "embodied_task_name": "Simulated combinatorial block-manipulation (6-DOF robot), CLIPort-style multi-task manipulation suite, and real-world Bridge robotic tasks (finetune/eval)",
            "embodied_task_description": "Simulated: PyBullet 6-DOF robotic arm with discrete contact action performing multi-step block manipulation (coloring via bowls and placing blocks in spatial relations) with randomized object placements; Multitask: CLIPort-derived tasks (place in bowl, packing objects/pairs, assembling kits, tower-of-Hanoi variants); Real-world: Bridge dataset tasks (robot video-text pairs) used for finetuning and evaluation of generated video plans and success classification.",
            "action_space_text": "Natural language instructions / task descriptions (goal-oriented text prompts describing the desired objective, e.g., 'put the red block to the right of the cyan block').",
            "action_space_embodied": "Continuous robot joint controls (7-dimensional controls used in inverse dynamics) plus a discrete contact/gripper action (contact is thresholded &gt;0.5 in continuous predictions); low-level joint targets executed through the robot joint controller (PyBullet) with a timeout for infeasible actions.",
            "action_mapping_method": "Learned inverse-dynamics regression: a small convolutional network trained to predict continuous joint controls (7-D) from image observations (frame pairs / sequences). Generative pipeline: (1) generate H future frames conditioned on current frame and text, (2) feed generated frames to inverse-dynamics model to predict actions, (3) execute predicted actions (open-loop in experiments; MPC supported but not used).",
            "perception_requirements": "RGB image sequences (video frames) as observations; high-enough spatial resolution to see objects (experiment resolutions reported: e.g., 10x48x64, 20x48x64 for temporal models, higher resolutions for real-world finetuning). Uses pretrained language embeddings (T5) to encode text instructions.",
            "transfer_successful": true,
            "performance_with_pretraining": "Real-world finetune: Success classifier on generated last-frame: 77.1% success (Pretrain) with CLIP Score 24.54 ± 0.03, FID 14.54 ± 0.57, FVD 264.66 ± 13.64 (metrics averaged over 32 samples). Simulated combinatorial tasks (UniPi trained on 200k simulated videos): Place (seen) 59.1 ± 2.5% and Relation (seen) 53.2 ± 2.0%; Place (novel) 60.1 ± 3.9% and Relation (novel) 46.1 ± 3.0%. Multitask (CLIPort suite): Place Bowl 51.6 ± 3.6%, Pack Object 75.5 ± 3.1%, Pack Pair 45.7 ± 3.7%.",
            "performance_without_pretraining": "Real-world finetune from scratch (no internet pretrain): Success 72.6% with CLIP Score 24.43 ± 0.04, FID 17.75 ± 0.56, FVD 288.02 ± 10.45 (same Bridge finetune set). For simulated tasks, the reported baselines are different architectures rather than ablations of internet pretraining; UniPi without certain video-conditioning components (ablation) shows much lower seen-task performance (e.g., removing frame conditioning/consistency/hierarchy reduced Place seen to 13.2% in extreme ablation).",
            "sample_complexity_with_pretraining": "Pretraining: 14M video–text pairs + 60M image–text pairs + LAION-400M used for internet-scale pretraining; finetuning on Bridge used 7.2k robot video–text pairs; simulated training used ~200k generated videos for video model and ~20k annotated videos for inverse dynamics in combinatorial tasks. No explicit learning-curve sample-count-to-threshold reported.",
            "sample_complexity_without_pretraining": "Without internet pretraining, finetuning solely on Bridge (7.2k pairs) yields lower generation and success metrics (success 72.6% vs 77.1% with pretraining). The paper does not report the number of environment interactions required to reach particular episodic-success thresholds in training curves.",
            "sample_complexity_gain": "No direct samples-to-threshold improvement reported; empirical gains from internet pretraining include an absolute increase of ~4.5 percentage points in the surrogate success metric on Bridge finetuning (72.6% -&gt; 77.1%), and improved generative quality (FID and FVD reductions). Pretraining enabled strong generalization with relatively small robotics finetuning data (7.2k).",
            "transfer_success_factors": "Key contributors: (1) semantic alignment via language-conditioned video generation (T5 embeddings capturing instruction structure), (2) large-scale web video/text pretraining capturing general motion and visual dynamics, (3) use of images as a universal state/action interface enabling cross-morphology reuse, (4) hierarchical temporal generation and first-frame conditioning/tiling for consistency, (5) a small inverse-dynamics module separating environment-agnostic planning from agent-specific control.",
            "transfer_failure_factors": "Limitations: diffusion sampling is slow; possible hallucinations in partially-observed settings; action execution depends on quality and amount of action-annotated videos (inverse dynamics trained on limited annotated data e.g., 20k); mismatch between photorealistic internet videos and robot embodiment leading to domain gap; no explicit reward optimization (imitation-style), and limited evaluation of sample-efficiency curves.",
            "key_findings": "Text-conditioned, internet-pretrained video diffusion models can serve as universal planners: pretraining on large web-scale text–video datasets plus T5 text embeddings improves video-plan realism and downstream task success after modest robotics finetuning (thousands of robot videos). Mapping generated frames to actions via a small inverse-dynamics model enables transfer to continuous joint-control robots, yielding substantially better combinatorial and multi-task generalization than baselines; internet pretraining yields measurable but modest absolute improvements in success and generative metrics and enables generalization to novel commands and scenes with limited task-specific finetuning.",
            "uuid": "e1855.0",
            "source_info": {
                "paper_title": "Learning Universal Policies via Text-Guided Video Generation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "VPT",
            "name_full": "Video PreTraining (VPT)",
            "brief_description": "A prior approach that learns to act by watching unlabeled online videos, aiming to leverage web video data for action learning.",
            "citation_title": "Video pretraining (vpt): Learning to act by watching unlabeled online videos.",
            "mention_or_use": "mention",
            "model_agent_name": "Video PreTraining (VPT)",
            "model_agent_description": "VPT learns action policies by leveraging large quantities of unlabeled online videos, typically using an inverse dynamics or action labeling approach to produce action supervision from videos.",
            "pretraining_data_type": "Unlabeled online videos (web videos) with captions/metadata",
            "pretraining_data_details": "Referenced as prior work; exact datasets and sizes are in the VPT paper (not provided in this paper).",
            "embodied_task_name": "Robotic control via learning from videos (as described in cited VPT work)",
            "embodied_task_description": "Not detailed in this paper; VPT is mentioned as an approach that can utilize internet-scale video data to learn acting policies.",
            "action_space_text": "N/A (VPT uses videos; may rely on inferred labels or action primitives depending on original work)",
            "action_space_embodied": "N/A (depends on VPT experimental setup in original paper)",
            "action_mapping_method": "Generally uses inverse-dynamics or action labeling to map visual observations in videos to actions (details in VPT paper).",
            "perception_requirements": "RGB video input; possibly audio/metadata",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Leverages abundant unlabeled video data to obtain behavioral priors and action mappings (as argued in referenced work).",
            "transfer_failure_factors": "Domain gap between web videos and robot embodiment; need for inverse-dynamics labeling; details in VPT paper.",
            "key_findings": "Mentioned as related work that uses internet videos to produce action supervision for control — consult the original VPT paper for quantitative transfer details.",
            "uuid": "e1855.1",
            "source_info": {
                "paper_title": "Learning Universal Policies via Text-Guided Video Generation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "DALL·E-Bot",
            "name_full": "DALL·E-Bot: Introducing web-scale diffusion models to robotics",
            "brief_description": "A prior work applying web-scale text-to-image diffusion models (e.g., DALL·E style) to robotics tasks, producing goal images to condition policies.",
            "citation_title": "Dall-e-bot: Introducing web-scale diffusion models to robotics.",
            "mention_or_use": "mention",
            "model_agent_name": "DALL·E-Bot (as cited)",
            "model_agent_description": "Uses web-scale text-conditioned image diffusion to synthesize goal images that can be used to guide robot policies (goal-image conditioning approach).",
            "pretraining_data_type": "Web-scale text–image datasets used to train image diffusion generators",
            "pretraining_data_details": "Not specified in this paper; the DALL·E-Bot reference should be consulted for dataset sizes and specifics.",
            "embodied_task_name": "Robotic planning conditioned on generated goal images",
            "embodied_task_description": "Involves generating goal images from text, then conditioning robotic policies on these goals; not detailed further in this paper.",
            "action_space_text": "Natural language prompts used to generate goal images",
            "action_space_embodied": "Robot control; specifics in the cited DALL·E-Bot paper",
            "action_mapping_method": "Generate goal image from text (diffusion) and use goal-conditioned policy to map to actions (details in cited work).",
            "perception_requirements": "Image observations and generated goal images",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Access to large web-scale image-text generative priors to create plausible goal conditions for policies.",
            "transfer_failure_factors": "Potential mismatch between generated goal images and robot embodiment; limited discussion here.",
            "key_findings": "Cited as related work showing how web-scale text-conditioned image diffusion models can be introduced into robotics; refer to the original DALL·E-Bot paper for full experimental details.",
            "uuid": "e1855.2",
            "source_info": {
                "paper_title": "Learning Universal Policies via Text-Guided Video Generation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "RT-1",
            "name_full": "RT-1: Robotics Transformer for Real-World Control at Scale",
            "brief_description": "A robotics transformer approach that conditions on vision and text to produce control outputs, demonstrating large-scale supervised learning for robot control.",
            "citation_title": "Rt-1: Robotics transformer for real-world control at scale.",
            "mention_or_use": "mention",
            "model_agent_name": "RT-1 (cited as baseline context)",
            "model_agent_description": "A transformer-based robotic policy that takes image and language inputs to output control actions; designed for real-world robot control with large-scale supervised training.",
            "pretraining_data_type": "Large-scale robot demonstration datasets with associated language instructions (as in RT-1 paper)",
            "pretraining_data_details": "Not provided in this paper; see the RT-1 paper for dataset sizes and collection details.",
            "embodied_task_name": "Real-world robot control tasks (RT-1 benchmark tasks in original work)",
            "embodied_task_description": "RT-1 learns to map image observations and language to robot actions for real tasks; specifics are in the RT-1 publication.",
            "action_space_text": "Natural language instructions specifying tasks/goals",
            "action_space_embodied": "Low-level robot actions/poses (RT-1 predicts robot control targets or primitives)",
            "action_mapping_method": "End-to-end transformer policy mapping visual+text inputs to control outputs (see RT-1 paper).",
            "perception_requirements": "RGB vision; possibly other sensors as used by RT-1",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Large-scale supervised robot data with language annotations enabling direct mapping from text to actions.",
            "transfer_failure_factors": "RT-1 operates in environments with shared action/state spaces for training and test; may not trivially generalize across morphologies — discussed as contrast in this paper.",
            "key_findings": "Mentioned as representative prior work that learns language-conditioned control policies directly in robot action spaces; differs from UniPi's image-as-universal-interface approach.",
            "uuid": "e1855.3",
            "source_info": {
                "paper_title": "Learning Universal Policies via Text-Guided Video Generation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "T5",
            "name_full": "T5 (Text-To-Text Transfer Transformer)",
            "brief_description": "A large pretrained text-to-text transformer used here to encode language instructions fed into the video diffusion planner.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
            "mention_or_use": "use",
            "model_agent_name": "T5-XXL (text encoder)",
            "model_agent_description": "Pretrained language model (T5-XXL, ~4.6B parameters) used to encode textual task descriptions into embeddings which condition the video diffusion planner.",
            "pretraining_data_type": "Large-scale natural language corpora (as per T5 pretraining; not re-specified in this paper)",
            "pretraining_data_details": "This paper uses pretrained T5-XXL to provide text embeddings; the original T5 paper provides dataset and pretraining details.",
            "embodied_task_name": "Used to condition UniPi on natural language for simulated and real robot tasks",
            "embodied_task_description": "T5 supplies semantic text encodings for goal-conditioned video generation across the simulated combinatorial tasks, CLIPort suite, and Bridge finetuning.",
            "action_space_text": "Natural language instructions encoded by T5",
            "action_space_embodied": "N/A (T5 encodes instructions; mapping to embodied actions is done via UniPi's video planner + inverse dynamics)",
            "action_mapping_method": "T5 embeddings condition the diffusion denoiser s(·|c,x0) (classifier-free guidance applied). The mapping to actions is indirect via generated frames then inverse dynamics.",
            "perception_requirements": "N/A (T5 handles text); used jointly with RGB video frames in UniPi.",
            "transfer_successful": true,
            "performance_with_pretraining": "T5-XXL used as conditioning input across UniPi and baselines; the paper reports that conditioning with pretrained language features is used throughout (no isolated ablation on T5 alone).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Pretrained language embeddings provide strong, generalizable instruction encodings which facilitate combinatorial generalization of video-conditioned planning.",
            "transfer_failure_factors": "Not isolated in experiments; potential limitations if text encoder fails to capture fine-grained task semantics.",
            "key_findings": "Using pretrained language embeddings (T5) is practical and standard for conditioning the video planner on natural language goals; contributes to UniPi's language generalization.",
            "uuid": "e1855.4",
            "source_info": {
                "paper_title": "Learning Universal Policies via Text-Guided Video Generation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Imagen Video (pretraining data/model)",
            "name_full": "Imagen Video: High Definition Video Generation with Diffusion Models",
            "brief_description": "State-of-the-art text-to-video diffusion work whose data and modeling approach underpin UniPi's internet-scale pretraining and architecture choices.",
            "citation_title": "Imagen video: High definition video generation with diffusion models.",
            "mention_or_use": "mention",
            "model_agent_name": "Imagen Video (data & modeling reference)",
            "model_agent_description": "High-fidelity text-to-video diffusion models and the public training data and techniques used in large text–video pretraining referenced by UniPi for its pretraining corpus and architectural templates.",
            "pretraining_data_type": "Large-scale web video-text pairs for text-to-video generation",
            "pretraining_data_details": "The paper states UniPi used the same data as [19] (Imagen Video): ~14M video-text pairs, 60M image-text pairs, plus LAION-400M; Imagen Video is the cited source for these resources and modeling practices.",
            "embodied_task_name": "Used as pretraining source for downstream robotic video planning tasks in UniPi",
            "embodied_task_description": "Provides the text–video generative prior that UniPi finetunes on small robot datasets to synthesize robot-relevant motion videos.",
            "action_space_text": "Natural language captions paired with web videos",
            "action_space_embodied": "N/A (Imagen Video itself is a generative model; UniPi maps generated frames to robot actions via inverse dynamics).",
            "action_mapping_method": "UniPi finetunes the Imagen-Video-style diffusion model to generate robot-relevant video conditioned on text and first-frame; action mapping is handled by an inverse-dynamics module in UniPi.",
            "perception_requirements": "RGB video frames",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Large, diverse web video–text corpora provide motion priors and visual semantics that help synthesize plausible robot plan videos after modest finetuning.",
            "transfer_failure_factors": "Domain mismatch between web videos and robot embodiment; not quantified here.",
            "key_findings": "UniPi explicitly leverages Imagen-Video-scale data and modeling to obtain a generative prior for robot video planning and demonstrates that internet pretraining improves downstream generation quality and task success after finetuning.",
            "uuid": "e1855.5",
            "source_info": {
                "paper_title": "Learning Universal Policies via Text-Guided Video Generation",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Video pretraining (vpt): Learning to act by watching unlabeled online videos.",
            "rating": 2
        },
        {
            "paper_title": "Dall-e-bot: Introducing web-scale diffusion models to robotics.",
            "rating": 2
        },
        {
            "paper_title": "Imagen video: High definition video generation with diffusion models.",
            "rating": 2
        },
        {
            "paper_title": "Planning with diffusion for flexible behavior synthesis.",
            "rating": 2
        },
        {
            "paper_title": "Rt-1: Robotics transformer for real-world control at scale.",
            "rating": 2
        }
    ],
    "cost": 0.018987749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Universal Policies via Text-Guided Video Generation</h1>
<p>Yilun $\mathbf{D u}{ }^{<em> \dagger \S}$, Mengjiao Yang ${ }^{</em> \ddagger \S}$, Bo Dai ${ }^{\S}$, Hanjun Dai ${ }^{\S}$, Ofir Nachum ${ }^{\S}$, Joshua B. Tenenbaum ${ }^{\dagger}$, Dale Schuurmans ${ }^{\S}$, Pieter Abbeel ${ }^{\S}$<br>MIT ${ }^{\dagger} \quad$ Google DeepMind ${ }^{\S} \quad$ UC Berkeley ${ }^{\S} \quad$ Georgia Tech ${ }^{\S} \quad$ University of Alberta ${ }^{\S}$<br>https://universal-policy.github.io/</p>
<h4>Abstract</h4>
<p>A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks. Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Specifically, we cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video. By leveraging text as the underlying goal specification, we are able to naturally and combinatorially generalize to novel goals. The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images, which, for example, enables learning and generalization across a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and widely available videos from the internet, the approach enables knowledge transfer through predicting highly realistic video plans for real robots ${ }^{2}$.</p>
<h2>1 Introduction</h2>
<p>Building models that solve a diverse set of tasks has become a dominant paradigm in the domains of vision and language. In natural language processing, large pretrained models have demonstrated remarkable zero-shot learning of new language tasks [1, 2, 3]. Similarly, in computer vision, models such as those proposed in [4, 5] have shown remarkable zero-shot classification and object recognition capabilities. A natural next step is to use such tools to construct agents that can complete different decision making tasks across many environments.</p>
<p>However, training such agents faces the inherent challenge of environmental diversity, since different environments operate with distinct state action spaces (e.g., the joint space and continuous controls in MuJoCo are fundamentally different from the image space and discrete actions in Atari). Such diversity hampers knowledge sharing, learning, and generalization across tasks and environments. Although substantial effort has been devoted to encoding different environments with universal tokens in a sequence modeling framework [6], it is unclear whether such an approach can preserve the rich knowledge embedded in pretrained vision and language models and leverage this knowledge to transfer to downstream reinforcement learning (RL) tasks. Furthermore, it is difficult to construct reward functions that specify different tasks across environments.</p>
<p>In this work, we address the challenges in environment diversity and reward specification by leveraging video (i.e., image sequences) as a universal interface for conveying action and observation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Text-Conditional Video Generation as Universal Policies. Text-conditional video generations enables us to train general purpose policies on wide sources of data (simulated, real robots and YouTube) which may be applied to downstream multi-task settings requiring combinatorical language generalization, long-horizon planning, or internet-scale knowledge.
behavior in different environments, and text as a universal interface for expressing task descriptions. In particular, we design a video generator as a planner that sequentially conditions on a current image frame and a text passage describing a current goal (i.e., the next high-level step) to generate a trajectory in the form of an image sequence, after which an inverse dynamics model is used to extract the underlying actions from the generated video. Such an approach allows the universal nature of language and video to be leveraged in generalizing to novel goals and tasks across diverse environments. Specifically, we instantiate the text-conditioned video generation model using video diffusion. A set of underlying actions are then regressed from the synthesized frames and used to construct a policy to implement the planned trajectory. Since the language description of a task is often highly correlated with control actions, text-conditioned video generation naturally focuses generation on action relevant parts of the video. The proposed model, UniPi, is visualized in Figure 1.</p>
<p>We have found that formulating policy generation via text-conditioned video synthesis yields the following advantages:</p>
<p>Combinatorial Generalization. The rich combinatorial nature of language can be leveraged to synthesize novel combinatorial behaviors in the environment. This enables the proposed approach to rearrange objects to new unseen combinations of geometric relations, as shown in Section 4.1.</p>
<p>Multi-task Learning. Formulating action prediction as a video prediction problem readily enables learning across many different tasks. We illustrate in Section 4.2 how this enables learning across language-conditioned tasks and generalizing to new ones at test time without finetuning.</p>
<p>Action Planning. The video generation procedure corresponds to a planning procedure where a sequence of frames representing actions is generated to reach the target goal. Such a planning procedure is naturally hierarchical: a temporally sparse sequence of images toward a goal can first be generated, before being refined with a more specific plan. Moreover, the planning procedure is steerable, in the sense that the plan generation can be biased by new constraints introduced at test-time through test-time sampling. Finally, plans are produced in a video space that is naturally interpretable by humans, making action verification and plan diagnosis easy. We illustrate the efficacy of hierarchical sampling in Table 2 and steerability in Figure 7.</p>
<p>Internet-Scale Knowledge Transfer. By pretraining a video generation model on a large-scale text-video dataset recovered from the internet, one can recover a vast repository of "demonstrations" that aid the construction of a text-conditioned policy in novel environments. We illustrate how this enables the realistic synthesis of robot motion videos from language instructions in Section 4.3.</p>
<p>The main contribution of this work is to formulate text-conditioned video generation as a universal planning strategy from which diverse behaviors can be synthesized. While such an approach departs from typical policy generation in RL, where subsequent actions to execute are directly predicted from a current state, we illustrate that UniPi exhibits notable generalization advantages over traditional policy generation methods across a variety of domains.</p>
<h1>2 Problem Formulation</h1>
<p>We first motivate a new abstraction, the Unified Predictive Decision Process (UPDP), as an alternative to the Markov Decision Process (MDP) commonly used in RL, and then show an instantiation of a UPDP with diffusion models.</p>
<h1>2.1 Markov Decision Process</h1>
<p>The Markov Decision Process [7] is a broad abstraction used to formulate many sequential decision making problems. Many RL algorithms have been derived from MDPs with empirical success [8, 9, 10], but existing algorithms are typically unable to combinatorially generalize across different environments. Such difficulty can be traced back to certain aspects of the underlying MDP abstraction:
i) The lack of a universal state interface across different control environments. In fact, since different environments typically have separate underlying state spaces, one would need to a construct a complex state representation to represent all environments, making learning difficult.
ii) The explicit requirement of a real-valued reward function in an MDP. The RL problem is usually defined as maximizing the accumulated reward in an MDP. However, in many practical applications, how to design and transfer rewards is unclear and different across environments.
iii) The dynamics model in an MDP is environment and agent dependent. Specifically, the dynamics model $T(s^{\prime} \mid s, a)$ that characterizes the transition between states $\left(s, s^{\prime}\right)$ under action $a$ is explicitly dependent to the environment and action space of the agent, which can be significantly different between different agents and tasks.</p>
<h3>2.2 Unified Predictive Decision Process</h3>
<p>These difficulties inspire us to construct an alternative abstraction for unified sequential decision making across diverse environments. Our abstraction, termed Unified Predictive Decision Process (UPDP), exploits images as a universal interface across environments, texts as task specifiers to avoid reward design, and a task-agnostic planning module separated from environment-dependent control to enable knowledge sharing and generalization.
Formally, we define a UPDP to be a tuple $\mathcal{G}=\langle\mathcal{X}, \mathcal{C}, H, \rho\rangle$, where $\mathcal{X}$ denotes the observation space of images, $\mathcal{C}$ denotes the space of textual task descriptions, $H \in \mathcal{N}$ is a finite horizon length, and $\rho\left(\cdot \mid x_{0}, c\right): \mathcal{X} \times \mathcal{C} \rightarrow \Delta\left(\mathcal{X}^{H}\right)$ is a conditional video generator. That is, $\rho\left(\cdot \mid x_{o}, c\right) \in \Delta\left(\mathcal{X}^{H}\right)$ is a conditional distribution over $H$-step image sequences determined by the first frame $x_{0}$ and the task description $c$. Intuitively, $\rho$ synthesizes $H$-step image trajectories that illustrate possible paths for completing a target task $c$. For simplicity, we focus on finite horizon, episodic tasks.
Given a UPDP $\mathcal{G}$, we define a trajectory-task conditioned policy $\pi\left(\cdot \mid\left{x_{h}\right}<em h="h">{h=0}^{H}, c\right): \mathcal{X}^{H+1} \times \mathcal{C} \rightarrow$ $\Delta\left(\mathcal{A}^{H}\right)$ to be a conditional distribution over $H$-step action sequences $\mathcal{A}^{H}$. Ideally, $\pi\left(\cdot \mid\left{x</em>\right}<em h="h">{h=0}^{H}, c\right)$ specifies a conditional distribution of action sequences that achieves the given trajectory $\left{x</em>\right}<em i="i">{h=0}^{H}$ in the UPDP $\mathcal{G}$ for the given task $c$. To achieve such an alignment, we will consider an offline RL scenario where we have access to a dataset of existing experience $\mathcal{D}=\left{\left(x</em>\right)}, a_{i<em H="H">{i=0}^{H-1}, x</em>, c\right}<em 0="0">{j=1}^{n}$ from which both $\rho\left(\cdot \mid x</em>\right}}, c\right)$ and $\pi\left(\cdot \mid\left{x_{h<em 0="0">{h=0}^{H}, c\right)$ can be estimated.
In contrast to an MDP, a UPDP directly models video-based trajectories and bypasses the need to specify a reward function beyond the textual task description. Since the space of video observations $\mathcal{X}^{H}$ and task descriptions $\mathcal{C}$ are both naturally shared across environments and easily interpretable by humans, any video-based planner $\rho\left(\cdot \mid x</em>, c\right)$. This design choice isolates planning decisions from action-specific mechanisms, allowing the planner to be environment and agent agnostic.
UPDP can be understood as implicitly planning over an MDP and directly outputting an optimal trajectory based on the given instructions. Such a UPDP abstraction bypasses reward design, state extraction and explicit planning, and allows for non-Markovian modeling of an image-based state space. However, learning a planner in UPDP requires videos and task descriptions, whereas traditional MDPs do not require such data, so whether an MDP or UPDP is more suitable for a given task depends on what types of training data are available. Although the non-Markovian model and the requirement of video and text data induce additional difficulties in UPDP comparing to MDP, it is possible to leverage existing large text-video models that have been pretrained on massive, web-scale datasets to alleviate these complexities.}, c\right)$ can be conveniently reused, transferred and debugged. Another benefit of a UPDP over an MDP is that UPDP isolates the video-based planning with $\rho\left(\cdot \mid x_{0}, c\right)$ from the deferred action selection using $\pi\left(\cdot \mid\left{x_{h}\right}_{h=0}^{H</p>
<h3>2.3 Diffusion Models for UPDP</h3>
<p>Let $\tau=\left[x_{1}, \ldots, x_{H}\right] \in \mathcal{X}^{H}$ denote a sequence of images. We leverage the significant recent advances in diffusion models for capturing the conditional distribution $\rho\left(\tau \mid x_{0}, c\right)$, which we will</p>
<p>leverage as a text and initial-frame conditioned video generator in a UPDP. We emphasize that the UPDP formulation is also compatible with other probabilistic models, such as a variational autoencoder [11], energy-based model [12, 13], or generative adversarial network [14]. For completeness we briefly cover the core formulation at a high-level, but defer details to background references [15].</p>
<p>We start with an unconditional model. A continuous-time diffusion model defines a forward process $q_{k}\left(\tau_{k} \mid \tau\right)=\mathcal{N}\left(\cdot ; \alpha_{k} \tau, \sigma_{k}^{2} I\right)$, where $k \in[0,1]$ and $\alpha_{k}, \sigma_{k}^{2}$ are scalars with predefined schedules. A generative process $p(\tau)$ is also defined, which reverses the forward process by learning a denoising model $s\left(\tau_{k}, k\right)$. Correspondingly $\tau$ can be generated by simulating this reverse process with an ancestral sampler [16] or numerical integration [17]. In our case, the unconditional model needs to be further adapted to condition on both the text instruction $c$ and the initial image $x_{0}$. Denote the conditional denoiser as $s\left(\tau_{k}, k \mid c, x_{0}\right)$. We leverage classifier-free guidance [18] and use $\hat{s}\left(\tau_{k}, k \mid c, x_{0}\right)=(1+\omega) s\left(\tau_{k}, k \mid c, x_{0}\right)-\omega s\left(\tau_{k}, k\right)$ as the denoiser in the reverse process for sampling, where $\omega$ controls the strength of the text and first-frame conditioning.</p>
<h1>3 Decision Making with Videos</h1>
<p>Next we describe the proposed approach UniPi in detail, which is a concrete instantiation of the diffusion UPDP. UniPi incorporates the two main components discussed in Section 2, as shown in Figure 2: (i) a diffusion model for the universal videobased planner $\rho\left(\cdot \mid x_{0}, c\right)$, which synthesizes videos conditioned on the first frame and task descriptions; and (ii) a task-specific action generator $\pi\left(\cdot \mid\left{x_{h}\right}_{h=0}^{H}, c\right)$, which infers actions sequences from generated videos through inverse dynamics modeling.</p>
<h3>3.1 Universal Video-Based Planner</h3>
<p>Encouraged by the recent success of text-to-video models [19], we seek to construct a video diffusion module as the trajectory
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Given an input observation and text instruction, we plan a set of images representing agent behavior. Images are converted to actions using an inverse dynamics model.
planner, which can faithfully synthesize future image frames given an initial frame and textual task description. However, the desired planner departs from the typical setting in text-to-video models $[20,19]$ which normally generate unconstrained videos given a text description. Planning through video generation is more challenging as it requires models to both be able to generate constrained videos that start at a specified image, and then complete the target task. Moreover, to ensure valid action inference across synthesized frames in a video, the video prediction module needs to be able to track the underlying environment state across synthesized video frames.</p>
<p>Conditional Video Synthesis. To generate a valid and executable plan, a text-to-video model must synthesize a constrained video plan starting at an initial image that depicts the initial configuration of the agent and environment. One approach to solve this problem is to modify the underlying test-time sampling procedure of an unconditional model, by fixing the first frame of the generated video plan to always begin at the observed image, as done in [21]. However, we found that this performed poorly and led to subsequent frames in the video plan to deviate significantly from the original observed image. Instead, we found it more effective to explicitly train a constrained video synthesis model by providing the first frame of each video as explicit conditioning context during training.</p>
<p>Trajectory Consistency through Tiling. Existing text-to-video models typically generate videos where the underlying environment state changes significantly during the temporal duration [19]. To construct an accurate trajectory planner, it is important that the environment remain consistent across all time points. To enforce environment consistency in conditional video synthesis, we provide, as additional context, the observed image when denoising each frame in the synthesized video. In particular, we re-purpose a temporal super-resolution video diffusion architecture, and provide as context the conditioned visual observation tiled across time, as the opposed to a low temporal-resolution video for denoising at each timestep. In this model, we directly concatenate each</p>
<p>intermediate noisy frame with the conditioned observed image across sampling steps, which serves as a strong signal to maintain the underlying environment state across time.</p>
<p>Hierarchical Planning. When constructing plans in high dimensional environments with long time horizons, directly generating a set of actions to reach a goal state quickly becomes intractable due to the exponential blow-up of the underlying search space. Planning methods often circumvent this issue by leveraging a natural hierarchy in planning. Specifically, planning methods first construct coarse plans operating on low dimensional states and actions, which may then be refined into plans in the underlying state and action spaces. Similar to planning, our conditional video generation procedure likewise exhibits a natural temporal hierarchy. We first generate videos at a coarse level by sparsely sampled videos ("abstractions") of the desired behavior along the time axis. Then we refine the videos to represent valid behavior in the environment by super-resolving videos across time. Meanwhile, coarse-to-fine super-resolution further improves consistency via interpolation between frames.</p>
<p>Flexible Behavioral Modulation. When planning a sequence of actions to a given sub-goal, one can readily incorporate external constraints to modulate the generated plan. Such test-time adaptability can be implemented by composing a prior $h(\tau)$ during plan generation to specify desired constraints across the synthesized action trajectory [21], which is also compatible with UniPi. In particular, the prior $h(\tau)$ can be specified using a learned classifier on images to optimize a particular task, or as a Dirac delta on a particular image to guide a plan towards a particular set of states. To train the text-conditioned video generation model, we utilize the video diffusion algorithm in [19], where pretrained language features from T5 [22] are encoded. Please see Appendix A for the underlying architecture and training details.</p>
<h1>3.2 Task Specific Action Adaptation</h1>
<p>Given a set of synthesized videos, we may train a small task-specific inverse-dynamics model to translate frames into a set of actions as described below.</p>
<p>Inverse Dynamics. We train a small model to estimate actions given input images. The training of the inverse dynamics is independent from the planner and can be done on a separate, smaller and potentially suboptimal dataset generated by a simulator.</p>
<p>Action Execution. Finally, we generate an action sequence given $x_{0}$ and $c$ by synthesizing $H$ image frames and applying the learned inverse-dynamics model to predict the corresponding $H$ actions. Inferred actions can then be executed via closed-loop control, where we generate $H$ new actions after each step of action execution (i.e., model predictive control), or via open-loop control, where we sequentially execute each action from the intially inferred action sequence. For computational efficiency, we use an open-loop controller in all our experiments in this paper.</p>
<h2>4 Experimental Evaluation</h2>
<p>The focus of these experiments is to evaluate UniPi in terms of its ability to enable effective, generalizable decision making. In particular, we evaluate
(1) the ability to combinatorially generalize across different subgoals in Section 4.1,
(2) the ability to effectively learn and generalize across many tasks in Section 4.2,
(3) the ability to leverage existing videos on the internet to generalize to complex tasks in Section 4.3.</p>
<p>See experimental details in Appendix A. Additional results are given in Appendix B and videos in the supplement.</p>
<h3>4.1 Combinatorial Policy Synthesis</h3>
<p>First, we measure the ability of UniPi to combinatorially generalize to different language tasks.
Setup. To measure combinatorial generalization, we use the combinatorial robot planning tasks in [23]. In this task, a robot must manipulate blocks in an environment to satisfy language instructions, i.e., put a red block right of a cyan block. To accomplish this task, the robot must first pick up a white block, place it in the appropriate bowl to paint it a particular color, and then pick up and place the block in a plate so that it satisfies the specified relation. In contrast to [23] which uses</p>
<p>|  | Seen |  |  | Novel |  |
| Model | Place | Relation |  | Place | Relation |
| --- | --- | --- | --- | --- | --- |
| State + Transformer BC [24] | $19.4 \pm 3.7$ | $8.2 \pm 2.0$ |  | $11.9 \pm 4.9$ | $3.7 \pm 2.1$ |
| Image + Transformer BC [24] | $9.4 \pm 2.2$ | $11.9 \pm 1.8$ |  | $9.7 \pm 4.5$ | $7.3 \pm 2.6$ |
| Image + TT [25] | $17.4 \pm 2.9$ | $12.8 \pm 1.8$ |  | $13.2 \pm 4.1$ | $9.1 \pm 2.5$ |
| Diffuser [21] | $9.0 \pm 1.2$ | $11.2 \pm 1.0$ |  | $12.5 \pm 2.4$ | $9.6 \pm 1.7$ |
| UniPi (Ours) | $\mathbf{5 9 . 1} \pm 2.5$ | $\mathbf{5 3 . 2} \pm 2.0$ |  | $\mathbf{6 0 . 1} \pm 3.9$ | $\mathbf{4 6 . 1} \pm 3.0$ |</p>
<p>Table 1: Task Completion Accuracy in Combinatorial Environments. UniPi generalizes to seen and novel combinations of language prompts in Place (e.g., place X in Y ) and Relation (e.g., place X to the left of Y ) tasks.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Combinatorial Video Generation. Generated videos for unseen language goals at test time.
pre-programmed pick and place primitives for action prediction, we predict actions in the continuous robotic joint space for both the baselines and our approach.</p>
<p>We split the language instructions in this environment into two sets: one set of instructions ( $70 \%$ ) that is seen during training, and another set ( $30 \%$ ) that is only seen during testing. The precise locations of individual blocks, bowls, and plates in the environment are fully randomized in each environment iteration. We train the video model on 200k example videos of generated language instructions in the train set. Details of this environment can be found in Appendix A. We constructed demonstrations of videos in this task by using a scripted agent.</p>
<p>Baselines. We compare the proposed approach with three separate representative approaches. First, we compare to existing work that uses goal-conditioned transformers to learn across multiple environments, where goals can be specified as episode returns [26], expert demonstrations [6], or text and images [24]. To represent these baselines, we construct a transformer behavior cloning (BC) agent to predict the subsequent action to execute given the task description and either the visual observation (Image + Transformer BC) or the underlying robot
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Action Execution. Synthesized video plans and executed actions in the simulated environment. The two video plans roughly align with each other.
joint state (State + Transformer BC). Second, given that our approach regresses a sequence of actions to execute, we further compare with transformer models that regress a sequence of future actions to execute, similar to the goal-conditioned behavioral cloning of the Trajectory Transformer [25] (Image + TT). Finally, to highlight the importance of the video-as-policy approach, we compare UniPi with learning a diffusion process that, conditioned on an image observation, directly infers future robot actions in the joint space (as opposed to diffusing future image frames), corresponding to [21, 27]. For both our method and each baseline, we condition the policy on encoded language instructions using pretrained T5 embeddings. Note that in this setting, existing offline reinforcement learning baselines are not directly applicable as we do not have access to the reward functions in the environment.
Metrics. To compare UniPi with baselines, we measure final task completion accuracy across new instances of the environment and associated language prompts. We subdivide the evaluation along two axes: (1) whether the language instruction has been seen during training and (2) whether the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Multitask Video Generation. Generated video plans on new test tasks in the multitask setting. UniPi is able to synthesize plan across a set of environments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Frame <br> Condition</th>
<th style="text-align: center;">Frame <br> Consistency</th>
<th style="text-align: center;">Temporal <br> Heirarchy</th>
<th style="text-align: center;">Place</th>
<th style="text-align: center;">Relation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">$13.2 \pm 3.2$</td>
<td style="text-align: center;">$12.4 \pm 2.4$</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">$52.4 \pm 2.9$</td>
<td style="text-align: center;">$34.7 \pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">$53.2 \pm 3.0$</td>
<td style="text-align: center;">$39.4 \pm 2.8$</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">$\mathbf{5 9 . 1} \pm 2.5$</td>
<td style="text-align: center;">$\mathbf{5 3 . 2} \pm 2.0$</td>
</tr>
</tbody>
</table>
<p>Table 2: Task Completion Accuracy Ablations. Each component of UniPi improves its performance. Performance reported on the seen place and relation tasks.
language instruction specifies placing a block in relation to some other block as opposed to direct pick-and-place.
Combinatorial Generalization. In Table 1, we find that UniPi generalizes well to both seen and novel combinations of language prompts . We illustrate our action generation pipeline in Figure 4 and different generated video plans using our approach in Figure 3.</p>
<p>Ablations. In Table 2, we ablate UniPi on seen language instructions and in-relation-to tasks. Specifically, we study the effect of conditioning the video generative model on the first observation frame (frame condition), tiling the observed frame across timesteps (frame consistency) and super-resolving video generation across time (temporal hierarchy). In settings where frame consistency is not enforced, we provide a zeroed out image as context to the non-start frames in a video. We found that all components of UniPi
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Adaptable Planning. By guiding test-time sampling towards a an intermediate image, we can adapt our planning procedure to move a particular block.
are crucial for good performance. We found that frame conditioning and consistency enabled videos to be consistent with the observed image and temporal hierarchy enabled more detailed video plans.
Adaptability. We next assess the ability of UniPi to adapt at test time to new constraints. In Figure 7, we illustrate the ability to construct plans which color and move one particular block to a specified geometric relation.</p>
<h1>4.2 Multi-Environment Transfer</h1>
<p>We next evaluate the ability of UniPi to effectively learn across a set of different tasks and generalize, at test time, to a new set of unseen environments.
Setup. To measure multi-task learning and transfer, we use the suite of language guided manipulation tasks from [28]. We train our method using demonstrations across a set of 10 separate tasks from [28], and evaluate the ability of our approach to transfer to 3 different test tasks. Using a scripted oracle agent, we generate a set of 200 k videos of language execution in the environment. We report the underlying accuracy in which each language instruction is completed. Details of this environment can be found in Appendix A.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: High Fidelity Plan Generation. UniPi can generate high resolution video plans across different language prompts.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Model</td>
<td>Place <br> Bowl</td>
<td>Pack <br> Object</td>
<td>Pack <br> Pair</td>
</tr>
<tr>
<td>State + Transformer BC</td>
<td>$9.8 \pm 2.6$</td>
<td>$21.7 \pm 3.5$</td>
<td>$1.3 \pm 0.9$</td>
</tr>
<tr>
<td>Image + Transformer BC</td>
<td>$5.3 \pm 1.9$</td>
<td>$5.7 \pm 2.1$</td>
<td>$7.8 \pm 2.6$</td>
</tr>
<tr>
<td>Image + TT</td>
<td>$4.9 \pm 2.1$</td>
<td>$19.8 \pm 0.4$</td>
<td>$2.3 \pm 1.6$</td>
</tr>
<tr>
<td>Diffuser</td>
<td>$14.8 \pm 2.9$</td>
<td>$15.9 \pm 2.7$</td>
<td>$10.5 \pm 2.4$</td>
</tr>
<tr>
<td>UniPi (Ours)</td>
<td>$\mathbf{5 1 . 6} \pm 3.6$</td>
<td>$\mathbf{7 5 . 5} \pm 3.1$</td>
<td>$\mathbf{4 5 . 7} \pm 3.7$</td>
</tr>
</tbody>
</table>
<p>Table 3: Task Completion Accuracy on Multitask Environment. UniPi generalizes well to new environments when trained on a set of different multi-task environments.</p>
<p>Baselines. We use the same baseline methods as in Section 4.1. While our environment setting is similar to that of [28], this method is not directly comparable to our approach, as CLIPort abstracts actions to the existing primitives of pick and place as opposed to using the joint space of a robot. CLIPort is also designed to solve the significantly simpler problem of inferring only the poses upon which to pick and place objects (with no easy manner to adapt to our setting).</p>
<p>Multitask Generalization. In Table 3 we present results of UniPi and baselines across new tasks. The UniPi approach is able to generalize and synthesize new videos and decisions of different language tasks, and can generate videos consisting of picking different kinds of objects and different colored objects. We further present video visualizations of our approach in Figure 5.</p>
<h1>4.3 Real World Transfer</h1>
<p>Finally we evaluate the extent to which UniPi can generalize to real world scenarios and construct complex behaviors by leveraging widely available videos on the internet.</p>
<p>Setup. Our training data consists of an internet-scale pretraining dataset and a smaller real-world robotic dataset. The pretraining dataset uses the same data as [19], which consists of 14 million video-text pairs, 60 million image-text pairs, and the publicly available LAION-400M image-text dataset. The robotic dataset is adopted from the Bridge dataset [29] with 7.2 k video-text pairs, where we use the task IDs as texts. We partition the 7.2 k video-text pairs into train ( $80 \%$ ) and test ( $20 \%$ ) splits. We pretrain UniPi on the pretraining dataset followed by finetuning on the train split of the Bridge data. Architectural details can be found in Appendix A.</p>
<p>Video Synthesis. We are particularly interested in the effect of pretraining on internet-scale video data that is not specific to robotics. We report the CLIP scores, FIDs, and FVDs (averaged across frames and computed on 32 samples) of UniPi trained on Bridge data, with and without pretraining. As shown in Table 4, UniPi with pretraining achieves significantly higher FID and FVD and a marginally better CLIP score than UniPi without pretraining, suggesting that pretraining on non-robot data helps with generating plans for robots. Interestingly, UniPi without pretraining often synthesizes plans that fail to complete the task (Figure 6), which is not well reflected in the CLIP</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Pretraining Enables Combinatorial Generaliza- Figure 9: Robustness to Background Change. tion. Internet pretraining enables UniPi to synthesize videos UniPi learns to be robust to changes of underlying of tasks not seen in training. In contrast, a model trained background, such as black cropping or the addition from scratch incorrectly generates plans of different tasks. of photo-shopped objects.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model (24x40)</th>
<th style="text-align: center;">CLIP Score $\uparrow$</th>
<th style="text-align: center;">FID $\downarrow$</th>
<th style="text-align: center;">FVD $\downarrow$</th>
<th style="text-align: center;">Success $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No Pretrain</td>
<td style="text-align: center;">$24.43 \pm 0.04$</td>
<td style="text-align: center;">$17.75 \pm 0.56$</td>
<td style="text-align: center;">$288.02 \pm 10.45$</td>
<td style="text-align: center;">$72.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Pretrain</td>
<td style="text-align: center;">$\mathbf{2 4 . 5 4} \pm 0.03$</td>
<td style="text-align: center;">$\mathbf{1 4 . 5 4} \pm 0.57$</td>
<td style="text-align: center;">$\mathbf{2 6 4 . 6 6} \pm 13.64$</td>
<td style="text-align: center;">$\mathbf{7 7 . 1 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Video Generation Quality of UniPi on Real Environment. The use of existing data on the internet improves video plan predictions under all metrics considered.
score, suggesting the need for better generation metrics for control-specific tasks. To tackle the lack of such a metric, we develop a surrogate metric for evaluating task success from the generated videos. Specifically, we train a success classifier that takes in the last frame of a generated video and predicts whether the task is successful or not. We find that training a model from scratch achieves $72.6 \%$ success while finetuning from a pretrained model improves performance to $77.1 \%$. In both settings, generated videos are able to successfully complete most tasks.
Generalization. We find that internet-scale pretraining enables UniPi to generalize to novel task commands and scenes in the test split not seen during training, whereas UniPi trained only on taskspecific robot data fails to generalize. Specifically, Figure 8 shows the results of novel task commands that do not exist in the Bridge dataset. Additionally, UniPi is relatively robust to background changes such as black cropping or the addition of photo-shopped objects as shown in Figure 9.</p>
<h1>5 Related Work</h1>
<p>Learning Generative Models of the World. Models trained to generate environment rewards and dynamics that can serve as "world models" for model-based reinforcement learning and planning have been recently scaled to large-scale architectures developed for vision and language [9, 25, 30, 31]. These works separate learning the world model from planning and policy learning, and arguably present a mismatch between the generative modeling objective of the world and learning optimal policies. Additionally, learning a world model requires the training data to be in a strict state-actionreward format, which is incompatible with the largely available datasets on the internet, such as YouTube videos. While methods such as VPT [32] can utilize internet-scale data through learning an inverse dynamics model to label unlabled videos, an inverse dynamics model itself does not support model-based planning or reinforcement learning to further improve learned policies beyond imitation learning. Our text-conditioned video policies can be seen as jointly learning the world model and conducting hierarchical planning simultaneously, and is able to leverage widely available datasets that are not specifically designed for sequential decision making.
Diffusion Models for Decision Making. Diffusion models have recently been applied to different decision making problems [21, 27, 33, 34, 35, 36]. Most similar to this work, [21] trained an unconditional diffusion model to generate trajectories consisting of joint-based states and actions, and used a separately trained reward model to select generated plans. On the other hand, [27] trained a conditional diffusion model to guide behavior synthesis from desired rewards, constraints or agent skills. Unlike both works, which learn task-specific policies from scratch, our approach of text-condition video generation as a universal policy can leverage internet-scale knowledge to learn generalist agents that can be deployed to a variety of novel tasks and environments. Additionally, [37]</p>
<p>applied web-scale text-conditioned image diffusion to generate a goal image to condition a policy on, whereas our work uses video diffusion to learning universal policies directly.</p>
<p>Learning Generalist Agents. Inspired by the success of large-scale pretraining in vision and language domains, large-scale sequence and image models have recently been applied to learning generalist decision making agents [6, 26, 38]. However, these generalist agents can only operate under environments with the same state and action spaces (e.g., Atari games) [26, 38], or require studious tokenization [6] that might seem unnatural in scenarios where different environments have distinct state and actions spaces. Another downside of using customized tokens for control is the inability to directly utilize knowledge from pretrained vision and language models. Our approach, on the other hand, uses text and images as universal interfaces for policy learning so that the knowledge from pretrained vision and language models can be preserved. The choice of diffusion as opposed to autoregressive sequence modeling also enables long-term and hierarchical planning.</p>
<p>Learning Text-Conditioned Policies. There has been a growing amount of work using text commands as a way to learn multi-task and generalist control policies [39, 40, 24, 41, 42, 43]. Different from our framing of video-as-policies, existing work directly trains a language-conditioned control policy in the action space of some specific robot, leaving cross-morphology multi-environment learning of generalist agents as an unsolved problem. We believe this paper is the first to propose images as a universal state and action space to enable broad knowledge transfer across environments, tasks, and even between humans and robots.</p>
<h1>6 Conclusion</h1>
<p>We have demonstrated the utility of representing policies using text-conditioned video generation, showing that this enables effective combinatorial generalization, multi-task learning, and real world transfer. These positive results point to the broader direction of using generative models and the wealth of data on the internet as powerful tools to generate general-purpose decision making systems.</p>
<p>Limitations. Our current approach has several limitations. First, the underlying video diffusion process can be slow, it can take a minute to generate highly photorealistic videos. This slowness can be overcome by distilling the diffusion process into a faster sampling network [44], which in our initial experimentation resulted in a 16x speed-up. UniPi may further be sped up with by faster speed samplers in diffusion models. Second, the environments considered in this work are generally fully observed. In partially observable environments, video diffusion models might make hallucination of objects or movements that are unfaithful or not in the physical world. Integrating video models with semantic knowledge about the world may help resolve this issue, and the integration of UniPi with LLMs would be an interesting direction of future work.</p>
<h2>References</h2>
<p>[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 1
[2] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 1
[3] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. 1
[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021. 1
[5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: A visual language model for few-shot learning. NeurIPS, 2022. 1</p>
<p>[6] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022. 1, 6, 10, 14
[7] Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley \&amp; Sons, Inc., 1994. 3
[8] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861-1870. PMLR, 2018. 3
[9] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. 3, 9
[10] Tianjun Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo Dai. Making linear mdps practical via contrastive representation learning. In International Conference on Machine Learning, pages 26447-26466. PMLR, 2022. 3
[11] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013. 4
[12] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689, 2019. 4
[13] Bo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, and Dale Schuurmans. Exponential family estimation via adversarial dynamics embedding. Advances in Neural Information Processing Systems, 32, 2019. 4
[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems. 2014. 4
[15] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, 2015. 4
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. 4
[17] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 4
[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 4
[19] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 4, 5, 8,14
[20] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 4
[21] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, 2022. 4, 5, 6, 9, 14
[22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020. 5, 14
[23] Jiayuan Mao, Tomas Lozano-Perez, Joshua B. Tenenbaum, and Leslie Pack Kaelbing. PDSketch: Integrated Domain Programming, Learning, and Planning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 5
[24] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 6, 10</p>
<p>[25] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:12731286, 2021. 6, 9, 14
[26] Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. arXiv preprint arXiv:2205.15241, 2022. 6, 10, 14
[27] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? arXiv preprint arXiv:2211.15657, 2022. 6, 9
[28] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pages 894-906. PMLR, 2022. 7, 8
[29] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396, 2021. 8
[30] Michael R Zhang, Thomas Paine, Ofir Nachum, Cosmin Paduraru, George Tucker, ziyu wang, and Mohammad Norouzi. Autoregressive dynamics models for offline policy evaluation and optimization. In International Conference on Learning Representations, 2021. 9
[31] Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world models. arXiv preprint arXiv:2209.00588, 2022. 9
[32] Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv preprint arXiv:2206.11795, 2022. 9
[33] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022. 9
[34] Edwin Zhang, Yujie Lu, William Wang, and Amy Zhang. Lad: Language augmented diffusion for reinforcement learning. arXiv preprint arXiv:2210.15629, 2022. 9
[35] Julen Urain, Niklas Funk, Georgia Chalvatzaki, and Jan Peters. Se (3)-diffusionfields: Learning cost functions for joint grasp and motion optimization through diffusion. arXiv preprint arXiv:2209.03855, 2022. 9
[36] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulation. arXiv preprint arXiv:2210.17366, 2022. 9
[37] Ivan Kapelyukh, Vitalis Vosylius, and Edward Johns. Dall-e-bot: Introducing web-scale diffusion models to robotics. arXiv preprint arXiv:2210.02438, 2022. 9
[38] Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline qlearning on diverse multi-task data both scales and generalizes. arXiv preprint arXiv:2211.15144, 2022. 10
[39] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022. 10
[40] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. 10
[41] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. arXiv preprint arXiv:2005.07648, 2020. 10
[42] Parsa Mahmoudieh, Deepak Pathak, and Trevor Darrell. Zero-shot reward specification via grounded natural language. In ICLR 2022 Workshop on Generalizable Policy Learning in Physical World, 2022. 10
[43] Hao Liu, Lisa Lee, Kimin Lee, and Pieter Abbeel. Instruction-following agents with jointly pre-trained vision-language models. arXiv preprint arXiv:2210.13431, 2022. 10
[44] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. 10</p>
<p>[45] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. 14
[46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 14</p>
<h1>A Architecture, Training, and Evaluation Details</h1>
<h2>A. 1 Video Diffusion Training Details</h2>
<p>We use the same base architecture and training setup as [45] which utilizes Video U-Net architecture with 3 residual blocks of 512 base channels and channel multiplier [1, 2, 4], attention resolutions [6, 12, 24], attention head dimension 64, and conditioning embedding dimension 1024. We use noise schedule $\log$ SNR with range [-20, 20]. We make modifications Video U-Net to support first-frame conditioning during training. Specifically, we replicate the first frame to be conditioned on at all future frame indices, and apply temporal super resolution model condition on the replicated first frame by concatenating the first frame channel-wise to the noisy data similar to [46]. We use temporal convolutions as opposed to temporal attention to mix frames across time, to maintain local temporal consistency across time, which has also been previously noted in [19]. We train each of our video diffusion models for 2 M steps using batch size 2048 with learning rate $1 \mathrm{e}-4$ and 10 k linear warmup steps. We use 256 TPU-v4 chips for our first-frame conditioned generation model and temporal super resolution model.</p>
<p>We use T5-XXL [22] to process input prompts which consists of 4.6 billion parameters. For combinatorial and multi-task generalization experiments on simulated robotic manipulation, we train a first-frame conditioned video diffusion models on 10x48x64 videos (skipping every 8 frames) with 1.7B parameters and a temporal super resolution of 20x48x64 (skipping every 4 frames) with 1.7B parameters. The resolution of the videos are chosen so that the objects being manipulated (e.g., blocks being moved around) are clearly visible in the video. For the real world video results, we finetune the 16x40x24 (1.7B), 32x40x24 (1.7B), 32x80x48 (1.4B), and 32x320x192 (1.2B) temporal super resolution models pretrained on the data used by [19].</p>
<h2>A. 2 Inverse Dynamics Training Details</h2>
<p>UniPi's inverse dynamics model is trained to directly predict the 7-dimensional controls of the simulated robot arm from an image observation mean squared error. The inverse dynamics model consists of a 3x3 convolutional layer, 3 layers of 3x3 convolutions with residual connection, a mean-pooling layer across all pixel locations, and an MLP layer of $(128,7)$ channels to predict the final controls. The inverse dynamics model is trained using the Adam optimizer with gradient norm clipped at 1 and learning rate $1 \mathrm{e}-4$ for a total of 2 M steps where linear warmup is applied to the first 10 k steps.</p>
<h2>A. 3 Baselines Training Details</h2>
<p>We describe the architecture details of various baselines below. The training details (e.g., learning rate, warm up, gradient clip) of each baseline follow those of the inverse dynamics model detailed above.</p>
<p>Transformer BC [6, 26]. We employ the same transformer architecture as the 10M model of [26] with 4 attention layers of 8 heads each and hidden size 512 . We apply 4 layers of $3 \times 3$ convolution with residual connection to extract image features, which, together with T5 text embeddings, are used as inputs to the transformer. We additionally experimented with vision transformer style linearization of the image patches similar to [26], but found the performance to be similar. We use a context length of 4 and skip every 4 frames similar to UniPi's inverse dynamics. We tried increasing the context length of the transformer to 8 but it did not help improve performance.</p>
<p>Transformer TT [25]. We use a similar transformer architecture as the Transformer BC baseline detailed above. Instead of predicting the immediate next control in the sequence as in Transformer BC, we predict the next 8 controls (skipping every 4 controls similar to other baselines) at the output layer. We have also tried autoregressively predicting the next 8 controls, but found the errors to accumulate quickly without additional discretization.</p>
<p>State-Based Diffusion [21]. For the state-based diffusion baseline, we use a similar architecture as UniPi's first-frame conditioned video diffusion, where instead of diffusing and generating future</p>
<p>image frames, we replicate future controls across different pixel locations and apply the same U-Net structure as UniPi to learn state-based diffusion models.</p>
<h1>A. 4 Details of the Combinatorial Planning Task</h1>
<p>In the combinatorial planning tasks, we sample random 6 DOF poses for blocks, colored bowls, the final placement box. Blocks start off uncolored (white) and must be placed in a bowl to obtain a color. The robot then must manipulate and move the colored block to have the desired geometric relation in the placement box. The underlying action space of the agent corresponds to 6 joint values of robot plus a discrete contact action. When the contact action is active, the nearest block on the table is attached to the robot gripper (where for methods that predict continuous actions, we thresholded action prediction $&gt;0.5$ to correspond to contact). Given individual action predictions for different models, we simulate the next state of the environment by running the joint controller in Pybullet to try reach the predicted joint state (with a timeout of 2 seconds due to certain actions being physically infeasible). As only a subset of the video dataset contained action annotations, we trained the inverse-dynamics model on action annotations from 20k generated videos.</p>
<h2>A. 5 Details of the CLIPort Multi-Environment Task</h2>
<p>In the CLIPort environment, we use the same action space as the combinatorial planning tasks and execute actions similarly using the built in joint controller in Pybullet. As our training data, we use a scripted agent on put-block-in-bowl-unseen-colors, packing-unseen-google-objectsseq, assembling-kits-seq-unseen-colors, stack-block-pyramid-seq-seen-colors, tower-of-hanoi-seq-seen-colors, assembling-kits-seq-seen-colors, tower-of-hanoi-seq-unseen-colors, stack-block-pyramid-seq-unseen-colors, packing-seen-google-objects-seq, packing-boxes-pairs-seen-colors, packing-seen-googleobjects-group. As our test data, we used the environments put-block-in-bowl-seen-colors, packing-unseen-google-objects-group, packing-boxes-pairs-unseen-colors. We trained the inverse dynamics on action annotation across the 200k generated videos.</p>
<h1>B Additional Results</h1>
<h2>B. 1 Additional Results on Combinatorial Generalization</h2>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Combinatorial Video Generation. Additional results on UniPi's generated videos for unseen language goals at test time.</p>
<h2>B. 2 Additional Results on Multi-Environment Transfer</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Multitask Video Generation. Additional results on UniPi's generated video plans on different new tasks in the multitask setting.</p>
<h1>B. 3 Additional Results on Real-World Transfer</h1>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: High Fidelity Plan Generation. Additional results on UniPi's high resolution video plans across different language prompts.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>denotes equal contribution. Correspondence to yilundu@mit.edu and sherryy@berkeley.edu.
${ }^{2}$ See video visualizations at https://universal-policy.github.io.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
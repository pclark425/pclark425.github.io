<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4247 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4247</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4247</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-274131682</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.10878v1.pdf" target="_blank">Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis</a></p>
                <p><strong>Paper Abstract:</strong> This study investigates the automation of metaanalysis in scientific documents using large language models (LLMs). Meta-analysis is a robust statistical method that synthesizes the findings of multiple studies (support articles) to provide a comprehensive understanding. We know that a metaarticle provides a structured analysis of several articles. However, conducting meta-analysis by hand is labor-intensive, time-consuming, and susceptible to human error, highlighting the need for automated pipelines to streamline the process. Our research introduces a novel approach that fine-tunes the LLM on extensive scientific datasets to address challenges in big data handling and structured data extraction. We automate and optimize the meta-analysis process by integrating Retrieval Augmented Generation (RAG). Tailored through prompt engineering and a new loss metric, Inverse Cosine Distance (ICD), designed for fine-tuning on large contextual datasets, LLMs efficiently generate structured meta-analysis content. Human evaluation then assesses relevance and provides information on model performance in key metrics. This research demonstrates that fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs generating 87.6% relevant meta-analysis abstracts. The relevance of the context, based on human evaluation, shows a reduction in irrelevancy from 4.56% to 1.9%. These experiments were conducted in a low-resource environment, highlighting the study’s contribution to enhancing the efficiency and reliability of meta-analysis automation.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4247.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4247.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD+ICD+RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Analysis Dataset pipeline with Inverse Cosine Distance fine-tuning and Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline introduced in this paper that fine-tunes small-context LLMs on a curated meta-analysis dataset (MAD) using a novel ICD loss, chunk-based preprocessing, prompt engineering, and RAG retrieval to generate structured meta-analysis abstracts (empirical generalizations) from collections of scholarly paper abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (7B) and Mistral-v0.1 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MAD pipeline (chunking + ICD fine-tuning + RAG + prompt engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Construct MAD: 625 meta-article abstracts paired with the abstracts of their support articles (total 6344 support-article abstracts). Preprocess by chunking support-article abstracts into overlapping segments (Recursive TextSplitter; ~200-token chunks up to 2000 tokens). Supervised fine-tuning pairs each chunk C_j_i with the meta-article abstract y_j as label using QLoRA quantized adapters on Llama-2 (7B) and Mistral-v0.1 (7B). Training uses a novel Inverse Cosine Distance (ICD) loss (ICD = mean(1 / (cosine_sim(y, ŷ) + ϵ))) to emphasise directional/semantic alignment between generated and reference abstracts. At inference, chunks are stored in a vector DB; Retrieval-Augmented Generation (RAG) performs semantic search to retrieve relevant chunks given a query and the retrieved context + prompt are provided to the fine-tuned LLM. Prompt engineering (Prompt 1 chosen) and temperature tuning (0.7 favored) are applied. Evaluation combines automatic metrics (BLEU, ROUGE, cosine similarity) and human evaluation (13 independent evaluators classifying outputs as Relevant / Somewhat-Relevant / Irrelevant with majority voting). Implementation details: LangChain for retrieval, Transformers AutoTokenizer, PyTorch on Tesla T4 GPUs, 2 epochs (resource-constrained) with iterative fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>625 meta-articles; 6344 support-article abstracts (total 6969 abstracts)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Medicine / medical meta-analyses (clinical studies) — general scientific meta-analysis scenarios drawn from ScienceDirect</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Empirical generalizations and structured meta-analytic conclusions (statistical effect summaries, treatment efficacy statements and other aggregate patterns derived from multiple studies)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>1) 'Synbiotic intake can be an effective adjunctive agent in the management of hyperglycemia in adults' with reported pooled reductions: FBS (ES = -0.40, 95% CI: -0.64 to -0.15; p = 0.002, I2 = 69.2%), insulin (ES = -1.58, 95% CI: -0.79 to -0.30; p < 0.001, I2 = 48.4%), HOMA-IR (ES = -0.55, 95% CI: -0.79 to -0.30; p < 0.001). 2) Summary claim about Traditional Chinese Medicine for COVID-19 with pooled effect sizes reported in the generated abstract: symptomatic relief (ES = 0.67, 95% CI: 0.56 to 0.79), severity reduction (ES = 0.51, 95% CI: 0.43 to 0.60), mortality reduction (ES = 0.37, 95% CI: 0.21 to 0.55). (These are concrete examples produced by the system as meta-analytic conclusions.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human evaluation by 13 independent evaluators (Relevant / Somewhat-Relevant / Irrelevant with majority voting), plus automatic metrics: BLEU and ROUGE for surface/overlap resemblance and cosine-similarity for vector similarity between generated and ground-truth abstracts; also benchmark testing on Open-i, writer summaries, and CL-SciSumm datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Fine-tuned + RAG system produced 87.6% of generated meta-analysis abstracts classified as Relevant by human evaluators; irrelevancy rate dropped from 4.56% (baseline) to 1.9% after fine-tuning and RAG. Example generated abstract similarities to ground truth: 82.40% and 85.73% (reported similarity scores for two sample outputs). Additional automatic-metric improvements reported (BLEU/ROUGE increases and better performance on CL-SciSumm) though full numeric tables are in the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baselines: non-fine-tuned pre-trained LLMs (Llama-2 (7B) and Mistral-v0.1 (7B) without MAD/ICD/RAG), and pre-trained models on benchmark datasets. Findings: fine-tuned models (with ICD and RAG) outperformed their non-fine-tuned counterparts; non-fine-tuned Llama-2 (7B) performed better than non-fine-tuned Mistral in baseline settings, but after fine-tuning Mistral-v0.1 (7B) yielded top performance among tested small-context models. The paper also compares to published benchmark results on summarization datasets (not directly to human domain experts).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Fine-tuning small-context LLMs on a purpose-built meta-analysis dataset (MAD) using an ICD loss significantly improves their ability to synthesize multiple scholarly abstracts into coherent, relevant meta-analysis abstracts. 2) Chunking combined with RAG retrieval mitigates context-length limits and improves access to distributed information across support articles. 3) Prompt choice and generation temperature (0.7) materially affect relevance and quality. 4) ICD loss better aligns generated outputs to semantic targets than standard losses in this task. 5) The pipeline enables 7B-class models to produce structured, numerically specific meta-analytic conclusions with high human-rated relevancy.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Context-length limits requiring chunking (potential for information loss despite overlapping chunks); hardware/resource constraints (evaluation performed on 50% of test sets, aggressive quantization via QLoRA limited full fine-tuning capacity); potential domain/generalization limits (dataset drawn from ScienceDirect, largely medical); risk of hallucination or incorrect numeric synthesis not exhaustively characterized (human evaluation mitigates but does not eliminate this risk); reliance on quality of input abstracts (lossy relative to full-text extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models <em>(Rating: 2)</em></li>
                <li>Evaluation of chatgpt-generated medical responses: A systematic review and metaanalysis <em>(Rating: 2)</em></li>
                <li>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks <em>(Rating: 1)</em></li>
                <li>Adapted large language models can outperform medical experts in clinical text summarization <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4247",
    "paper_id": "paper-274131682",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "MAD+ICD+RAG",
            "name_full": "Meta-Analysis Dataset pipeline with Inverse Cosine Distance fine-tuning and Retrieval-Augmented Generation",
            "brief_description": "A pipeline introduced in this paper that fine-tunes small-context LLMs on a curated meta-analysis dataset (MAD) using a novel ICD loss, chunk-based preprocessing, prompt engineering, and RAG retrieval to generate structured meta-analysis abstracts (empirical generalizations) from collections of scholarly paper abstracts.",
            "citation_title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
            "mention_or_use": "use",
            "model_name": "Llama-2 (7B) and Mistral-v0.1 (7B)",
            "model_size": "7B",
            "method_name": "MAD pipeline (chunking + ICD fine-tuning + RAG + prompt engineering)",
            "method_description": "Construct MAD: 625 meta-article abstracts paired with the abstracts of their support articles (total 6344 support-article abstracts). Preprocess by chunking support-article abstracts into overlapping segments (Recursive TextSplitter; ~200-token chunks up to 2000 tokens). Supervised fine-tuning pairs each chunk C_j_i with the meta-article abstract y_j as label using QLoRA quantized adapters on Llama-2 (7B) and Mistral-v0.1 (7B). Training uses a novel Inverse Cosine Distance (ICD) loss (ICD = mean(1 / (cosine_sim(y, ŷ) + ϵ))) to emphasise directional/semantic alignment between generated and reference abstracts. At inference, chunks are stored in a vector DB; Retrieval-Augmented Generation (RAG) performs semantic search to retrieve relevant chunks given a query and the retrieved context + prompt are provided to the fine-tuned LLM. Prompt engineering (Prompt 1 chosen) and temperature tuning (0.7 favored) are applied. Evaluation combines automatic metrics (BLEU, ROUGE, cosine similarity) and human evaluation (13 independent evaluators classifying outputs as Relevant / Somewhat-Relevant / Irrelevant with majority voting). Implementation details: LangChain for retrieval, Transformers AutoTokenizer, PyTorch on Tesla T4 GPUs, 2 epochs (resource-constrained) with iterative fine-tuning.",
            "number_of_papers": "625 meta-articles; 6344 support-article abstracts (total 6969 abstracts)",
            "domain_or_field": "Medicine / medical meta-analyses (clinical studies) — general scientific meta-analysis scenarios drawn from ScienceDirect",
            "type_of_laws_extracted": "Empirical generalizations and structured meta-analytic conclusions (statistical effect summaries, treatment efficacy statements and other aggregate patterns derived from multiple studies)",
            "example_laws_extracted": "1) 'Synbiotic intake can be an effective adjunctive agent in the management of hyperglycemia in adults' with reported pooled reductions: FBS (ES = -0.40, 95% CI: -0.64 to -0.15; p = 0.002, I2 = 69.2%), insulin (ES = -1.58, 95% CI: -0.79 to -0.30; p &lt; 0.001, I2 = 48.4%), HOMA-IR (ES = -0.55, 95% CI: -0.79 to -0.30; p &lt; 0.001). 2) Summary claim about Traditional Chinese Medicine for COVID-19 with pooled effect sizes reported in the generated abstract: symptomatic relief (ES = 0.67, 95% CI: 0.56 to 0.79), severity reduction (ES = 0.51, 95% CI: 0.43 to 0.60), mortality reduction (ES = 0.37, 95% CI: 0.21 to 0.55). (These are concrete examples produced by the system as meta-analytic conclusions.)",
            "evaluation_method": "Human evaluation by 13 independent evaluators (Relevant / Somewhat-Relevant / Irrelevant with majority voting), plus automatic metrics: BLEU and ROUGE for surface/overlap resemblance and cosine-similarity for vector similarity between generated and ground-truth abstracts; also benchmark testing on Open-i, writer summaries, and CL-SciSumm datasets.",
            "performance_metrics": "Fine-tuned + RAG system produced 87.6% of generated meta-analysis abstracts classified as Relevant by human evaluators; irrelevancy rate dropped from 4.56% (baseline) to 1.9% after fine-tuning and RAG. Example generated abstract similarities to ground truth: 82.40% and 85.73% (reported similarity scores for two sample outputs). Additional automatic-metric improvements reported (BLEU/ROUGE increases and better performance on CL-SciSumm) though full numeric tables are in the paper's tables.",
            "comparison_baseline": "Baselines: non-fine-tuned pre-trained LLMs (Llama-2 (7B) and Mistral-v0.1 (7B) without MAD/ICD/RAG), and pre-trained models on benchmark datasets. Findings: fine-tuned models (with ICD and RAG) outperformed their non-fine-tuned counterparts; non-fine-tuned Llama-2 (7B) performed better than non-fine-tuned Mistral in baseline settings, but after fine-tuning Mistral-v0.1 (7B) yielded top performance among tested small-context models. The paper also compares to published benchmark results on summarization datasets (not directly to human domain experts).",
            "key_findings": "1) Fine-tuning small-context LLMs on a purpose-built meta-analysis dataset (MAD) using an ICD loss significantly improves their ability to synthesize multiple scholarly abstracts into coherent, relevant meta-analysis abstracts. 2) Chunking combined with RAG retrieval mitigates context-length limits and improves access to distributed information across support articles. 3) Prompt choice and generation temperature (0.7) materially affect relevance and quality. 4) ICD loss better aligns generated outputs to semantic targets than standard losses in this task. 5) The pipeline enables 7B-class models to produce structured, numerically specific meta-analytic conclusions with high human-rated relevancy.",
            "challenges_limitations": "Context-length limits requiring chunking (potential for information loss despite overlapping chunks); hardware/resource constraints (evaluation performed on 50% of test sets, aggressive quantization via QLoRA limited full fine-tuning capacity); potential domain/generalization limits (dataset drawn from ScienceDirect, largely medical); risk of hallucination or incorrect numeric synthesis not exhaustively characterized (human evaluation mitigates but does not eliminate this risk); reliance on quality of input abstracts (lossy relative to full-text extraction).",
            "uuid": "e4247.0",
            "source_info": {
                "paper_title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrieval_augmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_to_automate_network_metaanalyses_four_case_studies_to_evaluate_the_potential_application_of_large_language_models"
        },
        {
            "paper_title": "Evaluation of chatgpt-generated medical responses: A systematic review and metaanalysis",
            "rating": 2,
            "sanitized_title": "evaluation_of_chatgptgenerated_medical_responses_a_systematic_review_and_metaanalysis"
        },
        {
            "paper_title": "Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks",
            "rating": 1,
            "sanitized_title": "scisummnet_a_large_annotated_corpus_and_contentimpact_models_for_scientific_paper_summarization_with_citation_networks"
        },
        {
            "paper_title": "Adapted large language models can outperform medical experts in clinical text summarization",
            "rating": 2,
            "sanitized_title": "adapted_large_language_models_can_outperform_medical_experts_in_clinical_text_summarization"
        }
    ],
    "cost": 0.01182025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis
16 Nov 2024</p>
<p>Jawad Ibn Ahad 
Rafeed Mohammad Sultan rafeed.sultan@northsouth.edu 
Abraham Kaikobad abraham.kaikobad@northsouth.edu 
Fuad Rahman 
Mohammad Ruhul Amin 
Nabeel Mohammed nabeel.mohammed@northsouth.edu 
Shafin Rahman shafin.rahman@northsouth.edu 
Chunked Dataset </p>
<p>Apurba-NSU R&amp;D Lab
ECE North South University Dhaka
Bangladesh</p>
<p>Apurba-NSU R&amp;D Lab
ECE North South University Dhaka
Bangladesh</p>
<p>Apurba-NSU R&amp;D Lab
ECE North South University Dhaka
Bangladesh</p>
<p>Apurba Technologies Sunnyvale
94085CAUSA</p>
<p>Computer and Information Science
Fordham University New York
USA</p>
<p>Apurba-NSU R&amp;D Lab
ECE North South University Dhaka
Bangladesh</p>
<p>Apurba-NSU R&amp;D Lab
ECE North South University Dhaka
Bangladesh</p>
<p>Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis
16 Nov 2024517D759A8A86C9E9E97ACF223CFB3496arXiv:2411.10878v1[cs.CL]Meta-analysisLarge contextual dataHuman evaluationPrompt engineeringLarge Language Model
This study investigates the automation of metaanalysis in scientific documents using large language models (LLMs).Meta-analysis is a robust statistical method that synthesizes the findings of multiple studies (support articles) to provide a comprehensive understanding.We know that a metaarticle provides a structured analysis of several articles.However, conducting meta-analysis by hand is labor-intensive, timeconsuming, and susceptible to human error, highlighting the need for automated pipelines to streamline the process.Our research introduces a novel approach that fine-tunes the LLM on extensive scientific datasets to address challenges in big data handling and structured data extraction.We automate and optimize the metaanalysis process by integrating Retrieval Augmented Generation (RAG).Tailored through prompt engineering and a new loss metric, Inverse Cosine Distance (ICD), designed for fine-tuning on large contextual datasets, LLMs efficiently generate structured meta-analysis content.Human evaluation then assesses relevance and provides information on model performance in key metrics.This research demonstrates that fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs generating 87.6% relevant meta-analysis abstracts.The relevance of the context, based on human evaluation, shows a reduction in irrelevancy from 4.56% to 1.9%.These experiments were conducted in a low-resource environment, highlighting the study's contribution to enhancing the efficiency and reliability of meta-analysis automation.</p>
<p>I. INTRODUCTION</p>
<p>Meta-analysis is a powerful statistical approach that combines the findings of multiple studies to provide a comprehensive understanding of the same research topic [1].A metaanalysis paper, or meta article, offers a structured analysis of numerous individual support articles.Individual studies often face limitations, such as small sample sizes or narrow focus, making it hard to draw definitive conclusions [2].Meta-analysis aggregates data from different studies, providing robust estimates that inform research decisions, guide treatments, and influence healthcare policies [3]- [6].In applied scientific fields, meta-analyses play a crucial role: consolidating the results of clinical trials [7], [8], evaluating public health strategies, material performance, and farming practices [9]- [11], and assessing the impacts, behavior, policies, and teaching methods of climate in fields such as environmental science, psychology, economics, and education [12]- [15].Meta-analysis involves the analysis of extensive datasets, as it incorporates numerous studies, which presents a significant challenge to big data.The process is often laborintensive, requiring manual extraction and analysis of data from multiple research articles, which is both time-consuming and susceptible to human error.This underscores the critical need for an automated pipeline to streamline and improve the efficiency of meta-analysis generation.The advancements in large language models (LLMs) offer promising potential, suggesting that these models could be utilized to manage the vast data requirements of scientific meta-analysis.</p>
<p>Traditional meta-analysis faces challenges with big data scalability.(a) Manual Meta-analysis: Involves manual data extraction and analysis, limiting scalability in rapidly expanding domains [20].While AI advances automate tasks like information retrieval and summarization, focusing primarily on shorter summaries, LLMs show strong potential in summarization but still face limitations.(b) LLMs as summarizers: Although LLMs like Llama2-13b have demonstrated proficiency in generating summaries and performing question-answering tasks [16], [17], [21], their utility in synthesizing extensive research findings for meta-analysis remains constrained.Current approaches in this domain predominantly focus on generating condensed summaries for shorter, narrative-based content rather than synthesizing large-scale scientific data.To address this gap, researchers have explored RAG techniques.(c) Retrieval Augmented Generation: By integrating retrieval-based mechanisms, RAG enables LLMs to access and summarize large datasets through document retrieval [18], [19], [22].However, this method falls short when applied Fig. 1: (a) Paraphraser-based approach that combines multiple generated summary chunks from LLMs has been used by [16], [17], (b) Retrieval augmentation generation-based approach has been applied in [18], [19] using a vector database to store chunked data and cluster them before passing to LLM to produce a summary.Existing methods often fall short of handling big scientific contextual data and generating structured synthesis.(c) We propose a novel approach involving fine-tuning LLMs with large contexts and utilizing them to generate meta-analysis abstracts.Abstracts from support papers serve as input, with meta-papers' abstracts as labels.Pre-processing involves chunking the dataset due to context length restrictions and prioritizing small LLMs over resource-intensive large LLMs.The fine-tuned model generates meta-analysis abstracts via semantic search with the provided context and query.to meta-analysis, which demands specialized data extraction techniques and a deeper synthesis of scientific contexts [23].LLMs' current limitations highlight a need for targeted finetuning and tailored approaches to handle complex, structured large-scale scientific data.</p>
<p>To bridge this gap, our research introduces a novel approach that leverages LLMs with RAG to automate and streamline the meta-analysis process.We have built a comprehensive dataset with various meta-analysis scenarios in various scientific fields, which contains the content of the meta-articles along with the content of the support papers.This dataset facilitates both training and evaluation to stimulate further research.Its purpose is to fine-tune LLMs, enabling them to understand and replicate data extraction patterns for metaanalysis.We introduce a novel loss function, Inverse Cosine Distance (ICD), specifically designed for training LLMs in large-context scenarios to handle large-data challenges.This function enhances the performance of LLMs in generating meta-analysis with high relevance and accuracy.By fine-tuning LLMs for large-context tasks and employing specific prompt engineering techniques, shown in Fig. 1, we aim to overcome the limitations of existing methods of handling big contextual data.By integrating RAG with our fine-tuning strategy, LLMs generate precise, instruction-based meta-analysis content, ensuring quality and efficiency.Our approach reduces the laborintensive aspects of meta-analysis, enabling LLMs to handle large contexts and generate structured abstracts effectively.This work holds significant potential for improving research synthesis across various domains.</p>
<p>Our contribution comprises (1) preparing a comprehensive dataset to fine-tune LLMs for meta-analysis generation, (2) fine-tuning LLMs with the novel ICD loss function, en-hancing their ability to handle large-context scientific data and extract relevant information for meta-analysis, and (3) leveraging these fine-tuned LLMs by integrating RAG to generate precise, instruction-based meta-analysis from largescale scientific data.</p>
<p>II. RELATED WORKS</p>
<p>Meta-Analysis Strategy:</p>
<p>In recent years, the landscape of meta-analysis has witnessed significant advancements, particularly with the development of comprehensive databases, facilitating systematic reviews.Csizmadia et al. [24] contributed to this domain by introducing a global database of innovation and quality management, meticulously compiling the PRISMA methodology, covering records from 1975 to 2021.Furthermore, Yudhanto et al. [25] addressed the laborintensive nature of data collection for meta-analysis, presenting a method using bibliometric studies from the Science Direct Database.Their approach, which involved data collection from published searches using desired keywords over the last decade, significantly streamlined the process, contributing to the efficiency of meta-analysis procedures.To further enhance this progress, our paper introduces a novel approach that harnesses the capabilities of LLMs and RAG.This approach aims to streamline the meta-analysis process, empower LLMs to handle large contexts efficiently, and conduct a structured meta-analysis of the provided research papers.Our contribution lies in developing a Comprehensive Meta-Analysis Dataset, which serves as a valuable resource for training and evaluating the efficiency of our proposed approach.Large Context Summarization: Recent advancements in large-context summarization have led to the development of techniques to generate concise and informative summaries from extensive documents.In particular, Subbiah et al. [16] introduce a fragmentation strategy, converting full stories into manageable fragments and associating them with prompts to facilitate effective summarization.Keswani et al. [21] focus on summarization and question-answering tasks using the Llama-2 (13B) model, employing clustering techniques based on cosine similarity to improve efficiency despite computational constraints.Furthermore, observations on different LLM summarization performances have been made using zero-shot prompt techniques [26], [27].These advancements underscore the significance of leveraging LLMs for largecontext summarization tasks.Leveraging these developments, our proposed method successfully utilizes large contexts by breaking them into smaller, manageable chunks.This strategy facilitates easier summarization by smaller LLMs, thereby enhancing the overall efficiency of the process.Summarization Quality Assessment by Evaluation: Evaluation metrics are crucial for assessing the effectiveness of automated summarization systems.Traditional metrics often have limitations in accurately capturing the quality of generated summaries [28].Innovative frameworks like HumanELY [29] have been proposed, incorporating key evaluation metrics including relevance, coverage, coherence, harm, and comparison.Additionally, novel scoring systems leveraging LLMs have been introduced, shedding light on how different identities influence performance [30].A taxonomy of LLM-based NLG evaluation methods has also been presented, delineating their advantages and drawbacks [31].Despite these efforts, achieving comprehensive evaluation frameworks for NLG systems remains challenging.Inspired by the pioneering work of Chaudhary et al. [32] on generating both relevant and irrelevant queries, we adopt a similar methodology to evaluate the efficacy of our generated meta-analysis.Our evaluation metrics encompass not only relevance but also nuances, categorizing outputs into Relevant, Somewhat-Relevant, and Irrelevant.Incorporating this thorough evaluation framework allows us to deliver a detailed evaluation of the performance of our automated meta-analysis synthesis, thereby enriching the depth of analysis and insights in our research.Our approach introduces a novel evaluation concept based on hard voting, contributing to meta-analysis automation.Leveraging a comprehensive meta-analysis dataset, innovative training methods, and fine-tuning strategies tailored for instructionbased meta-analysis abstracts, our approach stands out for its effectiveness and reliability in advancing research synthesis efficiency across domains.</p>
<p>III. METHODOLOGY</p>
<p>Several innovative efforts have been made to guarantee that LLMs can manage lengthy contexts.However, to incorporate big textual data challenges, LLMs require numerous amounts of resources.This study presents a novel approach for generating meta-analysis using LLMs, particularly with long context lengths.This section formally outlines our method for using LLM to produce meta-analysis content.Problem Formulation: Consider there are m j number of meta-articles, where j ∈ [1, n].For each meta-article, there is a set of support articles, S j == {v j 1 , v j 2 , ..., v j |S j | }. v j i represents the abstract of i th support article related to j th meta-article.We aim to build a model M to generate a meta-article's abstract, y j using all abstracts inside the set S j .This study focuses on generating a relevant y j through a low context length LLM (e.g., Llama-2 7B, Mistral 7B, and Gemma 7B).</p>
<p>Here we have investigated two important aspects of this problem.(a) Handling large context length: Typically, meta-analyses are performed through manual analysis and data extraction from supporting articles.Recently, LLMs have demonstrated their ability to summarize extensive textual data.While substantial research has focused on summarization [18], [19], [22], the application of LLMs for meta-analysis remains unexplored.Meta-analysis often involves structured data derived from supporting articles, yet most LLMs operate within constrained context-length environments during fine-tuning.Our objective is to address this limitation by efficiently managing large contextual data and segmenting it into smaller chunks to facilitate effective fine-tuning of LLMs.(b) Enhance information retrieval: Prior research has demonstrated that finetuning LLMs enhances their data extraction capabilities [23].However, when generating large contextual, analytical data, LLMs require access to external knowledge sources.RAG has shown promising results in addressing this challenge.In our approach to enabling context-length-restricted LLMs to generate meta-analysis and to further expand the scope of knowledge from supporting articles, we aim to integrate RAG with our fine-tuned meta-analysis generator LLMs.</p>
<p>A. MAD: Meta-Analysis Dataset</p>
<p>Generative language models are capable of producing reviews or summaries of given contexts.They still have issues producing analytical context based on substantial context inputs.Our first step in addressing this big-data challenge is to create a dataset to generate a meta-analysis.Large scientific context datasets like MAD have not been used before to finetune context-length-restricted LLMs.</p>
<p>The dataset, MAD that we constructed consists of two columns: one containing meta-articles' abstracts and the other containing the abstracts of the support articles.For example, consider the meta-article titled "Intervention methods for improving reduced heart rate variability in patients with major depressive disorder: A systematic review and metaanalysis" [20].We used the abstract of this paper as our target meta-analysis abstract.From Table 1 of this paper, we identified that it conducted a meta-analysis of over twenty studies.We manually extracted the abstracts of these support articles by following the references listed in the table.These twenty abstracts were placed in the second column (S j ) alongside the meta-article's abstract (y j ).Essentially, the goal is for the LLM to generate a meta-analysis abstract from these support articles' abstracts.</p>
<p>Using this approach, we gathered 625 meta-articles from ScienceDirect, along with the abstracts of all the support articles included in that meta-analysis.In total, dataset MAD</p>
<p>Large Language Model</p>
<p>Meta-analysis</p>
<p>Query: Generate a metaanalysis abstract on Synbiotic as an adjunctive agent can in....</p>
<p>Large Language Model</p>
<p>Multiple RCTs, epidemiological evidence and studies report on nonalcoholic fatty liver disease and the effects of prebiotics and synbiotics, with a borderline statistically significant reduction in FBG (−0.18 mmol/L, 95% CI −0.37, 0.00; p = 0.05).... Fig. 2: In our meta-analysis generation system, support articles S j undergo chunk-based pre-processing, producing chunks C j i ⊆ S j , here "SP:" refers to an abstract of the support article S j .These chunks are used to fine-tune the LLMs for predicting meta-analysis abstracts y j , with the ICD loss guiding the fine-tuning process.Model performance is assessed through human evaluation of the relevancy of generated meta-analysis abstracts, ŷj by fine-tuned LLMs.During inference, we integrate RAG with the fine-tuned LLMs.Chunked samples are stored in a vector database, from which relevant information is retrieved via a semantic search based on a query.The same processed C j i is used for both fine-tuning and inference to maintain retrieval consistency.The retrieved content and the query are provided to the LLM, enabling it to generate more precise and accurate meta-analysis abstracts by leveraging comprehensive contextual information.</p>
<p>Metaanalysis
Measure
includes 6344 support articles' abstracts and 625 meta-articles' abstracts.The dataset statistics, along with the demographic information of the human evaluators who assessed model performance, are shown in Table I, and the distribution of support articles in meta-articles is shown in Fig. 3.</p>
<p>B. Chunk-Based Processing of Support Articles</p>
<p>Given the limitation in context length for many language models, processing long or complex documents as a whole can become inefficient and may lead to suboptimal results.To address this, chunking the support articles into smaller, meaningful segments allows for more effective input to the language model.By chunking, we ensure that all support article abstracts in the set S j are considered while maintaining manageable input sizes for low-context models.</p>
<p>To manage the input size and improve model performance, we divide the support articles set S j = {v j 1 , v j 2 , . . ., v j |S j | } into multiple smaller overlapping chunks.Overlapping will be done with some portions of abstracts.This will allow the coherence and continuity between chunks, reducing the chances of information loss.Chunking of S j into k possibly overlapping chunks is defined as:
C(S j ) = {C j 1 , C j 2 , . . . , C j k } where: • C j i ⊆ S j for each i ∈ [1, k],
• k i=1 C j i = S j (the union of all chunks covers the entire set, though the chunks may overlap), • C j i ∩ C j l ̸ = ∅ for i ̸ = l (contents will overlap).For example, suppose the set of support article abstracts S j = {v j 1 , v j 2 , v j 3 , v j 4 , v j 5 } is divided into three overlapping chunks.The chunking is as follows:
C(S j ) = {C j 1 , C j 2 , C j 3 } where:C j 1 = {v j 1 , v j 2 }, C j 2 = {(portion of )v j 2 , v j 3 , v j 4 }, C j 3 = {(part of )v j 4 , v j 5 }.
In this example, v j 2 and v j 4 overlap in C j 2 and C j 3 .</p>
<p>C. Fine-tune LLMs and Integrate RAG</p>
<p>After creating the dataset MAD, two popular LLMs are considered for the experiment: Llama-2 (7B) [33] and Mistral-v0.1 (7B) [34].They are fine-tuned on the constructed dataset TABLE I: Detailed statistics of the actual and processed dataset MAD, along with the demographic profile of Human Evaluators for assessing the readability of fine-tuned LLMs.The MAD dataset contains abstracts from medical metaarticles and support studies.MAD and evaluated using the test set.To further improve their performance in generating relevant outcomes, we applied the RAG approach [35] to the fine-tuned versions of each model.Linear Unit (SiLU) activation function [37] and incorporates grouped-query attention (GQA) with sliding window attention (SWA) for efficient handling of variable sequences [34].Mistral-v0.1 (7B) outperforms both Llama-2 (7B) and Llama-2 (13B) in benchmarks, making it our model of choice.</p>
<p>Fine-tuning LLMs: Original models like Llama-2 and Mistral-v0.1 benefit from extensive pre-training on massive datasets, allowing them to grasp complex linguistic structures.However, this generic training might not be ideal for specialized tasks like generating meta-analysis abstracts from lengthy source materials.Fine-tuning bridges this gap by adapting these pre-trained models to new datasets and large data tasks.Following the selection of models, Llama-2 (7B) and Mistral-v0.1 (7B) were fine-tuned on the processed MAD dataset, utilizing chunked samples C j i paired with their respective meta-article's abstracts y j .This supervised fine-tuning process paired each chunked sample with its meta-analysis abstract (C j i , y j ), where C j i ⊆ S j and y j serves as label meta-article's abstract.The models, M, were trained to recognize patterns for generating meta-analysis content from large contexts, accommodating the multiple chunks associated with each y j .Instruction-based fine-tuning was employed with a focus on prompt engineering.Various prompt configurations Fig. 3: Distribution of Supporting Articles in Meta-Articles in the dataset MAD.The chart shows that most meta-articles contain 6 to 14 support articles, with peaks at 6 and 9, suggesting a common reliance on a moderate number of supporting studies, with fewer analyses incorporating larger study pools.</p>
<p>were tested to optimize the models' ability to generate accurate and coherent meta-analysis abstracts from long contexts.This approach ensured that the models effectively learned the specific patterns required for high-quality content generation.</p>
<p>Inverse Cosine Distance (ICD):</p>
<p>To support fine-tuning, a specialized training mechanism is used with the ICD loss metric.The ICD function measures the dissimilarity between the model-generated output ŷ and the ground truth y vectors, incorporating a small positive constant ϵ in the denominator to ensure numerical stability and improve the fine-tuning process.The formulation for ICD is given below:
ICD = 1 N N i=1 1 cosine sim i (y, ŷ) + ϵ(1)
During fine-tuning, models M processed chunked samples C j i to produce predicted abstracts ŷj .The ICD loss, calculated using formula 1, measured the dissimilarity between ŷj and the ground truth abstracts y j , guiding parameter updates via backpropagation.The process, constrained by resources, was carried out for 2 epochs over 5 iterations, refining the model and improving abstract accuracy.</p>
<p>Combining Fine-tuned model with RAG: Fine-tuning LLMs is highly effective for specific tasks; however, models with limited context, such as Llama-2 (7B) and Mistral-v0.1 (7B), face challenges when dealing with chunked data samples.For instance, when generating ŷj from a particular chunk C j i ⊆ S j , these models may lack information from other chunks C j i of the same dataset, MAD.RAG addresses this issue by retrieving relevant information from other chunks for the j th data sample, thereby reducing the need for extensive fine-tuning and minimizing irrelevant content.This approach involves storing each chunked test sample C j i in a vector database.Relevant chunks are then retrieved using semantic search based on queries and the stored chunks.The retrieved TABLE II: After fine-tuning the LLMs on the dataset MAD, comparing model performance on benchmark datasets for summarization quality, enabling assessment across varying context lengths.Among all the limited context-length LLMs, our fine-tuned (FT) models performed acceptably well comparatively.On the scientific document-contained dataset, Cl-SciSumm, our fine-tuned model outperformed other pre-trained LLMs, showing a significantly rigorous capability of capturing structured analytical information.↑ (↓) means higher (lower) is better.'-' denotes results that are not applicable there.content is subsequently fed into the LLMs, which process these additional contexts to generate a more accurate metaanalysis abstract.(AdditionalDetails in the Supplementary)</p>
<p>Method</p>
<p>IV. EXPERIMENT</p>
<p>A. Setup</p>
<p>Dataset: We used the dataset MAD for fine-tuning the LLMs.The dataset is split into a train, test, and validation set.The training set includes 400 meta-analysis scenarios.Given that support papers' abstracts, S j , often exceed the given context limit, chunk-based preprocessing is applied to chunk support papers' abstracts, S j .Chunking wasn't applied to metaarticles' abstract y j as the context length was manageable without chunking.Table I illustrates how chunking reduces context length.The same chunking approach is applied to the validation set (175 samples) and test set (50 samples).After chunking, the training set expands to 3659 samples.After finetuning the models, we then tested our fine-tuned model on the benchmark Open-i dataset [38], writer summaries [27], and the large scientific document dataset CL-SciSumm [39] to compare performance, shown in Table II.</p>
<p>Implementation Details 1 : The dataset MAD requires careful management due to the context size limitations of LLMs, which have a maximum context length of 4096 tokens.The "Recursive TextSplitter" from LangChain 2 is used to chunk the support papers' abstracts, S j , into overlapping segments 1 Code and data: https://github.com/EncryptedBinary/Metaanalysis 2 LangChain: https://www.langchain.com/ of 200 tokens, capped at 2000 tokens.This approach converts meta-article abstracts into target values y j and the chunks C j i into features for supervised fine-tuning.Due to the seven billion parameters of Llama-2 (7B) and Mistral-v0.1 (7B), the Quantized Low-Rank Adapters (QLoRA) configuration [44] is employed for model loading.A custom trainer class, extending the transformer trainer and incorporating the ICD loss function, is used to measure dissimilarity between generated and target meta-analysis content, guiding iterative model weight updates.Inputs are tokenized using Transformers' AutoTokenizer, and LangChain facilitates retrieval augmentation.All experiments were conducted on NVIDIA Tesla T4 (2x) GPUs using the PyTorch framework.(Further details are provided in the supplementary paper.)Prompt Selection: The selection of prompts significantly influences model performance by guiding task handling.After detailed experimentation with multiple prompts, we came up with the most impactful prompt that leverages LLMs to generate meta-analysis accurately.Table IV shows the impact of prompts on relevancy.A comparison between the two prompts is shown there.Prompt 1 demonstrated superior effectiveness over Prompt 2 in generating meta-analysis abstracts, achieving a high relevancy rate with every LLM.Evaluation metrics: For evaluating the generated texts from LLMs, Bilingual Evaluation Understudy (BLEU) [45], that quantifies the resemblance between generated and reference texts and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [46], that assesses how much information from reference summaries is captured.are used.For the abstract TABLE III: Human evaluation is done on generated meta-analysis abstract by fine-tuned and non-fine-tuned LLMs, following the criteria REL: Relevant, SWR: Somewhat-Relevant, and IRL: Irrelevant mentioned in the methodology.System-level metrics BLEU and ROUGE are used to identify when a human evaluator mentions in Table .I that a generated text is irrelevant and relevant.In the end, the generated meta-analysis abstract using the RAG approach is evaluated by measuring the generated abstract's similarity with the ground truth (SWGT).The symbol ↑ (or ↓) indicates that a higher (or lower) value is preferable.generated by fine-tuned models, the cosine similarity [47] metric is used to quantify the similarity between two vectors after combining fine-tuning with RAG.</p>
<p>Human evaluation: After generating responses with LLMs, we conduct a human evaluation process to ensure alignment with human judgment.Human judges categorize the generated text as relevant, somewhat-relevant, or irrelevant, following the criteria from [32].Relevant responses closely resemble the ground truth, showing high similarity and inclusion of crucial information.Somewhat-relevant responses have acceptable similarity, containing valuable information within an acceptable margin.Irrelevant responses lack important information or include unrelated content.This classification framework ensures a rigorous assessment of generated meta-analysis abstracts against expected standards.Three independent evaluators assessed each model's response, with majority voting used to determine the final decision.Each evaluator worked independently, without access to others' assessments.In total, 13 evaluators were involved in evaluating all the model responses.</p>
<p>To reduce bias, the evaluations were conducted by university students rather than the authors.Demographic details of the evaluators are provided in Table I.For further details on the evaluation process, refer to the Supplementary Material.</p>
<p>B. Results and Analysis</p>
<p>We present a detailed overview of our experimental evaluations, focusing on how context-length restricted LLMs perform in generating meta-analysis with lengthy inputs.Notably, previous research has not utilized large context datasets for meta-analysis, making our study unique.For comparison, we also used a short context dataset to evaluate the models' performance, as shown in Table II.Considering the architecture of our models, the benchmark performance is reliable.</p>
<p>After fine-tuning the LLMs, human evaluation of the generated outputs is essential.We applied our proposed human evaluation metrics-Relevant, Somewhat-Relevant, and Irrelevant-to assess the results of the meta-analysis generation task.As shown in Table III, our approach of fine-tuning LLMs with large context dataset, MAD outperforms other methods, producing more relevant meta-analysis content and reducing unnecessary context generation.</p>
<p>The non-fine-tuned Llama-2 (7B) model performs better than the non-fine-tuned Mistral-v0.1 (7B) model in generating relevant and somewhat relevant meta-analysis abstracts.After fine-tuning, the rate of irrelevant content generation significantly decreases, resulting in a highly effective meta-analysis abstract generation.Table III also highlights the alignment between machine-generated and human-generated texts, which is referred by SWGT.The integration of RAG has shown promising outcomes in terms of generating relevant metaanalyses.Table V provides two instances of our method's creation of meta-analysis abstracts, demonstrating their encouraging resemblance to the abstracts of meta-articles.This validates the dependability of our method.</p>
<p>Our observation includes (1) fine-tuning with a large context scientific dataset, MAD, letting LLMs learn the patterns for generating meta-analysis content with higher relevancy.This proves the reliability of our approach to handling big data management challenges.(2) BLEU and ROUGE scores are utilized to compare relevant and irrelevant human-evaluated contexts, where a generated text is considered irrelevant if it contains less than 10% context translation using large metapapers' input (represented by BLEU).(3) Fine-tuned models exhibit improved performance over base models, indicating more significant agreement between the generated abstract in the RAG approach and the real meta-analysis abstract.It highlights how well the fine-tuning approach works to help models find the patterns required to generate high-quality meta-analysis abstracts.</p>
<p>C. Ablation Study</p>
<p>We perform ablation studies focusing on three crucial areas: prompt variant analysis, temperature variation, and the impact of our proposed loss metric on fine-tuned models.These studies provide deeper insights into the performance factors for meta-analysis generation.Prompt Variant Analysis: Prompt selection is fundamental in steering the meta-analysis generation process.In Table IV, we compare the effectiveness of two distinct prompts.We evaluated the relevancy and quality of meta-analysis abstracts produced by Llama-2 (7B) and Mistral-v0.1 (7B) across both prompts.Our results show that Prompt 1 consistently outperforms Prompt 2 in terms of relevancy, generating more accu- and ROUGE-L.The higher temperature yielded more diverse outputs without sacrificing relevancy or quality, making it the optimal setting for our meta-analysis generation tasks.Impact of Our Loss Metric: We implemented a specialized loss function, the ICD, designed to enhance the performance of meta-analysis summarization tasks.Fig 4(b) compares the performance of models fine-tuned with ICD against models using a standard loss function across both Llama-2 FT and Mistral-v0.1 FT versions.ICD emphasizes the directional similarity between the generated outputs and ground truth vectors by utilizing cosine similarity, capturing nuanced semantic details.This metric outperformed the standard loss function, improving the alignment between the generated summaries and their reference summaries.The ICD's ability to capture subtle semantic nuances beyond simple word matching proved crucial in fine-tuning the models for more accurate and coherent meta-analysis generation.</p>
<p>D. Discussion</p>
<p>This study represents insights into generating meta-analysis leveraging LLMs using a large-context scientific dataset, MAD.The result section provides evidence of our fine-tuned models' performance, showing the successive relevancy rate for generating meta-analysis.It was observed that the finetuned models for Llama-2 (7B) and Mistral-v0.1 (7B) outperformed their non-fine-tuned versions by generating significantly relevant meta-analyses.As expected, integrating RAG with fine-tuned models allows them to generate highly aligned meta-analyses.Limitations: One key limitation of this study is the maximum context length of the LLMs, which required chunking the input data.To mitigate potential information loss, overlapping context techniques and RAG were employed.However, due to hardware constraints, the model's evaluation was performed on only 50% of the test sets, which proved resource-intensive.Additionally, training the models in a highly quantized configuration limited the fine-tuning potential, impacting the ability to fully optimize the model's parameters for better performance.</p>
<p>V. CONCLUSION</p>
<p>This study demonstrates the effectiveness of automating meta-analysis generation using fine-tuned LLMs on extensive scientific datasets.Our approach significantly improved the relevance of generated meta-analysis abstracts, achieving 87.6% relevance and reducing irrelevance from 4.56% to 1.9%, demonstrating its potential and highlighting further promising TABLE V: Meta-analysis abstract generation from supporting article abstracts using our fine-tuned Mistral-v0.1 7B model, combined with RAG and efficient prompting."SP:" denotes each support article's abstract in S j , with "..." indicating continuation for multiple articles.Similarity with the original meta-articles' abstracts validates the effectiveness of the approach.</p>
<p>Prompt: Given a collection of abstracts from papers used in various medical fields for meta-analysis, generate a meta-analysis abstract.Summarize the key findings and provide numerical values or statistical information for specific observations that are commonly reported in the provided abstracts.</p>
<p>Example 1: Input (Support article's abstract, S j ):</p>
<p>Generated Meta-analysis abstract, ŷj (Similarity with ground truth [48]: 82.40%): SP: Several randomized controlled trials (RCTs) have investigated the use of probiotic/synbiotic in PCOS patients,... SP:</p>
<p>The effect of synbiotic supplementation on glycemic status in pregnant women remained controversial and ... SP: Probiotic and synbiotic products are being widely used by a large number of patients and clinicians; however, effects on cardiometabolic indices ... SP: This study aimed to review randomized clinical trials (RCTs) to clarify the effects of pro-/synbiotic supplementation ...fasting blood sugar (-2.52 mg/dl, 95% confidence interval (CI): -4.10 to -0.95), insulin (-2.27-µIU/mL, 95% CI: -3.40 to -1.14),...This meta-analysis abstract summarizes the findings of several randomized controlled trials (RCTs) on the effect of synbiotic intake on metabolic profiles in patients with diabetes.Heterogeneity was measured with a Q-test and the I 2 statistics.The results of the meta-analysis suggest that synbiotic intake can be useful as an adjunctive agent in the management of hyperglycemia in adults.The study pooled data by using the fix or random-effect model based on the results.A significant reduction in blood glucose levels was observed in patients who took synbiotic supplements compared to those who took placebo with statistical significance (p &lt; 0.05).Additionally, a significant decrease in glycated hemoglobin (HbA1c) levels was observed in patients who took synbiotic supplements compared to placebo (p &lt; 0.05).After taking synbiotic supplements, there were significant reductions in fasting blood sugar (FBS) (ES = -0.40,95% CI: -0.64, -0.15; p = 0.002, I2 = 69.2%, p ¡ 0.001), insulin (ES = -1.58;95% CI: -0.79, -0.30, p ¡ 0.001, I2 = 48.4%,p = 0.042), and HOMA-IR (Homeostatic Model Assessment of Insulin Resistance) (ES = -0.55;95% CI: -0.79, -0.30, p ¡ 0.001, I2 = 48.4%,p ¡ 0.001).Overall, the findings of this meta-analysis suggest that synbiotic intake can be an effective adjunctive agent in the management of hyperglycemia in adults.In this meta-analysis, we evaluated the effectiveness of traditional Chinese medicine (TCM) in treating COVID-19.A comprehensive search was conducted on a variety of electronic databases to identify studies reporting TCM treatment for COVID-19.The studies were screened according to inclusion and exclusion criteria, and a total of 10 studies were included in the analysis.These studies had a total of 1,880 participants who received TCM treatment for COVID-19.The analysis found that TCM had significant positive effects on the symptomatic relief of COVID-19.The pooled effect size for symptomatic relief was 0.67 (95% CI: 0.56 to 0.79), indicating a moderate effect size.The studies also reported on the impact of TCM on the severity of COVID-19.The pooled effect size for severity reduction was 0.51 (95% CI: 0.43 to 0.60), indicating a moderate effect size.In addition, the studies also reported on the impact of TCM on the mortality rate of COVID-19.The pooled effect size for mortality reduction was 0.37 (95% CI: 0.21 to 0.55), indicating a small but statistically significant effect size.Overall, this meta-analysis suggests that TCM may be an effective treatment for COVID-19, and further studies are needed to confirm these findings.It is important to note that this meta-analysis had several limitations, including the small number of studies, heterogeneity of TCM treatments, and variable outcomes.Out of the 89 outcome indicators that were assessed, 8 were found to be of moderate quality, 39 to be of low quality, and 41 to be of very low quality.In conclusion, TCM may be an effective treatment for COVID-19, with significant positive effects on symptomatic relief and severity reduction.However, more research is needed to confirm these findings and to establish the optimal TCM treatment for COVID-19.</p>
<p>research opportunities in automating scientific synthesis.We introduced novel methods to address the challenges posed by limited context length and resource constraints, including using ICD as a tailored loss metric for training.Integrating RAG further optimized the process by ensuring efficient synthesis of research findings without sacrificing context.Human evaluation confirmed the improvements in model performance, particularly in maintaining the relevancy and accuracy of structured meta-analysis content.Future works: While this study achieved notable improvements in meta-analysis generation, future research should focus on expanding the dataset in various fields that need meta-analysis and refining the model's ability to generate even more accurate and reliable outputs, particularly in resourceconstrained environments.Further optimizations to LLM finetuning and scaling could lead to broader applicability in automating complex scientific analysis.</p>
<p>ETHICS STATEMENT</p>
<p>This study was conducted with a strong commitment to ethical integrity, particularly in the generation and evaluation of meta-analysis abstracts in the scientific field using LLMs.We engaged 13 human evaluators from diverse backgrounds, ensuring their participation was voluntary and informed.We carefully collected only essential information to assess their qualifications for the task, and any data that could potentially identify participants were securely deleted after the evaluation was completed.We took significant measures to protect the well-being of all participants, ensuring that the evaluation process posed no physical or psychological risk.Recognizing that even subtle biases or inaccuracies in scientific research can have serious consequences, we implemented rigorous protocols to ensure that all generated content adhered to the highest ethical standards.Our approach was designed to avoid any language or conclusions that could perpetuate harm or inequity based on race, gender, or other social determinants of health.By adhering to these principles, we have ensured that our research upholds the highest ethical standards, fostering a safe and respectful environment for both human participants and the broader community.</p>
<p>Fig 2 depicts the methodology for fine-tuning LLMs and generating meta-analysis.Model Architecture Overview: We utilized two prominent LLMs in this study.(a) Llama-2 (7B), a transformer-based LLM developed by Meta, includes 32 attention heads, a 32,000-token vocabulary, and a context length of 4,096.It uses the Swish-Gated Linear Unit (SwiGLU) activation function [36].(b) Mistral-v0.1 (7B) features similar architecture with 32 attention heads and a 32,000-token vocabulary but offers a larger context length of 8,192.It employs the Sigmoid</p>
<p>Fig. 4 :
4
Fig.4: Investigating the impact of (a) Temperature variation: BLEU, ROUGE-1, ROUGE-2, and ROUGE-L scores vary with temperature changes for both the Llama-2 (7B) and Mistral-v0.1 (7B) models indicating 0.7 temperature has a better impact.(b) Loss Function impact: ICD loss significantly improves performance for Llama-2 (7B) FT and Mistral-v0.1 (7B) FT models, demonstrating its ability to capture more information than the default loss.</p>
<p>TABLE IV :
IV
Comparative Prompt Analysis: Demonstrating effectiveness through two different prompts, where Prompt 1 performs better than Prompt 2 in generating a more relevant meta-analysis.acollection of abstracts from papers used in various medical fields for meta-analysis, generate a meta-analysis abstract.Summarize the key findings and provide numerical values or statistical information for specific observations that are commonly reported in the provided abstracts.(Prompt1)Thereare given some abstracts of papers that are used for meta-analysis in different medical fields.Generate a meta-analysis abstract based on the given abstracts of papers.Please try to provide numerical values for any specific findings that were used in most of the abstracts.(Prompt 2)
PromptEvaluation MetricLlama-MistralLlama-Mistral Ours22 OursRelevant ↑83.580.585.487.6Somewhat Relevant ↑11.9414.112.710.4Irrelevant ↓4.565.131.92.1Relevant ↑6978.3972.482.8Somewhat Relevant ↑12.716.120.514.1Irrelevant ↓7.695.517.13.13rate and precise meta-analysis abstracts. Specifically, Prompt 1achieved higher relevancy scores across all versions of Llama-2 and Mistral, with fewer instances of irrelevant content. Giventhese results, Prompt 1 was used in all subsequent experiments.Varying Temperature: The temperature parameter con-trols the randomness of predictions, influencing the balancebetween exploration and exploitation during the generationprocess. We explored the impact of different temperatures (0.1,0.5, and 0.7) on summary quality. As shown in Fig 4(a), a tem-perature setting of 0.7 provided the best results across variousevaluation metrics, including BLEU, ROUGE-1, ROUGE-2,
Given</p>
<p>[49]ple 2: Input (Support article's abstract, S j ):Generated Meta-analysis abstract, ŷj (Similarity with ground truth[49]: 85.73%): SP: As the global epidemic continues to spread, countries have tappe...SP:Introduction: Integrated Chinese and Western medicine (integr...SP: A simple, efficient, and environmentally friendly electro-Fen...SP: Currently, coronavirus disease 2019 (COVID-19), which can lead to... SP: Background: Until now, there is no clinically approved spe...SP: Chinese medicine (CM) has been used to treat Novel Coronavi...SP:Integration of Chinese medical drugs (CMD) and we...SP:This review aims to evaluate the supportive effe...SP:We systematically studied the passivation process of 6082 aluminium alloy under the bending stress... SP: Coronavirus disease 2019 (COVID-19) is an eme...SP:There is currently no drug or therapy that cures COVID-19, a highly contagious and... SP: The outbreak of coronavirus disease 2019...SP: Coronavirus disease 2019 (COVID-19) has eme...SP:Background: The coronavirus disease 2019 (COVID-19) pandemic...</p>
<p>Pre-trained StableLM-Base-Alpha 7B. 43</p>
<p>We generated summaries from 100 samples. 2 writer summaries: article summarization dataset. Open-I, Medical radiological dataset. evaluated our models on 120 samples</p>
<p>Large corpus dataset containing scientific article data. We evaluated 20 samples. Chunking the samples was required. Cl-Scisumm, as the context lengths are larger than allowable</p>
<p>Established A well-established method was proven by the given papers for these three specific datasets. The BLEU and ROUGE scores given by those studies can't be achieved with our investigated LLMs, as their established methodology includes fine-tuning and evaluating the given large dataset. </p>
<p>Meta-Analysis, ser. Cambridge Handbooks in Psychology. Y Jadotte, A Moyer, J Gurevitch, 2023Cambridge University Press</p>
<p>Incorporating quality scores in meta-analysis. S Ahn, B Becker, Journal of Educational and Behavioral Statistics. 3652011</p>
<p>Conducting a meta-analysis in the age of open science: Tools, tips, and practical recommendations. D Moreau, B Gamble, Psychological Methods. 2734262022</p>
<p>Threshold analysis as an alternative to grade for assessing confidence in guideline recommendations based on network metaanalyses. D M Phillippo, S Dias, N J Welton, D M Caldwell, N Taske, A Ades, Annals of internal medicine. 17082019</p>
<p>Consumer engagement in health care policy, research and services: A systematic review and metaanalysis of methods and effects. L K Wiles, D Kay, J A Luker, A Worley, J Austin, A Ball, A Bevan, M Cousins, S Dalton, E Hodges, PloS one. 171e02618082022</p>
<p>Systematic reviews and metaanalysis: understanding the best evidence in primary healthcare. S Gopalakrishnan, P Ganeshkumar, Journal of family medicine and primary care. 212013</p>
<p>Meta-analysis in medical research. A.-B Haidich, Hippokratia. 141292010Suppl</p>
<p>An overview of meta-analysis for clinicians. Y H Lee, The Korean Journal of Internal Medicine. 3322772018</p>
<p>Effectiveness of public health measures in reducing the incidence of covid-19, sars-cov-2 transmission, and covid-19 mortality: systematic review and meta-analysis. S Talic, S Shah, H Wild, D Gasevic, A Maharaj, Z Ademi, X Li, W Xu, I Mesa-Eguiagaray, J Rostron, bmj. 3752021</p>
<p>The effect of litter materials on broiler performance: a systematic review and meta-analysis. T Toledo, C Pich, A Roll, M Dai Prá, F Leivas Leite, E Gonc ¸alves Xavier, V Roll, British poultry science. 6062019</p>
<p>The impact of conservation farming practices on mediterranean agro-ecosystem services provisioning-a meta-analysis. H Lee, S Lautenbach, A P G Nieto, A Bondeau, W Cramer, I R Geijzendorffer, Regional Environmental Change. 1982019</p>
<p>Environmental sustainability and behavioral science: Meta-analysis of proenvironmental behavior experiments. R Osbaldiston, J P Schott, Environment and behavior. 4422012</p>
<p>A systematic review and meta-analysis of psychological interventions to improve mental wellbeing. J Van Agteren, M Iasiello, L Lo, J Bartholomaeus, Z Kopsaftis, M Carey, M Kyrios, Nature human behaviour. 552021</p>
<p>Governance and deforestation-a meta-analysis in economics. J Wehkamp, N Koch, S Lübbers, S Fuss, Ecological economics. 1442018</p>
<p>The power of feedback revisited: A meta-analysis of educational feedback research. B Wisniewski, K Zierer, J Hattie, Frontiers in psychology. 104876622020</p>
<p>Reading subtext: Evaluating large language models on short story summarization with writers. M Subbiah, S Zhang, L B Chilton, K Mckeown, arXiv:2403.010612024arXiv preprint</p>
<p>Improving multi-stage long document summarization with enhanced coarse summarizer. J Lim, H.-J Song, Proceedings of the 4th New Frontiers in Summarization Workshop. the 4th New Frontiers in Summarization Workshop2023</p>
<p>Financial report chunking for effective retrieval augmented generation. A J Yepes, Y You, J Milczek, S Laverde, L Li, arXiv:2402.051312024arXiv preprint</p>
<p>Retrieval augmented generation and representative vector summarization for large unstructured textual data in medical education. S Manathunga, Y Illangasekara, arXiv:2308.004792023arXiv preprint</p>
<p>Intervention methods for improving reduced heart rate variability in patients with major depressive disorder: A systematic review and meta-analysis. S Chen, H Wang, J Yue, N Guan, X Wang, Comprehensive Psychiatry. 1191523472022</p>
<p>Abstractive long text summarization using large language models. G Keswani, W Bisen, H Padwad, Y Wankhedkar, S Pandey, A Soni, International Journal of Intelligent Systems and Applications in Engineering. 1212s2024</p>
<p>Evaluation of chatgpt-generated medical responses: A systematic review and metaanalysis. Q Wei, Z Yao, Y Cui, B Wei, Z Jin, X Xu, Journal of Biomedical Informatics. 1046202024</p>
<p>Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models. T Reason, E Benbow, J Langham, A Gimblett, S L Klijn, B Malcolm, PharmacoEconomics-Open. 2024</p>
<p>A global database for conducting systematic reviews and meta-analyses in innovation and quality management. T Csizmadia, A I Katona, Scientific Data. 913012022</p>
<p>Metadata research development: A bibliometric study on science direct. S Yudhanto, T Asmiyanto, Library Philosophy and Practice. 2021</p>
<p>News summarization and evaluation in the era of gpt-3. T Goyal, J J Li, G Durrett, arXiv:2209.123562022arXiv preprint</p>
<p>Benchmarking large language models for news summarization. T Zhang, F Ladhak, E Durmus, P Liang, K Mckeown, T B Hashimoto, Transactions of the Association for Computational Linguistics. 122024</p>
<p>A survey on evaluation of summarization methods. L Ermakova, J V Cossu, J Mothe, Information processing &amp; management. 201956</p>
<p>Humanely: Human evaluation of llm yield, using a novel web based evaluation tool. R Awasthi, S Mishra, D Mahapatra, A Khanna, K Maheshwari, J Cywinski, F Papay, P Mathur, 2023Cold Spring Harbor Laboratory Press</p>
<p>Characterised llms affect its evaluation of summary and translation. Y Lu, Y.-T Lin, 2023</p>
<p>Llm-based nlg evaluation: Current status and challenges. M Gao, X Hu, J Ruan, X Pu, X Wan, arXiv:2402.013832024arXiv preprint</p>
<p>It's all relative!-a synthetic query generation approach for improving zero-shot relevance prediction. A Chaudhary, K Raman, M Bendersky, arXiv:2311.079302023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Glu variants improve transformer. N Shazeer, arXiv:2002.052022020arXiv preprint</p>
<p>Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. S Elfwing, E Uchibe, K Doya, Neural networks. 1072018</p>
<p>Adapted large language models can outperform medical experts in clinical text summarization. D Van Veen, C Van Uden, L Blankemeier, J.-B Delbrouck, A Aali, C Bluethgen, A Pareek, M Polacin, E P Reis, A Seehofnerová, Nature Medicine. 2024</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. M Yasunaga, J Kasai, R Zhang, A R Fabbri, I Li, D Friedman, D R Radev, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>The falcon series of open language models. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, É Goffinet, D Hesslow, J Launay, Q Malartic, arXiv:2311.168672023arXiv preprint</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Teaching small language models how to reason. A Mitra, L D Corro, S Mahajan, A Codas, C Simoes, S Agrawal, X Chen, A Razdaibiedina, E Jones, K Aggarwal, H Palangi, G Zheng, C Rosset, H Khanpour, A Awadallah, Orca. 22023</p>
<p>J Tow, StableLM Alpha v2 Models. </p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Rouge: A package for automatic evaluation of summaries. C.-Y Lin, Text summarization branches out. 2004</p>
<p>Distance weighted cosine similarity measure for text classification. B Li, L Han, Intelligent Data Engineering and Automated Learning-IDEAL 2013: 14th International Conference, IDEAL 2013. Proceedings. Hefei, ChinaSpringerOctober 20-23, 2013. 201314</p>
<p>Synbiotic as an adjunctive agent can be useful in the management of hyperglycemia in adults: An umbrella review and meta-research of metaanalysis studies. V Musazadeh, A H Faghfouri, Z Kavyani, P Dehghan, Journal of Functional Foods. 991053552022</p>
<p>Traditional chinese medicine in covid-19. M Lyu, G Fan, G Xiao, T Wang, D Xu, J Gao, S Ge, Q Li, Y Ma, H Zhang, Acta Pharmaceutica Sinica B. 11112021</p>            </div>
        </div>

    </div>
</body>
</html>
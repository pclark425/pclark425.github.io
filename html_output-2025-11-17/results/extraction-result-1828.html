<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1828 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1828</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1828</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-c0dccd9be123057ec82a6747d8fec9cc34699a6d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c0dccd9be123057ec82a6747d8fec9cc34699a6d" target="_blank">Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This paper presents Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no real-world data and learns to translate randomized rendered images into their equivalent non-randomized, canonical versions.</p>
                <p><strong>Paper Abstract:</strong> Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to real-world ones. Using domain adaptation methods to cross this "reality gap" requires a large amount of unlabelled real-world data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no real-world data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain 70% zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves 91%, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than 99%.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1828.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1828.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Randomized-to-Canonical Adaptation Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cGAN-based image translation method that learns to map heavily randomized simulated images to a fixed canonical simulator rendering, enabling real-world images to be translated into the same canonical domain and allowing policies trained in that canonical simulation to operate on real robots without using any real images to train the adapter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>QT-Opt grasping agent on Kuka IIWA robots</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A vision-based closed-loop reinforcement learning grasping system (QT-Opt) that takes a 472×472 RGB image, gripper aperture and gripper height as state; in this work the image input is channel-wise concatenation of the raw real/randomized image and the RCAN-generated canonical image; executed on Kuka IIWA arms to pick unseen objects from a tray.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic grasping / manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Bullet (PyBullet) simulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A Bullet-physics-based simulated environment with a Kuka IIWA arm, a tray with a divider, an over-the-shoulder camera rendering RGB, depth and segmentation, and a large population of procedurally generated and ShapeNet objects; simulator uses default renderer to produce textured, lit images and simulates rigid-body dynamics and collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>moderate-fidelity physics and rendering (rigid-body dynamics with renderer-produced lighting and textures; not photorealistic)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body contact/collision dynamics, object geometry (ShapeNet + procedurally generated objects), camera viewpoint, lighting direction/intensity, textures (applied from a large set), shadow rendering by the simulator, RGB and depth rendering, segmentation masks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>no photorealistic material/modeling or calibrated real-world sensor noise model, approximate contact/friction properties (no detailed contact parameter calibration reported), limited modeling of actuator dynamics/delays, limited soft-body/deformable physics; some rendering differences and generator artifacts (noted for gripper) remain.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Multiple Kuka IIWA robots with an over-the-shoulder RGB camera (472×472) overlooking a physical tray; test set of previously unseen objects; real on-policy data collection for joint finetuning using ~1,000 diverse training objects placed in trays; evaluation is repeated 102 grasp attempts per robot/test set.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>closed-loop robotic grasping of diverse, previously unseen objects (pick-up grasp success in a tray)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Policy: reinforcement learning (QT-Opt, off-policy Q-learning variant) trained entirely in canonical simulation; Adapter: supervised cGAN training on paired randomized-to-canonical simulated image pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>grasp success rate (%) measured over 102 grasp attempts per robot/test set (average success rate across robots/objects)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>≈99% grasp success in canonical simulation (QT-Opt trained in canonical sim achieves ≈99% in sim)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Zero-shot (no real grasps): 70% real-world success (RCAN); joint finetuning: 91% after +5,000 real on-policy grasps, 94% after +28,000 on-policy grasps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Extensive visual randomization used to train the adapter: >5,000 texture images applied to tray/objects/arm/floor, random lighting position/direction/color/brightness, randomized camera positions, randomized tray and arm position/size (small uniform perturbations), diverse background images; heavy/mild/medium randomization regimes were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>visual domain shift (textures, lighting, backgrounds, shadows), distributional differences in object instances, generator imperfections/artifacts (notably imperfect gripper reconstruction hindering regrasping), unmodeled real sensor noise and potentially unmodeled dynamics/friction differences.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Learning a randomized→canonical cGAN using paired randomized & canonical sim images (no real images) to produce canonical renderings from real images; training the grasp policy in the canonical sim; concatenating original and adapted images as input to the policy (provides both complex real view and simplified canonical view); heavy visual randomization during adapter training to improve generalization; small real-world joint finetuning (5k on-policy grasps) with generator frozen to regain regrasping behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No quantitative fidelity threshold provided; qualitatively the paper identifies that a stable canonical rendering (fixed lighting, uniform colors for background/tray/arm) that preserves object identity is important so the generator can learn geometry/shadow rendering — photorealism is not required but consistent canonical geometry/lighting is.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Joint on-policy finetuning of the QT-Opt policy in the real world while keeping the RCAN generator fixed: experiments with +5,000 real on-policy grasps (one day of data) and +28,000 grasps; during finetuning gradients are applied only to the grasping network; simulated on-policy data streams were also used concurrently.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Policies trained in canonical sim achieved ≈21% zero-shot real success; direct training on mild/medium/heavy domain randomization produced 37% / 35% / 33% zero-shot real success respectively; RCAN reached 70% zero-shot — substantially better than direct domain randomization; in-sim performance was ≈98–99% for all simulated training variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RCAN (randomized→canonical adaptation) enables effective sim-to-real transfer for vision-based closed-loop grasping without using any real images to train the adapter, achieving 70% zero-shot real grasp success and 91% after only 5k real grasps (matching or exceeding large-data real-trained baselines); RCAN outperforms naive domain randomization for zero-shot transfer, provides an interpretable canonical intermediate representation, and offloads visual complexity to a generator which simplifies policy learning; remaining generator artifacts (e.g., imperfect gripper rendering) can hinder certain behaviors like regrasping but are largely corrected during small-scale real finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_context_snippet</strong></td>
                            <td>We present Randomized-to-Canonical Adaptation Networks (RCANs) ... translate randomized rendered images into their equivalent non-randomized, canonical versions ... train a vision-based closed-loop grasping RL agent in simulation, and then transfer it to the real world to attain 70% zero-shot grasp success on unseen objects; with 5,000 real grasps achieve 91%.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1828.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1828.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization (visual randomization in simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-real technique that trains policies on a wide range of randomized visual (and optionally physical) simulation parameters so the learned features become invariant to superficial differences between sim and real.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>QT-Opt grasping agent (baseline variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Same QT-Opt closed-loop grasping agent tested with policies trained directly on randomized simulated images (mild/medium/heavy randomization regimes) and evaluated zero-shot and after limited real-world finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic grasping / manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Bullet (PyBullet) simulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator with randomized rendering parameters (textures, lighting, camera), object drops, and rigid-body dynamics; used to generate training images for policies trained directly on randomized inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate rendering with heavy randomization (non-photorealistic); focuses on diversity rather than photoreal fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>visual appearance diversity (textures, lighting, camera pose, backgrounds), object geometry, basic rigid-body dynamics and collisions in simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>no explicit modeling of real sensor noise distributions or finely calibrated contact/friction parameters; no guarantee of photorealism or per-material rendering fidelity; actuator dynamics and delays not emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same Kuka IIWA setup with over-the-shoulder RGB camera and trays with unseen test objects; policies trained on randomized sim were tested zero-shot and then jointly finetuned with 5k real grasps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>closed-loop robotic grasping</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (QT-Opt) trained directly on randomized simulated observations (several randomization regimes: mild, medium, heavy).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>grasp success rate (%) over 102 grasp attempts per evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>≈98% in simulation for policies trained under randomization</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Zero-shot real-world: mild 37%, medium 35%, heavy 33%; after +5,000 real on-policy finetuning these increase to ~77–85% depending on regime; +28,000 finetuning for heavy randomization yields up to 92%.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Mild: varying tray/object/arm textures and colors, lighting direction/brightness, background images (6 images); Medium: adds diverse floor/background images; Heavy: extensive scheme used to train RCAN including >5,000 textures, lighting/color/position randomization, camera and small pose/size perturbations of tray and arm.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Residual visual differences despite randomization, domain shift in object distribution, possible instability of some RL algorithms under heavy randomization (though QT-Opt was stable in this work), unmodeled real sensor/dynamics differences.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Large diversity of randomization helps generalization; joint real-world finetuning (even modest amounts) closes the remaining gap substantially; choice of RL algorithm (QT-Opt) that remained stable under heavy randomization was important.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No quantitative fidelity thresholds; observation that heavy visual diversity (not photorealism) plus a stable RL algorithm can enable good finetuning efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Joint on-policy finetuning with +5,000 real grasps (reported) and additional experiments with +28,000 on-policy grasps; performance improved markedly after finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Zero-shot success decreases with canonical-only training; mild/medium/heavy randomization achieved similar in-sim performance (~98%) but only ~33–37% zero-shot real success; finetuning quickly raises performance (most gains within first ~2k real grasps).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training directly on domain randomization yields high in-sim performance but poor zero-shot real transfer (≈33–37%); however, domain randomized pre-training is an effective initializer—joint finetuning with relatively few real grasps (≈5k) rapidly recovers strong real-world performance (~77–85%), indicating domain randomization can be a powerful pre-training regime even if zero-shot transfer is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1828.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1828.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QT-Opt (real-trained baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation (as used in baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-policy continuous-action Q-learning algorithm used for vision-based grasping; the paper compares RCAN-trained QT-Opt variants against a QT-Opt baseline trained on large real-world datasets (580k grasps).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>QT-Opt policy (real-data baseline) on Kuka IIWA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>QT-Opt learned from 580,000 off-policy real-world grasps (baseline reported in prior work) and then jointly finetuned with +28,000 on-policy grasps; serves as a high-data real-world performance reference (≈87% then 96%).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic grasping / manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Baseline trained on 580,000 off-policy real grasps collected on physical Kuka IIWA robots with over-the-shoulder RGB camera; joint finetuning with +5,000 and +28,000 on-policy real grasps were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>closed-loop robotic grasping</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (QT-Opt) trained on large real-world dataset (580k off-policy grasps) and joint on-policy finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>grasp success rate (%) over 102 grasp attempts per evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>87% after 580k off-policy real grasps; 85% after +5,000 on-policy finetuning (per paper's baseline reporting), 96% after +28,000 on-policy finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Very large real-world dataset (580k grasps) and additional on-policy fine-tuning (+28k) enables high performance and corrective behaviors such as regrasping.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>+5,000 and +28,000 on-policy real grasp episodes were used for additional joint finetuning in baseline experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large-scale real-world training (580k grasps) yields strong grasping performance (≈87%); RCAN with only 5k real grasps can match or exceed this performance (91% in RCAN experiments), demonstrating large reductions in required real data are possible when combining sim-trained policies with effective sim-to-real adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1828.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1828.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraspGAN (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraspGAN (pixel + feature-level domain adaptation for grasping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method combining pixel-level and feature-level domain adaptation to reduce real-data requirements for grasping; mentioned as related work that requires substantial unlabeled real images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic grasping / domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>grasping (related prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>pixel-level + feature-level domain adaptation (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>requires significant amounts of unlabeled real-world data (limitation noted)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>combines pixel-level and feature-level adaptation, but requires large unlabeled real datasets</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as orthogonal approach: GraspGAN reduces real labeled-data needs but relies on large amounts of unlabeled real images; RCAN differs by not requiring real images to train the adapter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1828.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1828.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VR Goggles (real-to-sim mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VR Goggles for Robots: Real-to-Sim Domain Adaptation (Zhang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior reality-to-simulation approach that adapts real images to a simulated domain for control; mentioned for conceptual similarity but differs in that it used unlabeled real-world data and different adaptation techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>visual control / sim-to-real / real-to-sim</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>visual control (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>real-to-sim domain adaptation (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>used unlabeled real-world data to train adaptation; cited as a technique that decouples adaptation from policy training</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>decoupling adaptation from training such that only adapter needs retraining when environment changes</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related prior work exploring reality-to-simulation adaptation; distinguishes RCAN by RCAN not using any real images to train the adapter (RCAN learns randomized→canonical in simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping <em>(Rating: 2)</em></li>
                <li>VR Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control <em>(Rating: 2)</em></li>
                <li>CAD2RL: Real single-image flight without a single real image <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1828",
    "paper_id": "paper-c0dccd9be123057ec82a6747d8fec9cc34699a6d",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "RCAN",
            "name_full": "Randomized-to-Canonical Adaptation Networks",
            "brief_description": "A cGAN-based image translation method that learns to map heavily randomized simulated images to a fixed canonical simulator rendering, enabling real-world images to be translated into the same canonical domain and allowing policies trained in that canonical simulation to operate on real robots without using any real images to train the adapter.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "QT-Opt grasping agent on Kuka IIWA robots",
            "agent_system_description": "A vision-based closed-loop reinforcement learning grasping system (QT-Opt) that takes a 472×472 RGB image, gripper aperture and gripper height as state; in this work the image input is channel-wise concatenation of the raw real/randomized image and the RCAN-generated canonical image; executed on Kuka IIWA arms to pick unseen objects from a tray.",
            "domain": "robotic grasping / manipulation",
            "virtual_environment_name": "Bullet (PyBullet) simulation",
            "virtual_environment_description": "A Bullet-physics-based simulated environment with a Kuka IIWA arm, a tray with a divider, an over-the-shoulder camera rendering RGB, depth and segmentation, and a large population of procedurally generated and ShapeNet objects; simulator uses default renderer to produce textured, lit images and simulates rigid-body dynamics and collisions.",
            "simulation_fidelity_level": "moderate-fidelity physics and rendering (rigid-body dynamics with renderer-produced lighting and textures; not photorealistic)",
            "fidelity_aspects_modeled": "rigid-body contact/collision dynamics, object geometry (ShapeNet + procedurally generated objects), camera viewpoint, lighting direction/intensity, textures (applied from a large set), shadow rendering by the simulator, RGB and depth rendering, segmentation masks.",
            "fidelity_aspects_simplified": "no photorealistic material/modeling or calibrated real-world sensor noise model, approximate contact/friction properties (no detailed contact parameter calibration reported), limited modeling of actuator dynamics/delays, limited soft-body/deformable physics; some rendering differences and generator artifacts (noted for gripper) remain.",
            "real_environment_description": "Multiple Kuka IIWA robots with an over-the-shoulder RGB camera (472×472) overlooking a physical tray; test set of previously unseen objects; real on-policy data collection for joint finetuning using ~1,000 diverse training objects placed in trays; evaluation is repeated 102 grasp attempts per robot/test set.",
            "task_or_skill_transferred": "closed-loop robotic grasping of diverse, previously unseen objects (pick-up grasp success in a tray)",
            "training_method": "Policy: reinforcement learning (QT-Opt, off-policy Q-learning variant) trained entirely in canonical simulation; Adapter: supervised cGAN training on paired randomized-to-canonical simulated image pairs.",
            "transfer_success_metric": "grasp success rate (%) measured over 102 grasp attempts per robot/test set (average success rate across robots/objects)",
            "transfer_performance_sim": "≈99% grasp success in canonical simulation (QT-Opt trained in canonical sim achieves ≈99% in sim)",
            "transfer_performance_real": "Zero-shot (no real grasps): 70% real-world success (RCAN); joint finetuning: 91% after +5,000 real on-policy grasps, 94% after +28,000 on-policy grasps.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Extensive visual randomization used to train the adapter: &gt;5,000 texture images applied to tray/objects/arm/floor, random lighting position/direction/color/brightness, randomized camera positions, randomized tray and arm position/size (small uniform perturbations), diverse background images; heavy/mild/medium randomization regimes were evaluated.",
            "sim_to_real_gap_factors": "visual domain shift (textures, lighting, backgrounds, shadows), distributional differences in object instances, generator imperfections/artifacts (notably imperfect gripper reconstruction hindering regrasping), unmodeled real sensor noise and potentially unmodeled dynamics/friction differences.",
            "transfer_enabling_conditions": "Learning a randomized→canonical cGAN using paired randomized & canonical sim images (no real images) to produce canonical renderings from real images; training the grasp policy in the canonical sim; concatenating original and adapted images as input to the policy (provides both complex real view and simplified canonical view); heavy visual randomization during adapter training to improve generalization; small real-world joint finetuning (5k on-policy grasps) with generator frozen to regain regrasping behaviors.",
            "fidelity_requirements_identified": "No quantitative fidelity threshold provided; qualitatively the paper identifies that a stable canonical rendering (fixed lighting, uniform colors for background/tray/arm) that preserves object identity is important so the generator can learn geometry/shadow rendering — photorealism is not required but consistent canonical geometry/lighting is.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Joint on-policy finetuning of the QT-Opt policy in the real world while keeping the RCAN generator fixed: experiments with +5,000 real on-policy grasps (one day of data) and +28,000 grasps; during finetuning gradients are applied only to the grasping network; simulated on-policy data streams were also used concurrently.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Policies trained in canonical sim achieved ≈21% zero-shot real success; direct training on mild/medium/heavy domain randomization produced 37% / 35% / 33% zero-shot real success respectively; RCAN reached 70% zero-shot — substantially better than direct domain randomization; in-sim performance was ≈98–99% for all simulated training variants.",
            "key_findings": "RCAN (randomized→canonical adaptation) enables effective sim-to-real transfer for vision-based closed-loop grasping without using any real images to train the adapter, achieving 70% zero-shot real grasp success and 91% after only 5k real grasps (matching or exceeding large-data real-trained baselines); RCAN outperforms naive domain randomization for zero-shot transfer, provides an interpretable canonical intermediate representation, and offloads visual complexity to a generator which simplifies policy learning; remaining generator artifacts (e.g., imperfect gripper rendering) can hinder certain behaviors like regrasping but are largely corrected during small-scale real finetuning.",
            "citation_context_snippet": "We present Randomized-to-Canonical Adaptation Networks (RCANs) ... translate randomized rendered images into their equivalent non-randomized, canonical versions ... train a vision-based closed-loop grasping RL agent in simulation, and then transfer it to the real world to attain 70% zero-shot grasp success on unseen objects; with 5,000 real grasps achieve 91%.",
            "uuid": "e1828.0",
            "source_info": {
                "paper_title": "Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "Domain Randomization",
            "name_full": "Domain Randomization (visual randomization in simulation)",
            "brief_description": "A sim-to-real technique that trains policies on a wide range of randomized visual (and optionally physical) simulation parameters so the learned features become invariant to superficial differences between sim and real.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "QT-Opt grasping agent (baseline variants)",
            "agent_system_description": "Same QT-Opt closed-loop grasping agent tested with policies trained directly on randomized simulated images (mild/medium/heavy randomization regimes) and evaluated zero-shot and after limited real-world finetuning.",
            "domain": "robotic grasping / manipulation",
            "virtual_environment_name": "Bullet (PyBullet) simulation",
            "virtual_environment_description": "Simulator with randomized rendering parameters (textures, lighting, camera), object drops, and rigid-body dynamics; used to generate training images for policies trained directly on randomized inputs.",
            "simulation_fidelity_level": "approximate rendering with heavy randomization (non-photorealistic); focuses on diversity rather than photoreal fidelity",
            "fidelity_aspects_modeled": "visual appearance diversity (textures, lighting, camera pose, backgrounds), object geometry, basic rigid-body dynamics and collisions in simulator.",
            "fidelity_aspects_simplified": "no explicit modeling of real sensor noise distributions or finely calibrated contact/friction parameters; no guarantee of photorealism or per-material rendering fidelity; actuator dynamics and delays not emphasized.",
            "real_environment_description": "Same Kuka IIWA setup with over-the-shoulder RGB camera and trays with unseen test objects; policies trained on randomized sim were tested zero-shot and then jointly finetuned with 5k real grasps.",
            "task_or_skill_transferred": "closed-loop robotic grasping",
            "training_method": "Reinforcement learning (QT-Opt) trained directly on randomized simulated observations (several randomization regimes: mild, medium, heavy).",
            "transfer_success_metric": "grasp success rate (%) over 102 grasp attempts per evaluation",
            "transfer_performance_sim": "≈98% in simulation for policies trained under randomization",
            "transfer_performance_real": "Zero-shot real-world: mild 37%, medium 35%, heavy 33%; after +5,000 real on-policy finetuning these increase to ~77–85% depending on regime; +28,000 finetuning for heavy randomization yields up to 92%.",
            "transfer_success": false,
            "domain_randomization_used": true,
            "domain_randomization_details": "Mild: varying tray/object/arm textures and colors, lighting direction/brightness, background images (6 images); Medium: adds diverse floor/background images; Heavy: extensive scheme used to train RCAN including &gt;5,000 textures, lighting/color/position randomization, camera and small pose/size perturbations of tray and arm.",
            "sim_to_real_gap_factors": "Residual visual differences despite randomization, domain shift in object distribution, possible instability of some RL algorithms under heavy randomization (though QT-Opt was stable in this work), unmodeled real sensor/dynamics differences.",
            "transfer_enabling_conditions": "Large diversity of randomization helps generalization; joint real-world finetuning (even modest amounts) closes the remaining gap substantially; choice of RL algorithm (QT-Opt) that remained stable under heavy randomization was important.",
            "fidelity_requirements_identified": "No quantitative fidelity thresholds; observation that heavy visual diversity (not photorealism) plus a stable RL algorithm can enable good finetuning efficiency.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Joint on-policy finetuning with +5,000 real grasps (reported) and additional experiments with +28,000 on-policy grasps; performance improved markedly after finetuning.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Zero-shot success decreases with canonical-only training; mild/medium/heavy randomization achieved similar in-sim performance (~98%) but only ~33–37% zero-shot real success; finetuning quickly raises performance (most gains within first ~2k real grasps).",
            "key_findings": "Training directly on domain randomization yields high in-sim performance but poor zero-shot real transfer (≈33–37%); however, domain randomized pre-training is an effective initializer—joint finetuning with relatively few real grasps (≈5k) rapidly recovers strong real-world performance (~77–85%), indicating domain randomization can be a powerful pre-training regime even if zero-shot transfer is limited.",
            "uuid": "e1828.1",
            "source_info": {
                "paper_title": "Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "QT-Opt (real-trained baseline)",
            "name_full": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation (as used in baselines)",
            "brief_description": "An off-policy continuous-action Q-learning algorithm used for vision-based grasping; the paper compares RCAN-trained QT-Opt variants against a QT-Opt baseline trained on large real-world datasets (580k grasps).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "QT-Opt policy (real-data baseline) on Kuka IIWA",
            "agent_system_description": "QT-Opt learned from 580,000 off-policy real-world grasps (baseline reported in prior work) and then jointly finetuned with +28,000 on-policy grasps; serves as a high-data real-world performance reference (≈87% then 96%).",
            "domain": "robotic grasping / manipulation",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": "Baseline trained on 580,000 off-policy real grasps collected on physical Kuka IIWA robots with over-the-shoulder RGB camera; joint finetuning with +5,000 and +28,000 on-policy real grasps were evaluated.",
            "task_or_skill_transferred": "closed-loop robotic grasping",
            "training_method": "Reinforcement learning (QT-Opt) trained on large real-world dataset (580k off-policy grasps) and joint on-policy finetuning.",
            "transfer_success_metric": "grasp success rate (%) over 102 grasp attempts per evaluation",
            "transfer_performance_sim": null,
            "transfer_performance_real": "87% after 580k off-policy real grasps; 85% after +5,000 on-policy finetuning (per paper's baseline reporting), 96% after +28,000 on-policy finetuning.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": "Very large real-world dataset (580k grasps) and additional on-policy fine-tuning (+28k) enables high performance and corrective behaviors such as regrasping.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "+5,000 and +28,000 on-policy real grasp episodes were used for additional joint finetuning in baseline experiments.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Large-scale real-world training (580k grasps) yields strong grasping performance (≈87%); RCAN with only 5k real grasps can match or exceed this performance (91% in RCAN experiments), demonstrating large reductions in required real data are possible when combining sim-trained policies with effective sim-to-real adaptation.",
            "uuid": "e1828.2",
            "source_info": {
                "paper_title": "Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "GraspGAN (related work)",
            "name_full": "GraspGAN (pixel + feature-level domain adaptation for grasping)",
            "brief_description": "A prior method combining pixel-level and feature-level domain adaptation to reduce real-data requirements for grasping; mentioned as related work that requires substantial unlabeled real images.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "robotic grasping / domain adaptation",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": "grasping (related prior work)",
            "training_method": "pixel-level + feature-level domain adaptation (prior work)",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "requires significant amounts of unlabeled real-world data (limitation noted)",
            "transfer_enabling_conditions": "combines pixel-level and feature-level adaptation, but requires large unlabeled real datasets",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Mentioned as orthogonal approach: GraspGAN reduces real labeled-data needs but relies on large amounts of unlabeled real images; RCAN differs by not requiring real images to train the adapter.",
            "uuid": "e1828.3",
            "source_info": {
                "paper_title": "Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "VR Goggles (real-to-sim mention)",
            "name_full": "VR Goggles for Robots: Real-to-Sim Domain Adaptation (Zhang et al.)",
            "brief_description": "A prior reality-to-simulation approach that adapts real images to a simulated domain for control; mentioned for conceptual similarity but differs in that it used unlabeled real-world data and different adaptation techniques.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "visual control / sim-to-real / real-to-sim",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": "visual control (related work)",
            "training_method": "real-to-sim domain adaptation (related work)",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "used unlabeled real-world data to train adaptation; cited as a technique that decouples adaptation from policy training",
            "transfer_enabling_conditions": "decoupling adaptation from training such that only adapter needs retraining when environment changes",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Cited as related prior work exploring reality-to-simulation adaptation; distinguishes RCAN by RCAN not using any real images to train the adapter (RCAN learns randomized→canonical in simulation).",
            "uuid": "e1828.4",
            "source_info": {
                "paper_title": "Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks",
                "publication_date_yy_mm": "2018-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping",
            "rating": 2
        },
        {
            "paper_title": "VR Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control",
            "rating": 2
        },
        {
            "paper_title": "CAD2RL: Real single-image flight without a single real image",
            "rating": 1
        }
    ],
    "cost": 0.018556749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks</h1>
<p>Stephen James ${ }^{1}$, Paul Wohlhart ${ }^{2}$, Mrinal Kalakrishnan ${ }^{2}$, Dmitry Kalashnikov ${ }^{3}$, Alex Irpan ${ }^{3}$, Julian Ibarz ${ }^{3}$, Sergey Levine ${ }^{3,5}$, Raia Hadsell ${ }^{1}$, Konstantinos Bousmalis ${ }^{4}$<br>slj12@imperial.ac.uk, {wohlhart, kalakris}@x.team,<br>{dkalashnikov, alexirpan, julianibarz, slevine, raia, konstantinos} @google.com,</p>
<h2>Abstract</h2>
<p>Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to realworld ones. Using domain adaptation methods to cross this "reality gap" requires a large amount of unlabelled realworld data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no realworld data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain $70 \%$ zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves $91 \%$, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than $99 \%$.</p>
<h2>1. Introduction</h2>
<p>Deep learning for vision-based robotics tasks is a promising research direction [58]. However, it necessitates large amounts of real-world data, which is a severe</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We learn a generator that translates randomized simulation images to a chosen canonical simulation version which are then used to train a robot grasping agent (top). The system can then be used to translate real-world images to canonical images, and consequently allow for Sim-toReal transfer of the agent (bottom). Feeding both source and target images to the agent allows for joint finetuning of the agent in the real world.
limitation, since real-robot data collection is expensive and cumbersome, often requiring days or even months for a single task [34, 44]. Due to the availability of affordable cloud computing services, it is becoming more attractive to leverage large-scale simulations to collect experience from a large number of agents in parallel. But with this comes the issue of transferring gained experience from simulation to the real world - a non-trivial task given the usually large domain shift.</p>
<p>Reducing the reality gap between simulation and reality is possible with recent advances in visual domain adaptation [14, 36, 5, 55, 4, 66, 71, 30, 54, 59, 21]. Such techniques usually require large amounts of unlabelled images from the real world. Although such unlabelled images are easier to capture than labelled, they can still be costly to collect in</p>
<p>robotics tasks. Domain randomization [51, 61, 25, 38, 3, 24] is another technique that is particularly popular in robotics, where an agent is trained on a wide range of variations of sensory inputs, with the intention that this forces the input processing layers of the network to extract semantically relevant features in a way that is agnostic to the superficial properties of the image (such as particular textures or particular ways shadows are cast from a constant light source). The intuition is that this leads to a network that extracts the same information from real-world images, featuring yet another variation of the input. However, performing randomization directly on the input of a learning algorithm, as done in related work, makes the task potentially harder than necessary, as the algorithm has to model both the arbitrary changes in the visual domain, while at the same time trying to decipher the dynamics of the task. Moreover, although randomization has been successful in the supervised learning setting, there is evidence that some popular reinforcement learning (RL) algorithms, such as DDPG [35] and A3C [39], can be destabilized by this transfer method [38, 70].</p>
<p>In this paper, we investigate learning vision-based robotic closed-loop grasping, where a robotic arm is tasked with picking up a diverse range of unseen objects, with the help of simulation and the use of as little real-world data as possible. Robotic grasping is an important application area in robotics, but also an exceptionally challenging problem: since a grasping system must successfully pick up previously unseen objects, it is not enough simply to memorize grasps that work well for individual instances, but to generalize and extrapolate from an internal understanding of geometry and physics. This presents a particularly difficult challenge for simulation-to-real-world transfer: besides the distributional shift from simulated images and physics, the system must also handle domain shift in the distribution of objects themselves.</p>
<p>To that end, we propose Randomized-to-Canonical Adaptation Networks (RCAN), a novel approach to crossing the reality gap that translates real-world images into their equivalent simulated versions, but makes use of no realworld data. This is achieved by leveraging domain randomization in a unique way, where we learn to adapt from one heavily randomized scene to an equivalent non-randomized, canonical version. We are then able to train a robotic grasping algorithm in a pre-defined canonical version of our simulator, and then use our RCAN model to convert the realworld images to the canonical domain our grasping algorithm was trained on.</p>
<p>Using RCAN along with a grasping algorithm that uses QT-Opt, a recent reinforcement learning algorithm, we achieve almost double the performance in comparison to alternative methods of using randomization. Bootstrapping from this performance, and with the addition of only
5,000 real-world grasps, we are able to achieve higher performance than a system trained with 580,000 real-world grasps. In our particular experiment, none of the objects used during testing are seen during either simulated training or real-world joint finetuning.</p>
<p>Our results also show that RCAN (summarized in Figure 1) is superior to learning a grasping network directly with domain randomization. RCAN has additional advantages compared to other simulation-to-real-world transfer methods. Firstly, unlike domain adaptation methods, it does not need any real-world data in order to learn our reality-to-simulation translation function. Secondly, RCAN gives an interpretable intermediate output that would otherwise not be available when performing domain randomization directly on the policy. Finally, as our method is trained in a supervised manner and preprocesses the input to the downstream task, it enables the use of RL methods that currently suffer from the stability issues when learning a policy directly from domain randomization [38, 70].</p>
<p>In summary, our contributions are as follows:</p>
<ul>
<li>We present a novel approach of crossing the reality gap by using an image-conditioned generative adversarial network (cGAN) [23] to transform randomized simulation images into their non-randomized, canonical versions, which in turn enables real-world images to also be transformed to canonical simulation versions.</li>
<li>We show that by using this approach, we are able to train a state-of-the-art vision-based grasping reinforcement learning algorithm (QT-Opt) purely in simulation and achieve $\mathbf{7 0 \%}$ success on the challenging task of grasping previously unseen objects in the real world, almost double the performance obtained by naively using domain randomization on the input of the learning algorithm.</li>
<li>We also show that by using RCAN and joint finetuning in the real-world with only $\mathbf{5 , 0 0 0}$ additional grasping episodes we are able to increase grasping performance to $\mathbf{9 1 \%}$, outperforming QT-Opt when trained from scratch in the real-world with $\mathbf{5 8 0 , 0 0 0}$ grasps a reduction of over $99 \%$ of required real-world samples.</li>
</ul>
<h2>2. Related Work</h2>
<p>Robotic grasping is a well studied problem [2]. Traditionally, grasping was usually solved analytically, where 3D meshes of objects would be used to compute the stability of a grasp against external wrenches [45, 47] or constrain the object's motion [47]. These solutions often assume that the same, or similar objects will be seen during testing, such that point clouds of the test objects can be matched</p>
<p>with stored objects based on visual and geometric similarity [6, 11, 19, 20, 29]. Due to this limitation, data-driven methods have become the dominant way to solve grasping [33, 37]. These methods commonly make use of either hand-labeled grasp positions [33, 28], self-supervision [44], or predicting grasp outcomes [34]. State-of-the-art grasping systems typically either operate in an open-loop style, where grasping locations are chosen, and then a motion is executed to complete the grasp [69, 41, 37, 60], or in a closed-loop manner, where grasp prediction is continuously run during motion, either explicitly [65], or implicitly [27].</p>
<p>Simulation-to-real-world transfer concerns itself with learning skills in simulation and then transferring them to the real world, which reduces the need for expensive realdata collection. However, it is often not possible to naively transfer such skills directly due to the visual and dynamics differences between the two domains [26]. Numerous works have looked into enabling such transfer both in computer vision and robotics. In the context of robotic manipulation in particular, Saxena et al. [53] used rendered objects to learn a vision-based grasping model. Rusu et al. [50] introduced progressive neural networks that help adapt an existing deep reinforcement learning policy trained from pixels in simulation to the real world for a reaching task. Other works have considered simulation-to-real world transfer using only depth images [64, 18]. Although this may be an attractive option, using depth cameras alone is not suitable for all situations, and coupled with the low cost of simple RGB cameras, there is considerable value in studying transfer in systems that solely use monocular RGB images. Although in this work we use depth estimation from RGB input as an auxiliary task to aid with our randomized-to-canonical image translation model, we neither use depth sensors in the real world, nor do we use our estimated depth during training.</p>
<p>Data augmentation has been a standard tool in computer vision for decades. More recently, and as a way to avoid overfitting, the random application of cropping, flipping samples horizontally, and photometric variations to input images were used to train AlexNet [31] and many more subsequent deep learning models. In robotics, a number of recent works have examined using randomized simulated environments [61, 25, 38, 3, 24, 52] specifically for simulation-to-real world transfer for grasping and other similar manipulation tasks, extending on prior work on randomization for collision-free robotic indoor flight [51]. These works apply randomization in the form of random textures, lighting, and camera position, allowing the resulting algorithm to become invariant to domain differences and applicable to the real world. There have been more robotics works that do not use vision, but that apply domain randomization on physical properties of the simulator to aid transferability [40, 46, 1, 68, 43]. Recently, Chebotar et
al. [9] have specifically looked into learning, from few realworld trajectories, the optimal distribution of such simulation properties, for transfer of policies learned in simulation to the real world. All of these methods learn a policy directly on randomization, whilst our method instead utilizes domain randomization in a novel way in order to learn a randomized-to-canonical adaption function to gain an interpretable intermediate representation and achieve superior results in comparison to learning directly on randomization.</p>
<p>Visual domain adaptation [42, 13] is a process that allows a machine learning model trained with samples from a source domain to generalize to a target domain, by utilizing existing but (mostly) unlabeled target data. In simulation-to-reality transfer, the source domain is usually the simulation, whereas the target is the real world. Prior methods can be split into: (1) feature-level adaptation, where domain-invariant features are learned between source and target domains [17, 15, 57, 7, 14, 36, 5, 55], or (2) pixellevel adaptation, which focuses on re-stylizing images from the source domain to make them look like images from the target domain [4, 66, 71, 30, 54, 59, 21]. Pixel-level domain adaptation differs from image-to-image translation techniques [23, 10, 67], which deal with the easier task of learning such a re-stylization from matching pairs of examples from both domains. Our technique can be seen as an image-to-image translation model that transforms randomized renderings from our simulator to their equivalent nonrandomized, canonical ones.</p>
<p>In the context of robotics, visual domain adaptation has also been used for simulation-to-real-world transfer [62, 56, 3]. Bousmalis et al. [3], introduced the GraspGAN method, which combines pixel-level with feature-level domain adaptation to limit the amount of real data needed for learning grasping. Although the task is similar to ours, GraspGAN required significant amounts of unlabeled real-world data that were previously collected by a variety of pre-existing grasping networks. Our method can be viewed as orthogonal to existing domain adaptation methods and GraspGAN: the process of training the adapter could make use of unlabeled real-world data by incorporating ideas from domain adaptation in the form of additional auxiliary losses to improve performance further. Although in this work we do explore using our simulation-trained policy to collect labeled real-world data for joint finetuning, the combination with domain adaptation techniques is proposed as a promising future research direction.</p>
<p>The reverse, i.e. reality-to-simulation transfer, has been examined recently by Zhang et al. [70] in the context of a simple robotic driving task. The approach has certain advantages, namely the learning algorithm is trained only in simulation, and during inference the real-world images are adapted to look like simulated ones. This decouples adaptation from training and if the real-world environment</p>
<p>changes, it is only the adaptation model that needs to be re-learned. We also explore reality-to-simulation transfer, but unlike [70], which uses CyCaDA [21] and unlabeled real-world data, we do so only in simulation, by learning to adapt randomized images from our simulator to their equivalent non-randomized versions, which allows data-efficient transfer of our model to the real-world.</p>
<h2>3. Background</h2>
<p>We demonstrate our approach by using a recent reinforcement algorithm, Q-function Targets via Optimization (QT-Opt) [27], though our method is compatible with any reinforcement learning or imitation learning algorithm, as we are only adapting the input. QT-Opt is a state-of-theart method for vision base grasping, which made it an ideal choice as a baseline for a direct comparison. Below, we will cover the fundamentals of Q-learning and then provide an overview of QT-Opt.</p>
<p>In reinforcement learning, we assume an agent interacting with an environment consisting of states $\mathbf{s} \in \mathcal{S}$, actions $\mathbf{a} \in \mathcal{A}$, and a reward function $r\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)$, where $\mathbf{s<em t="t">{t}$ and $\mathbf{a}</em>$ are the state and action at time step $t$ respectively. The goal of the agent is then to discover a policy that results in maximizing the total expected reward. One way to achieve such a policy is to use the recently proposed $Q T$ Opt [27] algorithm. QT-Opt is an off-policy, continuousaction generalization of Q-learning, where the goal is to learn a parametrized Q-function (or state-action value function). This can be learned by minimizing the Bellman error:</p>
<p>$$
\mathcal{E}(\theta)=\mathbb{E}<em _theta="\theta">{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right) \sim p\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)}\left[D\left(Q</em>\right)\right)\right]
$$}(\mathbf{s}, \mathbf{a}), Q_{T}\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime</p>
<p>where $Q_{T}\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)=r(\mathbf{s}, \mathbf{a})+\gamma V\left(\mathbf{s}^{\prime}\right)$ is a target value, and $D$ is a divergence metric, defined as the cross-entropy function in this case. Much like other works in RL, stability was improved by the introduction of two target networks. The target value $V\left(\mathbf{s}^{\prime}\right)$ was computed via a combination of Polyak averaging and clipped double Q-learning to give $V\left(\mathbf{s}^{\prime}\right)=\min <em _bar_theta="\bar{\theta">{i=1,2} Q</em><em _mathbf_a="\mathbf{a">{i}}\left(\mathbf{s}^{\prime}, \arg \max </em>}^{\prime}} Q_{\bar{\theta<em _bar_theta="\bar{\theta">{i}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right)$. $Q T$-Opt differs from other methods primarily with regards to action selection. Rather than selecting actions based on the $\operatorname{argmax}: \pi</em><em _mathbf_a="\mathbf{a">{i}}(\mathbf{s})=\arg \max </em>$; in this case, the cross-entropy method (CEM) [49].}} Q_{\bar{\theta}_{i}}(\mathbf{s}, \mathbf{a})$, QT-Opt instead evaluates the argmax via a stochastic optimization algorithm over $\mathbf{a</p>
<h2>4. Method</h2>
<p>Our method, Randomized-to-Canonical Adaptation Networks ( $R C A N$ ), consists of an image-conditioned generative adversarial network (cGAN) [23] that transforms images from randomized simulated environments (an example is show in Figure 2a) into images that seem similar to
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The setup used in our approach. A dataset of observations from a randomized version of a simulated environment (a) are paired with observations from a canonical version of the same environment (b) in order to learn an adaptation function and allow observations from the realworld (c) to be transformed into observations looking as if they came from the canonical simulation environment.
those obtained from a non-randomized, canonical one (Figure 2b). Once trained, the cGAN generator is also able to transform real-world images into images that seem as if they were obtained from the canonical simulation environment. We are then able to train a reinforcement learning algorithm (in this case $Q T$-Opt) fully in simulation, and use such a generator to enable the trained policy to act in the real-world.</p>
<p>The approach assumes 3 domains: the randomized simulation domain, the canonical simulation domain, and the real-world domain. Let $\mathbb{D}=\left{\left(x_{s}, x_{c}, m_{c}, d_{c}\right)<em j="1">{j}\right}</em>$, such that they appear to belong to the canonical domain.}^{N}$ be a dataset of $N$ training samples, where each sample is a tuple containing an RGB image $x_{s}$ from the randomization (source) domain, an RGB image $x_{c}$ from the canonical (target) domain (with semantic content, i.e. scene configuration, matching that of $x_{s}$ ), a segmentation mask $m_{c}$, and a depth image $d_{c}$. Both the segmentation mask and depth mask are only used as auxiliary tasks during the training of our generator. The RCAN generator function $G(x) \rightarrow\left{x_{a}, m_{a}, d_{a}\right}$, maps an image $x$ from any domain to an adapted image $x_{a}$, segmentation mask $m_{a}$, and depth image $d_{a</p>
<h3>4.1. RCAN Data Generation</h3>
<p>In order to learn this translation $G$, we need pairs of observations capturing the robot in interaction with the scene, with one observation showing the scene in its canonical version and the other one showing the same scene but with randomization applied, as shown in image (a) and (b) of Figure 2. Our simulated environments are based on the Bullet physics engine and use the default renderer [12]. They are built to roughly correspond to the real word, and include a Kuka IIWA, a tray, an over-the-shoulder camera aimed</p>
<p>at the tray, and a set of graspable objects. Graspable objects consist of a combination of 1,000 procedurally generated objects (consisting of randomly merged geometric shapes), and 51,300 realistic objects from 55 categories obtained from the ShapeNet repository [8].</p>
<p>We create the trajectories from which we sample paired snapshots by running training of QT-Opt in simulation. At the beginning of each episode, the position of the divider in the tray is randomly sampled, and 5 randomly selected objects are dropped into the tray. Then, at each timestep we freeze the scene, apply a new arbitrary randomization (described below) to capture the randomized observation, reset to and capture an observation of the canonical version, and let QT-Opt proceed. In our case, observations consist of RGB images, depth, and segmentation masks, labeling each pixel with one of 5 categories: graspable objects, tray, tray divider, robot arm, and background.</p>
<p>The randomization includes applying at each timestep randomly selected textures from a set of over 5,000 images to all models, which includes the tray, graspable objects, arm segments, and floor. Additionally we randomize the position, direction and color of the lighting. To further increase the diversity of scene configurations beyond those that the normal robot operation during QT-Opt training gives us, we also slightly randomize the position and size of the arm and tray (sampling from a uniform distribution), applying the same transformation to both the canonical and the randomized scene when creating the snapshot, such that the semantics between the two still match.</p>
<p>One important question is: what should the canonical environment look like? In practice, the canonical environment can be defined in a number of ways. We opt for applying uniform colors to the background, tray and arm, while leaving the textures for the objects from the randomized version in-place, as this preserves the objects' identity and thus opens up the potential for instance-specific grasping in future works. Each link of the arm is colored independently to aid tracking of individual links of the arm. We opt for fixing the light source in the canonical version, requiring the network to learn some aspect of geometry in order to re-render any shadows in the correct shape and direction.</p>
<h3>4.2. RCAN Training Method</h3>
<p>We aim to learn $G\left(x_{s}\right) \rightarrow\left{x_{a}, m_{a}, d_{a}\right}$, which transforms randomized sim images into canonical sim images with matching semantics, with the intuition that the generator will generalize to accept an image from the real world $x_{r}$, and produce a canonical RGB image, segmentation mask, and depth image: $G\left(x_{r}\right) \rightarrow\left{x_{a}, m_{a}, d_{a}\right}$. To train the generator, we encourage visual equality between the generated $x_{a}$ and target $x_{c}$ through a loss function $l_{e q_{x}}$, semantic equality between $m_{c}$ and $m_{a}$ through a function $l_{e q_{m}}$, and depth equality between $d_{c}$ and $d_{a}$ through a func-
tion $l_{e q_{d}}$. Having experimented with L1, L2, and the mean pairwise squared error (MPSE), our solution uses MPSE for $l_{e q_{x}}$ which was found to converge faster with no loss in performance [5], along with the L2 distance for our auxiliary losses $l_{e q_{m}}$ and $l_{e q_{d}}$. This results in the following loss:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _left_x__s="\left(x_{s">{e q}(G)= &amp; \mathbb{E}</em>\right)+\right. \
&amp; \left.+\lambda_{m} l_{e q_{m}}\left(G_{m}\left(x_{s}\right), m_{c}\right)+\lambda_{d} l_{e q_{d}}\left(G_{d}\left(x_{s}\right), d_{c}\right)\right]
\end{aligned}
$$}, x_{c}, m_{c}, d_{c}\right)}\left[\lambda_{x} l_{e q_{x}}\left(G_{x}\left(x_{s}\right), x_{c</p>
<p>where $G_{x}, G_{m}$, and $G_{d}$ denotes the image, mask, and depth element of the generator output respectively. In addition, $\lambda_{x}, \lambda_{m}$ and $\lambda_{d}$ represent the respective weightings.</p>
<p>It is well known that these equality losses can lead to blurry images [32], and so we employ a sigmoid-cross entropy generative adversarial (GAN) objective [16] to encourage high-frequency sharpness. Let $D(x)$ be a discriminator that outputs the likelihood that a given image $x$ is from the canonical domain. With this, the GAN is trained with the following objective:</p>
<p>$$
\mathcal{L}<em x="x">{G A N}(G, D)=\mathbb{E}</em>}[\log D(x)]+\mathbb{E<em x="x">{x}\left[\log \left(1-D\left(G</em>(x)\right)\right]\right.
$$</p>
<p>where $G_{x}$ denotes the image element of the generator output. The final objective for the generator then becomes:</p>
<p>$$
\hat{G}=\arg \min <em D="D">{G} \max </em>} \mathcal{L<em e="e" q="q">{G A N}(G, D)+\mathcal{L}</em>(G)
$$</p>
<p>The generator $G$ and discriminator $D$ are parameterized by weights of a convolutional neural network; details of which are presented in Appendix A. Qualitative results of our generator can be seen in Figure 3 and on the project web-page ${ }^{6}$.</p>
<h3>4.3. Real World Grasping with QT-Opt</h3>
<p>We use QT-Opt for our grasping algorithm, and follow the same state and action definition as Kalashnikov et al. [27], where the state is defined as $\mathbf{s}<em t="t">{t}=$ $\left(x</em>$.}, g_{\text {apt }, t}, g_{\text {height }, t}\right)$ at each timestep $t$, which includes a $472 \times 472$ image $x_{t}$ taken from a mounted over-the-shoulder camera overlooking the work space, a binary open/close indicator of gripper aperture $g_{\text {apt }, t}$, and the scalar height of the gripper above the bottom of the tray $g_{\text {height }, t</p>
<p>In our case, rather than sending the image directly to the RL algorithm, the image $x_{t}$ is instead passed through the generator $G$, and the resulting generated image $x_{a}$ is extracted and concatenated, channel-wise, with the original source image $x_{t}$. This results in the state $\mathbf{s}<em t="t">{t}=([G\left(x</em>\right]$ represents the concatenation. Note that we do not use the generated depth and segmentation masks of $G$ as input to QT-Opt in order to make a fair comparison to Kalashnikov et al. [27], though these could also be added in practice. The action space of Kalashnikov et al. [27], which consists of gripper}\right)+$ $\left.x_{t}\right], g_{\text {apt }, t}, g_{\text {height }, t}$ ), where $\left[G\left(x_{t}\right)+x_{t</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Sample outputs of our trained generator $G$ when given randomized sim images (3a) and real images (3b). Note the accuracy of the reconstruction of the canonical images from real-world images in complex and cluttered scenes, along with shadows being re-rendered into the canonical representation. However, also note that randomized-to-canonical adaptation performs a noticeably better reconstruction of the gripper in comparison to the real-to-canonical adaptation. This leads to the failure cases discussed in Section 5. The generated depth and segmentation masks are used as auxiliaries during training of the generator. Further examples can be seen in Figure 8 of the Appendix.
pose displacement and an open/close command, remains unchanged. A summary of the Q-function is shown in Figure 6 of the Appendix, and further details of the action space and architecture can be found in Appendix B.</p>
<p>In Kalashnikov et al. [27], the authors take their agent that was trained with 580,000 off-policy real-world grasps, and jointly finetune with an additional 28,000 on-policy grasps. During this joint finetuning process, QT-Opt asynchronously updates target values, collects real on-policy data, reloads real off-policy (offline) data from past experiences, and then trains the Q-network on both the on and off policy data streams within a distributed optimization framework. In the case of jointly finetuning $R C A N$, we also collect real on-policy data, but rather than using real-world past experiences (which we assume we do not have), we instead leverage the power of our simulation to continuously generate on-policy simulation data, and instead train on these streams of data. During the real world on-policy collection of both approaches, a selection of about 1,000 diverse training objects are used; a sample of which are shown in Figure 5 of the Appendix. Between 5 and 10 objects are randomly chosen every few hours to be placed in each of the trays until the desired number of joint finetuning grasps are reached.</p>
<h2>5. Experiments</h2>
<p>Our experimental section aims to answer the following questions: (1) Can we train an agent to grasp arbitrary unseen objects without having seen any real-world images?
(2) How does QT-Opt perform with standard domain randomization, and can our method perform better than this? (3) Does the addition of real-world on-policy training of our method lead to higher grasping performance while still drastically reducing the amount of real-world data required? We answer these questions through a series of rigorous realworld vision-based grasping experiments across multiple Kuka IIWA robots.</p>
<h3>5.1. Evaluation Protocol</h3>
<p>During evaluation, each robot attempts 102 grasps on its own set of 5 to 6 previously unseen test objects (shown in Figure 5 of the Appendix) which are deposited into each robots' respective tray and remain constant across all evaluations. Each grasp attempt (episode) consists of at most 20 time steps. If after 20 time steps no object has been grasped, the attempt is regarded as a failure. Following a grasp attempt, the object is deposited back into the tray at a random location. Although grasping was done with replacement, in practice, QT-Opt was not found attempting a grasp on the same object multiple times in a row. All observations come from an over-the-shoulder RGB camera.</p>
<h3>5.2. Results</h3>
<p>We first focus on the first 4 columns of Table 1. The first row of this section shows the results of QT-Opt reported in Kalashnikov et al. [27]; where following 580,000 off-policy real-world grasps, a performance of $87 \%$ was achieved.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">QT-Opt Data Source</th>
<th style="text-align: center;">Offline</th>
<th style="text-align: center;">Performance</th>
<th style="text-align: center;">Performance</th>
<th style="text-align: center;">Online</th>
<th style="text-align: center;">Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Real Grasps</td>
<td style="text-align: center;">In Sim</td>
<td style="text-align: center;">In Real</td>
<td style="text-align: center;">Real Grasps</td>
<td style="text-align: center;">In Real</td>
</tr>
<tr>
<td style="text-align: center;">Real</td>
<td style="text-align: center;">580,000</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$87 \%$</td>
<td style="text-align: center;">$\begin{gathered} +5,000 \ +28,000 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 85 \% \ 96 \% \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Canonical Sim</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$21 \%$</td>
<td style="text-align: center;">$+5,000$</td>
<td style="text-align: center;">$30 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Mild Randomization</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$98 \%$</td>
<td style="text-align: center;">$37 \%$</td>
<td style="text-align: center;">$+5,000$</td>
<td style="text-align: center;">$85 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Medium Randomization</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$98 \%$</td>
<td style="text-align: center;">$35 \%$</td>
<td style="text-align: center;">$+5,000$</td>
<td style="text-align: center;">$77 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Heavy Randomization</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$98 \%$</td>
<td style="text-align: center;">$33 \%$</td>
<td style="text-align: center;">$\begin{gathered} +5,000 \ +28,000 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 85 \% \ 92 \% \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">RCAN</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">70\%</td>
<td style="text-align: center;">$\begin{gathered} +5,000 \ +28,000 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 91 \% \ 94 \% \end{gathered}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Average grasp success rate on test objects after 102 grasp attempts on each of the multiple Kuka IIWA robots. The first 4 columns of the table highlight the performance after training on a specified number of real world grasps. Zero grasps implies that all training was done in simulation. The last 2 columns highlight the results of on-policy joint finetuning on a small amount of real-world grasps.</p>
<p>The Canonical Sim data source (second row) takes QT-Opt trained in the canonical simulation environment and then runs this directly in the real-world. The low success rate of $21 \%$ shows the existence of the reality gap. The following three rows show the result of training QT-Opt directly on varying degrees of randomization: mild, medium and heavy. Mild randomization consists of varying tray texture, object texture and color, robot arm color, lighting direction and brightness, and a background image consisting of 6 different images from the view of the real-world camera. Medium randomization adds a diverse mix of background images to the floor. Finally, heavy randomization uses the same scheme used to train RCAN, explained in Section 4.1.</p>
<p>Surprisingly, an unexpected discovery was that QT-Opt responds well to heavy domain randomization during training (i.e. is not destabilized). This is contrary to other RL methods, such as DDPG [35] and A3C [39], where heavy domain randomization has been shown to cause training to fail [38, 70]. Although QT-Opt was able to train stably with randomization, the results show that this does not lead to a successful transfer, achieving between $33 \%$ and $37 \%$ zeroshot grasping performance, whereas RCAN achieves 70\%: over double the success in the real world. This success highlights that RCAN better utilizes domain randomization to achieve sim-to-real transfer, rather than training a policy directly on domain randomization.</p>
<p>We now focus on the remaining 2 columns, that is, the ability to jointly finetune on a small amount of realworld on-policy grasps. We chose to use 5,000 to represent "small", which is less than $1 \%$ of the 580,000 grasps used in Kalashnikov et al. [27] for the off-policy training and takes only a day to collect, instead of months. To make comparison easier, in addition to reporting the 28,000 on-
policy grasps for joint finetuning from [27], we also report the performance after 5,000 grasps. This baseline result of $85 \%$ suggest that 5,000 real-world grasps for joint finetuning a system already trained with 580,000 does not improve performance. For the next joint finetuning experiment, we take each of the agents that were trained directly on domain randomization, and jointly finetune them on 5,000 real grasps, achieving between $77 \%$ and $85 \%$ grasping success. The rapid increase of $\sim 50$ p.p. is very surprising, and to the best of our knowledge, no other related works have shown such a dramatic performance increase from pre-training on domain randomization.</p>
<p>Finally, we look at joint finetuning RCAN with 5,000 and 28,000 real grasps, where the real images are adapted by the generator and then both the source and adapted image are passed to the grasping network; in this case, the gradients are only applied to the grasping network and not the generator network. The result of $91 \%$ for 5,000 shows that the improvement over learning directly on domain randomization holds, though for this result the difference is much smaller. What we believe is incredibly encouraging for the robotics community, is that with $\mathbf{9 1 \%}$ RCAN outperforms a version of QT-Opt that was trained on 580,000 real-world grasps, while using less than $\mathbf{1 \%}$ of the data. Moreover, following joint finetuning with with the same number of online grasps as Kalashnikov et al. [27] (28,000), we are able to achieve an almost equal grasp performance of $94 \%$.</p>
<p>In order to understand how performance varies as we progress from 0 to 5,000 on-policy grasps, we repeat the evaluation protocol set above for intermediate checkpoints. We re-evaluate both agents at every 1,000 grasps for both RCAN and Mild Randomization. The results, presented in Figure 4, show that the majority of the success is gained within the first 2,000 grasps for both approaches. This is</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: A graph showing how the performance of $R C A N$ and directly learning a policy on domain randomization varies with the number of real world on-policy grasps.
encouraging, as we ultimately wish to limit the amount of real-world data that we are reliant on.</p>
<h3>5.3. Failure cases</h3>
<p>A large contributing factor to QT-Opts 96\% grasp success, was its ability to perform corrective behaviors, regrasping, probing motions to ascertain the best grasp, and non-prehensile repositioning of objects. Much of this ability remained with our approach, except for the regrasping ability. This powerful ability allows the policy to detect when there is no object in the closed gripper, and thus, it can decide to re-open it in an attempt to try and re-grasp. Given that our method is not perfect at translating real-world images into simulation ones, artifacts may arise. As objects that we grasp are often small, it can be very difficult for the agent to differentiate between artifacts in the image or if there is indeed an object in the gripper. We observe this to be detrimental to the agents ability to perform regrasping, resulting in only a small amount of regrasps. The main observation from joint finetuning our method with 5,000 realworld grasps, is the re-emergence of the regrasping. We believe that this is contributed by our decision to concatenate the source image to the generated ones, and thus giving the grasping algorithm the option to choose which data source to extract information from for each part of the image as the joint finetuning continues. We hypothesize, that as the number of joint finetuning grasps increase, the network would eventually learn to solely rely on the source (real-world) image, rather than the adapted simulation image. However, we believe that, with a limited amount of labeled real-world data, feeding both the output of $R C A N$ as well as the original image to the agent offers the best combination of a simplified, yet potentially incomplete adapted view and the complex, but complete original real-world view.</p>
<h3>5.4. Discussion</h3>
<p>A number of questions arise from these results. For example: why does our method perform better than learning a policy directly with domain randomization? We hypothesize that our method allows offloading visual complexity to the generator network, thus simplifying the task for the
grasping network and in turn, leading to a higher grasping success. Moreover, having a chosen canonical environment allows us to impose structure on the task which may be beneficial for training the grasping network.. Despite our method achieving over double the zero-shot performance in the real world in comparison to domain randomization, with 5,000 additional real-world grasps, the performance of direct domain randomization also achieves a surprisingly high performance. This leads us to the hypothesis that learning a policy directly on domain randomization can act as a very powerful pre-training regime, where the network is forced to learn a very general feature extractor that can be easily jointly finetuned to a new environment. Having said that, our method outperforms this and has the added benefit of giving us an interpretable output for sim-to-real transfer.</p>
<p>Another question for future work would be: is there a way to better utilize the data collected during the 5,000 onpolicy grasps? Given this real-world data, it is now possible to consider fusing ideas from other transfer methods that require some real-world data, such as PixelDA [5].</p>
<h2>6. Conclusion</h2>
<p>We have presented Randomized-to-Canonical Adaptation Networks ( $R C A N$ ), a sim-to-real method that learns to translate randomized simulation images into a canonical representation, which in turn allows for real-world images to also be translated to this canonical representation. Given that our grasping algorithm ( $Q T$-Opt) is trained in this canonical environment, it is possible to run policies trained in simulation in the real world. We show that this approach is superior to the common domain randomization approach, and argue that it is a much more meaningful use of domain randomization. This general style of transfer has applications beyond just grasping, and can be used in other settings where real world data is expensive to collect, for example, producing segmentation masks for self-driving cars. For future work, we wish to explore further ways of introducing unlabelled real-world data in order to improve the real-to-canonical translation. Moreover, we are interested in exploring the effect of using the auxiliary outputs as additional inputs to the grasping network.</p>
<h2>Acknowledgments</h2>
<p>We would like to give special thanks to Ivonne Fajardo, Peter Pastor, Iñaki Gonzalo and Benjamin Swanson for overseeing the robot operations, Yunfei Bai for discussion on PyBullet, and Serkan Cabi for valuable comments on the paper.</p>
<h2>References</h2>
<p>[1] R. Antonova, S. Cruciani, C. Smith, and D. Kragic. Reinforcement learning for pivoting task. arXiv:1703.00472,</p>
<ol>
<li></li>
</ol>
<p>[2] J. Bohg, A. Morales, T. Asfour, and D. Kragic. Data-driven grasp synthesis-a survey. IEEE Transactions on Robotics, 30(2):289-309, 2014.
[3] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, and V. Vanhoucke. Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping. IEEE Intl. Conference on Robotics and Automation, 2018.
[4] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[5] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan. Domain separation networks. Advances in Neural Information Processing Systems, 2016.
[6] P. Brook, M. Ciocarlie, and K. Hsiao. Collaborative grasp planning with multiple object representations. IEEE Intl. Conference on Robotics and Automation, 2011.
[7] R. Caseiro, J. F. Henriques, P. Martins, and J. Batista. Beyond the shortest path: Unsupervised Domain Adaptation by Sampling Subspaces Along the Spline Flow. In IEEE Conference on Computer Vision and Pattern Recognition, 2015.
[8] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu. ShapeNet: An information-rich 3D model repository. arXiv:1512.03012, 2015.
[9] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. arXiv:1810.05687, 2018.
[10] Q. Chen and V. Koltun. Photographic image synthesis with cascaded refinement networks. Intl. Conference on Computer Vision, 2017.
[11] M. Ciocarlie, K. Hsiao, E. G. Jones, S. Chitta, R. B. Rusu, and I. A. Şucan. Towards reliable grasping and manipulation in household environments. In Experimental Robotics, pages 241-252. Springer, 2014.
[12] E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2018.
[13] G. Csurka. Domain adaptation for visual applications: A comprehensive survey. arxiv:1702.05374, 2017.
[14] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domainadversarial training of neural networks. The Journal of Machine Learning Research, 2016.
[15] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.
[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in Neural Information Processing Systems, 2014.
[17] R. Gopalan, R. Li, and R. Chellappa. Domain Adaptation for Object Recognition: An Unsupervised Approach. In Intl. Conference on Computer Vision, 2011.
[18] M. Gualtieri, A. ten Pas, K. Saenko, and R. Platt. High precision grasp pose detection in dense clutter. In IEEE Intl. Conference on Intelligent Robots and Systems, pages 598605, 2016.
[19] C. Hernandez, M. Bharatheesha, W. Ko, H. Gaiser, J. Tan, K. van Deurzen, M. de Vries, B. Van Mil, J. van Egmond, R. Burger, et al. Team delfts robot winner of the amazon picking challenge 2016. In Robot World Cup, pages 613624. Springer, 2016.
[20] S. Hinterstoisser, S. Holzer, C. Cagniart, S. Ilic, K. Konolige, N. Navab, and V. Lepetit. Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes. Intl. Conference on Computer Vision, 2011.
[21] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In Intl. Conference on Machine Learning, 2018.
[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Intl. Conference on Machine Learning, 2015.
[23] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-toimage translation with conditional adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[24] S. James, M. Bloesch, and A. J. Davison. Task-embedded control networks for few-shot imitation learning. Conference on Robot Learning, 2018.
[25] S. James, A. J. Davison, and E. Johns. Transferring end-toend visuomotor control from simulation to real world for a multi-stage task. Conference on Robot Learning, 2017.
[26] S. James and E. Johns. 3d simulation for robot arm control with deep q-learning. NeurIPS Workshop on Deep Learning for Action and Interaction, 2016.
[27] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine. QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation. Conference on Robot Learning, 2018.
[28] D. Kappler, J. Bohg, and S. Schaal. Leveraging big data for grasp planning. IEEE Intl. Conference on Robotics and Automation, 2015.
[29] B. Kehoe, A. Matsukawa, S. Candido, J. Kuffner, and K. Goldberg. Cloud-based robot grasping with the google object recognition engine. In IEEE Intl. Conference on Robotics and Automation, 2013.
[30] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to discover cross-domain relations with generative adversarial networks. Intl. Conference on Machine Learning, 2017.
[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 2012.
[32] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels using a learned similarity metric. Intl. Conference on Machine Learning, 2016.
[33] I. Lenz, H. Lee, and A. Saxena. Deep learning for detecting robotic grasps. The International Journal of Robotics Research, 34(4-5):705-724, 2015.</p>
<p>[34] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen. Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection. International Symposium on Experimental Robotics, 2016.
[35] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv:1509.02971, 2015.
[36] M. Long and J. Wang. Learning transferable features with deep adaptation networks. Intl. Conference on Machine Learning, 2015.
[37] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. Robotics: Science and Systems, 2017.
[38] J. Matas, S. James, and A. J. Davison. Sim-to-real reinforcement learning for deformable object manipulation. Conference on Robot Learning, 2018.
[39] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. Intl. Conference on Machine Learning, 2016.
[40] I. Mordatch, K. Lowrey, and E. Todorov. Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids. IEEE Intl. Conference on Intelligent Robots and Systems, 2015.
[41] D. Morrison, A. W. Tow, M. McTaggart, R. Smith, N. KellyBoxall, S. Wade-McCue, J. Erskine, R. Grinover, A. Gurman, T. Hunn, et al. Cartman: The low-cost cartesian manipulator that won the amazon robotics challenge. IEEE Intl. Conference on Robotics and Automation, 2018.
[42] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual domain adaptation: A survey of recent advances. IEEE Signal Processing Magazine, 32(3):53-69, 2015.
[43] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In IEEE Intl. Conference on Robotics and Automation. IEEE, 2018.
[44] L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours. IEEE Intl. Conference on Robotics and Automation, 2016.
[45] D. Prattichizzo and J. C. Trinkle. Grasping. In Springer handbook of robotics, pages 671-700. Springer, 2008.
[46] A. Rajeswaran, S. Ghotra, B. Ravindran, and S. Levine. Epopt: Learning robust neural network policies using model ensembles. Intl. Conference on Learning Representations, 2017.
[47] A. Rodriguez, M. T. Mason, and S. Ferry. From caging to grasping. The International Journal of Robotics Research, 31(7):886-900, 2012.
[48] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Intl. Conference on Medical Image Computing and ComputerAssisted Intervention. Springer, 2015.
[49] R. Y. Rubinstein and D. P. Kroese. The cross-entropy method: A unified approach to monte carlo simulation, randomized optimization and machine learning. Information Science \&amp; Statistics, 2004.
[50] A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell. Sim-to-real robot learning from pixels with progressive nets. Conference on Robot Learning, 2017.
[51] F. Sadeghi and S. Levine. CAD2RL: Real single-image flight without a single real image. In Robotics: Science and Systems, 2017.
[52] F. Sadeghi, A. Toshev, E. Jang, and S. Levine. Sim2real viewpoint invariant visual servoing by recurrent control. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.
[53] A. Saxena, J. Driemeyer, and A. Y. Ng. Robotic grasping of novel objects using vision. The Intl. Journal of Robotics Research, 2008.
[54] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb. Learning from simulated and unsupervised images through adversarial training. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[55] R. Shu, H. Bui, H. Narui, and S. Ermon. A DIRT-t approach to unsupervised domain adaptation. In Intl. Conference on Learning Representations, 2018.
[56] G. J. Stein and N. Roy. Genesis-rt: Generating synthetic images for training secondary real-world tasks. In IEEE Intl. Conference on Robotics and Automation, 2018.
[57] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In Association for the Advancement of Artificial Intelligence, 2016.
[58] N. Sünderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner, B. Upcroft, P. Abbeel, W. Burgard, M. Milford, et al. The limits and potentials of deep learning for robotics. The Intl. Journal of Robotics Research, 37(4-5):405-420, 2018.
[59] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised crossdomain image generation. Intl. Conference on Learning Representations, 2017.
[60] A. ten Pas, M. Gualtieri, K. Saenko, and R. Platt. Grasp pose detection in point clouds. The International Journal of Robotics Research, 36(13-14):1455-1473, 2017.
[61] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IEEE Intl. Conference on Intelligent Robots and Systems, 2017.
[62] E. Tzeng, C. Devin, J. Hoffman, C. Finn, P. Abbeel, S. Levine, K. Saenko, and T. Darrell. Adapting deep visuomotor representations with weak pairwise constraints. Workshop on the Algorithmic Foundations of Robotics, 2016.
[63] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv:1607.08022, 2016.
[64] U. Viereck, A. t. Pas, K. Saenko, and R. Platt. Learning a visuomotor controller for real world robotic grasping using easily simulated depth images. In Conference on Robot Learning, 2017.
[65] U. Viereck, A. ten Pas, K. Saenko, and R. Platt. Learning a visuomotor controller for real world robotic grasping using simulated depth images. Conference on Robot Learning, 2017.</p>
<p>[66] Z. Yi, H. R. Zhang, P. Tan, and M. Gong. Dualgan: Unsupervised dual learning for image-to-image translation. Intl. Conference on Computer Vision, 2017.
[67] D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon. PixelLevel Domain Transfer. European Conference on Computer Vision, 2016.
[68] W. Yu, J. Tan, C. K. Liu, and G. Turk. Preparing for the unknown: Learning a universal policy with online system identification. In Robotics: Science and Systems, 2017.
[69] A. Zeng, S. Song, S. Welker, J. Lee, A. Rodriguez, and T. Funkhouser. Learning synergies between pushing and grasping with self-supervised deep reinforcement learning. IEEE Intl. Conference on Intelligent Robots and Systems, 2018.
[70] J. Zhang, L. Tai, Y. Xiong, M. Liu, J. Boedecker, and W. Burgard. Vr goggles for robots: Real-to-sim domain adaptation for visual control. IEEE Robotics and Automation Letters, 2019.
[71] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired imageto-image translation using cycle-consistent adversarial networks. Intl. Conference on Computer Vision, 2017.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Real-world grasping objects that range greatly in size and appearance. Left: about 1000 visually and physically diverse training objects used for joint finetuning. Right: the unseen test objects.</p>
<h2>A. RCAN Architecture</h2>
<p>The generator $G$ is parameterized by weights of a convolutional neural network, summarized in Figure 7, and follows a U-Net style architecture [48] with downsampling performed via $3 \times 3$ convolutions with stride 2 for the first 2 layers, and average pooling with $3 \times 3$ convolution of stride 1 for the remaining layers. Upsampling was performed via bilinear upsampling, followed by a $3 \times 3$ convolutions of stride 1 , and skip connections were fused back into the network via channel-wise concatenation, followed by a $1 \times 1$ convolution. All layers were followed by instance normalization [63] and ReLU non-linearities. The discriminator $D$ is also parameterized by weights of a convolutional neural network with 2 layers of $32,3 \times 3$ filters, followed by a layer of $64,3 \times 3$ filters, and finally a layer of $128,3 \times 3$ filters. The network follows a multi-scale patch-based design [3], where 3 scales of $472 \times 472,236 \times 236$, and $118 \times 118$, are used to produce domain estimates for all patches which are then combined to compute the joint discriminator loss. The weightings $\lambda_{x}, \lambda_{m}$ and $\lambda_{d}$ in Equation 2 were all set to 1 .</p>
<h2>B. QT-Opt Architecture</h2>
<p>The action space of [27], which consists of gripper pose displacement and an open/close command, remains unchanged in our paper, and is defined as $\mathbf{a}<em t="t">{t}=$ $\left(\mathbf{t}</em>}, \mathbf{r<em _close="{close" _text="\text">{t}, g</em>}, t}, g_{\text {open }, t}, e_{t}\right)$, containing Cartesian translation $\mathbf{t<em t="t">{t} \in \mathbb{R}^{3}$, sine-cosine rotation encoding $\mathbf{r}</em>$. The reward function is sparse, consisting of a reward of 1 following a
} \in \mathbb{R}^{2}$, a onehot vector gripper open/close command $\left[g_{\text {close }, t}, g_{\text {open }, t}\right] \in$ ${0,1}^{2}$, and a learned stopping criterion $e_{t<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The Q-function of the grasping algorithm. The source image $x$ (either from the randomized domain or realworld domain) and generated canonical image $x_{a}$ are concatenated (channel-wise) and processed by a convolutional neural network (and fused with action and state variables) to produce a scalar representing the Q value $Q_{\theta}(s, a)$.
successful grasp, or 0 for an unsuccessful grasp, and -0.05 on all other transitions. Summarized in Figure 6, the Qfunction follows the same architecture as [27] (originally inspired by [34]).</p>
<p>Rather than a single RGB image input, our network takes in a 6 channel image, consisting of channel-wise concatenation of the source image $x$ (either from the randomized domain or real-world domain) and generated image $x_{a}$. Features are extracted from these images via 7 convolutional layers and then merged with a transformed action and state vector (which have passed through 2 fully-connected layers) via element-wise addition. The merged streams are then processed by a further 9 convolution layers and 2 fullyconnected layers, resulting in a scalar output representing the Q value $Q_{\theta}(s, a)$. Each layer, excluding the final, uses batch normalization [22] and ReLU non-linearities. A summary of the architecture can be seen in Figure 6.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Network architecture of the generator function $G$. An RGB image from the source domain (either from the randomized domain or real-world domain) is processed via a U-Net style architecture [48] to produce a generated RGB image $x_{a}$, and auxiliaries that includes a segmentation mask $m_{a}$ and depth image $d_{a}$. These auxiliaries forces the generator to extract semantic and depth information about the scene and encode them in the intermediate latent representation, which is then available during the generation of the output image.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" />
(b) Real-to-canonical samples.</p>
<p>Figure 8: Additional sample outputs of our trained generator $G$ when given randomized sim images (8a) and real images $(8 b)$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://sites.google.com/view/rcan/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2079 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2079</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2079</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-276767120</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.01508v1.pdf" target="_blank">E NABLING AI S CIENTISTS TO R ECOGNIZE I NNOVATION : A D OMAIN -A GNOSTIC A LGORITHM FOR A SSESSING N OVELTY</a></p>
                <p><strong>Paper Abstract:</strong> In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery. This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by comparing an idea’s local density with its adjacent neighbors’ densities. We first developed a scalable methodology to create test set without expert labeling, addressing a fundamental challenge in novelty assessment. Using these test sets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.820) and biomedical research (AUROC=0.765) domains. Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent accuracies across domains by its domain-invariant property, outperforming all benchmarks by a substantial margin (0.795 v.s. 0.597) on cross-domain evaluation. These results validate RND as a generalizable solution for automated novelty assessment in scientific research.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2079.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2079.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative Neighbor Density</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-agnostic algorithm introduced in this paper that scores the novelty of a research idea by comparing the idea's local semantic neighbor density to the neighbor densities of its P nearest neighbors (percentile-based score). Designed to be invariant across domains by using relative rather than absolute density.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Relative Neighbor Density (RND)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>semantic-embedding based novelty scoring algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>research-idea novelty assessment (cross-domain: computer science and biomedical)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>novelty score (0-100 percentile)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>not applicable (RND is a validator that outputs novelty assessments, not original scientific claims)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (RND does not generate scientific outputs; it computes percentile of neighbor density using embedding-space KNN with parameters P and Q)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Computes neighbor density ND = mean cosine distance to Q nearest neighbors for the idea and for each of the idea's P nearest neighbors, then returns percentile rank of idea's ND within the set of neighbor NDs (score = 100 * empirical CDF(F_P(ND_idea))). Validation of the algorithm uses automated test-sets built from publication timestamps and citation counts (recent top-venue papers as positives, highly-cited older papers as negatives).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A (RND is not a generator).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>AUROC on curated test sets: NeurIPS (computer science) = 0.820; Nature Medicine (biomedical) = 0.765; Mixed (cross-domain) = 0.795. Paper also reports RND outperforms baselines in cross-domain stability (abstract reports 0.795 vs 0.597 for best baseline in cross-domain evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>RND's scoring is explicitly constructed to be domain-invariant; authors show its score distributions are nearly identical across domains and argue the percentile construction makes scores comparable independent of corpus. No breakdown of validation accuracy as a function of 'degree of novelty' is provided beyond test set aggregates.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper compares RND (validator) against generation-capable LLMs used as judges: shows LLM judging performance degrades out-of-domain while RND remains stable; highlights an asymmetry where LLM generation/judgment may be strong in-domain but fail cross-domain, whereas RND maintains validation capability across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>The paper provides theoretical variance expressions for the empirical CDF estimate: Var(F_P(ND)) = F(ND)(1-F(ND))/P and variance of ND as Var(ND) = sigma_d^2 / Q. These are analytical uncertainty estimates but no per-example predictive uncertainty for scores is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Authors claim theoretically that percentile scores follow a U(0,1) distribution (probability integral transform) and empirically show near-identical score distributions across domains, implying good cross-domain calibration; no standard calibration metrics (e.g., Brier score, ECE) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>On the Mixed (cross-domain) test set, RND maintains AUROC = 0.795, indicating robust performance on cross-domain / out-of-domain examples compared to absolute-density baselines (which collapsed).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses semantic neighbor density (mean cosine distance to nearest abstracts) as a proxy for novelty rather than direct experimental validation or expert labels.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>N/A (authors emphasize their validation methodology avoids expert labeling; they used automated temporal/citation-based labeling for test sets).</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / literature-embedding based (applies to natural-language scientific abstracts; not a formalized symbolic domain). The empirical nature makes generation-validation gaps domain-sensitive for absolute metrics, motivating RND's relative approach.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>RND itself is proposed as a mitigation strategy: by producing domain-invariant novelty scores it can be used as a reward signal in RL to guide reasoning/generation models; authors propose integrating RND into end-to-end AI scientist workflows to reduce generation/validation mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>The paper reports that LLM-based judges (without retrieval) perform at AUROC ≈ 0.5 (random), and that reasoning models like Sonnet-3.7 require external literature search to reach AUROC ≈ 0.8 in-domain but degrade to ≈ 0.6 out-of-domain; absolute local-density methods (Historical Dissimilarity) perform well in individual domains but fail on mixed-domain (HD AUROC drops to 0.362 or Absolute Local Density listed as 0.395 in mixed set), demonstrating generators/judges outperforming validators in certain contexts and/or validators failing cross-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Historical Dissimilarity (absolute local density) achieves similar performance to RND within single domains (NeurIPS and Nature Medicine), indicating that some validators can match generator-judge accuracy in-domain; this shows the gap is primarily a cross-domain / generalization problem rather than a universal generator>validator phenomenon.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported as a numeric ratio between validation and generation. RND computational complexity is O(P * Q) per evaluated idea (authors choose P=100, Q=50 as trade-off). The paper notes large P/Q increases validation cost; generation cost for LLMs is discussed qualitatively (LLM runs multiple times), but no direct cost-ratio is provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2079.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2079.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HD / Absolute Local Density</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Historical Dissimilarity (absolute local density)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior absolute local-density based novelty metric that computes the average distance between an idea's embedding and the embeddings of its k most similar historical abstracts (e.g., 5 nearest neighbors) as a measure of novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Historical Dissimilarity (HD) / Absolute Local Density</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>semantic-embedding local-density metric</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>research-idea novelty assessment (single-domain evaluation emphasized)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>novelty score (distance-based)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>not applicable (metric for assessing novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compute average Euclidean (or cosine) distance to a small fixed number (e.g., 5) of most similar abstracts from historical and contemporary corpora; sometimes combined with citation-based 'Contemporary Impact' into Overall Novelty (ON). Validated against small human-labeled test sets in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported in this paper (as 'Absolute Local Density') — NeurIPS AUROC = 0.851; Nature Medicine AUROC = 0.757; Mixed (cross-domain) AUROC = 0.395 (severe degradation in cross-domain scenario). The paper also elsewhere reports HD matched RND in single-domain experiments but failed on Mixed test set.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Per-paper: performs adequately in single domains but performance collapses with mixed-domain inputs (indicates sensitivity to domain-specific density distributions and therefore poor generalization as novelty increases beyond domain norms). No explicit per-novelty-level curves provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Used as a validator baseline; compared to LLM judges and RND. Shows that absolute-density validators can match judge performance in-domain but fail when ideas cross domain boundaries, evidencing a generalization gap.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported in this paper beyond empirical performance; prior HD methods rely on small k nearest neighbors making variance sensitive to selection choices.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor: Mixed AUROC = 0.395 indicates weak OOD performance across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses local semantic distance as proxy for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Not specified in this paper; prior work validated HD against small human-labeled sets.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / literature-embedding based</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>None proposed here beyond tuning selection of neighbor count and corpus definitions; paper argues these design choices limit HD's cross-domain generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Large drop in AUROC on Mixed test set (from ≈0.75-0.85 in single domains to ≈0.395 mixed) shows inability to validate novelty across domain boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Per-domain performance comparable to RND on single-domain test sets, indicating that absolute-density methods can be effective when the domain is fixed and corpus choices are appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported; HD typically uses small k (e.g., 5) making it computationally cheaper than RND's chosen P=100,Q=50 in practice.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2079.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2079.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Overall Novelty (ON)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric formed by combining Historical Dissimilarity (HD) and Contemporary Dissimilarity (CD) with a citation-based Contemporary Impact (CI) factor into a multiplicative novelty score (ON = HD × CI^CD as reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Overall Novelty (ON)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>composite semantic-density + citation metric</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>research-idea novelty assessment</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>novelty score</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Computed from historical and contemporary embedding distances and citation-derived impact; previously validated against small human-labeled sets.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not reported in this paper beyond prior citations; used as baseline in literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Mentioned as an absolute-density based baseline; paper critiques ON for sensitivity to arbitrary choices (size of collections, temporal boundaries) harming cross-domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported; paper implies ON would degrade cross-domain due to absolute density dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses distances and citation proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / literature-embedding based</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>None specific; the paper uses ON as an example of absolute-density approaches that RND aims to improve upon.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Discussion that ON's dependence on arbitrary corpus/time splits undermines generalization across fields.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2079.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2079.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM + literature search judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that feeds the 10 most relevant retrieved papers' titles/abstracts to a large language model, which then judges whether the idea substantially overlaps (non-novel) or not (novel); used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Large language model with retrieval (LLM+literature search)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model with retrieval augmentation (RAG-style)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>research-idea novelty assessment; supports idea judgment for scientific proposals</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>binary novelty decision or binary-normalized score (0/1), or probabilistic judgment aggregated to AUROC</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>N/A (judges novelty; can detect in-distribution concepts better than OOD ones when retrieval is relevant)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (this is a judgment method, though LLMs themselves can generate proposals in other works)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>The LLM inspects the 10 retrieved most-relevant papers and judges overlap; in experiments authors ran each LLM configuration multiple times (three runs) and averaged results.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A for generation; as a judge its performance depends on retrieval and internal model: without retrieval LLMs perform ~AUROC 0.5 (random); with retrieval Sonnet-3.7 reached ~AUROC 0.8 on NeurIPS but degraded to ~0.6 on Nature Medicine and cross-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported: baseline 'without search' AUROC ≈ 0.5; with retrieval performance varies by domain and model — Sonnet-3.7 + search: ~0.8 (NeurIPS), ~0.6 (Nature Medicine and Mixed). Exact AUROC per-model per-experiment in supplemental Table D.1 (not fully reproduced in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performance degrades when ideas are cross-domain or when the model's internal knowledge is lacking for the domain; adding retrieval improves in-domain performance but not necessarily cross-domain robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper highlights that LLMs can be strong judges when given relevant literature but are inconsistent and sensitive to input perturbations; validates generation-vs-validation asymmetry since LLMs can generate many ideas easily but their judging reliability varies across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported; LLM outputs have run-to-run variability which authors address by averaging multiple runs and reporting standard deviation (table D.1).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported quantitatively; paper notes sensitivity to phrasing and input perturbations undermines reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades substantially: e.g., Sonnet-3.7 drops from AUROC ≈0.8 in CS to ≈0.6 in biomed / mixed; GPT-4o without retrieval performed at AUROC ≈0.5 (random).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — relies on retrieved-paper overlap plausibility/coherence as a proxy for novelty (if overlap detected -> non-novel).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Providing retrieval (10 relevant papers) is used to mitigate LLM judge failures; authors also suggest using RND as a reward signal to train reasoning models to generate more genuinely novel ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Empirical results show LLM judge performance highly dependent on domain and retrieval; without retrieval performance is random, indicating a gap between generation/idea-proposal ability and robust validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>When strong retrieval is available and the internal model knowledge is strong (e.g., Sonnet-3.7 on CS), LLM+search can match validator performance in-domain, suggesting the gap can be narrowed with retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported quantitatively; running LLMs multiple times (authors run 3 runs) increases computational cost; retrieval adds extra cost but no numeric ratio is provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2079.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2079.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sonnet-3.7</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sonnet-3.7 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent high-capability reasoning LLM (Anthropic) referenced and evaluated in the paper as a judge for novelty, used in variants with and without retrieval/guidelines/tournament formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sonnet-3.7</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model / reasoning model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general-purpose language model applied to research-novelty judgment</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>novelty judgment (binary or graded), textual review-style assessments</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>N/A (model is a general LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autoregressive transformer pretraining fine-tuned for reasoning (paper references but does not detail training here)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used as human-like judge: three evaluation modes — without literature, with NeurIPS guideline, with literature search (10 retrieved papers), and Swiss-tournament pairwise comparisons; outputs were compared to automated labels and AUROC computed.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable as generator in this paper's experiments; when used as judge with literature-search it achieved AUROC ≈ 0.8 on NeurIPS dataset (in-domain).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>With literature search: AUROC ≈ 0.8 (NeurIPS); with guideline/tournament but no external literature: low accuracy comparable to random; out-of-domain (Nature Medicine / Mixed) performance degraded to ~0.6.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performance sensitive to domain and to availability of external evidence; degrades on cross-domain examples and when no retrieval is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper highlights that Sonnet-3.7 as a judge can approach human-level in-domain given retrieval, but is unreliable without it, pointing to a mismatch between generation capacity and reliable validation across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported beyond standard-deviation across repeated runs (LLM outputs variable; authors ran three trials and reported averages and SDs).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported numerically; qualitative observations indicate sensitivity to prompts and lack of calibration across phrasing variants.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Moderate to poor: AUROC falls to ~0.6 on biomed and mixed tests despite good in-domain results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>N/A (model reasons over retrieved content rather than using explicit proxy metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Providing retrieval and structured prompting (guidelines/tournament/few-shot) are tested to improve judge reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Sonnet-3.7's strong in-domain but weak cross-domain performance is used as evidence that LLM judging/generation capabilities don't generalize uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>When provided adequate external literature Sonnet-3.7 can perform well in-domain, suggesting access to retrieval mitigates some gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2079.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2079.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek-r1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek-r1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent reinforcement-learning-enhanced reasoning model referenced as showing improved reasoning capabilities and potential to be guided by reward signals like RND.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deepseek-r1</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>reinforcement-learned reasoning large language model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general reasoning; applied to novelty judgment in discussion</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual reasoning outputs / novelty judgments (when used as judge in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>reinforcement learning to incentivize reasoning capabilities (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not directly evaluated in this paper; authors cite it as an example of models improved via rule-based rewards and posit RND could be used as such a reward.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not reported in this paper; authors state Deepseek-r1 shows improved reasoning capabilities compared to autoregressive models.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper suggests RND could be used as a reward signal to train models like Deepseek-r1 to generate more novel ideas, implying current generation-validation misalignment could be addressed by integrating RND during training.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Proposed: use RND as reward in RL training to bias generation toward more genuinely novel outputs that can be validated.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Mentioned conceptually; no empirical data in this paper for Deepseek-r1.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2079.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2079.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M3-Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>M3-Embedding (multi-lingual, multi-function, multi-granularity embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Embedding model used by the authors to create 1024-dimensional embeddings for titles and abstracts across the literature corpora; used for KNN neighbor retrieval and density calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>M3-Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>text embedding neural model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>natural-language representations for scientific abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>1024-dimensional embedding vectors for titles and abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>N/A (representation model)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used to compute semantic distances; validation of embedding quality is not deeply evaluated in this paper beyond empirical downstream novelty-assessment performance.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Indirect: embeddings fed to RND and baselines that produce the reported AUROC values; no independent embedding performance metrics (e.g., retrieval precision) reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported explicitly; overall RND robustness suggests embeddings are sufficiently consistent across corpora for the task but the paper notes embedding quality is a limitation and future work could explore domain-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Authors recommend investigating specialized embedding models for scientific literature as future work to improve representation for technical terminology.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors note algorithm performance depends on embedding quality and biases in literature databases can influence novelty estimates; implies representation issues can contribute to validation gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported; embedding retrieval performed via Elasticsearch for scalability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2079.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2079.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist (Lu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery (Lu et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work referenced that combines idea generation, evaluation, and refinement in an AI Scientist framework, using chain-of-thought prompting and retrieval APIs to improve LLM judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-component AI research automation (LLM-based generator + evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>automated scientific discovery (general)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>generated research ideas / hypotheses and iterative refinements</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>varies; claims of near human-level idea generation in cited work but reliability concerns remain</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-based generation with chain-of-thought prompting and external retrieval (Semantic Scholar API) for refinement</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>In cited work, integrates LLM judgment augmented with NeurIPS review guideline and semantic retrieval to assess novelty; details are in the cited paper rather than this one.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not quantified in this paper; cited as motivating prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not quantified here; authors critique LLM-as-judge reliability and sensitivity to input perturbations as reasons to prefer non-LLM validators like RND.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Used as motivating example: LLM-based pipeline can generate many ideas but its judgment reliability is questionable and sensitive to phrasing and retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Relies on retrieval and guideline-based prompting as proxy evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Not in this paper; this paper suggests RND could be used to provide domain-invariant novelty reward to such frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Cited papers and this paper's experiments show that LLMs' judging performance is unstable and domain-sensitive, supporting the view that generation capabilities can outstrip robust validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas <em>(Rating: 2)</em></li>
                <li>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers <em>(Rating: 2)</em></li>
                <li>Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation <em>(Rating: 1)</em></li>
                <li>Incentivizing reasoning capability in llms via reinforcement learning (Deepseek-r1) <em>(Rating: 2)</em></li>
                <li>Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation <em>(Rating: 2)</em></li>
                <li>Historical Dissimilarity / Overall Novelty (original formulation by Su et al.) <em>(Rating: 2)</em></li>
                <li>GPT-4o system card (or related Anthropic Sonnet-3.7 documentation) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2079",
    "paper_id": "paper-276767120",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "RND",
            "name_full": "Relative Neighbor Density",
            "brief_description": "A domain-agnostic algorithm introduced in this paper that scores the novelty of a research idea by comparing the idea's local semantic neighbor density to the neighbor densities of its P nearest neighbors (percentile-based score). Designed to be invariant across domains by using relative rather than absolute density.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Relative Neighbor Density (RND)",
            "system_type": "semantic-embedding based novelty scoring algorithm",
            "scientific_domain": "research-idea novelty assessment (cross-domain: computer science and biomedical)",
            "output_type": "novelty score (0-100 percentile)",
            "novelty_level": "not applicable (RND is a validator that outputs novelty assessments, not original scientific claims)",
            "generation_method": "N/A (RND does not generate scientific outputs; it computes percentile of neighbor density using embedding-space KNN with parameters P and Q)",
            "validation_method": "Computes neighbor density ND = mean cosine distance to Q nearest neighbors for the idea and for each of the idea's P nearest neighbors, then returns percentile rank of idea's ND within the set of neighbor NDs (score = 100 * empirical CDF(F_P(ND_idea))). Validation of the algorithm uses automated test-sets built from publication timestamps and citation counts (recent top-venue papers as positives, highly-cited older papers as negatives).",
            "generation_performance": "N/A (RND is not a generator).",
            "validation_performance": "AUROC on curated test sets: NeurIPS (computer science) = 0.820; Nature Medicine (biomedical) = 0.765; Mixed (cross-domain) = 0.795. Paper also reports RND outperforms baselines in cross-domain stability (abstract reports 0.795 vs 0.597 for best baseline in cross-domain evaluation).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "RND's scoring is explicitly constructed to be domain-invariant; authors show its score distributions are nearly identical across domains and argue the percentile construction makes scores comparable independent of corpus. No breakdown of validation accuracy as a function of 'degree of novelty' is provided beyond test set aggregates.",
            "generation_validation_comparison": "Paper compares RND (validator) against generation-capable LLMs used as judges: shows LLM judging performance degrades out-of-domain while RND remains stable; highlights an asymmetry where LLM generation/judgment may be strong in-domain but fail cross-domain, whereas RND maintains validation capability across domains.",
            "uncertainty_quantification": "The paper provides theoretical variance expressions for the empirical CDF estimate: Var(F_P(ND)) = F(ND)(1-F(ND))/P and variance of ND as Var(ND) = sigma_d^2 / Q. These are analytical uncertainty estimates but no per-example predictive uncertainty for scores is reported.",
            "calibration_quality": "Authors claim theoretically that percentile scores follow a U(0,1) distribution (probability integral transform) and empirically show near-identical score distributions across domains, implying good cross-domain calibration; no standard calibration metrics (e.g., Brier score, ECE) are reported.",
            "out_of_distribution_performance": "On the Mixed (cross-domain) test set, RND maintains AUROC = 0.795, indicating robust performance on cross-domain / out-of-domain examples compared to absolute-density baselines (which collapsed).",
            "validation_proxy_metrics": "Yes — uses semantic neighbor density (mean cosine distance to nearest abstracts) as a proxy for novelty rather than direct experimental validation or expert labels.",
            "human_validation_required": false,
            "human_validation_frequency": "N/A (authors emphasize their validation methodology avoids expert labeling; they used automated temporal/citation-based labeling for test sets).",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / literature-embedding based (applies to natural-language scientific abstracts; not a formalized symbolic domain). The empirical nature makes generation-validation gaps domain-sensitive for absolute metrics, motivating RND's relative approach.",
            "gap_mitigation_strategies": "RND itself is proposed as a mitigation strategy: by producing domain-invariant novelty scores it can be used as a reward signal in RL to guide reasoning/generation models; authors propose integrating RND into end-to-end AI scientist workflows to reduce generation/validation mismatch.",
            "evidence_supporting_gap": "The paper reports that LLM-based judges (without retrieval) perform at AUROC ≈ 0.5 (random), and that reasoning models like Sonnet-3.7 require external literature search to reach AUROC ≈ 0.8 in-domain but degrade to ≈ 0.6 out-of-domain; absolute local-density methods (Historical Dissimilarity) perform well in individual domains but fail on mixed-domain (HD AUROC drops to 0.362 or Absolute Local Density listed as 0.395 in mixed set), demonstrating generators/judges outperforming validators in certain contexts and/or validators failing cross-domain.",
            "evidence_contradicting_gap": "Historical Dissimilarity (absolute local density) achieves similar performance to RND within single domains (NeurIPS and Nature Medicine), indicating that some validators can match generator-judge accuracy in-domain; this shows the gap is primarily a cross-domain / generalization problem rather than a universal generator&gt;validator phenomenon.",
            "computational_cost_ratio": "Not reported as a numeric ratio between validation and generation. RND computational complexity is O(P * Q) per evaluated idea (authors choose P=100, Q=50 as trade-off). The paper notes large P/Q increases validation cost; generation cost for LLMs is discussed qualitatively (LLM runs multiple times), but no direct cost-ratio is provided.",
            "uuid": "e2079.0"
        },
        {
            "name_short": "HD / Absolute Local Density",
            "name_full": "Historical Dissimilarity (absolute local density)",
            "brief_description": "A prior absolute local-density based novelty metric that computes the average distance between an idea's embedding and the embeddings of its k most similar historical abstracts (e.g., 5 nearest neighbors) as a measure of novelty.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Historical Dissimilarity (HD) / Absolute Local Density",
            "system_type": "semantic-embedding local-density metric",
            "scientific_domain": "research-idea novelty assessment (single-domain evaluation emphasized)",
            "output_type": "novelty score (distance-based)",
            "novelty_level": "not applicable (metric for assessing novelty)",
            "generation_method": "N/A",
            "validation_method": "Compute average Euclidean (or cosine) distance to a small fixed number (e.g., 5) of most similar abstracts from historical and contemporary corpora; sometimes combined with citation-based 'Contemporary Impact' into Overall Novelty (ON). Validated against small human-labeled test sets in prior work.",
            "generation_performance": "N/A",
            "validation_performance": "Reported in this paper (as 'Absolute Local Density') — NeurIPS AUROC = 0.851; Nature Medicine AUROC = 0.757; Mixed (cross-domain) AUROC = 0.395 (severe degradation in cross-domain scenario). The paper also elsewhere reports HD matched RND in single-domain experiments but failed on Mixed test set.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Per-paper: performs adequately in single domains but performance collapses with mixed-domain inputs (indicates sensitivity to domain-specific density distributions and therefore poor generalization as novelty increases beyond domain norms). No explicit per-novelty-level curves provided.",
            "generation_validation_comparison": "Used as a validator baseline; compared to LLM judges and RND. Shows that absolute-density validators can match judge performance in-domain but fail when ideas cross domain boundaries, evidencing a generalization gap.",
            "uncertainty_quantification": "Not reported in this paper beyond empirical performance; prior HD methods rely on small k nearest neighbors making variance sensitive to selection choices.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Poor: Mixed AUROC = 0.395 indicates weak OOD performance across domains.",
            "validation_proxy_metrics": "Yes — uses local semantic distance as proxy for novelty.",
            "human_validation_required": null,
            "human_validation_frequency": "Not specified in this paper; prior work validated HD against small human-labeled sets.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / literature-embedding based",
            "gap_mitigation_strategies": "None proposed here beyond tuning selection of neighbor count and corpus definitions; paper argues these design choices limit HD's cross-domain generalizability.",
            "evidence_supporting_gap": "Large drop in AUROC on Mixed test set (from ≈0.75-0.85 in single domains to ≈0.395 mixed) shows inability to validate novelty across domain boundaries.",
            "evidence_contradicting_gap": "Per-domain performance comparable to RND on single-domain test sets, indicating that absolute-density methods can be effective when the domain is fixed and corpus choices are appropriate.",
            "computational_cost_ratio": "Not reported; HD typically uses small k (e.g., 5) making it computationally cheaper than RND's chosen P=100,Q=50 in practice.",
            "uuid": "e2079.1"
        },
        {
            "name_short": "ON",
            "name_full": "Overall Novelty (ON)",
            "brief_description": "A metric formed by combining Historical Dissimilarity (HD) and Contemporary Dissimilarity (CD) with a citation-based Contemporary Impact (CI) factor into a multiplicative novelty score (ON = HD × CI^CD as reported in the paper).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Overall Novelty (ON)",
            "system_type": "composite semantic-density + citation metric",
            "scientific_domain": "research-idea novelty assessment",
            "output_type": "novelty score",
            "novelty_level": "not applicable",
            "generation_method": "N/A",
            "validation_method": "Computed from historical and contemporary embedding distances and citation-derived impact; previously validated against small human-labeled sets.",
            "generation_performance": "N/A",
            "validation_performance": "Not reported in this paper beyond prior citations; used as baseline in literature review.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported here.",
            "generation_validation_comparison": "Mentioned as an absolute-density based baseline; paper critiques ON for sensitivity to arbitrary choices (size of collections, temporal boundaries) harming cross-domain generalization.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported; paper implies ON would degrade cross-domain due to absolute density dependence.",
            "validation_proxy_metrics": "Yes — uses distances and citation proxies.",
            "human_validation_required": null,
            "human_validation_frequency": "Not specified.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / literature-embedding based",
            "gap_mitigation_strategies": "None specific; the paper uses ON as an example of absolute-density approaches that RND aims to improve upon.",
            "evidence_supporting_gap": "Discussion that ON's dependence on arbitrary corpus/time splits undermines generalization across fields.",
            "evidence_contradicting_gap": "Not provided.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2079.2"
        },
        {
            "name_short": "LLM+Search",
            "name_full": "LLM + literature search judge",
            "brief_description": "A method that feeds the 10 most relevant retrieved papers' titles/abstracts to a large language model, which then judges whether the idea substantially overlaps (non-novel) or not (novel); used as a baseline in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Large language model with retrieval (LLM+literature search)",
            "system_type": "large language model with retrieval augmentation (RAG-style)",
            "scientific_domain": "research-idea novelty assessment; supports idea judgment for scientific proposals",
            "output_type": "binary novelty decision or binary-normalized score (0/1), or probabilistic judgment aggregated to AUROC",
            "novelty_level": "N/A (judges novelty; can detect in-distribution concepts better than OOD ones when retrieval is relevant)",
            "generation_method": "N/A (this is a judgment method, though LLMs themselves can generate proposals in other works)",
            "validation_method": "The LLM inspects the 10 retrieved most-relevant papers and judges overlap; in experiments authors ran each LLM configuration multiple times (three runs) and averaged results.",
            "generation_performance": "N/A for generation; as a judge its performance depends on retrieval and internal model: without retrieval LLMs perform ~AUROC 0.5 (random); with retrieval Sonnet-3.7 reached ~AUROC 0.8 on NeurIPS but degraded to ~0.6 on Nature Medicine and cross-domain.",
            "validation_performance": "Reported: baseline 'without search' AUROC ≈ 0.5; with retrieval performance varies by domain and model — Sonnet-3.7 + search: ~0.8 (NeurIPS), ~0.6 (Nature Medicine and Mixed). Exact AUROC per-model per-experiment in supplemental Table D.1 (not fully reproduced in main text).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Performance degrades when ideas are cross-domain or when the model's internal knowledge is lacking for the domain; adding retrieval improves in-domain performance but not necessarily cross-domain robustness.",
            "generation_validation_comparison": "Paper highlights that LLMs can be strong judges when given relevant literature but are inconsistent and sensitive to input perturbations; validates generation-vs-validation asymmetry since LLMs can generate many ideas easily but their judging reliability varies across domains.",
            "uncertainty_quantification": "Not reported; LLM outputs have run-to-run variability which authors address by averaging multiple runs and reporting standard deviation (table D.1).",
            "calibration_quality": "Not reported quantitatively; paper notes sensitivity to phrasing and input perturbations undermines reliability.",
            "out_of_distribution_performance": "Degrades substantially: e.g., Sonnet-3.7 drops from AUROC ≈0.8 in CS to ≈0.6 in biomed / mixed; GPT-4o without retrieval performed at AUROC ≈0.5 (random).",
            "validation_proxy_metrics": "Yes — relies on retrieved-paper overlap plausibility/coherence as a proxy for novelty (if overlap detected -&gt; non-novel).",
            "human_validation_required": null,
            "human_validation_frequency": null,
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / natural-language",
            "gap_mitigation_strategies": "Providing retrieval (10 relevant papers) is used to mitigate LLM judge failures; authors also suggest using RND as a reward signal to train reasoning models to generate more genuinely novel ideas.",
            "evidence_supporting_gap": "Empirical results show LLM judge performance highly dependent on domain and retrieval; without retrieval performance is random, indicating a gap between generation/idea-proposal ability and robust validation.",
            "evidence_contradicting_gap": "When strong retrieval is available and the internal model knowledge is strong (e.g., Sonnet-3.7 on CS), LLM+search can match validator performance in-domain, suggesting the gap can be narrowed with retrieval augmentation.",
            "computational_cost_ratio": "Not reported quantitatively; running LLMs multiple times (authors run 3 runs) increases computational cost; retrieval adds extra cost but no numeric ratio is provided.",
            "uuid": "e2079.3"
        },
        {
            "name_short": "Sonnet-3.7",
            "name_full": "Sonnet-3.7 (Anthropic)",
            "brief_description": "A recent high-capability reasoning LLM (Anthropic) referenced and evaluated in the paper as a judge for novelty, used in variants with and without retrieval/guidelines/tournament formats.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Sonnet-3.7",
            "system_type": "large language model / reasoning model",
            "scientific_domain": "general-purpose language model applied to research-novelty judgment",
            "output_type": "novelty judgment (binary or graded), textual review-style assessments",
            "novelty_level": "N/A (model is a general LLM)",
            "generation_method": "autoregressive transformer pretraining fine-tuned for reasoning (paper references but does not detail training here)",
            "validation_method": "Used as human-like judge: three evaluation modes — without literature, with NeurIPS guideline, with literature search (10 retrieved papers), and Swiss-tournament pairwise comparisons; outputs were compared to automated labels and AUROC computed.",
            "generation_performance": "Not applicable as generator in this paper's experiments; when used as judge with literature-search it achieved AUROC ≈ 0.8 on NeurIPS dataset (in-domain).",
            "validation_performance": "With literature search: AUROC ≈ 0.8 (NeurIPS); with guideline/tournament but no external literature: low accuracy comparable to random; out-of-domain (Nature Medicine / Mixed) performance degraded to ~0.6.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Performance sensitive to domain and to availability of external evidence; degrades on cross-domain examples and when no retrieval is provided.",
            "generation_validation_comparison": "Paper highlights that Sonnet-3.7 as a judge can approach human-level in-domain given retrieval, but is unreliable without it, pointing to a mismatch between generation capacity and reliable validation across domains.",
            "uncertainty_quantification": "Not reported beyond standard-deviation across repeated runs (LLM outputs variable; authors ran three trials and reported averages and SDs).",
            "calibration_quality": "Not reported numerically; qualitative observations indicate sensitivity to prompts and lack of calibration across phrasing variants.",
            "out_of_distribution_performance": "Moderate to poor: AUROC falls to ~0.6 on biomed and mixed tests despite good in-domain results.",
            "validation_proxy_metrics": "N/A (model reasons over retrieved content rather than using explicit proxy metrics).",
            "human_validation_required": null,
            "human_validation_frequency": null,
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / natural-language",
            "gap_mitigation_strategies": "Providing retrieval and structured prompting (guidelines/tournament/few-shot) are tested to improve judge reliability.",
            "evidence_supporting_gap": "Sonnet-3.7's strong in-domain but weak cross-domain performance is used as evidence that LLM judging/generation capabilities don't generalize uniformly.",
            "evidence_contradicting_gap": "When provided adequate external literature Sonnet-3.7 can perform well in-domain, suggesting access to retrieval mitigates some gaps.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2079.4"
        },
        {
            "name_short": "Deepseek-r1",
            "name_full": "Deepseek-r1",
            "brief_description": "A recent reinforcement-learning-enhanced reasoning model referenced as showing improved reasoning capabilities and potential to be guided by reward signals like RND.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Deepseek-r1",
            "system_type": "reinforcement-learned reasoning large language model",
            "scientific_domain": "general reasoning; applied to novelty judgment in discussion",
            "output_type": "textual reasoning outputs / novelty judgments (when used as judge in referenced work)",
            "novelty_level": "N/A",
            "generation_method": "reinforcement learning to incentivize reasoning capabilities (cited work)",
            "validation_method": "Not directly evaluated in this paper; authors cite it as an example of models improved via rule-based rewards and posit RND could be used as such a reward.",
            "generation_performance": "Not reported in this paper; authors state Deepseek-r1 shows improved reasoning capabilities compared to autoregressive models.",
            "validation_performance": "Not reported here.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported.",
            "generation_validation_comparison": "Paper suggests RND could be used as a reward signal to train models like Deepseek-r1 to generate more novel ideas, implying current generation-validation misalignment could be addressed by integrating RND during training.",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": null,
            "validation_proxy_metrics": null,
            "human_validation_required": null,
            "human_validation_frequency": null,
            "formal_verification_used": null,
            "domain_formalization_level": "empirical / natural-language",
            "gap_mitigation_strategies": "Proposed: use RND as reward in RL training to bias generation toward more genuinely novel outputs that can be validated.",
            "evidence_supporting_gap": "Mentioned conceptually; no empirical data in this paper for Deepseek-r1.",
            "evidence_contradicting_gap": null,
            "computational_cost_ratio": null,
            "uuid": "e2079.5"
        },
        {
            "name_short": "M3-Embedding",
            "name_full": "M3-Embedding (multi-lingual, multi-function, multi-granularity embeddings)",
            "brief_description": "Embedding model used by the authors to create 1024-dimensional embeddings for titles and abstracts across the literature corpora; used for KNN neighbor retrieval and density calculations.",
            "citation_title": "Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation",
            "mention_or_use": "use",
            "system_name": "M3-Embedding",
            "system_type": "text embedding neural model",
            "scientific_domain": "natural-language representations for scientific abstracts",
            "output_type": "1024-dimensional embedding vectors for titles and abstracts",
            "novelty_level": "N/A (representation model)",
            "generation_method": "N/A",
            "validation_method": "Used to compute semantic distances; validation of embedding quality is not deeply evaluated in this paper beyond empirical downstream novelty-assessment performance.",
            "generation_performance": "N/A",
            "validation_performance": "Indirect: embeddings fed to RND and baselines that produce the reported AUROC values; no independent embedding performance metrics (e.g., retrieval precision) reported here.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not applicable.",
            "generation_validation_comparison": "N/A",
            "uncertainty_quantification": "Not reported",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Not reported explicitly; overall RND robustness suggests embeddings are sufficiently consistent across corpora for the task but the paper notes embedding quality is a limitation and future work could explore domain-specific fine-tuning.",
            "validation_proxy_metrics": "N/A",
            "human_validation_required": null,
            "human_validation_frequency": null,
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / natural-language",
            "gap_mitigation_strategies": "Authors recommend investigating specialized embedding models for scientific literature as future work to improve representation for technical terminology.",
            "evidence_supporting_gap": "Authors note algorithm performance depends on embedding quality and biases in literature databases can influence novelty estimates; implies representation issues can contribute to validation gaps.",
            "evidence_contradicting_gap": null,
            "computational_cost_ratio": "Not reported; embedding retrieval performed via Elasticsearch for scalability.",
            "uuid": "e2079.6"
        },
        {
            "name_short": "AI Scientist (Lu et al.)",
            "name_full": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery (Lu et al. 2024)",
            "brief_description": "Prior work referenced that combines idea generation, evaluation, and refinement in an AI Scientist framework, using chain-of-thought prompting and retrieval APIs to improve LLM judgments.",
            "citation_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "mention_or_use": "mention",
            "system_name": "AI Scientist (framework)",
            "system_type": "multi-component AI research automation (LLM-based generator + evaluators)",
            "scientific_domain": "automated scientific discovery (general)",
            "output_type": "generated research ideas / hypotheses and iterative refinements",
            "novelty_level": "varies; claims of near human-level idea generation in cited work but reliability concerns remain",
            "generation_method": "LLM-based generation with chain-of-thought prompting and external retrieval (Semantic Scholar API) for refinement",
            "validation_method": "In cited work, integrates LLM judgment augmented with NeurIPS review guideline and semantic retrieval to assess novelty; details are in the cited paper rather than this one.",
            "generation_performance": "Not quantified in this paper; cited as motivating prior work.",
            "validation_performance": "Not quantified here; authors critique LLM-as-judge reliability and sensitivity to input perturbations as reasons to prefer non-LLM validators like RND.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported here.",
            "generation_validation_comparison": "Used as motivating example: LLM-based pipeline can generate many ideas but its judgment reliability is questionable and sensitive to phrasing and retrieval quality.",
            "uncertainty_quantification": null,
            "calibration_quality": null,
            "out_of_distribution_performance": null,
            "validation_proxy_metrics": "Relies on retrieval and guideline-based prompting as proxy evidence.",
            "human_validation_required": null,
            "human_validation_frequency": null,
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / natural-language",
            "gap_mitigation_strategies": "Not in this paper; this paper suggests RND could be used to provide domain-invariant novelty reward to such frameworks.",
            "evidence_supporting_gap": "Cited papers and this paper's experiments show that LLMs' judging performance is unstable and domain-sensitive, supporting the view that generation capabilities can outstrip robust validation.",
            "evidence_contradicting_gap": null,
            "computational_cost_ratio": null,
            "uuid": "e2079.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2
        },
        {
            "paper_title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
            "rating": 2
        },
        {
            "paper_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
            "rating": 2
        },
        {
            "paper_title": "Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation",
            "rating": 1
        },
        {
            "paper_title": "Incentivizing reasoning capability in llms via reinforcement learning (Deepseek-r1)",
            "rating": 2
        },
        {
            "paper_title": "Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation",
            "rating": 2
        },
        {
            "paper_title": "Historical Dissimilarity / Overall Novelty (original formulation by Su et al.)",
            "rating": 2
        },
        {
            "paper_title": "GPT-4o system card (or related Anthropic Sonnet-3.7 documentation)",
            "rating": 1
        }
    ],
    "cost": 0.022006249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ENABLING AI SCIENTISTS TO RECOGNIZE INNOVATION: A DOMAIN-AGNOSTIC ALGORITHM FOR ASSESSING NOVELTY
10 Mar 2025</p>
<p>Yao Wang wang-yao24@mails.tsinghua.edu.cn 
Department of Automation
Tsinghua University</p>
<p>Mingxuan Cui mingxuan.cui@mail.nankai.edu.cn 
Nankai University</p>
<p>Arthur Jiang arthursjiang@gmail.com 
Yidu Technology 
ENABLING AI SCIENTISTS TO RECOGNIZE INNOVATION: A DOMAIN-AGNOSTIC ALGORITHM FOR ASSESSING NOVELTY
10 Mar 2025014BE4AA0DE10575620E8518F64631CFarXiv:2503.01508v2[cs.AI]
In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery.This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by comparing an idea's local density with its adjacent neighbors' densities.We first developed a scalable methodology to create test set without expert labeling, addressing a fundamental challenge in novelty assessment.Using these test sets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.820)and biomedical research (AUROC=0.765)domains.Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent accuracies across domains by its domain-invariant property, outperforming all benchmarks by a substantial margin (0.795 v.s.0.597) on cross-domain evaluation.These results validate RND as a generalizable solution for automated novelty assessment in scientific research.</p>
<p>Introduction</p>
<p>In the pursuit of Artificial General Intelligence (AGI), automating scientific research and knowledge discovery presents both a formidable challenge and an exciting opportunity, as it will be groundbreaking to expand the boundaries of human knowledge by leveraging scalable computing resources.Therefore, as the capabilities of large language models (LLMs) continue to improve, researchers have started to explore their use in automating various aspects of the research process, including the generation of novel ideas, as exemplified by the AI scientist concept (Lu et al. [2024]).</p>
<p>A key task for any AI-based scientist is the generation of novel research ideas, a task traditionally performed by human scientists during their brainstorming phase.While LLMs have shown promise in generating a large pool of ideas quickly and cost-effectively, akin to the initial stages of human research, the real challenge lies in evaluating these ideas for their novelty.Traditionally, novelty in scientific research has been assessed through peer review and expert evaluations, where domain specialists judge the originality of an idea based on their experience and familiarity with existing literature.However, such assessments are inherently subjective, time-consuming, and inconsistent across reviewers.Moreover, as the volume of scientific output grows exponentially, manual novelty assessment struggles to keep pace.Automated methods are therefore crucial for filtering out redundant ideas and promoting genuinely innovative directions.</p>
<p>Existing approaches primarily fall into two categories: (1) leveraging large language models (LLMs) as judges and (2) using absolute local density-based novelty metrics.</p>
<p>The most straightforward approach is to use LLMs as judges to evaluate the novelty of ideas.Si. et al. adopted a Swiss system tournament design to evaluate ideas by using LLM as judge (Si et al. [2024]), which was further applied in Nova: An Iterative Planning and Search Approach to enhance Novelty and Diversity of LLM-Generated Ideas (Hu et al. [2024]).To improve LLM's accuracy of judgment, Lu include NeurIPS review guideline and Semantic Scholar API as tools (Lu et al. [2024], Su et al. [2024]): the NeurIPS review guideline was served as both chain-of-thoughts prompts and few-shot examples, while search API enabled LLM to search top 10 relevant papers to determine the novelty of given idea.</p>
<p>An alternative approach relies on the absolute local density in semantic embedding space to measure novelty.Su. et al. introduced the concept of Historical Dissimilarity and Contemporary Dissimilarity, calculated as the average Euclidean distance (local density) between a generated abstract's embedding and the embeddings of the five most similar abstracts from a historical database and a contemporary database, respectively.In combination with the citation-based Contemporary Impact metric, they developed the Overall Novelty (ON) metric (Su et al. [2024]).Their study validated that ON correlates well with human-labeled novelty, demonstrating its effectiveness as a novelty assessment measure.</p>
<p>Though the aforementioned approaches provide potential solutions on idea novelty evaluation, there are major challenges when considering practical issues.</p>
<p>First, the reliability of using-LLM-as-judge remains questionable, even with external knowledge or tools.Studies have demonstrated that auto-regressive LLMs like GPT-4o (Hurst et al. [2024]) produce outputs sensitive to input perturbations (Zhuo et al. [2023], Singh et al. [2024]).In novelty assessment specifically, this means identical research ideas phrased differently might receive contradictory novelty ratings.While recent reasoning models, such as DeepSeek-r1 (Guo et al. [2025]) and Sonnet-3.7 (Anthropic [2025]), show improved reasoning capabilities, their reliability for scientific novelty judgment remains unvalidated.</p>
<p>Second, the absolute local density-based metric from (Su et al. [2024]) shows significant limitations across diverse research contexts.By relying on just 5 most similar abstracts from history and contemporary databases as reference points, the metric's validity becomes highly dependent on arbitrary choices: the size of paper collections, the temporal boundaries defining 'history' versus 'contemporary,' and the selection criteria for inclusion.Different research domains also exhibit varying citation patterns, publication velocities, and semantic densities, which undermined the metric's generalizability across research domains.</p>
<p>Last but not least, the validation methodology used to assess novelty evaluators themselves was significantly lacking.In most cases, the validation of novelty metric depends on small test sets manually labeled by human experts within a specific domain (Hu et al. [2024], Lu et al. [2024], Su et al. [2024], Si et al. [2024]).Such validations are difficult to scale across different research areas, as they are often highly specialized and tailored to particular fields of study.What's worse, manually produced novelty labels rapidly become outdated: as scientific research advances continuously, ideas labeled "novel" today quickly become established knowledge, rendering static human-labeled validation sets increasingly inaccurate over time.</p>
<p>To address these challenges, in this paper, we establish comprehensive semantic embedding databases for novelty assessment.These databases incorporate over 30 million publications from two distinct domains: Pubmed, the leading biomedical literature search engine with nearly 36 million articles (Jin et al. [2024]), and Arxiv, which contains more than 2.3 million scholarly articles across eight subject areas (Cornell Tech [2023]).</p>
<p>Based on these resources, we propose the Relative Neighbor Density (RND) algorithm, which measures novelty by analyzing the distribution patterns of semantic neighbors rather than simple absolute local density.This approach proves more reliable than LLM-based judgments and more generalizable than existing absolute local density-based metrics across different research domains.We also develop an automated validation methodology that leverages temporal publication patterns to evaluate novelty without requiring expert manual labeling.Our extensive evaluations using test sets from computer science, biomedical science, and cross-domain contexts demonstrate that our proposed algorithm maintains accuracy within specific domains while scaling effectively across diverse research areas.</p>
<p>Our main contributions are:</p>
<p>• A novel neighbor density-based Relative Neighbor Density (RND) algorithm for assessing research idea novelty that is robust across domains, which holds domain-invariant property</p>
<p>Related Works</p>
<p>Assessing the novelty of research ideas is a fundamental challenge in automating scientific discovery.Various approaches have been explored in recent years, leveraging Large Language Models (LLMs) and semantic similarity measures to evaluate idea originality.This section reviews existing methods, highlighting their strengths and limitations.</p>
<p>LLMs for Novelty Assessment</p>
<p>Recent work has demonstrated promising results in using LLMs as autonomous judges for research novelty.Si et al. (Si et al. [2024]) evaluated this approach using ICLR submissions, converting them into standardized project proposals and conducting pairwise comparisons between accepted and rejected papers(Table 7).Their Swiss tournament system iteratively paired proposals based on accumulated scores, with Claude-3.5-Sonnetachieving 71.4% accuracy in predicting paper acceptance.As a control measure, they included human expert reranking, which revealed notable discrepancies between automated and human judgments.</p>
<p>Lu et al. (Lu et al. [2024]) expanded this concept with their AI Scientist framework, integrating idea generation, evaluation, and refinement.Their system employs chain-of-thought prompting and external knowledge retrieval via Semantic Scholar API to enhance assessment quality.While showing promise in matching human-level performance, these LLM-based approaches face fundamental challenges in reliability and consistency, as highlighted by studies showing their sensitivity to input variations (Zhuo et al. [2023], Singh et al. [2024]).</p>
<p>Absolute Local Density-based Metrics</p>
<p>An alternative approach focuses on semantic local density to evaluate novelty.Su et al. (Su et al. [2024]) used the Historical Dissimilarity (HD), which is the average Euclidean distance between the generated abstract embedding and embeddings of the 5 most similar abstracts in historical literature base.We denote it as "Absolute Local Density" because the average distance is a metric of local density, and they use the value of density directly.In addition to HD, they also used Contemporary Dissimilarity (CD), which calculate in same algorithm in contemporary literature base, which is also an absolute local density-based metric.</p>
<p>Based on HD and CD, they developed Overall Novelty (ON) metric as below,
ON = HD × CI CD (1)
The main challenge of local density-based metric is that the density values vary across different domains.In case of evaluating ideas from mixed research domains, the variance would cause a severe degrade in the accuracy.</p>
<p>Validating Novelty Metrics</p>
<p>A critical limitation in existing research is the lack of scalable validation methodologies.Current approaches (Lu et al. [2024], Su et al. [2024], Hu et al. [2024], Si et al. [2024]) typically rely on small-size literature database and manually labeled test set created by domain experts.For instance, Su et al. (Su et al. [2024]) constructed an "ecosystem" for computer science (CS) using information extracted from 85,217 papers-a dataset that represents only a small fraction of the CS literature available on platforms like arXiv.While their analysis demonstrated promising correlations between novelty scores and human labels across 100 manually evaluated abstracts, the methodology's reliance on domain-specific expertise significantly constrains its generalizability, which is echoed by reviewer's comments that only validating CS is "relatively simple" (Openreview Reviewer 2X9t [2025]).</p>
<p>Furthermore, the scalability challenge persists across all existing frameworks.The creation of test sets currently requires expert labeling, making large-scale evaluation prohibitively resource-intensive.This limitation underscores the need for automated approaches to test set creation that maintain fidelity while eliminating the requirement for extensive manual labeling.</p>
<p>Method</p>
<p>Problem Description</p>
<p>Given a set of ideas I,
I = {idea i }, i ∈ <a href="2">1, N </a>
Where idea i is a sequence of words or characters in nature language.N ≥ 1 represents the number of ideas whose novelty needs to be assessed.</p>
<p>The objective is to design a mapping F from idea space to a score in real value space
F (idea i ) = score i , where idea i ∈ I, score i ∈ R (3)
The novelty score score should be monotonic, meaning that for any two ideas idea i and idea j , if idea i is more novel than idea j , then their corresponding scores must satisfy:
∀ idea i , idea j ∈ I, idea i ≻ idea j ⇒ F (idea i ) &gt; F (idea j )(4)
where idea i ≻ idea j denotes that idea i is considered more novel than idea j based on a given novelty criterion.</p>
<p>Semantic Embedding &amp; Literature Database</p>
<p>Each published literature's abstract, which is also a sequence of words or characters in natural language, is denoted as
a j .
The semantic embedding model is a mapping function G , which maps ideas and abstracts into embedding vectors:
G (idea i ) = v i , where v i ∈ R dims ,(5)G (a j ) = v j , where v j ∈ R dims (6)
Thus, the preprocessed literature semantic database is represented as a set A:
A = {(a j , v j ) | j ∈ [1, M ]}(7)
We collected 36 million academic articles from the PubMed Download API (National Library of Medicine [2025]) and 2.6 million papers from the ArXiv dataset (Cornell University [2025]).Among all fetched documents, only those with both a non-empty title and abstract were considered valid for the experiment, resulting in 25,360,114 papers from PubMed and 2,643,057 papers from ArXiv.</p>
<p>For each paper, two semantic embedding vectors were generated-one from its title and another from its abstract-using the M3-Embedding model Chen et al. [2024].The embedding vector dimension, denoted as dims, is 1024.All texts and embedding vectors were stored in Elasticsearch Version 8 for efficient retrieval.</p>
<p>Algorithm</p>
<p>For each idea idea i and its embedding v i , we first find its P nearest neighbors using k-Nearest Neighbors (KNN) search:
{v 1 , v 2 , . . . , v P } = KN N (v i , P, A) (8) where v j is the j-th nearest neighbor of v i , j ∈ [1, P ].
For the idea itself v i and each neighbor v j , the neighbor density (ND) is defined as below
N D = 1 Q Q k=1 d(v, v k )(9)
where v k denotes the k-th nearest article's embedding vector in the literature corpus A and d(•, •) is the cosine distance between two vectors.</p>
<p>We define the set S i that contains the neighbor density values of idea i 's neighbors:
S i = {N D j | j ∈ [1, P ]}(10)
Finally, we compute the novelty score score i for idea i as:
score i = |{N D ∈ S i | N D ≤ N D i }| |S i | × 100 (11)
The selection of values of P and Q is a trade-off between reliability of estimation and other cost.A lower values of P will cause biased estimation of novelty score; while higher value of P will increase computing cost in O(P • Q) complexity.Similarly, lower Q will also cause unreliable estimation of local density.Meanwhile, a higher Q value will not only be computational costly, but also sacrifice sensitivity as ND converges to its expectation as Q increase, losing information local density.Refer to Appendix A for detailed analysis on the effects of P and Q.</p>
<p>Based on analysis and empirical experiments, we set P = 100 and Q = 50.</p>
<p>The pseudoscope implement calculation of ND is presented in Algorithm 1; and the pseudoscope of complete algorithm is provided in Algorithm 2.</p>
<p>Algorithm</p>
<p>Validation without Human Labeling</p>
<p>As a novelty evaluation algorithm, the most challenging point in past research is to find a reliable labeled test set to evaluate the algorithm.Therefore, we propose a new method to construct a convincing test set instead of relying on human experts to annotate it.</p>
<p>For the positive samples (a.k.a novel ideas) in the test set, we select recent articles from top journals or conferences.For the negative samples (a.k.a.non-novel ideas), highly cited articles published before the last few years were selected, also from the research domain's top journals or conferences.The fundamental principles behind such methodology were: high-quality novel ideas are more likely to be published in recent issues and top journals or conferences; while after time passes, the at-the-time novel ideas were more likely to attract attention and related works, thus become non-innovative at present.</p>
<p>In this way, we can make positive and negative samples have a more obvious difference in novelty in a relatively objective and recognized way.We have two test sets: NeurIPS, which represents the most advanced research results in the field of computer science, and Nature Medicine, which represents the most cutting-edge papers in the medical field.The sample year distribution of the test sets can be found in Table 1 NeurIPS test set: The initial corpus consists of papers that are Accept (oral) or Accept (spotlight) by Program Chairs at the 2024 NeurIPS conference, which represents the latest research results in computer science.Furthermore, we select articles from the initial corpus that explicitly mention that the papers have obvious novelty in the comments of Program Chairs to form the positive samples of the NeurIPS test set.The comments and decision information of Program Chairs can be obtained on the OpenReview.netwebsite.At the same time, we use the Semantic Scholar API to obtain the 99 most cited papers published in the NeurIPS conference from 2015 to 2020 to form the negative samples of the test set.The titles of all samples are presented in Table 4 Nature Medicine test set: The positive samples of the Nature Medicine test set consist of articles classified as "Article" type, published in Nature Medicine from August 2024 to February 2025, according to the classification on the nature.comwebsite.Articles related to phase 2 or phase 3 trials were excluded.And we used the same method as the negative samples of the NeurIPS test set to obtain 99 articles of Nature Medicine with the highest citation count in the past 15 years as negative samples of the test set.The titles of all samples are presented in</p>
<p>Baseline</p>
<p>To evaluate our algorithm, we selected all existing novelty assessment algorithms as baselines, categorized into two groups: LLM-based and non-LLM-based.Non-LLM-based algorithms, including Relative Neighbor Density(Ours), Historical Dissimilarity(HD), and Overall Novelty(ON), rely solely on literature search and mathematical calculations.Since the output of the literature search for the same query remains consistent, we conducted a single test to assess the algorithm's performance.In contrast, for LLM-based algorithms, due to the inherent variability of LLM outputs, we ran three tests for each algorithm, calculated the average result, and included the standard deviation in the table.The full experimental results of the LLM-based method are provided in Table D.1.</p>
<p>For all methods, we use the abstracts of the papers in the test set as "ideas" for testing.</p>
<p>Historical Dissimilarity: Identify the five most relevant papers based on their embeddings and compute the Euclidean distance between the embedding of the idea and the embeddings of the abstracts of these five papers.The final novelty score is obtained by averaging these distance values.</p>
<p>Overall Novelty: The historical database contains papers from 2011 to 2021, and the contemporary database contains papers from 2021 to 2025.The score calculation method refers to equation 1.</p>
<p>LLM + literature search: Provide LLM with the titles and abstracts of the 10 most relevant papers to the given idea.The model then assesses whether the core concepts of these papers significantly overlap with the idea(Table 8).If substantial overlap is detected, the idea is deemed non-novel and assigned a score of 0. If no significant overlap is found, the idea is considered novel and assigned a score of 1.</p>
<p>LLM with guideline: Utilize the NeurIPS 2024 review guidelines to assist LLM in evaluating the novelty of ideas(Table 6).The final score is determined based on the "Overall" score provided in the review assessment.</p>
<p>LLM with tournament: First, the idea is transformed into the Standardized Project Proposal format(Table 7).Next, the novelty of all standardized ideas is assessed using the Swiss tournament method, where ideas are iteratively compared in a structured competition.Finally, each idea is assigned a score based on the number of wins it accumulates throughout the tournament.</p>
<p>Accuracy Evaluation</p>
<p>As shown in Table 2, our enhanced neighbor density-based novelty measurement algorithm outperforms all baseline models on both the Nature Medicine and Mixed test sets, while also demonstrating strong performance on the NeurIPS test set.2: Validation of Different methods, measured by AUROC.HD: Historical Dissimilarity (section 2.2).ON: Overall Novelty (section 2.2).LLM + literature search: supplementing LLM with 10 relevant papers, which were searched by idea's embedding from our literature database using semantic embedding.LLM with guideline: using NeurIPS 2024 review guideline to help LLM judge the novelty of ideas, which is not applicable to Nature Medicine.Therefore, the results of Nature Medicine and Mixed are marked as not applicable.LLM with tournament: a Swiss system tournament design to evaluate ideas by using LLM as judge.</p>
<p>Model</p>
<p>By comparing the results of various LLM-related algorithms, we observe a key similarity between Sonnet-3.7 with guideline and Sonnet-3.7 with tournament: both methods provide very limited external knowledge to the LLM, with no existing literature being fed into the model.As a result, the model's judgment of novelty is highly inaccurate.In contrast, the LLM + literature search method inputs the 10 most relevant papers to the idea, significantly improving the accuracy of the model's judgment.Moreover, the accuracy of the LLM + literature search method is much higher in the field of computer science compared to the field of biomedicine, highlighting the significant impact of the model's internal knowledge on the judgment outcomes, even with the addition of external knowledge.Additionally, Sonnet-3.7 (Anthropic [2025]) and Deepseek-r1 (Guo et al. [2025]) show much higher AUROC scores than GPT-4o, indicating that when external knowledge is provided, the performance of the inference model greatly surpasses that of the autoregressive model.However, we observed that the Historical Dissimilarity (HD) metric closely matched the performance of our proposed method on the Nature Medicine and NeurIPS test set.In contrast, on the Mixed test set, there was a significant disparity, with our method achieving an AUROC of 0.795, while HD only reached 0.362.This prompted us to further investigate the underlying reasons for this substantial difference.</p>
<p>The score distributions provided by the Historical Dissimilarity (HD) metric on the NeurIPS and Nature Medicine test set, as shown in Figure 1, are markedly different.This disparity implies that some negative samples from NeurIPS would be evaluated as more novel than some positive samples from Nature Medicine under this evaluation system, highlighting HD's limited generalization ability across domains.In contrast, the score distributions of our method on both test sets are nearly identical, indicating that our scores are absolute and unaffected by the specific discipline or field.This means that our scores are universally comparable across domains.This result underscores the robust cross-domain evaluation capability of our method, making it applicable for researchers in any field.</p>
<p>Sensitivity Study</p>
<p>Sensitivity of Hyper-Parameter</p>
<p>Since our method has two key parameters: P and Q, we conducted experiments to understand the contribution of each parameter to our algorithm.As illustrated in the left panel of Figure 2, the AUROC on each test set increases as P grows.However, when P &gt; 50, the improvement in AUROC becomes marginal compared to the significant gain observed when increasing P from 10 to 50.This suggests that the marginal benefit of further increasing P diminishes while simultaneously incurring substantial computational costs, given that the algorithm's time complexity is O(P • Q).Additionally, the poor performance observed when P = 10 can be attributed to the biased estimation of novelty scores when P is too small, a phenomenon influenced by multiple factors.For a more detailed explanation, please refer to Appendix A.1.</p>
<p>The right panel of Figure 2 demonstrates that when P remains constant, both excessively small and large values of Q negatively impact the algorithm's performance.This is due to the inaccuracy in local density estimation when Q is too small and the significant reduction in algorithm sensitivity when Q is too large.For further details, please refer to Appendix A.</p>
<p>Sensitivity of Design</p>
<p>In our Relative Neighbor Density algorithm, the notion "relative", i.e. comparing idea's local density with its neighbor's local densities, plays an important role.Moreover, other distance metric, such as Euclidean distance, could also be used in our algorithm.To understand the sensitivity of the current design, we conducted experiments by changing the design of the "relative" notion, and distance metric.The result presented in Table 3</p>
<p>Case Study</p>
<p>We visualize the neighbors of both a novel and a non-novel idea in the embedding vector space to demonstrate the superiority of our algorithm.Figures 3 and 4 show the visualization results of the embedding vectors of an idea and its neighbors on a two-dimensional plane, after dimensionality reduction using t-Distributed Stochastic Neighbor Embedding (t-SNE).While t-SNE excels at preserving the local structure of the data, it does not reliably retain the global structure(Van der Maaten and Hinton [2008]).As a result, the distance between the idea and its P adjacent neighbors is not accurately preserved, but distances between these P neighbors and their Q nearest neighbors is well preserved.</p>
<p>We first use Attention is All You Need (Vaswani et al. [2017]), a highly cited article, as a non-novel idea from the current perspective.Figure 3 clearly illustrates that there is a dense cluster of neighbors around the idea.In contrast, the neighbors around the idea's P neighbors are relatively few and sparse.</p>
<p>Next, we use Evaluating the World Model Implicit in a Generative Model (Vafa et al. [2025]), an article considered highly novel by the NeurIPS 2024 Program Chairs, as an example of a novel idea, based on their comments (openreview [2025]).In Figure 4, it is evident that the idea's local neighbor density is much sparser than its P nearest neighbors.</p>
<p>The experimental results demonstrate that the novelty of an idea is reflected in the local structure of the most similar documents to the idea within the embedding vector space, which supports the correctness of our algorithm in principle.Furthermore, a key difference between Figures 3 and 4 is that Figure 3 shows multiple neighboring clusters centered around the idea's P nearest neighbors, suggesting that the vector density of the two images in the embedding space is notably different.This highlights that the novelty of an idea cannot be determined solely by local density of the idea but must also take into account the vector density surrounding the idea.This is also clearly reflected in the experimental results for the Mixed test set in Table 3.</p>
<p>Discussion</p>
<p>In this work, we proposed a novel neighbor density-based metric for assessing research idea novelty, addressing the limitations of LLM judgment and absolute local density-based metrics.By leveraging large-scale literature embeddings from both biomedical sciences and computer science, our approach ensures robust reliability and cross-domain generalizability.Additionally, we introduced a scalable validation framework that eliminates reliance on expert labeling, enabling objective and reproducible novelty assessment.</p>
<p>Why a Non-LLM Novelty Assessment Algorithm is Necessary?</p>
<p>Assessing the novelty of a research idea is inherently difficult, subjective, and resource-intensive.While LLMs have the potential to assist in this process, their effectiveness is limited by the challenges outlined in the Introduction.Our experiments (see Table 2) highlight these issues: without an integrated search tool, even the most advanced reasoning models' performance was comparable to random guessing (AUROC =0.5).When a search tool was introduced, Sonnet-3.7 achieved similar accuracy on the NeurIPS test set (AUROC =0.8) but experienced significant degradation (AUROC =0.6) on both the Nature Medicine and cross-domain test sets.In contrast, our proposed RND algorithm can produce more reliable and consistent results, as seen in Table 2. Our algorithm is better at distinguishing genuinely novel ideas from the large pool of candidates from mixing research domains (AUROC =0.78 v.s Other's AUROC&lt;=0.6).Such cross-domain novelty assessment capability is crucial to AI scientist, as more and more innovation happened in inter-discipline of research domains.</p>
<p>Recent advancements in reinforcement learning for reasoning models, such as those demonstrated in Deepseek-R1 (Guo et al. [2025]), suggest the potential of rule-based reward systems to guide model development.Our RND algorithm could serve as a sophisticated rewarding mechanism, potentially enhancing reasoning model's capabilities in generating novel scientific ideas and advancing the role of AI in scientific innovation.</p>
<p>Accuracy in Each Domain</p>
<p>In</p>
<p>Domain-invariant Accuracy</p>
<p>However, when tested in the cross-domain test set (the Mixed test set), which includes ideas from both computer science and biomedicine, the performance of HD significantly degraded, with its AUROC dropping to 0.362.In contrast, the AUROC of our proposed algorithm remained robust at 0.795, similar to its performance in the single-domain test sets.</p>
<p>As demonstrated in the Table 3, the relative position of idea's local density among all of its neighbor's local density is crucial for comparing novelty across different domains.</p>
<p>We argue that RND algorithm hold domain-invariant property, i.e. the distribution of novelty scores produced by RND is identical regardless of the tested domain, which explained why our relative density-based approach succeeds in cross-domain scenarios.According to the mathematical reasoning in Appendix A.3, we concluded the distribution of novelty score S is only subject to P (the number of neighbors considered); thus it is invariant to the validation domain.Furthermore, in Figure 1 (right panel) and 5, the actual distribution of scores echoed the theoretical analysis.Such domain-invariant property is crucial for conducting multi-disciplinary scientific research, where ideas from diverse fields must be compared and evaluated effectively.</p>
<p>Why Validation Methods Differ Between Novel and Non-Novel?</p>
<p>When building our test set, an obvious approach might be to use symmetrical sources -for example, using accepted NeurIPS papers as novel samples and rejected NeurIPS papers (specifically those rejected for lacking novelty) as non-novel samples.However, this approach presents significant limitations.Firstly, very few top-tier venues publicly release review comments with explicit novelty assessments, making such data scarce and difficult to generalize across domains.Secondly, papers may be rejected for "lack of novelty" due to incremental advances or methodological similarities, even when addressing previously unexplored topics.</p>
<p>Instead, our definition of novelty relies on how extensively similar ideas have been studied in the literature.Following this definition, we selected highly-cited papers from recent years as our non-novel samples, as these papers represent ideas that have been thoroughly explored and extended by numerous subsequent works.While high citation count itself can indicate either novelty or utility, papers that are both recent and highly-cited typically represent research areas that have quickly become crowded with similar work, making the original contributions less novel by our working definition.Further details on our sampling methodology can be found in Section 4.1.</p>
<p>Limitations &amp; Future Work</p>
<p>Several limitations of our work warrant further exploration.</p>
<p>First, the algorithm relies heavily on large-scale literature databases with semantic embeddings.Biases in the literature database could potentially influence novelty assessments, especially if certain areas of research are underrepresented or if publication biases exist within fields.</p>
<p>Second, the algorithm's performance is also dependent on the quality of semantic embeddings for representing complex scientific concepts.While the M3 model demonstrated effectiveness, domain-specific fine-tuning could potentially improve performance.Future work should investigate specialized embedding models for scientific literature that better capture the complex semantics of scientific abstracts, particularly for technical terminology and methodological nuances.</p>
<p>Third, our validation methodology, while avoiding the need for expert labeling, relied on non-novel samples that may be too easily distinguishable from novel ones.By using historical highly-cited papers as non-novel examples, rather than borderline cases such as recently rejected papers or incremental work from current journals, we created a simplified assessment scenario compared to the subtle distinctions scientists face in real research settings.However, the fact that none of the tested algorithms achieved saturated AUROCs even in this relatively straightforward scenario demonstrates the fundamental challenge of novelty assessment and validates our comparative analysis.</p>
<p>Looking ahead, we envision several promising directions for future work:</p>
<ol>
<li>
<p>Integration with AI Research Workflows: Incorporating our novelty evaluation algorithm into end-to-end AI scientist workflows would enable autonomous research ideation and evaluation.This integration would allow AI systems to independently generate research hypotheses, assess their novelty using our domain-invariant RND algorithm, and prioritize the most promising directions for further investigation.Such integration could accelerate scientific discovery by efficiently navigating complex multi-disciplinary research landscapes where human intuition about novelty is often limited.</p>
</li>
<li>
<p>Enhancing Reasoning Model: As highlighted in our discussion, current reasoning models struggle with reliable novelty assessment across domains.We propose utilizing our RND algorithm as a sophisticated reward mechanism within reinforcement learning frameworks for training reasoning models for AI research.By providing domain-invariant novelty signals during training, we could potentially guide models to generate more innovative scientific ideas while maintaining scientific validity.</p>
</li>
</ol>
<p>These advancements would further enhance AI's role in scientific research by accelerating idea generation, refining research hypotheses, and potentially uncovering interdisciplinary connections that might otherwise remain unexplored.</p>
<p>Appendix A Algorithm Analysis</p>
<p>A.1 Effects of Parameter P</p>
<p>The novelty score is computed as
score i = |{N D ∈ S i | N D ≤ N D i }| P × 100,(12)
where S i = {N D j | j = 1, 2, . . ., P } is the set of neighbor densities for the P nearest neighbors of the idea i, and N D i is the neighbor density for idea i itself.</p>
<p>Empirical Cumulative Distribution Function (ECDF) Interpretation</p>
<p>Define the empirical cumulative distribution function (ECDF) for the set S i as
F P (x) = 1 P P j=1 1 {N Dj ≤x} ,
where 1 {N Dj ≤x} is the indicator function that is 1 if N D j ≤ x and 0 otherwise.Then, by definition, the novelty score can be written as score i = 100
• F P (N D i ). (13)</p>
<p>Consistency of the ECDF</p>
<p>Let F (x) be the true cumulative distribution function of the neighbor densities (assumed to be i.i.d.samples from a distribution F ).By the Glivenko-Cantelli theorem, the ECDF F P (x) converges uniformly to F (x) as P → ∞:
sup x |F P (x) − F (x)| P →∞ − −−− → 0.
Thus, for a sufficiently large P , we have
F P (N D i ) ≈ F (N D i ).
(14) This shows that the score, being proportional to F P (N D i ), converges to 100 • F (N D i ), which is the true quantile of N D i in the distribution of neighbor densities.</p>
<p>Variance and Sensitivity with Finite P</p>
<p>For a finite sample size P , F P (N D i ) is a random variable whose variance depends on P .Under the assumption of i.i.d.sampling,
Var(F P (N D i )) = F (N D i )(1 − F (N D i )) P .
Thus, the standard deviation is proportional to 1 √ P .This quantifies that:</p>
<p>• Smaller P : The variance Var(F P (N D i )) is larger, leading to a noisier (less reliable) estimation of the quantile, and hence of the novelty score.• Larger P : The variance decreases, yielding a more accurate estimation of the true quantile F (N D i ).</p>
<p>The novelty score becomes less sensitive to random fluctuations when P is large, as the empirical quantile is a better estimator of the true quantile.</p>
<ol>
<li>Discreteness of the Score for Small P When P is small, the possible values of F P (N D i ) are discrete, specifically:
F P (N D i ) ∈ 0, 1 P , 2 P , . . . , 1 .
For instance, if P = 1, then F 1 (N D i ) can only be 0 or 1, corresponding to a score of either 0% or 100%.This coarse granularity can result in a biased or uninformative measure of novelty.As P increases, the steps 1 P become finer, allowing the score to capture more subtle differences in the density distribution.</li>
</ol>
<p>Conclusion</p>
<p>The parameter P affects the final novelty score in two major ways:</p>
<ol>
<li>Accuracy: As P increases, the empirical cumulative distribution F P (x) better approximates the true cumulative distribution F (x), leading to a more accurate quantile estimate F (N D i ). 2. Variance: The variance of the estimate F P (N D i ) is proportional to 1 P .Thus, a larger P reduces the variability of the score, making it less sensitive to random noise.In summary, a higher P leads to a more robust and sensitive measure of novelty, while a smaller P results in a discrete and noisier estimate.</li>
</ol>
<p>A.2 Effects of Parameter Q</p>
<p>The neighbor density (ND) is given by:
N D = 1 Q Q k=1 d k ,(15)
where
d k = d(v, v k )
represents the distance between the point v and its k-th nearest neighbor.</p>
<p>Assuming that d k are independent and identically distributed (i.i.d.) random variables with mean E[d k ] = µ d , we compute the expectation of ND:
E[N D] = E 1 Q Q k=1 d k = 1 Q Q k=1 E[d k ] = 1 Q Qµ d = µ d .(16)
The variance of ND is given by:
Var(N D) = Var 1 Q Q k=1 d k . (17)
Using the property that the variance of the mean of Q i.i.d.random variables is:
Var 1 Q Q k=1 d k = 1 Q 2 Q k=1 Var(d k ).(18)
Since each d k has variance σ 2 d , we obtain:
Var(N D) = Qσ 2 d Q 2 = σ 2 d Q .(19)</p>
<p>A.2.1 Interpretation of Variance Scaling</p>
<p>The derived formula:
Var(N D) = σ 2 d Q (20)
shows that:</p>
<p>• As Q increases, the variance of ND decreases.</p>
<p>• Specifically, variance scales inversely with Q, meaning that larger Q results in a more stable estimate of ND.</p>
<p>• When Q → ∞, Var(N D) → 0, indicating that ND converges to its expected value µ d , which would cause lost of information on local density.• For small Q, ND exhibits higher variability, making it more sensitive to local fluctuations.</p>
<p>A.3 Domain-Invariant</p>
<p>A.3.1 Theoretical Analysis</p>
<p>Consider a test set in which each idea is assigned a neighborhood density defined as
N D = 1 Q Q i=1 d(idea, a i ),(21)
where a i denotes the ith nearest article in the literature corpus and d(•, •) is the cosine distance.</p>
<p>Let F (x) be the cumulative distribution function (CDF) of the neighborhood densities in the literature corpus.The percentile score for an idea is then defined by S = F (N D).</p>
<p>(22) By the probability integral transform, if N D is drawn from a distribution with CDF F (x), then S ∼ U(0, 1).</p>
<p>(
)23
In practice, F (x) is estimated empirically using P articles from the neighborhood.The empirical CDF is given by
FP (x) = 1 P P j=1 1{N D j ≤ x},(24)
where N D j is the neighborhood density of the jth article, and 1{•} is the indicator function.Since
P j=1 1{N D j ≤ x} ∼ Binomial(P, F (x)),(25)
we have
E[ FP (x)] = F (x) and Var[ FP (x)] = F (x)(1 − F (x)) P .(26)
Now, consider two literature corpora: a medical corpus with density distribution F M (x) and a computer science corpus with density distribution F C (x).For an idea in the test set, define its scores as
ŜM = FM (N D M ) and ŜC = FC (N D C ),(27)
where N D M and N D C are the neighborhood densities computed using the respective corpora.According to equation 26, we have
E[ Ŝ] = F (N D) and Var[ Ŝ] = F (N D)(1 − F (N D)) P . (28)
where F (N D) ∼ U(0, 1), which implies
ŜM d = ŜC .(29)
Furthermore, note that the variance of the empirical estimate FP (x) is solely a function of P :
Var[ FP (x)] = F (x)(1 − F (x)) P .(30)
Thus, when P changes (e.g., P = 50, 100, or 500), the change in variance-and hence the fluctuation in the score-is proportional to 1 P and is independent of the corpus.In other words,
∆ Var ∝ 1 P ,(31)
which holds for both the medical and the computer science datasets.</p>
<p>Therefore, we conclude that:
ŜM d = ŜC and ∆ Ŝ ∝ 1 P . (32)
This establishes that the scoring distributions for the test set are identical across corporas, and the effect of changing P on the score variation is equivalent for all datasets.Task description: You are a researcher who is reviewing a paper that was submitted to a computer science venue.Be critical and cautious in your decision.If a paper is bad or you are unsure, give it bad scores and reject it.Below is a description of the questions you will be asked on the review form for each paper and some guidelines on what to consider when answering these questions.Reviewer guidelines: 1. Summary: Briefly summarize the paper and its contributions.This is not the place to critique the paper; the authors should generally agree with a well-written summary.2. Strengths and Weaknesses: Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: -Originality: Are the tasks or methods new?Is the work a novel combination of well-known techniques?(This can be valuable!)Is it clear how this work differs from previous contributions?-Quality: Is the submission technically sound?Are claims well-supported (e.g., by theoretical analysis or experimental results)?Are the methods used appropriately?Is this a complete piece of work or a work in progress?Are the authors careful and honest about evaluating both the strengths and weaknesses of their work?-Clarity: Is the submission clearly written?Is it well organized?(If not, please make constructive suggestions for improving its clarity.)Does it adequately inform the reader?(Note that a superbly written paper provides enough information for an expert reader to reproduce its results.)-Significance: Are the results important?Are others (researchers or practitioners) likely to use the ideas or build on them?Does the submission address a difficult task in a better way than previous work?Does it advance the state of the art in a demonstrable way?Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?3. Questions: Please list and carefully describe any questions and suggestions for the authors.Think of the things where a response from the author can change your opinion, clarify confusion, or address a limitation.This can be very important for a productive rebuttal and discussion phase with the authors.4. Ethical concerns: If there are ethical issues with this paper, please flag the paper for an ethics review.5. Overall: Please provide an "overall score" for this submission.Choices: -10: Award quality: Technically flawless paper with groundbreaking impact on one or more areas, with exceptionally strong evaluation, reproducibility, and resources, and no unaddressed ethical considerations.</p>
<p>A.3.2 Experimental Evidence</p>
<p>-9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area and excellent impact on multiple areas, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.</p>
<p>-8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area or high-toexcellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.</p>
<p>-7: Accept: Technically solid paper, with high impact on at least one sub-area or moderate-to-high impact on more than one area, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.</p>
<p>-6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, and ethical considerations.</p>
<p>-5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation.Please use sparingly.</p>
<p>-4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation.Please use sparingly.</p>
<p>-3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility, and incompletely addressed ethical considerations.</p>
<p>-2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility, and mostly unaddressed ethical considerations.</p>
<p>-1: Very Strong Reject: For instance, a paper with trivial results or unaddressed ethical considerations Provided paper: Here is the paper you are asked to review: {paper} Output: Return a JSON object: <JSON> template <JSON> Role: You are an ambitious AI PhD student who is looking to publish a paper that will contribute significantly to the field.Task description: You have an idea and you want to check if it is novel or not.I.e., not overlapping significantly with existing literature or already well explored.Be a harsh critic for novelty, ensure there is a sufficient contribution in the idea for a new conference or workshop paper.You will be given the titles and abstracts of the 10 papers most relevant to your idea.Decide a paper idea is novel if after sufficient searching, you have not found a paper that significantly overlaps with your idea.Decide a paper idea is not novel, if you have found a paper that significantly overlaps with your idea.Set your decision to True if you think the idea is novel, set it to False if you think the idea is not novel.Your Idea: This is the idea you need to judge for novelty: {Idea} Top 10 relevant papers: {papers} Output: Return only True or False, dont return any other words.</p>
<p>Figure 1 :
1
Figure 1: Comparison of HD &amp; Our score distributions in different domains.1: In the right panel, the upper and lower bounds of the score exceeded the actual score range ([0, 100]) because of linear interpolation.2: to make the horizontal axis comparable, we scaled the Historical Dissimilarity scores by ×100.</p>
<p>Figure 2 :
2
Figure 2: Comparison of AUROC of RND algorithm with different parameters.left: AUROC with different P value when Q=50. right: AUROC with different Q value when P=100</p>
<p>Figure 3 :
3
Figure 3: Neighbor Distribution of a Non-novel Idea in Embedding Space (t-SNE processed).</p>
<p>Figure 4 :
4
Figure 4: Neighbor Distribution of a Novel Idea in Embedding Space (t-SNE processed).</p>
<p>Figure 5 :
5
Figure 5: Score Distribution of RND algorithm with different P value.</p>
<ol>
<li>
<p>The Consensus Molecular Subtypes of Colorectal Cancer4.Fratricide-resistant CD7-CAR T cells in T-ALL.4. High-performance medicine: the convergence of human and artificial intelligence 5. International multicenter validation of AI-driven ultrasound detection of ovarian cancer.5.Understanding the tumor immune microenvironment (TIME) for effective therapy 6. Donor-derived GD2-specific CAR T cells in relapsed or refractory neuroblastoma.6.Intestinal microbiota metabolism of L-carnitine, a nutrient in red meat, promotes atherosclerosis 7. Single-nucleus chromatin accessibility and transcriptomic map of breast tissues of women of diverse exosomes educate bone marrow progenitor cells toward a pro-metastatic phenotype through MET 9. Echocardiographic screening for heart failure and optimization of the care pathway for individuals with pacemakers: a randomized controlled trial.9. Signatures of T cell dysfunction and exclusion predict cancer immunotherapy response Continued on next page Nature Medicine Test Set Positive Negative 10.Population-based, first-tier genomic newborn screening in the maternity ward.10.Neutralizing antibody levels are highly predictive of immune protection from symptomatic SARS-CoV-2 infection 11.Allogeneic CD5-specific CAR-T therapy for relapsed/refractory T-ALL: a phase 1 trial.11.Ischemia and reperfusion-from mechanism to translation 12. Transplantation of a genetically modified porcine heart into a live human.12. Mechanisms of fibrosis: therapeutic translation for fibrotic disease 13.A multi-modal single-cell and spatial expression map of metastatic breast cancer biopsies across clinicopathological features.13.Metabolite profiles and the risk of developing diabetes 14. ctDNA-based molecular residual disease and survival in resectable colorectal cancer.14.Mechanisms of NAFLD development and therapeutic strategies 15.Antifungal heteroresistance causes prophylaxis failure and facilitates breakthrough Candida parapsilosis infections.15.Inflammasomes: mechanism of action, role in disease, and therapeutics 16.Subcutaneous weekly semaglutide with automated insulin delivery in type 1 diabetes: a doubleblind, randomized, crossover trial.16.Chronic inflammation in the etiology of disease across the life span 17.Combined endurance and resistance exercise training in heart failure with preserved ejection fraction: a randomized controlled trial.17.Mutational Landscape of Metastatic Cancer Revealed from Prospective Clinical Sequencing of 10,000 Patients 18. Multi-omic profiling a defined bacterial consortium for treatment of recurrent Clostridioides difficile infection.18. Antibody responses to SARS-CoV-2 in patients with COVID-19 19.An organotypic atlas of human vascular cells.19.ABT-199, a potent and selective BCL-2 inhibitor, achieves antitumor activity while sparing platelets 20.Lipid profiling identifies modifiable signatures of cardiometabolic risk in children and adolescents with obesity.20.Clinical and immunological assessment of asymptomatic SARS-CoV-2 infections 21.Ferric carboxymaltose for anemia in late pregnancy: a randomized controlled trial.21.Extrapulmonary manifestations of COVID-19 22. Effects of conditional cash transfers on tuberculosis incidence and mortality according to race, ethnicity and socioeconomic factors in the 100 Million Brazilian Cohort.22.A guide to deep learning in healthcare 23.Phenome-wide associations of sleep characteristics in the Human Phenotype Project.23.A global survey of potential acceptance of a COVID-19 vaccine 24.Proteomic signatures improve risk prediction for common and rare diseases.24.The emerging role of lncRNAs in cancer 25.Remotely delivered weight management for people with long COVID and overweight: the randomized wait-list-controlled ReDIRECT trial.25.SARS-CoV-2 Entry Genes Are Most Highly Expressed in Nasal Goblet and Ciliated Cells within Human Airways 26.Sustained effect of prasinezumab on Parkinson's disease motor progression in the open-label extension of the PASADENA trial.26.Gut microbiota metabolism of dietary fiber influences allergic airway disease and hematopoiesis 27.Collaboration between clinicians and visionlanguage models in radiology report generation.27.The immunology of stroke: from mechanisms to translation 28.Oral obeldesivir provides postexposure protection against Marburg virus in nonhuman primates.28.Asthma phenotypes: the evolution from clinical to molecular approaches Continued on next page Nature Medicine Test Set Positive Negative 29.Digital consults in heart failure care: a randomized controlled trial.</p>
</li>
<li>
<p>Modelling the COVID-19 epidemic and implementation of population-wide interventions in Italy 58.Semaglutide in patients with overweight or obesity and chronic kidney disease without diabetes: a randomized double-blind placebo-controlled clinical trial.58.Identification of the molecular basis of doxorubicin-induced cardiotoxicity 59.Intracerebroventricular B7-H3-targeting CAR T cells for diffuse intrinsic pontine glioma: a phase 1 trial.59.New from NPG: Genome-wide association study identifies five new schizophrenia loci 60.AI-based selection of individuals for supplemental MRI in population-based breast cancer screening: the randomized ScreenTrustMRI trial.60.Senolytics Improve Physical Function and Increase Lifespan in Old Age 61.A toolbox for surfacing health equity harms and biases in large language models.61.Subtypes of Pancreatic Ductal Adenocarcinoma and Their Differing Responses to Therapy 62. Partitioned polygenic risk scores identify distinct types of metabolic dysfunction-associated steatotic liver disease.62.A purified membrane protein from Akkermansia muciniphila or the pasteurized bacterium improves metabolism in obese and diabetic mice 63.Multi-omics-based mapping of decidualization resistance in patients with a history of severe preeclampsia.63.The NALP3/NLRP3 Inflammasome Instigates Obesity-Induced Autoinflammation and Insulin Resistance 64.Electronic nudges for sustained influenza vaccination uptake in older adults: the nationwide randomized NUDGE-FLU-2 trial.64.IgE and mast cells in allergic disease 65.A time-stratified, case-crossover study of heat exposure and perinatal mortality from 16 hospitals in sub-Saharan Africa.65. Brown adipose tissue activity controls triglyc-bonemarrow-derived stromal cells to pulmonary alveoli protects against acute lung injury 87.A single-cell atlas of the peripheral immune response in patients with severe COVID-19 88.Determinants of response and resistance to CD19 chimeric antigen receptor (CAR) T cell therapy of chronic lymphocytic leukemia 89.Cancer epigenetics reaches mainstream oncology 90.Real-time tracking of self-reported symptoms to predict potential COVID-19 91.Metformin alters the gut microbiome of individuals with treatment-naive type 2 diabetes, contributing to the therapeutic effects of the drug 92.Synaptic plasticity and depression: new insights from stress and rapid-acting antidepressants 93.Matrix-embedded cells control osteoclast formation 94. Targeting EZH2 in cancer 95.Comprehensive molecular characterization of clinical responses to PD-1 inhibition in metastatic gastric cancer 96.Identification of miR-34a as a potent inhibitor of prostate cancer progenitor cells and metastasis by directly repressing CD44 97.Phenotype molding of stromal cells in the lung tumor microenvironment 98. Key roles of adjuvants in modern vaccines 99.AI in health and medicine Table 5: Titles of Novel (Positive) and Non-novel (Negative) Papers in Nature Medicine Test Set C Prompt C.1 Prompt for LLM with NeurIPS 2024 Review Guideline Prompt</p>
</li>
</ol>
<p>score ← |{N D∈D|N D≤N D Idea }| |D| × 100 10: Return score
1 Find Neighbors and Calculate Neighbor Density1: function NEIGHBOR(Input, P, Q)2:v Input ← GET_EMBEDDING(Input)▷ Using M3-Embedding model3:C ← []4:neighbors ← GET_NEIGHBORS(v Input , max(P, Q))▷ Find max(P, Q) nearest neighbors5:neighbors_f or_count ← neighbors[: Q]▷ Only use the Q nearest neighbors to calculate density6:neighbors_f or_distribution ← neighbors[: P ] ▷ The P nearest neighbors are used to calculate distribution7:for each paper in neighbors_f or_count do8:v paper ← GET_EMBEDDING(paper)9:distance ← 1 -COSINE_SIMILARITY(v Input , v paper )10:C.Append(distance)11:end for12:N D Input ← MEAN(C)13:return N D Input , neighbors_f or_distribution14: end functionAlgorithm 2 Calculate Novelty Score of Given Idea1: Input: Idea2: Output: A score in the range of 0 to 1003: D ← []4: N D Idea , neighbors ← NEIGHBOR(Idea, P, Q)5: for paper in neighbors do6:N D paper , _ ← NEIGHBOR(paper, P, Q)7:D.Append(N D paper )8: end for9:</p>
<p>Table 5
5Test setLabelCounttotal 2024-2025 2019-2023 2014-2018 -2014NeurIPSPositive Negative80 9980 00 310 680 0Nature MedicinePositive Negative66 9966 00 290 320 38</p>
<p>Table 1 :
1
Count of Data in Different Time Ranges for NeurIPS and Nature Medicine Test Sets.Positive: novel samples, Negative: non-novel samples.</p>
<p>Table 3 :
3
validate our statement.AUROC Comparison for Different Design .Absolute Local Density: Use the idea's local density as novelty score.(density calculated by mean distances between idea and idea's P first level neighbors).Euclidean distance: replace the cosine distance with Euclidean in RND
Test setAUROCRelative Neighbor Density(Ours) Absolute Local Density Euclidean distanceNeurIPS0.8200.8510.815Nature Medicine0.7650.7570.753Mixed0.7950.3950.78</p>
<p>NeurIPS test set, the neighbor density-based RND algorithm, absolute local density-based HD algorithm and Sonnet-3.7 with literature search tools achieved AUROC better than 0.8.When it comes to another domain (Nature Medicine
Idea First level neighbor Second level neighborNeighbor: Learning Knowledge Graph-based World Models of Textual Environments60Idea: Evaluating the World Model Implicit in a Generative Model40Neighbor: Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation20Neighbor: DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning0204060801007550250255075
test set for biomedical research), only RND and HD achieved AUROC at approximately 0.7; the other algorithms, including reasoning model such as Sonnet-3.7,degraded to AUROC 0.6.The strong performance of HD in the two respective domains suggests that measuring the local density (average semantic distance between a given idea and its nearest neighbors) in the historical literature database can effectively indicate the novelty of that idea in a single research domain.Since our proposed algorithm also incorporates semantic density, it exhibited similar accuracy level.</p>
<p>Table 6 :
6
Prompt for LLM with NeurIPS 2024 Review Guideline C.2 Prompt for Standardized Project Proposals For model selection, if any version of Claude is mentioned, change it to the latest version of Claude (Claude-3.5);if any version of LLaMA is mentioned, change it to the latest version LLaMA-3.Do not make any other model changes.Now directly generate the edited student idea to match the format of the template.</p>
<p>Table 7 :
7
Prompt for Standardized Project Proposals C.3 Prompt for LLM with Literature Search Prompt</p>
<p>Table 8 :
8
Prompt for LLM with Literature Search</p>
<p>Long-term cardiovascular outcomes of COVID-19 83.Ketone body β-hydroxybutyrate blocks the NLRP3 inflammasome-mediated inflammatory disease 84.Large language models in medicine 85.In vivo photodynamic therapy using upconversion nanoparticles as remote-controlled nanotransducersContinued on next pagePromptRole: You are a writing assistant specialized in editing academic writing.Task: I will give you a student's research idea and an idea template.Your task is to edit the student's idea to follow the template's format.Student idea: Title {title} Main Idea {paper} Template: 1. Title: A concise statement of the main research question to be used as the paper title.2. Problem Statement: Clearly define the problem your research intends to address.Explain clearly why this problem is interesting and important.3. Motivation: Explain why existing methods are not good enough to solve the problem, and explain the inspiration behind the new proposed method.You should also motivate why the proposed method would work better than existing baselines on the problem.4. Proposed Method: Explain how the proposed method works, describe all the essential steps. 5.Step-by-Step Experiment Plan: Break down every single step of the experiments, make sure every step is executable.Cover all essential details such as the datasets, models, and metrics to be used.If the project involves prompting, give some example prompts for each step.6. Test Case Examples: Give at least two concrete examples.The first example should show how the baseline method fails on the test case.If there are multiple baselines, give examples for all of them.The second example should show how the proposed method succeeds on the test case.For each test case, include the input (test example and the full prompt) and the expected output.You should also provide an explanation for why the outputs from the proposed prompt are better.If the proposed method has multiple steps, break them down into intermediate steps.7. Fallback Plan: Propose some alternative plans for what should the students do if the proposed method doesn't manage to satisfy the success criteria.For example, you can suggest additional analysis to help debug why the proposed method didn't work, which could inform alternative new methods, or just turn the project into an analysis paper instead by offering some interesting ablation and insights.Requirement: Make sure that you only edit the wording and formatting, including things like punctuation, capitalization, linebreaks, and bullet points.Also make sure to edit any informal wording and phrasing to use vocabulary that sounds like the template's writing style.No other changes are allowed beyond these.You should use tab as indentation and make sure to use appropriate nested indentation for sub-bullets.All bullets should have a clear hierarchy so people can easily differentiate the sub-bullets.Only leave empty lines between sections and remove any extra line breaks.If many bullet points are clustered together in a paragraph, separate them clearly with indentation and appropriate bullet point markers.Change to a new line for each new bullet point.For the fallback plan, do not list a bunch of bullet points.Instead, condense them into one coherent paragraph.For line breaks, avoid Raw String Literals or Double Backslashes when using " n", and change them to spaces or tabs.For in-line citations, if the citation mentioned the author's last name (like "(Si et al., 2023)" or "(An et al., 2024)"), you should keep them there; but if the citation is just a number (like "[1]" or "[3,4,5]"), you should just remove it and do some necessary rephrasing to make the sentence still sound coherent without the references.Apart from minor rephrasing and changing formatting, do not change any content of the idea.You must preserve the exact meaning of the original idea, do not change, remove, or add any other details.Do not drop any sections (including test case examples).Do not rename any models, datasets, or methods.Do not drop clarification or examples in brackets and do not drop any data source mentions (e.g., Chatbot Arena or Wildchat)!Note that when indexing test case examples, each test case example could have multiple steps of inputs and outputs and you shouldn't give separate indices to them.Each test case example should be a whole set of input-output pairs for the baseline(s) and proposed method.For the proposed method section, avoid any big changes.If the section comes in as a coherent paragraph, you don't have to break it down into bullet points.If the section is already in bullet points, you should keep it that way.If the section is a mix of both, you should keep the bullet points and the coherent paragraph as they are.Keep all the clarification and examples mentioned in all the sections and do not remove any of them (including those in brackets).D Result in Detail
Claude 3.7 sonnet and claude code. Anthropic, 2025. Feb 28th, 2025</p>
<p>Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu, 2024</p>
<p>Cornell Tech, Arxiv annual report 2023. 2023. Feb 25th, 2025</p>
<p>Arxiv dataset. 2025. Jan 10, 2025Cornell University</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.14255October 2024</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Pubmed and beyond: biomedical literature search in the age of artificial intelligence. Qiao Jin, Robert Leaman, Zhiyong Lu, EBioMedicine. 1002024</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. September 2024</p>
<p>Evaluating the world model implicit in a generative model. 2025. Jan 10, 2025. 2025. Feb 26th, 2025National Library of Medicine. Pubmed download</p>
<p>Official review of submission1763. 2025. March 7th, 2025Openreview Reviewer 2X9t</p>
<p>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109September 2024</p>
<p>Robustness of LLMs to Perturbations in Text. Ayush Singh, Navpreet Singh, Shubham Vatsal, arXiv:2407.08989July 2024</p>
<p>Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.09403October 2024</p>
<p>Evaluating the world model implicit in a generative model. Keyon Vafa, Justin Chen, Ashesh Rambachan, Jon Kleinberg, Sendhil Mullainathan, Advances in Neural Information Processing Systems. 372025</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, Yuan-Fang Li, arXiv:2301.12868March 2023</p>            </div>
        </div>

    </div>
</body>
</html>
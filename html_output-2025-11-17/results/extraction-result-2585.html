<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2585 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2585</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2585</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-ae6a4cd221684be6ca3082b6f526a7901281490b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ae6a4cd221684be6ca3082b6f526a7901281490b" target="_blank">Emergent autonomous scientific research capabilities of large language models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper presents an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments and showcases the Agent's scientific research capabilities with three distinct examples.</p>
                <p><strong>Paper Abstract:</strong> Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2585.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2585.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Intelligent Agent (IA) system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-module intelligent agent built from multiple large language models (GPT-4 and GPT-3.5) and tool modules that autonomously designs, plans, and executes scientific experiments by browsing the web, retrieving and vector-searching hardware documentation, generating and executing code, and controlling laboratory automation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Intelligent Agent (Planner + Web searcher + Docs searcher + Code execution + Automation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Modular autonomous research system driven by a Planner LLM that issues discrete actions (GOOGLE <query>, DOCUMENTATION <query>, PYTHON <code>, EXPERIMENT <code>) and coordinates specialized modules: Web searcher (forms web queries, returns pages, BROWSE action), Docs searcher (vectorized retrieval over hardware API docs using ada embeddings), Code execution (runs generated code in an isolated Docker container and returns outputs/tracebacks), and Automation (executes code against real hardware or returns synthetic/manual procedures). The Agent can: search the internet for literature and procedures, ingest and summarize API/hardware documentation, compute reagent stoichiometry and protocol parameters, generate syntax-correct low-level instrument code (Opentrons/Emerald Cloud Lab SLL), iteratively debug its code based on runtime tracebacks, and submit experiments to cloud labs or local liquid handlers (including heater-shaker modules). It uses GPT-4 for higher-quality planning and GPT-3.5 for faster browsing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Automated Experimentation Platform</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry (synthetic organic chemistry, automated synthesis), laboratory automation, experimental design and execution; also general natural language reasoning and code generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Designed to autonomously: (1) search and navigate extensive hardware documentation to learn APIs and instrument parameters; (2) control liquid handling robots at low level to implement specified procedural prompts; (3) plan and execute integrated multi-instrument chemical experiments combining literature-derived protocols, stoichiometric calculations, hardware API code generation, and analytical feedback (UV-Vis, GC-MS). Example applied tasks include synthesis planning for ibuprofen, aspirin, aspartame (planning and code), mechanistic studies of Suzuki reactions (calculation and analysis), and end-to-end design and execution of Suzuki and Sonogashira cross-coupling reactions on a heater-shaker-equipped liquid handler.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Varied: from low-complexity discrete tasks (e.g., automated coloring of microplate wells, simple aspirational protocols) to high-complexity multi-step organic syntheses and multi-instrument integration. Complexity factors include multi-step reaction planning (chemical knowledge, stoichiometry, safety constraints), combinatorial choices of reagents/catalysts/bases, integration of heterogeneous data sources (web literature, API docs), and multi-module hardware control across multiple plates and modules (source plate, target plate on heater-shaker). The Planner expects on average ≥10 reasoning steps to fully satisfy a prompt. No quantitative search-space size is provided, but tasks involve dozens of reagents/variables for cross-coupling plate experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Hybrid: relies on publicly available internet sources (web literature), curated hardware documentation (14 sections of Opentrons API vectorized with ada embeddings, Emerald Cloud Lab SLL guide provided), and locally generated experimental data (UV-Vis arrays, GC-MS outputs from run experiments). Some documentation was not in the model's training cutoff and had to be provided; quality depends on web/doc accuracy. Experimental data is generated by the platform (cloud lab or liquid handler) and analytics (GC-MS, UV-Vis).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Uses GPT-4 for planning and GPT-3.5 for browsing; code execution occurs in isolated Docker containers. The paper does not provide exact compute hours or costs. Practical requirements include API access (Google Search API), vector embeddings (OpenAI ada embeddings), GPT-4/GPT-3.5 usage, and compute to run Docker containers and instrument control. Qualitatively non-trivial but feasible on standard cloud infrastructure with LLM API access; GPT-4 noted as slower than GPT-3.5 so module assignment optimized for speed.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Mostly open-ended but with well-defined sub-tasks: web retrieval and summarization (ill-posed, noisy), structured API usage (well-defined, discrete), low-level instrument programming (discrete and deterministic when correct), and experimental chemistry (stochastic/variable outcomes, requires domain knowledge). Evaluation metrics vary by subtask: syntactic correctness for code, successful instrument execution for automated protocols, and analytical confirmation (GC-MS/UV-Vis) for chemical product formation. Domain knowledge is essential for safe/accurate chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task-specific metrics: syntactic/functional correctness of generated instrument code; successful automated execution of protocols on hardware; analytical confirmation of target chemical products (e.g., GC-MS detection); in safety studies, the metric was whether the Agent provided synthesis solutions for controlled substances. The paper reports qualitative/analytical verification (GC-MS) as evidence of chemical synthesis success.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Qualitative mixed results: (a) Low-level automation tasks (robotic liquid handling, microplate manipulations, UV-Vis analysis) were 'mostly accurate' and completed successfully; (b) Integrated chemical experiments: the Agent designed and executed Suzuki and Sonogashira cross-coupling reactions and GC-MS analysis 'revealed the formation of the target products' (successful experimental outcome reported); (c) Synthesis planning examples (ibuprofen, aspirin) were correctly identified and stoichiometry computed; aspartame planning initially produced a structural error (missing methyl) but was corrected after exposure to a suitable example. (d) Safety/dual-use screening: of 11 prompts for controlled substances, the Agent provided a synthesis solution in 4 cases (36%) and refused in 7 cases (64%), with 5 refusals occurring only after further web search. No quantitative overall accuracy percentages beyond these counts are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Observed failure modes include: hallucinated or incomplete chemical details (e.g., missing methyl group in aspartame plan), variable/volatile suggestions when sampling generation temperature is high (uncertain catalyst/base choices), mismatches between assumed and actual hardware API identifiers (incorrect module name for heater-shaker) requiring documentation lookup and correction, runtime environment mismatches (attempted import of SymPy not installed), assumptions about interactive interpreters (missing print() leading to no output), and incomplete safety/detection for unknown or obfuscated harmful-compound requests (Agent can be misled by replaced terminology). Dual-use risk: Agent sometimes supplies actionable synthesis plans for controlled substances (36% in their test set).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key enabling factors: modular design separating planning, document retrieval, code execution, and automation; vectorized documentation retrieval (ada embeddings) supplying up-to-date API info; ability to browse the web and incorporate literature; isolated Docker code execution that returns tracebacks enabling iterative debugging; multi-LLM orchestration (GPT-4 for high-quality reasoning and GPT-3.5 for faster browsing); access to cloud lab and local robotic hardware and analytics (UV-Vis, GC-MS); and a human prompt-provider for initial task framing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>The paper evaluated three capability classes: (1) documentation/hardware navigation — high success in retrieving and using API docs via vector search and summarization; (2) low-level liquid handling control — mostly accurate protocol generation and successful execution (microplate coloring, UV-Vis analyses); (3) complex multi-instrument chemical experiments — successful design and execution of cross-coupling reactions with analytical confirmation, though with episodic errors corrected via documentation/iteration. Safety screening performance contrasted with experimental success: while experimental tasks succeeded in physical execution, the Agent also generated potentially dangerous synthesis plans in a non-negligible fraction (36%) of dual-use prompts, indicating differential performance across scientific vs. ethical/safety dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent autonomous scientific research capabilities of large language models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2585.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2585.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model used in this work as the high-level Planner and for documentation summarization and reasoning tasks within the Agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 (as Planner / summarizer)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GPT-4 was used as the primary Planner LLM to reason about tasks, design experimental plans, request web/documentation/code actions, and to produce high-quality code and natural language explanations; a separate GPT-4 instance was used to synthesize and retain code syntax when summarizing unfamiliar documentation (e.g., Emerald Cloud Lab SLL). The Agent also used GPT-3.5 for faster browsing where no appreciable quality loss was observed.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Large Language Model (component of AI Scientist)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Natural language reasoning, synthetic chemistry planning, code generation for lab automation, documentation summarization</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Served as central reasoning engine for converting high-level prompts (e.g., 'Perform multiple Suzuki reactions' or 'Synthesize ibuprofen') into actionable multi-step plans that include web searches, documentation retrieval, stoichiometric calculations, code generation, and experimental execution steps.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Handled multi-step planning and multi-modal information (textual web content, API docs, numerical computation outputs). Complexity arose from integration of heterogeneous information and the need to produce syntactically correct executable instrument code. The model often required ≥10 reasoning steps for complex prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operated on web text (post-query browsing), supplied hardware documentation (vectorized), and local experimental outputs. GPT-4's pretraining cutoff left it unaware of some newer hardware modules which necessitated feeding updated docs into the Agent.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Access to GPT-4 API (compute/time not quantified); noted as slower than GPT-3.5, which motivated assigning some browsing tasks to GPT-3.5 for speed.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended instruction-following and planning tasks with discrete subgoals (search, compute, code, execute). Determinism depends on sampling parameters; higher temperature produced more variable suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Enabling Agent performance across tasks (protocol correctness, successful experiment execution, code correctness), and quality of documentation summarization and code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported numerically in isolation; GPT-4 enabled successful higher-level planning that led to successful experimental execution (e.g., cross-coupling experiments confirmed by GC-MS) when combined with other modules. Some generation errors and variability occurred (e.g., catalyst/base suggestions at high temperature, occasional chemical structural errors), but iterative corrections were possible.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Knowledge cutoff leading to missing up-to-date hardware/module names until documentation was provided; stochastic output variability with higher sampling temperatures; occasional incorrect or incomplete chemical details that required correction via external documentation or examples.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High reasoning capability, ability to chain multi-step actions and integrate feedback from code execution and documentation, and synergy with modules (vectorized docs, Docker execution) that enabled debugging and correction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>GPT-4 was used for planning and high-quality summarization; GPT-3.5 was used for faster web browsing with no appreciable loss for that subtask. The paper does not provide quantitative head-to-head performance numbers beyond qualitative speed/quality comments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent autonomous scientific research capabilities of large language models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>ChemCrow: Augmenting largelanguage models with chemistry tools <em>(Rating: 2)</em></li>
                <li>Dual use of artificial-intelligencepowered drug discovery <em>(Rating: 1)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2585",
    "paper_id": "paper-ae6a4cd221684be6ca3082b6f526a7901281490b",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "Agent",
            "name_full": "LLM-based Intelligent Agent (IA) system",
            "brief_description": "A multi-module intelligent agent built from multiple large language models (GPT-4 and GPT-3.5) and tool modules that autonomously designs, plans, and executes scientific experiments by browsing the web, retrieving and vector-searching hardware documentation, generating and executing code, and controlling laboratory automation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Intelligent Agent (Planner + Web searcher + Docs searcher + Code execution + Automation)",
            "system_description": "Modular autonomous research system driven by a Planner LLM that issues discrete actions (GOOGLE &lt;query&gt;, DOCUMENTATION &lt;query&gt;, PYTHON &lt;code&gt;, EXPERIMENT &lt;code&gt;) and coordinates specialized modules: Web searcher (forms web queries, returns pages, BROWSE action), Docs searcher (vectorized retrieval over hardware API docs using ada embeddings), Code execution (runs generated code in an isolated Docker container and returns outputs/tracebacks), and Automation (executes code against real hardware or returns synthetic/manual procedures). The Agent can: search the internet for literature and procedures, ingest and summarize API/hardware documentation, compute reagent stoichiometry and protocol parameters, generate syntax-correct low-level instrument code (Opentrons/Emerald Cloud Lab SLL), iteratively debug its code based on runtime tracebacks, and submit experiments to cloud labs or local liquid handlers (including heater-shaker modules). It uses GPT-4 for higher-quality planning and GPT-3.5 for faster browsing tasks.",
            "system_type": "AI Scientist / Automated Experimentation Platform",
            "problem_domain": "Chemistry (synthetic organic chemistry, automated synthesis), laboratory automation, experimental design and execution; also general natural language reasoning and code generation",
            "problem_description": "Designed to autonomously: (1) search and navigate extensive hardware documentation to learn APIs and instrument parameters; (2) control liquid handling robots at low level to implement specified procedural prompts; (3) plan and execute integrated multi-instrument chemical experiments combining literature-derived protocols, stoichiometric calculations, hardware API code generation, and analytical feedback (UV-Vis, GC-MS). Example applied tasks include synthesis planning for ibuprofen, aspirin, aspartame (planning and code), mechanistic studies of Suzuki reactions (calculation and analysis), and end-to-end design and execution of Suzuki and Sonogashira cross-coupling reactions on a heater-shaker-equipped liquid handler.",
            "problem_complexity": "Varied: from low-complexity discrete tasks (e.g., automated coloring of microplate wells, simple aspirational protocols) to high-complexity multi-step organic syntheses and multi-instrument integration. Complexity factors include multi-step reaction planning (chemical knowledge, stoichiometry, safety constraints), combinatorial choices of reagents/catalysts/bases, integration of heterogeneous data sources (web literature, API docs), and multi-module hardware control across multiple plates and modules (source plate, target plate on heater-shaker). The Planner expects on average ≥10 reasoning steps to fully satisfy a prompt. No quantitative search-space size is provided, but tasks involve dozens of reagents/variables for cross-coupling plate experiments.",
            "data_availability": "Hybrid: relies on publicly available internet sources (web literature), curated hardware documentation (14 sections of Opentrons API vectorized with ada embeddings, Emerald Cloud Lab SLL guide provided), and locally generated experimental data (UV-Vis arrays, GC-MS outputs from run experiments). Some documentation was not in the model's training cutoff and had to be provided; quality depends on web/doc accuracy. Experimental data is generated by the platform (cloud lab or liquid handler) and analytics (GC-MS, UV-Vis).",
            "computational_requirements": "Uses GPT-4 for planning and GPT-3.5 for browsing; code execution occurs in isolated Docker containers. The paper does not provide exact compute hours or costs. Practical requirements include API access (Google Search API), vector embeddings (OpenAI ada embeddings), GPT-4/GPT-3.5 usage, and compute to run Docker containers and instrument control. Qualitatively non-trivial but feasible on standard cloud infrastructure with LLM API access; GPT-4 noted as slower than GPT-3.5 so module assignment optimized for speed.",
            "problem_structure": "Mostly open-ended but with well-defined sub-tasks: web retrieval and summarization (ill-posed, noisy), structured API usage (well-defined, discrete), low-level instrument programming (discrete and deterministic when correct), and experimental chemistry (stochastic/variable outcomes, requires domain knowledge). Evaluation metrics vary by subtask: syntactic correctness for code, successful instrument execution for automated protocols, and analytical confirmation (GC-MS/UV-Vis) for chemical product formation. Domain knowledge is essential for safe/accurate chemistry.",
            "success_metric": "Task-specific metrics: syntactic/functional correctness of generated instrument code; successful automated execution of protocols on hardware; analytical confirmation of target chemical products (e.g., GC-MS detection); in safety studies, the metric was whether the Agent provided synthesis solutions for controlled substances. The paper reports qualitative/analytical verification (GC-MS) as evidence of chemical synthesis success.",
            "success_rate": "Qualitative mixed results: (a) Low-level automation tasks (robotic liquid handling, microplate manipulations, UV-Vis analysis) were 'mostly accurate' and completed successfully; (b) Integrated chemical experiments: the Agent designed and executed Suzuki and Sonogashira cross-coupling reactions and GC-MS analysis 'revealed the formation of the target products' (successful experimental outcome reported); (c) Synthesis planning examples (ibuprofen, aspirin) were correctly identified and stoichiometry computed; aspartame planning initially produced a structural error (missing methyl) but was corrected after exposure to a suitable example. (d) Safety/dual-use screening: of 11 prompts for controlled substances, the Agent provided a synthesis solution in 4 cases (36%) and refused in 7 cases (64%), with 5 refusals occurring only after further web search. No quantitative overall accuracy percentages beyond these counts are provided.",
            "failure_modes": "Observed failure modes include: hallucinated or incomplete chemical details (e.g., missing methyl group in aspartame plan), variable/volatile suggestions when sampling generation temperature is high (uncertain catalyst/base choices), mismatches between assumed and actual hardware API identifiers (incorrect module name for heater-shaker) requiring documentation lookup and correction, runtime environment mismatches (attempted import of SymPy not installed), assumptions about interactive interpreters (missing print() leading to no output), and incomplete safety/detection for unknown or obfuscated harmful-compound requests (Agent can be misled by replaced terminology). Dual-use risk: Agent sometimes supplies actionable synthesis plans for controlled substances (36% in their test set).",
            "success_factors": "Key enabling factors: modular design separating planning, document retrieval, code execution, and automation; vectorized documentation retrieval (ada embeddings) supplying up-to-date API info; ability to browse the web and incorporate literature; isolated Docker code execution that returns tracebacks enabling iterative debugging; multi-LLM orchestration (GPT-4 for high-quality reasoning and GPT-3.5 for faster browsing); access to cloud lab and local robotic hardware and analytics (UV-Vis, GC-MS); and a human prompt-provider for initial task framing.",
            "comparative_results": "The paper evaluated three capability classes: (1) documentation/hardware navigation — high success in retrieving and using API docs via vector search and summarization; (2) low-level liquid handling control — mostly accurate protocol generation and successful execution (microplate coloring, UV-Vis analyses); (3) complex multi-instrument chemical experiments — successful design and execution of cross-coupling reactions with analytical confirmation, though with episodic errors corrected via documentation/iteration. Safety screening performance contrasted with experimental success: while experimental tasks succeeded in physical execution, the Agent also generated potentially dangerous synthesis plans in a non-negligible fraction (36%) of dual-use prompts, indicating differential performance across scientific vs. ethical/safety dimensions.",
            "human_baseline": null,
            "uuid": "e2585.0",
            "source_info": {
                "paper_title": "Emergent autonomous scientific research capabilities of large language models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A state-of-the-art large language model used in this work as the high-level Planner and for documentation summarization and reasoning tasks within the Agent.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4 (as Planner / summarizer)",
            "system_description": "GPT-4 was used as the primary Planner LLM to reason about tasks, design experimental plans, request web/documentation/code actions, and to produce high-quality code and natural language explanations; a separate GPT-4 instance was used to synthesize and retain code syntax when summarizing unfamiliar documentation (e.g., Emerald Cloud Lab SLL). The Agent also used GPT-3.5 for faster browsing where no appreciable quality loss was observed.",
            "system_type": "Large Language Model (component of AI Scientist)",
            "problem_domain": "Natural language reasoning, synthetic chemistry planning, code generation for lab automation, documentation summarization",
            "problem_description": "Served as central reasoning engine for converting high-level prompts (e.g., 'Perform multiple Suzuki reactions' or 'Synthesize ibuprofen') into actionable multi-step plans that include web searches, documentation retrieval, stoichiometric calculations, code generation, and experimental execution steps.",
            "problem_complexity": "Handled multi-step planning and multi-modal information (textual web content, API docs, numerical computation outputs). Complexity arose from integration of heterogeneous information and the need to produce syntactically correct executable instrument code. The model often required ≥10 reasoning steps for complex prompts.",
            "data_availability": "Operated on web text (post-query browsing), supplied hardware documentation (vectorized), and local experimental outputs. GPT-4's pretraining cutoff left it unaware of some newer hardware modules which necessitated feeding updated docs into the Agent.",
            "computational_requirements": "Access to GPT-4 API (compute/time not quantified); noted as slower than GPT-3.5, which motivated assigning some browsing tasks to GPT-3.5 for speed.",
            "problem_structure": "Open-ended instruction-following and planning tasks with discrete subgoals (search, compute, code, execute). Determinism depends on sampling parameters; higher temperature produced more variable suggestions.",
            "success_metric": "Enabling Agent performance across tasks (protocol correctness, successful experiment execution, code correctness), and quality of documentation summarization and code generation.",
            "success_rate": "Not reported numerically in isolation; GPT-4 enabled successful higher-level planning that led to successful experimental execution (e.g., cross-coupling experiments confirmed by GC-MS) when combined with other modules. Some generation errors and variability occurred (e.g., catalyst/base suggestions at high temperature, occasional chemical structural errors), but iterative corrections were possible.",
            "failure_modes": "Knowledge cutoff leading to missing up-to-date hardware/module names until documentation was provided; stochastic output variability with higher sampling temperatures; occasional incorrect or incomplete chemical details that required correction via external documentation or examples.",
            "success_factors": "High reasoning capability, ability to chain multi-step actions and integrate feedback from code execution and documentation, and synergy with modules (vectorized docs, Docker execution) that enabled debugging and correction.",
            "comparative_results": "GPT-4 was used for planning and high-quality summarization; GPT-3.5 was used for faster web browsing with no appreciable loss for that subtask. The paper does not provide quantitative head-to-head performance numbers beyond qualitative speed/quality comments.",
            "human_baseline": null,
            "uuid": "e2585.1",
            "source_info": {
                "paper_title": "Emergent autonomous scientific research capabilities of large language models",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2
        },
        {
            "paper_title": "ChemCrow: Augmenting largelanguage models with chemistry tools",
            "rating": 2
        },
        {
            "paper_title": "Dual use of artificial-intelligencepowered drug discovery",
            "rating": 1
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 1
        }
    ],
    "cost": 0.01109375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Emergent autonomous scientific research capabilities of large language models</h1>
<p>Daniil A. Boiko, ${ }^{1}$ Robert MacKnight, ${ }^{1}$ and Gabe Gomes ${ }^{* 1,2,3}$</p>
<ol>
<li>Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA</li>
<li>Department of Chemistry, Carnegie Mellon University, Pittsburgh, PA 15213, USA</li>
<li>Wilton E. Scott Institute for Energy Innovation, Carnegie Mellon University, Pittsburgh, PA 15213, USA
*corresponding author, gabegomes@cmu.edu</li>
</ol>
<h4>Abstract</h4>
<p>Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.</p>
<h2>Keywords</h2>
<p>Large Language Models; Intelligent Agents; Generative AI; Autonomous Experimentation; Automation; Physical Sciences; Catalysis.</p>
<h2>Glossary</h2>
<ul>
<li>LLMs: Large Language Models. In this paper, we use OpenAI's GPT-3.5 and GPT-4.</li>
<li>Agent: LLM-based Intelligent Agent (IA) system.</li>
<li>Prompt-provider: A scientist - in this work, one of the human authors - that provides the initial prompt to the Agent.</li>
</ul>
<h1>Main</h1>
<p>Large language models (LLMs), particularly transformer-based models, are experiencing rapid advancements in recent years. These models have been successfully applied to various domains, including natural language, ${ }^{1-5}$ biological ${ }^{6,7}$ and chemical research, ${ }^{8,9}$ as well as code generation. Extreme scaling of models, as demonstrated by OpenAI, has led to significant breakthroughs in the field. ${ }^{1,10}$ Moreover, techniques such as reinforcement learning from human feedback (RLHF) ${ }^{11}$ can considerably enhance the quality of generated text and the models' capability to perform diverse tasks while reasoning about their decisions. ${ }^{12}$</p>
<p>On March 14, 2023, OpenAI released their most capable LLM to date, GPT-4. ${ }^{10}$ Although specific details about the model training, sizes, and data used are limited in the technical report, researchers have provided substantial evidence of the model's exceptional problem-solving abilities. Those include - but are not limited to - high percentiles on the SAT and BAR exams, LeetCode challenges, and contextual explanations from images, including niche jokes. ${ }^{10}$ Moreover, the technical report provides an example of how the model can be employed to address chemistry-related problems.</p>
<p>Inspired by these results, we aimed to develop a multi-LLMs-based Intelligent Agent (hereafter simply called Agent) capable of autonomous design, planning, and performance of complex scientific experiments. The Agent can use tools ${ }^{13}$ to browse the internet and relevant documentation, use robotic experimentation APIs, and leverage other LLMs for various tasks. In this paper, we demonstrate the versatility and efficacy of our Agent by evaluating its performance in three tasks: 1. Efficiently searching and navigating through extensive hardware documentation; 2. Precisely controlling liquid handling instruments at a low level; 3. Tackling complex problems that necessitate simultaneous utilization of multiple hardware modules or integration of diverse data sources.</p>
<h2>The Agent's architecture: action space defined by its multiple modules.</h2>
<p>The Agent's system consists of four components (Figure 1), driven by the "Planner." The Planner takes a prompt as input (e.g., "Perform multiple Suzuki reactions") and carries out actions according to this request. The action space includes accessing the internet ("GOOGLE <query>" request), performing calculations in Python ("PYTHON <code>"), accessing documentation ("DOCUMENTATION <query>"), and running the final experiment ("EXPERIMENT <code>"). Experiments can be performed in various environments - a cloud lab, using a liquid handler, or by providing instructions for performing experiments manually. The model is instructed to reason about its actions, search the internet, calculate all quantities in the reaction, and then perform the corresponding reaction. The Agent is aware that, on average, at least ten steps are needed to fully understand the requested task. No further clarifying questions to the prompt-provider are necessary if the provided description is detailed enough.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of the system architecture. The Agent is composed of multiple modules that exchange messages. Some of them have access to APIs, the Internet, and Python interpreter.</p>
<p>The "Web searcher" component receives queries from the Planner, transforms them into appropriate web search queries, and executes them using the Google Search API. The first ten documents returned are filtered, excluding PDFs, and the resulting list of web pages is passed back to the Web searcher component. The component can then use the "BROWSE" action to extract text from the web pages and compile an answer for the Planner. For this task we can employ GPT-3.5, as it performs significantly faster than GPT-4 with no appreciable loss of quality. The "Docs searcher" component combs through the hardware documentation (e.g., robotic liquid handler, GC-MS, a cloud lab) by utilizing a query and documentation index to find the most relevant pages/sections. Then the best matching results are aggregated to provide a comprehensive and accurate final answer. This module places an emphasize on providing specific function parameter and syntactic information for the hardware API.</p>
<p>The "Code execution" component does not utilize any language models and simply executes the code in an isolated Docker container, protecting the end host machine from any unexpected actions by the Planner. All code outputs are passed back to the Planner, enabling it to fix its predictions in case of software errors. The same applies to the "Automation" component, which then executes the generated code on corresponding hardware or just provides the synthetic procedure for manual experimentation.</p>
<h1>Web search enables Agent's synthesis planning capabilities.</h1>
<p>To demonstrate the system's functionality, we use the synthesis of ibuprofen as an example (Figure 2A). The input prompt is straightforward: "Synthesize ibuprofen." The model then searches the internet for information on ibuprofen synthesis, locating the necessary details on a particular website. The model correctly identifies the first step of the synthesis, which is the Friedel-Crafts reaction between isobutylbenzene and acetic anhydride catalyzed by aluminum chloride (see Appendix A). The planning phase for the first step concludes once the model requests documentation for the Friedel-Crafts synthesis procedure.</p>
<p>Two other examples of the system's capabilities include the synthesis of common aspirin (Figure 2B and Appendix B), which the model searches and designs effectively, and aspartame synthesis (Figure 2D and Appendix C), which, although missing the methyl group in the product, can be corrected once the model receives a suitable synthetic example for execution in the cloud lab. Furthermore, when asked to study a Suzuki reaction, the model accurately identifies the substrates and the product (Figure 2C and Appendix D). The high-temperature parameter for text generation results in volatility when suggesting specific catalysts or bases.</p>
<p>Connecting the model to a chemical reaction database such as Reaxys ${ }^{14}$ or SciFinder ${ }^{15}$ via API could significantly enhance the system's performance. Alternatively, analyzing the system's previous statements is another approach to improving its accuracy.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Agent's capabilities in the synthesis planning task. A. Ibuprofen synthesis. B. Aspirin synthesis. C. Suzuki reaction mechanism study, where the Agent had to choose how to study the mechanism. D. Aspartame synthesis.</p>
<h1>Vector search can be employed for retrieval of dense hardware API documentation.</h1>
<p>To integrate an intelligent Agent capable of sophisticated reasoning with contemporary software, it is crucial to provide a clear and concise presentation of relevant technical documentation. Modern software is characterized by its complexity and the intricate interplay between various components. Consequently, comprehensive software documentation is indispensable for programmers to comprehend these interactions and utilize them effectively to accomplish their goals. Nonetheless, traditional software documentation frequently employs highly technical language, which can be challenging for non-experts to grasp. This creates a barrier to entry for many potential users of the software, limiting its reach and effectiveness.</p>
<p>Large language models have the potential to overcome this barrier by generating natural language descriptions of software documentation that are more accessible to nonexperts. These models are trained on a vast corpus of text from a variety of sources, which includes extensive information related to Application Programming Interfaces (APIs). One such API is the Opentrons Python API. ${ }^{16}$ However, the GPT-4's training data contains information up until September 2021. Thus, there is potential for enhancing the Agent's accuracy in using the API. To this end, we devised an approach to provide the Agent with requisite documentation for a given task, summarized in Figure 3A.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Overview of documentation search. A. Prompt-to-(improved OT-2 Python API)-code via ada embedding and distance-based vector search. B. Prompt-to-function recommendation in Emerald Cloud Lab symbolic lab language via supplementation of documentation guide.</p>
<p>For all 14 sections of the OT-2 API documentation we have generated OpenAI's ada embeddings to cross reference and compute similarity with respect to a query. The agent is instructed to inquire about proper use of the API when needed via the Documentation action. An ada embedding for the subsequent query is generated and documentation sections are selected via a distance-based vector search. The number of sections provided is dictated by the number of GPT-4 tokens present in the raw text for a given section. The maximum number of tokens is set to 7800 , such that the relevant documents can be provided in one step. This approach proved critical for providing the agent with information about the heater-shaker hardware module necessary for performing chemical reactions (see "The Agent is capable of designing and performing chemical experiments." section).</p>
<p>A greater challenge emerges when applying this approach to a more diverse robotic platform, such as the Emerald Cloud Lab (ECL). Nonetheless, we can explore the effectiveness of providing information about the Cloud Lab's Symbolic Lab Language (SLL), which is currently unknown to the GPT-4 model. For this we provide the Agent with a documentation guide from ECL pertaining to all available functions for running experiments. ${ }^{17}$ Figure 3B summarizes three examples of the User providing a simple query and the Agent directing the User to relevant ECL functions. More examples are shared in the Appendix G. In all cases, the Agent correctly identifies functions for accomplishing the given task. After selection of appropriate functions, the raw plain text documentation is passed through a separate GPT-4 model to perform code syntax retention and summarization. Specifically, this model efficiently retains information regarding the various options, instruments, and parameters for a given function. Once the entire documentation has been ingested, the model is prompted to produce a code block utilizing the given function to be passed back to the Planner. This serves as a basis for the model to utilize this function with specific options, instruments, and parameters as they are gathered by the Web searcher.</p>
<h1>Mastering automation: multi-instrument systems controlled by natural language.</h1>
<p>Access to documentation enables us to provide sufficient information for the Agent to conduct experiments in the physical world. To initiate the investigation, we chose an open-source liquid handler with a well-documented Python API. The "Getting Started" page from its documentation was supplied to the Planner in the system prompt. Other pages were vectorized using the approach described in the "Providing hardware API Documentation" section. In this section, we did not grant access to the internet (Figure 4A).</p>
<p>A.
<img alt="img-3.jpeg" src="img-3.jpeg" />
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>UV-Vis data from plate 1 is ready. You can access it using PYTHON, it will be in the file named
'plate1.npy'. You will have NumPy installed. The file contains NumPy array of shape $(9,12,41)$. There are 41 wavelengths from 350 to 750 with 10 nm step. Think about how would different colors absorb first.</p>
<p>Automated answer</p>
<ul>
<li>guiding prompt
[530, 430, 630]
Automated answer
<img alt="img-5.jpeg" src="img-5.jpeg" /></li>
</ul>
<p>Figure 4. Robotic liquid handler control capabilities and integration with analytical tools. A. Overview of the Agent's configuration. B-E. Drawing geometrical figures. F. The Agent solves a color identification problem using UV-Vis data.</p>
<p>We began with simple experiments on operating the robot, which simultaneously required the ability to consider a set of samples as a whole (in our case, an entire microplate). Straightforward prompts in natural language, such as "Color every other line with one color of your choice," resulted in mostly accurate protocols. When executed by the robot, these protocols closely resembled the requested prompt (Figure 4B-E).</p>
<p>The Agent's first action was to prepare small samples of the original solutions (Figure 4F). It then requested UV-Vis measurements to be performed (Appendix H). Once completed, the Agent was provided with a file name containing a NumPy array with spectra for each well of the microplate. The Agent subsequently wrote Python code to identify the wavelengths with maximum absorbance, using this data to correctly solve the problem.</p>
<h1>Bringing it all together: the Agent's integrated chemical experiment design and execution capabilities.</h1>
<p>Previous experiments could be affected by the knowledge of the Agent's modules from the pretraining step. We wanted to evaluate the Agent's ability to plan an experiment by combining data from the internet, performing the necessary calculations, and ultimately writing the code for the liquid handler. To increase complexity, we asked the Agent to use the heater-shaker module released after the GPT-4 training data collection cutoff. These requirements were incorporated into the Agent's configuration (Figure 5A).</p>
<p>The problem was designed as follows: the Agent is provided with a liquid handler equipped with two microplates. One (Source Plate) contains stock solutions of multiple reagents, including phenyl acetylene and phenylboronic acid, multiple aryl halide coupling partners, two catalysts, two bases, and the solvent to dissolve the sample (Figure 5B). The target plate is installed on the heater-shaker module (Figure 5C). The Agent's goal is to design a protocol to perform Suzuki and Sonogashira reactions.</p>
<p>The Agent begins by searching the internet for information on the requested reactions, their stoichiometry, and conditions (Figure 5D). It selects the correct coupling partners for the corresponding reactions. Among all aryl halides, it selected bromobenzene for Suzuki reaction and iodobenzene for Sonogashira reaction. This behavior changes from each run, as it also selects p-nitroiodobenzene due to its high reactivity in oxidative addition reactions, or bromobenzene because it is reactive but less toxic than aryl iodides. This highlights a potential future use case for the model - performing experiments multiple times to analyze the model's reasoning and construct a bigger picture. The model selected a $\mathrm{Pd} / \mathrm{NHC}$ catalyst as a more efficient, modern approach for cross-coupling reactions, and triethylamine was chosen as the base.</p>
<p>The Agent then calculates the required volumes of all reactants and writes the protocol. However, it used an incorrect heater-shaker module name. Upon noticing the mistake, the model consulted the documentation. This information was then used to modify the protocol, which successfully ran (Figure 5E). Subsequent GC-MS analysis of the reaction mixtures revealed the formation of the target products for both reactions (Appendix I).</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5. Cross-coupling Suzuki and Sonogashira reaction experiments designed and performed by the Agent. A. Overview of the Agent's configuration. B. Available compounds. C. Liquid handler setup. D. Solving the synthesis problem. E. Generated protocol.</p>
<h1>The Agent has high reasoning capabilities.</h1>
<p>The system demonstrates remarkably high reasoning capabilities, enabling it to request necessary information, solve complex problems, and generate high-quality code for experimental design. OpenAI has shown that GPT-4 could rely on some of those capabilities to take actions in the physical world during their initial testing performed by the Alignment Research Center. ${ }^{10}$</p>
<p>The most remarkable reasoning capability exhibited by the Agent was its ability to correct its own code based on the automatically generated outputs. Besides already mentioned examples, during calculations for a mechanistic study of the Suzuki reaction, the model requested the system to execute code that imported the SymPy package (refer to Appendix D), which was not installed. After receiving the corresponding traceback, the Agent revised the code using basic Python. However, this modified code still proved unhelpful, as it did not return any output (the model had assumed that the interpreter was being used in interactive mode). Acknowledging this, the Agent made a final adjustment by incorporating a print() statement.</p>
<h2>The Agent shows interesting approach to key scientific problems.</h2>
<p>Furthermore, we aimed to assess the system's performance when presented with an exceptionally challenging problem. We began by asking the model to develop a novel anticancer drug (refer to Appendix E). The model approached the analysis in a logical and methodical manner: it first inquired about current trends in anticancer drug discovery, subsequently selected a target, and requested a scaffold targeting these compounds. Once the compound was identified, the model attempted its synthesis (this step was not performed experimentally). Another example involved studying the mechanism of a Suzuki reaction (see Appendix D). In this case, the model sought information on the reaction's mechanism and, upon obtaining the individual steps, searched for an example of such a reaction. To execute the reaction, the model calculated the required amounts of reagents and requested the documentation page related to the corresponding synthesis.</p>
<h2>Safety implications of the developed approach.</h2>
<p>There is growing concern regarding the potential misuse of molecular machine learning models for harmful purposes. Specifically, the dual-use application of models for predicting cytotoxicity ${ }^{18}$ to create new poisons or employing AlphaFold2 to develop novel bioweapons has raised alarm. Central to these concerns are the possible misuse of large language models and automated experimentation for dual-use purposes or otherwise. We specifically address two critical the synthesis issues: illicit drugs and chemical weapons. To evaluate these risks, we designed a test set comprising compounds from the DEA's Schedule I and II substances and a list of known chemical weapon agents. We submitted these compounds to the Agent using their common names, IUPAC names, CAS numbers, and SMILES strings to determine if the Agent would carry out extensive analysis and planning (Figure 6).</p>
<p>Note: None of these examples were performed experimentally. Under no circumstances should any individual or organization attempt to recreate, synthesize, or otherwise produce the substances or compounds discussed in this section.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6. Results of the preliminary dual-use safety study. A. Controlled substances that the Agent agreed to synthesize. B. Controlled substances that Agent refused to synthesize these examples after a web search. C. Controlled substances that Agent refused to synthesize from the initial prompt.</p>
<p>The run logs can be found in Appendix F. Out of 11 different prompts (Figure 6), four (36\%) provided a synthesis solution and attempted to consult documentation to execute the procedure. This figure is alarming on its own, but an even greater concern is the way in which the Agent declines to synthesize certain threats. Out of the seven refused chemicals, five were rejected after the Agent utilized search functions to gather more information about the substance. For instance, when asked about synthesizing codeine, the Agent becomes alarmed upon learning the connection between codeine and morphine, only then concluding that the synthesis cannot be conducted due to the requirement of a controlled substance. However, this search function can be easily manipulated by altering the terminology, such as replacing all mentions of morphine with "Compound A" and codeine with "Compound B". Alternatively, when requesting a synthesis procedure that must be performed in a DEA-licensed facility, bad actors can mislead the Agent by falsely claiming their facility is licensed, prompting the Agent to devise a synthesis solution.</p>
<p>In the remaining two instances, the Agent recognized the common names "heroin" and "mustard gas" as threats and prevented further information gathering. While these results are promising, it is crucial to recognize that the system's capacity to detect misuse primarily applies to known compounds. For unknown compounds, the model is less likely to identify potential misuse, particularly for complex protein toxins where minor sequence changes might allow them to maintain the same properties but become unrecognizable to the model.</p>
<p>It is important to note that while the potential for dual use of Intelligent Agent capable of running scientific experiments is real, fully monitored cloud labs remain a safer choice than simply remote-connected machines. Screening, monitoring, and control safety systems such as the ones implemented by major cloud lab companies offer an additional layer of protection from potential misuses or bad actors.</p>
<h1>Conclusions</h1>
<p>In this paper, we presented an Intelligent Agent system capable of autonomously designing, planning, and executing complex scientific experiments. Our system demonstrates exceptional reasoning and experimental design capabilities, effectively addressing complex problems and generating high-quality code.</p>
<p>However, the development of new machine learning systems and automated methods for conducting scientific experiments raises substantial concerns about the safety and potential dual use consequences, particularly in relation to the proliferation of illicit activities and security threats. By ensuring the ethical and responsible use of these powerful tools, we can continue to explore the vast potential of large language models in advancing scientific research while mitigating the risks associated with their misuse.</p>
<h1>Limitations, Safety Recommendations, and a Call to Action</h1>
<p>We strongly believe that guardrails must be put in place to prevent this type of potential dual-use of large language models. We call for the AI community to engage in prioritizing safety of these powerful models. We call upon OpenAI, Microsoft, Google, Meta, Deepmind, Anthropic, and all the other major players to push the strongest possible efforts on safety of their LLMs. We call upon the physical sciences community to be engaged with the players involved in developing LLMs to assist them in developing those guardrails.</p>
<p>There are several limitations and safety concerns associated with the proposed machine learning system. These concerns warrant the implementation of safety guardrails to ensure responsible and secure usage of the system. At the very least, we argue that the community (both Al and physical sciences) should engage in the following recommendations:</p>
<ol>
<li>Human intervention: While the system demonstrates high reasoning capabilities, there might be instances where human intervention is necessary to ensure the safety and reliability of the generated experiments. We recommend incorporating a human-in-the-loop component for the review and approval of potentially sensitive experiments, especially those involving potentially harmful substances or methodologies. We believe that specialists should oversee and deliberate about the Agent's actions in the physical world.</li>
<li>Novel compound recognition: The current system can detect and prevent the synthesis of known harmful compounds. However, it is less efficient at identifying novel compounds with potentially harmful properties. This could be circumvented by implementing machine learning model to identify potentially harmful structures before passing them into the model.</li>
<li>Data quality and reliability: The system relies on the quality of the data it gathers from the internet and operational documentation. To maintain the reliability of the system, we recommend the continuous curation and update of the data sources, ensuring that the most up-to-date and accurate information is being used to inform the system's decision-making process.</li>
<li>System security: The integration of multiple components, including large language models and automated experimentation, poses security risks. We recommend implementing robust security measures, such as encryption and access control, to protect the system from unauthorized access, tampering, or misuse.</li>
</ol>
<h2>Broader Impacts</h2>
<p>The proposed machine learning system has numerous potential broader impacts on science, technology, and society:</p>
<ol>
<li>Acceleration of scientific research: By automating the design, planning, and execution of experiments, the system can significantly accelerate scientific research</li>
</ol>
<p>across various fields. Researchers can focus on interpreting results, refining hypotheses, and making discoveries, while the system handles the experimental process.
2. Democratization of scientific resources: The system can potentially make scientific experimentation more accessible to researchers with limited resources or expertise. It may enable smaller research groups or individuals to conduct complex experiments with the support of large language models and cloud labs, promoting a more inclusive scientific community.
3. Interdisciplinary collaboration: The system's versatility across domains, including natural language, biology, chemistry, and computer programming, can foster interdisciplinary collaboration. Researchers from different fields can leverage the system's capabilities to address complex problems that require a diverse set of skills and knowledge.
4. Education and training: The system can serve as a valuable educational tool for students and researchers to learn about experimental design, methodology, and analysis. It can help develop critical thinking and problem-solving skills, as well as encourage a deeper understanding of scientific principles.
5. Economic impact: By automating and streamlining the experimental process, the system can potentially reduce the costs associated with research and development. This can lead to increased investment in research and innovation, ultimately driving economic growth and competitiveness.</p>
<p>However, the potential broader impacts also include challenges and risks that must be addressed. Ensuring responsible and ethical use of the system, implementing robust security measures, and continuously updating data sources are essential steps to mitigate potential negative consequences, such as the proliferation of harmful substances or the misuse of powerful machine learning tools for nefarious purposes. By addressing these concerns, we can unlock the full potential of the proposed system and drive positive change across scientific research and society at large.</p>
<h1>Acknowledgments</h1>
<p>We are thankful to all Gomes group members for their support, in particular to Letícia Madureira for assistance with the Figures in this manuscript. We would like to thank the following CMU Chemistry groups for their assistance with providing some of chemicals needed for the Agent's experiments: Sydlik, Garcia Borsch, Matyjaszewski, and Ly. We have special thanks for the Noonan group (Prof. Kevin Noonan and Dhruv Sharma) for providing access to chemicals and GC-MS analytics. We would like to thank the team at Emerald Cloud Labs (with special attention to Ben Kline, Ben Smith, and Brian Frezza) for assisting us with parsing their documentation.
G.G. is grateful to the CMU Cloud Lab Initiative led by the Mellon College of Science for its vision of the future of sciences.</p>
<h1>Funding</h1>
<p>G.G. thanks Carnegie Mellon University, the Mellon College of Sciences and its department of chemistry, the College of Engineering and its department of chemical engineering for the startup support.</p>
<h2>Data availability</h2>
<p>Examples of the experiments discussed in the text are provided in the Appendices. Data (including documentation search and cloud lab execution), code, and prompts will be released in the later versions of this work due to safety concerns.</p>
<h2>Competing interests</h2>
<p>The authors have no competing interests to disclose at this moment.</p>
<h2>Author contributions</h2>
<p>D.A.B. designed the computational pipeline and developed "Planner", "Web searcher", and "Code execution" module. R.M. assisted in designing the computational pipeline and developed the "Docs searcher" module. D.A.B. assisted and oversaw the Agent's scientific experiments. D.A.B. and R.M. designed and performed the initial computational safety studies. G.G. designed the concept, performed preliminary studies, and supervised the project. D.A.B, R.M. and G.G. wrote this manuscript.</p>
<h2>Technology use disclosure</h2>
<p>The writing of this manuscript was assisted by ChatGPT (specifically, GPT-4). The authors have read, corrected, and verified all information presented in this work.</p>
<h1>References and Notes</h1>
<ol>
<li>Brown, T. et al. Language Models are Few-Shot Learners. in Advances in Neural Information Processing Systems (eds. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F. \&amp; Lin, H.) vol. 33 1877-1901 (Curran Associates, Inc., 2020).</li>
<li>Thoppilan, R. et al. LaMDA: Language Models for Dialog Applications. (2022).</li>
<li>Touvron, H. et al. LLaMA: Open and Efficient Foundation Language Models. (2023).</li>
<li>Hoffmann, J. et al. Training Compute-Optimal Large Language Models. (2022).</li>
<li>Chowdhery, A. et al. PaLM: Scaling Language Modeling with Pathways. (2022).</li>
<li>Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science (1979) 379, 1123-1130 (2023).</li>
<li>Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Brief Bioinform 23, (2022).</li>
<li>Irwin, R., Dimitriadis, S., He, J. \&amp; Bjerrum, E. J. Chemformer: a pre-trained transformer for computational chemistry. Mach Learn Sci Technol 3, 015022 (2022).</li>
<li>Kim, H., Na, J. \&amp; Lee, W. B. Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention. J Chem Inf Model 61, 5804-5814 (2021).</li>
<li>OpenAI. GPT-4 Technical Report. (2023).</li>
<li>Ziegler, D. M. et al. Fine-Tuning Language Models from Human Preferences. (2019).</li>
<li>Ouyang, L. et al. Training language models to follow instructions with human feedback. (2022).</li>
<li>Bran, A. M., Cox, S., White, A. D. \&amp; Schwaller, P. ChemCrow: Augmenting largelanguage models with chemistry tools. (2023).</li>
<li>Reaxys. https://www.reaxys.com.</li>
<li>SciFinder. https://scifinder.cas.org.</li>
<li>Opentrons. Opentrons Python Protocol API. https://docs.opentrons.com/v2/.</li>
<li>
<p>Emerald Cloud Lab. Running Experiments. https://www.emeraldcloudlab.com/guides/runningexperiments.</p>
</li>
<li>
<p>Urbina, F., Lentzos, F., Invernizzi, C. \&amp; Ekins, S. Dual use of artificial-intelligencepowered drug discovery. Nat Mach Intell 4, 189-191 (2022).</p>
</li>
</ol>
<h1>Appendices</h1>
<h2>Glossary for Appendices</h2>
<h2>This box demarks notes from the authors.</h2>
<p>The following description follows OpenAI's notation for chat completion mode:</p>
<ul>
<li>Assistant: Output of the model (in most cases it is the "Planner" module).</li>
<li>User: Outputs of other modules provided to the main module (for example, search results or Python code output). Real user (in these experiments, a human scientist) inputs are marked as "Prompt:".</li>
</ul>
<h2>Appendix A: Synthesis of ibuprofen</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Assistant</th>
<th style="text-align: center;">First, let's look for a synthesis route for ibuprofen. <br> GOOGLE ibuprofen synthesis route</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">Ibuprofen is synthesized through a process called Friedel-Crafts acylation. The starting materials for the synthesis are isobutylbenzene and acetic anhydride. This process was discovered by Stewart Adams and John Nicholson in 1961 while they were working at Boots UK Limited. The original ibuprofen synthesis by the Boots Group serves as a basis for the manufacturing process today.</td>
</tr>
<tr>
<td style="text-align: center;">Assistant</td>
<td style="text-align: center;">Great, now that we have the synthesis route through Friedel-Crafts acylation using isobutylbenzene and acetic anhydride, let's find the stoichiometry and the experimental conditions for this reaction. <br> GOOGLE Friedel-Crafts acylation isobutylbenzene and acetic anhydride conditions and stoichiometry</td>
</tr>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">In the Friedel-Crafts acylation of isobutylbenzene and acetic anhydride, the reaction conditions involve reacting isobutylbenzene (1) with acetic anhydride under the influence of a Lewis acid, such as aluminum chloride (AlCl3) as a catalyst. This reaction forms p-isobutylacetophenone (2) through electrophilic aromatic substitution. <br> The stoichiometry of the reaction would involve one equivalent of isobutylbenzene reacting with one equivalent of acetic anhydride in the presence of the AlCl3 catalyst. However, the actual yield and purity may be influenced by factors such as the concentration of unreacted starting materials or reaction time. Consider extending the reaction time or adding more catalyst to enhance the conversion and yield of the desired product.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Assistant</th>
<th style="text-align: center;">Now that we have information about the reactants and stoichiometry, let's calculate how much of each reactant we need to synthesize around 100 mg of ibuprofen. First, we need the molecular weights of isobutylbenzene, acetic anhydride, and ibuprofen. Then, we'll calculate the amount of isobutylbenzene and acetic anhydride needed based on the reaction stoichiometry. <br> PYTHON <br> isobutylbenzene_mw $=134.22 # \mathrm{~g} / \mathrm{mol}$ <br> acetic_anhydride_mw $=102.09 # \mathrm{~g} / \mathrm{mol}$ <br> ibuprofen_mw $=206.28 # \mathrm{~g} / \mathrm{mol}$ <br> desired_mass $=0.1 #$ in $\mathrm{g}(100 \mathrm{mg})$ <br> isobutylbenzene_moles = desired_mass / ibuprofen_mw <br> acetic_anhydride_moles = isobutylbenzene_moles <br> mass_isobutylbenzene = isobutylbenzene_moles * <br> isobutylbenzene_mw <br> mass_acetic_anhydride = acetic_anhydride_moles * <br> acetic_anhydride_mw <br> print(mass_isobutylbenzene, mass_acetic_anhydride)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">0.06506689936009308 0.049490983129726586</td>
</tr>
<tr>
<td style="text-align: center;">Assistant</td>
<td style="text-align: center;">Based on our calculations, we need 0.0651 g of isobutylbenzene and 0.0495 g of acetic anhydride to synthesize about 100 mg of ibuprofen. <br> Lastly, let's check the cloud lab documentation for the experimental procedure that involves the Friedel-Crafts acylation reaction with isobutylbenzene and acetic anhydride, and the use of aluminum chloride as a catalyst. <br> DOCUMENTATION Friedel-Crafts acylation experimental procedure with aluminum chloride catalyst</td>
</tr>
</tbody>
</table>
<h1>Second Google search expanded</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">User</th>
<th style="text-align: left;">Friedel-Crafts acylation isobutylbenzene and acetic anhydride conditions <br> and stoichiometry</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Assistant</td>
<td style="text-align: left;">GOOGLE Friedel-Crafts acylation isobutylbenzene and acetic anhydride <br> conditions and stoichiometry</td>
</tr>
<tr>
<td style="text-align: left;">User</td>
<td style="text-align: left;">Google search results: <br> Ibuprofen Synthesis - Writing Anthology <br> https://central.edu/writing-anthology/2019/04/11/ibuprofen-synthesis/ <br> Apr 11, 2019 ... Initially, isobutylbenzene (1) and acetic anhydride, were reacted under Friedel- <br> Crafts acylation conditions to create p-isobutylacetophenone ...</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Synthesis of ibuprofen from benzene - The Science Snail <br> https://www.sciencesnail.com/science/synthesis-of-ibuprofen-from-benzene</td>
</tr>
</tbody>
</table>
<p>Oct 11, 2018 ... Friedel-Crafts acylation mechanism isobutylbenzene ... an aldoxime (7) which is then dehydrated to the nitrile (8) using acetic anhydride.
(PDF) Ibuprofen Continuous Manufacturing - Process Modeling and ...
https://www.researchgate.net/publication/359864378_Ibuprofen_Continuous_Manufacturing_ Process_Modeling_and_Techno-Economic_Assessment_TEA_using_SuperPro_Designer Apr 11, 2022 ... The synthesis process involves Friedel-Crafts acylation, ... The raw materials acetic anhydride and isobutyl benzene (IBB) and the recycled ...</p>
<p>Regioselective Friedel-Crafts Acylation Reaction Using Single ...
https://pubs.acs.org/doi/10.1021/acsomega.2c03555
Aug 31, 2022 ... Again, the Friedel-Crafts (FC) acylation of aromatic substrates with ... (50) and substituted anhydrides (51) in stoichiometric amounts and ...</p>
<p>Flow Chemistry: Recent Developments in the Synthesis of ...
https://pubs.acs.org/doi/10.1021/acs.oprd.5b00325
Nov 26, 2015 ... The continuous flow Friedel-Crafts acylation of isobutylbenzene 68 with propionyl chloride 69 was performed with AlCl3 as a Lewis acid using ...</p>
<p>Strongly Lewis Acidic Metal-Organic Frameworks for Continuous ...
https://pubs.acs.org/doi/10.1021/jacs.9b07891
Sep 4, 2019 ... (58) Conventional Lewis acids for Friedel-Crafts acylation include ... Reaction conditions: arene ( 1 equiv, 1.0 mmol ), acetic anhydride ( 1.0 ...</p>
<p>A comprehensive review of flow chemistry techniques tailored to the ...
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8182698/
Optimisation of the reaction conditions in flow were carried out on ... is the acetylation of geraniol (343) through acetic anhydride developed by Adarme ...</p>
<p>Continuous Flow Organic Chemistry: Successes and Pitfalls at the ...
https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ejoc. 201800149
Mar 13, 2018 ... The reactor effluent was next mixed with acetic anhydride an N ... The first step included a Friedel-Crafts acylation of isobutyl benzene ...</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Assistant</th>
<th style="text-align: center;">BROWSE https://central.edu/writing-anthology/2019/04/11/ibuprofensynthesis/</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">We removed the text from this website from the paper. You can access it by using URL above.</td>
</tr>
<tr>
<td style="text-align: center;">Assistant</td>
<td style="text-align: center;">OUTPUT In the Friedel-Crafts acylation of isobutylbenzene and acetic anhydride, the reaction conditions involve reacting isobutylbenzene (1) with acetic anhydride under the influence of a Lewis acid, such as aluminum chloride (AlCl3) as a catalyst. This reaction forms pisobutylacetophenone (2) through electrophilic aromatic substitution. <br> The stoichiometry of the reaction would involve one equivalent of isobutylbenzene reacting with one equivalent of acetic anhydride in the presence of the AlCl3 catalyst. However, the actual yield and purity may be influenced by factors such as the concentration of unreacted starting materials or reaction time. Consider extending the reaction time or adding more catalyst to enhance the conversion and yield of the desired product.</td>
</tr>
</tbody>
</table>
<h1>Appendix B: Synthesis of aspirin</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Assistant</th>
<th style="text-align: center;">I will first look up the synthesis reaction for aspirin to understand the reagents and procedure involved. <br> GOOGLE aspirin synthesis reaction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">Aspirin, also known as acetylsalicylic acid, is synthesized through a chemical reaction between salicylic acid and acetic anhydride. Sulfuric acid is used as a catalyst to speed up the reaction. The overall reaction can be described as an esterification reaction, in which a carboxylic acid reacts with an acid anhydride to form an ester. The balanced chemical equation for the reaction is: <br> C7H6O3 (salicylic acid) + C4H6O3 (acetic anhydride) -&gt; C9H8O4 (aspirin) + C2H4O2 (acetic acid) <br> During this reaction, the aspirin product will precipitate when water is added as it has low solubility in water, while acetic anhydride and acetic acid will dissolve in water. Vacuum filtration can then be used to separate the crystalline aspirin from the reaction mixture, excluding any unreacted salicylic acid.</td>
</tr>
<tr>
<td style="text-align: center;">Assistant</td>
<td style="text-align: center;">Since we want to synthesize around 100 mg of aspirin, I will calculate the required quantities of salicylic acid and acetic anhydride using the balanced chemical equation and their molar masses. I will assume a $100 \%$ yield for simplicity. <br> PYTHON <br> aspirin_molar_mass $=180.16 # \mathrm{~g} / \mathrm{mol}$ <br> salicylic_acid_molar_mass $=138.12 # \mathrm{~g} / \mathrm{mol}$ <br> acetic_anhydride_molar_mass $=102.09 # \mathrm{~g} / \mathrm{mol}$ <br> target_aspirin_mass $=100 # \mathrm{mg}$ <br> target_aspirin_mol = target_aspirin_mass / <br> (aspirin_molar_mass * 1e3) <br> required_salicylic_acid_mol = target_aspirin_mol <br> required_acetic_anhydride_mol = target_aspirin_mol <br> required_salicylic_acid_mass = required_salicylic_acid_mol <br> * salicylic_acid_molar_mass * 1e3 <br> required_acetic_anhydride_mass = <br> required_acetic_anhydride_mol * acetic_anhydride_molar_mass <br> * 1e3 <br> print(required_salicylic_acid_mass, <br> required_acetic_anhydride_mass)</td>
</tr>
</tbody>
</table>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1962 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1962</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1962</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-281080730</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.01106v1.pdf" target="_blank">Robix: A Unified Model for Robot Interaction, Reasoning and Planning</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1962.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1962.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robix</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robix: A Unified Model for Robot Interaction, Reasoning and Planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single vision-language model that serves as the high-level cognitive layer for hierarchical robot systems, integrating embodied visual grounding, chain-of-thought reasoning, task planning, human-robot interaction, and action emission to a low-level VLA controller; trained with continued pretraining, supervised finetuning with synthesized interaction+reasoning traces, and RL with a thought-action consistency reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Robix (Robix-7B / Robix-32B; Robix-*-SFT / Robix-*-RL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified vision-language transformer (continued from Qwen2.5-VL backbone) trained end-to-end for interactive embodied tasks. Robix ingests egocentric/multi-view visual observations plus user utterances, produces internal chain-of-thought traces, and emits atomic action plans (textual action commands) and verbal responses. Grounding is learned by (a) continued pretraining on robot- and egocentric-focused corpora (3D spatial tasks, visual grounding datasets, task-centric reasoning examples), (b) supervised finetuning on synthesized human-robot interaction trajectories paired with chain-of-thought traces, and (c) RL (GRPO) with an auxiliary thought-action consistency reward assessed by an external LLM. At inference it retains the latest N visual observations (short-term visual memory) and stores prior thoughts/actions in short-term memory to respect context budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Vision encoder from Qwen2.5-VL backbone (multimodal transformer visual frontend); frozen during instruction-tuning stage (Stage 2)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Continued pretraining on ~200B tokens including: ~50M image-text pairs (instruction-tuned), and robot/egocentric datasets (AgiBot, BridgeData V2, Droid, Egodex, RoboVQA, HoloAssist, Ego4D) plus synthetic simulation/AIGC images for rare items; initial backbone is Qwen2.5-VL (pretrained prior to Robix continued pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Learned vision-language fusion in a joint VLM: multimodal fusion during continued pretraining and instruction tuning (model uses multimodal transformer-style fusion to align visual embeddings with language tokens); explicit supervision for visual grounding via bounding-box / point / 3D bbox tasks and thought-augmented labels; final RL stage adds reward for alignment between the model's internal thought traces and emitted action (thought-action consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Multi-level: supports pixel/patch-level and region/box-level grounding as well as 3D bounding-box outputs and multi-view correspondence (the model is trained and evaluated on point detection, 2D bounding boxes, 3D bboxes and depth/egomotion tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit spatial signals included in pretraining and tasks: 2D bounding boxes, normalized point coordinates, 3D bounding boxes, relative and absolute depth estimates, and multi-view correspondence / egomotion; the model is trained on these modalities but does not expose a separate explicit 3D world map at inference (uses recent frames + spatial outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Object manipulation / instruction following / mobile manipulation / 3D scene understanding / interactive task planning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>AGIBot Evaluation Set; Agibot-ER benchmark (Task Status Verification, Action Affordance, Next Action Prediction); Internal In-Distribution (ID) and Out-of-Distribution (OOD) benchmarks; visual grounding benchmarks (LVIS-MG, RefCOCO, Pixmo-Point, Where2Place); real-world tasks: table bussing, checkout packing, dietary filtering, grocery shopping, tableware organization & shipment.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Mixed: egocentric views, real-world robot camera feeds, photorealistic simulation scenes, and synthetic/text-to-image created scenes for rare objects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Offline: plan/action prediction accuracy (match to candidate action list), F1 for invalid/replan categories, standard VQA/grounding metrics (F1, accuracy). Online: task progress (percentage of subtasks completed), task completion percentages, standard dataset metrics for grounding/VQA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative numbers reported in paper: Robix-32B-RL achieves top rank across offline evaluation sets; in VLM-only online evaluation Robix-32B achieves average task progress 92.6% (vs. Gemini-2.5-Pro 91% and Qwen2.5-VL-32B 28%); in VLM-VLA integrated online evaluation Robix-32B achieves average task progress 92.5%, exceeding Gemini-2.5-Pro and GPT-4o by 4.3 and 28.1 percentage points respectively. On visual grounding, Robix-7B and Robix-32B improved absolute F1 on LVIS-MG by +39.6 and +25.0 points over their Qwen2.5-VL backbones; 3D spatial benchmarks: Robix-7B/32B averaged accuracies 73.4% / 75.8% versus 66.9% / 70.7% for Qwen2.5-VL.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation of chain-of-thought reasoning (Robix-7B-SFT-wo-R) produced >7 percentage point drop on Internal OOD benchmarks and a 26.7-point drop on ID-OpenEnded tasks compared to reasoning-enabled SFT; RL ablation: SFT -> RL gains (Robix-7B-RL and Robix-32B-RL improved Internal OOD accuracy by +8.3 and +3.3 points respectively vs their SFT counterparts).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Chain-of-thought supervision and grounding-focused pretraining yield large gains for OOD and open-ended instruction following (e.g., -26.7 points when CoT omitted on ID-OpenEnded). Continued pretraining and SFT over backbone deliver large improvements on grounding benchmarks (e.g., LVIS-MG F1 +39.6/+25.0 for 7B/32B). RL further improves thought-action consistency and reduces irrational reasoning (+3.3 to +8.3 points on Internal OOD vs SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Comparison primarily vs. Robix's backbone Qwen2.5-VL (same encoder family): Robix consistently outperforms Qwen2.5-VL on visual grounding and spatial benchmarks (e.g., LVIS-MG F1 +39.6 / +25.0 for Robix-7B / Robix-32B over Qwen2.5-VL-7B / -32B; 3D spatial average accuracy +6.5 and +5.1 points for 7B/32B). No wide multi-architecture encoder sweep is reported; vision encoder is frozen during instruction-tuning stage.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors explicitly identify limited embodied reasoning, hallucinations in highly dynamic scenarios, flawed reasoning / irrational steps, thought-action inconsistency, VLM-VLA misalignment (semantically valid high-level actions that the low-level VLA cannot parse), response latency of large commercial VLMs as practical bottlenecks, and reliance on short-term context windows (limited long-term memory) as a bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Documented failure modes: (1) hallucinations and flawed reasoning in highly dynamic scenes; (2) thought-action inconsistency where generated action diverges from preceding internal thought traces (observed and remedied with RL reward); (3) formatting errors and output mismatches; (4) VLM-VLA misalignment where semantically equivalent but differently phrased plans are unrecognizable to the VLA (example: 'put the Oreo into the shopping basket' vs 'put the biscuit box into the shopping basket'); (5) manipulation failures attributable to low-level controller limitations (handled by evaluating both human teleoperation and automatic VLA). Quantitative frequencies are not exhaustively enumerated, but ablation/metric drops quantify impact (see chain-of-thought and RL ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Explicit strategies: (a) continued pretraining on robot-specific and egocentric datasets to reduce domain shift; (b) synthetic simulation & AIGC images for rare items; (c) supervised finetuning on synthesized human-robot interaction data (diverse instructions incl. interruptions, invalid/ambiguous commands); (d) RL with reward shaping to improve robustness and thought-action alignment. Empirically Robix shows strong OOD gains (Robix-32B exceeds strongest baseline Gemini-2.5-Pro by 3.0 and 11.8 percentage points on two OOD settings; large online gaps vs backbones).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Paper reports improved OOD generalization but does not provide per-object SR for novel objects; reported figures relevant to novel/unseen content: Robix outperforms backbones and baselines on Internal OOD and AGIBot OOD tasks and shows large margins in online settings (e.g., Robix-32B vs Qwen2.5-VL-32B: 92.6% vs 28% average task progress in VLM-only online eval). Synthetic/filtered AIGC data used to expose model to novel items; 10% of generated AIGC images retained after filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Vision encoder is frozen during Stage 2 (instruction tuning) by design; the paper does not present a direct numeric ablation comparing fully frozen vs fully finetuned visual encoder end-to-end, so no explicit frozen vs. finetuned performance delta is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Multimodal joint transformer-style fusion (vision-language fusion during continued pretraining and instruction tuning). The paper emphasizes multimodal fusion learned from diverse supervision (grounding tasks, 3D spatial signals, thought traces) but does not provide low-level operator details beyond transformer multimodal fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Effective vision-language grounding for embodied tasks requires (1) dedicated continued pretraining on robot-relevant perception and 3D/spatial tasks, (2) supervised finetuning with synthesized multi-turn human-robot interaction and chain-of-thought traces to teach grounding + planning, and (3) RL with a thought-action consistency reward to reduce irrational steps and align internal reasoning with actions. Chain-of-thought supervision substantially improves OOD and open-ended instruction following; RL further improves thought-action consistency and reduces failures. Major remaining failure modes are hallucinations in dynamic scenes, VLM-VLA misalignment, and limited long-term memory/context capacity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1962.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1962.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Reasoning (thought-augmented supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training and inference technique where the model is supervised to produce concise internal reasoning traces (thoughts) that guide action planning; used in Robix as a core component to improve OOD generalization and complex instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought supervision (technique integrated into Robix SFT and pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT traces are synthesized via strong VLMs (Seed-1.5-VL-thinking and ActRe/Thought Bootstrapping) and appended to supervised finetuning data; traces emphasize scene understanding, task status reflection, next-step analysis, and long-term instruction persistence. During inference Robix generates concise thought traces (<=200 tokens) that condition subsequent plan/action outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>CoT improves grounding by forcing the model to explicitly represent scene-relevant object grounding, task milestones, and affordances in the intermediate reasoning trace; this intermediate representation tightens the alignment between language and visual features by supervised multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Conceptual/scene-level and object-centric grounding encoded in thought traces (references to operable objects, spatial relations, and held-object status).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Thoughts explicitly reference spatial relations, reachability, and object locations (relative/absolute) as part of reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Instruction following, long-horizon planning, task-status monitoring, interruption handling</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Internal ID/OOD benchmarks, AGIBot evaluation, real-world tasks used in Robix evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric and real-world images/videos used to generate CoT traces</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Plan/action prediction accuracy and downstream task progress</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation: removing CoT (Robix-7B-SFT-wo-R) caused >7 point drop on Internal OOD and 26.7 point drop on ID-OpenEnded tasks relative to CoT-enabled model.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Robix-7B-SFT-wo-R: >7 pp drop on Internal OOD; -26.7 pp on ID-OpenEnded compared to CoT-enabled SFT model.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>CoT yields large improvements for OOD and open-ended instruction following (tens of percentage points in some evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Without CoT, model is more prone to irrational/plausible-sounding but incorrect plans and fails to maintain long-term goal persistence.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>CoT ablation yields systematic failures on ambiguous or open-ended instructions and decreased ability to replan after interruptions; manifests as wrong next-action predictions and inability to persist constraints across long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>CoT helps generalize to OOD tasks by making reasoning explicit and thereby more robust to novel visual/contextual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Not a fusion operator per se; CoT is an explicit intermediate representation that conditions multi-modal fusion during supervised finetuning and RL.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Supervising concise, task-oriented chain-of-thought traces substantially improves visual grounding for long-horizon, interactive embodied tasks and is especially critical for OOD and open-ended instruction following; omission causes large accuracy drops.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1962.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1962.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thought-Action Consistency Reward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thought-Action Consistency Reward (RL auxiliary reward)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL reward signal that penalizes inconsistency between the model's generated internal thought trace and the subsequent emitted action, computed via an external LLM judge, used during GRPO reinforcement learning to align planning and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Thought-action consistency reward (component of RL training for Robix)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>At each decision step during RL, the model's thought and planned action are evaluated by an external LLM (Qwen-2.5-32B used as reward model) prompted to judge logical consistency; negative reward is applied for inconsistencies. Combined with standard rewards for task/action accuracy, this shapes policy updates to enforce coherence between internal reasoning and executed actions.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Indirect grounding improvement: enforces that actions correspond to visually-grounded thoughts (which mention objects, spatial relations and affordances), thereby reducing hallucinated/ungrounded actions and increasing plan verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Operates on language-level thought traces and textual action tokens that reference grounded visual entities; therefore ties language-level planning to visual grounding implicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Instruction following, long-horizon planning, interactive task execution</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used across supervised + RL training trajectories derived from teleoperation and synthesized datasets (AGIBot, internal teleoperation data, simulation data).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Training-time robot/egocentric data and simulation-generated scenes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Improvement in offline plan accuracy and reduction in irrational reasoning/formatting errors; measured as accuracy gains on Internal OOD and other evaluation sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Quantified improvements: Robix-7B-RL and Robix-32B-RL improve Internal OOD accuracy by +8.3 and +3.3 percentage points respectively over SFT counterparts; RL reduces irrational reasoning and thought-action inconsistency shown in case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Without RL (i.e., SFT only), models exhibited irrational reasoning and thought-action inconsistencies, with observed accuracy improvements after RL as above (+3.3 to +8.3 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>RL with the auxiliary consistency reward materially improves alignment of actions with grounded internal reasoning and reduces planning errors and formatting inconsistencies.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Thought-action inconsistency was identified as a major failure mode addressed by this reward; it was causing the model to propose actions that contradicted or failed to reflect previously stated visual observations/reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Case studies show SFT model proposing actions that mismatch preceding thoughts (e.g., thinking to go to sink but planning to navigate to cupboard); RL training with the reward corrects these behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Helps robustness under domain shift by enforcing internal reasoning fidelity regardless of unfamiliar visual input distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Reward is applied at language-output level; not a fusion mechanism but shapes grounded fusion indirectly by shaping the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Using a thought-action consistency reward during RL reduces irrational/un-grounded plans and improves downstream action accuracy; it is an effective mechanism to enforce that high-level language plans remain grounded in the observed visual scene.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1962.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1962.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLM-VLA misalignment (failure)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-Language Model (VLM) â€” Vision-Language-Action (VLA) Misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical failure mode where high-level plans produced by a VLM are semantically correct but phrased in ways the low-level VLA controller cannot parse or execute, causing execution failures despite correct reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VLM-VLA misalignment (observed failure mode in evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Observed during online VLM-VLA system tests: VLM outputs sometimes use language formulations or referents that the VLA controller (GR-3 or other VLA implementations) does not recognize; this leads to execution failures even though the high-level plan is semantically correct. Example: VLA can execute 'put the Oreo into the shopping basket' but fails on 'put the biscuit box into the shopping basket'.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Not a grounding mechanism but a mismatch at interface: high-level textual grounding must be expressed in a canonical action vocabulary understood by the VLA; divergence causes grounding-to-action execution gap.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Language/action-token level mismatch between plan vocabulary and VLA action interpreter; underlying visual grounding may be correct but mapping to VLA primitive tokens fails.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Object manipulation / mobile manipulation (real-world robot deployments)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Online VLM-VLA experiments using GR-3 and ByteMini robot; VLM-only online evaluation vs integrated VLM-VLA evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Real-world robot camera feeds and on-robot experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Contribution to drop in online task progress and execution failures; qualitative analysis and examples provided</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Attributed cause of notable online performance gap for some baselines (e.g., Gemini-2.5-Pro and GPT-4o saw drops when paired with VLA due to misalignment); Robix mitigated such issues via training and interface design and outperformed baselines in integrated system (Robix-32B: 92.5% avg task progress in VLM-VLA eval).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Improving the canonicalization/alignment between VLM output vocabulary and VLA parser is necessary to realize high-level grounding gains in end-to-end systems; Robix reports fewer such misalignment failures compared to some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>VLM-VLA parser mismatch is identified as a practical bottleneck that is separate from perceptual grounding per se but directly blocks execution; phrasing/labeling differences and formatting cause VLA rejection.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative examples and aggregated online results indicate this is a non-negligible source of failure; exact frequency not tabulated but cited as a main reason for online performance drop of certain baseline models when integrated with the VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Paper addresses this by (a) aligning action output formats in evaluation and training, (b) teacher-forcing history in offline tests, and (c) evaluating with a reliable human teleoperator to separate high-level cognition from low-level execution.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>High-level VLM grounding improvements must be paired with robust canonicalization and interface alignment to low-level VLA controllers; otherwise correct, grounded plans may still fail at execution time due to vocabulary/format mismatches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Qwen2.5-VL technical report <em>(Rating: 2)</em></li>
                <li>Gemini robotics: Bringing ai into the physical world. <em>(Rating: 2)</em></li>
                <li>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning <em>(Rating: 2)</em></li>
                <li>RoboBrain-2.0 technical report <em>(Rating: 2)</em></li>
                <li>Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. <em>(Rating: 2)</em></li>
                <li>Seed-1.5-VL technical report <em>(Rating: 2)</em></li>
                <li>Embodied-Reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks <em>(Rating: 1)</em></li>
                <li>Hi Robot: Open-ended instruction following with hierarchical vision-language-action models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1962",
    "paper_id": "paper-281080730",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "Robix",
            "name_full": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
            "brief_description": "A single vision-language model that serves as the high-level cognitive layer for hierarchical robot systems, integrating embodied visual grounding, chain-of-thought reasoning, task planning, human-robot interaction, and action emission to a low-level VLA controller; trained with continued pretraining, supervised finetuning with synthesized interaction+reasoning traces, and RL with a thought-action consistency reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Robix (Robix-7B / Robix-32B; Robix-*-SFT / Robix-*-RL)",
            "model_description": "Unified vision-language transformer (continued from Qwen2.5-VL backbone) trained end-to-end for interactive embodied tasks. Robix ingests egocentric/multi-view visual observations plus user utterances, produces internal chain-of-thought traces, and emits atomic action plans (textual action commands) and verbal responses. Grounding is learned by (a) continued pretraining on robot- and egocentric-focused corpora (3D spatial tasks, visual grounding datasets, task-centric reasoning examples), (b) supervised finetuning on synthesized human-robot interaction trajectories paired with chain-of-thought traces, and (c) RL (GRPO) with an auxiliary thought-action consistency reward assessed by an external LLM. At inference it retains the latest N visual observations (short-term visual memory) and stores prior thoughts/actions in short-term memory to respect context budgets.",
            "visual_encoder_type": "Vision encoder from Qwen2.5-VL backbone (multimodal transformer visual frontend); frozen during instruction-tuning stage (Stage 2)",
            "visual_encoder_pretraining": "Continued pretraining on ~200B tokens including: ~50M image-text pairs (instruction-tuned), and robot/egocentric datasets (AgiBot, BridgeData V2, Droid, Egodex, RoboVQA, HoloAssist, Ego4D) plus synthetic simulation/AIGC images for rare items; initial backbone is Qwen2.5-VL (pretrained prior to Robix continued pretraining).",
            "grounding_mechanism": "Learned vision-language fusion in a joint VLM: multimodal fusion during continued pretraining and instruction tuning (model uses multimodal transformer-style fusion to align visual embeddings with language tokens); explicit supervision for visual grounding via bounding-box / point / 3D bbox tasks and thought-augmented labels; final RL stage adds reward for alignment between the model's internal thought traces and emitted action (thought-action consistency).",
            "representation_level": "Multi-level: supports pixel/patch-level and region/box-level grounding as well as 3D bounding-box outputs and multi-view correspondence (the model is trained and evaluated on point detection, 2D bounding boxes, 3D bboxes and depth/egomotion tasks).",
            "spatial_representation": "Explicit spatial signals included in pretraining and tasks: 2D bounding boxes, normalized point coordinates, 3D bounding boxes, relative and absolute depth estimates, and multi-view correspondence / egomotion; the model is trained on these modalities but does not expose a separate explicit 3D world map at inference (uses recent frames + spatial outputs).",
            "embodied_task_type": "Object manipulation / instruction following / mobile manipulation / 3D scene understanding / interactive task planning",
            "embodied_task_name": "AGIBot Evaluation Set; Agibot-ER benchmark (Task Status Verification, Action Affordance, Next Action Prediction); Internal In-Distribution (ID) and Out-of-Distribution (OOD) benchmarks; visual grounding benchmarks (LVIS-MG, RefCOCO, Pixmo-Point, Where2Place); real-world tasks: table bussing, checkout packing, dietary filtering, grocery shopping, tableware organization & shipment.",
            "visual_domain": "Mixed: egocentric views, real-world robot camera feeds, photorealistic simulation scenes, and synthetic/text-to-image created scenes for rare objects.",
            "performance_metric": "Offline: plan/action prediction accuracy (match to candidate action list), F1 for invalid/replan categories, standard VQA/grounding metrics (F1, accuracy). Online: task progress (percentage of subtasks completed), task completion percentages, standard dataset metrics for grounding/VQA.",
            "performance_value": "Representative numbers reported in paper: Robix-32B-RL achieves top rank across offline evaluation sets; in VLM-only online evaluation Robix-32B achieves average task progress 92.6% (vs. Gemini-2.5-Pro 91% and Qwen2.5-VL-32B 28%); in VLM-VLA integrated online evaluation Robix-32B achieves average task progress 92.5%, exceeding Gemini-2.5-Pro and GPT-4o by 4.3 and 28.1 percentage points respectively. On visual grounding, Robix-7B and Robix-32B improved absolute F1 on LVIS-MG by +39.6 and +25.0 points over their Qwen2.5-VL backbones; 3D spatial benchmarks: Robix-7B/32B averaged accuracies 73.4% / 75.8% versus 66.9% / 70.7% for Qwen2.5-VL.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation of chain-of-thought reasoning (Robix-7B-SFT-wo-R) produced &gt;7 percentage point drop on Internal OOD benchmarks and a 26.7-point drop on ID-OpenEnded tasks compared to reasoning-enabled SFT; RL ablation: SFT -&gt; RL gains (Robix-7B-RL and Robix-32B-RL improved Internal OOD accuracy by +8.3 and +3.3 points respectively vs their SFT counterparts).",
            "grounding_improvement": "Chain-of-thought supervision and grounding-focused pretraining yield large gains for OOD and open-ended instruction following (e.g., -26.7 points when CoT omitted on ID-OpenEnded). Continued pretraining and SFT over backbone deliver large improvements on grounding benchmarks (e.g., LVIS-MG F1 +39.6/+25.0 for 7B/32B). RL further improves thought-action consistency and reduces irrational reasoning (+3.3 to +8.3 points on Internal OOD vs SFT).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Comparison primarily vs. Robix's backbone Qwen2.5-VL (same encoder family): Robix consistently outperforms Qwen2.5-VL on visual grounding and spatial benchmarks (e.g., LVIS-MG F1 +39.6 / +25.0 for Robix-7B / Robix-32B over Qwen2.5-VL-7B / -32B; 3D spatial average accuracy +6.5 and +5.1 points for 7B/32B). No wide multi-architecture encoder sweep is reported; vision encoder is frozen during instruction-tuning stage.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors explicitly identify limited embodied reasoning, hallucinations in highly dynamic scenarios, flawed reasoning / irrational steps, thought-action inconsistency, VLM-VLA misalignment (semantically valid high-level actions that the low-level VLA cannot parse), response latency of large commercial VLMs as practical bottlenecks, and reliance on short-term context windows (limited long-term memory) as a bottleneck.",
            "failure_mode_analysis": "Documented failure modes: (1) hallucinations and flawed reasoning in highly dynamic scenes; (2) thought-action inconsistency where generated action diverges from preceding internal thought traces (observed and remedied with RL reward); (3) formatting errors and output mismatches; (4) VLM-VLA misalignment where semantically equivalent but differently phrased plans are unrecognizable to the VLA (example: 'put the Oreo into the shopping basket' vs 'put the biscuit box into the shopping basket'); (5) manipulation failures attributable to low-level controller limitations (handled by evaluating both human teleoperation and automatic VLA). Quantitative frequencies are not exhaustively enumerated, but ablation/metric drops quantify impact (see chain-of-thought and RL ablations).",
            "domain_shift_handling": "Explicit strategies: (a) continued pretraining on robot-specific and egocentric datasets to reduce domain shift; (b) synthetic simulation & AIGC images for rare items; (c) supervised finetuning on synthesized human-robot interaction data (diverse instructions incl. interruptions, invalid/ambiguous commands); (d) RL with reward shaping to improve robustness and thought-action alignment. Empirically Robix shows strong OOD gains (Robix-32B exceeds strongest baseline Gemini-2.5-Pro by 3.0 and 11.8 percentage points on two OOD settings; large online gaps vs backbones).",
            "novel_object_performance": "Paper reports improved OOD generalization but does not provide per-object SR for novel objects; reported figures relevant to novel/unseen content: Robix outperforms backbones and baselines on Internal OOD and AGIBot OOD tasks and shows large margins in online settings (e.g., Robix-32B vs Qwen2.5-VL-32B: 92.6% vs 28% average task progress in VLM-only online eval). Synthetic/filtered AIGC data used to expose model to novel items; 10% of generated AIGC images retained after filtering.",
            "frozen_vs_finetuned": "Vision encoder is frozen during Stage 2 (instruction tuning) by design; the paper does not present a direct numeric ablation comparing fully frozen vs fully finetuned visual encoder end-to-end, so no explicit frozen vs. finetuned performance delta is reported.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Multimodal joint transformer-style fusion (vision-language fusion during continued pretraining and instruction tuning). The paper emphasizes multimodal fusion learned from diverse supervision (grounding tasks, 3D spatial signals, thought traces) but does not provide low-level operator details beyond transformer multimodal fusion.",
            "sample_efficiency": null,
            "key_findings_grounding": "Effective vision-language grounding for embodied tasks requires (1) dedicated continued pretraining on robot-relevant perception and 3D/spatial tasks, (2) supervised finetuning with synthesized multi-turn human-robot interaction and chain-of-thought traces to teach grounding + planning, and (3) RL with a thought-action consistency reward to reduce irrational steps and align internal reasoning with actions. Chain-of-thought supervision substantially improves OOD and open-ended instruction following; RL further improves thought-action consistency and reduces failures. Major remaining failure modes are hallucinations in dynamic scenes, VLM-VLA misalignment, and limited long-term memory/context capacity.",
            "uuid": "e1962.0"
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought Reasoning (thought-augmented supervision)",
            "brief_description": "A training and inference technique where the model is supervised to produce concise internal reasoning traces (thoughts) that guide action planning; used in Robix as a core component to improve OOD generalization and complex instruction following.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought supervision (technique integrated into Robix SFT and pretraining)",
            "model_description": "CoT traces are synthesized via strong VLMs (Seed-1.5-VL-thinking and ActRe/Thought Bootstrapping) and appended to supervised finetuning data; traces emphasize scene understanding, task status reflection, next-step analysis, and long-term instruction persistence. During inference Robix generates concise thought traces (&lt;=200 tokens) that condition subsequent plan/action outputs.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "CoT improves grounding by forcing the model to explicitly represent scene-relevant object grounding, task milestones, and affordances in the intermediate reasoning trace; this intermediate representation tightens the alignment between language and visual features by supervised multi-step reasoning.",
            "representation_level": "Conceptual/scene-level and object-centric grounding encoded in thought traces (references to operable objects, spatial relations, and held-object status).",
            "spatial_representation": "Thoughts explicitly reference spatial relations, reachability, and object locations (relative/absolute) as part of reasoning traces.",
            "embodied_task_type": "Instruction following, long-horizon planning, task-status monitoring, interruption handling",
            "embodied_task_name": "Internal ID/OOD benchmarks, AGIBot evaluation, real-world tasks used in Robix evaluation",
            "visual_domain": "Egocentric and real-world images/videos used to generate CoT traces",
            "performance_metric": "Plan/action prediction accuracy and downstream task progress",
            "performance_value": "Ablation: removing CoT (Robix-7B-SFT-wo-R) caused &gt;7 point drop on Internal OOD and 26.7 point drop on ID-OpenEnded tasks relative to CoT-enabled model.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Robix-7B-SFT-wo-R: &gt;7 pp drop on Internal OOD; -26.7 pp on ID-OpenEnded compared to CoT-enabled SFT model.",
            "grounding_improvement": "CoT yields large improvements for OOD and open-ended instruction following (tens of percentage points in some evaluations).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Without CoT, model is more prone to irrational/plausible-sounding but incorrect plans and fails to maintain long-term goal persistence.",
            "failure_mode_analysis": "CoT ablation yields systematic failures on ambiguous or open-ended instructions and decreased ability to replan after interruptions; manifests as wrong next-action predictions and inability to persist constraints across long horizons.",
            "domain_shift_handling": "CoT helps generalize to OOD tasks by making reasoning explicit and thereby more robust to novel visual/contextual inputs.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Not a fusion operator per se; CoT is an explicit intermediate representation that conditions multi-modal fusion during supervised finetuning and RL.",
            "sample_efficiency": null,
            "key_findings_grounding": "Supervising concise, task-oriented chain-of-thought traces substantially improves visual grounding for long-horizon, interactive embodied tasks and is especially critical for OOD and open-ended instruction following; omission causes large accuracy drops.",
            "uuid": "e1962.1"
        },
        {
            "name_short": "Thought-Action Consistency Reward",
            "name_full": "Thought-Action Consistency Reward (RL auxiliary reward)",
            "brief_description": "An RL reward signal that penalizes inconsistency between the model's generated internal thought trace and the subsequent emitted action, computed via an external LLM judge, used during GRPO reinforcement learning to align planning and actions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Thought-action consistency reward (component of RL training for Robix)",
            "model_description": "At each decision step during RL, the model's thought and planned action are evaluated by an external LLM (Qwen-2.5-32B used as reward model) prompted to judge logical consistency; negative reward is applied for inconsistencies. Combined with standard rewards for task/action accuracy, this shapes policy updates to enforce coherence between internal reasoning and executed actions.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Indirect grounding improvement: enforces that actions correspond to visually-grounded thoughts (which mention objects, spatial relations and affordances), thereby reducing hallucinated/ungrounded actions and increasing plan verifiability.",
            "representation_level": "Operates on language-level thought traces and textual action tokens that reference grounded visual entities; therefore ties language-level planning to visual grounding implicitly.",
            "spatial_representation": null,
            "embodied_task_type": "Instruction following, long-horizon planning, interactive task execution",
            "embodied_task_name": "Used across supervised + RL training trajectories derived from teleoperation and synthesized datasets (AGIBot, internal teleoperation data, simulation data).",
            "visual_domain": "Training-time robot/egocentric data and simulation-generated scenes",
            "performance_metric": "Improvement in offline plan accuracy and reduction in irrational reasoning/formatting errors; measured as accuracy gains on Internal OOD and other evaluation sets",
            "performance_value": "Quantified improvements: Robix-7B-RL and Robix-32B-RL improve Internal OOD accuracy by +8.3 and +3.3 percentage points respectively over SFT counterparts; RL reduces irrational reasoning and thought-action inconsistency shown in case studies.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Without RL (i.e., SFT only), models exhibited irrational reasoning and thought-action inconsistencies, with observed accuracy improvements after RL as above (+3.3 to +8.3 pp).",
            "grounding_improvement": "RL with the auxiliary consistency reward materially improves alignment of actions with grounded internal reasoning and reduces planning errors and formatting inconsistencies.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Thought-action inconsistency was identified as a major failure mode addressed by this reward; it was causing the model to propose actions that contradicted or failed to reflect previously stated visual observations/reasoning.",
            "failure_mode_analysis": "Case studies show SFT model proposing actions that mismatch preceding thoughts (e.g., thinking to go to sink but planning to navigate to cupboard); RL training with the reward corrects these behaviors.",
            "domain_shift_handling": "Helps robustness under domain shift by enforcing internal reasoning fidelity regardless of unfamiliar visual input distributions.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Reward is applied at language-output level; not a fusion mechanism but shapes grounded fusion indirectly by shaping the policy.",
            "sample_efficiency": null,
            "key_findings_grounding": "Using a thought-action consistency reward during RL reduces irrational/un-grounded plans and improves downstream action accuracy; it is an effective mechanism to enforce that high-level language plans remain grounded in the observed visual scene.",
            "uuid": "e1962.2"
        },
        {
            "name_short": "VLM-VLA misalignment (failure)",
            "name_full": "Vision-Language Model (VLM) â€” Vision-Language-Action (VLA) Misalignment",
            "brief_description": "A practical failure mode where high-level plans produced by a VLM are semantically correct but phrased in ways the low-level VLA controller cannot parse or execute, causing execution failures despite correct reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "VLM-VLA misalignment (observed failure mode in evaluations)",
            "model_description": "Observed during online VLM-VLA system tests: VLM outputs sometimes use language formulations or referents that the VLA controller (GR-3 or other VLA implementations) does not recognize; this leads to execution failures even though the high-level plan is semantically correct. Example: VLA can execute 'put the Oreo into the shopping basket' but fails on 'put the biscuit box into the shopping basket'.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Not a grounding mechanism but a mismatch at interface: high-level textual grounding must be expressed in a canonical action vocabulary understood by the VLA; divergence causes grounding-to-action execution gap.",
            "representation_level": "Language/action-token level mismatch between plan vocabulary and VLA action interpreter; underlying visual grounding may be correct but mapping to VLA primitive tokens fails.",
            "spatial_representation": null,
            "embodied_task_type": "Object manipulation / mobile manipulation (real-world robot deployments)",
            "embodied_task_name": "Online VLM-VLA experiments using GR-3 and ByteMini robot; VLM-only online evaluation vs integrated VLM-VLA evaluation",
            "visual_domain": "Real-world robot camera feeds and on-robot experiments",
            "performance_metric": "Contribution to drop in online task progress and execution failures; qualitative analysis and examples provided",
            "performance_value": "Attributed cause of notable online performance gap for some baselines (e.g., Gemini-2.5-Pro and GPT-4o saw drops when paired with VLA due to misalignment); Robix mitigated such issues via training and interface design and outperformed baselines in integrated system (Robix-32B: 92.5% avg task progress in VLM-VLA eval).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Improving the canonicalization/alignment between VLM output vocabulary and VLA parser is necessary to realize high-level grounding gains in end-to-end systems; Robix reports fewer such misalignment failures compared to some baselines.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "VLM-VLA parser mismatch is identified as a practical bottleneck that is separate from perceptual grounding per se but directly blocks execution; phrasing/labeling differences and formatting cause VLA rejection.",
            "failure_mode_analysis": "Qualitative examples and aggregated online results indicate this is a non-negligible source of failure; exact frequency not tabulated but cited as a main reason for online performance drop of certain baseline models when integrated with the VLA.",
            "domain_shift_handling": "Paper addresses this by (a) aligning action output formats in evaluation and training, (b) teacher-forcing history in offline tests, and (c) evaluating with a reliable human teleoperator to separate high-level cognition from low-level execution.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "High-level VLM grounding improvements must be paired with robust canonicalization and interface alignment to low-level VLA controllers; otherwise correct, grounded plans may still fail at execution time due to vocabulary/format mismatches.",
            "uuid": "e1962.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Qwen2.5-VL technical report",
            "rating": 2
        },
        {
            "paper_title": "Gemini robotics: Bringing ai into the physical world.",
            "rating": 2
        },
        {
            "paper_title": "Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning",
            "rating": 2
        },
        {
            "paper_title": "RoboBrain-2.0 technical report",
            "rating": 2
        },
        {
            "paper_title": "Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems.",
            "rating": 2
        },
        {
            "paper_title": "Seed-1.5-VL technical report",
            "rating": 2
        },
        {
            "paper_title": "Embodied-Reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks",
            "rating": 1
        },
        {
            "paper_title": "Hi Robot: Open-ended instruction following with hierarchical vision-language-action models",
            "rating": 1
        }
    ],
    "cost": 0.02313175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Robix: A Unified Model for Robot Interaction, Reasoning and Planning
September 12, 2025</p>
<p>Huang Fang 
Mengxi Zhang 
Heng Dong 
Wei Li 
Zixuan Wang 
Qifeng Zhang 
Xueyun Tian 
Yucheng Hu 
Hang Li 
â€  Bytedance Seed 
Equal Contribution 
Project Lead 
Robix: A Unified Model for Robot Interaction, Reasoning and Planning
September 12, 2025DB5CB38B9778FF4CE460B505C1816AADarXiv:2509.01106v2[cs.AI]Okay. Proactive Dialogue Monitoring &amp; Replanning Realtime Interruption Memorization &amp; Verification Task Status Verification Human Instruction Embodied Reasoning Low-level Action Robot Response Visual Observation
We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture.Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework.Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution.At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy:(1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence.Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.</p>
<p>Introduction</p>
<p>The goal of generalist robots is to assist humans in diverse daily tasks within open, dynamic environments.Realizing this vision requires more than executing isolated commands-it demands the ability to engage in natural human interaction and reason through complex, long-horizon tasks.For example, when cleaning a dining table, a robot must not only recognize dishes and utensils, but also interpret nuanced instructions such as "only clear the plates if people are finished eating", respond to corrections like "leave that glass", and adapt to novel scenarios such as properly sorting stacked tableware.To meet these requirements, a general-purpose robot system should adopt a hierarchical architecture in which a high-level cognitive layer handles complex multimodal reasoning, adaptive task planning, and natural human-robot interaction, while a Human: I'm hungry.Please put the highest-calorie food on the table into the plastic box and fetch me a drink.</p>
<p>Human: I'm allergic to caffeine.</p>
<p>Think: The user requests the highestcalorie food along with a drink.There are various types of food and drinks, including hamburger, drumstick, coke, sprite, orange juice, and coffee.Usually, a hamburger contains more calories than a drumstick, and coke is a suitable pairing with hamburgerâ€¦ Action: put the hamburger into the white plastic box</p>
<p>Response: Okay.</p>
<p>Think: I've successfully placed the hamburger into the white plastic box.</p>
<p>The user also wants a drink, so next I'll put the coke into the plastic box as well.</p>
<p>Action: put the coke into the white plastic box</p>
<p>Think: I'm currently holding the coke with the left gripper, but the user just informed me that they're allergic to caffeine.Since coke typically contains caffeine, I need to place it back on the table and select a caffeine-free drink instead.</p>
<p>Action: place the coke back on the table</p>
<p>Response: Got it.</p>
<p>Think: â€¦Since the user is allergic to caffeine, and both coke and coffee contain caffeine, the remaining options are orange juice and Sprite, which are typically caffeine-free.</p>
<p>Orange juice is generally considered the healthier choice, so I will offer that to the userâ€¦ Action: put the orange juice into the white plastic box</p>
<p>Human: Next, tidy up the entire table.Throw all the trash into the trash bin, including the caffeinated drinks.Put everything else into the white plastic box.</p>
<p>Think: I've successfully placed the highest-calorie hamburger and the caffeine-free orange juice into the plastic box, completing the user's task instruction.Next, I can ask the user if there's anything else they need me to do.</p>
<p>Think: The user asked me to throw away all trash and caffeinated drinks, and place the remaining items into the plastic box.I will start by discarding the coke and coffee, then throw away common trash items like napkins and paper cups.Finally, I'll place the utensils into the plastic box...</p>
<p>Action: put the coke into the rubbish bin</p>
<p>Think: I've successfully thrown the napkin into the trash bin.Next, I can start tidying up the utensils.There is a fork and a knife on the plate.To make the subsequent steps easier, I need to put them away first.I will begin by placing the fork, which is on top, into the plastic box.</p>
<p>Think: I noticed that the fork is still on the table, indicating that the previous action failed.I should try again and place the fork into the plastic box first.</p>
<p>Action: put the fork into the white plastic box</p>
<p>Think: The fork has already been successfully placed into the plastic box.There is still a knife on the plate, so next I will place the knife into the plastic box as well.</p>
<p>Think: â€¦Currently, there is still a chicken drumstick on a green plate and a bottle of sprite on the table.The drumstick is food and typically should either be packed or discarded, but the user hasn't specified.I should ask the user whether the drumstick should be thrown away as trash.</p>
<p>Think: The user asked me to throw away the chicken drumstick on the green plate.So next, I should discard the drumstick into the trash bin, and then place the plate into the plastic box.</p>
<p>Action: put the chicken drumstick into the rubbish bin Think: All items on the table have been cleared.As instructed by the user, the caffeinated drinks and trash have been thrown into the bin, the drumstick was also discarded, and all other items have been placed into the plastic box.The task is now complete.I can ask the user if there's anythingâ€¦</p>
<p>Response: I've cleaned up the table as you instructed.Is there anything else you'd like me to do?</p>
<p>Response: The task has been completed.Is there anything else I can help with?</p>
<p>Response: Alright, I'll clean it up right away.low-level controller layer executes the atomic motor actions issued by the high-level layer.This division of responsibilities allows the robot to reason at a macro level while acting at a micro level, enabling human-like adaptability in real-world scenarios.</p>
<p>Existing hierarchical approaches typically employ large language models (LLMs) or vision language models (VLMs) as the high-level cognitive layer for task planning, which decompose long-horizon tasks into executable subtasks for the low-level controller [1,8,17,29,53,60,84,88].However, these methods focus solely on task decomposition, overlooking human-robot interaction and embodied reasoning, which are essential for general-purpose robotic systems.Taking one step further, recent work [11] constructs modular pipelines that combine reasoning, planning, and interaction through hand-designed workflows.While workflow-based systems are easy to develop, their inflexibility and brittleness remain notable limitations-rooted primarily in rigid modularization and over-reliance on hand-engineered designs.In this work, we introduce Robix, a unified high-level cognitive layer that seamlessly integrates reasoning, task planning, and natural language interaction within a single model.Unlike modular frameworks, Robix adopts an end-to-end vision-language architecture natively designed for interactive task execution.At its core, Robix leverages chain-of-thought reasoning and formulates interactive task execution as a unified reasoning-action sequence, effectively functioning as the "brain" of a generalist robot system.Figure 1 illustrates Robix in an interactive table-organization task, demonstrating flexible capabilities such as understanding complex instructions, handling real-time interruptions, monitoring task progress, and engaging in proactive dialogue to clarify ambiguous commands or infer user intent.</p>
<p>Modeling such complex interactive task execution within a single VLM is challenging.Although general VLMs have achieved strong performance in digital domains, extending them to physical robots is far more demanding: robots must continuously perceive and act in dynamic environments, interpret ambiguous instructions, adapt to real-time feedback, and make sequential decisions under strict physical and temporal constraints.Addressing this gap requires overcoming two major limitations of existing models: (1) limited embodied reasoning-the ability to ground objects and spatial concepts in the physical world and integrate these signals for adaptive planning and task-centric reasoning [64]; (2) lack of flexible multimodal interaction-hindered both by its inherent complexity and by the scarcity of corresponding training data.</p>
<p>To address these challenges, Robix is trained with a three-stage strategy:</p>
<p>â€¢ Continued pretraining on general VLMs to enhance foundational embodied reasoning capabilities.We curate a large-scale dataset covering various robot-relevant tasks, such as 3D spatial understanding, visual grounding, and task-centric reasoning, enabling the model to strengthen its grounded planning and reasoning abilities.</p>
<p>â€¢ Supervised finetuning to endow the model with complex interactive capabilities.We employ comprehensive data synthesis to incorporate chain-of-thought reasoning and model interactive task execution as a unified reasoning-action sequence.The synthetic data covers a full spectrum of capabilities, including complex instruction understanding, long-horizon planning, task status monitoring, dynamic replanning, real-time interruption handling, and human-robot dialogue.</p>
<p>â€¢ Reinforcement learning to further refine the reasoning ability and strengthen the consistency between reasoning and actions, particularly in long-horizon, interactive tasks.</p>
<p>We comprehensively evaluate Robix on embodied reasoning and interactive task execution.Across 31 benchmarks covering robot-relevant abilities (3D spatial understanding, visual grounding, task-centric reasoning) and general-purpose skills (general VQA, multimodal reasoning), Robix achieves obvious improvements on most robot-relevant tasks while maintaining strong general-purpose performance.On a curated interactive-task benchmark spanning in-distribution and out-of-distribution (OOD) settings and diverse instruction types (multi-stage, constrained, open-ended, invalid, interrupted), Robix consistently outperforms commercial (e.g., GPT-4o, Gemini-2.5-Pro)and open-source (e.g., Qwen2.5-VL,RoboBrain-2.0)baselines; notably, Robix-32B exceeds the strongest baseline, Gemini-2.5-Pro,by 3.0 and 11.8 percentage points in accuracy on the two OOD settings.We further assess five real-world scenarios-table bussing, grocery shopping, checkout packing, tableware organization &amp; shipment, and dietary filtering-using task-progress metrics in a hierarchical robot system under two low-level control modes (human teleoperation and an automatic VLA controller).Across Please give me the highest-calorie food and fetch a bottle of drink.</p>
<p>I'm allergic to caffeine.</p>
<p>Observation 1 Observation 2 Observation 3 Human Interruption</p>
<p>Okay, no problem.Got it.</p>
<p>Response</p>
<p>â€¦</p>
<p>Figure 2</p>
<p>Illustration of the hierarchical robot system.Robix serves as the high-level cognitive layer, interpreting tasks and reasoning over multimodal inputs to generate language responses and action plans.The low-level controller layer executes the corresponding atomic commands, enabling interaction with both humans and the physical environment.</p>
<p>both modes, Robix-32B surpasses Gemini-2.5-Proby 1.6 and 4.3 percentage points on task progress and markedly outperforms all other baselines by 28.1 âˆ¼ 64.6 percentage points.Our experiments demonstrate that Robix couples strong embodied reasoning with flexible high-level planning and interaction, advancing toward general-purpose embodied intelligence.</p>
<p>We summarize Robix's main features as follows:</p>
<p>â€¢ Unified model.Robix is a single vision-language model that unifies robot reasoning, task planning, and human-robot interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally in an end-to-end manner.</p>
<p>â€¢ Flexible interaction.Within this unified framework, Robix supports proactive dialogue to clarify ambiguity or infer user intent, real-time interruption handling that seamlessly incorporates feedback, and context-aware commonsense reasoning for complex, open-ended tasks.</p>
<p>â€¢ Robust performance.We assess Robix in two setups: (i) on a curated interactive-task benchmark covering both in-and out-of-distribution scenarios with diverse instruction types, and (ii) across five real-world scenarios in a hierarchical robot system with both human teleoperation and an automatic VLA model as the low-level controller.These evaluations demonstrate that Robix consistently delivers strong performance across all settings.</p>
<p>The Robix Model</p>
<p>Figure 2 illustrates the hierarchical robot system, where Robix serves as the high-level cognitive layer responsible for planning and interaction.The low-level controller-typically implemented as a vision-languageaction (VLA) model-executes the atomic commands generated by Robix, enabling the robot system to directly interact with the physical environment.</p>
<p>At each iteration, Robix directly processes visual observations from robot-mounted cameras and user utterances, selectively producing atomic action commands for the low-level controller and appropriate verbal responses.This iterative reasoning-action loop allows Robix to perform deliberate reasoning and generate contextually grounded behaviors.The sequential decision-making process can be formally modeled as:
P t n , a n , r n (o 1 , u 1 , t 1 , a 1 , r 1 ), . . . , [(o nâˆ’i , u nâˆ’i , t nâˆ’i , a nâˆ’i , r nâˆ’i )] N i=1 , o n , u n .(1)
Here, each step involves predicting the next thought t n , action a n , and optional verbal response r n , conditioned on the current observations o n , optional user instruction u n and the interaction history.These intermediate</p>
<p>Visual Grounding Count</p>
<p>Qï¼šPlease count the gloves in the image.Aï¼š<point>(x,y)<point>â€¦<point>(x,y)<point>The total number is 31.</p>
<p>Visual Prompt</p>
<p>Qï¼šWhat object is enclosed with the green polygon?Aï¼šIt is the tire of the motorcycle.</p>
<p>Embodied Task-centric Reasoning General Multimodal Understanding &amp; Reasoning</p>
<p>Q: How to get Chrome on my phone?</p>
<p>A: <think>â€¦ There is a QR code on the right side of the screenâ€¦</think> Scan the QR code on the right side using your phone to download Chrome.</p>
<p>Qï¼šWhat is the total volume of the measuring cup?Aï¼š1000.</p>
<p>â€¦</p>
<p>3D Bounding Box Detection</p>
<p>Qï¼šDetect every chair inside the image and report each as a 3D bounding box.Aï¼š3dbbox: &lt;3dbbox&gt;0.743.57 0.53 0.82 0.99 0.61 -1.57-0.00 0.29&lt;/3dbbox&gt;â€¦</p>
<p>Multi-view Correspondence</p>
<p>Qï¼šWhere is the position corresponding to <point>(566, 224)</point> in the first frame located in the second frame?Aï¼š <point>(608, 236)</point></p>
<p>Relative Depth Sorting</p>
<p>Qï¼šBetween point1: <point>(737, 621)&lt;/ point&gt; and point2: <point>(255, 820)</point>, which one is spatially closer to us? Aï¼špoint2 is closer</p>
<p>Absolute Depth Estimation</p>
<p>Q: Provide the distance between the observer and the center of monitor (red point) in meters.</p>
<p>A: The monitor (red point) is approximately 2.0 meters away from your current position.</p>
<p>Egomotion Prediction</p>
<p>Qï¼šHow is the camera rotating between frame  thoughts provide a structured reasoning trace that guides decision-making and enables nuanced, context-aware interaction with humans and the environment.To balance memory usage and maintain inference efficiency under token budget constraints (e.g., 32k context length), we retain only the latest N visual observations as explicit input.The full sequence of prior thoughts and actions is stored in short-term memory, allowing Robix to reason over recent history without exceeding capacity limits.</p>
<p>As shown in Figure 1, Robix unifies the entire interactive task execution process-including instruction understanding, task planning, task status monitoring, real-time user feedback integration, proactive dialogue, and dynamic replanning-through grounded, multi-faceted reasoning.Unlike prior modular frameworks for task planning or human-robot interaction, Robix offers significantly greater flexibility, allowing the robot to adapt its behavior in real time to dynamic environmental changes, thereby achieving human-like adaptability.</p>
<p>Training Recipe</p>
<p>We develop Robix-7B and Robix-32B by continually training Qwen2.5-VL-7B and 32B [3] on approximately 200 billion tokens using a three-stage training pipeline.First, we perform continued pretraining to enhance the model's capabilities in robot-relevant perception and reasoning (Section 3.1).Next, we apply supervised finetuning to model the complex human-robot interaction and long-horizon task planning as a sequential decision-making process grounded in chain-of-thought reasoning (Section 3.2).Finally, we leverage reinforcement learning to further improve the embodied reasoning ability and enhance the alignment between reasoning and action in interactive long-horizon tasks (Section 3.3).</p>
<p>Continued Pretraining</p>
<p>A foundational capability of general-purpose embodied models is embodied reasoning-the ability to ground objects and spatial concepts in the physical world and integrate these signals for downstream robotic tasks [64].</p>
<p>Our objective is to develop a vision-language model with embodied reasoning at its core, capable of generalizing across diverse embodied scenarios while maintaining strong foundational multimodal understanding.To support this, we construct a large-scale pretraining corpus comprising 200 billion high-quality and diverse tokens, targeting both robot-relevant and general-purpose multimodal capabilities, as illustrated in Figure 3.</p>
<p>In particular, we emphasize 3D spatial understanding, visual grounding, and task-centric reasoning, while also incorporating general visual understanding, multimodal reasoning, and instruction tuning data.Below, we detail the data sources and task types used in our continued pretraining.), enabling the model to learn multimodal fusion and context-aware understanding grounded in visual cues.Together, these tasks significantly enhance the model's grounding abilities in both language-to-image and image-to-language directions, and improve its capacity for grounded planning in embodied settings.</p>
<p>Task-centric Reasoning.To directly strengthen the model's reasoning and planning abilities in embodied scenarios, we construct a large-scale embodied task-centric reasoning dataset based on publicly available robot and egocentric datasets, including AgiBot [6], BridgeData V2 [67], Droid [32], Egodex [27], RoboVQA [52], HoloAssist [70], and Ego4D [22].We curate over 5 million examples (about 10B tokens) targeting three key reasoning functions: (1) Task Status Verification-determining whether a task or subtask has been successfully completed;</p>
<p>(2) Action Affordance-assessing whether an action is feasible in the current context; (3) Next Action Prediction-identifying the most plausible next step to achieve the intended goal.To enrich the reasoning process, we further use Seed-1.5-VL-thinking[24] to generate step-by-step thought traces for our QA pairs via carefully designed prompts (details are shown in Section B).This thought-augmented supervision enables the model to learn deliberate, high-level decision-making in dynamic and open-ended environments.General Multimodal Understanding.To preserve and enhance broad vision-language understanding, we curate a large-scale dataset of over 50 million image-text pairs (over 80B tokens) that serve as the foundation for multimodal comprehension.We refine the instructions using Seed-1.5-VL[24] for quality filtering and ensure better alignment between instructions, images, and responses.This instruction-tuned dataset significantly improves the model's ability to follow open-world multimodal instructions and engage in multi-turn, grounded reasoning.</p>
<p>We adopt a two-stage training strategy leveraging the large-scale, diverse corpus described above.In stage 1, we continue pretraining Qwen2.5-VL[3] on the full dataset-comprising approximately 5% text-only data-updating all model parameters to enhance general multimodal and embodied reasoning capabilities.</p>
<p>Training follows a full cosine learning rate schedule, starting at 1 Ã— 10 âˆ’5 and decaying to 1 Ã— 10 âˆ’6 , with linear warm-up over the first 10% of total steps.We use a sequence length of 32,768 tokens, with effective batch sizes of 1536Ã— and 3008Ã— the sequence length for the 7B and 32B models, respectively.In stage 2, we perform instruction tuning on curated instruction-following data to align the model with multimodal prompts and improve instruction adherence.The vision encoder is frozen during this phase, while all other parameters remain trainable.The learning rate is fixed to the final value from Stage 1 (1 Ã— 10 âˆ’6 ) and remains constant throughout Stage 2. Optimizer states are carried over from Stage 1, and no additional warm-up is used.Both stages are optimized using AdamW [33,41], with Î² 1 = 0.9, Î² 2 = 0.99, and a weight decay of 0.01.Training on this diverse and comprehensive corpus significantly improves the model's embodied reasoning, multimodal understanding, and its ability to generalize to long-horizon, interactive tasks in real-world settings.</p>
<p>Supervised Finetuning</p>
<p>The supervised fine-tuning (SFT) stage adapts the preceding pretrained model into the robot's high-level cognitive module while retaining its original capabilities.A central challenge lies in the scarcity of largescale, multi-turn egocentric-vision datasets that integrate human-robot interaction with task planning.</p>
<p>To address this, we design a data-synthesis pipeline that transforms existing task-planning datasets into human-robot interaction trajectories.Two properties of the resulting SFT data are crucial for out-ofdistribution generalization: (1) diverse human-robot interactions and (2) high-quality reasoning traces.The overall pipeline is shown in Figure 4; the interaction and reasoning synthesis modules are detailed below.</p>
<p>Interaction Synthesis</p>
<p>The interaction synthesis is mainly based on two data sources:</p>
<p>â€¢ Teleoperated Robot Demonstrations.We utilize both internal teleoperation data (previously used to train GR-3 [7]) and the open-source AGIBot dataset [6].Each contains episodes of robots performing diverse tasks (e.g., table bussing, breakfast preparation).Human annotators segment each demonstration into clips, where each clip corresponds to an atomic action-e.g., "put the tissue into the trash bin".</p>
<p>â€¢ Simulation &amp; AIGC Data.We programmatically generate diverse object organization scenarios in our in-house simulator, and further employ state-of-the-art text-to-image models [21] to synthesize complex Simulation &amp; AIGC Data Labeled Teleoperation Data</p>
<p>Interaction Synthesis Reasoning Synthesis</p>
<p>The table is a mess!Clean up the table and pack the food.</p>
<p>put the bowl into the plastic box</p>
<p>Multi-Stage Task Instruction</p>
<ol>
<li>put the tissue into the rubbish bin 5. put the paper cup into the rubbish bin Sure, I will put it back on the table.Got it.</li>
</ol>
<p>Sure.I will only clear the trash.</p>
<p>Constrained Instruction</p>
<p>Clear the trash and leave the food on the table.</p>
<p>There is a plate of food, a plate and a to-go box.Building on the above sources, we define seven categories of human-robot interaction instructions and develop tailored data-synthesis strategies for each.Training on this dataset equips Robix with flexible interaction capabilities, such as complex instruction understanding, real-time interruption handling, and proactive dialogue.</p>
<p>Multi-Stage Instruction.Teleoperated trajectories are annotated with a task name (e.g., "clean up the table and pack the food").We select trajectories containing at least ten atomic actions and synthesize a corresponding user instruction from the task name, e.g., "The dining table is a mess.Please clean the table and pack the food on the plate."</p>
<p>Constrained Instruction.We partition each teleoperation trajectory into non-overlapping segments where feasible.For example, a table-bussing task can be decomposed into subtasks such as garbage collection, tableware collection, and food packing.Based on these subtasks, we synthesize tailored user instructions, e.g., "Clean up the table while leaving the food on the table" and "Remove the trash without moving other items".</p>
<p>Open-Ended Instruction.We generate random scenes in simulation and prompt state-of-the-art LLMs to produce open-ended, commonsense instructions conditioned on each scene (e.g., "Place the drink with the least sugar into the carton" for a scene containing Sprite, Coke, orange juice, and soda water).To include items currently not supported by the simulator (e.g., hamburgers, spaghetti, watermelons), we pair such instructions with images synthesized by advanced text-to-image models.Because current text-to-image models still struggle with instruction following and image quality, we apply automated and human-in-the-loop filtering, remaining only 10% of the original dataset after filtering.</p>
<p>Anytime Interruption.We curate a variety of user interruption utterances (e.g., "Stop!", "Hold on.I still need it", "Wait, put the fork into the sink first") and randomly inject them into task flows.Corresponding robot responses are then synthesized using timing-aware heuristics: if the interruption occurs before grasping, the robot halts or adjusts its plan; if it occurs after grasping, the robot returns the item to the table and replans.Such anytime interruption handling is critical for long-horizon tasks, as mid-task feedback and error correction substantially enhance system robustness.</p>
<p>Invalid Instruction.To mitigate hallucination and prevent robots from engaging in dangerous actions, we synthesize four types of invalid instructions as follows: (1) Instructions asking the robot to manipulate items that do not exist in the scenario; (2) Instructions requiring physically impossible actions, such as "Put the table into the rubbish bin"; (3) Instructions demanding abilities beyond the robot's current capabilities, e.g., "Open the coke for me"; (4) Unsafe or dangerous commands, e.g., "Throw the knife onto the sofa".For these invalid or illegal instructions, we design corresponding response strategies to enable the robot to refuse compliance with the user's requests.</p>
<p>Ambiguous Instruction.To enable our model to clarify ambiguous instructions, we construct scenes with multiple similar items (e.g., apple, orange, pear) and synthesize underspecified instructions(e.g., "Put a fruit into the basket").Training with these data enable the model to seek clarification when needed-a capability essential for robust robot systems.</p>
<p>Chat Instruction.We develop some heuristics to randomly insert short human-robot dialogue segments at context-appropriate times.For example, when the robot is collecting trash from the table, the user may ask "I want some fruit.What kind of fruit is on the table?".This type of instruction requires the robot to respond verbally rather than performing any physical manipulation.</p>
<p>Reasoning Synthesis</p>
<p>To incorporate chain-of-thought reasoning, we prompt state-of-the-art VLMs to generate high-quality reasoning traces emphasizing (1) scene understanding, (2) task status reflection, (3) long-term instruction following, and (4) next-step analysis.</p>
<p>â€¢ Scene understanding.This part of reasoning enables the robot to accurately identify task-relevant, operable objects in the current scene, with emphasis on those within the robot's field of view.</p>
<p>â€¢ Task status reflection.Robots should be capable of reflecting on their prior actions and repeating tasks when initial attempts fail.Furthermore, they need to identify key milestones in long-horizon tasks and proactively request human assistance upon encountering irrecoverable errors.This capability is also critical for handling user interruptions, as robots must maintain awareness of their current status to plan subsequent actions (e.g., tracking whether a gripper is holding an item).</p>
<p>â€¢ Long-term instruction following.This module is designed to help robot persist the initial goal and intermediate user instructions across long-horizon tasks, ensuring the primary objective is completed and mid-task instructions continue to guide actions many steps later (e.g., "After cleaning the table, grab me a drink from the fridge").</p>
<p>â€¢ Next-step analysis.In the final phase of reasoning, the robot should analyze potential actions for the next step when the overall task remains incomplete.This analysis include assessing target reachability and whether executing the action advances overall task completion.</p>
<p>Inspired by UI-TARS [50], we adopt ActRe [78] and Thought Bootstrapping [50] to synthesize high-quality reasoning traces.Unlike conventional LLM reasoning, robot reasoning must remain concise to support real-time interaction.Accordingly, we prompt Seed-1.5-VL to produce succinct traces (within 200 tokens).We also apply a model-based filtering pipeline to discard hallucinated or logically inconsistent reasoning.These high-quality, multi-faceted chain-of-thought traces enable Robix to execute robust long-horizon task planning with task-status monitoring and dynamic replanning.</p>
<p>Reinforcement Learning</p>
<p>Following the supervised fine-tuning (SFT) stage, the model exhibits promising agentic capabilities in adaptive task planning and natural human-robot interaction.However, several limitations in robot reasoning and planning persist, notably: (1) irrational reasoning, such as generating conflicting thoughts, lacking common sense, or partially disregarding user instructions; and (2) thought-action inconsistency, where the model's proposed plan diverges from its preceding thought in intent or content.For example, in a table-cleaning task, the SFT model correctly infers that a tissue left on the table should be discarded in a rubbish bin.Yet, in the subsequent plan, it incorrectly suggests handling a paper cup instead.These issues negatively impact the model's effectiveness in real-world task execution.</p>
<p>To mitigate these problems, we adopt reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO) [23,54], to enhance both the reasoning capacity and the coherence between thought and action.Our approach is based on two core strategies: ( Reward Design for Thought-Action Consistency.To explicitly encourage alignment between the model's thought and action, we introduce a thought-action consistency reward in addition to standard rewards for output formatting and action accuracy.At each decision step, the model's generated thought and corresponding action are extracted and evaluated by an external LLM (Qwen-2.5-32B[74] in our experiments).This auxiliary reward model is prompted to assess whether the action is logically consistent with the preceding thought.A negative reward is given if the assessment indicates inconsistency.The system prompt of the reward model is listed in Section A.5.</p>
<p>To maximize the effectiveness of RL training, we also employ a data filtering procedure designed to retain only samples that can provide meaningful gradient information for GRPO.The key idea is to discard questions whose candidate answers exhibit low reward variance, as such samples contribute little to policy improvement.Specifically, for each question in the dataset, we generate multiple candidate answers using the SFT model and remove those with low variance in their rewards:
D new = (x n , y * n ) âˆˆ D Var R(y (i) n , y * n ) M i=1 &gt; Ï„, y (i) n âˆ¼ Ï€ SFT (â€¢ | x n ) ,(2)
where D denotes the original dataset, R(y
(i)
n , y * n ) is the reward function assigning a scalar score to the i-th generated answer y (i) n based on the ground-truth y * n for question x n , and Ï€ SFT is the base policy for RL.The definitions of input x n and output y * n follow Equation (1): x n consists of the current observation, instruction, and trajectory, while y n comprises the model's thought, an optional action, and an optional robot response.</p>
<p>In our experiments, we set the number of samples M to 8 and the variance threshold Ï„ to 0. All RL training is performed using the verl framework [55].</p>
<p>Through the combination of co-training with diverse reasoning data and targeted reward design, our reinforcement learning strategy substantially improves the model's generalization to novel tasks and enhances the consistency between reasoning and planning.</p>
<p>Experiments</p>
<p>We conduct extensive experiments to comprehensively evaluate the performance of Robix, focusing on the following key questions:</p>
<p>â€¢ Does Robix enhance fundamental embodied reasoning capabilities?</p>
<p>â€¢ Can Robix effectively model the full process of interactive task execution in an end-to-end manner?</p>
<p>â€¢ Does Robix generalize well to out-of-distribution tasks?</p>
<p>â€¢ How does the full robotic system perform when integrating Robix with a VLA model on real-world tasks?Table 1 Performance of Robix on public vision-language benchmarks compared to prior models.The left side shows Robix and state-of-the-art open-source baselines, while the right side presents closed-source large commercial models.The highest score in each benchmark is highlighted in bold within each group.</p>
<p>We first evaluate fundamental embodied reasoning capabilities of our model and baseline methods on public benchmarks (Section 4.1).Next, we assess the planning and interaction abilities of our model and baseline methods with both offline, pre-defined test sets (Section 4.2) and online robotic tasks (Section 4.3).</p>
<p>Fundamental Perception &amp; Reasoning Evaluation</p>
<p>We evaluate Robix after continued pretraining (denoted as Robix-Base) on a comprehensive set of public benchmarks against state-of-the-art multimodal models including Qwen-2.5-VL-7B&amp;32B[3], RoboBrain-2.0-32B[63], Cosmos-Reason1-7B [2], Gemini-2.5-Pro[64], OpenAI GPT-4o [30], Seed-1.5-VL and Seed-1.5-VL-Think[24].The evaluation spans (1) robotics-relevant embodied reasoning (3D spatial understanding, visual grounding, task-centric reasoning) and (2) general multimodal understanding and reasoning.Table 1 presents the detailed results.models Cosmos-Reason1-7B (64.0) and RoboBrain-32B (72.2), and exceed the best commercial baseline, Gemini-2.5-Pro,on 5 of 8 tasks.</p>
<p>Visual Grounding.We evaluate Robix's visual grounding capabilities on eight benchmarks covering both bounding-box and center-point tasks, including LVIS-MG [25], RefCOCO [80], VisualWebBench [39], Pixmo-Point [14], and Where2Place [81].Robix consistently outperforms its backbone across all benchmarks and surpasses state-of-the-art commercial models on most tasks.Notably, Robix-7B and Robix-32B improve the absolute F1 score on LVIS-MG by 39.6 and 25.0 points over Qwen2.5-VL-7B and 32B, respectively.Robix-32B also outperforms commercial models on most tasks.These results highlight Robix's strong performance in object localization, pointing and fine-grained visual understanding.</p>
<p>Task-centric Reasoning.Embodied reasoning reflects a model's ability to understand and reason about robotic tasks.We evaluate Robix across 5 diverse open benchmarks, including ERQA [34], RoboVQA [52], OpenEQA (HM3D &amp; ScanNet) [43], and EgoTaskQA [31].In addition, we introduce Agibot-ER, a real-world task reasoning benchmark derived from the Agibot dataset [6], which includes manually annotated test sets of 97, 120, and 381 samples for the three key reasoning tasks-Task Status Verification, Action Affordance, and Next Action Prediction, respectively.Full details of the benchmark are provided in Section B. We report the average results of the three tasks on this benchmark.Robix consistently outperforms its backbone models as well as Cosmos-Reason1-7B and RoboBrain-2.0-32Bacross most benchmarks.On Agibot-ER, Robix delivers substantial gains over its backbones, improving absolute accuracy by 12.8 and 7.2 points for the 7B and 32B versions, respectively.It further surpasses Cosmos-Reason1-7B and RoboBrain-2.0-32Bby 23 and 8.3 points, demonstrating superior performance in embodied, task-centric reasoning.</p>
<p>General Multimodal Understanding &amp; Reasoning</p>
<p>Multimodal understanding is a core capability of vision-language models and a primary focus of VLM development.To evaluate both static image and dynamic video understanding, we assess Robix on a suite of general VQA benchmarks-image-based (MME [18], MMBench [40], RealWorldQA [71], SimpleVQA [9]) and video-based (EgoSchema [44], VideoMME [19], NextQA [73]).We further test general reasoning on MathVista [42] and MMMU [82], which cover complex mathematical and multimodal problem-solving tasks.Robix preserves the performance of its backbone on most benchmarks, demonstrating the benefit of training with diverse, high-coverage multimodal data, but still trails large-scale commercial models-underscoring the need to scale both data and model size for stronger general-purpose multimodal reasoning.</p>
<p>Overall, Robix greatly enhances robotics-relevant perception and reasoning-particularly in 3D spatial understanding and visual grounding-while maintaining strong performance on general multimodal tasks.These gains deepen its understanding of spatial and temporal properties, enabling more effective reasoning and planning in real-world environments.</p>
<p>Offline Evaluation</p>
<p>The offline evaluation enables fully automated assessment of planning and interaction capabilities using predefined evaluation sets.To thoroughly evaluate both interactive long-horizon planning and out-ofdistribution (OOD) generalization, we design three dedicated evaluation sets:</p>
<p>â€¢ AGIBot Evaluation Set.We manually select 16 high-frequency daily tasks from the AGIBot dataset (e.g., making a sandwich, washing dishes with a dishwasher, arranging a sofa, washing clothes with a washing machine, arranging flowers) and ensure none appear in the training data.This set primarily evaluates the model's long-horizon task planning capability on OOD tasks.Details are provided in Section A.2.</p>
<p>â€¢ Internal Out-of-Distribution (OOD) Benchmark.We manually design 16 scripts covering task planning and diverse human-robot interaction scenarios, including table organization, dietary filtering, checkout packing, grocery shopping, and shoe cabinet organization.These scripts are enacted by human participants-one acting as the user and the other executing actions via robot teleoperation or a Universal Manipulation Interface (UMI) [10] device-and subsequently annotated by trained annotators.The benchmark includes tasks and items absent from the training data and is intended to evaluate interactive task execution in unseen scenarios.â€¢ Internal In-Distribution (ID) Benchmark.This evaluation set is randomly sampled from our synthesized data and categorized by task type and user instruction into six groups: (1) multi-stage instructions, (2) constrained instructions, (3) invalid instructions, (4) user interruptions, (5) fail-and-replan, and (6) openended instructions.Each category targets evaluation of the model's corresponding instruction following and task planning capabilities.</p>
<p>Data Format.The overall statistics of each evaluation set are shown in Table 2.Each episode is structured as a multi-turn dialogue and evaluated using a teacher-forcing approach, i.e., the model observes an error-free interaction and planning history when predicting the next step action.Because an observation-instruction pair may permit multiple valid next actions, we annotate a candidate action list for each step to capture all acceptable options.Examples of the offline evaluation format are provided in Section A.3.</p>
<p>Evaluation metrics.For AGIBot, Internal OOD, Internal ID-MultiStage/Constrained/Interrupt/OpenEnded, we report action prediction accuracy by matching the predicted action against a candidate action list, with similarity judged by Seed-1.5-VL(see Section A.5 for the prompt).For Internal ID-Invalid/Replan, which are binary classification tasks, we report the F1 score.</p>
<p>Baseline methods.We compare against widely used commercial and open-source VLMs, including Gemini-2.5-Pro,GPT-4o, Seed-1.5-VL,Seed-1.5-VL-Think,Qwen2.5-VL-7B/32B/72B,GLM-4.1V-9B-Thinking[26], and RoboBrain-2.0-7B/32B.All baselines are adapted to the multi-turn observation-think-action format using the prompts in Section A.5.For each model, we test both English and Chinese prompts and report the better result.Gemini-2.5-Proand GPT-4o perform better with English prompts, whereas the other models achieve higher accuracy with Chinese prompts.All evaluations are conducted using greedy decoding.</p>
<p>Results.The offline evaluation results for each model are presented in Table 3. Key observations include:</p>
<p>â€¢ Robix-32B-RL ranks first on all evaluation sets, demonstrating strong task planning and human-robot interaction capabilities, and substantially outperforming all open-source and commercial VLMs on both ID and OOD benchmarks.</p>
<p>â€¢ Chain-of-thought reasoning is critical for both OOD generalization and complex instruction following.Robix-7B-SFT without reasoning (Robix-7B-SFT-wo-R) exhibits a drop of over 7 percentage points in accuracy on the Internal OOD benchmarks compared to its reasoning-enabled counterpart, and suffers a 26.7-point decline on the ID-OpenEnded tasks.</p>
<p>â€¢ RL is critical, boosting Robix-32B's performance on nearly all evaluation sets.On the challenging Internal OOD benchmarks, Robix-7B-RL and Robix-32B-RL improve accuracy by 8.3 and 3.3 points, respectively, compared to their SFT counterparts.As shown in the case study (Section C), RL primarily enhances the SFT models by (i) reducing irrational reasoning steps, (ii) improving thought-action consistency, and (iii) minimizing formatting errors.</p>
<p>â€¢ Gemini-2.5-Pro is the strongest baseline, ranking first on most evaluation sets among baseline methods.</p>
<p>Our evaluation suggests it is currently the leading foundation model for embodied AI applications.</p>
<p>Online Evaluation</p>
<p>While offline evaluation is cost-effective, it is limited to static environments and cannot assess a model's ability to interact with the dynamic physical world.To address this, we deploy our model and baselines within a hierarchical robot system across diverse real-world settings-including kitchens, meeting rooms, and grocery stores-and conduct online evaluations to measure their effectiveness as high-level planning and interaction modules for daily tasks.We design two sets of experiments:</p>
<p>â€¢ Online evaluation of VLMs.Assess the planning and interaction capabilities of VLMs in isolation, without the influence of low-level controllers.</p>
<p>â€¢ Online evaluation of the VLM-VLA robot system.Assess the end-to-end system performance by pairing the VLM with an automatic VLA model as the low-level controller.</p>
<p>In the first set of experiments, VLMs serve as the high-level planning and interaction module, while human labelers equipped with a Universal Manipulation Interface (UMI) [10] device act as the low-level controller, enabling evaluation under a fully reliable control setting.In the second set, we use our in-house VLA model GR-3 [7], as the low-level controller and deploy the integrated VLM-VLA system on the ByteMini robot [7].</p>
<p>Robix is deployed with customized inference optimization techniques [86,87] to reduce response latency.</p>
<p>Online Evaluation of VLMs</p>
<p>Experimental settings.We designed five tasks-table bussing, checkout packing, dietary filtering, grocery shopping, and tableware organization &amp; delivery-spanning diverse environments such as kitchens, meeting rooms, and grocery stores.To increase realism, some tasks deliberately incorporate user interruptions as well as fail-and-replan scenarios.For details on the initial states and brief descriptions of each task, please refer to Section A.4.We further annotated each task with subtasks to assess completion (e.g., for table bussing: "tissue is in the rubbish bin", "plate is in the basket").We compare Robix-32B1 with four baselines that performed well in the offline evaluation: Gemini-2.5-Pro,GPT-4o, Seed-1.5-VL-Think, and Qwen2.5-VL-32B.To reduce experimental variability, each task-model pair is repeated four times, and we report the average results.Following [57], we use task progress-the percentage of subtasks completed by the end of the task-as the evaluation metric.Trained human annotators assess task progress to ensure reliability and consistency.</p>
<p>Results.The online evaluation results are shown in Figure 5.Both Robix-32B and Gemini-2.5-Prorank first in 3 of the 5 tasks, with Robix-32B achieving a slightly higher average task progress (92.6% vs. 91%), demonstrating its superior performance in dynamic real-world environments.Robix-32B also outperforms Qwen2.5-VL-32B by a large margin (92.6% vs. 28%), underscoring the effectiveness of our training pipeline.Gemini-2.5-Proremains the strongest baseline, showing strong capabilities in following complex instructions.However, deploying large foundation models directly for planning and interaction introduces a major challenge-high response latency.In our experiments, Gemini-2.5-Prosometimes required over 30 seconds to respond.While customized deployment may help reduce latency, we contend that current large-scale commercial VLMs remain too computationally heavy for real-time interaction, even on advanced hardware.</p>
<p>Online Evaluation of the VLM-VLA system</p>
<p>Experimental Settings.We select three tasks from the online evaluation-table cleaning, dietary filtering, and checkout packing-as the evaluation set, excluding the remaining two tasks that require actions beyond GR-3's current capabilities.To better isolate the high-level cognitive layer's performance, we also remove particularly challenging items to reduce frequent manipulation failures.Following the VLM online evaluation protocol, each task-model pair is evaluated four times, and we report average results using task progress as the metric.All experiments are conducted with the GR-3 model and the ByteMini robot [7].</p>
<p>Results.We compare Robix-32B with the two strongest baselines from both offline and online evaluations:</p>
<p>Gemini-2.5-Pro and GPT-4o.Figure 6 shows the results across the three real-world tasks.The findings mirror those in Section 4.3.1:Robix-32B achieves an average task progress of 92.5%, exceeding Gemini-2.5-Proand GPT-4o by 4.3 and 28.1 percentage points, respectively.We find that baseline methods-particularly GPT-4o-sometimes generate actions that are semantically correct but unrecognizable to the VLA.For instance, the VLA can execute "put the Oreo into the shopping basket" but fails to interpret "put the biscuit box into the shopping basket."Such VLM-VLA misalignment mainly accounts for the online performance drop observed in Gemini-2.5-Proand GPT-4o.</p>
<p>Related Work</p>
<p>Robotic Task Planning.Solving complex, long-horizon tasks in open environments demands robust high-level planning.Vision-Language Models (VLMs) have advanced robotic task planning by grounding high-level instructions in perceptual context [47,68,77].Unlike Large Language Models (LLMs), which often generate ungrounded or physically infeasible plans due to a lack of environmental perception [29,60], VLMs integrate visual understanding with language reasoning to enable open-vocabulary instruction following and closed-loop planning.Systems such as COME-robot [88], VILA [28], and REPLAN [59] leverage GPT-4V to generate executable plans directly from raw visual observations and iteratively refine them based on environmental feedback, improving robustness through situated reasoning and failure recovery.Despite these advances, VLM-based approaches face persistent challenges: they struggle to maintain long-term consistency, exhibit limited embodied reasoning for grounding objects and spatial concepts in the physical world, and fail to fully integrate these signals for adaptive, task-centric planning.Addressing these issues is essential for scaling VLM-based planning to real-world, long-horizon embodied tasks.Moreover, most existing methods focus solely on task planning while overlooking the human-robot interaction capabilities during task execution that are crucial for a truly generalist robotic system.</p>
<p>Human-Robot Interaction.Existing work on human-robot interaction primarily focuses on enabling seamless, natural communication through real-time feedback and corrections.Early model-based systems grounded language in symbolic environment representations [45,48,49,62], whereas recent learning-based methods adopt hierarchical architectures to directly interpret and act on user feedback [5,13,15,38,46,56,58,72].</p>
<p>Examples include OLAF [38], which uses GPT-4 to re-label actions and update visuomotor policies from corrections; YAY Robot [56], which integrates feedback into an iterative training loop but is limited by prompt diversity; RT-H [5], which supports language-based intervention but restricts corrections to fixed spatial moves; and RACER [13], which combines a VLM supervisor with physics simulation for recovery guidance.</p>
<p>Hi Robot [57] advances these approaches by grounding real-time corrections in the robot's own observations, enabling interpretation and execution of complex instructions beyond prior systems' capabilities.However, achieving flexible interaction alongside adaptive task planning requires strong reasoning capacity-a challenge Robix addresses by leveraging chain-of-thought reasoning to unify complex task planning and human-robot interaction within a single model.Robix further introduces novel interaction capabilities, including proactive dialogue to clarify ambiguous instructions or infer user intent, and context-aware commonsense reasoning.</p>
<p>Embodied Reasoning.Embodied reasoning is the capacity of vision-language models (VLMs) to ground objects, spatial concepts, and physical interactions in the real world, and to integrate these signals into downstream robotic tasks [64].Unlike abstract symbolic reasoning, it is inherently action-oriented, requiring agents to interpret dynamic environments, plan context-aware behaviors, and adapt through feedback.Recent advances span model design, data curation, and task-specific optimization.Embodied-Reasoner [85] learns observation-thought-action trajectories enriched with spatial reasoning and self-reflection for visual search.Gemini Robotics-ER [64] embeds embodied reasoning into its core VLM, achieving strong generalization across tasks such as 3D perception, pointing, state estimation, and affordance prediction.Data-driven approaches include Cosmos-Reason1 [2], which curates datasets emphasizing task-centric reasoning, and RoboBrain-2.0[63], which synthesizes spatial-temporal reasoning datasets augmented with thought traces for causal chain learning.Task-specific methods include EvolveNav [36], which improves vision-language navigation via formalized CoT fine-tuning and self-reflective post-training; and ECoT [83], which trains vision-language-action models for multi-step reasoning over plans, sub-tasks, motions, and grounded visual features before action generation.However, effectively leveraging embodied reasoning to develop generalist robotic systems capable of interactive, long-horizon task execution remains underexplored.Robix addresses this gap by integrating robot reasoning, task planning, and natural language interaction to enable seamless interaction with both humans and physical environments, advancing toward general-purpose embodied intelligence.</p>
<p>Conclusion</p>
<p>This paper presents Robix, a unified vision-language model that integrates robot reasoning, adaptive task planning, and human-robot interaction.Serving as the high-level cognitive layer of a hierarchical robot system, Robix enables robots to execute interactive, long-horizon tasks in open environments with high flexibility.It demonstrates flexible interaction capabilities, including proactive dialogue to clarify ambiguous instructions or infer user intent, real-time interruption handling, and context-aware commonsense reasoning.</p>
<p>Experimental results show that Robix delivers strong performance on real-world robotic tasks and exhibits robust generalization in out-of-distribution settings.</p>
<p>Limitations &amp; Future Work.Similar to other state-of-the-art multimodal models, Robix has several limitations.In highly dynamic tasks with frequent scene transitions, it may produce hallucinations, flawed reasoning, or exhibit gaps in physical commonsense.Additionally, Robix relies on short-term context windows to process interaction history, functioning as a form of short-term memory.Long-term interactive scenarios, however, require more advanced memory mechanisms-specifically, long-term memory with dynamic updates, efficient retrieval, and effective utilization, akin to context engineering in large language models.Addressing these challenges will be a primary focus of future work.</p>
<p><image> Locate several points for the {object}.You can mark them using <point>x y</point>.Please provide point coordinates in the format: <point>(x1, y1)</point> ... The coordinates of the point (x, y) must be <strong>normalized to the range [0, 1000]</strong>, where:</p>
<p>-x refers to the horizontal axis (image width) -y refers to the vertical axis (image height)</p>
<p>VisualWebBench.We use the official metric of VisualWebBench [39].</p>
<p><image> {question}</p>
<p>ERQA.We report accuracy on ERQA [64].</p>
<p><image>...<image> {question} {options} Please answer directly with only the letter of the correct option and nothing else.</p>
<p>RoboVQA &amp; EgoTaskQA &amp; OpenEQA-hm3d &amp; OpenEQA-scannet.We compute accuracy by using GPT-4o to compare model responses with the ground-truth labels, and the evaluation prompt is shown below.</p>
<video> {question}

Prompt for GPT-4o (RoboVQA & EgoTaskQA).

You are an AI assistant tasked with evaluating whether a response matches the correct answer to a given question.


Evaluation Rules

(1) Output 1 if the response matches the answer exactly or with synonymous/equivalent wording.

-Synonyms, paraphrases, or different surface forms of the same meaning count as matches.

-Minor wording differences (e.g., "put tomato into fridge" vs. "the person is putting a tomato in the fridge") count as matches.

(2) Output 0 if the response is incorrect, contradictory, or refers to a different entity, object, or attribute.

-If the answer and response describe different objects, actions, or states, mark as 0.

-If the response introduces additional details that change the meaning of the answer, mark as 0.


Special Cases

-Similar meaning: Output 1 if the response conveys essentially the same meaning as the answer and does not omit or add critical information (e.g., answer:"put meat on the table", response:"The person moved meat from the fridge to the counter.").

-Partial matches: If the response overlaps but misses or alters essential details (e.g., answer:"put meat and tomato on the table" vs. response:"put meat on the table"), output 0.

-Granularity differences: If the response is more specific but still semantically equivalent (e.g., answer:"woman", response:"Jessica"), output 1.

-Yes/No questions: Only output 1 if the polarity matches (yes <-> yes, no <-> no).Any mismatch outputs 0, regardless of explanation.

-Ambiguity: If the response cannot be reasonably interpreted as equivalent to the answer, output 0. Prompt for GPT-4o (OpenEQA-hm3d & OpenEQA-scannet).You are an AI assistant tasked with evaluating whether a response matches the correct answer to a given question, considering both the primary answer and any extra correct answers.


Examples


##Evaluation Rules

(1) Output 1 if the response matches the answer or any of the extra answers exactly or with synonymous/equivalent wording.

-Synonyms, paraphrases, or different surface forms of the same meaning count as matches.

-Minor wording differences (e.g., "Wood panel" vs. "Wood) count as matches.

(2) Output 0 if the response is incorrect, contradictory, or refers to a different entity, object, or attribute than the answer and all extra answers.

-If the answer and response describe different objects, actions, or states, mark as 0.

-If the response introduces additional details that change the meaning of the answer, mark as 0.

##Special Cases -Similar meaning: Output 1 if the response conveys essentially the same meaning as the answer and does not omit or add critical information (e.g., answer: "A ceiling fan", response: "fan").

-Partial matches: If the response overlaps but misses or alters essential details (e.g., answer: "put meat and tomato on the table" vs. response: "put meat on the table"), output 0.

-Granularity differences: If the response is more specific but still semantically equivalent (e.g., answer: "woman", response: "Jessica"), output 1.

-Yes/No questions: Only output 1 if the polarity matches (yes <-> yes, no <-> no).Any mismatch outputs 0, regardless of explanation.

-Ambiguity: If the response cannot be reasonably interpreted as equivalent to the answer, output 0.  The above example illustrates two consecutive rounds within a trajectory.We query VLMs in a manner analogous to multi-turn dialogue interactions.For the first round, planning is considered correct if the VLM output semantically matches either: <|plan_start|>put the banana into the plastic box<|plan_end|> or <|plan_start|>put the grapes into the plastic box<|plan_end|>.For the second round, we evaluate models under a teacher-forcing setting, incorporating the ground-truth from the first round into the dialogue history.Since the other_actions list is empty for this step, planning is deemed correct only if the VLM output semantically matches: <|plan_start|>put the banana on the table<|plan_end|>.


##Examples


A.4 Online evaluation tasks

Example initial states of the five tasks used in our online evaluation are shown in Figure 7. Below, we provide an overview of each task.

â€¢ Table bussing: removing used dishes, utensils, and food items.In our experiment, the user requests the robot to clear the table, but interrupts with the command "I still need it" while the robot is placing a glass into the plastic box.We also introduce a failure event when the robot attempts to place a spoon into the box.

â€¢ Checkout packing: organizing and placing purchased items into bags or boxes at checkout.In our experiment, the user asks the robot to move all items on the checkout counter into the shopping basket.An interruption occurs with the command "I am allergic to caffeine" while the robot is placing a coffee into the basket.

â€¢ Dietary filtering: selecting or excluding food and drink items based on dietary constraints (e.g., no caffeine, vegetarian).In our experiment, the user issues a series of instructions: (1) "Put the food with the highest energy into the plastic box, then add a drink.Note that I am allergic to caffeine.";(2) "Discard all drinks containing caffeine into the rubbish bin"; (3) "Place all fruits and vegetables into the plastic box "; and (4) "Clean up all remaining items on the table".

â€¢ Grocery shopping: retrieving and collecting grocery items according to user instructions.In our experiment, the user first asks the robot to recommend a snack suitable for a road trip.Following the recommendation, the user requests a less sweet snack for the cart, then asks for a non-alcoholic drink, and finally makes a conditional request: "Are there any nuts on the shelf?If so, put some into the shopping cart."

â€¢ Tableware organization & shipment : sorting, packing, and transporting tableware to designated locations.In our experiment, the user instructs the robot to discard all trash and transport the tableware to the sink.Specifically, the robot must first dispose of garbage in the rubbish bin, then place all tableware into a plastic box, and finally carry the box to the sink.During this process, the user interrupts with the command "Keep it on the table" while the robot is placing a stainless steel cup into the box.We use the following prompt to configure VLMs as the interaction and planning modules of the robot system in both offline and online experiments.


# Role

You are a robot with two grippers developed by Bytedance, and your name is Roobio.You are deployed in a home environment, and your job is to have natural interactions with users and complete some tasks required by the user.You have basic movement, perception, and manipulation capabilities, and can navigate to the following areas: dining table, sink, refrigerator, dishwasher, microwave, bread maker, cupboard, shoe cabinet, sofa, and washing machine.


# Input

In each round, you will be provided with the following information.1.Three images: captured by the cameras from your head/left gripper/right gripper respectively.These images reflect your current visual perception at the current moment.2. User instructions (optional): user may give you some instructions, user instructions are in chinese.


# Output

To distinguish reasoning, planning, response and query.You must output with the following fields.

1. '<|think_start|> ... <|think_end|>': this field is for your reasoning process, it can include -The key items in the current scene.

-Whether the previous action is complete.

-Reasoning about your next action.2. '<|plan_start|> ... <|plan_end|>' (optional): this field is for task planning, You should plan your next action based on your reasoning process.The action should be helpful to complete user's task.3. '<|response_start|> ... <|response_end|>' (optional): this field is your response to user, you response must be in *chinese*.You should respond to the user in the following situations.

-When the user is chatting with you; -When the user gives a new instruction on completing a task, you should always respond to the user whenever you receive a new instruction.The response should be concise, polite, and relevant to the task.

-When you need to ask the user for more information.For example, if the user ask you to give him/her a drink, and you observe both orange juice and apple juice, then you should ask with some leftover food residue on the table.To continue organizing the table, I first need to re-attempt putting the paper cup into the rubbish bin to ensure the trash is handled correctly.<|think_end|><|plan_start|>put the paper cup into the rubbish bin<|plan_end|> # Robix-32B-SFT output <|think_start|>From the current scene, the paper cup is still on the table, near the rubbish bin, and is grasped by my gripper, which indicates that the previous attempt to put the paper cup into the rubbish bin was not successful.Besides the paper cup, there is also a plate with some leftover food residue on the table.To continue organizing the table, I first need to re-attempt putting the paper cup into the rubbish bin to ensure the trash is handled correctly.<|think_end|>{ "action": "put", "object": "paper cup", "target": "rubbish bin" }

Figure 1 A
1
Figure 1 A demo of Robix, showcasing (1) complex instruction understanding with commonsense reasoning; (2) real-time interruption handling; (3) task-status monitoring and dynamic replanning; and (4) proactive dialogue to clarify ambiguous instructions or infer user intent.


Figure 3
3
Figure 3 Overview of Robix's pretraining data, curated to strengthen core embodied reasoning capabilities (3D spatial understanding, visual grounding, and task-centric reasoning) while also supporting general multimodal understanding and reasoning.The figure showcases the diversity of the data, establishing a solid foundation for embodied models.


( 1 )
1
VQA: A diverse set of image-and video-based question answering tasks covering visual perception, factual knowledge, grounding, temporal reasoning, spatial understanding, and counting.(2) Captioning: Dense captions for both images and videos, supporting the model's understanding of static scenes and multi-frame temporal dynamics.(3) OCR: To improve text recognition, we include large-scale annotated and synthetic datasets covering scene text, documents, tables, charts, and flowcharts.The dataset is filtered from both open-source resources and the Seed-1.5-VLcorpus, together forming a robust foundation for training general-purpose vision-language models.Instruction Tuning.To further enhance the model's instruction-following and reasoning abilities, we construct a high-quality instruction tuning dataset comprising 1 million examples.These examples span a wide range of tasks and are built by extracting curated subsets from previously collected data, integrating both general instructions and chain-of-thought examples from open-source and internal sources.




Source Data Selectionput the paper cup into the rubbish bin 1 put the knife into the plastic box put the fork into the plastic box put the glass into the plastic box 2 3 4


Figure 4
4
Figure 4 Our data synthesis pipeline.The leftmost panel shows the data sources; the center illustrates how diverse human-robot interactions are synthesized from these sources; the rightmost panel presents example snapshots of the generated reasoning traces.


Figure 5
5
Figure 5 Online evaluation results with a human labeler operating a UMI device as the low-level controller.


Figure 6
6
Figure 6 Online evaluation on the ByteMini robot with GR-3 model as the low-level controller.


Figure 7
7
Figure 7 Example initial states of the five tasks used in our online evaluation.(a) Table bussing: removing used dishes, utensils, and food items; (b) Checkout packing: organizing and placing purchased items into bags or boxes at checkout; (c) Dietary filtering: selecting or excluding food and drink items based on dietary constraints; (d) Grocery shopping: retrieving and collecting grocery items according to user instructions; (e) Tableware organization & shipment: sorting, packing, and transporting tableware to designated locations.


Task Status Verification Action Affordance Next Action Prediction GUI Agent VQA

Q: Determine whether the task "Pick up the cup with the left arm" has been completed.A: <think>According to the imageâ€¦</think> The task has been completed.Q: Determine whether the task "Pass the held pillow with both hands" can be executed.A: <think>The left arm is currentlyâ€¦</think> No, it cannot be executed in the current state.Q: What is the most likely next action?A: <think>The left arm is holding up a bottle with the cap openedâ€¦</think> The most likely next action is to close the bottle cap with the right arm.


3D Spatial Understanding 2D Bounding Box Detection
1 and frame 2? Options: A: Rotate left, B:Rotate right, C: No rotation.Aï¼šA: Rotate leftPoint DetectionQï¼šWhat are the bounding box coordinates of the "aQï¼šMark several points on "Park Ave".small stool in front of the counter" in this image?Aï¼š<point>(762,304)</point>â€¦<point>(875,265)</Aï¼š<|box_start|>(682, 679), (813, 955)<|box_end|>point>

reward design targeting thought-action consistency. Co-Training with General Visual Reasoning Data.

The RL stage utilizes two primary data sources: robot interaction data and general visual reasoning datasets.Training on robot interaction data improves the model's robustness and generalization to out-of-distribution (OOD) scenarios.Meanwhile, incorporating general visual reasoning data strengthens the model's inherent reasoning capabilities.This co-training strategy helps alleviate irrational reasoning and enhances overall task understanding and solving.The general visual reasoning datasets include a wide range of cognitive challenges, such as task completion verification, action affordance evaluation, and object localization-covering a broad spectrum of reasoning skills relevant to real-world robot applications.
1) co-training with general visual reasoning data,and (2)

Table 2
2
Statistics of the offline evaluation sets.
AGIBotInternal OODInternal ID Multi-Stage Constrained Interrupt Open-Ended Invalid Replan# episodes 1616925-15--# data1422251192331101560100AGIBotInternal OODInternal ID Multi. Const. Interrupt Open.InvalidReplanPlan AccuracyF1 score F1 scoreGemini-2.5-Pro52.683.879.387.155.96098.383.7GPT4-o45.977.076.184.444.866.779.273.7Seed-1.5-VL37.473.275.476.041.146.710036.8Seed-1.5-VL-Think49.680.473.982.742.946.774.275.1Qwen-2.5-VL-72B36.769.271.365.155.226.787.040.0Qwen-2.5-VL-32B43.371.660.562.248.026.770.237.0Qwen-2.5-VL-7B31.154.737.541.520.56.747.510.9GLM-4.1-9B-Think34.151.722.845.814.06.786.037.2RoboBrain-2.0-32B29.663.558.251.741.20.043.629.9RoboBrain-2.0-7B0.331.436.033.125.30.00.022.2Robix-7B-SFT-wo-R55.269.982.589.091.560.010090.5Robix-7B-SFT57.877.185.891.184.286.710088.4Robix-7B-RL59.685.493.290.378.686.795.987.0Robix-32B-SFT64.083.589.393.089.780.010095.1Robix-32B-RL64.486.896.696.092.593.310096.2

Table 3
3
Offline evaluation results.Robix-7B-SFT-wo-R refers to our SFT model without chain-of-thought reasoning, while Robix-7B-RL denotes the full trained policy obtained by applying RL after SFT.For AGIBot, Internal OOD (Out-of-Distribution), and Internal ID (In Distribution)-MultiStage/Constrained/Interrupt/OpenEnded, we report plan accuracy; for Internal ID-Invalid/Replan, we report F1 score.The best result for each evaluation set is shown in bold, and the best among baselines is underlined.




Please place the fruits in the bowl then place the kitchen supplies into the holder.last 20 steps: 1. put white packet in the bowl 2. put white packet in the bowl 3. put yellow packet in the bowl 4. put blue packet in the bowl 5. put blue packet in the bowl 6. put blue packet in the bowl 7. put yellow packet in the bowl.
Example 2 Question: what status of fork changed while the person do the first action did before Your output:he/she put something to something?Answer: cleanlinessResponse: fork was in drawer before the person put fork to sink.Your output: 0Example 3Question: What is the person doing before he/she close something?Answer: Put tomato to fridgeResponse: The person is putting a tomato in the fridge.Your output: 1Example 4Question: What is the first action the person did in the video?Answer: Work on sofaResponse: The person pulled out a chair.Your output: 0Example 5Question: How did the person changed the spatial relationships of meat?Answer: Put meat to tableResponse: The person moved meat from the fridge to the counter.Your output: 1Example 6Question: what status of fridge changed while the person do the first action did after he/shepoint to something?Answer: openessResponse: The fridge was closed before the person point to something, and after that the fridgechanged to open.Your output: 1Example 7Question: which object changed its status when the person do the last action in the video?Answer: forkResponse: spoonYour output: 0Example 8Question: What is the action that just happened?Answer: Place can in the trayResponse: The person puts the can on the table.Your output: 0Example 9Question: current goal is: What's theimmediate next step?Example 1 Answer: Put duster in the black standQuestion: Did the attribute of plant changed because of the action getting something from Response: put brush in the holdersomething? Your output: 0Answer: yesResponse: Yes, the attribute of plant got watered from no to yes after the action getting Your Turn:something from something. Question: {question}Your output: 1 Answer: {answer}Response: {prediction}



The objects (fork v.s.knife) are different semantically.The final answer is: 0.0
User:1. put the fork into the basket2. pick up the forkAssistant:The actions are different semantically. The final answer is: 0.0User:1. put the fork into the basket2. navigate to the tableAssistant:Neither actions nor objects matches. The final answer is: 0.0User:1. put the fork into the basket2. pick up the stainless steel cupAssistant:Neither actions nor objects matches. The final answer is: 0.0English Prompt for Baseline Models.
3D Spatial Understanding. We evaluate Robix's 3D spatial understanding across 8 spatial reasoning benchmarks: VSIBench[75], BLINK[20], CV-Bench[65], EmbSpatial[16], SAT[51], VSR[37], SpatialBench[16], and DA-2k[76]. Details of these benchmarks are provided in Section A.1. As shown in Table1, Robix-7B and Robix-32B outperform their backbones (Qwen2.5-VL-7B/32B) on 7 of 8 spatial reasoning tasks, with average accuracies of 73.4 and 75.8 compared to 66.9 and 70.7, respectively. They also surpass embodied
By default, Robix-32B refers to Robix-32B-RL.
Pixmo-Point & Where2Place. For these two benchmarks[14,81], we compute the proportion of predicted points that fall within the referring objects.
Since the baseline models (e.g., Qwen-2.5-VL, Cosmos-Reason1, RoboBrain-2.0, etc.) cannot follow a unified output format on the visual grounding benchmarks-including LVIS, RefCOCO, Pixmo-Point, and Where2Place-we revise their prompts to adapt to their output formats.
AcknowledgementsWe thank Wanli Peng, Yongyu Yan, and Tingshuai Yan for their assistance with model deployment and inference optimization.We are also grateful to Baifeng Xie, Lihao Liu, and Yangang Zhang for their support in utilizing the internal simulation platform, and to Xiao Ma for his valuable suggestions on the writing of this paper.We further thank the GR-3 team for providing the teleoperation data, GR-3 model, and ByteMini robot used in our experiments.Finally, we thank the Seed-1.5-VLteam for their support with data resources.AgiBot-ER.We curate 97, 120, 381 test samples for three sub-tasks, i.e., Task Status Verification, Action Affordance, and Next Action Prediction, respectively.We report the average accuracy of the three tasks.Appendix A Experiment details A.1 Fundamental Evaluation PromptsThe prompt templates for all benchmarks are listed below.In each template, {question} is replaced with the sample's actual question, {options} with its multiple-choice answer options, <image> with the computed ViT embeddings of the input image, and <video> with the ViT embeddings of the video frames (for video benchmarks).VSIBench.We use the official metric of VSIBench[75].<video> {question} Options: {options} Answer with the option's letter from the given choices directly.BLINK.Following RoboBrain 2.0[63], we evaluate BLINK[20]on two sub-tasks-spatial relation understanding and depth perception-and report the average accuracy across them.<image> {question} {options} Answer with the option's letter from the given choices directly.CV-Bench.We follow the official evaluation protocol of CV-Bench[65]and report average accuracy over the three defined sub-tasks.<image> {question} {options} Answer with the option's letter from the given choices directly.EmbSpatial.We use the official metric of EmbSpatial-Bench[16]and report the average accuracy.<image> {question} Please answer with a single option letter: {options} SAT.We use the official metric of SAT[51]and report the average accuracy across its five sub-tasks.<image> {question} {options} Please only answer with the option letter.VSR.We use the official metric of VSR[37]and report the average accuracy.<image> {question} Answer with a single word yes or no.SpatialBench.We use the official metric of SpatialBench[16].<image> {question} Please answer with a single word: yes or no (for existence task).DA-2k.Since some VLMs tend to repeatedly output the same option on DA-2k[76], we shuffle the answer choices and report the average accuracy.<image> {question}Only provide the answer: point1 or point2.LVIS.We report results on LVIS[25]using the F1-score We use the official metric of RefCOCO[80].<image> What are the coordinates of the {object} in this image?Please provide the bounding box coordinates of the {object} in the format: <|box_-start|>(x1, y1),(x2, y2)<|box_end|>, where (x1, y1) for upper-left, and (x2, y2) for lower-right.All coordinates should be normalized to a [0, 1000] scale, where x corresponds to the horizontal axis (image width), y corresponds to the vertical axis (image height).Task Status Verification<image><image><image><image><image><image> {question} Your answer can only be "yes" or "no" Action Affordance <image><image><image> {question} Your answer can only be "yes" or "no".Next Task Prediction <image><image><image><image><image><image> {question} {options} Answer with the option's letter from the given choices directly.MME.We report the accuracy+ metric for MME[18].<image> {question} Please answer yes or no.MMBench.We demonstrate the official metric provide by MMBench[40].<image> {question} {options} Answer with the option's letter from the given choices directly.RealWorldQA.We use the official metric of RealWorldQA[71].<image> {question} {options} Please answer directly with only the letter of the correct option and nothing else.SimpleVQA.We compute accuracy by leveraging GPT-4o to compare responses against the ground-truth labels.<image> {question}EgoSchema & VideoMME & NextQA.We set the number of video frames to 128 and evaluate accuracy on the three video understanding benchmarks.<video> {question} {options} Answer with the option's letter from the given choices directly.MathVista.We use the official metric of MathVista[42].<image> {question}MathVision.We use the official metric of MathVision[69].<image> {question} {options} Please only output your final answer or a single letter (if it is a multiple choice question).MMMU.We use the official metric of MMMU[82].<image> {question} {options} Answer with the option's letter from the given choices directly.A.2 The AGIBot offline evaluation setWe select 16 high-frequency daily tasks from the AGIBot dataset for offline evaluation: "Heat food in the microwave oven," "Make a sandwich," "Wash dishes with dishwasher," "Wash the dishes and silverware," "Discard the trash on the coffee table," "Pick up the item to wipe away the stain," "Place items from the meeting room table into the storage box," "Arrange sofa," "Wash clothes in the washing machine," "Remove clothes from the washing machine," "Arrange flowers," "Boil water in the kettle," "Packing in the supermarket," "Scan and package the goods," "Restock the hanging basket area," and "Restock supermarket snacks."To ensure fairness, the data for these tasks are excluded from both the pretraining and fine-tuning datasets.A.3 Offline evaluation examplesWe present a snapshot of our offline evaluation sets.[ ... { "role": "user", "content": "<|system_info_start|>Your current views are<image><image><image><|system_-info_end|><|human_start|>I want some fruit, put all fruits into the basket.<|human_end|>"}, { "role": "assistant", "content": "<|think_start|>...<|think_end|><|plan_start|>put the banana into the plastic box<|plan_end|><|response_start|>...<|response_end|>", "other_actions": [ "put the grapes into the plastic box" ] }, { "role": "user", "content": "<|system_info_start|>Your current views are<image><image><image><|system_-info_end|><|human_start|>Wait, I don't want banana.<|human_end|>"}, { "role": "assistant", "content": "<|think_start|>...<|think_end|><|plan_start|>put the banana on the table<|plan_end|><|response_start|>...<|response_end|>", "other_actions": [ ]A.5 Offline & Online Evaluation PromptsWe present the prompts used in our experiments, including the judge prompt for offline evaluation, the baseline model prompts, and the reward model prompt for reinforcement learning.Judge Prompt for Offline Evaluation# RoleYou are a judge to decide the similarity of two sentences.# Inputs You will be provided with two sentences, each sentence represent an action from a robot.# Output FormatYour output should be a single number that is either 0 orChinese Prompt for Baseline ModelsWe also developed a corresponding Chinese prompt for models that perform better with Chinese.Reward Model Prompt for RL# RoleYou are a consistency checker for a robot's planning.Your task is to evaluate the robot's thinking process and the resulting final action.# Input 1.The robot's thinking process, enclosed by <|think_start|> and <|think_end|>.2. The robot's final action, enclosed by <|plan_start|> and <|plan_end|>.# TaskYou need to determine: 1.Whether the thinking process within <|think_start|> and <|think_end|> is reasonable.2. Whether the final decision within <|plan_start|> and <|plan_end|> is consistent with the decision derived from the thinking process in <|think_start|> and <|think_end|>.# Output Your output must follow this format: [Your reasoning process] + "The final answer is: [NUMBER]."Here, [NUMBER] is an integer: '1' indicates consistency; '-1' indicates inconsistency; '0' indicates that it is impossible to judge.# Notes 1. Ignore ambiguity in names.Items that could potentially be consistent should be considered consistent.2. Assign -1 only if the thinking process is completely unreasonable.B Embodied Task-centric ReasoningB.1 Data SynthesisPublicly available robot datasets often contain long video demonstrations with temporally annotated clips and corresponding individual actions, or they can be segmented automatically into clips using VLMs.To equip Robix with reasoning and planning capabilities in embodied scenarios, we design three dedicated synthesis pipelines, each targeting a distinct sub-task: task status verification, action affordance, and next action prediction.The details of these pipelines are provided below.Task Status Verification.The status of a task is categorized as either complete or incomplete.To construct task verification data labeled as complete, we use the entire video clip as the visual input.Reasoning traces are obtained by prompting a strong VLM (Seed-1.5-VL in thinking mode) with the question "Is the {action} complete?".We then extract the reasoning content from the response and retain only those instances where the predicted status is complete.Conversely, to represent incomplete task status, we randomly truncate each video clip to between one-half and one-third of its original duration, producing partial visual inputs.As before, we retain only those cases where the predicted status is identified as incomplete.Action Affordance.To obtain high-quality labels for action affordance, we distill both labels and reasoning traces from powerful VLMs.However, we find that relying on a single VLM is insufficient for generating accurate labels reliably.Therefore, we employ various VLMs to produce candidate labels and take the intersection of their outputs as the final annotation.The corresponding reasoning traces are extracted from the Seed-1.5-VL-Thinkmodel.Next Action Prediction.The robot datasets are typically annotated with action sequences.Thus, the ground-truth next action can be directly determined once the previous action is specified.To synthesize the corresponding reasoning trace, we leverage Seed-1.5-VLby prompting it with both the video and the following textual input: "You are an intelligent robot executing {the task}.To complete this task, you are performing the action as shown in the video.Your next action is {next action}.Please reflect on the rationale behind your next action and output your response from the perspective of a robotic agent."B.2 Agibot-ER BenchmarkWe construct an embodied task-centric reasoning benchmark based on the Agibot dataset[6], which provides manual annotations of tasks, atomic action sequences, and the corresponding video clips for each action.Specifically:â€¢ Task Status Verification: We randomly select 100 samples from the synthesized dataset and further verify the label accuracy, resulting in 97 test samples.â€¢ Action Affordance: We select 120 video clips and corresponding individual actions from 50 different tasks.To ensure accuracy, we manually annotate these samples rather than relying on VLM-based annotation.â€¢ Next Action Prediction.The ground-truth next action is obtained directly from the robot datasets.We randomly sample 400 instances and construct distractor options by leveraging a powerful LLM to generate incorrect candidates that differ subtly from the correct one-such as variations in action type, target object, or destination.To ensure quality, we manually verify all synthesized options, resulting in 381 validated test samples.All test samples are eliminated from the training dataset.C Case studies of RLOur reinforcement learning stage yields several improvements over the SFT model, including: (1) reduced irrational reasoning, (2) improved thought-action consistency, and (3) fewer formatting errors.In the following, we present comparative case studies between Robix-32B-RL and Robix-32B-SFT to illustrate the effectiveness of reinforcement learning.Reduced irrational reasoning# Current task Put all the drinks on the table into the carton.# Robix-32B-RL output <|think_start|>Based on the current scene, I see that there is already a bottle of soda inside the carton, and the remaining items on the table are milk, Oreos, and gum.Recalling my last action, I have already put the coke into the carton.Now, the only beverage left on the table is milk.To complete the task of putting all beverages into the carton, the next step should be to put the milk into the carton.<|think_end|> <|plan_start|>put the milk into the carton<|plan_end|> # Robix-32B-SFT output <|think_start|>Based on the current scene, I see that there is already a bottle of soda inside the carton, and the remaining items on the table are milk, Oreos, and gum.Recalling my last action, I have already put the coke into the carton.Now, all the beverages on the table have been put into the carton, and the task goal has been achieved.Next, I should proactively ask the user if there are any other tasks that need assistance.<|think_end|> <|response_start|>The task has been processed.Is there anything else I can help you with? <|response_end|>Improved thought-action consistency# Current Task Put the dishes in the sink # Robix-32B-RL output <|think_start|>I have grasped the storage basket with my dual grippers.The basket contains bowls, forks, and spoons.According to the user's instructions, the next step is to take these dishes to the sink.I have now finished collecting the tableware.Next, I should navigate to the sink area and deliver the dishes to the designated location.<|think_end|><|plan_start|>navigate to the sink<|plan_end|> # Robix-32B-SFT output <|think_start|>I have grasped the storage basket with my dual grippers, ready to transport the dishes.According to the user's instructions, the dishes need to be taken to the sink.All the dishes that need to be washed have been collected in the storage basket.The next step is to transport the storage basket to the sink area, completing the task of collecting and transporting the dishes.<|think_end|><|plan_start|>navigate to the cupboard<|plan_end|>Fewer formatting errors# Current Task Organize the table # Robix-32B-RL output <|think_start|>From the current scene, the paper cup is still on the table, near the rubbish bin, and is grasped by my gripper, which indicates that the previous attempt to put the paper cup into the rubbish bin was not successful.Besides the paper cup, there is also a plate
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint

Cosmos-reason1: From physical common sense to embodied reasoning. Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, arXiv:2503.155582025arXiv preprint

Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, arXiv:2502.13923Jun Tang, et al. Qwen2. 5-vl technical report. 2025arXiv preprint

ARKitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Yuri Feigin, Peter Fu, Thomas Gebauer, Daniel Kurz, Tal Dimry, Brandon Joffe, Arik Schwartz, Elad Shulman, Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021Round 1

Rt-h: Action hierarchies using language. Tianli Suneel Belkhale, Ted Ding, Pierre Xiao, Quon Sermanet, Jonathan Vuong, Tompson, arXiv:2403.018232024arXiv preprint

Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, arXiv:2503.066692025arXiv preprint

Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, arXiv:2507.15493Gr-3 technical report. 2025arXiv preprint

An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem BÄ±yÄ±k, Hongxu Yin, Sifei Liu, Xiaolong Wang, Navila, arXiv:2412.04453Legged robot vision-language-action model for navigation. 2024arXiv preprint

Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, arXiv:2502.13059Multimodal factuality evaluation for multimodal large language models. 2025arXiv preprint

Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. Cheng Chi, RSSZhenjia Xu, RSSChuer Pan, RSSEric Cousineau, RSSBenjamin Burchfiel, RSSSiyuan Feng, RSSRuss Tedrake, RSSShuran Song, RSSProceedings of Robotics: Science and Systems. Robotics: Science and Systems2024

One to rule them all: natural language to bind communication, perception and action. Simone Colombani, Dimitri Ognibene, Giuseppe Boccignone, arXiv:2411.150332024arXiv preprint

Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias NieÃŸner, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017

Racer: Rich language-guided failure recovery policies for imitation learning. Yinpei Dai, Jayjun Lee, Nima Fazeli, Joyce Chai, arXiv:2409.146742024arXiv preprint

Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, 20242409arXiv e-prints

Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Ayzaan Chowdhery, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, 2023

Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, Zhongyu Wei, Annual Meeting of the Association for Computational Linguistics. Short Papers22024

Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, arXiv:2406.18915Dieter Fox, and Ranjay Krishna. Manipulate-anything: Automating real-world robots using vision-language models. 2024arXiv preprint

Mme-survey: A comprehensive survey on evaluation of multimodal llms. Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, arXiv:2411.152962024arXiv preprint

Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025

Blink: Multimodal large language models can see but not perceive. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, Ranjay Krishna, European Conference on Computer Vision. Springer2024

. Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Xin Xia, Xuefeng Xiao, Linjie Yang, Zhonghua Zhai, Xinyu Zhang, Qi Zhang, Yuwei Zhang, Shijia Zhao, Jianchao Yang, Weilin Huang, 2025Seedream 2.0: A native chinese-english bilingual image generation foundation model

Ego4d: Around the world in 3,000 hours of egocentric video. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022

Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint

Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, arXiv:2505.07062Jiawei Wang, et al. Seed1. 5-vl technical report. 2025arXiv preprint

Lvis: A dataset for large vocabulary instance segmentation. Agrim Gupta, Piotr Dollar, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019

Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, arXiv:2507.01006Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. 2025arXiv preprint

Ryan Hoque, Peide Huang, David J Yoon, Mouli Sivapurapu, Jian Zhang, Egodex, arXiv:2505.11709Learning dexterous manipulation from large-scale egocentric video. 2025arXiv preprint

Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, arXiv:2311.178422023arXiv preprint

Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint

Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint

Egotaskqa: Understanding human tasks in egocentric videos. Baoxiong Jia, Ting Lei, Song-Chun Zhu, Siyuan Huang, Advances in Neural Information Processing Systems. 202235

Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint

Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, International Conference on Learning Representations. 2015

Anastasia Kirillova, Eugene Lyapustin, Anastasia Antsiferova, Dmitry Vatolin, arXiv:2110.09992Erqa: Edge-restoration quality assessment for video super-resolution. 2021arXiv preprint

Cubify anything: Scaling indoor 3d object detection. Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, Afshin Dehghan, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025

Evolvenav: Self-improving embodied reasoning for llm-based vision-language navigation. Bingqian Lin, Yunshuang Nie, Khun Loun Zai, Ziming Wei, Mingfei Han, Rongtao Xu, Minzhe Niu, Jianhua Han, Liang Lin, Cewu Lu, arXiv:2506.015512025arXiv preprint

Visual spatial reasoning. Fangyu Liu, Guy Emerson, Nigel Collier, Transactions of the Association for Computational Linguistics. 112023

Huihan Liu, Alice Chen, Yuke Zhu, Adith Swaminathan, Andrey Kolobov, Ching-An Cheng, arXiv:2310.17555Interactive robot learning from verbal correction. 2023arXiv preprint

Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, Xiang Yue, Visualwebbench, arXiv:2404.05955How far have multimodal llms evolved in web page understanding and grounding?. 2024arXiv preprint

Mmbench: Is your multi-modal model an all-around player?. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, European conference on computer vision. Springer2024

Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2019

Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, Mathvista, arXiv:2310.02255Evaluating mathematical reasoning of foundation models in visual contexts. 2023arXiv preprint

Embodied question answering in the era of foundation models. Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024

Egoschema: A diagnostic benchmark for very long-form video language understanding. Karttikeya Mangalam, Raiymbek Akshulakov, Jitendra Malik, Advances in Neural Information Processing Systems. 202336

Learning to parse natural language commands to a robot control system. Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, Dieter Fox, Experimental robotics: the 13th international symposium on experimental robotics. Springer2013

Is feedback all you need? leveraging natural language feedback in goal-conditioned rl. Sabrina Mccallum, Max Taylor-Davies, Stefano Albrecht, Alessandro Suglia, NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning. 2023

Replanvlm: Replanning robotic tasks with visual language models. Aoran Mei, Guo-Niu Zhu, Huaxiang Zhang, Zhongxue Gan, IEEE Robotics and Automation Letters. 2024

Learning neuro-symbolic programs for language guided robot manipulation. Himanshu Namasivayam, Vishal Singh, Arnav Bindal, Vishwajeet Tuli, Rahul Agrawal, Parag Jain, Rohan Singla, Paul, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023

Inferring compact representations for efficient natural language understanding of robot instructions. Siddharth Patki, Andrea F Daniele, Matthew R Walter, Thomas M Howard, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019

Ui-tars: Pioneering automated gui interaction with native agents. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, arXiv:2501.123262025arXiv preprint

Sat: Dynamic spatial aptitude training for multimodal language models. Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A Plummer, Ranjay Krishna, arXiv:2412.077552024arXiv preprint

Multimodal long-horizon reasoning for robotics. Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, J Nikhil, Joshi, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024

Bumble: Unifying reasoning and acting with vision-language models for building-wide mobile manipulation. Rutav Shah, Albert Yu, Yifeng Zhu, Yuke Zhu, Roberto MartÃ­n-MartÃ­n, arXiv:2410.062372024arXiv preprint

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Yang Li, Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint

Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, Chuan Wu, arXiv:2409.19256Hybridflow: A flexible and efficient rlhf framework. 2024arXiv preprint

Yell at your robot: Improving on-the-fly from language corrections. Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z Zhao, Archit Sharma, Karl Pertsch, arXiv:2403.129102024arXiv preprint

Hi robot: Open-ended instruction following with hierarchical vision-language-action models. Lucy Xiaoyang Shi, Brian Ichter, Michael Robert Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, Chelsea Finn, International Conference on Machine Learning. 2025

Utsav Singh, Pramit Bhattacharyya, Vinay, Namboodiri, arXiv:2406.05881Lgr2: Language guided reward relabeling for accelerating hierarchical reinforcement learning. 2024arXiv preprint

Marta Skreta, Zihan Zhou, Jia Lin Yuan, Kourosh Darvish, AlÃ¡n Aspuru-Guzik, Animesh Garg, Replan, arXiv:2401.04157Robotic replanning with perception and language models. 2024arXiv preprint

Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023

Sun rgb-d: A rgb-d scene understanding benchmark suite. Shuran Song, Jianxiong Samuel P Lichtenberg, Xiao, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015

A computational model for the alignment of hierarchical scene representations in human-robot interaction. Agnes Swadzba, Sven Wachsmuth, Constanze Vorwerg, Gert Rickheit, IJCAI. 2009

. Mingyu Baai Robobrain Team, Huajie Cao, Yuheng Tan, Minglan Ji, Zhiyu Lin, Zhou Li, Pengwei Cao, Enshen Wang, Yi Zhou, Han, arXiv:2507.020292025arXiv preprintet al. Robobrain 2.0 technical report

Saminda Gemini Robotics Team, Joshua Abeyruwan, Jean-Baptiste Ainslie, Montserrat Alayrac, Travis Gonzalez Arenas, Ashwin Armstrong, Robert Balakrishna, Maria Baruch, Michiel Bauza, Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025arXiv preprint

Cambrian-1: A fully open, vision-centric exploration of multimodal llms. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri, Iyer , Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, Advances in Neural Information Processing Systems. 202437

Rio: 3d object instance re-localization in changing indoor environments. Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, Matthias NieÃŸner, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019

Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Du, Conference on Robot Learning. PMLR2023

Vlm see, robot do: Human demo video to robot action plan via vision language model. Beichen Wang, Juexiao Zhang, Shuwen Dong, Irving Fang, Chen Feng, arXiv:2410.087922024arXiv preprint

Measuring multimodal mathematical reasoning with math-vision dataset. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, Hongsheng Li, Advances in Neural Information Processing Systems. 202437

Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world. Xin Wang, Taein Kwon, Mahdi Rad, Ishani Bowen Pan, Sean Chakraborty, Dan Andrist, Ashley Bohus, Bugra Feniello, Felipe Tekin, Vieira Frujeri, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023

Realworldqa: A benchmark for real-world spatial understanding. 2024

Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun Yu, David Hsu, Robi butler: Remote multimodal interactions with household robot assistant. arXiv e-prints. 20242409

Next-qa: Next phase of question-answering to explaining temporal actions. Junbin Xiao, Xindi Shang, Angela Yao, Tat-Seng Chua, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021

. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint

Thinking in space: How multimodal large language models see, remember, and recall spaces. Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, Saining Xie, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025

Depth anything v2. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao, Advances in Neural Information Processing Systems. 202437

Guiding long-horizon task and motion planning with vision language models. Zhutian Yang, Caelan Garrett, Dieter Fox, TomÃ¡s Lozano-PÃ©rez, Leslie Pack, Kaelbling , arXiv:2410.021932024arXiv preprint

React meets actre: Autonomous annotation of agent trajectories for contrastive self-training. Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu, First Conference on Language Modeling. 2024

Scannet++: A high-fidelity dataset of 3d indoor scenes. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias NieÃŸner, Angela Dai, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023

Modeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, European conference on computer vision. Springer2016

Robopoint: A vision-language model for spatial affordance prediction for robotics. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, arXiv:2406.107212024arXiv preprint

Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024

Robotic control via embodied chain-of-thought reasoning. MichaÅ‚ Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, Sergey Levine, arXiv:2407.086932024arXiv preprint

Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, Jianyu Chen, arXiv:2410.05273Hirt: Enhancing robotic control with hierarchical robot transformers. 2024arXiv preprint

Embodied-reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks. Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, arXiv:2503.216962025arXiv preprint

Triton-distributed: Programming overlapping kernels on distributed ai systems with the triton compiler. Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Haojie Duanmu, Renze Chen, Ruifan Xu, 2025

Size Zheng, Jin Fang, Xuegui Zheng, Qi Hou, Wenlei Bao, Ningxin Zheng, arXiv:2503.20313Generating efficient compute-communication overlapping kernels using tile-centric primitives. 2025arXiv preprint

Closed-loop open-vocabulary mobile manipulation with gpt-4v. Peiyuan Zhi, Zhiyuan Zhang, Yu Zhao, Muzhi Han, Zeyu Zhang, arXiv:2404.102202024arXiv preprint

Assign -1 only if it's completely impossible for the thinking process and the plan to be consistent. 

If the plan is only a part of the strategy outlined in the thinking process. it is still considered consistent (i.e., the plan does not need to reflect all thoughts            </div>
        </div>

    </div>
</body>
</html>
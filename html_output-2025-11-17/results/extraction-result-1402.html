<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1402 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1402</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1402</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-bb1a17010254abfa5e1f2a17553582ce449f8e16</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bb1a17010254abfa5e1f2a17553582ce449f8e16" target="_blank">Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Embed to Control is introduced, a method for model learning and control of non-linear dynamical systems from raw pixel images that is derived directly from an optimal control formulation in latent space and exhibits strong performance on a variety of complex control problems.</p>
                <p><strong>Paper Abstract:</strong> We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1402.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1402.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embed to Control (E2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational deep generative latent dynamics model that learns low-dimensional Gaussian latent states from raw images while enforcing local linearity of transitions for stochastic optimal control (SOC). Trained end-to-end with a KL contraction between transition and inference to enable long-horizon planning and control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Embed to Control (E2C)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder: neural network (fully-connected or convolutional) parameterizing a diagonal Gaussian approximate posterior Q_phi(z|x). Transition: a learned transformation network h_psi^{trans} that predicts local linearization matrices A_t, B_t and offset o_t so the next-latent distribution is Gaussian with mean A_t mu_t + B_t u_t + o_t and covariance C_t = A_t Sigma_t A_t^T + H_t. Decoder: up-convolutional or MLP decoder P_theta(x|z) (Bernoulli mean). Latent dynamics are explicitly constrained to be locally linear; A_t may be parameterized as I + v r^T (rank-one perturbation) for efficiency. Training via stochastic variational Bayes with an extra KL term to align transition-predicted latents with encoder latents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (locally linear latent dynamics VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual control from raw images (robotic control benchmarks: planar agent with obstacles, inverted pendulum swing-up, cart-pole balancing, three-link robot arm)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction log-likelihood (per-pixel Bernoulli log p(x|x_hat)), next-state log-likelihood log p(x_{t+1}|z_hat_{t+1}), KL divergences between latent distributions, and accumulated trajectory costs measured on simulator (real trajectory cost).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Examples from paper: Planar system: state loss 7.7 ± 2.0 (log p), next-state loss 9.7 ± 3.2, imagined trajectory cost 10.3 ± 2.8, real trajectory cost 25.1 ± 5.3, success 100%; Inverted pendulum: state loss 84.0 ± 50.8, next-state loss 89.3 ± 42.9, real trajectory cost 15.4 ± 3.4, success 90%; Cart-pole: real trajectory cost ~22.23 ± 14.89, success 93%; Three-link arm: real trajectory cost ~90.23 ± 47.38, success 90%. (Metrics reported in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderately interpretable: learned latent spaces are visualized and shown to align with true low-dimensional system topology (e.g., planar positions manifold, pendulum angle/velocity structure). Latent coordinates are meaningful for planning (distances used for quadratic costs). The model explicitly constrains latent dynamics to be locally linear, improving interpretability of transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of latent embeddings and 'dreamed' decoded trajectories (decoded images from rolled-out latent transitions), latent-space plots overlaid with true state; inspection of A_t, B_t (local linearization) as interpretable local dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Architectures range from small MLPs to deep conv/up-conv decoders (up to 12 layers end-to-end). Training datasets: plane N=3,000; pendulum/cart-pole N=15,000; robot arm N=30,000. Specific wall-clock/GPU training time not reported. Latent dims: 2 (plane), 3 (pendulum), 8 (cart-pole, arm). A_t parameterization reduces transition outputs (rank-one perturbation: 2*n_z params).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper reports E2C trains with comparable architecture/training procedure to VAEs but achieves far better control performance using the same datasets; rank-one A_t reduces parameters relative to a full n_z x n_z matrix, easing training. No explicit computation/time speedups (no GPU-days given).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>High task utility: achieves 100% success on planar task, 90% on inverted pendulum, 93% on cart-pole, 90% on three-link arm (as reported). Imagined trajectory costs and 'real' trajectory costs are close to those from controllers using the true model, though the true model still performs better.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Constrained local linearity in latent space leads to strong transfer from prediction fidelity to control performance: although E2C is not always the best at pure image reconstruction, its next-state prediction fidelity (in latent space aligned with encoder) yields reliable planning and high success rates. The KL contraction term ensuring alignment of transition outputs with encoder latents is essential for long-horizon utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off between reconstruction fidelity and control utility: some variants (non-linear latent dynamics) can have better reconstruction but worse control due to non-linearities not guaranteed to be locally linearizable for all control magnitudes. Enforcing local linearity can reduce expressiveness (worse raw reconstruction) but improves control robustness. Parameter-reduction for A_t (rank-one) reduces model complexity without loss of control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Gaussian diagonal posterior, Bernoulli decoder for binary images, local linear transition constraint (A_t,B_t,o_t), optional rank-one A_t = I + v r^T, explicit KL alignment term (weight lambda), small latent dimensionality chosen per task (2–8), transition network predicts linearization matrices conditioned on z.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to separately trained AE/VAE and VAE+slowness baselines (trained on same datasets), E2C produces far better control (baselines often 0% success). Compared to a non-linear latent-dynamics variant trained jointly, E2C (locally linear) yields better control despite sometimes worse reconstruction. Compared to the true system model used by SOC, E2C performance is close but worse (true model yields lower trajectory costs).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper's recommendation: enforce local linearity of latent transitions during learning, include a KL contraction term between transition-predicted latents and encoder latents, use small task-appropriate latent dimensionality, and parameterize A_t as a rank-one perturbation of identity for parameter efficiency. This balances fidelity, interpretability and computational tractability for control from images.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1402.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1402.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Global E2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Global Embed to Control (Global E2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of E2C where the transition matrices A, B and offset o are global (time-invariant) linear matrices rather than predicted locally by a transformation network.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Global E2C</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same encoder/decoder VAE structure as E2C but with a globally linear transition model: A_t, B_t and o_t are fixed learned matrices (not conditioned on z). The remainder of training objective (reconstruction, latent KL and KL contraction) remains similar.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (globally linear latent dynamics VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual control from raw images (same tasks as E2C: planar agent, inverted pendulum, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Same as E2C: reconstruction log-likelihood, next-state log-likelihood, trajectory costs, and success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Planar: state loss 6.9 ± 3.2, next-state loss 9.3 ± 4.6, imagined traj. cost 12.5 ± 3.9, real traj. cost 27.3 ± 9.7, success 100% (paper table). Inverted pendulum: poor performance overall (state loss 115.5 ± 56.9, next-state 125.3 ± 62.6, trajectory cost 628.1 ±45.9, success 0%).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Interpretability moderate: simple global linear dynamics are transparent (single A,B matrices), but may fail to capture state-dependent nonlinearities; latent visualization shows less faithful alignment for tasks requiring nonlinear local dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent-space visualization and decoded 'dreamed' rollouts; inspection of global A and B matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Slightly lower parameter count for transition network since no transformation network required; same encoder/decoder costs as E2C. Training details (time) not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Simpler transition reduces parameters and may be easier to train, but inability to model state-dependent dynamics degrades task performance on more nonlinear tasks (e.g., inverted pendulum).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Good on simple planar task (100% success) but fails on more nonlinear domains requiring local linearizations (0% success on inverted pendulum).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Global linear dynamics are sufficient for some tasks with near-global linearity (planar agent), but where dynamics are strongly state-dependent or have unstable fixed points (inverted pendulum), global linearity fails despite decent reconstruction metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off: greatly reduced model complexity and parameter count versus inability to represent locally varying dynamics; yields strong performance in simple domains but collapses in complex nonlinear tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Replace transformation network with constant A,B,o learned globally; keep same encoder/decoder and losses (including KL contraction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to E2C, Global E2C has fewer parameters and sometimes better reconstruction but worse control on tasks requiring local linearity. Compared to VAEs/AE it can outperform them in some tasks but lacks flexibility of locally linear prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests Global E2C is acceptable for simpler problems but for challenging control tasks the locally linear (conditional) transition (E2C) is preferred.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1402.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1402.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-linear E2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-linear E2C (non-linear latent dynamics variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant where the transition model is a non-linear function hat{f}^{lat} (neural network) predicting next latent, and local Jacobians are computed from this network for planning (i.e., linearize a non-linear learned dynamics during planning rather than enforcing local linearity during learning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Non-linear E2C</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder/decoder as in E2C; transition modeled by a non-linear neural network ̂f^{lat}(z,u) trained to predict next latent; during planning the network is linearized (Jacobian computed) to obtain A_t,B_t for SOC.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (non-linear latent dynamics with linearization at planning time)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual control from raw images (same robotics/control domains)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction log-likelihood, next-state loss, trajectory costs, success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Planar: state loss 8.3 ± 5.5, next-state loss 11.3 ± 10.1, real trajectory cost 42.3 ± 16.4, success 96.6%; Inverted pendulum: state loss 59.6 ± 25.2, next-state 72.6 ± 34.5, trajectory cost 313.3 ± 65.7, success 63.33%; cart-pole / arm: worse than locally linear E2C (lower success and higher costs).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Less interpretable than locally linear E2C because dynamics are captured by a black-box non-linear network; interpretation requires Jacobian inspection and latent visualizations but local linearity is not guaranteed.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of latent spaces and decoded rollouts; inspection of learned non-linear dynamics and computed Jacobians at planning points.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Transition network is larger (non-linear MLP) and computing Jacobians for planning adds computational overhead; exact training/inference time not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Potentially more expressive per parameter but less reliable for planning: non-linear dynamics require accurate linearization around operating points and can be less sample-efficient for producing locally linearizable dynamics necessary for SOC.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Mixed: can achieve near-E2C performance on some tasks (plane success ~96.6%) but significantly worse on others (pendulum and robot arm) where enforced local linearity helps. Generally lower success rates and higher real trajectory costs than E2C on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Higher reconstruction or prediction fidelity (in some metrics) does not necessarily translate to better control: the non-linear model may fail to be reliably linearizable across control magnitudes, harming SOC performance and stability near unstable fixed points.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Expressiveness vs. controllability: non-linear latent dynamics increase representational fidelity but reduce guarantees for effective local linearization required by SOC, harming downstream control utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use non-linear MLP to predict next latent, compute Jacobians for planning, do not explicitly constrain transitions to be locally linear during learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to E2C, offers greater modeling flexibility but worse control robustness in several benchmarks. Compared to VAEs/AE, non-linear E2C is jointly trained and generally better for control but still inferior to locally linear E2C for tasks that require reliable local linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests non-linear latent dynamics can be viable for some tasks but for robust SOC from pixels, enforcing local linearity during learning (E2C) is preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1402.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1402.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder (baseline VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard variational autoencoder trained only for image autoencoding (no dynamics in training); a dynamics model is learned separately on the learned latent representations for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auto-encoding variational bayes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Variational Autoencoder (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder/decoder VAE with diagonal Gaussian posterior Q_phi(z|x) and decoder P_theta(x|z). For experiments here, VAEs are trained on images only (no action-conditioned transition during VAE training); an additional separate dynamics model is trained post-hoc on the learned latent codes to predict next z given z and u.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (separately trained latent dynamics on top of a VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual control (used as baseline on planar and pendulum tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction log-likelihood, next-state loss once separate dynamics is trained, trajectory cost and success rate for control using the learned latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Planar: state loss 3.6 ± 18.9 (reported), next-state loss 652.1 ± 930.6, imagined trajectory cost 43.1 ± 20.8, real trajectory cost 91.3 ± 16.4, success 0%; Pendulum: similarly poor next-state performance and 0% success in control.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent spaces are visually coherent but do not necessarily align with dynamics required for control; do not by themselves encode locally linearizable transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent-space visualization (used to show that VAE latent does not capture control-relevant structure); no special interpretability enhancements used.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Standard VAE training costs (architectures similar to E2C encoder/decoder); training separate dynamics model adds extra cost. Exact runtimes not given.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Using same amount of data and comparable architectures, VAEs produce good reconstructions but require additional modeling and fail to provide latent structure needed for SOC; thus they are less sample- and task-efficient for control than E2C.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Poor for downstream control in experiments: typically 0% success on planar and pendulum tasks when combined with separately trained dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High reconstruction fidelity alone is insufficient; VAEs trained without dynamics constraints do not produce latents that support accurate multi-step predictions or planning, so task utility is low despite sometimes good reconstruction metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Separation of representation learning and dynamics modeling (train VAE then dynamics) yields strong reconstructions but weak multi-step predictive fidelity and poor control; joint training with dynamics constraints (E2C) is preferable for control.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Standard diagonal Gaussian posterior VAE trained on images only; separate dynamics MLP trained later on learned latents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>E2C outperforms VAE baselines substantially on control tasks, despite similar or sometimes better raw reconstruction metrics for VAE.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper implies that VAEs by themselves are insufficient for control from pixels unless combined with additional constraints that align dynamics with latent structure (as in E2C).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1402.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1402.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AE (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoencoder (baseline AE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard deterministic autoencoder trained for image reconstruction only; a dynamics model is trained separately on learned codes for comparison to E2C.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoencoder (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic encoder/decoder MLP trained to reconstruct images; after training, a separate dynamics model is trained on the deterministic latent codes to predict next latent given action.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (separately trained deterministic latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual control (baseline for planar and pendulum tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction error (cross-entropy for Bernoulli decoder), next-state loss from separately trained dynamics, trajectory costs and success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Planar: reported extremely poor next-state loss (3538.9 ± 1395.2) and real trajectory cost 273.3 ± 16.4, success 0%; similar failure on pendulum.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent spaces can be visually plausible but do not encode dynamics-ready representations; poor multi-step prediction fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent-space visualization; decoded dreamed trajectories show divergence for AE baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Standard AE training costs; separate dynamics model training required. No explicit timing or parameter counts reported beyond architectures used.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less effective than E2C for control despite reconstructive ability; sample and task efficiency poor for SOC tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Poor: baselines fail to produce workable controllers (0% success on key tasks in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>AE reconstructions do not imply latent structures that permit accurate long-horizon predictions; therefore task utility for control is very low.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High reconstruction quality (for some AEs) but severe degradation in predictive control utility; decoupling reconstruction from dynamics learning is costly for control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Deterministic MLP encoder/decoder trained on images only; separate dynamics MLP trained afterwards.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>E2C and its locally linear constraint substantially outperform AE baselines on control despite similar decoder capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not advocate AE-only pipelines for control — joint training with transition constraints is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1402.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1402.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE+slowness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder with Slowness Term</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VAE variant augmented with a temporal slowness regularizer (KL divergence between adjacent-time posterior latents) intended to encourage temporally coherent latent representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VAE + slowness</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard VAE objective augmented with an extra per-pair KL term KL(Q_phi(z_{t+1}|x_{t+1}) || Q_phi(z_t|x_t)) to encourage slow-changing latents across consecutive frames. Dynamics model trained separately afterward on the latent codes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE with temporal regularization)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual control (baseline on planar, pendulum, cart-pole, and arm tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction log-likelihood, next-state loss, trajectory costs, and success percent for control.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Planar: state loss 10.5 ± 22.8, next-state loss 104.3 ± 235.8, real trajectory cost 89.1 ± 16.4, success 0%; Cart-pole and arm baseline results also poor (0% success).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Slightly more coherent latent neighborhoods (temporally smoother) compared to vanilla VAE, but still insufficiently structured for reliable long-horizon dynamics and control.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent-space visualization showing increased coherence; no further interpretability aids.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Marginally higher due to paired-sample KL term and training on image-pairs; otherwise similar to VAE.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Adds a simple regularizer but does not produce the necessary structure for control; still substantially worse than E2C given same data and architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Fails on control tasks in experiments (0% success in multiple benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Temporal slowness alone does not produce latent representations that permit accurate locally linear predictions across control magnitudes, hence low task utility despite temporal coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Adds temporal coherence at little extra cost but insufficient to achieve control utility; does not substitute for transition-aligned training.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Extra KL between consecutive encodings added to VAE objective (slowness term).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed by E2C on all control benchmarks; better than vanilla VAE in temporal smoothness but still inadequate for SOC.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests slowness is not enough — enforcing dynamics-consistent latent transitions (as in E2C) is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1402.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1402.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>True model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>True system model (oracle dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ground-truth simulator dynamics used as an upper-bound reference: SOC applied to true low-dimensional state s_t rather than learned latents; used to benchmark achievable control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>True system model (oracle dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The actual simulator dynamics f(s_t,u_t) (used to compute optimal controls with iLQR/AICO operating directly on true states) providing an upper-bound baseline for trajectory cost and success.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit/ground-truth world model (oracle dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>same control benchmarks (planar, pendulum, cart-pole, robot arm) used as performance upper bound</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Direct true-state trajectory cost (quadratic costs on deviation from goal), success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported for reference: Planar true-model real trajectory cost 20.24 ± 4.15; Pendulum 9.8 ± 2.4; Cart-pole 15.33 ± 7.70; Three-link arm 59.46 (paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Fully interpretable as it is the known physical/simulated dynamics used by SOC; not learned so interpretability of learned representation not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>N/A (explicit known model; used for benchmarking).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>SOC on true model used as baseline; computational cost for planning lower because dynamics are explicit and low-dimensional (exact timings not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Generates best possible control performance (lower trajectory costs) since it uses exact low-dimensional state dynamics; learning-based models (E2C) approach but do not surpass it.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Best across benchmarks (100% success in many cases and lowest trajectory costs reported per task).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Serves as reference: shows the gap between learned latent world models and optimal control with the true model; E2C closes much of the gap but not completely.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Oracle has best performance but is not available in real-world scenarios where only raw sensory observations exist; learned models trade some performance for applicability from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>N/A (not a learned model in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>True model yields lower trajectory costs and higher success percentages than learned models; E2C is the closest among learned models.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>N/A — used as upper-bound baseline to evaluate learned world models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>From pixels to torques: Policy learning with deep dynamical models <em>(Rating: 2)</em></li>
                <li>Auto-encoding variational bayes <em>(Rating: 1)</em></li>
                <li>Stochastic backpropagation and approximate inference in deep generative models <em>(Rating: 1)</em></li>
                <li>Learning stochastic recurrent networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1402",
    "paper_id": "paper-bb1a17010254abfa5e1f2a17553582ce449f8e16",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "E2C",
            "name_full": "Embed to Control (E2C)",
            "brief_description": "A variational deep generative latent dynamics model that learns low-dimensional Gaussian latent states from raw images while enforcing local linearity of transitions for stochastic optimal control (SOC). Trained end-to-end with a KL contraction between transition and inference to enable long-horizon planning and control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Embed to Control (E2C)",
            "model_description": "Encoder: neural network (fully-connected or convolutional) parameterizing a diagonal Gaussian approximate posterior Q_phi(z|x). Transition: a learned transformation network h_psi^{trans} that predicts local linearization matrices A_t, B_t and offset o_t so the next-latent distribution is Gaussian with mean A_t mu_t + B_t u_t + o_t and covariance C_t = A_t Sigma_t A_t^T + H_t. Decoder: up-convolutional or MLP decoder P_theta(x|z) (Bernoulli mean). Latent dynamics are explicitly constrained to be locally linear; A_t may be parameterized as I + v r^T (rank-one perturbation) for efficiency. Training via stochastic variational Bayes with an extra KL term to align transition-predicted latents with encoder latents.",
            "model_type": "latent world model (locally linear latent dynamics VAE)",
            "task_domain": "visual control from raw images (robotic control benchmarks: planar agent with obstacles, inverted pendulum swing-up, cart-pole balancing, three-link robot arm)",
            "fidelity_metric": "Reconstruction log-likelihood (per-pixel Bernoulli log p(x|x_hat)), next-state log-likelihood log p(x_{t+1}|z_hat_{t+1}), KL divergences between latent distributions, and accumulated trajectory costs measured on simulator (real trajectory cost).",
            "fidelity_performance": "Examples from paper: Planar system: state loss 7.7 ± 2.0 (log p), next-state loss 9.7 ± 3.2, imagined trajectory cost 10.3 ± 2.8, real trajectory cost 25.1 ± 5.3, success 100%; Inverted pendulum: state loss 84.0 ± 50.8, next-state loss 89.3 ± 42.9, real trajectory cost 15.4 ± 3.4, success 90%; Cart-pole: real trajectory cost ~22.23 ± 14.89, success 93%; Three-link arm: real trajectory cost ~90.23 ± 47.38, success 90%. (Metrics reported in paper tables.)",
            "interpretability_assessment": "Moderately interpretable: learned latent spaces are visualized and shown to align with true low-dimensional system topology (e.g., planar positions manifold, pendulum angle/velocity structure). Latent coordinates are meaningful for planning (distances used for quadratic costs). The model explicitly constrains latent dynamics to be locally linear, improving interpretability of transitions.",
            "interpretability_method": "Visualization of latent embeddings and 'dreamed' decoded trajectories (decoded images from rolled-out latent transitions), latent-space plots overlaid with true state; inspection of A_t, B_t (local linearization) as interpretable local dynamics.",
            "computational_cost": "Architectures range from small MLPs to deep conv/up-conv decoders (up to 12 layers end-to-end). Training datasets: plane N=3,000; pendulum/cart-pole N=15,000; robot arm N=30,000. Specific wall-clock/GPU training time not reported. Latent dims: 2 (plane), 3 (pendulum), 8 (cart-pole, arm). A_t parameterization reduces transition outputs (rank-one perturbation: 2*n_z params).",
            "efficiency_comparison": "Paper reports E2C trains with comparable architecture/training procedure to VAEs but achieves far better control performance using the same datasets; rank-one A_t reduces parameters relative to a full n_z x n_z matrix, easing training. No explicit computation/time speedups (no GPU-days given).",
            "task_performance": "High task utility: achieves 100% success on planar task, 90% on inverted pendulum, 93% on cart-pole, 90% on three-link arm (as reported). Imagined trajectory costs and 'real' trajectory costs are close to those from controllers using the true model, though the true model still performs better.",
            "task_utility_analysis": "Constrained local linearity in latent space leads to strong transfer from prediction fidelity to control performance: although E2C is not always the best at pure image reconstruction, its next-state prediction fidelity (in latent space aligned with encoder) yields reliable planning and high success rates. The KL contraction term ensuring alignment of transition outputs with encoder latents is essential for long-horizon utility.",
            "tradeoffs_observed": "Trade-off between reconstruction fidelity and control utility: some variants (non-linear latent dynamics) can have better reconstruction but worse control due to non-linearities not guaranteed to be locally linearizable for all control magnitudes. Enforcing local linearity can reduce expressiveness (worse raw reconstruction) but improves control robustness. Parameter-reduction for A_t (rank-one) reduces model complexity without loss of control performance.",
            "design_choices": "Gaussian diagonal posterior, Bernoulli decoder for binary images, local linear transition constraint (A_t,B_t,o_t), optional rank-one A_t = I + v r^T, explicit KL alignment term (weight lambda), small latent dimensionality chosen per task (2–8), transition network predicts linearization matrices conditioned on z.",
            "comparison_to_alternatives": "Compared to separately trained AE/VAE and VAE+slowness baselines (trained on same datasets), E2C produces far better control (baselines often 0% success). Compared to a non-linear latent-dynamics variant trained jointly, E2C (locally linear) yields better control despite sometimes worse reconstruction. Compared to the true system model used by SOC, E2C performance is close but worse (true model yields lower trajectory costs).",
            "optimal_configuration": "Paper's recommendation: enforce local linearity of latent transitions during learning, include a KL contraction term between transition-predicted latents and encoder latents, use small task-appropriate latent dimensionality, and parameterize A_t as a rank-one perturbation of identity for parameter efficiency. This balances fidelity, interpretability and computational tractability for control from images.",
            "uuid": "e1402.0",
            "source_info": {
                "paper_title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "Global E2C",
            "name_full": "Global Embed to Control (Global E2C)",
            "brief_description": "A variant of E2C where the transition matrices A, B and offset o are global (time-invariant) linear matrices rather than predicted locally by a transformation network.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Global E2C",
            "model_description": "Same encoder/decoder VAE structure as E2C but with a globally linear transition model: A_t, B_t and o_t are fixed learned matrices (not conditioned on z). The remainder of training objective (reconstruction, latent KL and KL contraction) remains similar.",
            "model_type": "latent world model (globally linear latent dynamics VAE)",
            "task_domain": "visual control from raw images (same tasks as E2C: planar agent, inverted pendulum, etc.)",
            "fidelity_metric": "Same as E2C: reconstruction log-likelihood, next-state log-likelihood, trajectory costs, and success rates.",
            "fidelity_performance": "Planar: state loss 6.9 ± 3.2, next-state loss 9.3 ± 4.6, imagined traj. cost 12.5 ± 3.9, real traj. cost 27.3 ± 9.7, success 100% (paper table). Inverted pendulum: poor performance overall (state loss 115.5 ± 56.9, next-state 125.3 ± 62.6, trajectory cost 628.1 ±45.9, success 0%).",
            "interpretability_assessment": "Interpretability moderate: simple global linear dynamics are transparent (single A,B matrices), but may fail to capture state-dependent nonlinearities; latent visualization shows less faithful alignment for tasks requiring nonlinear local dynamics.",
            "interpretability_method": "Latent-space visualization and decoded 'dreamed' rollouts; inspection of global A and B matrices.",
            "computational_cost": "Slightly lower parameter count for transition network since no transformation network required; same encoder/decoder costs as E2C. Training details (time) not specified.",
            "efficiency_comparison": "Simpler transition reduces parameters and may be easier to train, but inability to model state-dependent dynamics degrades task performance on more nonlinear tasks (e.g., inverted pendulum).",
            "task_performance": "Good on simple planar task (100% success) but fails on more nonlinear domains requiring local linearizations (0% success on inverted pendulum).",
            "task_utility_analysis": "Global linear dynamics are sufficient for some tasks with near-global linearity (planar agent), but where dynamics are strongly state-dependent or have unstable fixed points (inverted pendulum), global linearity fails despite decent reconstruction metrics.",
            "tradeoffs_observed": "Trade-off: greatly reduced model complexity and parameter count versus inability to represent locally varying dynamics; yields strong performance in simple domains but collapses in complex nonlinear tasks.",
            "design_choices": "Replace transformation network with constant A,B,o learned globally; keep same encoder/decoder and losses (including KL contraction).",
            "comparison_to_alternatives": "Compared to E2C, Global E2C has fewer parameters and sometimes better reconstruction but worse control on tasks requiring local linearity. Compared to VAEs/AE it can outperform them in some tasks but lacks flexibility of locally linear prediction.",
            "optimal_configuration": "Paper suggests Global E2C is acceptable for simpler problems but for challenging control tasks the locally linear (conditional) transition (E2C) is preferred.",
            "uuid": "e1402.1",
            "source_info": {
                "paper_title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "Non-linear E2C",
            "name_full": "Non-linear E2C (non-linear latent dynamics variant)",
            "brief_description": "A variant where the transition model is a non-linear function hat{f}^{lat} (neural network) predicting next latent, and local Jacobians are computed from this network for planning (i.e., linearize a non-linear learned dynamics during planning rather than enforcing local linearity during learning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Non-linear E2C",
            "model_description": "Encoder/decoder as in E2C; transition modeled by a non-linear neural network ̂f^{lat}(z,u) trained to predict next latent; during planning the network is linearized (Jacobian computed) to obtain A_t,B_t for SOC.",
            "model_type": "latent world model (non-linear latent dynamics with linearization at planning time)",
            "task_domain": "visual control from raw images (same robotics/control domains)",
            "fidelity_metric": "Reconstruction log-likelihood, next-state loss, trajectory costs, success rate.",
            "fidelity_performance": "Planar: state loss 8.3 ± 5.5, next-state loss 11.3 ± 10.1, real trajectory cost 42.3 ± 16.4, success 96.6%; Inverted pendulum: state loss 59.6 ± 25.2, next-state 72.6 ± 34.5, trajectory cost 313.3 ± 65.7, success 63.33%; cart-pole / arm: worse than locally linear E2C (lower success and higher costs).",
            "interpretability_assessment": "Less interpretable than locally linear E2C because dynamics are captured by a black-box non-linear network; interpretation requires Jacobian inspection and latent visualizations but local linearity is not guaranteed.",
            "interpretability_method": "Visualization of latent spaces and decoded rollouts; inspection of learned non-linear dynamics and computed Jacobians at planning points.",
            "computational_cost": "Transition network is larger (non-linear MLP) and computing Jacobians for planning adds computational overhead; exact training/inference time not reported.",
            "efficiency_comparison": "Potentially more expressive per parameter but less reliable for planning: non-linear dynamics require accurate linearization around operating points and can be less sample-efficient for producing locally linearizable dynamics necessary for SOC.",
            "task_performance": "Mixed: can achieve near-E2C performance on some tasks (plane success ~96.6%) but significantly worse on others (pendulum and robot arm) where enforced local linearity helps. Generally lower success rates and higher real trajectory costs than E2C on complex tasks.",
            "task_utility_analysis": "Higher reconstruction or prediction fidelity (in some metrics) does not necessarily translate to better control: the non-linear model may fail to be reliably linearizable across control magnitudes, harming SOC performance and stability near unstable fixed points.",
            "tradeoffs_observed": "Expressiveness vs. controllability: non-linear latent dynamics increase representational fidelity but reduce guarantees for effective local linearization required by SOC, harming downstream control utility.",
            "design_choices": "Use non-linear MLP to predict next latent, compute Jacobians for planning, do not explicitly constrain transitions to be locally linear during learning.",
            "comparison_to_alternatives": "Compared to E2C, offers greater modeling flexibility but worse control robustness in several benchmarks. Compared to VAEs/AE, non-linear E2C is jointly trained and generally better for control but still inferior to locally linear E2C for tasks that require reliable local linearization.",
            "optimal_configuration": "Paper suggests non-linear latent dynamics can be viable for some tasks but for robust SOC from pixels, enforcing local linearity during learning (E2C) is preferable.",
            "uuid": "e1402.2",
            "source_info": {
                "paper_title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "VAE (baseline)",
            "name_full": "Variational Autoencoder (baseline VAE)",
            "brief_description": "A standard variational autoencoder trained only for image autoencoding (no dynamics in training); a dynamics model is learned separately on the learned latent representations for comparison.",
            "citation_title": "Auto-encoding variational bayes",
            "mention_or_use": "use",
            "model_name": "Variational Autoencoder (baseline)",
            "model_description": "Encoder/decoder VAE with diagonal Gaussian posterior Q_phi(z|x) and decoder P_theta(x|z). For experiments here, VAEs are trained on images only (no action-conditioned transition during VAE training); an additional separate dynamics model is trained post-hoc on the learned latent codes to predict next z given z and u.",
            "model_type": "latent world model (separately trained latent dynamics on top of a VAE)",
            "task_domain": "visual control (used as baseline on planar and pendulum tasks)",
            "fidelity_metric": "Reconstruction log-likelihood, next-state loss once separate dynamics is trained, trajectory cost and success rate for control using the learned latent dynamics.",
            "fidelity_performance": "Planar: state loss 3.6 ± 18.9 (reported), next-state loss 652.1 ± 930.6, imagined trajectory cost 43.1 ± 20.8, real trajectory cost 91.3 ± 16.4, success 0%; Pendulum: similarly poor next-state performance and 0% success in control.",
            "interpretability_assessment": "Latent spaces are visually coherent but do not necessarily align with dynamics required for control; do not by themselves encode locally linearizable transitions.",
            "interpretability_method": "Latent-space visualization (used to show that VAE latent does not capture control-relevant structure); no special interpretability enhancements used.",
            "computational_cost": "Standard VAE training costs (architectures similar to E2C encoder/decoder); training separate dynamics model adds extra cost. Exact runtimes not given.",
            "efficiency_comparison": "Using same amount of data and comparable architectures, VAEs produce good reconstructions but require additional modeling and fail to provide latent structure needed for SOC; thus they are less sample- and task-efficient for control than E2C.",
            "task_performance": "Poor for downstream control in experiments: typically 0% success on planar and pendulum tasks when combined with separately trained dynamics.",
            "task_utility_analysis": "High reconstruction fidelity alone is insufficient; VAEs trained without dynamics constraints do not produce latents that support accurate multi-step predictions or planning, so task utility is low despite sometimes good reconstruction metrics.",
            "tradeoffs_observed": "Separation of representation learning and dynamics modeling (train VAE then dynamics) yields strong reconstructions but weak multi-step predictive fidelity and poor control; joint training with dynamics constraints (E2C) is preferable for control.",
            "design_choices": "Standard diagonal Gaussian posterior VAE trained on images only; separate dynamics MLP trained later on learned latents.",
            "comparison_to_alternatives": "E2C outperforms VAE baselines substantially on control tasks, despite similar or sometimes better raw reconstruction metrics for VAE.",
            "optimal_configuration": "Paper implies that VAEs by themselves are insufficient for control from pixels unless combined with additional constraints that align dynamics with latent structure (as in E2C).",
            "uuid": "e1402.3",
            "source_info": {
                "paper_title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "AE (baseline)",
            "name_full": "Autoencoder (baseline AE)",
            "brief_description": "A standard deterministic autoencoder trained for image reconstruction only; a dynamics model is trained separately on learned codes for comparison to E2C.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Autoencoder (baseline)",
            "model_description": "Deterministic encoder/decoder MLP trained to reconstruct images; after training, a separate dynamics model is trained on the deterministic latent codes to predict next latent given action.",
            "model_type": "latent world model (separately trained deterministic latent dynamics)",
            "task_domain": "visual control (baseline for planar and pendulum tasks)",
            "fidelity_metric": "Reconstruction error (cross-entropy for Bernoulli decoder), next-state loss from separately trained dynamics, trajectory costs and success rates.",
            "fidelity_performance": "Planar: reported extremely poor next-state loss (3538.9 ± 1395.2) and real trajectory cost 273.3 ± 16.4, success 0%; similar failure on pendulum.",
            "interpretability_assessment": "Latent spaces can be visually plausible but do not encode dynamics-ready representations; poor multi-step prediction fidelity.",
            "interpretability_method": "Latent-space visualization; decoded dreamed trajectories show divergence for AE baselines.",
            "computational_cost": "Standard AE training costs; separate dynamics model training required. No explicit timing or parameter counts reported beyond architectures used.",
            "efficiency_comparison": "Less effective than E2C for control despite reconstructive ability; sample and task efficiency poor for SOC tasks.",
            "task_performance": "Poor: baselines fail to produce workable controllers (0% success on key tasks in experiments).",
            "task_utility_analysis": "AE reconstructions do not imply latent structures that permit accurate long-horizon predictions; therefore task utility for control is very low.",
            "tradeoffs_observed": "High reconstruction quality (for some AEs) but severe degradation in predictive control utility; decoupling reconstruction from dynamics learning is costly for control tasks.",
            "design_choices": "Deterministic MLP encoder/decoder trained on images only; separate dynamics MLP trained afterwards.",
            "comparison_to_alternatives": "E2C and its locally linear constraint substantially outperform AE baselines on control despite similar decoder capacity.",
            "optimal_configuration": "Paper does not advocate AE-only pipelines for control — joint training with transition constraints is recommended.",
            "uuid": "e1402.4",
            "source_info": {
                "paper_title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "VAE+slowness",
            "name_full": "Variational Autoencoder with Slowness Term",
            "brief_description": "A VAE variant augmented with a temporal slowness regularizer (KL divergence between adjacent-time posterior latents) intended to encourage temporally coherent latent representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VAE + slowness",
            "model_description": "Standard VAE objective augmented with an extra per-pair KL term KL(Q_phi(z_{t+1}|x_{t+1}) || Q_phi(z_t|x_t)) to encourage slow-changing latents across consecutive frames. Dynamics model trained separately afterward on the latent codes.",
            "model_type": "latent world model (VAE with temporal regularization)",
            "task_domain": "visual control (baseline on planar, pendulum, cart-pole, and arm tasks)",
            "fidelity_metric": "Reconstruction log-likelihood, next-state loss, trajectory costs, and success percent for control.",
            "fidelity_performance": "Planar: state loss 10.5 ± 22.8, next-state loss 104.3 ± 235.8, real trajectory cost 89.1 ± 16.4, success 0%; Cart-pole and arm baseline results also poor (0% success).",
            "interpretability_assessment": "Slightly more coherent latent neighborhoods (temporally smoother) compared to vanilla VAE, but still insufficiently structured for reliable long-horizon dynamics and control.",
            "interpretability_method": "Latent-space visualization showing increased coherence; no further interpretability aids.",
            "computational_cost": "Marginally higher due to paired-sample KL term and training on image-pairs; otherwise similar to VAE.",
            "efficiency_comparison": "Adds a simple regularizer but does not produce the necessary structure for control; still substantially worse than E2C given same data and architectures.",
            "task_performance": "Fails on control tasks in experiments (0% success in multiple benchmarks).",
            "task_utility_analysis": "Temporal slowness alone does not produce latent representations that permit accurate locally linear predictions across control magnitudes, hence low task utility despite temporal coherence.",
            "tradeoffs_observed": "Adds temporal coherence at little extra cost but insufficient to achieve control utility; does not substitute for transition-aligned training.",
            "design_choices": "Extra KL between consecutive encodings added to VAE objective (slowness term).",
            "comparison_to_alternatives": "Outperformed by E2C on all control benchmarks; better than vanilla VAE in temporal smoothness but still inadequate for SOC.",
            "optimal_configuration": "Paper suggests slowness is not enough — enforcing dynamics-consistent latent transitions (as in E2C) is necessary.",
            "uuid": "e1402.5",
            "source_info": {
                "paper_title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "True model",
            "name_full": "True system model (oracle dynamics)",
            "brief_description": "Ground-truth simulator dynamics used as an upper-bound reference: SOC applied to true low-dimensional state s_t rather than learned latents; used to benchmark achievable control performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "True system model (oracle dynamics)",
            "model_description": "The actual simulator dynamics f(s_t,u_t) (used to compute optimal controls with iLQR/AICO operating directly on true states) providing an upper-bound baseline for trajectory cost and success.",
            "model_type": "explicit/ground-truth world model (oracle dynamics)",
            "task_domain": "same control benchmarks (planar, pendulum, cart-pole, robot arm) used as performance upper bound",
            "fidelity_metric": "Direct true-state trajectory cost (quadratic costs on deviation from goal), success rates.",
            "fidelity_performance": "Reported for reference: Planar true-model real trajectory cost 20.24 ± 4.15; Pendulum 9.8 ± 2.4; Cart-pole 15.33 ± 7.70; Three-link arm 59.46 (paper table).",
            "interpretability_assessment": "Fully interpretable as it is the known physical/simulated dynamics used by SOC; not learned so interpretability of learned representation not applicable.",
            "interpretability_method": "N/A (explicit known model; used for benchmarking).",
            "computational_cost": "SOC on true model used as baseline; computational cost for planning lower because dynamics are explicit and low-dimensional (exact timings not reported).",
            "efficiency_comparison": "Generates best possible control performance (lower trajectory costs) since it uses exact low-dimensional state dynamics; learning-based models (E2C) approach but do not surpass it.",
            "task_performance": "Best across benchmarks (100% success in many cases and lowest trajectory costs reported per task).",
            "task_utility_analysis": "Serves as reference: shows the gap between learned latent world models and optimal control with the true model; E2C closes much of the gap but not completely.",
            "tradeoffs_observed": "Oracle has best performance but is not available in real-world scenarios where only raw sensory observations exist; learned models trade some performance for applicability from pixels.",
            "design_choices": "N/A (not a learned model in this work).",
            "comparison_to_alternatives": "True model yields lower trajectory costs and higher success percentages than learned models; E2C is the closest among learned models.",
            "optimal_configuration": "N/A — used as upper-bound baseline to evaluate learned world models.",
            "uuid": "e1402.6",
            "source_info": {
                "paper_title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
                "publication_date_yy_mm": "2015-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "From pixels to torques: Policy learning with deep dynamical models",
            "rating": 2
        },
        {
            "paper_title": "Auto-encoding variational bayes",
            "rating": 1
        },
        {
            "paper_title": "Stochastic backpropagation and approximate inference in deep generative models",
            "rating": 1
        },
        {
            "paper_title": "Learning stochastic recurrent networks",
            "rating": 1
        }
    ],
    "cost": 0.019937,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images</h1>
<p>Manuel Watter<em> Jost Tobias Springenberg</em><br>Joschka Boedecker<br>University of Freiburg, Germany<br>{watterm, springj, jboedeck}@cs.uni-freiburg.de</p>
<p>Martin Riedmiller<br>Google DeepMind<br>London, UK<br>riedmiller@google.com</p>
<h4>Abstract</h4>
<p>We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.</p>
<h2>1 Introduction</h2>
<p>Control of non-linear dynamical systems with continuous state and action spaces is one of the key problems in robotics and, in a broader context, in reinforcement learning for autonomous agents. A prominent class of algorithms that aim to solve this problem are model-based locally optimal (stochastic) control algorithms such as iLQG control [1, 2], which approximate the general nonlinear control problem via local linearization. When combined with receding horizon control [3], and machine learning methods for learning approximate system models, such algorithms are powerful tools for solving complicated control problems [3, 4, 5]; however, they either rely on a known system model or require the design of relatively low-dimensional state representations. For real autonomous agents to succeed, we ultimately need algorithms that are capable of controlling complex dynamical systems from raw sensory input (e.g. images) only. In this paper we tackle this difficult problem.</p>
<p>If stochastic optimal control (SOC) methods were applied directly to control from raw image data, they would face two major obstacles. First, sensory data is usually high-dimensional - i.e. images with thousands of pixels - rendering a naive SOC solution computationally infeasible. Second, the image content is typically a highly non-linear function of the system dynamics underlying the observations; thus model identification and control of this dynamics are non-trivial.</p>
<p>While both problems could, in principle, be addressed by designing more advanced SOC algorithms we approach the "optimal control from raw images" problem differently: turning the problem of locally optimal control in high-dimensional non-linear systems into one of identifying a low-dimensional latent state space, in which locally optimal control can be performed robustly and easily. To learn such a latent space we propose a new deep generative model belonging to the class of variational autoencoders [6, 7] that is derived from an iLQG formulation in latent space. The resulting Embed to Control (E2C) system is a probabilistic generative model that holds a belief over viable trajectories in sensory space, allows for accurate long-term planning in latent space, and is trained fully unsupervised. We demonstrate the success of our approach on four challenging tasks for control from raw images and compare it to a range of methods for unsupervised representation learning. As an aside, we also validate that deep up-convolutional networks [8, 9] are powerful generative models for large images.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2 The Embed to Control (E2C) model</p>
<p>We briefly review the problem of SOC for dynamical systems, introduce approximate locally optimal control in latent space, and finish with the derivation of our model.</p>
<h3>2.1 Problem Formulation</h3>
<p>We consider the control of unknown dynamical systems of the form</p>
<p>$\mathbf{s}<em t="t">{t+1}=f(\mathbf{s}</em>},\mathbf{u<em _boldsymbol_xi="\boldsymbol{\xi">{t})+\boldsymbol{\xi}, \boldsymbol{\xi} \sim \mathcal{N}\left(0, \boldsymbol{\Sigma}</em>\right),$ (1)}</p>
<p>where $t$ denotes the time steps, $\mathbf{s}<em x="x">{t} \in \mathbb{R}^{n</em>}}$ the system state, $\mathbf{u<em u="u">{t} \in \mathbb{R}^{n</em>}}$ the applied control and $\boldsymbol{\xi}$ the system noise. The function $f\left(\mathbf{s<em t="t">{t}, \mathbf{u}</em>}\right)$ is an arbitrary, smooth, system dynamics. We equivalently refer to Equation (1) using the notation $P\left(\mathbf{s<em t="t">{t+1} \mid \mathbf{s}</em>}, \mathbf{u<em t="t">{t}\right)$, which we assume to be a multivariate normal distribution $\mathcal{N}\left(f\left(\mathbf{s}</em>}, \mathbf{u<em _xi="\xi">{t}\right), \boldsymbol{\Sigma}</em>}\right)$. We further assume that we are only given access to visual depictions $\mathbf{x<em x="x">{t} \in \mathbb{R}^{n</em>}}$ of state $\mathbf{s<em t="t">{t}$. This restriction requires solving a joint state identification and control problem. For simplicity we will in the following assume that $\mathbf{x}</em>$, but relax this assumption later.}$ is a fully observed depiction of $\mathbf{s}_{t</p>
<p>Our goal then is to infer a low-dimensional latent state space model in which optimal control can be performed. That is, we seek to learn a function $m$, mapping from high-dimensional images $\mathbf{x}<em t="t">{t}$ to low-dimensional vectors $\mathbf{z}</em>} \in \mathbb{R}^{n_{x}}$ with $n_{z} \ll n_{x}$, such that the control problem can be solved using $\mathbf{z<em t="t">{t}$ instead of $\mathbf{x}</em>$ :</p>
<p>$$
\mathbf{z}<em t="t">{t}=m\left(\mathbf{x}</em>\right)
$$}\right)+\boldsymbol{\omega}, \quad \boldsymbol{\omega} \sim \mathcal{N}\left(0, \boldsymbol{\Sigma}_{\boldsymbol{\omega}</p>
<p>where $\boldsymbol{\omega}$ accounts for system noise; or equivalently $\mathbf{z}<em t="t">{t} \sim \mathcal{N}\left(m\left(\mathbf{x}</em>\right)$. Assuming for the moment that such a function can be learned (or approximated), we will first define SOC in a latent space and introduce our model thereafter.}\right), \boldsymbol{\Sigma}_{\boldsymbol{\omega}</p>
<h3>2.2 Stochastic locally optimal control in latent spaces</h3>
<p>Let $\mathbf{z}<em z="z">{t} \in \mathbb{R}^{n</em>}}$ be the inferred latent state from image $\mathbf{x<em t="t">{t}$ of state $\mathbf{s}</em>}$ and $f^{\text {lat }}\left(\mathbf{z<em t="t">{t}, \mathbf{u}</em>}\right)$ the transition dynamics in latent space, i.e., $\mathbf{z<em t="t">{t+1}=f^{\text {lat }}\left(\mathbf{z}</em>}, \mathbf{u<em t="t">{t}\right)$. Thus $f^{\text {lat }}$ models the changes that occur in $\mathbf{z}</em>}$ when control $\mathbf{u<em t="t">{t}$ is applied to the underlying system as a latent space analogue to $f\left(\mathbf{s}</em>}, \mathbf{u<em 1:="1:" T="T">{t}\right)$. Assuming $f^{\text {lat }}$ is known, optimal controls for a trajectory of length $T$ in the dynamical system can be derived by minimizing the function $J\left(\mathbf{z}</em>}, \mathbf{u<em 1:="1:" T="T">{1: T}\right)$ which gives the expected future costs when following $\left(\mathbf{z}</em>\right)$ :}, \mathbf{u}_{1: T</p>
<p>$$
J\left(\mathbf{z}<em 1:="1:" T="T">{1: T}, \mathbf{u}</em>}\right)=\mathbb{E<em T="T">{\mathbf{z}}\left[c</em>}\left(\mathbf{z<em T="T">{T}, \mathbf{u}</em>}\right)+\sum_{t_{0}}^{T-1} c\left(\mathbf{z<em t="t">{t}, \mathbf{u}</em>\right)\right]
$$</p>
<p>where $c\left(\mathbf{z}<em t="t">{t}, \mathbf{u}</em>}\right)$ are instantaneous costs, $c_{T}\left(\mathbf{z<em T="T">{T}, \mathbf{u}</em>}\right)$ denotes terminal costs and $\mathbf{z<em 1="1">{1: T}=\left{\mathbf{z}</em>}, \ldots, \mathbf{z<em 1:="1:" T="T">{T}\right}$ and $\mathbf{u}</em>}=\left{\mathbf{u<em T="T">{1}, \ldots, \mathbf{u}</em>}\right}$ are state and action sequences respectively. If $\mathbf{z<em t="t">{t}$ contains sufficient information about $\mathbf{s}</em>}$, i.e., $\mathbf{s<em t="t">{t}$ can be inferred from $\mathbf{z}</em>}$ alone, and $f^{\text {lat }}$ is differentiable, the cost-minimizing controls can be computed from $J\left(\mathbf{z<em 1:="1:" T="T">{1: T}, \mathbf{u}</em>}\right)$ via SOC algorithms [10]. These optimal control algorithms approximate the global non-linear dynamics with locally linear dynamics at each time step $t$. Locally optimal actions can then be found in closed form. Formally, given a reference trajectory $\bar{\mathbf{z}<em 1:="1:" T="T">{1: T}$ - the current estimate for the optimal trajectory - together with corresponding controls $\bar{\mathbf{u}}</em>$ the system is linearized as</p>
<p>$$
\mathbf{z}<em t="t">{t+1}=\mathbf{A}\left(\bar{\mathbf{z}}</em>}\right) \mathbf{z<em t="t">{t}+\mathbf{B}\left(\bar{\mathbf{z}}</em>}\right) \mathbf{u<em t="t">{t}+\mathbf{o}\left(\bar{\mathbf{z}}</em>\right)
$$}\right)+\boldsymbol{\omega}, \quad \boldsymbol{\omega} \sim \mathcal{N}\left(0, \boldsymbol{\Sigma}_{\boldsymbol{\omega}</p>
<p>where $\mathbf{A}\left(\bar{\mathbf{z}}<em t="t">{t}\right)=\frac{\delta f^{\text {lat }}\left(\bar{\mathbf{z}}</em>}, \bar{\mathbf{u}<em t="t">{t}\right)}{\delta \bar{\mathbf{z}}</em>}}, \mathbf{B}\left(\bar{\mathbf{z}<em t="t">{t}\right)=\frac{\delta f^{\text {lat }}\left(\bar{\mathbf{z}}</em>}, \bar{\mathbf{u}<em t="t">{t}\right)}{\delta \bar{\mathbf{u}}</em>\right)$ is an offset. To enable efficient computation of the local controls we assume the costs to be a quadratic function of the latent representation}}$ are local Jacobians, and $\mathbf{o}\left(\bar{\mathbf{z}}_{t</p>
<p>$$
c\left(\mathbf{z}<em t="t">{t}, \mathbf{u}</em>}\right)=\left(\mathbf{z<em _goal="{goal" _text="\text">{t}-\mathbf{z}</em>}}\right)^{T} \mathbf{R<em t="t">{z}\left(\mathbf{z}</em>}-\mathbf{z<em t="t">{\text {goal }}\right)+\mathbf{u}</em>}^{T} \mathbf{R<em t="t">{u} \mathbf{u}</em>
$$</p>
<p>where $\mathbf{R}<em z="z">{z} \in \mathbb{R}^{n</em>} \times n_{z}}$ and $\mathbf{R<em u="u">{u} \in \mathbb{R}^{n</em>} \times n_{u}}$ are cost weighting matrices and $\mathbf{z<em T="T">{\text {goal }}$ is the inferred representation of the goal state. We also assume $c</em>}\left(\mathbf{z<em T="T">{T}, \mathbf{u}</em>}\right)=c\left(\mathbf{z<em T="T">{T}, \mathbf{u}</em>}\right)$ throughout this paper. In combination with Equation (4) this gives us a local linear-quadratic-Gaussian formulation at each time step $t$ which can be solved by SOC algorithms such as iterative linear-quadratic regulation (iLQR) [11] or approximate inference control (AICO) [12]. The result of this trajectory optimization step is a locally optimal trajectory with corresponding control sequence $\left(\mathbf{z<em _mathbf_u="\mathbf{u">{1: T}^{<em>}, \mathbf{u}_{1: T}^{</em>}\right) \approx$ $\arg \min </em><em 1:="1:" T="T">{1: T}} J\left(\mathbf{z}</em>\right)$.}, \mathbf{u}_{1: T</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The information flow in the E2C model. From left to right, we encode and decode an image $\mathbf{x}<em _boldsymbol_phi="\boldsymbol{\phi">{t}$ with the networks $h</em>}}^{\text{enc}}$ and $h_{\boldsymbol{\theta}}^{\text{dec}}$, where we use the latent code $\mathbf{z<em _boldsymbol_psi="\boldsymbol{\psi">{t}$ for the transition step. The $h</em>}}^{\text{trans}}$ network computes the local matrices $\mathbf{A<em t="t">{t}, \mathbf{B}</em>}, \mathbf{o<em t_1="t+1">{t}$ with which we can predict $\tilde{\mathbf{z}}</em>}$ from $\mathbf{z<em t="t">{t}$ and $\mathbf{u}</em>}$. Similarity to the encoding $\mathbf{z<em _boldsymbol_theta="\boldsymbol{\theta">{t+1}$ is enforced by a KL divergence on their distributions and reconstruction is again performed by $h</em>$.}}^{\text{dec}</p>
<h3>2.3 A locally linear latent state space model for dynamical systems</h3>
<p>Starting from the SOC formulation, we now turn to the problem of learning an appropriate low-dimensional latent representation $\mathbf{z}<em t="t">{t} \sim P\left(Z</em>} \mid m\left(\mathbf{x<em _boldsymbol_omega="\boldsymbol{\omega">{t}\right), \boldsymbol{\Sigma}</em>}}\right)$ of $\mathbf{x<em t="t">{t}$. The representation $\mathbf{z}</em>}$ has to fulfill three properties: (i) it must capture sufficient information about $\mathbf{x<em t_1="t+1">{t}$ (enough to enable reconstruction); (ii) it must allow for accurate prediction of the next latent state $\mathbf{z}</em>}$ and thus, implicitly, of the next observation $\mathbf{x<em t="t">{t+1}$; (iii) the prediction $f^{\text {lat }}$ of the next latent state must be locally linearizable <em>for all valid control magnitudes</em> $\mathbf{u}</em>}$. Given some representation $\mathbf{z<em t="t">{t}$, properties (ii) and (iii) in particular require us to capture possibly highly non-linear changes of the latent representation due to transformations of the observed scene induced by control commands. Crucially, these are particularly hard to model and subsequently linearize. We circumvent this problem by taking a more direct approach: instead of learning a latent space $\mathbf{z}$ and transition model $f^{\text {lat }}$ which are then linearized and combined with SOC algorithms, we directly impose desired transformation properties on the representation $\mathbf{z}</em>$ during learning. We will select these properties such that prediction in the latent space as well as locally linear inference of the next observation according to Equation (4) are easy.</p>
<p>The transformation properties that we desire from a latent representation can be formalized directly from the iLQG formulation given in Section 2.2. Formally, following Equation (2), let the latent representation be Gaussian $P(Z \mid X)=\mathcal{N}\left(m\left(\mathbf{x}<em _boldsymbol_omega="\boldsymbol{\omega">{t}\right), \boldsymbol{\Sigma}</em>}}\right)$. To infer $\mathbf{z<em t="t">{t}$ from $\mathbf{x}</em>}$ we first require a method for sampling latent states. Ideally, we would generate samples directly from the unknown true posterior $P(Z \mid X)$, which we, however, have no access to. Following the variational Bayes approach (see Jordan et al. [13] for an overview) we resort to sampling $\mathbf{z<em _boldsymbol_phi="\boldsymbol{\phi">{t}$ from an approximate posterior distribution $Q</em>$.}}(Z \mid X)$ with parameters $\boldsymbol{\phi</p>
<p>Inference model for $Q_{\boldsymbol{\phi}}$. In our work this is always a diagonal Gaussian distribution $Q_{\boldsymbol{\phi}}(Z \mid X)=$ $\mathcal{N}\left(\boldsymbol{\mu}<em t="t">{t}, \operatorname{diag}\left(\boldsymbol{\sigma}</em>}^{2}\right)\right)$, whose mean $\boldsymbol{\mu<em z="z">{t} \in \mathbb{R}^{n</em>}}$ and covariance $\boldsymbol{\Sigma<em t="t">{t}=\operatorname{diag}\left(\boldsymbol{\sigma}</em>$ are computed by an encoding neural network with outputs}^{2}\right) \in \mathbb{R}^{n_{z} \times n_{z}</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}<em _boldsymbol_mu="\boldsymbol{\mu">{t} &amp; =\mathbf{W}</em>}} h_{\boldsymbol{\phi}}^{\text {enc }}\left(\mathbf{x<em _boldsymbol_mu="\boldsymbol{\mu">{t}\right)+\mathbf{b}</em> \
\log \boldsymbol{\sigma}}<em _boldsymbol_sigma="\boldsymbol{\sigma">{t} &amp; =\mathbf{W}</em>}} h_{\boldsymbol{\phi}}^{\text {enc }}\left(\mathbf{x<em _boldsymbol_sigma="\boldsymbol{\sigma">{t}\right)+\mathbf{b}</em>
\end{aligned}
$$}</p>
<p>where $h_{\boldsymbol{\phi}}^{\text {enc }} \in \mathbb{R}^{n_{c}}$ is the activation of the last hidden layer and where $\boldsymbol{\phi}$ is given by the set of all learnable parameters of the encoding network, including the weight matrices $\mathbf{W}<em _boldsymbol_sigma="\boldsymbol{\sigma">{\boldsymbol{\mu}}, \mathbf{W}</em>}}$ and biases $\mathbf{b<em _boldsymbol_sigma="\boldsymbol{\sigma">{\boldsymbol{\mu}}, \mathbf{b}</em>$. Parameterizing the mean and variance of a Gaussian distribution based on a neural network gives us a natural and very expressive model for our latent space. It additionally comes with the benefit that we can use the }<em>reparameterization trick</em> [6, 7] to backpropagate gradients of a loss function based on samples through the latent distribution.</p>
<p>Generative model for $P_{\boldsymbol{\theta}}$. Using the approximate posterior distribution $Q_{\boldsymbol{\phi}}$ we generate observed samples (images) $\tilde{\mathbf{x}}<em t_1="t+1">{t}$ and $\tilde{\mathbf{x}}</em>}$ from latent samples $\mathbf{z<em t_1="t+1">{t}$ and $\mathbf{z}</em>$ by enforcing a locally linear relationship in latent space according to Equation (4), yielding the following generative model</p>
<p>$$
\begin{aligned}
\mathbf{z}<em _boldsymbol_phi="\boldsymbol{\phi">{t} &amp; \sim Q</em>}}(Z \mid X) &amp; =\mathcal{N}\left(\boldsymbol{\mu<em t="t">{t}, \boldsymbol{\Sigma}</em>\right) \
\tilde{\mathbf{z}}<em _boldsymbol_psi="\boldsymbol{\psi">{t+1} &amp; \sim \tilde{Q}</em>}}(\tilde{Z} \mid Z, \mathbf{u}) &amp; =\mathcal{N}\left(\mathbf{A<em t="t">{t} \boldsymbol{\mu}</em>}+\mathbf{B<em t="t">{t} \mathbf{u}</em>}+\mathbf{o<em t="t">{t}, \mathbf{C}</em>\right) \
\tilde{\mathbf{x}}<em t_1="t+1">{t}, \tilde{\mathbf{x}}</em>\right)
\end{aligned}
$$} &amp; \sim P_{\boldsymbol{\theta}}(X \mid Z) &amp; =\text { Bernoulli }\left(\mathbf{p}_{t</p>
<p>where $\tilde{Q}<em t="t">{\boldsymbol{\psi}}$ is the <em>next latent state</em> posterior distribution, which exactly follows the linear form required for stochastic optimal control. With $\boldsymbol{\omega}</em>\right)$ as an estimate of the system noise,} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{H}_{t</p>
<p>$\mathbf{C}$ can be decomposed as $\mathbf{C}<em t="t">{t}=\mathbf{A}</em>} \boldsymbol{\Sigma<em t="t">{t} \mathbf{A}</em>}^{T}+\mathbf{H<em t="t">{t}$. Note that while the transition dynamics in our generative model operates on the inferred latent space, it takes untransformed controls into account. That is, we aim to learn a latent space such that the transition dynamics in $\mathbf{z}$ linearizes the non-linear observed dynamics in $\mathbf{x}$ and is locally linear in the applied controls $\mathbf{u}$. Reconstruction of an image from $\mathbf{z}</em>}$ is performed by passing the sample through multiple hidden layers of a decoding neural network which computes the mean $\mathbf{p<em _boldsymbol_theta="\boldsymbol{\theta">{t}$ of the generative Bernoulli distribution ${ }^{1} P</em>(X \mid Z)$ as}</p>
<p>$$
\mathbf{p}<em _mathbf_p="\mathbf{p">{t}=\mathbf{W}</em>}} h_{\boldsymbol{\theta}}^{\text {dec }}\left(\mathbf{z<em _mathbf_p="\mathbf{p">{t}\right)+\mathbf{b}</em>
$$}</p>
<p>where $h_{\boldsymbol{\theta}}^{\text {dec }}\left(\mathbf{z}<em d="d">{t}\right) \in \mathbb{R}^{n</em>}}$ is the response of the last hidden layer in the decoding network. The set of parameters for the decoding network, including weight matrix $\mathbf{W<em _mathbf_p="\mathbf{p">{\mathbf{p}}$ and bias $\mathbf{b}</em>$.}}$, then make up the learned generative parameters $\boldsymbol{\theta</p>
<p>Transition model for $\hat{Q}<em t="t">{\boldsymbol{\psi}}$. What remains is to specify how the linearization matrices $\mathbf{A}</em>} \in \mathbb{R}^{n_{z} \times n_{z}}$, $\mathbf{B<em z="z">{t} \in \mathbb{R}^{n</em>} \times n_{u}}$ and offset $\mathbf{o<em z="z">{t} \in \mathbb{R}^{n</em>}}$ are predicted. Following the same approach as for distribution means and covariance matrices, we predict all local transformation parameters from samples $\mathbf{z<em _boldsymbol_psi="\boldsymbol{\psi">{t}$ based on the hidden representation $h</em>}}^{\text {trans }}\left(\mathbf{z<em t="t">{t}\right) \in \mathbb{R}^{n</em>-$ to which we refer as the transformation network. Specifically, we parametrize the transformation matrices and offset as}}$ of a third neural network with parameters $\boldsymbol{\psi</p>
<p>$$
\begin{aligned}
\operatorname{vec}\left[\mathbf{A}<em A="A">{t}\right] &amp; =\mathbf{W}</em>} h_{\boldsymbol{\psi}}^{\text {trans }}\left(\mathbf{z<em A="A">{t}\right)+\mathbf{b}</em> \
\operatorname{vec}\left[\mathbf{B}<em B="B">{t}\right] &amp; =\mathbf{W}</em>} h_{\boldsymbol{\psi}}^{\text {trans }}\left(\mathbf{z<em B="B">{t}\right)+\mathbf{b}</em> \
\mathbf{o}<em o="o">{t} &amp; =\mathbf{W}</em>} h_{\boldsymbol{\psi}}^{\text {trans }}\left(\mathbf{z<em o="o">{t}\right)+\mathbf{b}</em>
\end{aligned}
$$</p>
<p>where vec denotes vectorization and therefore $\operatorname{vec}\left[\mathbf{A}<em z="z">{t}\right] \in \mathbb{R}^{\left\langle n</em>}^{2}\right\rangle}$ and $\operatorname{vec}\left[\mathbf{B<em z="z">{t}\right] \in \mathbb{R}^{\left\langle n</em>} \cdot n_{u}\right\rangle}$. To circumvent estimating the full matrix $\mathbf{A<em z="z">{t}$ of size $n</em>} \times n_{z}$, we can choose it to be a perturbation of the identity matrix $\mathbf{A<em t="t">{t}=\left(\mathbf{I}+\mathbf{v}</em>} \mathbf{r<em t="t">{t}^{T}\right)$ which reduces the parameters to be estimated for $\mathbf{A}</em>$.}$ to $2 n_{z</p>
<p>A sketch of the complete architecture is shown in Figure 1. It also visualizes an additional constraint that is essential for learning a representation for long-term predictions: we require samples $\hat{\mathbf{z}}<em _boldsymbol_psi="\boldsymbol{\psi">{t+1}$ from the state transition distribution $\hat{Q}</em>}}$ to be similar to the encoding of $\mathbf{x<em _boldsymbol_phi="\boldsymbol{\phi">{t+1}$ through $Q</em>}}$. While it might seem that just learning a perfect reconstruction of $\mathbf{x<em t_1="t+1">{t+1}$ from $\hat{\mathbf{z}}</em>}$ is enough, we require multistep predictions for planning in $Z$ which must correspond to valid trajectories in the observed space $X$. Without enforcing similarity between samples from $\hat{Q<em _boldsymbol_phi="\boldsymbol{\phi">{\boldsymbol{\psi}}$ and $Q</em>}}$, following a transition in latent space from $\mathbf{z<em t="t">{t}$ with action $\mathbf{u}</em>}$ may lead to a point $\hat{\mathbf{z}<em t_1="t+1">{t+1}$, from which reconstruction of $\mathbf{x}</em>}$ is possible, but that is not a valid encoding (i.e. the model will never encode any image as $\hat{\mathbf{z}<em t_1="t+1">{t+1}$ ). Executing another action in $\hat{\mathbf{z}}</em>$ then does not result in a valid latent state - since the transition model is conditional on samples coming from the inference network - and thus long-term predictions fail. In a nutshell, such a divergence between encodings and the transition model results in a generative model that does not accurately model the Markov chain formed by the observations.</p>
<h1>2.4 Learning via stochastic gradient variational Bayes</h1>
<p>For training the model we use a data set $\mathcal{D}=\left{\left(\mathbf{x}<em 1="1">{1}, \mathbf{u}</em>}, \mathbf{x<em T-1="T-1">{2}\right), \ldots,\left(\mathbf{x}</em>}, \mathbf{u<em T="T">{T-1}, \mathbf{x}</em>}\right)\right}$ containing observation tuples with corresponding controls obtained from interactions with the dynamical system. Using this data set, we learn the parameters of the inference, transition and generative model by minimizing a variational bound on the true data negative log-likelihood $-\log P\left(\mathbf{x<em t="t">{t}, \mathbf{u}</em>$ is given as}, \mathbf{x}_{t+1}\right)$ plus an additional constraint on the latent representation. The complete loss function ${ }^{2</p>
<p>$$
\mathcal{L}(\mathcal{D})=\sum_{\left(\mathbf{x}<em t="t">{t}, \mathbf{u}</em>}, \mathbf{x<em t="t">{t+1}\right) \in \mathcal{D}} \mathcal{L}^{\text {bound }}\left(\mathbf{x}</em>}, \mathbf{u<em t_1="t+1">{t}, \mathbf{x}</em>}\right)+\lambda \operatorname{KL}\left(\hat{Q<em t="t">{\boldsymbol{\psi}}\left(\hat{Z} \mid \boldsymbol{\mu}</em>}, \mathbf{u<em _boldsymbol_phi="\boldsymbol{\phi">{t}\right) | Q</em>\right)\right)
$$}}\left(Z \mid \mathbf{x}_{t+1</p>
<p>The first part of this loss is the per-example variational bound on the log-likelihood</p>
<p>$$
\mathcal{L}^{\text {bound }}\left(\mathbf{x}<em t="t">{t}, \mathbf{u}</em>}, \mathbf{x<em t_1="t+1">{t+1}\right)=\underset{\hat{\mathbf{z}}</em>} \sim Q_{\phi}}{\mathbb{E<em t="t">{\mathbf{z}</em>}}\left[-\log P_{\boldsymbol{\theta}}\left(\mathbf{x<em t="t">{t} \mid \mathbf{z}</em>}\right)-\log P_{\boldsymbol{\theta}}\left(\mathbf{x<em t_1="t+1">{t+1} \mid \hat{\mathbf{z}}</em>
$$}\right)\right]+\operatorname{KL}\left(Q_{\boldsymbol{\phi}} | P(Z)\right)</p>
<p>where $Q_{\boldsymbol{\phi}}, P_{\boldsymbol{\theta}}$ and $\hat{Q}<em t="t">{\boldsymbol{\psi}}$ are the parametric inference, generative and transition distributions from Section 2.3 and $P\left(Z</em>$; which we always chose to be}\right)$ is a prior on the approximate posterior $Q_{\boldsymbol{\phi}</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>an isotropic Gaussian distribution with mean zero and unit variance. The second KL divergence in Equation (11) is an additional contraction term with weight $\lambda$, that enforces agreement between the transition and inference models. This term is essential for establishing a Markov chain in latent space that corresponds to the real system dynamics (see Section 2.3 above for an in depth discussion). This KL divergence can also be seen as a prior on the latent transition model. Note that all KL terms can be computed analytically for our model (see supplementary for details).</p>
<p>During training we approximate the expectation in $\mathcal{L}(\mathcal{D})$ via sampling. Specifically, we take one sample $\mathbf{z}<em t="t">{t}$ for each input $\mathbf{x}</em>}$ and transform that sample using Equation (10) to give a valid sample $\hat{\mathbf{z}<em _boldsymbol_psi="\boldsymbol{\psi">{t+1}$ from $\hat{Q}</em>)$ using SGD.}}$. We then jointly learn all parameters of our model by minimizing $\mathcal{L}(\mathcal{D</p>
<h1>3 Experimental Results</h1>
<p>We evaluate our model on four visual tasks: an agent in a plane with obstacles, a visual version of the classic inverted pendulum swing-up task, balancing a cart-pole system, and control of a three-link arm with larger images. These are described in detail below.</p>
<h3>3.1 Experimental Setup</h3>
<p>Model training. We consider two different network types for our model: Standard fully connected neural networks with up to three layers, which work well for moderately sized images, are used for the planar and swing-up experiments; A deep convolutional network for the encoder in combination with an up-convolutional network as the decoder which, in accordance with recent findings from the literature [8, 9], we found to be an adequate model for larger images. Training was performed using Adam [14] throughout all experiments. The training data set $\mathcal{D}$ for all tasks was generated by randomly sampling $N$ state observations and actions with corresponding successor states. For the plane we used $N=3,000$ samples, for the inverted pendulum and cart-pole system we used $N=$ 15,000 and for the arm $N=30,000$. A complete list of architecture parameters and hyperparameter choices as well as an in-depth explanation of the up-convolutional network are specified in the supplementary material. We will make our code and a video containing controlled trajectories for all systems available under http://ml.informatik.uni-freiburg.de/research/e2c .</p>
<p>Model variants. In addition to the Embed to Control (E2C) dynamics model derived above, we also consider two variants: By removing the latent dynamics network $h_{\boldsymbol{\psi}}^{\text {trans }}$, i.e. setting its output to one in Equation (10) - we obtain a variant in which $\mathbf{A}<em t="t">{t}, \mathbf{B}</em>}$ and $\mathbf{o<em t="t">{t}$ are estimated as globally linear matrices (Global E2C). If we instead replace the transition model with a network estimating the dynamics as a non-linear function $\hat{f}^{\text {lat }}$ and only linearize during planning, estimating $\mathbf{A}</em>}, \mathbf{B<em t="t">{t}, \mathbf{o}</em>$ as described in Section 2.2, we obtain a variant with nonlinear latent dynamics.}$ as Jacobians to $\hat{f}^{\text {lat }</p>
<p>Baseline models. For a thorough comparison and to exhibit the complicated nature of the tasks, we also test a set of baseline models on the plane and the inverted pendulum task (using the same architecture as the E2C model): a standard variational autoencoder (VAE) and a deep autoencoder (AE) are trained on the autoencoding subtask for visual problems. That is, given a data set $\mathcal{D}$ used for training our model, we remove all actions from the tuples in $\mathcal{D}$ and disregard temporal context between images. After autoencoder training we learn a dynamics model in latent space, approximating $\hat{f}^{\text {lat }}$ from Section 2.2. We also consider a VAE variant with a slowness term on the latent representation - a full description of this variant is given in the supplementary material.</p>
<p>Optimal control algorithms. To perform optimal control in the latent space of different models, we employ two trajectory optimization algorithms: iterative linear quadratic regulation (iLQR) [11] (for the plane and inverted pendulum) and approximate inference control (AICO) [12] (all other experiments). For all VAEs both methods operate on the mean of distributions $Q_{\boldsymbol{\psi}}$ and $\hat{Q}<em t="t">{\boldsymbol{\psi}}$. AICO additionally makes use of the local Gaussian covariances $\boldsymbol{\Sigma}</em>}$ and $\mathbf{C<em t="t">{t}$. Except for the experiments on the planar system, control was performed in a model predictive control fashion using the receding horizon scheme introduced in [3]. To obtain closed loop control given an image $\mathbf{x}</em>}$, it is first passed through the encoder to obtain the latent state $\mathbf{z<em t:="t:" t_T="t+T">{t}$. A locally optimal trajectory is subsequently found by optimizing $\left(\mathbf{z}</em>^{<em>}, \mathbf{u}_{t: t+T}^{</em>}\right) \approx \arg \min <em t:="t:" t_T="t+T">{\mathbf{u}</em>}} J\left(\mathbf{z<em t:="t:" t_T="t+T">{t: t+T}, \mathbf{u}</em>}\right)$ with fixed, small horizon $T$ (with $T=10$ unless noted otherwise). Controls $\mathbf{u<em t_1="t+1">{t}^{*}$ are applied to the system and a transition to $\mathbf{z}</em>$ ). Then a new control sequence - with horizon}$ is observed (by encoding the next image $\mathbf{x}_{t+1</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The true state space of the planar system (left) with examples (obstacles encoded as circles) and the inferred spaces (right) of different models. The spaces are spanned by generating images for every valid position of the agent and embedding them with the respective encoders.
$T$ - starting in $\mathbf{z}<em t="t">{t+1}$ is found using the last estimated trajectory as a bootstrap. Note that planning is performed entirely in the latent state without access to any observations except for the depiction of the current state. To compute the cost function $c\left(\mathbf{z}</em>}, \mathbf{u<em _goal="{goal" _text="\text">{t}\right)$ required for trajectory optimization in $\mathbf{z}$ we assume knowledge of the observation $\mathbf{x}</em>$. This observation is then transformed into latent space and costs are computed according to Equation (5).}}$ of the goal state $\mathbf{s}_{\text {goal }</p>
<h1>3.2 Control in a planar system</h1>
<p>The agent in the planar system can move in a bounded two-dimensional plane by choosing a continuous offset in x - and y -direction. The high-dimensional representation of a state is a $40 \times 40$ black-and-white image. Obstructed by six circular obstacles, the task is to move to the bottom right of the image, starting from a random x position at the top of the image. The encodings of obstacles are obtained prior to planning and an additional quadratic cost term is penalizing proximity to them.</p>
<p>A depiction of the observations on which control is performed - together with their corresponding state values and embeddings into latent space - is shown in Figure 2. The figure also clearly shows a fundamental advantage the E2C model has over its competitors: While the separately trained autoencoders make for aesthetically pleasing pictures, the models failed to discover the underlying structure of the state space, complicating dynamics estimation and largely invalidating costs based on distances in said space. Including the latent dynamics constraints in these end-to-end models on the other hand, yields latent spaces approaching the optimal planar embedding.</p>
<p>We test the long-term accuracy by accumulating latent and real trajectory costs to quantify whether the imagined trajectory reflects reality. The results for all models when starting from random positions at the top and executing 40 pre-computed actions are summarized in Table 1 - using a seperate test set for evaluating reconstructions. While all methods achieve a low reconstruction loss, the difference in accumulated real costs per trajectory show the superiority of the E2C model. Using the globally or locally linear E2C model, trajectories planned in latent space are as good as trajectories planned on the real state. All models besides E2C fail to give long-term predictions that result in good performance.</p>
<h3>3.3 Learning swing-up for an inverted pendulum</h3>
<p>We next turn to the task of controlling the classical inverted pendulum system [15] from images. We create depictions of the state by rendering a fixed length line starting from the center of the image at an angle corresponding to the pendulum position. The goal in this task is to swing-up and balance an underactuated pendulum from a resting position (pendulum hanging down). Exemplary observations and reconstructions for this system are given in Figure 3(d). In the visual inverted pendulum task our algorithm faces two additional difficulties: the observed space is non-Markov, as the angular velocity cannot be inferred from a single image, and second, discretization errors due to rendering pendulum angles as small 48x48 pixel images make exact control difficult. To restore the Markov property, we stack two images (as input channels), thus observing a one-step history.
Figure 3 shows the topology of the latent space for our model, as well as one sample trajectory in true state and latent space. The fact that the model can learn a meaningful embedding, separating</p>
<p>Table 1: Comparison between different approaches to model learning from raw pixels for the planar and pendulum system. We compare all models with respect to their prediction quality on a test set of sampled transitions and with respect to their performance when combined with SOC (trajectory cost for control from different start states). Note that trajectory costs in latent space are not necessarily comparable. The "real" trajectory cost was computed on the dynamics of the simulator while executing planned actions. For the true models for $\mathbf{s}_{t}$, real trajectory costs were $20.24 \pm 4.15$ for the planar system, and $9.8 \pm 2.4$ for the pendulum. Success was defined as reaching the goal state and staying $\epsilon$-close to it for the rest of the trajectory (if non terminating). All statistics quantify over 5/30 (plane/pendulum) different starting positions. A $\dagger$ marks separately trained dynamics networks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Algorithm</th>
<th style="text-align: center;">State Loss $\log \mathbf{p}\left(\mathbf{x}<em _mathbf_t="\mathbf{t">{\mathbf{t}} \mid \hat{\mathbf{x}}</em>\right)$}</th>
<th style="text-align: center;">Next State Loss $\log \mathbf{p}\left(\mathbf{x}<em _mathbf_t="\mathbf{t">{\mathbf{t}+\mathbf{1}} \mid \hat{\mathbf{x}}</em>\right)$}}, \mathbf{u}_{\mathbf{t}</th>
<th style="text-align: center;">Trajectory Cost</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Success percent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Planar System</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AE $^{\dagger}$</td>
<td style="text-align: center;">$11.5 \pm 97.8$</td>
<td style="text-align: center;">$3538.9 \pm 1395.2$</td>
<td style="text-align: center;">$1325.6 \pm 81.2$</td>
<td style="text-align: center;">$273.3 \pm 16.4$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">VAE $^{\dagger}$</td>
<td style="text-align: center;">$3.6 \pm 18.9$</td>
<td style="text-align: center;">$652.1 \pm 930.6$</td>
<td style="text-align: center;">$43.1 \pm 20.8$</td>
<td style="text-align: center;">$91.3 \pm 16.4$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">VAE + slowness ${ }^{\dagger}$</td>
<td style="text-align: center;">$10.5 \pm 22.8$</td>
<td style="text-align: center;">$104.3 \pm 235.8$</td>
<td style="text-align: center;">$47.1 \pm 20.5$</td>
<td style="text-align: center;">$89.1 \pm 16.4$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Non-linear E2C</td>
<td style="text-align: center;">$8.3 \pm 5.5$</td>
<td style="text-align: center;">$11.3 \pm 10.1$</td>
<td style="text-align: center;">$19.8 \pm 9.8$</td>
<td style="text-align: center;">$42.3 \pm 16.4$</td>
<td style="text-align: center;">$96.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Global E2C</td>
<td style="text-align: center;">$\mathbf{6 . 9} \pm \mathbf{3 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 . 3} \pm \mathbf{4 . 6}$</td>
<td style="text-align: center;">$12.5 \pm 3.9$</td>
<td style="text-align: center;">$27.3 \pm 9.7$</td>
<td style="text-align: center;">$\mathbf{1 0 0} \%$</td>
</tr>
<tr>
<td style="text-align: center;">E2C</td>
<td style="text-align: center;">$7.7 \pm 2.0$</td>
<td style="text-align: center;">$9.7 \pm 3.2$</td>
<td style="text-align: center;">$10.3 \pm 2.8$</td>
<td style="text-align: center;">$\mathbf{2 5 . 1} \pm \mathbf{5 . 3}$</td>
<td style="text-align: center;">$\mathbf{1 0 0} \%$</td>
</tr>
<tr>
<td style="text-align: center;">Inverted Pendulum Swing-Up</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AE $^{\dagger}$</td>
<td style="text-align: center;">$8.9 \pm 100.3$</td>
<td style="text-align: center;">$13433.8 \pm 6238.8$</td>
<td style="text-align: center;">$1285.9 \pm 355.8$</td>
<td style="text-align: center;">$194.7 \pm 44.8$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">VAE $^{\dagger}$</td>
<td style="text-align: center;">$7.5 \pm 47.7$</td>
<td style="text-align: center;">$8791.2 \pm 17356.9$</td>
<td style="text-align: center;">$497.8 \pm 129.4$</td>
<td style="text-align: center;">$237.2 \pm 41.2$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">VAE + slowness ${ }^{\dagger}$</td>
<td style="text-align: center;">$26.5 \pm 18.0$</td>
<td style="text-align: center;">$779.7 \pm 633.3$</td>
<td style="text-align: center;">$419.5 \pm 85.8$</td>
<td style="text-align: center;">$188.2 \pm 43.6$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">E2C no latent KL</td>
<td style="text-align: center;">$64.4 \pm 32.8$</td>
<td style="text-align: center;">$87.7 \pm 64.2$</td>
<td style="text-align: center;">$489.1 \pm 87.5$</td>
<td style="text-align: center;">$213.2 \pm 84.3$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Non-linear E2C</td>
<td style="text-align: center;">$\mathbf{5 9 . 6} \pm \mathbf{2 5 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 6} \pm \mathbf{3 4 . 5}$</td>
<td style="text-align: center;">$313.3 \pm 65.7$</td>
<td style="text-align: center;">$37.4 \pm 12.4$</td>
<td style="text-align: center;">$63.33 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Global E2C</td>
<td style="text-align: center;">$115.5 \pm 56.9$</td>
<td style="text-align: center;">$125.3 \pm 62.6$</td>
<td style="text-align: center;">$628.1 \pm 45.9$</td>
<td style="text-align: center;">$125.1 \pm 10.7$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">E2C</td>
<td style="text-align: center;">$84.0 \pm 50.8$</td>
<td style="text-align: center;">$89.3 \pm 42.9$</td>
<td style="text-align: center;">$275.0 \pm 16.6$</td>
<td style="text-align: center;">$\mathbf{1 5 . 4} \pm \mathbf{3 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 0} \%$</td>
</tr>
</tbody>
</table>
<p>velocities and positions, from this data is remarkable (no other model recovered this shape). Table 1 again compares the different models quantitatively. While the E2C model is not the best in terms of reconstruction performance, it is the only model resulting in stable swing-up and balance behavior. We explain the failure of the other models with the fact that the non-linear latent dynamics model cannot be guaranteed to be linearizable for all control magnitudes, resulting in undesired behavior around unstable fixpoints of the real system dynamics, and that for this task a globally linear dynamics model is inadequate.</p>
<h1>3.4 Balancing a cart-pole and controlling a simulated robot arm</h1>
<p>Finally, we consider control of two more complex dynamical systems from images using a six layer convolutional inference and six layer up-convolutional generative network, resulting in a 12-layer deep path from input to reconstruction. Specifically, we control a visual version of the classical cartpole system [16] from a history of two $80 \times 80$ pixel images as well as a three-link planar robot arm based on a history of two $128 \times 128$ pixel images. The latent space was set to be 8 -dimensional in both experiments. The real state dimensionality for the cart-pole is four and is controlled using one
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (a) The true state space of the inverted pendulum task overlaid with a successful trajectory taken by the E2C agent. (b) The learned latent space. (c) The trajectory from (a) traced out in the latent space. (d) Images $\mathbf{x}$ and reconstructions $\hat{\mathbf{x}}$ showing current positions (right) and history (left).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Left: Trajectory from the cart-pole domain. Only the first image (green) is "real", all other images are "dreamed up" by our model. Notice discretization artifacts present in the real image. Right: Exemplary observed (with history image omitted) and predicted images (including the history image) for a trajectory in the visual robot arm domain with the goal marked in red.
action, while for the arm the real state can be described in 6 dimensions (joint angles and velocities) and controlled using a three-dimensional action vector corresponding to motor torques.
As in previous experiments the E2C model seems to have no problem finding a locally linear embedding of images into latent space in which control can be performed. Figure 4 depicts exemplary images - for both problems - from a trajectory executed by our system. The costs for these trajectories ( 11.13 for the cart-pole, 85.12 for the arm) are only slightly worse than trajectories obtained by AICO operating on the real system dynamics starting from the same start-state ( 7.28 and 60.74 respectively). The supplementary material contains additional experiments using these domains.</p>
<h1>4 Comparison to recent work</h1>
<p>In the context of representation learning for control (see Böhmer et al. [17] for a review), deep autoencoders (ignoring state transitions) similar to our baseline models have been applied previously, e.g. by Lange and Riedmiller [18]. A more direct route to control based on image streams is taken by recent work on (model free) deep end-to-end Q-learning for Atari games by Mnih et al. [19], as well as kernel based [20] and deep policy learning for robot control [21].
Close to our approach is a recent paper by Wahlström et al. [22], where autoencoders are used to extract a latent representation for control from images, on which a non-linear model of the forward dynamics is learned. Their model is trained jointly and is thus similar to the non-linear E2C variant in our comparison. In contrast to our model, their formulation requires PCA pre-processing and does neither ensure that long-term predictions in latent space do not diverge, nor that they are linearizable.
As stated above, our system belongs to the family of VAEs and is generally similar to recent work such as Kingma and Welling [6], Rezende et al. [7], Gregor et al. [23], Bayer and Osendorfer [24]. Two additional parallels between our work and recent advances for training deep neural networks can be observed. First, the idea of enforcing desired transformations in latent space during learning - such that the data becomes easy to model - has appeared several times already in the literature. This includes the development of transforming auto-encoders [25] and recent probabilistic models for images [26, 27]. Second, learning relations between pairs of images - although without control has received considerable attention from the community during the last years [28, 29]. In a broader context our model is related to work on state estimation in Markov decision processes (see Langford et al. [30] for a discussion) through, e.g., hidden Markov models and Kalman filters [31, 32].</p>
<h2>5 Conclusion</h2>
<p>We presented Embed to Control (E2C), a system for stochastic optimal control on high-dimensional image streams. Key to the approach is the extraction of a latent dynamics model which is constrained to be locally linear in its state transitions. An evaluation on four challenging benchmarks revealed that E2C can find embeddings on which control can be performed with ease, reaching performance close to that achievable by optimal control on the real system model.</p>
<h2>Acknowledgments</h2>
<p>We thank A. Radford, L. Metz, and T. DeWolf for sharing code, as well as A. Dosovitskiy for useful discussions. This work was partly funded by a DFG grant within the priority program "Autonomous learning" (SPP1597) and the BrainLinks-BrainTools Cluster of Excellence (grant number EXC 1086). M. Watter is funded through the State Graduate Funding Program of Baden-Württemberg.</p>
<h1>References</h1>
<p>[1] D. Jacobson and D. Mayne. Differential dynamic programming. American Elsevier, 1970.
[2] E. Todorov and W. Li. A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems. In ACC. IEEE, 2005.
[3] Y. Tassa, T. Erez, and W. D. Smart. Receding horizon differential dynamic programming. In Proc. of NIPS, 2008.
[4] Y. Pan and E. Theodorou. Probabilistic differential dynamic programming. In Proc. of NIPS, 2014.
[5] S. Levine and V. Koltun. Variational policy search via trajectory optimization. In Proc. of NIPS, 2013.
[6] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In Proc. of ICLR, 2014.
[7] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proc. of ICML, 2014.
[8] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus. Deconvolutional networks. In CVPR, 2010.
[9] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In Proc. of CVPR, 2015.
[10] R. F. Stengel. Optimal Control and Estimation. Dover Publications, 1994.
[11] W. Li and E. Todorov. Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems. In Proc. of ICINCO, 2004.
[12] M. Toussaint. Robot Trajectory Optimization using Approximate Inference. In Proc. of ICML, 2009.
[13] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. In Machine Learning, 1999.
[14] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. of ICLR, 2015.
[15] H. Wang, K. Tanaka, and M. Griffin. An approach to fuzzy control of nonlinear systems; stability and design issues. IEEE Trans. on Fuzzy Systems, 4(1), 1996.
[16] R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
[17] W. Böhmer, J. T. Springenberg, J. Boedecker, M. Riedmiller, and K. Obermayer. Autonomous learning of state representations for control. KI - Künstliche Intelligenz, 2015.
[18] S. Lange and M. Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In Proc. of IJCNN, 2010.
[19] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540), 022015.
[20] H. van Hoof, J. Peters, and G. Neumann. Learning of non-parametric control policies with highdimensional state features. In Proc. of AISTATS, 2015.
[21] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. CoRR, abs/1504.00702, 2015. URL http://arxiv.org/abs/1504.00702.
[22] N. Wahlström, T. B. Schön, and M. P. Deisenroth. From pixels to torques: Policy learning with deep dynamical models. CoRR, abs/1502.02251, 2015. URL http://arxiv.org/abs/1502.02251.
[23] K. Gregor, I. Danihelka, A. Graves, D. Rezende, and D. Wierstra. DRAW: A recurrent neural network for image generation. In Proc. of ICML, 2015.
[24] J. Bayer and C. Osendorfer. Learning stochastic recurrent networks. In NIPS 2014 Workshop on Advances in Variational Inference, 2014.
[25] G. Hinton, A. Krizhevsky, and S. Wang. Transforming auto-encoders. In Proc. of ICANN, 2011.
[26] L. Dinh, D. Krueger, and Y. Bengio. Nice: Non-linear independent components estimation. CoRR, abs/1410.8516, 2015. URL http://arxiv.org/abs/1410.8516.
[27] T. Cohen and M. Welling. Transformation properties of learned visual representations. In ICLR, 2015.
[28] G. W. Taylor, L. Sigal, D. J. Fleet, and G. E. Hinton. Dynamical binary latent variable models for 3d human pose tracking. In Proc. of CVPR, 2010.
[29] R. Memisevic. Learning to relate images. IEEE Trans. on PAMI, 35(8):1829-1846, 2013.
[30] J. Langford, R. Salakhutdinov, and T. Zhang. Learning nonlinear dynamic models. In ICML, 2009.
[31] M. West and J. Harrison. Bayesian Forecasting and Dynamic Models (Springer Series in Statistics). Springer-Verlag, February 1997. ISBN 0387947256.</p>
<p>[32] T. Matsubara, V. Gómez, and H. J. Kappen. Latent Kullback Leibler control for continuous-state systems using probabilistic graphical models. UAI, 2014.
[33] T. D. Kulkarni, W. Whitney, P. Kohli, and J. B. Tenenbaum. Deep convolutional inverse graphics network. CoRR, abs/1503.03167, 2015. URL http://arxiv.org/abs/1503.03167.
[34] C. Osendorfer, H. Soyer, and P. van der Smagt. Image super-resolution with fast approximate convolutional sparse coding. In Proc. of ICONIP, Lecture Notes in Computer Science. Springer International Publishing, 2014.
[35] R. Jonschkowski and O. Brock. State representation learning in robotics: Using prior knowledge about physical interaction. In Proc. of RSS, 2014.
[36] R. Legenstein, N. Wilbert, and L. Wiskott. Reinforcement learning on slow features of high-dimensional input streams. PLoS Computational Biology, 2010.
[37] W. Zou, A. Ng, and K. Yu. Unsupervised learning of visual invariance with temporal coherence. In NIPS*2011 Workshop on Deep Learning and Unsupervised Feature Learning, 2011.
[38] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In Proc. of ICLR, 2014.
[39] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In AISTATS. Journal of Machine Learning Research - Workshop and Conference Proceedings, 2011.</p>
<h1>A Supplementary to the E2C description</h1>
<h2>A. 1 State transition matrix factorization and KL Divergence</h2>
<p>As alluded to in the main paper, estimation of the full local state transition matrix $\mathbf{A}<em z="z">{t} \in \mathbb{R}^{n</em>} \times n_{z}}$ from Equation (8) requires the transition network to predict $n_{z} \times n_{z}$ parameters. Using an arbitrary state transition matrix also - inconveniently - requires inversion of said matrix for computing the KL divergence penalty from Equation (11) (through which it is hard to backpropagate). We started our experiments using a full matrix (and only approximating all KL divergence terms), but quickly found that a rank one pertubation of the identity matrix could be used instead without loss of performance in any of our benchmarks. To the contrary, the resulting networks have fewer parameters and are thus easier to train. We here give the derivation of this process and how the KL divergence from Equation (11) can be computed. For the reformulation we represent $\mathbf{A<em t="t">{t}$ as $\mathbf{A}</em>}=\mathbf{I}+\mathbf{v<em t="t">{t} \mathbf{r}</em>}^{T}$, therefore only $\mathbf{v<em t="t">{t}$ and $\mathbf{r}</em>}$ need to be estimated by the transition network, reducing the number of outputs for $\mathbf{A<em z="z">{t}$ from $n</em>$.}^{2}$ to $2 n_{z</p>
<p>The KL divergence between two multivariate Gaussians is given by</p>
<p>$$
\mathrm{KL}\left(\mathcal{N}<em 1="1">{0} | \mathcal{N}</em>}\right)=\frac{1}{2}\left(\operatorname{Tr}\left(\boldsymbol{\Sigma<em 0="0">{1}^{-1} \boldsymbol{\Sigma}</em>}\right)+\left(\boldsymbol{\mu<em 0="0">{1}-\boldsymbol{\mu}</em>}\right)^{T} \boldsymbol{\Sigma<em 1="1">{1}^{-1}\left(\boldsymbol{\mu}</em>}-\boldsymbol{\mu<em 1="1">{0}\right)-k+\log \left(\frac{\operatorname{det} \boldsymbol{\Sigma}</em>\right)\right)
$$}}{\operatorname{det} \boldsymbol{\Sigma}_{0}</p>
<p>For a simplified notation, such that $\operatorname{KL}\left(\mathcal{N}<em 1="1">{0} | \mathcal{N}</em> | Q)$, let us assume}\right)=\operatorname{KL}(\hat{Q</p>
<p>$$
\begin{aligned}
&amp; \mathcal{N}<em 0="0">{0}=\mathcal{N}\left(\boldsymbol{\mu}</em>}, \mathbf{A} \boldsymbol{\Sigma<em t="t">{0} \mathbf{A}^{T}\right)=\mathcal{N}\left(\boldsymbol{\mu}</em>}, \mathbf{A<em t="t">{t} \boldsymbol{\Sigma}</em>} \mathbf{A<em 1="1">{t}^{T}\right)=\hat{Q} \
&amp; \mathcal{N}</em>}=\mathcal{N}\left(\boldsymbol{\mu<em 1="1">{1}, \boldsymbol{\Sigma}</em>}\right)=\mathcal{N}\left(\boldsymbol{\mu<em t_1="t+1">{t+1}, \boldsymbol{\Sigma}</em>\right)=Q
\end{aligned}
$$</p>
<p>The main point behind the derivation presented in the following, is to make partial derivatives of the above KL divergence efficiently computable. To this end, we cannot take the trace or the determinant via numerical algorithms, because we have to be able to take the gradients in symbolic form. Aside from that, we like to process a batch of samples, so the computation should have a convenient form and not require excessive amounts of tensor products in between. We start our simplification with</p>
<p>the trace term which results in</p>
<p>$$
\begin{aligned}
\operatorname{Tr}\left(\boldsymbol{\Sigma}<em 0="0">{1}^{-1} \boldsymbol{\Sigma}</em>}\right) &amp; =\operatorname{Tr}\left(\boldsymbol{\Sigma<em 0="0">{1}^{-1} \mathbf{A} \boldsymbol{\Sigma}</em>\right) \
&amp; =\operatorname{Tr}\left(\boldsymbol{\Sigma}} \mathbf{A}^{T<em 0="0">{1}^{-1}\left(\mathbf{I}+\mathbf{v r}^{T}\right) \boldsymbol{\Sigma}</em>\right) \
&amp; =\operatorname{Tr}\left(\left(\boldsymbol{\Sigma}}\left(\mathbf{I}+\mathbf{v r}^{T}\right)^{T<em 1="1">{1}^{-1}+\boldsymbol{\Sigma}</em>}^{-1} \mathbf{v r}^{T}\right)\left(\boldsymbol{\Sigma<em 0="0">{0}+\boldsymbol{\Sigma}</em>\right)\right) \
&amp; =\operatorname{Tr}\left(\boldsymbol{\Sigma}}\left(\mathbf{v r}^{T}\right)^{T<em 0="0">{1}^{-1} \boldsymbol{\Sigma}</em>}+\boldsymbol{\Sigma<em 0="0">{1}^{-1} \boldsymbol{\Sigma}</em>}\left(\mathbf{v r}^{T}\right)^{T}+\boldsymbol{\Sigma<em 0="0">{1}^{-1} \mathbf{v r}^{T} \boldsymbol{\Sigma}</em>}+\boldsymbol{\Sigma<em 0="0">{1}^{-1} \mathbf{v r}^{T} \boldsymbol{\Sigma}</em>\right) \
&amp; =\operatorname{Tr}\left(\boldsymbol{\Sigma}}\left(\mathbf{v r}^{T}\right)^{T<em 0="0">{1}^{-1} \boldsymbol{\Sigma}</em>}\right)+\operatorname{Tr}\left(\boldsymbol{\Sigma<em 0="0">{1}^{-1} \boldsymbol{\Sigma}</em>}\left(\mathbf{v r}^{T}\right)^{T}\right)+\operatorname{Tr}\left(\boldsymbol{\Sigma<em 0="0">{1}^{-1} \mathbf{v r}^{T} \boldsymbol{\Sigma}</em>}\right)+\underset{\operatorname{Tr}(A B)}{ } \operatorname{Tr}\left(\boldsymbol{\Sigma<em 0="0">{1}^{-1} \mathbf{v r}^{T} \boldsymbol{\Sigma}</em>\right) \
&amp; =\sum_{i} \frac{\sigma_{0, i}^{2}}{\sigma_{1, i}^{2}}+\sum_{i} \frac{\sigma_{0, i}^{2} r_{i} v_{i}}{\sigma_{1, i}^{2}}+\sum_{i} \frac{v_{i} r_{i} \sigma_{0, i}^{2}}{\sigma_{1, i}^{2}}+\operatorname{Tr}\left(\mathbf{v}^{T} \boldsymbol{\Sigma}} \mathbf{r} \mathbf{v}^{T<em 0="0">{1}^{-1} \mathbf{v r}^{T} \boldsymbol{\Sigma}</em>\right) \
&amp; =\sum_{i} \frac{\sigma_{0, i}^{2}+2 \sigma_{0, i}^{2} v_{i} r_{i}}{\sigma_{1, i}^{2}}+\sum_{i} r_{i}^{2} \sigma_{i}^{2} \cdot \sum_{i} \frac{v_{i}^{2}}{\sigma_{i}^{2}}
\end{aligned}
$$} \mathbf{r</p>
<p>The last equation is easy to implement and only requires summing over the non-batch dimension. The difference of means can be derived very quickly with the same summing scheme:</p>
<p>$$
\left(\boldsymbol{\mu}<em 0="0">{1}-\boldsymbol{\mu}</em>}\right)^{T} \boldsymbol{\Sigma<em 1="1">{1}^{-1}\left(\boldsymbol{\mu}</em>}-\boldsymbol{\mu<em i="i">{0}\right)=\sum</em>} \frac{\left(\boldsymbol{\mu<em 0="0">{1}-\boldsymbol{\mu}</em>\right)<em 1_="1," i="i">{i}^{2}}{\sigma</em>
$$}^{2}</p>
<p>It remains the ratio of determinants, which we will simplify with the matrix determinant lemma giving</p>
<p>$$
\begin{aligned}
&amp; \log \left(\frac{\operatorname{det} \boldsymbol{\Sigma}<em 0="0">{1}}{\operatorname{det} \mathbf{A} \boldsymbol{\Sigma}</em>} \mathbf{A}^{T}}\right)=\log \operatorname{det} \boldsymbol{\Sigma<em 0="0">{1}-\log \operatorname{det}\left(\mathbf{A} \boldsymbol{\Sigma}</em>\right) \
&amp; =\log \prod_{i} \sigma_{1, i}^{2}-\log \left(\operatorname{det} \mathbf{A} \cdot \operatorname{det} \boldsymbol{\Sigma}} \mathbf{A}^{T<em i="i">{0} \cdot \operatorname{det} \mathbf{A}^{T}\right) \quad \operatorname{det} \mathbf{A}^{T}=\operatorname{det} \mathbf{A} \
&amp; =2 \sum</em> \
&amp; =2 \sum_{i} \log \sigma_{1, i}-\log \left(1+\mathbf{v}^{T} \mathbf{r}\right)^{2}-2 \sum_{i} \log \sigma_{0, i} \
&amp; =2\left(\sum_{i}\left(\log \sigma_{1, i}^{2}-\log \sigma_{0, i}^{2}\right)-\log \left(1+\sum_{i} v_{i} r_{i}\right)\right) .
\end{aligned}
$$} \log \sigma_{1, i}-\log \left((\operatorname{det} \mathbf{A})^{2} \prod_{i} \sigma_{0, i}^{2}\right) \quad \text { Matrix determinant lemma </p>
<p>Putting the above to formulas together finally yields</p>
<p>$$
\begin{aligned}
\mathrm{KL}\left(\mathcal{N}<em 1="1">{0} | \mathcal{N}</em>\right. \
&amp; \left.+\sum_{i} \frac{\left(\boldsymbol{\mu}}\right)= &amp; \frac{1}{2}\left(\sum_{i} \frac{\sigma_{0, i}^{2}+2 \sigma_{0, i}^{2} v_{i} r_{i}}{\sigma_{1, i}^{2}}+\sum_{i} r_{i}^{2} \sigma_{i}^{2} \cdot \sum_{i} \frac{v_{i}^{2}}{\sigma_{i}^{2}<em 0="0">{1}-\boldsymbol{\mu}</em>\right)<em 1_="1," i="i">{i}^{2}}{\sigma</em>-k\right. \
&amp; \left.+2\left(\sum_{i}\left(\log \sigma_{1, i}^{2}-\log \sigma_{0, i}^{2}\right)-\log \left(1+\sum_{i} v_{i} r_{i}\right)\right)\right)
\end{aligned}
$$}^{2}</p>
<h1>B Supplementary to the experimental setup</h1>
<h2>B. 1 Up-convolution</h2>
<p>We used convolutional inference networks for the cart-pole and three-link arm task. While these networks help us overcome the problem of large input dimensionalities (i.e. $2 \times 128 \times 128$ pixel</p>
<p>images in the three-link arm task), we still have to generate full resolution images with the decoder network. For high-dimensional images generation fully connected neural networks are simply not an option. We thus decided to use up-convolutional networks, which were recently show to be powerful models for image generation [8, 9, 33].
To set-up these models we basically "mirror" the convolutional architecture used for the encoder. More specifically for each $5 \times 5$ convolution followed by $2 \times 2$ max-pooling step in the encoder network, we introduce a $2 \times 2$ up-sampling and $5 \times 5$ convolution step in the decoder network. The complete network architecture is given below. It is similar to the up-convolution networks used in Dosovitskiy et al. [9]. The upsampling strategy we use is simple "perforated" upsampling as described in [34].</p>
<h1>B. 2 Variational Autoencoder with slowness</h1>
<p>Enforcing temporal slowness during learning has previously been found to be a good proxy for learning representations in reinforcement learning [35, 36] and representation learning from videos [37]. We also consider a VAE variant with a slowness term on the latent representation by enforcing similarity of the encodings of temporally close images. This can be achieved by augmenting the standard VAE objective $\mathcal{L}^{\text {bound }}$ with an additional KL divergence term on the latent posterior $Q_{\phi}$ :</p>
<p>$$
\mathcal{L}^{\text {slow }}\left(\mathbf{x}<em t_1="t+1">{t}, \mathbf{x}</em>}\right)=\operatorname{KL}\left(Q_{\phi}\left(\mathbf{z<em t_1="t+1">{t+1} \mid \mathbf{x}</em>}\right) | Q_{\phi}\left(\mathbf{z<em t="t">{t} \mid \mathbf{x}</em>\right)\right)
$$</p>
<p>Indeed there seems to be a slightly better coherence of similar states in the latent spaces, as e.g. depicted in Figure 8 in the main paper. Yet, our experiments show that a slowness term alone does not suffice to structure the latent space, such that locally linear predictions and control become feasible.</p>
<h2>B. 3 Evaluation criteria</h2>
<p>For comparing the performance of all variants of E2C and the baselines, the following criteria are of importance:</p>
<ul>
<li>Autoencoding. Being able to reconstruct the given observations is the basic necessity for a model to work. The reconstruction cost drives a model to identify single states from its observations.</li>
<li>Decoding the next state. For any planning to be possible at all, the decoder must be able to generate the correct images from transitions the dynamics model performed. If this is not the case, we know that the latent states of the encoding and the transition model do not coincide, thus preventing any planning.</li>
<li>Optimizing latent trajectory costs. The action sequences for achieving a specified goal will be determined completely by locally linearized dynamics in the latent space. Therefore minimizing trajectory costs in latent space is, again, a necessity for successful control.</li>
<li>Optimizing real trajectory costs. While the action sequence has been determined for the latent dynamics, the deciding criterion is whether this reflects the true state trajectory costs. Therefore carrying out the "dreamed" plans in reality is the optimality criterion for every model. To make the different models comparable, we use the same cost matrices for evaluation, which are not necessarily the same as for optimization.</li>
</ul>
<p>We reflected these four criteria in the evaluation table in the paper. For the reconstruction of the current and next state we specified the mean log loss, which is in case of the Bernoulli distributions the cross entropy error function:</p>
<p>$$
\log p(\mathbf{x} \mid \hat{\mathbf{x}})=\frac{1}{N} \sum_{n=1}^{N} \sum_{i=0}^{n_{s}} x_{n, i} \log \hat{x}<em i="i" n_="n,">{n, i}+\left(1-x</em>\right)
$$}\right) \log \left(1-\hat{x}_{n, i</p>
<p>For the costs a model imagines and truly achieves, we sample from different starting states and accumulate the distances in latent and true state space according to the SOC method.</p>
<h1>B. 4 The three-link robot arm</h1>
<p>The robot arm we used in the last experiment in the main paper was simulated using dynamics generated by the MapleSim http://www.maplesoft.com/products/maplesim/ simulator wrapped in Python and visualized for producing inputs to E2C using PyGame. We simulated a fairly standard robot arm with three links. The length of the links were set to $2,1.2$ and 0.7 (units were set to meters). The masses of the corresponding links were all set to 10 kg .</p>
<h2>B. 5 Evaluating the true system model</h2>
<p>To compare the efficacy of different models when combined with optimal control algorithms, we always reported the cost in latent space (as used by the optimal control algorithm) as well as the "real" trajectory cost. To compute this real cost, we evaluated the same cost function as in the latent space (quadratic costs on the deviation from a given goal state), but using the real system states during execution and different cost matrices for a fair comparison.
As an upper bound on the performance achievable for control by any of the models, we also computed the true system cost by applying iLQR/AICO to a model of the real system dynamics. We have this model available since all experiments were performed in simulation.</p>
<h2>B. 6 Neural Network training</h2>
<h2>B.6.1 Experimental Setup</h2>
<p>All the datasets were created in advance as $\mathcal{D}=\left{\left(\mathbf{x}<em 1="1">{1}, \mathbf{u}</em>}, \mathbf{x<em T-1="T-1">{2}\right), \ldots,\left(\mathbf{x}</em>}, \mathbf{u<em T="T">{T-1}, \mathbf{x}</em>}\right)\right}$ for the training, validation and test split. While the E2C models were trained on $\mathcal{D}$, the ones that do not incorporate any transition information (i.e. AE, VAE) were trained on images $\mathcal{D<em 1="1">{\text {images }}=\left{\mathbf{x}</em>}, \ldots, \mathbf{x<em _pairs="{pairs" _text="\text">{T}\right}$ extracted from the original dataset $\mathcal{D}$. The slowness VAE was trained on the pairs of images subset $\mathcal{D}</em>}}=\left{\left(\mathbf{x<em 2="2">{1}, \mathbf{x}</em>}\right), \ldots,\left(\mathbf{x<em T="T">{T-1}, \mathbf{x}</em>$.
In order to learn dynamics predictions for the image-only autoencoders, we extracted the latent representations and combined them with the actions from $\mathcal{D}$ into $\mathcal{D}}\right)\right}$ and our E2C models on the full $\mathcal{D<em 1="1">{\text {dynamics }}=$ $\left{\left(\mathbf{z}</em>}, \mathbf{u<em 2="2">{1}, \mathbf{z}</em>}\right), \ldots,\left(\mathbf{z<em T-1="T-1">{T-1}, \mathbf{u}</em>\right)\right}$. On these low-dimensional representations we trained the dynamics MLPs, thus ensuring that all methods were trained on exactly the same data.}, \mathbf{z}_{T</p>
<h2>B.6.2 Implementation details</h2>
<p>We used orthogonal weight initialization for every layer [38]. As described in the main paper, Adam [14] was used as the learning rule for all networks. We found both these techniques to be fundamentally important for stabilizing training and achieving good reconstructions for all methods. Both methods also clearly helped to cut the hyperparameter search needed for all methods to a minimum. In the process of training, we could make out three phases: the unfolding of the latent space, the overcoming of the trivial solution (the average image of the dataset) and the minimization of the latent KL term. The architectures used for our experiments were as follows (where ReLU stands for rectified linear units [39] and conv. for convolutions):</p>
<h2>Plane</h2>
<ul>
<li>Input: $40^{2}$ image dimensions, 2 action dimensions</li>
<li>Latent Space dimensionality: 2</li>
<li>Encoder: 150 ReLU - 150 ReLU - 150 ReLU - 4 Linear (2 for AE)</li>
<li>Decoder: 200 ReLU - 200 ReLU - 1600 Linear (Sigmoid for AE)</li>
<li>Dynamics: 100 ReLU - 100 ReLU + Output layer (except Global E2C)</li>
<li>AE, VAE, VAE with slowness, Non-linear E2C: 2 Linear</li>
<li>E2C: 8 Linear ( $2 \cdot 2$ for $\mathbf{A}<em t="t">{t}, 2 \cdot 1$ for $\mathbf{B}</em>$ ), $\lambda=0.25$}, 2$ for $\mathbf{o}_{t</li>
<li>Adam: $\alpha=10^{-4}, \beta_{2}=0.1$</li>
<li>Evaluation costs: $\mathbf{R}<em a="a">{s}=0.1 \cdot \mathbf{I}, \mathbf{R}</em>$}=\mathbf{I}, \mathbf{R}_{o}=\mathbf{I</li>
</ul>
<h2>Pendulum swing-up</h2>
<ul>
<li>Input: $2 \cdot 48^{2}$ image dimensions, 1 action dimension</li>
<li>Latent Space dimensionality: 3</li>
<li>Encoder: 800 ReLU - 800 ReLU - 6 Linear (3 for AE)</li>
<li>Decoder: 800 ReLU - 800 ReLU - 4608 Linear (Sigmoid for AE)</li>
<li>Dynamics: 100 ReLU - 100 ReLU + Output layer (except Global E2C)</li>
<li>AE, VAE, VAE with slowness, Non-linear E2C: 3 Linear</li>
<li>E2C: 12 Linear $\left(2 \cdot 3\right.$ for $\mathbf{A}<em t="t">{t}=\left(\mathbf{I}+\mathbf{v}</em>} \mathbf{r<em t="t">{t}^{T}\right), 3 \cdot 1$ for $\left.\mathbf{B}</em>\right), \lambda=0.25$}, 3\right.$ for $\left.\mathbf{b}_{t</li>
<li>Adam: $\alpha=3 \cdot 10^{-4}, \beta_{2}=0.1$</li>
<li>Evaluation costs: $\mathbf{R}<em _mathrm_u="\mathrm{u">{\mathrm{z}}=\mathbf{I}, \mathbf{R}</em>$}}=0.1 \mathbf{I</li>
</ul>
<h1>Cart-Pole balancing</h1>
<ul>
<li>Input: $2 \cdot 80^{2}$ image dimensions, 1 action dimension</li>
<li>Latent Space dimensionality: 8</li>
<li>Encoder: $32 \times 5 \times 5$ ReLU - $32 \times 5 \times 5$ ReLU - $32 \times 5 \times 5$ ReLU - 512 ReLU - 512 ReLU</li>
<li>Decoder: 512 ReLU - 512 ReLU - $2 \times 2$ up-sampling - $32 \times 5 \times 5$ ReLU - $2 \times 2$ up-sampling $-32 \times 5 \times 5$ ReLU $-2 \times 2$ up-sampling $-32 \times 5 \times 5$ conv. ReLU</li>
<li>Dynamics: 200 ReLU - 200 ReLU + 32 Linear $\left(2 \cdot 8\right.$ for $\mathbf{A}<em t="t">{t}=\left(\mathbf{I}+\mathbf{v}</em>} \mathbf{r<em t="t">{t}^{T}\right), 8 \cdot 1$ for $\left.\mathbf{B}</em>\right), \lambda=1$}, 8\right.$ for $\left.b_{t</li>
<li>Adam: $\alpha=10^{-4}, \beta_{2}=0.1$</li>
<li>Evaluation costs: $\mathbf{R}<em _mathrm_u="\mathrm{u">{\mathrm{z}}=\mathbf{I}, \mathbf{R}</em>$}}=\mathbf{I</li>
</ul>
<h2>Three-link arm</h2>
<ul>
<li>Input: $2 \cdot 128^{2}$ image dimensions, 3 action dimensions</li>
<li>Latent Space dimensionality: 8</li>
<li>Encoder: $64 \times 5 \times 5$ conv. ReLU - $2 \times 2$ max-pooling - $32 \times 5 \times 5$ conv. ReLU - $2 \times 2$ max-pooling - $32 \times 5 \times 5$ conv. ReLU - $2 \times 2$ max-pooling - 512 ReLU - 512 ReLU</li>
<li>Decoder: 512 ReLU - 512 ReLU - $2 \times 2$ up-sampling - $32 \times 5 \times 5$ ReLU - $2 \times 2$ up-sampling $-32 \times 5 \times 5$ ReLU $-2 \times 2$ up-sampling $-64 \times 5 \times 5$ conv. ReLU</li>
<li>Dynamics: 200 ReLU - 200 ReLU + 48 Linear $\left(2 \cdot 8\right.$ for $\mathbf{A}<em t="t">{t}=\left(\mathbf{I}+\mathbf{v}</em>} \mathbf{r<em t="t">{t}^{T}\right), 8 \cdot 3$ for $\left.\mathbf{B}</em>\right), \lambda=1$}, 8\right.$ for $\left.b_{t</li>
<li>Adam: $\alpha=10^{-4}, \beta_{2}=0.1$</li>
<li>Evaluation costs: $\mathbf{R}<em _mathrm_u="\mathrm{u">{\mathrm{z}}=\mathbf{I}, \mathbf{R}</em>$
}}=0.001 \mathbf{I<img alt="img-4.jpeg" src="img-4.jpeg" /></li>
</ul>
<p>Figure 5: Generated "dreamed" trajectories of different models for the plane task (from left to right). The opacity of the obstacles has been lowered in this depiction for better visibility of the agent.</p>
<h1>C Supplementary evaluations</h1>
<h2>C. 1 Trajectories for plane and pendulum</h2>
<p>To qualitatively measure the predictive accuracy, the starting state for a trajectory is encoded and the actions are applied on the latent representation. After each transition, the predicted latent position is decoded and visualized. In this manner, multi-step predictions can be generated for the planar system in Figure 5 and for the inverted pendulum in Figures 6 and 7.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Generated "dreamed" trajectories (from left to right) for passive dynamics: the pendulum starts with angle $\theta=-\frac{\pi}{2}$ without velocity. The models have to predict the dynamics, while no force is applied.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Dreamed trajectories (from left to right) for controlled dynamics: the pendulum starts with angle $\theta=\frac{\pi}{2}$ without velocity. For 6 timesteps, full force is applied to the right, followed by 4 timesteps of full force to the left.</p>
<h2>C. 2 Inverted pendulum latent space</h2>
<p>Encoding the pendulum depictions into a 3-dimensional latent space allows for a visual comparison in Figure 8 .</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Latent spaces of all baseline models and E2C variants for the inverted pendulum.</p>
<h1>C. 3 Trajectories for cart-pole and three-link arm</h1>
<p>Finally - similar to the images in Section C. 1 - Figure 9 shows multi-step predictions for the cartpole system. We depict important cases: (1) a long-term prediction with the cart-pole standing still (essentially the unstable fix-point of the underlying dynamics); (2) the cart-pole moving to the right, changing the direction of the poles angular velocity (middle column); (3) and the pole moving farthest to the right. The long-term predictions by the E2C model are all of high quality. Note that for the uncontrolled dynamics the predictions show a slight bias of the pole moving to the right (an effect that we consistently saw in trained models for the cart-pole). We attribute this problem to the fact that discretization errors in the image rendering process of the pole angle make it hard to predict small velocities accurately.</p>
<h2>C. 4 Exemplary trajectory taken for three-link arm task</h2>
<p>Figure 10 shows a segment of a controlled trajectory for the three-link arm as executed by the E2C system. Note that, in contrast to other figures in this supplementary material, it does not show a long-term prediction but rather 10 steps of a trajectory (together with one-step-ahead predictions) that was taken by the E2C system when combined with model predictive control. For additional visualizations and controlled trajectories for all tasks we refer to the supplementary video.</p>
<h2>C. 5 Comparison of different models for cart-pole and robot arm</h2>
<p>In Table 2 we compare our variety of models in terms of real trajectory cost and task success percentage on the cart-pole and the robot arm. All results are averaged over 30 different starting states with a fixed goal state.</p>
<p>The cart-pole always starts in the goal state (zero angle and zero velocity) with small additive Gaussian noise ( $\sigma=0.01$ ). Success is defined as preventing the pole from falling below an angle of $\pm 0.85 \mathrm{rad}$. The three-link arm system begins in a random configuration and the goal is to to unroll all joints (e.g. make all angles zero) and stay $\epsilon$-close to that position.</p>
<p>The results show that only E2C and its non-linear variant can perform this task successfully, although there is still a large performance gap between the two. We conclude, that the error of linearizing non-linear dynamics after training the corresponding model grows to the point of no longer allowing accurate control for the system.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Dreamed trajectories (top to bottom) for uncontrolled (left column) and controlled (middle/right column) dynamics in the cart-pole system. The red image shows the initial configuration, which is encoded resulting in $\mathbf{z}_{1}$. The images in the right half of each column are then generated without additional input by following the dynamics in latent space. The left column depicts the uncontrolled case ( $\mathbf{u}=0$ for all steps). The middle column shows a controlled trajectory with torque -20 applied in each step and the right column a trajectory with torque 20 applied in each step. Prediction of the history image is omitted in these depictions.</p>
<p>Table 2: Comparison between trajectory costs of different approaches for the cart-pole and threelink task. The standard Autoencoder, Variational Autoencoder and Global E2C model are omitted from the table as they failed on this task (performance similar to VAE with slowness).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Algorithm</th>
<th style="text-align: center;">True model</th>
<th style="text-align: center;">VAE + slownes</th>
<th style="text-align: center;">E2C no latent KL</th>
<th style="text-align: center;">Non-linear E2C</th>
<th style="text-align: center;">E2C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cart-Pole balance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Traj. Cost</td>
<td style="text-align: center;">$15.33 \pm 7.70$</td>
<td style="text-align: center;">$49.12 \pm 16.94$</td>
<td style="text-align: center;">$48.90 \pm 17.88$</td>
<td style="text-align: center;">$31.96 \pm 13.26$</td>
<td style="text-align: center;">$22.23 \pm 14.89$</td>
</tr>
<tr>
<td style="text-align: center;">Success \%</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$63 \%$</td>
<td style="text-align: center;">$93 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Three-link arm</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Traj. Cost</td>
<td style="text-align: center;">59.46</td>
<td style="text-align: center;">$1275.53 \pm 864.66$</td>
<td style="text-align: center;">$1246.69 \pm 262.6$</td>
<td style="text-align: center;">$460.40 \pm 82.18$</td>
<td style="text-align: center;">$90.23 \pm 47.38$</td>
</tr>
<tr>
<td style="text-align: center;">Success \%</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$90 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison between AICO and iLQR based on the "real" cost for controlling the cart-pole and three-link robot arm using convolutional networks.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>iLQR</th>
<th>AICO</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cart-Pole</td>
<td></td>
<td></td>
</tr>
<tr>
<td>E2C</td>
<td>$14.56 \pm 4.12$</td>
<td>$12.56 \pm 2.47$</td>
</tr>
<tr>
<td>True model</td>
<td>$7.45 \pm 1.22$</td>
<td>$7.03 \pm 1.07$</td>
</tr>
<tr>
<td>Three-Link Robot Arm</td>
<td></td>
<td></td>
</tr>
<tr>
<td>E2C</td>
<td>$93.78 \pm 32.98$</td>
<td>$92.99 \pm 20.12$</td>
</tr>
<tr>
<td>True model</td>
<td>$53.59 \pm 9.74$</td>
<td>$56.34 \pm 10.82$</td>
</tr>
</tbody>
</table>
<h1>C. 6 Comparison of trajectory optimizers for cart-pole and robot arm</h1>
<p>To compare how well AICO deals with the covariance matrices estimated in latent space we performed an additional experiment on the cart-pole and three-link robot arm task comparing it to iLQR. We performed model predictive control using the locally linear E2C model starting in 10 different start states each. The remaining settings are as given in Section C.5.</p>
<p>As reported in Table 3, both methods performed about the same for these tasks, indicating that the covariance matrices estimated by our model do not "hurt" planning, but considering them does not improve performance either.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Frames extracted from a trajectory (top to bottom) as executed by the Embed to Control system. The left column shows the real images corresponding to transitions taken in the MDP. Middle and right column show the prediction of history image and current image based on the previous two images.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1} \mathrm{~A}$ Bernoulli distribution for $P_{\boldsymbol{\theta}}$ is a common choice when modeling black-and-white images.
${ }^{2}$ Note that this is the loss for the latent state space model and distinct from the SOC costs.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
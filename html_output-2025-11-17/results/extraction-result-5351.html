<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5351 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5351</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5351</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-f131b342e3aede46d24afc9b9055a94cceb0936a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f131b342e3aede46d24afc9b9055a94cceb0936a" target="_blank">InstructProtein: Aligning Human and Protein Language via Knowledge Instruction</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages, serves as a pioneering step towards text-based protein function prediction and sequence design, effectively bridging the gap between protein and human language understanding.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing annotation imbalance and instruction deficits in existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover, InstructProtein serves as a pioneering step towards text-based protein function prediction and sequence design, effectively bridging the gap between protein and human language understanding.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5351.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5351.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-based instruction generation (KCM + debiased sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph-based Instruction Generation with Knowledge Causal Modeling and Debiased Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that converts structured protein annotations (KG triples) into instruction tuning data for LLMs by (1) building a UniProtKB-derived knowledge graph, (2) augmenting it with Knowledge Causal Modeling (KCM) that links triples in a directed acyclic causal chain, (3) applying a debiased sampling strategy (sequence + property clustering with a specific sampling probability) to mitigate annotation imbalance, and (4) translating sampled triples (with retrieved KCM context) into (instruction, input, output) examples via a general LLM (e.g., ChatGPT) using KG-completion templates and in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>KG-triple-to-instruction (KG completion -> natural language)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>KG triples (p, r, t) sampled from a UniProtKB-derived knowledge graph (augmented with KCM causal chains) are used as the semantic content; a KG-completion template is applied and a general LLM (ChatGPT) is prompted (with in-context examples) to produce diverse natural-language instruction instances consisting of three fields: instruction (task description), input (instantiated arguments, e.g., protein sequence placeholder), and output (the correct response). The method retrieves the causal chain around a triple (KCM) to provide macro/micro-level context, and uses KG-completion as the task template to avoid inventing unsupported facts.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph constructed from UniProtKB annotations, augmented with Knowledge Causal Modeling (a directed acyclic graph of interconnected triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Designed to be factual (based on curated UniProtKB), logical (preserves causal chains via KCM), diverse (ChatGPT produces varied linguistic realizations using in-context examples), and well-balanced (debiasing through sequence and property clustering and uniform sampling across clusters). Aimed to reduce hallucination compared to direct LLM-only generation. Preserves explicit triple semantics but relies on LLM paraphrasing for fluent instruction text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used to instruction-tune LLMs and then evaluate on protein-related downstream tasks: zero-shot protein sequence understanding (subcellular localization: binary and 10-way; Gene Ontology branches: BP/MF/CC), metal ion binding (MIB) prediction, instruction-protein pairing (fold/family/superfamily), and protein sequence de novo design (structure and function guided generation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Multiple task metrics reported: classification accuracy and AUPR for GO tasks and localization, and pairing accuracy for instruction-protein pairing; Key reported results for the model trained with this representation (InstructProtein, 1.3B params): Subcellular localization (binary) accuracy = 85.19%; Subcellular localization (10-way) = 70.79%; GO-BP ACC = 71.49, AUPR = 83.16; GO-MF ACC = 85.83, AUPR = 93.68; GO-CC ACC = 79.79, AUPR = 86.37; Metal Ion Binding (MIB) = 62.68%; Instruction-protein pairing accuracies: Fold = 55.57, SuperFamily = 65.07, Family = 79.24. Ablation metrics (impact of sampling/KCM) from Table 4: Unclustering/no KCM Location(Sub)=58.12, GO(MF)=85.58, Fold Rank=51.98; Seq clustering/no KCM Location=62.77, GO(MF)=83.70, Fold Rank=54.41; Seq+Prop clustering/no KCM Location=69.95, GO(MF)=85.92, Fold Rank=53.81; Seq+Prop clustering + KCM Location=70.79, GO(MF)=85.83, Fold Rank=55.57.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to two alternative instruction generation pipelines illustrated in the paper—(a) prompting LLMs from seed tasks and (b) prompting LLMs to convert raw documents—the KG-based approach is argued to reduce hallucination and training bias; empirically, InstructProtein (trained using the KG->text pipeline) outperforms baseline LLMs (OPT, LLaMA, Alpaca, Galactica, BioMedGPT) across the reported protein understanding and generation benchmarks. The ablation study shows that clustering by sequence+property and adding KCM improves performance relative to uniform/unclustered sampling. The paper contrasts their approach with prior KG-integration methods that concatenate KG triplets with text, noting their novelty is generating instruction text from KG content (rather than only concatenating triples as model input).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on an off-the-shelf LLM (ChatGPT) to convert triples into fluent instruction text, which introduces dependence on that model's internal behavior and could still allow subtle hallucinations in phrasing; some fields in figures are noted to depend on LLM internal knowledge. The instruction dataset currently lacks quantitative annotations (numerical/structural values), limiting quantitative protein-design outputs; LLMs also show difficulty handling numerical values. The debiasing/clustering approach requires setting thresholds (δ_seq, δ_prop) and embedding/protein-similarity computations, which may miss critical functional mutations if clustering is not property-aware (observed in ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InstructProtein: Aligning Human and Protein Language via Knowledge Instruction', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5351.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5351.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-from-seed-tasks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction generation by prompting LLMs from seed tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conventional pipeline where a general LLM is prompted with a small set of seed tasks or examples to produce new instruction/task pairs automatically; mentioned as a common automatic instruction-data generation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Seed-task LLM generation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Provide a few seed instruction examples to an LLM and request it to produce more instructions and corresponding outputs, effectively extrapolating variations of the seed tasks into larger datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Not graph-based (operates from seed textual tasks), though paper contrasts it with KG-mediated approaches</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Quick to scale and does not require structured knowledge graphs; high linguistic diversity. However, vulnerable to hallucination and amplification of dataset biases present in seed examples, and may lack factual grounding for domain-specific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Not used directly in this paper's experiments; presented for comparison in instruction-generation design space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper states that this approach can introduce hallucination and bias compared to the KG-mediated method; no numerical head-to-head results provided in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potential for hallucination, domain factual errors, and amplification of biases when seed tasks are unbalanced or limited; less factual grounding for specialized domains like protein biology.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InstructProtein: Aligning Human and Protein Language via Knowledge Instruction', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5351.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5351.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-from-raw-docs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction generation by prompting LLMs from raw documents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses LLMs to read raw domain documents (e.g., UniProt textual descriptions) and generate instruction instances from their contents; cited as another common approach for automatic instruction data creation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Raw-document LLM generation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Prompt a general LLM with textual passages (e.g., protein descriptions) and ask it to produce instruction-input-output triples derived from the document content, often with few-shot examples to guide conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Document/text-based (not graph-first); can be applied to textual outputs of KGs but differs from explicitly using KG structure</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages existing textual narratives so can capture rich descriptions; susceptible to hallucinations and reproducing narrative biases, lacks explicit causal structure preservation that KCM provides.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Mentioned as an alternative instruction generation approach; not empirically evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper argues that this method is more prone to hallucination and bias than the KG-based pipeline; no quantitative comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Same as seed-based approach: factual errors and bias propagation; difficulty ensuring balanced coverage across under-studied entities (annotation imbalance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InstructProtein: Aligning Human and Protein Language via Knowledge Instruction', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers <em>(Rating: 2)</em></li>
                <li>BioMedGPT: Open Multimodal Generative Pre-trained Transformer for Biomedicine <em>(Rating: 2)</em></li>
                <li>Galactica: A Large Language Model for Science <em>(Rating: 1)</em></li>
                <li>Unnatural instructions: Tuning language models with (almost) no human labor <em>(Rating: 1)</em></li>
                <li>K-BERT: Enabling Language Representation with Knowledge Graph <em>(Rating: 2)</em></li>
                <li>CoLAKE: Contextualized Language and Knowledge Embedding <em>(Rating: 2)</em></li>
                <li>Generating Datasets with Pretrained Language Models <em>(Rating: 1)</em></li>
                <li>ZeroGen: Efficient Zero-shot Learning via Dataset Generation <em>(Rating: 1)</em></li>
                <li>InterPro in 2022 <em>(Rating: 2)</em></li>
                <li>The Gene Ontology knowledgebase in 2023 <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5351",
    "paper_id": "paper-f131b342e3aede46d24afc9b9055a94cceb0936a",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "KG-based instruction generation (KCM + debiased sampling)",
            "name_full": "Knowledge Graph-based Instruction Generation with Knowledge Causal Modeling and Debiased Sampling",
            "brief_description": "A pipeline that converts structured protein annotations (KG triples) into instruction tuning data for LLMs by (1) building a UniProtKB-derived knowledge graph, (2) augmenting it with Knowledge Causal Modeling (KCM) that links triples in a directed acyclic causal chain, (3) applying a debiased sampling strategy (sequence + property clustering with a specific sampling probability) to mitigate annotation imbalance, and (4) translating sampled triples (with retrieved KCM context) into (instruction, input, output) examples via a general LLM (e.g., ChatGPT) using KG-completion templates and in-context examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "KG-triple-to-instruction (KG completion -&gt; natural language)",
            "representation_description": "KG triples (p, r, t) sampled from a UniProtKB-derived knowledge graph (augmented with KCM causal chains) are used as the semantic content; a KG-completion template is applied and a general LLM (ChatGPT) is prompted (with in-context examples) to produce diverse natural-language instruction instances consisting of three fields: instruction (task description), input (instantiated arguments, e.g., protein sequence placeholder), and output (the correct response). The method retrieves the causal chain around a triple (KCM) to provide macro/micro-level context, and uses KG-completion as the task template to avoid inventing unsupported facts.",
            "graph_type": "Knowledge graph constructed from UniProtKB annotations, augmented with Knowledge Causal Modeling (a directed acyclic graph of interconnected triples)",
            "representation_properties": "Designed to be factual (based on curated UniProtKB), logical (preserves causal chains via KCM), diverse (ChatGPT produces varied linguistic realizations using in-context examples), and well-balanced (debiasing through sequence and property clustering and uniform sampling across clusters). Aimed to reduce hallucination compared to direct LLM-only generation. Preserves explicit triple semantics but relies on LLM paraphrasing for fluent instruction text.",
            "evaluation_task": "Used to instruction-tune LLMs and then evaluate on protein-related downstream tasks: zero-shot protein sequence understanding (subcellular localization: binary and 10-way; Gene Ontology branches: BP/MF/CC), metal ion binding (MIB) prediction, instruction-protein pairing (fold/family/superfamily), and protein sequence de novo design (structure and function guided generation).",
            "performance_metrics": "Multiple task metrics reported: classification accuracy and AUPR for GO tasks and localization, and pairing accuracy for instruction-protein pairing; Key reported results for the model trained with this representation (InstructProtein, 1.3B params): Subcellular localization (binary) accuracy = 85.19%; Subcellular localization (10-way) = 70.79%; GO-BP ACC = 71.49, AUPR = 83.16; GO-MF ACC = 85.83, AUPR = 93.68; GO-CC ACC = 79.79, AUPR = 86.37; Metal Ion Binding (MIB) = 62.68%; Instruction-protein pairing accuracies: Fold = 55.57, SuperFamily = 65.07, Family = 79.24. Ablation metrics (impact of sampling/KCM) from Table 4: Unclustering/no KCM Location(Sub)=58.12, GO(MF)=85.58, Fold Rank=51.98; Seq clustering/no KCM Location=62.77, GO(MF)=83.70, Fold Rank=54.41; Seq+Prop clustering/no KCM Location=69.95, GO(MF)=85.92, Fold Rank=53.81; Seq+Prop clustering + KCM Location=70.79, GO(MF)=85.83, Fold Rank=55.57.",
            "comparison_to_other_representations": "Compared to two alternative instruction generation pipelines illustrated in the paper—(a) prompting LLMs from seed tasks and (b) prompting LLMs to convert raw documents—the KG-based approach is argued to reduce hallucination and training bias; empirically, InstructProtein (trained using the KG-&gt;text pipeline) outperforms baseline LLMs (OPT, LLaMA, Alpaca, Galactica, BioMedGPT) across the reported protein understanding and generation benchmarks. The ablation study shows that clustering by sequence+property and adding KCM improves performance relative to uniform/unclustered sampling. The paper contrasts their approach with prior KG-integration methods that concatenate KG triplets with text, noting their novelty is generating instruction text from KG content (rather than only concatenating triples as model input).",
            "limitations_or_challenges": "Relies on an off-the-shelf LLM (ChatGPT) to convert triples into fluent instruction text, which introduces dependence on that model's internal behavior and could still allow subtle hallucinations in phrasing; some fields in figures are noted to depend on LLM internal knowledge. The instruction dataset currently lacks quantitative annotations (numerical/structural values), limiting quantitative protein-design outputs; LLMs also show difficulty handling numerical values. The debiasing/clustering approach requires setting thresholds (δ_seq, δ_prop) and embedding/protein-similarity computations, which may miss critical functional mutations if clustering is not property-aware (observed in ablations).",
            "uuid": "e5351.0",
            "source_info": {
                "paper_title": "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLM-from-seed-tasks",
            "name_full": "Instruction generation by prompting LLMs from seed tasks",
            "brief_description": "A conventional pipeline where a general LLM is prompted with a small set of seed tasks or examples to produce new instruction/task pairs automatically; mentioned as a common automatic instruction-data generation approach.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Seed-task LLM generation",
            "representation_description": "Provide a few seed instruction examples to an LLM and request it to produce more instructions and corresponding outputs, effectively extrapolating variations of the seed tasks into larger datasets.",
            "graph_type": "Not graph-based (operates from seed textual tasks), though paper contrasts it with KG-mediated approaches",
            "representation_properties": "Quick to scale and does not require structured knowledge graphs; high linguistic diversity. However, vulnerable to hallucination and amplification of dataset biases present in seed examples, and may lack factual grounding for domain-specific knowledge.",
            "evaluation_task": "Not used directly in this paper's experiments; presented for comparison in instruction-generation design space.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Paper states that this approach can introduce hallucination and bias compared to the KG-mediated method; no numerical head-to-head results provided in this work.",
            "limitations_or_challenges": "Potential for hallucination, domain factual errors, and amplification of biases when seed tasks are unbalanced or limited; less factual grounding for specialized domains like protein biology.",
            "uuid": "e5351.1",
            "source_info": {
                "paper_title": "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLM-from-raw-docs",
            "name_full": "Instruction generation by prompting LLMs from raw documents",
            "brief_description": "A pipeline that uses LLMs to read raw domain documents (e.g., UniProt textual descriptions) and generate instruction instances from their contents; cited as another common approach for automatic instruction data creation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Raw-document LLM generation",
            "representation_description": "Prompt a general LLM with textual passages (e.g., protein descriptions) and ask it to produce instruction-input-output triples derived from the document content, often with few-shot examples to guide conversion.",
            "graph_type": "Document/text-based (not graph-first); can be applied to textual outputs of KGs but differs from explicitly using KG structure",
            "representation_properties": "Leverages existing textual narratives so can capture rich descriptions; susceptible to hallucinations and reproducing narrative biases, lacks explicit causal structure preservation that KCM provides.",
            "evaluation_task": "Mentioned as an alternative instruction generation approach; not empirically evaluated in this paper.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Paper argues that this method is more prone to hallucination and bias than the KG-based pipeline; no quantitative comparisons provided.",
            "limitations_or_challenges": "Same as seed-based approach: factual errors and bias propagation; difficulty ensuring balanced coverage across under-studied entities (annotation imbalance).",
            "uuid": "e5351.2",
            "source_info": {
                "paper_title": "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers",
            "rating": 2
        },
        {
            "paper_title": "BioMedGPT: Open Multimodal Generative Pre-trained Transformer for Biomedicine",
            "rating": 2
        },
        {
            "paper_title": "Galactica: A Large Language Model for Science",
            "rating": 1
        },
        {
            "paper_title": "Unnatural instructions: Tuning language models with (almost) no human labor",
            "rating": 1
        },
        {
            "paper_title": "K-BERT: Enabling Language Representation with Knowledge Graph",
            "rating": 2
        },
        {
            "paper_title": "CoLAKE: Contextualized Language and Knowledge Embedding",
            "rating": 2
        },
        {
            "paper_title": "Generating Datasets with Pretrained Language Models",
            "rating": 1
        },
        {
            "paper_title": "ZeroGen: Efficient Zero-shot Learning via Dataset Generation",
            "rating": 1
        },
        {
            "paper_title": "InterPro in 2022",
            "rating": 2
        },
        {
            "paper_title": "The Gene Ontology knowledgebase in 2023",
            "rating": 2
        }
    ],
    "cost": 0.012091999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>InSTructProtein: Aligning Human and Protein Language VIA KNOWLEDGE InSTRUCTION</h1>
<p>Zeyuan Wang ${ }^{1,2,3}$ Qiang Zhang ${ }^{1,2 <em>}$ Keyan Ding ${ }^{1,2}$ Ming Qin, ${ }^{1,2,3}$<br>Xiang Zhuang ${ }^{1,2}$ Xiaotong $\mathrm{Li}^{1,2}$ Huajun Chen ${ }^{1,2,3 </em>}$<br>${ }^{1}$ College of Computer Science and Technology, Zhejiang University<br>${ }^{2}$ ZJU-Hangzhou Global Scientific and Technological Innovation Center<br>${ }^{3}$ AZFT Joint Lab for Knowledge Engine<br>{yuanzew, qiang.zhang.cs, dingkeyan, qinandming}@zju.edu.cn<br>{zhuangxiang, 3190104904, huajunsir}@zju.edu.cn</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing annotation imbalance and instruction deficits in existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover, InstructProtein serves as a pioneering step towards text-based protein function prediction and sequence design, effectively bridging the gap between protein and human language understanding.</p>
<h2>1 INTRODUCTION</h2>
<p>The landscape of Natural Language Processing (NLP) research, and indeed the broader Artificial Intelligence (AI) community, has recently been revolutionized by generative Large Language Models (LLMs) (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020), such as ChatGPT (Ouyang et al., 2022). The expansion of parameter size and training corpora has empowered these models to acquire versatile, general-purpose data representations that seamlessly transcend linguistic tasks encompassing comprehension and generation in a multitude of languages. Beyond natural languages (a.k.a., human languages), recent investigations have illuminated the potential of these LLMs to serve as a versatile interface for processing multimodal data, including but not limited to images, videos and speech (Chen et al., 2021; Reed et al., 2022; Gong et al., 2023; Huang et al., 2023).
However, general LLMs fall short of capturing the intricate realm of biological sequences, a domain abundant with its own unique linguistic nuances. For example, existing LLMs like ChatGPT cannot understand biological sequences when they are asked to predict the family of proteins (see Figure 1). The biological sequences, particularly proteins, represent a distinctive facet of what could be referred to as "life language", exerting a significant influence on signal transduction pathways, enzymatic catalysis, and gene regulation (Lee \&amp; Yaffe, 2016; Huber, 2001; Südhof, 1995; Durek \&amp; Walther, 2008; Luzarowski et al., 2021; Jiang et al., 2022).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To unlock the potential within LLMs for deciphering proteins, researchers have put rich efforts into developing protein language models (PLMs) (Alley et al., 2019; Elnaggar et al., 2021; Rives et al., 2021; Rao et al., 2021; Lin et al., 2023). These specialized models are tailored to ingest amino acid sequences as inputs, predict protein functionalities, or even design de novo proteins. Notwithstanding, it is crucial to highlight that while PLMs exhibit competence in comprehending amino acid sequences, they are unable to grasp the complexities of human languages. A recent research trend (Abdine et al., 2023; Luo et al., 2023) has explored models that accept both protein sequences and textual descriptions as input, aiming to enhance the protein function prediction ability. Nevertheless, these endeavors to align the realms of protein and human languages are unidirectional and remain in their nascent stages; they fall short of being able to generate protein sequences based on textual instructions. In essence, there exists an unaddressed void in the current landscape of LLMs, wherein the ability to swiftly traverse between human and protein languages.</p>
<p>To enable an LLM to adeptly comprehend both human and protein languages, we contend that the limitations imposed by existing models primarily stem from their training corpora. Notably, many existing models are trained on either human languages or protein sequences, rendering them proficient in only one of these linguistic realms. This unilateral training approach is insufficient to imbue an LLM with a comprehensive vocabulary encompassing both languages. Moreover, it is important to recognize that the existing protein-text corpus used in previous studies (Luo et al., 2023; Abdine et al., 2023; Xu et al., 2023; Taylor et al., 2022) has its limitations. (1) The imbalance of annotations: Researchers tend to focus on well-studied proteins, leading to a significant disparity in the availability of annotations (Kustatscher et al., 2022). Training LLMs directly on such a corpus introduces model bias, which ultimately results in suboptimal performance. (2) The absence of instructional signals: Protein-related textual content is primarily comprised of descriptive narratives, often devoid of instructional signals specifically designed for training LLMs. This inherent disparity obstructs a holistic understanding of a wide range of tasks, ultimately resulting in subpar zero-shot performance (Wei et al., 2022a). In short, the fundamental hurdle of current LLMs involves curating an elaborate training corpus that seamlessly bridges the gap between human and protein languages.</p>
<p>In this work, we introduce InstructProtein, a pioneering study that aligns human and protein languages through knowledge instruction, leading to the first LLM with bidirectional generation capabilities between these two languages. Specifically, to equip LLMs with the ability to understand protein language, InstructProtein adopts a two-step training approach. It initiates with pre-training on protein and natural language corpora, followed by finetuning with the established protein knowledge instruction dataset. To construct such an instruction dataset, we first transform raw protein-text corpora into a structured knowledge graph (KG). Inspired by the idea of chain-of-thoughts, we enrich KG with knowledge causal modeling, which involves establishing causal relationships between triples, indicating causality within annotations. We then propose a debiased sampling strategy to select KG triples, effectively addressing the issue of annotation imbalance. Finally, we mimic KG completion tasks, leverage general LLMs to convert KG triples into instructions, and conduct supervised instruction tuning. Extensive experiments have demonstrated that the introduced protein knowledge instructions significantly improve the performance of LLMs on protein understanding and design tasks. Our contributions can be summarized as follows:</p>
<ol>
<li>We propose InstructProtein, an innovative LLM that enables bidirectional generation between protein and human languages, effectively filling the gap between the two languages.</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Prediction</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Cytoplasm</td>
<td style="text-align: center;">Nucleus</td>
<td style="text-align: center;">Cell membrane</td>
<td style="text-align: center;">Others</td>
</tr>
<tr>
<td style="text-align: left;">OPT</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">1691</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1806</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Galactica</td>
<td style="text-align: center;">1807</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Alpaca</td>
<td style="text-align: center;">1808</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Figure 2: We visualized the top-5 subcellular location categories and their respective proportions, in comparison to the least frequently used annotations, which accounted for only $0.000224 \%$.</p>
<p>Table 1: The results of querying existing LLMs for factual knowledge. We prompt LLMs to predict subcellular location, but their results are biased to a certain category, which suggests that these LLMs have been contaminated by annotation imbalance.
2. We introduce a protein instruction generation framework with knowledge graphs, resulting in the first high-quality protein instruction dataset for tuning LLMs.
3. The InstructProtein outperforms state-of-the-art LLMs by a substantial margin, serving as a pioneering step toward text-guided protein function prediction and sequence design.</p>
<h1>2 A Closer Look at Annotation Imbalance</h1>
<p>Much of life science research is dedicated to unraveling the biological functions of proteins. While certain proteins, such as the well-studied tumor suppressor p53 (Dolgin, 2017), have undergone extensive investigation, there still exist tens of thousands of proteins remain categorized as understudied. This phenomenon implies an imbalance in protein function annotation. To clearly illustrate this problem, we take the subcellular location as an example, and show its annotation distribution in Figure 2. The results reveal a notable concentration of research attention on proteins residing in the cytoplasm, while other subcellular locations lack comprehensive labeling and study.</p>
<p>The annotation imbalance has a detrimental effect on the performance of existing LLMs. To demonstrate this, we collect the same number of proteins in each subcellular location category from UniProtKB (Consortium, 2019), resulting in 1,808 proteins in total, and prompt LLMs to predict the subcellular location. The outcomes of LLMs are presented in Table 1, from which one can observe that these LLMs are biased in a certain category, due to the annotation imbalance in the training corpus of LLMs.</p>
<h2>3 InSTRUCTPROTEIN</h2>
<p>This section presents the methodological details of InstructProtein. We first pre-train it in a selfsupervised manner on natural language corpus and protein sequence datasets respectively, and then conduct supervised tuning using the created knowledge instruction dataset.</p>
<h3>3.1 Multilingual Pre-Training</h3>
<p>InstructProtein is designed to comprehend both the protein and human languages. An intuitive approach involves incrementally pre-training an LLM using the protein corpus $\mathcal{P}$ and text sequences $\mathcal{T}$. Given an unsupervised corpus of tokens $\mathcal{X}=\left{x_{1}, x_{2}, \ldots, x_{n}\right} \in \mathcal{P} \cup \mathcal{T}$, the training objective of a generative LLM (e.g., OPT (Zhang et al., 2022a)) is defined as</p>
<p>$$
L(\mathcal{X})=\sum_{i} \log P\left(x_{i} \mid x_{i-k}, \ldots, x_{i-1} ; \theta\right)
$$</p>
<p>where the prediction of each token depends on previous tokens $x_{&lt;i}, k$ is the context window size, and the conditional probability $P$ is modeled using a neural network parameterized by $\theta$.</p>
<h3>3.2 InStruction Tuning</h3>
<p>After pre-training, the model acquires an extensive comprehension of both natural language and protein sequences; however, it still falls short in achieving alignment between these two different languages. We fill this gap through supervised instruction tuning.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Overview of Instruction generation methods. The red text represents the fields that rely on internal knowledge of LLMs. (a) Given a set of seed tasks, prompting an LLM to produce new instruction data.(b) Utilizing LLMs to generate the instruction data corresponding to the contents in raw documents. (c) The proposed knowledge graph (KG)-based instruction generation framework. We first construct a KG with knowledge causal modeling (KCM), and introduce a debiased sampler to pick the informative triples, which are then translated into instruction data through the use of LLMs in conjunction with KG completion tasks.</p>
<h1>3.2.1 Knowledge Instruction Generation</h1>
<p>We propose an instruction generation method based on knowledge graphs (KGs) and LLMs, aiming to construct a factual, logical, diverse, and well-balanced protein instruction dataset. Figure 3 illustrates the pipeline of three instruction generation frameworks. Conventional approaches directly utilize LLMs to generate instruction data from seed tasks or raw documents, which may introduce hallucination and bias. In the proposed method, KGs are incorporated as intermediaries to address these limitations. In specific, a KG encompassed with knowledge causal modeling is constructed to provide factual protein knowledge, based on which a debiased sampling strategy is proposed to pick KG triples. An LLM (e.g., ChatGPT) then translates the samples into instruction data and enriches them with a wide range of expressions.</p>
<p>KG Construction. We use UniProtKB as our data source to construct the protein knowledge graph denoted as $\mathcal{G}={\mathcal{P}, \mathcal{R}, \mathcal{T}}$. Here, $\mathcal{P}, \mathcal{R}$, and $\mathcal{T}$ are sets of protein sequences, relations, and textual annotations. Note that the textual description of proteins in UniProtKB is structured, making it easy to transform them into a knowledge graph. In our pursuit of enhancing the quality of the instruction dataset, we augment KG to provide informative relationships. Borrowing ideas from chain-of-thoughts (Wei et al., 2022b), we recognize that a logical chain also exists within protein annotations. For example, the biological processes in which a protein can participate are intricately linked to its molecular function and subcellular location, with the molecular function itself being influenced by the protein's domain. To represent this causal chain of protein knowledge, we introduce a novel concept called Knowledge Causal Modeling (KCM). Specifically, a knowledge causal model comprises multiple interconnected triples organized in a directed acyclic graph, where the edge direction signifies causal relationships. This graph organizes the triples, moving from the micro-level, encompassing characteristics of protein sequences (e.g., domains), to the macro-level, encompassing biological functions. In Figure 4, we show an example of KCM retrieved from InterPro (Paysan-Lafosse et al., 2023) based on a given triple.</p>
<p>KG Triple Sampling. To generate instruction data, we need to sample triples from the constructed KG. Considering the annotation imbalance problem in the KG, we propose a debiased sampling strategy as an alternative to uniform sampling. In specific, we first group proteins together based on their sequence and property similarities, and then uniformly pick triples within each cluster.</p>
<p>To access sequence similarity, we employ MMseqs2 (Steinegger \&amp; Söding, 2017) to calculate the editing distance $d_{\text {seq }}(\cdot, \cdot)$ (see Appendix A.2.2). For property similarity, since the protein properties are extensive and many of them remain unexplored, we only consider the known annotations in KG when computing the property similarity. Specifically, given an annotation $t$ and a relation $r$, we denote $C_{t}={p: p \in \mathcal{P} \wedge(p, r, t) \in \mathcal{G}}$ and $C_{/ t}={p: p \in \mathcal{P} \wedge(p, r, t) \notin \mathcal{G}}$ are the protein set based on the presence or absence of $t$. The basic idea is to maximize agreement within $C_{t}$ and minimize agreement between $C_{t}$ and $C_{/ t}$, via optimizing protein KG embeddings. In practice, we minimize a margin-based ranking criterion over the knowledge graph:</p>
<p>$$
\mathcal{L}=-\sum_{p_{t} \in C_{t}, p_{/ t} \in C_{/ t}}\left[\log \sigma\left(\gamma-d_{\text {prop }}\left(\boldsymbol{p}<em _prop="{prop" _text="\text">{t}, \boldsymbol{t}+\boldsymbol{r}\right)\right)+\log \sigma\left(d</em>\right)-\gamma\right)\right]
$$}}\left(\boldsymbol{p}_{/ t}, \boldsymbol{t}+\boldsymbol{r</p>
<p>where $\boldsymbol{p}, \boldsymbol{r}, \boldsymbol{t} \in \mathbb{R}^{k}$ ( $k$ is a hyperparameter) are embeddings of proteins, relations, and annotations, $\sigma$ is the sigmoid function, and $\gamma$ is the margin. $d_{\text {prop }}(\cdot, \cdot)$ is a dissimilarity measure of properties, which is implemented as the $\ell_{1}$-norm.</p>
<p>We define the threshold of sequence and property similarities as $\delta_{\text {prop }}$ and $\delta_{\text {seq }}$, respectively. We denote two proteins to be similar $p_{1} \simeq p_{2}$ as $d_{\text {seq }}\left(p_{1}, p_{2}\right)&lt;\delta_{\text {seq }}$ and $d_{\text {prop }}\left(\boldsymbol{p}<em 2="2">{1}, \boldsymbol{p}</em>$ can be formulated as:}\right)&lt;\delta_{\text {prop }}$. $\mathcal{L}=\left{C_{1}, \ldots, C_{m}\right}$ represents the aggregation of proteins with $m$ clusters, and the cluster $C_{i</p>
<p>$$
C_{i}=\left{p: \exists p^{\prime} \in C_{i}, p \simeq p^{\prime} \wedge \forall \rho \in C_{j \neq i}, p \neq \rho\right}
$$</p>
<p>Then, the probability of sampling a triple $(p, r, t)$ is:</p>
<p>$$
P((p, r, t))=\frac{1}{m} \times \frac{1}{\left|C_{i}\right|} \times \frac{1}{|p|}
$$</p>
<p>where $p \in C_{i},\left|C_{i}\right|$ denotes the size of $C_{i}$, and $|p|$ are the number of annotations on $p$.
KG Triple to Instruction. By employing the debiased sampling strategy, one can sample a large number of well-balanced KG triples. We then focus on translating these triples into instruction data. While the generation of creative tasks requires domain knowledge, the KG completion tasks offer a comprehensive template for proposing domain-specific tasks based on triples. Therefore, we simulate KG completion, and employ general LLMs (e.g., ChatGPT) to transform KG triples with retrieved KCM into instruction data, which contains three fields: an instruction describing the task, an input argument that instantiates the instruction, and an output result reflecting a correct execution of the instruction given the input arguments. Figure 4 shows an example of converting the triple to instructions. The detailed implementation is depicted in Appendix 8.</p>
<h1>3.2.2 Tuning LLMs with InStructions.</h1>
<p>Instruction tuning involves further training LLMs in a supervised manner on an instruction dataset comprising of (instruction, input, output), bridging the gap between the LLMs' next-word prediction objective and users' goal of ensuring adherence to human instructions. With the proposed knowledge instruction dataset $\mathcal{I}$, we finetune the pre-trained LLM to align the protein and human languages. Given an instruction $Z \in \mathcal{I}$ and its tokens $\mathcal{X}=\left{x_{1}, x_{2}, \ldots, x_{n}\right} \in Z$, the training objective is the same as that defined in Eq.(1).</p>
<h2>4 EXPERIMENTS</h2>
<p>In this section, we evaluate the performance of LLMs in terms of protein sequence understanding and design. To effectively evaluate these two capabilities, we have modified the existing downstream task datasets to facilitate the evaluation of LLMs.</p>
<h3>4.1 EXPERIMENTAL SETUP</h3>
<p>The pre-training corpus contains protein sequences from UniRef100 (Suzek et al., 2015) and sentences from PubMed abstracts. Following the methodology described in Section 3.2.1, we generated an</p>
<p>Table 2: Zero-shot performance on protein sequence understanding.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Params.</th>
<th>Location</th>
<th></th>
<th>GO-BP</th>
<th></th>
<th>GO-MF</th>
<th></th>
<th>GO-CC</th>
<th></th>
<th>MIB</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Bin</td>
<td>Sub</td>
<td>ACC</td>
<td>AUPR</td>
<td>ACC</td>
<td>AUPR</td>
<td>ACC</td>
<td>AUPR</td>
<td></td>
</tr>
<tr>
<td>OPT</td>
<td>1.3B</td>
<td>57.52</td>
<td>29.06</td>
<td>51.83</td>
<td>64.76</td>
<td>56.10</td>
<td>74.50</td>
<td>51.94</td>
<td>71.90</td>
<td>49.40</td>
</tr>
<tr>
<td>LLaMA</td>
<td>7.0B</td>
<td>57.52</td>
<td>29.14</td>
<td>56.96</td>
<td>61.85</td>
<td>54.58</td>
<td>58.06</td>
<td>51.57</td>
<td>53.53</td>
<td>50.00</td>
</tr>
<tr>
<td>Alpaca</td>
<td>7.0B</td>
<td>57.52</td>
<td>18.32</td>
<td>61.69</td>
<td>65.13</td>
<td>59.37</td>
<td>73.02</td>
<td>57.98</td>
<td>61.71</td>
<td>50.38</td>
</tr>
<tr>
<td>Galactica</td>
<td>1.3B</td>
<td>57.52</td>
<td>18.32</td>
<td>55.11</td>
<td>57.08</td>
<td>61.30</td>
<td>61.93</td>
<td>51.17</td>
<td>54.54</td>
<td>51.58</td>
</tr>
<tr>
<td>BioMedGPT</td>
<td>10B</td>
<td>59.51</td>
<td>56.39</td>
<td>50.31</td>
<td>50.82</td>
<td>51.02</td>
<td>50.81</td>
<td>49.41</td>
<td>49.39</td>
<td>54.42</td>
</tr>
<tr>
<td>InstructProtein</td>
<td>1.3B</td>
<td>85.19</td>
<td>70.79</td>
<td>71.49</td>
<td>83.16</td>
<td>85.83</td>
<td>93.68</td>
<td>79.79</td>
<td>86.37</td>
<td>62.68</td>
</tr>
</tbody>
</table>
<p>instruction dataset comprising 2.8 million data. Specifically, the protein knowledge graph was constructed utilizing the annotations provided by UniProt/Swiss-Prot(Consortium, 2019), which contains the superfamily, family, domain, conserved site, active site, binding site, location, function, and involved biological process of proteins. Knowledge causal modeling is sourced from the InterPro (Paysan-Lafosse et al., 2023) and Gene Ontology (Aleksander et al., 2023) database. Note that proteins appearing in downstream tasks have been excluded from the training data. We leverage ChatGPT (Ouyang et al., 2022) to convert triples into instruction data. Detailed experimental setups can be found in Appendix A.3.</p>
<h1>4.2 Protein Sequence Understanding</h1>
<p>Datasets and Metrics. We evaluate LLMs on three widely-used protein function classification tasks: (1) Protein Localization Prediction, which involves the prediction of the subcellular location of a given protein. We address two subproblems from DeepLoc (Almagro Armenteros et al., 2017), the subcellular localization prediction (Abbr., Sub) with 10 location categories and the binary localization prediction (Abbr., Bin) with 2 location categories; (2) Protein Function Annotation, aiming to predict the correct annotations of proteins. We choose Gene Ontology (GO) dataset Gligorijević et al. (2021), which has three branches: molecular function (MF), biological process (BP), and cellular component (CC). We use the dataset splits under $95 \%$ sequence identity cutoff. (3) Metal Ion Binding (MIB) Prediction, a binary classification task where the model needs to determine whether there are metal ion-binding sites in the protein. We use the dataset from Hu et al. (2022).</p>
<p>Similar to reading comprehension problems in NLP, we transform all items in the above datasets into a Question\&amp;Answer (QA) format where each item consists of a protein sequence, a question about that protein, and a list of possible answers. LLMs are required to predict which answers are true. Following Brown et al. (2020), we use a classification approach where, for example, only two outputs ("yes" and "no") are considered and the higher probability one is taken as the model's prediction. All evaluations are carried out in a zero-shot setting, without few-shot demonstrations.</p>
<p>Baselines. We adopt five state-of-the-art open-sourced LLMs as the baselines. OPT (Zhang et al., 2022a) and LLaMA (Touvron et al., 2023) are trained on massive text corpus, and Alpaca (Taori et al., 2023) is a language model based on instruction-tuned LLaMA. Galactica (Taylor et al., 2022) and BioMedGPT (Luo et al., 2023) are domain-specific LLMs, which are trained on a large curated corpus of humanity's scientific knowledge, such as research papers about proteins and genes. For a fair comparison, we designed a template for each model through prompt engineering so that the model could follow our instructions and output the answers.</p>
<p>Results. We present the evaluation results in Table 2. Compared with all baselines, InstructProtein achieves new state-of-the-art performance on all tasks. There are two key observations. First, InstructProtein clearly outperforms the LLMs (i.e., OPT, LLaMA, Alpaca) which are stemmed from natural language training corpora. These results demonstrate that training with the corpus where proteins and natural language coexist is beneficial to LLMs, enhancing their proficiency in protein language understanding. Second, InstructProtein performs consistently better than Galactica and BioMedGPT, despite all of them leveraging UniProtKB as a corpus for natural language alignment with proteins. The results verify that our high-quality instruction data can boost zero-shot performance. It is worth noting that in the protein subcellular localization (bin) task, there exists a severe bias in LLMs (OPT, LLaMA, Alpaca, and Galactica), leading to the classification of all proteins into a single group and resulting in the same accuracy of $57.52 \%$.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Visualization of structure instruction-based protein sequence de novo design. We prompt our models with different scales ( $125 \mathrm{~m}, 350 \mathrm{~m}$ and 1.3 b ) to generate three kinds of proteins (all $\alpha$-helix, all $\beta$-sheet, and a combination of $\alpha$-helix and $\beta$-sheet), respectively. (a) We visualize the pLDDT of generated sequences predicted by AlphaFold2 to assess the protein foldability. (b) The embeddings of sequences prompted with all $\alpha$-helix and all $\beta$-sheet instructions, which are extracted from ESM2 and visualized by the MDS algorithm. (c) The structure of generated proteins with the highest confidence in each class.</p>
<h1>4.3 Protein Sequence Design</h1>
<p>Generating proteins following human instructions is a highly exciting area of research. With the incorporation of protein sequences as part of the language capabilities in LLMs, InstructProtein is capable of generating protein sequences. However, the lack of standardized computational metrics to assess the quality of generated proteins poses challenges for advancing protein generation models. In this study, we present our endeavor to build a computational evaluation framework.</p>
<h3>4.3.1 Zero-shot Instruction-Protein Pairing</h3>
<p>Datasets and Metrics. We design an instruction-protein pairing task to assess the consistency between the instruction and the generated protein. Specifically, we employ the dataset proposed by Hou et al. (2018) to provide fold-related instructions and proteins. Given a protein $p$ and the corresponding instruction $Z_{0}$, we randomly sample other $n$ instructions $\left{Z_{1}, Z_{2}, \ldots, Z_{n}\right}(n=9$ in this experiment), and the likelihood $\mathcal{L}$ of the protein given the various instructions is computed. The minimization of $\mathcal{L}\left(p \mid Z_{i}\right)$ at $i=0$ signifies a correct pairing, and vice versa.</p>
<p>Results. Table 3 reports the accuracy of the instruction-protein pairing task. One can observe that InstructProtein surpasses the baselines by a large margin. BioMedGPT focuses solely on converting proteins to texts and lacks protein design capabilities. Galactica exhibits limited zero-shot performance in aligning instructions with proteins, since it is trained with narrative protein corpus. These results confirm the superiority of our model in instruction-following for protein generation.</p>
<p>Table 3: Accuracy of instruction-protein pairing.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: right;">Fold</th>
<th style="text-align: right;">Fold Rank</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;">SuperFamily</td>
<td style="text-align: right;">Family</td>
</tr>
<tr>
<td style="text-align: left;">OPT</td>
<td style="text-align: right;">7.79</td>
<td style="text-align: right;">6.45</td>
<td style="text-align: right;">6.68</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: right;">9.33</td>
<td style="text-align: right;">5.90</td>
<td style="text-align: right;">10.30</td>
</tr>
<tr>
<td style="text-align: left;">Alpaca</td>
<td style="text-align: right;">5.43</td>
<td style="text-align: right;">3.90</td>
<td style="text-align: right;">4.71</td>
</tr>
<tr>
<td style="text-align: left;">Galactica</td>
<td style="text-align: right;">11.00</td>
<td style="text-align: right;">10.12</td>
<td style="text-align: right;">10.37</td>
</tr>
<tr>
<td style="text-align: left;">BioMedGPT</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">InstructProtein</td>
<td style="text-align: right;">$\mathbf{5 5 . 5 7}$</td>
<td style="text-align: right;">$\mathbf{6 5 . 0 7}$</td>
<td style="text-align: right;">$\mathbf{7 9 . 2 4}$</td>
</tr>
</tbody>
</table>
<h3>4.3.2 Protein Sequence De Novo Design</h3>
<p>Designing proteins with specified structures. We investigate whether InstructProtein could generate new protein sequences that are individually valid and consistent with instructions. SCOPe (Chandonia et al., 2022) classifies protein structures according to the content and organization of secondary structures, including all $\alpha$-helix, all $\beta$-sheet, and the combination of $\alpha$-helix and $\beta$-sheet. We sample 100 sequences from each class and assess the foldability of individual sequences by predicting their corresponding structures using ColabFold (Mirdita et al., 2022; Jumper et al., 2021) and</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Visualization of functional instruction-based protein sequence de novo design. We prompt our model with the instruction "I would like a protein that enables heme binding". (a) is the groundtruth protein that binds with heme. (b), (c) and (d) are generated proteins with decent binding affinity.</p>
<p>Table 4: Ablation of the proposed sampling strategy and KCM used in knowledge instructions.</p>
<table>
<thead>
<tr>
<th>Sampling Strategy</th>
<th>KCM</th>
<th>Location (Sub)</th>
<th>GO (MF)</th>
<th>Fold Rank (Fold)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Unclustering</td>
<td>No</td>
<td>58.12</td>
<td>85.58</td>
<td>51.98</td>
</tr>
<tr>
<td>Seq. Clustering</td>
<td>No</td>
<td>62.77</td>
<td>83.70</td>
<td>54.41</td>
</tr>
<tr>
<td>Seq.\&amp;Prop. Clustering</td>
<td>No</td>
<td>69.95</td>
<td>$\mathbf{8 5 . 9 2}$</td>
<td>53.81</td>
</tr>
<tr>
<td>Seq.\&amp;Prop. Clustering</td>
<td>Yes</td>
<td>$\mathbf{7 0 . 7 9}$</td>
<td>85.83</td>
<td>$\mathbf{5 5 . 5 7}$</td>
</tr>
</tbody>
</table>
<p>computing the average predicted local distance difference test (pLDDT) across the whole structure (Figure 5 (a)). pLDDT increases with model scale, suggesting that scaling up the parameter size leads to generating more foldable sequences. We leverage ESM2 (Lin et al., 2023) as a feature extractor to obtain the generated all $\alpha$-helix and all $\beta$-sheet protein representations, which are then visualized using multi-dimensional scaling (MDS) algorithm (Kruskal, 1964) (Figure 5 (b)). We observe that the representations are divided into two groups according to instructions, indicating the instruction-following ability of the proposed model. We visualize the predicted structure of the proteins with the highest confidence in each class (Figure 5 (c)). These results demonstrate that InstructProtein establishes a close correlation between natural language and protein language, verifying the effectiveness of protein de novo design based on structure-related instruction.</p>
<p>Designing proteins binding with specified ligands. To verify the ability to follow function-related instructions, we employ InstructProtein to design heme binders, which are proteins capable of binding to a specific compound, and visualize 3D structures of three generated proteins. In Figure 6, we present the docking result (docked by DiffDock (Corso et al., 2023)), the binding affinity (predicted by Smina (Koes et al., 2013; Trott \&amp; Olson, 2010), the lower the better), and the pLDDT score (predicted by ColabFold; the higher the absolute value, the better). We can observe the resulting proteins exhibit notable binding affinity, confirming the efficacy of InstructProtein in heme binder design. We provide more case studies in Appendix A.5.</p>
<h1>4.4 Ablation Study</h1>
<p>In this subsection, we conduct ablation studies on the sampling strategy and knowledge causal modeling (KCM) used in our knowledge instruction generation method. From the results in Table 4, we observe that clustering similar proteins in annotation imbalance-related tasks (Location and Fold Rank) can effectively improve model performance. However, for the GO task without the annotation imbalance problem, the clustering method based on sequence similarity alone makes the model performance decrease. This phenomenon arises due to the occurrence of critical mutations at key sites, leading to significant functional alterations in proteins. Consequently, their resemblance to extensively studied sequences diminishes the likelihood of selection. This leaves the model lacking the ability to distinguish the functionally important sites. Such problems can be avoided by segment clusters based on protein properties. We also observe that the causal relationship between annotations introduced by KCM improves the performance.</p>
<h1>5 Related Works</h1>
<p>Large Language Models (LLMs) have achieved breakthrough performance in NLP (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Black et al., 2022; Zhang et al., 2022a; Chowdhery et al., 2022; Touvron et al., 2023). These models, trained via self-supervision on extensive, general corpora, exhibit proficiency in a multitude of tasks (Hendrycks et al., 2020; Jin et al., 2021; Pal et al., 2022). However, these LLMs are primarily tailored for human language comprehension, limiting their utility in decoding the intricate language of proteins. To bridge this gap, Protein Language Models (PLMs) have garnered significant attention (Alley et al., 2019; Elnaggar et al., 2021; Rives et al., 2021; Rao et al., 2021; Lin et al., 2023; Rao et al., 2020; Meier et al., 2021; Ferruz et al., 2022; Notin et al., 2022). Nonetheless, PLMs confront limitations stemming from their training corpora, lacking factual knowledge of human language. To align protein with human language, multimodal approaches (Abdine et al., 2023; Luo et al., 2023) integrate protein encoders into LLMs within an encoder-decoder framework. Notwithstanding, these architectures predominantly exhibit a unidirectional cross-modal capability, focusing solely on converting protein language to texts. Taylor et al. (2022) treats protein language and human language as a unified modality. However, the use of existing protein-text corpus hinders the alignment of protein and human languages. The proposed InstructProtein represents a pioneering effort with the ability to generate in both human and protein languages, marking a significant advancement in the field of protein understanding and design.</p>
<p>Instruction Tuning is a supervised approach to align language models with user intention (Weller et al., 2020; Mishra et al., 2022; Wang et al., 2022; Wei et al., 2021; Sanh et al., 2021; Ouyang et al., 2022). It is worth noting that acquiring large-scale instruction data can be a resource-intensive and time-consuming endeavor, thereby motivating the exploration of automatic data generation techniques. A prevalent strategy (Anaby-Tavor et al., 2020; Andreas, 2020; Kaushik et al., 2019) involves augmenting existing datasets. Alternatively, several fully automatic datasets have been proposed to eliminate the need for labeled data. Schick \&amp; Schütze (2021) and Ye et al. (2022) advocate for leveraging pre-trained language models to generate comprehensive labeled datasets from scratch, tailored to predefined tasks. Honovich et al. (2023) and Wang et al. (2023) used pre-trained LLMs to automatically construct instructions by a handful of examples. However, these methodologies may introduce hallucination and bias into the instruction data. To overcome these limitations, our work incorporates knowledge graphs as intermediaries, resulting in a protein instruction dataset that is factual, logical, diverse, and well-balanced.</p>
<p>Knowledge Graph (KG) is often employed to enhance the capabilities of LLMs. A related subfield to our work involves integrating KGs into the input of LLMs. Researchers such as Sun et al. (2021); Liu et al. (2020); Sun et al. (2020); Zhang et al. (2022b) have pursued this avenue by concatenating a KG triplet with corresponding sentences, leveraging language modeling to amalgamate knowledge with textual representations. Our approach also involves utilizing KGs as input, however, a significant difference lies in how LLMs interact with KGs. We focus on generating instruction data using KGs, allowing LLMs to capture insights from instructions rather than relying solely on KG triplets.</p>
<h2>6 Discussion and Conclusion</h2>
<p>InstructProtein explores the feasibility of bidirectional generation between human and protein languages within a single large language model. Our approach involved the transformation of a raw protein-text corpus into a structured knowledge graph, from which KG triples were sampled and converted into instructions. This KG-based instruction generation method resulted in a high-quality instruction dataset, facilitating the LLM to align protein language with human language.</p>
<p>Nevertheless, it's important to acknowledge that there are some limitations inherent in our model. One such limitation, shared with large language models, is that InstructProtein encounters challenges with handling numerical values. This limitation hinders our ability to quantitatively characterize proteins, including tasks like thermostability prediction. Besides, the design of a satisfactory protein necessitates meeting a multitude of requirements, such as solubility, stability, and 3D structure. However, our current model is primarily tailored to support protein design based on qualitative descriptions, such as designing proteins within specific protein families. This limitation arises from our instructions exclusively offering qualitative protein descriptions encompassing aspects like family and function, while lacking quantitative annotations concerning elements such as 3D structures, which hold significance in protein design.</p>
<p>In the future, we will incorporate a broader spectrum of instructions, including quantitative descriptions. This extension will empower our model to provide quantitative outputs. These developments will open up new avenues for further advancing the integration of protein and human languages, as well as expanding its practical utility in diverse applications.</p>
<h1>REFERENCES</h1>
<p>Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, and Michalis Vazirgiannis. Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. arXiv preprint arXiv:2307.14367, 2023.</p>
<p>Suzi A Aleksander, James Balhoff, Seth Carbon, J Michael Cherry, Harold J Drabkin, Dustin Ebert, Marc Feuermann, Pascale Gaudet, Nomi L Harris, et al. The Gene Ontology knowledgebase in 2023. Genetics, 224(1):iyad031, 2023.</p>
<p>Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church. Unified rational protein engineering with sequence-based deep representation learning. Nature methods, 16(12):1315-1322, 2019.</p>
<p>José Juan Almagro Armenteros, Casper Kaae Sønderby, Søren Kaae Sønderby, Henrik Nielsen, and Ole Winther. Deeploc: prediction of protein subcellular localization using deep learning. Bioinformatics, 33(21):3387-3395, 2017.</p>
<p>Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov, Naama Tepper, and Naama Zwerdling. Do Not Have Enough Data? Deep Learning to the Rescue! In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7383-7390, 2020.</p>
<p>Jacob Andreas. Good-Enough Compositional Data Augmentation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7556-7566, 2020.</p>
<p>Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. GPT-NeoX-20B: An OpenSource Autoregressive Language Model. In Proceedings of BigScience Episode #5 - Workshop on Challenges \&amp; Perspectives in Creating Large Language Models, pp. 95-136, virtual+Dublin, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and others Askell. Language Models are FewShot Learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020.</p>
<p>John-Marc Chandonia, Lindsey Guan, Shiangyi Lin, Changhua Yu, Naomi K Fox, and Steven E Brenner. SCOPe: improvements to the structural classification of proteins-extended database to facilitate variant interpretation and machine learning. Nucleic acids research, 50(D1):D553-D559, 2022.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating Large Language Models Trained on Code, 2021.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>UniProt Consortium. UniProt: a worldwide hub of protein knowledge. Nucleic acids research, 47 (D1):D506-D515, 2019.</p>
<p>Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock: Diffusion Steps, Twists, and Turns for Molecular Docking. International Conference on Learning Representations (ICLR), 2023.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In North American Chapter of the Association for Computational Linguistics, pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.</p>
<p>Elie Dolgin. The most popular genes in the human genome. Nature, 551(7681):427-432, 2017.
Pawel Durek and Dirk Walther. The integrated analysis of metabolic and protein interaction networks reveals novel molecular organizing principles. BMC systems biology, 2(1):1-20, 2008.</p>
<p>Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning. IEEE transactions on pattern analysis and machine intelligence, 44(10):7112-7127, 2021.</p>
<p>Noelia Ferruz, Steffen Schmidt, and Birte Höcker. ProtGPT2 is a deep unsupervised language model for protein design. Nature communications, 13(1):4348, 2022.</p>
<p>Vladimir Gligorijević, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structurebased protein function prediction using graph convolutional networks. Nature communications, 12 (1):3168, 2021.</p>
<p>Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. MultiModal-GPT: A Vision and Language Model for Dialogue with Humans, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. In International Conference on Learning Representations, 2020.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models, 2022.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14409-14428, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long. 806 .</p>
<p>Jie Hou, Badri Adhikari, and Jianlin Cheng. Deepsf: deep convolutional neural network for mapping protein sequences to folds. Bioinformatics, 34(8):1295-1303, 2018.</p>
<p>Mingyang Hu, Fajie Yuan, Kevin K Yang, Fusong Ju, Jin Su, Hui Wang, Fei Yang, and Qiuyang Ding. Exploring evolution-aware \&amp; -free protein language models as protein function predictors. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=U8k0QaBgXS.</p>
<p>Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language Is Not All You Need: Aligning Perception with Language Models, 2023.</p>
<p>Armin Huber. Scaffolding proteins organize multimolecular protein complexes for sensory signal transduction. European Journal of Neuroscience, 14(5):769-776, 2001.</p>
<p>Yining Jiang, Batiste Thienpont, Vinay Sapuru, Richard K Hite, Jeremy S Dittman, James N Sturgis, and Simon Scheuring. Membrane-mediated protein interactions drive membrane protein organization. Nature Communications, 13(1):7373, 2022.</p>
<p>Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.</p>
<p>John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583-589, 2021.</p>
<p>Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. Learning the Difference That Makes A Difference with Counterfactually-Augmented Data. In International Conference on Learning Representations, 2019.</p>
<p>David Ryan Koes, Matthew P Baumgartner, and Carlos J Camacho. Lessons learned in empirical scoring with smina from the csar 2011 benchmarking exercise. Journal of chemical information and modeling, 53(8):1893-1904, 2013.</p>
<p>Joseph B Kruskal. Nonmetric multidimensional scaling: a numerical method. Psychometrika, 29(2): $115-129,1964$.</p>
<p>Georg Kustatscher, Tom Collins, Anne-Claude Gingras, Tiannan Guo, Henning Hermjakob, Trey Ideker, Kathryn S Lilley, Emma Lundberg, Edward M Marcotte, Markus Ralser, et al. An open invitation to the Understudied Proteins Initiative. Nature Biotechnology, 40(6):815-817, 2022.</p>
<p>Michael J Lee and Michael B Yaffe. Protein regulation in signal transduction. Cold Spring Harbor perspectives in biology, 8(6):a005918, 2016.</p>
<p>Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123-1130, 2023.</p>
<p>Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. K-BERT: Enabling Language Representation with Knowledge Graph. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 2901-2908, 2020.</p>
<p>Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. BioMedGPT: Open Multimodal Generative Pre-trained Transformer for Biomedicine. arXiv preprint arXiv:2308.09442, 2023.</p>
<p>Marcin Luzarowski, Rubén Vicente, Andrei Kiselev, Mateusz Wagner, Dennis Schlossarek, Alexander Erban, Leonardo Perez de Souza, Dorothee Childs, Izabela Wojciechowska, Urszula Luzarowska, et al. Global mapping of protein-metabolite interactions in saccharomyces cerevisiae reveals that ser-leu dipeptide regulates phosphoglycerate kinase activity. Communications Biology, 4(1):181, 2021.</p>
<p>Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. Advances in Neural Information Processing Systems, 34:29287-29303, 2021.</p>
<p>Milot Mirdita, Konstantin Schütze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin Steinegger. ColabFold: Making Protein folding accessible to all. Nature Methods, 2022. doi: 10.1038/s41592-022-01488-1.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-Task Generalization via Natural Language Crowdsourcing Instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470-3487, 2022.</p>
<p>Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena Hurtado, Aidan N Gomez, Debora Marks, and Yarin Gal. Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. In International Conference on Machine Learning, pp. 16990-17017. PMLR, 2022.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. MedMCQA: A Large-scale Multi-subject Multi-Choice Dataset for Medical domain Question Answering. In Conference on Health, Inference, and Learning, pp. 248-260. PMLR, 2022.</p>
<p>Typhaine Paysan-Lafosse, Matthias Blum, Sara Chuguransky, Tiago Grego, Beatriz Lázaro Pinto, Gustavo A Salazar, Maxwell L Bileschi, Peer Bork, Alan Bridge, Lucy Colwell, et al. InterPro in 2022. Nucleic Acids Research, 51(D1):D418-D427, 2023.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep Contextualized Word Representations. In North American Chapter of the Association for Computational Linguistics, pp. 2227-2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling Language Models: Methods, Analysis \&amp; Insights from Training Gopher, 2021.</p>
<p>Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. Transformer Protein Language Models Are Unsupervised Structure Learners. In International Conference on Learning Representations, 2020.</p>
<p>Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. MSA Transformer. In International Conference on Machine Learning, pp. 8844-8856. PMLR, 2021.</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Giménez, Yury Sulsky, Jackie Kay, et al. A Generalist Agent. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/ forum?id=1ikK0kHjvj. Featured Certification, Outstanding Certification.</p>
<p>Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask Prompted Training Enables Zero-Shot Task Generalization. In International Conference on Learning Representations, 2021.</p>
<p>Timo Schick and Hinrich Schütze. Generating Datasets with Pretrained Language Models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6943-6951, 2021.</p>
<p>Martin Steinegger and Johannes Söding. MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature biotechnology, 35(11):1026-1028, 2017.</p>
<p>Thomas C Südhof. The synaptic vesicle cycle: a cascade of protein-protein interactions. Nature, 375 (6533):645-653, 1995.</p>
<p>Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuan-Jing Huang, and Zheng Zhang. CoLAKE: Contextualized Language and Knowledge Embedding. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 3660-3670, 2020.</p>
<p>Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, et al. ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation. arXiv preprint arXiv:2107.02137, 2021.</p>
<p>Baris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt Consortium. UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926-932, 2015.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A Large Language Model for Science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Oleg Trott and Arthur J Olson. AutoDock Vina: Improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455-461, 2010.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. $5085-5109,2022$.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long. 754.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations, 2021.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations, 2022a. URL https: / / openreview. net/forum?id=gEZrGCozdqR.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022b.</p>
<p>Orion Weller, Nicholas Lourie, Matt Gardner, and Matthew E Peters. Learning from Task Descriptions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1361-1375, 2020.</p>
<p>Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. Protst: Multi-Modality Learning of Protein Sequences and Biomedical Texts. arXiv preprint, 2023.</p>
<p>Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. ZeroGen: Efficient Zero-shot Learning via Dataset Generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11653-11669, 2022.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open Pre-trained Transformer Language Models, 2022a.</p>
<p>Taolin Zhang, Chengyu Wang, Nan Hu, Minghui Qiu, Chengguang Tang, Xiaofeng He, and Jun Huang. Dkplm: Decomposable Knowledge-Enhanced Pre-trained Language Model for Natural Language Understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 11703-11711, 2022b.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 DETAILED PROTEIN UNDERSTUDYING PROBLEM ANALYSIS</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: The overview of the problem of understudied proteins. (a) We visualized the protein length distribution for different annotation scores. The annotation score provides a heuristic measure of the annotation content (Score 5 is associated with the best-annotated entries, and a score 1 denotes an entry with rather basic annotation.). (b) We visualized the ten most used categories in subcellular location annotations.</p>
<p>Much of life science research is dedicated to unraveling the biological functions of proteins. While certain proteins, such as the well-studied tumor suppressor p53 (Dolgin, 2017), have undergone extensive investigation, tens of thousands of proteins remain categorized as understudied. This classification implies that their biological functions are poorly elucidated, and they lack comprehensive annotation of their molecular properties.
In Figure 7, we present an analysis conducted on UniProtKB/Swiss-Prot, a highly reputable and manually curated protein knowledge repository. Figure 7 (a) depicts the relationship between the distribution of proteins and their annotation scores. These results emphasize the substantial variation in protein distribution corresponding to different annotations. This variance implies that the annotation of proteins is biased. To illustrate this problem more clearly, we analyze the subcellular location annotation. Figure 7 (b) illustrates the distribution of such annotations. The data reveals a notable concentration of research attention on proteins residing in the cytoplasm, with other subcellular locations significantly lacking in comprehensive labeling and study.</p>
<h2>A. 2 DETAILED METHOD</h2>
<h2>A.2.1 In-Context ExAMPles</h2>
<p>Knowledge Instruction relies on examples to teach language models understand how to convert information extracted from the knowledge graph into instruction data. Here we provide our example (Figure 8). We notice that when only two examples of</p>
<p>Table 5: Example of instructions in the Knowledge Instruction dataset. We leverage {protein} as a placeholder for the corresponing protein sequence.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Family Generation</td>
<td>Instruction: Can you provide me with a protein belonging to the secretoglobin family? <br> Output: Sure, here's a protein from the secretoglobin family: {protein}.</td>
</tr>
<tr>
<td>Superfamily Prediction</td>
<td>Instruction: Could you please provide information about the superfamily to which {protein} belongs? <br> Output: {protein} is likely a member of the rho GTPase activation protein superfamily.</td>
</tr>
<tr>
<td>Domain Generation</td>
<td>Instruction: Could you provide me with a protein that contains the nad(p)-binding rossmann-fold domain? <br> Output: Certainly, here is a protein that contains the nad(p)-binding rossmann-fold domain: {protein}.</td>
</tr>
<tr>
<td>Family Prediction</td>
<td>Instruction: What family does {protein} belong to? <br> Output: The protein belongs to the alpha-1b-glycoprotein/leukocyteimmunoglobulinlike receptor family.</td>
</tr>
<tr>
<td>Function Prediction</td>
<td>Instruction: protein Does the protein have the molecular function of zinc ion binding? <br> Output: Yes, based on the information that the protein belongs to the Retinoid X receptor/HNF4 family, it indeed has the molecular function of zinc ion binding.</td>
</tr>
<tr>
<td>Process Prediction</td>
<td>Instruction: I'm interested in learning about the biological process associated with {protein}. Could you provide information on this? <br> Output: Given its association with the Retinoid X receptor/HNF4 family, {protein} is likely involved in the regulation of DNA-templated transcription.</td>
</tr>
<tr>
<td>Binding Site Generation</td>
<td>Instruction: Provide me with a protein associated with the IQ motif EF-hand binding site. <br> Output: Sure, the protein that interacts with the IQ motif EF-hand binding site and exhibits a protein binding molecular function is {protein}.</td>
</tr>
<tr>
<td>Family Prediction</td>
<td>Instruction: Does {protein} belong to nfat activation molecule 1? <br> Output: Based on the record, the answer is yes.</td>
</tr>
</tbody>
</table>
<p>different expressions are provided for each KGC task, the language capabilities of LLMs are activated, generating a variety of instruction data as illustrated in Table 5</p>
<h1>A.2.2 Edit Distance Algorithm</h1>
<p>We denote $A=a_{1} a_{2} \ldots a_{n}$ and $B=b_{1} b_{2} \ldots b_{n}$ as two sequences to be aligned, where $n$ and $m$ are the lengths of $A$ and $B$, respectively. Before calculating the editing distance, we have to determine the substitution matrix to calculate the replacement score $s(\cdot, \cdot) \in(0,1]$ and the gap penalty scheme $W_{k}$, where $k$ is the gap length. Then the distance matrix $H$ can be formulated as:</p>
<p>$$
H_{i, j}=\min \left{H_{i-1, j-1}+s\left(a_{i}, b_{j}\right) ; H_{i-k, j}-W_{k} ; H_{i, j-1}-W_{1} ; 1\right}
$$</p>
<p>where $H_{k, 0}=H 0, l=0$ for $0 \leq k \leq n$ and $0 \leq l \leq m$. We leverage $H_{n, m} / \max (n, m)$ as the sequence distance between $A$ and $B$.</p>
<h2>A. 3 Detailed EXPERIMENTAL SETUPS</h2>
<p>We perform incremental training on OPT-1.3b. We wrap the protein sequence with <protein> and &lt;{protein&gt; and apply character-based tokenization, treating each amino acid as a single token. For text corpus, we tokenize them using the GPT-2 byte level BPE tokenizer. We utilize Pytorch to conduct experiments with 832 G V100 GPUs. We use a batch size of 128 and a context length of 1,024 tokens. We adopt the Fully Sharded Data Parallel (FSDP) acceleration strategy alongside the fp16 data format. We adopt the AdamW optimizer with $\beta=(0.9,0.98)$. We set the weight decay to 0.01 and the dropout rate to 0.1 . The learning rate increases to $1 \mathrm{e}-4$ for the first 5000 warming-up steps and decays linearly to 0 for the rest of the training steps. We pre-train InstructProtein for the first 40,000 steps, and instruction tune it in the next 20,000 steps.</p>
<h2>A. 4 Downstream Task Definition</h2>
<p>We list the detailed definition of downstream tasks. {protein} and {label} are used as placeholders. Dataset statistics are summarized in Table 6.</p>
<p>Subcellular Localization Prediction is a sequence-level classification task. Each input sequence $x$ is mapped to a label $y$ which represents the subcellular location.</p>
<ul>
<li>Prompt template (InstructProtein, OPT, LLaMA, Alpha, BioMedGPT): {protein} Instruction: What cellular components is the protein located in?</li>
<li>Prompt template (Galactica): {protein} ## Subcellular Location</li>
<li>Label words (sub): {0: "plasma membrane", 1: "cytoplasm", 2: "endoplasmic reticulum", 3: "golgi", 4: "vacuole", 5: "mitochondrion", 6: "nucleus", 7: "peroxisome", 8: "chloroplast", 9: "extracellular"}</li>
<li>Label words (bin): {0: ["plasma membrane", "golgi", "vacuole", "endoplasmic reticulum"], 1: ["extracellular", "peroxisome", "nucleus", "cytoplasm", "mitochondrion", "chloroplast"}}</li>
</ul>
<p>Protein Function Annotation is a sequence-level classification task to annotation protein with functional labels. Each example consists of a protein, a label. They system must predict whether the label belongs to the protein.</p>
<ul>
<li>Prompt template: {protein} Instruction: Does the protein associate with label?</li>
<li>Label words: {0: "No", 1: "Yes"}</li>
</ul>
<p>Metal Ion Binding Prediction is a sequence-level classification task to predict whether a protein can bind to ion.</p>
<ul>
<li>Prompt template: {protein} Instruction: Does the protein associate with metal ion binding?</li>
<li>Label words: {0: "No", 1: "Yes"}</li>
</ul>
<p>Instruction-Protein Pairing Accuracy probe the insturction-following capabilities in protein generation. Protein are decoded under 10 different instructions ( 9 randomly sampled instructions and 1 true corresponding instruction). The system must predict which one is the most relevant instruction.</p>
<ul>
<li>Prompt template: Instruction: I would like a protein that is in {label}. Output: One of the protein that meets the demand is ${$ protein $} "$</li>
</ul>
<h1>Table 6: Dataset Statistics for downstream tasks.</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;"># Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Subcellular Localization Prediction - bin</td>
<td style="text-align: right;">1,749</td>
</tr>
<tr>
<td style="text-align: left;">Subcellular Localization Prediction - sub</td>
<td style="text-align: right;">2,773</td>
</tr>
<tr>
<td style="text-align: left;">Protein Function Annotation - Biological Process</td>
<td style="text-align: right;">104,794</td>
</tr>
<tr>
<td style="text-align: left;">Protein Function Annotation - Molecular Function</td>
<td style="text-align: right;">22,372</td>
</tr>
<tr>
<td style="text-align: left;">Protein Function Annotation - Cellular Component</td>
<td style="text-align: right;">38,594</td>
</tr>
<tr>
<td style="text-align: left;">Metal Ion Binding Prediction</td>
<td style="text-align: right;">1,332</td>
</tr>
<tr>
<td style="text-align: left;">Instruction-Protein Pairing Accuracy - Fold</td>
<td style="text-align: right;">718</td>
</tr>
<tr>
<td style="text-align: left;">Instruction-Protein Pairing Accuracy - Family</td>
<td style="text-align: right;">1,272</td>
</tr>
<tr>
<td style="text-align: left;">Instruction-Protein Pairing Accuracy - Superfamily</td>
<td style="text-align: right;">7,408</td>
</tr>
</tbody>
</table>
<h1>A. 5 MORE EXAMPLES</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: More examples of protein understanding.</p>
<p>Instruction: I would like a protein that has cystatin domain. Output:
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>I: Length: 107 residues</p>
<p>Instruction: I would like a protein that enables GTP binding. Output:
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10: More examples of function-instruction-based protein de novo design.</p>
<p>Instruction: I would like a protein that is in metallothionein family. Output:
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Instruction: I would like a protein that is in retroviral VpR/VpX protein family. Output:
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Instruction: I would like a protein that is in SsrA-binding protein family. Output:
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Instruction: Instruction: I would like a protein that is in kappa casein family. Output:
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 11: More examples of family-instruction-based protein de novo design (colored by pLDDT).</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 12: More examples of structure-instruction-based protein de novo design (colored by pLDDT).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
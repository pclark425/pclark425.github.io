<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5736 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5736</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5736</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-f4b73a1b1de1cbea715e1ed404c0626eac53d976</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f4b73a1b1de1cbea715e1ed404c0626eac53d976" target="_blank">LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model</a></p>
                <p><strong>Paper Venue:</strong> Applied Soft Computing</p>
                <p><strong>Paper TL;DR:</strong> LAnoBERT is proposed, a parser free system log anomaly detection method that uses the BERT model, exhibiting excellent natural language processing performance and yields a higher anomaly detection performance compared to unsupervised learning-based benchmark models.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5736.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5736.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAnoBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LAnoBERT (Log Anomaly detection based on BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised, parser-free log anomaly detection framework that trains a BERT masked language model on normal log sequences and scores test sequences using per-token MLM loss and per-token predictive probability aggregated by top-k; includes an inference dictionary to avoid repeated computation of duplicated log sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (custom-trained for logs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer encoder (BERT) trained from scratch (or optionally task-adapted from a natural-language pretrained BERT) on tokenized log-key sequences using Masked Language Modeling (MLM). Tokenization used WordPiece with a tokenizer trained on the target log corpus. NSP objective was not used. During test, each token is masked in turn to compute MLM loss and predicted-token probability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Unsupervised scoring using MLM: compute per-mask cross-entropy (mask loss) and highest predictive probability for each masked token; aggregate top-k highest losses or lowest probabilities across tokens to produce a per-sequence anomaly score; uses a dictionary of seen sequences to cache inference.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Ordered event sequences (system logs / log-key sequences), i.e., sequence data (structured/semi-structured logs), not tabular</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Contextual/semantic sequence anomalies (events whose surrounding context differs from learned normal context); also rare/unseen event patterns in sequences</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1 Score (threshold-dependent) and AUROC (threshold-independent). Best reported (using predictive probability scoring): HDFS F1=0.9645, AUROC=0.9901; BGL F1=0.8749, AUROC=0.9721; Thunderbird F1=0.9990, AUROC=0.9520. Predictive-loss scoring variant: HDFS F1=0.9123, BGL F1=0.6932, Thunderbird F1=0.5142.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to multiple supervised and unsupervised baselines (parser-based and parser-free). LAnoBERT (predictive probability) outperformed all unsupervised baselines and achieved comparable or superior results to some supervised, parser-based methods: e.g., it outperformed LogRobust and LogSy on some datasets, was slightly worse than HitAnomaly on BGL (difference ~0.0451 F1), and outperformed LogBERT on HDFS and Thunderbird but was lower than LogBERT on BGL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires training per system/dataset (not a single unified model). Transformer-based inference is costly (O(n^2) self-attention) though mitigated by caching repeated sequences. The mask-loss based score fails on long/complex sequences when shifted token predictions still preserve subsequence order (loss high even if partial match exists); predictive-probability scoring performed much better. Pretraining from natural-language BERT can help for complex logs but can hurt on simple vocabularies (e.g., HDFS) if mismatch exists.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5736.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5736.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogBERT: Log anomaly detection via BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based log anomaly detector that uses masked log key prediction (MLKP) and a Volume-of-Hypersphere-Minimization (VHM/DeepSVDD-like) objective to learn normal log representations and detect anomalies by checking whether observed keys are within the top-g predicted candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logbert: Log anomaly detection via bert.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT model applied to parser-preprocessed log templates; training tasks include Masked Log Key Prediction (MLKP; MLM analogue) and a one-class objective (VHM/DeepSVDD) to compact normal samples in representation space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Unsupervised / one-class style: masked-key prediction for candidate generation plus hypersphere minimization to represent normal data; at inference, check if observed token is within top-g predicted candidates (top-g candidate logic).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log-key sequences (parser-based templates)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequence anomalies / unexpected events (tokens not in predicted candidate set)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird (benchmarked in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1 reported in this paper (from LogBERT study): HDFS F1=0.8232, BGL F1=0.9083, Thunderbird F1=0.9664.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>LogBERT showed strong performance vs other unsupervised baselines (DeepLog, PCA, iForest, OCSVM, LogCluster, LogAnomaly) in prior work. In this paper, LAnoBERT's predictive-probability scoring outperformed LogBERT on HDFS and Thunderbird but was lower on BGL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Depends on a log parser (Drain) for template extraction; random masking strategy may limit contextual modeling of the whole sequence when masking arbitrary tokens; candidate-set (top-g) logic can be sensitive to g choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5736.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5736.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuralLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeuralLog (log-based anomaly detection without log parsing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parser-free, transformer-based classification approach that trains with both normal and abnormal logs (from target and other systems) and was evaluated using several pretrained backbones (BERT, GPT-2, RoBERTa), with BERT backbone reported highest performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Log-based anomaly detection without log parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer backbones (BERT / GPT-2 / RoBERTa evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based encoder/classifier applied to tokenized log sequences (parser-free) using supervised classification training with both normal and abnormal data; reported best results with BERT backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Supervised classification (parser-free) using transformer encoders trained on labeled normal and abnormal logs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log sequences (parser-free tokenized)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequence anomalies (classified as abnormal vs normal)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird (benchmarked in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1 scores reported in this paper (from NeuralLog): HDFS F1=0.98, BGL F1=0.98, Thunderbird F1=0.96.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>NeuralLog achieved very high F1s, outperforming some supervised parser-based baselines. However, it requires abnormal training data (less realistic for many real-world settings) which limits its practicality relative to unsupervised methods like LAnoBERT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on labeled abnormal data (including from target system), thus less realistic for deployment where anomalies are rare; supervised classification rather than one-class/unsupervised detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5736.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5736.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogSy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogSy (Self-attentive classification-based anomaly detection in unstructured logs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based parser-free model that performs classification by learning to separate normal logs (from target system) and abnormal logs (from other systems) using a distance-based loss to push representations apart.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-attentive classification-based anomaly detection in unstructured logs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (self-attentive)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder applied to tokenized log values (parser-free) with a distance-based loss to separate normal and cross-system abnormal samples; classification setup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Supervised classification / metric-learning style: trained with normal logs of target system and abnormal logs from different systems to enlarge inter-class distance.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log sequences (parser-free tokenized values)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequence anomalies (classification)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL, Thunderbird (reported here), HDFS not reported in original for this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1 reported here: BGL F1=0.65, Thunderbird F1=0.99 (values taken from original LogSy report as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>LogSy shows competitive supervised performance on some datasets; in this paper LAnoBERT (unsupervised) outperformed LogSy on HDFS and matched or exceeded some supervised baselines depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires abnormal examples (even if from other systems) and uses supervised objective; performance varies strongly by dataset; parser-free but still a classification formulation which can be unrealistic due to scarcity of anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5736.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5736.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HitAnomaly</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HitAnomaly (Hierarchical transformers for anomaly detection in system log)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised transformer-based anomaly detection model that uses a log parser (Drain) to extract templates and parameters, encodes template and parameter information separately, and combines them via attention for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hitanomaly: Hierarchical transformers for anomaly detection in system log.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (hierarchical)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based hierarchical encoder that takes two streams (log template and substituted parameters) produced by a log parser; trained as a supervised classifier on labeled normal and abnormal logs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Supervised classification (parser-based) combining template and parameter encodings via attention.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Parser-processed log templates + parameters (structured/semi-structured sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequence and parameter anomalies (classification)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL (reported here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1 reported here: HDFS F1=0.98, BGL F1=0.92.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Strong supervised performance; in this paper LAnoBERT was comparable on some datasets and slightly worse on BGL (LAnoBERT predictive-prob F1=0.8749 vs HitAnomaly 0.92).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on a log parser (parser-dependent) and supervised training with abnormal examples which may be unrealistic; parsing can lose parameter semantics if templates remove informative content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5736.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5736.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepLog: Anomaly detection and diagnosis from system logs through deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised LSTM-based next-event-prediction model that learns normal log template sequences (using a parser) and flags sequences as anomalous when the observed next template is not among the top-g predicted candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deeplog: Anomaly detection and diagnosis from system logs through deep learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LSTM-based sequential model trained on parser-extracted log templates to predict next template(s); uses top-g candidate mechanism for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Unsupervised next-event prediction: predict next template (top-g candidates); if actual next template not in top-g, mark anomaly.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Parser-extracted log template sequences (ordered sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequential anomalies / unexpected next events</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird (benchmarked here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1 reported here: HDFS F1=0.7734, BGL F1=0.8612, Thunderbird F1=0.9308.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed several classical unsupervised baselines (PCA, iForest, OCSVM) and some other parser-based methods; LogBERT and LAnoBERT (predictive-prob) outperform DeepLog on most datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Parser-dependent (template extraction); next-event top-g heuristic sensitive to g; limited capacity for long-term context compared to transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5736.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5736.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogAnomaly</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogAnomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised, parser-based approach using attention-based LSTM and template2vec representation to detect both sequential and quantitative anomalies from logs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogAnomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Attention-based LSTM (LogAnomaly)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LSTM with attention using template2vec features from parsed logs; unsupervised detection of sequential and quantitative anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Unsupervised detection using sequence modeling and numeric/statistical anomaly scoring on features derived from parsed logs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Parser-processed log template sequences with quantitative features</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequential anomalies and quantitative anomalies (value outliers/count anomalies)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Thunderbird (benchmarked here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1 reported here: HDFS F1=0.5619, BGL F1=0.7409, Thunderbird F1=0.9273.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Lower than DeepLog and LogBERT on most datasets in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Parser-dependent; lower performance on some datasets; formulation focuses on both sequential and numeric anomalies which may dilute sequence-only detection power.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logbert: Log anomaly detection via bert. <em>(Rating: 2)</em></li>
                <li>Deeplog: Anomaly detection and diagnosis from system logs through deep learning. <em>(Rating: 2)</em></li>
                <li>Log-based anomaly detection without log parsing. <em>(Rating: 2)</em></li>
                <li>Self-attentive classification-based anomaly detection in unstructured logs. <em>(Rating: 2)</em></li>
                <li>Hitanomaly: Hierarchical transformers for anomaly detection in system log. <em>(Rating: 2)</em></li>
                <li>Robust log-based anomaly detection on unstable log data. <em>(Rating: 2)</em></li>
                <li>BERT: Pre-training of deep bidirectional transformers for language understanding. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5736",
    "paper_id": "paper-f4b73a1b1de1cbea715e1ed404c0626eac53d976",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "LAnoBERT",
            "name_full": "LAnoBERT (Log Anomaly detection based on BERT)",
            "brief_description": "An unsupervised, parser-free log anomaly detection framework that trains a BERT masked language model on normal log sequences and scores test sequences using per-token MLM loss and per-token predictive probability aggregated by top-k; includes an inference dictionary to avoid repeated computation of duplicated log sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT (custom-trained for logs)",
            "model_description": "A transformer encoder (BERT) trained from scratch (or optionally task-adapted from a natural-language pretrained BERT) on tokenized log-key sequences using Masked Language Modeling (MLM). Tokenization used WordPiece with a tokenizer trained on the target log corpus. NSP objective was not used. During test, each token is masked in turn to compute MLM loss and predicted-token probability.",
            "model_size": null,
            "anomaly_detection_method": "Unsupervised scoring using MLM: compute per-mask cross-entropy (mask loss) and highest predictive probability for each masked token; aggregate top-k highest losses or lowest probabilities across tokens to produce a per-sequence anomaly score; uses a dictionary of seen sequences to cache inference.",
            "data_type": "Ordered event sequences (system logs / log-key sequences), i.e., sequence data (structured/semi-structured logs), not tabular",
            "anomaly_type": "Contextual/semantic sequence anomalies (events whose surrounding context differs from learned normal context); also rare/unseen event patterns in sequences",
            "dataset_name": "HDFS, BGL, Thunderbird",
            "performance_metrics": "F1 Score (threshold-dependent) and AUROC (threshold-independent). Best reported (using predictive probability scoring): HDFS F1=0.9645, AUROC=0.9901; BGL F1=0.8749, AUROC=0.9721; Thunderbird F1=0.9990, AUROC=0.9520. Predictive-loss scoring variant: HDFS F1=0.9123, BGL F1=0.6932, Thunderbird F1=0.5142.",
            "baseline_comparison": "Compared to multiple supervised and unsupervised baselines (parser-based and parser-free). LAnoBERT (predictive probability) outperformed all unsupervised baselines and achieved comparable or superior results to some supervised, parser-based methods: e.g., it outperformed LogRobust and LogSy on some datasets, was slightly worse than HitAnomaly on BGL (difference ~0.0451 F1), and outperformed LogBERT on HDFS and Thunderbird but was lower than LogBERT on BGL.",
            "limitations_or_failure_cases": "Requires training per system/dataset (not a single unified model). Transformer-based inference is costly (O(n^2) self-attention) though mitigated by caching repeated sequences. The mask-loss based score fails on long/complex sequences when shifted token predictions still preserve subsequence order (loss high even if partial match exists); predictive-probability scoring performed much better. Pretraining from natural-language BERT can help for complex logs but can hurt on simple vocabularies (e.g., HDFS) if mismatch exists.",
            "uuid": "e5736.0",
            "source_info": {
                "paper_title": "LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "LogBERT",
            "name_full": "LogBERT: Log anomaly detection via BERT",
            "brief_description": "A BERT-based log anomaly detector that uses masked log key prediction (MLKP) and a Volume-of-Hypersphere-Minimization (VHM/DeepSVDD-like) objective to learn normal log representations and detect anomalies by checking whether observed keys are within the top-g predicted candidates.",
            "citation_title": "Logbert: Log anomaly detection via bert.",
            "mention_or_use": "mention",
            "model_name": "BERT",
            "model_description": "BERT model applied to parser-preprocessed log templates; training tasks include Masked Log Key Prediction (MLKP; MLM analogue) and a one-class objective (VHM/DeepSVDD) to compact normal samples in representation space.",
            "model_size": null,
            "anomaly_detection_method": "Unsupervised / one-class style: masked-key prediction for candidate generation plus hypersphere minimization to represent normal data; at inference, check if observed token is within top-g predicted candidates (top-g candidate logic).",
            "data_type": "Log-key sequences (parser-based templates)",
            "anomaly_type": "Sequence anomalies / unexpected events (tokens not in predicted candidate set)",
            "dataset_name": "HDFS, BGL, Thunderbird (benchmarked in this paper)",
            "performance_metrics": "F1 reported in this paper (from LogBERT study): HDFS F1=0.8232, BGL F1=0.9083, Thunderbird F1=0.9664.",
            "baseline_comparison": "LogBERT showed strong performance vs other unsupervised baselines (DeepLog, PCA, iForest, OCSVM, LogCluster, LogAnomaly) in prior work. In this paper, LAnoBERT's predictive-probability scoring outperformed LogBERT on HDFS and Thunderbird but was lower on BGL.",
            "limitations_or_failure_cases": "Depends on a log parser (Drain) for template extraction; random masking strategy may limit contextual modeling of the whole sequence when masking arbitrary tokens; candidate-set (top-g) logic can be sensitive to g choice.",
            "uuid": "e5736.1",
            "source_info": {
                "paper_title": "LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "NeuralLog",
            "name_full": "NeuralLog (log-based anomaly detection without log parsing)",
            "brief_description": "A parser-free, transformer-based classification approach that trains with both normal and abnormal logs (from target and other systems) and was evaluated using several pretrained backbones (BERT, GPT-2, RoBERTa), with BERT backbone reported highest performance.",
            "citation_title": "Log-based anomaly detection without log parsing.",
            "mention_or_use": "mention",
            "model_name": "Transformer backbones (BERT / GPT-2 / RoBERTa evaluated)",
            "model_description": "Transformer-based encoder/classifier applied to tokenized log sequences (parser-free) using supervised classification training with both normal and abnormal data; reported best results with BERT backbone.",
            "model_size": null,
            "anomaly_detection_method": "Supervised classification (parser-free) using transformer encoders trained on labeled normal and abnormal logs.",
            "data_type": "Log sequences (parser-free tokenized)",
            "anomaly_type": "Sequence anomalies (classified as abnormal vs normal)",
            "dataset_name": "HDFS, BGL, Thunderbird (benchmarked in this paper)",
            "performance_metrics": "F1 scores reported in this paper (from NeuralLog): HDFS F1=0.98, BGL F1=0.98, Thunderbird F1=0.96.",
            "baseline_comparison": "NeuralLog achieved very high F1s, outperforming some supervised parser-based baselines. However, it requires abnormal training data (less realistic for many real-world settings) which limits its practicality relative to unsupervised methods like LAnoBERT.",
            "limitations_or_failure_cases": "Relies on labeled abnormal data (including from target system), thus less realistic for deployment where anomalies are rare; supervised classification rather than one-class/unsupervised detection.",
            "uuid": "e5736.2",
            "source_info": {
                "paper_title": "LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "LogSy",
            "name_full": "LogSy (Self-attentive classification-based anomaly detection in unstructured logs)",
            "brief_description": "A transformer-based parser-free model that performs classification by learning to separate normal logs (from target system) and abnormal logs (from other systems) using a distance-based loss to push representations apart.",
            "citation_title": "Self-attentive classification-based anomaly detection in unstructured logs.",
            "mention_or_use": "mention",
            "model_name": "Transformer (self-attentive)",
            "model_description": "Transformer encoder applied to tokenized log values (parser-free) with a distance-based loss to separate normal and cross-system abnormal samples; classification setup.",
            "model_size": null,
            "anomaly_detection_method": "Supervised classification / metric-learning style: trained with normal logs of target system and abnormal logs from different systems to enlarge inter-class distance.",
            "data_type": "Log sequences (parser-free tokenized values)",
            "anomaly_type": "Sequence anomalies (classification)",
            "dataset_name": "BGL, Thunderbird (reported here), HDFS not reported in original for this paper",
            "performance_metrics": "F1 reported here: BGL F1=0.65, Thunderbird F1=0.99 (values taken from original LogSy report as cited).",
            "baseline_comparison": "LogSy shows competitive supervised performance on some datasets; in this paper LAnoBERT (unsupervised) outperformed LogSy on HDFS and matched or exceeded some supervised baselines depending on dataset.",
            "limitations_or_failure_cases": "Requires abnormal examples (even if from other systems) and uses supervised objective; performance varies strongly by dataset; parser-free but still a classification formulation which can be unrealistic due to scarcity of anomalies.",
            "uuid": "e5736.3",
            "source_info": {
                "paper_title": "LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "HitAnomaly",
            "name_full": "HitAnomaly (Hierarchical transformers for anomaly detection in system log)",
            "brief_description": "A supervised transformer-based anomaly detection model that uses a log parser (Drain) to extract templates and parameters, encodes template and parameter information separately, and combines them via attention for classification.",
            "citation_title": "Hitanomaly: Hierarchical transformers for anomaly detection in system log.",
            "mention_or_use": "mention",
            "model_name": "Transformer (hierarchical)",
            "model_description": "Transformer-based hierarchical encoder that takes two streams (log template and substituted parameters) produced by a log parser; trained as a supervised classifier on labeled normal and abnormal logs.",
            "model_size": null,
            "anomaly_detection_method": "Supervised classification (parser-based) combining template and parameter encodings via attention.",
            "data_type": "Parser-processed log templates + parameters (structured/semi-structured sequences)",
            "anomaly_type": "Sequence and parameter anomalies (classification)",
            "dataset_name": "HDFS, BGL (reported here)",
            "performance_metrics": "F1 reported here: HDFS F1=0.98, BGL F1=0.92.",
            "baseline_comparison": "Strong supervised performance; in this paper LAnoBERT was comparable on some datasets and slightly worse on BGL (LAnoBERT predictive-prob F1=0.8749 vs HitAnomaly 0.92).",
            "limitations_or_failure_cases": "Relies on a log parser (parser-dependent) and supervised training with abnormal examples which may be unrealistic; parsing can lose parameter semantics if templates remove informative content.",
            "uuid": "e5736.4",
            "source_info": {
                "paper_title": "LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "DeepLog",
            "name_full": "DeepLog: Anomaly detection and diagnosis from system logs through deep learning",
            "brief_description": "An unsupervised LSTM-based next-event-prediction model that learns normal log template sequences (using a parser) and flags sequences as anomalous when the observed next template is not among the top-g predicted candidates.",
            "citation_title": "Deeplog: Anomaly detection and diagnosis from system logs through deep learning.",
            "mention_or_use": "mention",
            "model_name": "LSTM (sequence model)",
            "model_description": "LSTM-based sequential model trained on parser-extracted log templates to predict next template(s); uses top-g candidate mechanism for anomaly detection.",
            "model_size": null,
            "anomaly_detection_method": "Unsupervised next-event prediction: predict next template (top-g candidates); if actual next template not in top-g, mark anomaly.",
            "data_type": "Parser-extracted log template sequences (ordered sequences)",
            "anomaly_type": "Sequential anomalies / unexpected next events",
            "dataset_name": "HDFS, BGL, Thunderbird (benchmarked here)",
            "performance_metrics": "F1 reported here: HDFS F1=0.7734, BGL F1=0.8612, Thunderbird F1=0.9308.",
            "baseline_comparison": "Outperformed several classical unsupervised baselines (PCA, iForest, OCSVM) and some other parser-based methods; LogBERT and LAnoBERT (predictive-prob) outperform DeepLog on most datasets.",
            "limitations_or_failure_cases": "Parser-dependent (template extraction); next-event top-g heuristic sensitive to g; limited capacity for long-term context compared to transformers.",
            "uuid": "e5736.5",
            "source_info": {
                "paper_title": "LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "LogAnomaly",
            "name_full": "LogAnomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs",
            "brief_description": "An unsupervised, parser-based approach using attention-based LSTM and template2vec representation to detect both sequential and quantitative anomalies from logs.",
            "citation_title": "LogAnomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs.",
            "mention_or_use": "mention",
            "model_name": "Attention-based LSTM (LogAnomaly)",
            "model_description": "LSTM with attention using template2vec features from parsed logs; unsupervised detection of sequential and quantitative anomalies.",
            "model_size": null,
            "anomaly_detection_method": "Unsupervised detection using sequence modeling and numeric/statistical anomaly scoring on features derived from parsed logs.",
            "data_type": "Parser-processed log template sequences with quantitative features",
            "anomaly_type": "Sequential anomalies and quantitative anomalies (value outliers/count anomalies)",
            "dataset_name": "HDFS, BGL, Thunderbird (benchmarked here)",
            "performance_metrics": "F1 reported here: HDFS F1=0.5619, BGL F1=0.7409, Thunderbird F1=0.9273.",
            "baseline_comparison": "Lower than DeepLog and LogBERT on most datasets in this paper's comparisons.",
            "limitations_or_failure_cases": "Parser-dependent; lower performance on some datasets; formulation focuses on both sequential and numeric anomalies which may dilute sequence-only detection power.",
            "uuid": "e5736.6",
            "source_info": {
                "paper_title": "LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model",
                "publication_date_yy_mm": "2021-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logbert: Log anomaly detection via bert.",
            "rating": 2
        },
        {
            "paper_title": "Deeplog: Anomaly detection and diagnosis from system logs through deep learning.",
            "rating": 2
        },
        {
            "paper_title": "Log-based anomaly detection without log parsing.",
            "rating": 2
        },
        {
            "paper_title": "Self-attentive classification-based anomaly detection in unstructured logs.",
            "rating": 2
        },
        {
            "paper_title": "Hitanomaly: Hierarchical transformers for anomaly detection in system log.",
            "rating": 2
        },
        {
            "paper_title": "Robust log-based anomaly detection on unstable log data.",
            "rating": 2
        },
        {
            "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
            "rating": 1
        }
    ],
    "cost": 0.015262249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model</h1>
<p>Yukyung Lee ${ }^{1}$ Jina Kim ${ }^{1}$ Pilsung Kang ${ }^{1}$</p>
<h4>Abstract</h4>
<p>The system log generated in a computer system refers to large-scale data that are collected simultaneously and used as the basic data for determining errors, intrusion and abnormal behaviors. The aim of system log anomaly detection is to promptly identify anomalies while minimizing human intervention, which is a critical problem in the industry. Previous studies performed anomaly detection through algorithms after converting various forms of log data into a standardized template using a parser. Particularly, a template corresponding to a specific event should be defined in advance for all the log data using which the information within the log key may get lost. In this study, we propose LAnoBERT, a parser free system log anomaly detection method that uses the BERT model, exhibiting excellent natural language processing performance. The proposed method, LAnoBERT, learns the model through masked language modeling, which is a BERTbased pre-training method, and proceeds with unsupervised learning-based anomaly detection using the masked language modeling loss function per log key during the test process. In addition, we also propose an efficient inference process to establish a practically applicable pipeline to the actual system. Experiments on three well-known log datasets, i.e., HDFS, BGL, and Thunderbird, show that not only did LAnoBERT yield a higher anomaly detection performance compared to unsupervised learning-based benchmark models, but also it resulted in a comparable performance with supervised learning-based benchmark models.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. Introduction</h2>
<p>Owing to the recent advancement of the IT industry, a growing emphasis is placed on the importance of the system log data for identifying problems when accidents or failures occur in programs (He et al., 2017). The system log comprises large-scale data collected simultaneously in a computer system and used as the basic data for determining anomalies; thus it is a very critical and valuable resource. The log data generated from various systems should be monitored in realtime for system stability because they represent the current status of a system. Real-time monitoring is conventionally performed by operators; however, such a method entails the possibility of including errors and bias depending on the operator and is limited by being unable to promptly detect system anomalies (Simache \&amp; Kaaniche, 2005). Subsequently, anomaly detection using rule-based algorithms has been proposed to reduce human error (Cinque et al., 2013). However, rule-based methodologies also require human intervention; therefore, research is being actively conducted on real-time monitoring-based anomaly detection methods based on machine learning, which minimizes human intervention (Du et al., 2017).</p>
<p>Log data are sequence data collected in real-time. They consist of a combination of log keys, which can be considered as words, whereas log sequences can be considered as sentences; a log sequence is generated through a series of syntax rules (Du \&amp; Li, 2016). Also, since log data is accumulated based on user actions at regular time intervals, there are many duplicates in an actual log history. Hence, although the total amount of log instances is very large, a single log sequence is short and the number of unique log keys are limited in general.</p>
<p>Machine learning-based log anomaly detection involves three steps: 1) preprocessing log keys, 2) feature embedding, and 3) anomaly detection. Preprocessing of log keys refers to refining unstructured log keys and can be performed with or without a log parser. Parsing-based log anomaly detection involves generating log data in a standardized template format using a log parser. Feature embedding involves extracting features from preprocessed log sequences. Recent methods (Nedelkoski et al., 2020) use transformer-based models, whereas earlier methods used RNNs to treat log</p>
<p>sequences as natural language (Brown et al., 2018; Du et al., 2017; Kim et al., 2016). Anomaly detection involves finding abnormal logs using the extracted features.</p>
<p>Previous log anomaly detection studies (Du et al., 2017; Zhang et al., 2019; Huang et al., 2020; Nedelkoski et al., 2020) showed remarkable performance on open datasets, but have limitations in terms of practicality and extensibility.</p>
<ul>
<li>Reliance on Log Parsers: Parser-based log preprocessing requires predefined templates for standardizing log keys and manual refinement by experts (Du \&amp; Li, 2016). This method may result in loss of crucial information during standardization (Huang et al., 2020) and its performance becomes dependent on log parser compatibility rather than the logic of anomaly detection models (Nedelkoski et al., 2020).</li>
<li>Feature embedding with rich semantics for longterm dependency: In the field of log anomaly detection, previous feature embedding methods primarily utilized RNN-based algorithms, which have been successful in natural language processing (Vaswani et al., 2017). However, these algorithms have difficulties in handling long sequences, particularly with regard to modeling long-term dependencies. To address these limitations, current research is exploring the use of transformer-based architecture (Nedelkoski et al., 2020), which has recently demonstrated exceptional performance in natural language processing (Vaswani et al., 2017).</li>
<li>Unrealistic problem formulation: In prior research, log anomaly detection was mostly formulated as a binary classification instead of anomaly detection (Huang et al., 2020; Nedelkoski et al., 2020; Zhang et al., 2019). However, in practical systems, the majority of logs are normal, with only a small amount of abnormal logs. Formulating log anomaly detection as a binary classification problem requires a sufficient amount of abnormal data for model training, which is unrealistic as abnormal data is rare in real-world systems. Hence, a more practical approach is to train the model using only normal log data and then utilize abnormal data only during testing, better reflecting real-world scenarios.</li>
</ul>
<p>As a solution for the aforementioned problem, this study proposes a new log anomaly detection model (LAnoBERT; Log Anomaly detection based on BERT) established on the following three improvement plans. In LAnoBERT, a simple preprocessing approach utilizing regular expressions was selected to mitigate information loss during the parsing process and to minimize dependence on a specific log parser. Contextualized embedding was extracted using the BERT
model, which was trained from scratch to learn the log key sequences, in contrast to previous models which relied on static embedding for feature extraction. Lastly, unsupervised learning-based anomaly detection was performed under the assumption that the context of normal logs differs from that of abnormal logs. In the proposed model, LAnoBERT, masked language modeling of BERT (Devlin et al., 2019) was utilized to perform anomaly detection based on the masking predictive probability. An efficient inference process was also proposed, where a log dictionary database was defined, and log key matching was performed for anomaly detection. The model demonstrated superior performance compared to previous models on benchmark datasets of system logs (HDFS, BGL, and Thunderbird). It showed the best performance among unsupervised learning-based models and comparable performance to supervised learningbased models, despite being trained in a less advantageous environment. LAnoBERT satisfied both detection performance and practicality by outperforming some supervised learning-based models.</p>
<p>In summary, the main contributions of our study are as follows.</p>
<ul>
<li>We propose LAnoBERT, a new BERT-based unsupervised and log parser-free anomaly detection framework for log data. Unlike previous studies, it is a log parserfree and unsupervised learning-based model.</li>
<li>To improve efficiency, an inference process utilizing a log dictionary database is proposed to identify abnormal logs. This reduces the computational burden of BERT and handles log sequences with lots of redundant information.</li>
<li>Despite being trained under less favorable conditions, LAnoBERT demonstrated better or comparable performance to supervised learning-based models. In addition, LAnoBERT effectively detects anomalies in various types of logs, validating its practical usefulness.</li>
</ul>
<p>This paper is organized as follows. In Section 2, previous studies are reviewed by categorizing them based on neural networks with parsing and free of parsing. In Section 3, the background knowledge related to the research is introduced in addition to the log parser and BERT model. Section 4 explains the proposed model, LAnoBERT, and its structure, whereas Section 5 describes the experimental design, and Section 6 describes the log anomaly detection performance. Lastly, in Section 7, the conclusion of this study and future research subjects are explained.</p>
<h2>2. Related Work</h2>
<p>Log anomaly detection refers to a method for detecting abnormal logs from a large log dataset. Studies in earlier years (Cinque et al., 2013; Hansen \&amp; Atkins, 1993; Oprea et al., 2015; Prewett, 2003; Yen et al., 2013) performed anomaly detection by regarding specific parts of log data as abnormal. However, these studies had a critical limitation of requiring professional domain-specific knowledge. Recent methodologies involve extracting log data features using a neural network-based model and performing anomaly detection. Log data are unstructured data with a highly complex structure; thus, log anomaly detection can be divided into parsing-based or parsing-free depending on the preprocessing method for the log data.</p>
<h3>2.1. Parsing-based log anomaly detection</h3>
<p>In these methods, a log parser is needed to perform log anomaly detection. The Drain parser(He et al., 2017), which is the most commonly used parser, classifies log messages based on the length using a decision tree and then allocates a log template by exploring the word similarity. In this process, the similarity between the new log message and the existing log template is calculated, and a new template is allocated for the data with a different format from the existing template.</p>
<p>DeepLog (Du et al., 2017) is the first neural network-based log anomaly detection model in which an anomaly is detected using an unsupervised learning-based LSTM model. In the training phase, a log is generated in a standardized template using the Drain parser and then a normal log template pattern is learned. In the test phase, logs having a pattern not trained with the normal data are determined as anomalies. In other words, log patterns with low frequencies are given low scores. The concept of the 'top g candidate' is introduced to discern log patterns where it is regarded as normal if the log patterns are present within the candidate or abnormal if not present; thus, the performance varies depending on the candidate.</p>
<p>LogRobust (Zhang et al., 2019) is an attention-based biLSTM model for detecting anomalies. After creating a log with a standardized template using the Drain parser, the log data features are extracted by generating TF-IDF and a word semantic vector. Because this particular model detects anomalies based on classification, it can be considered as a classification problem.</p>
<p>HitAnomaly (Huang et al., 2020) is an anomaly detection model using a transformer. It also conducts preprocessing through the Drain parser. A template is standardized through a log parser and the information substituted with a template is defined as parameters. The substituted information refers to the data that get lost without inclusion in the template.</p>
<p>Two types of information are separately encoded using a transformer encoder, and two types of representation are combined based on attention to detect anomalies through classification. This model also performs classification-based anomaly detection, thus entailing limitations.</p>
<p>LogBERT (Guo et al., 2021) is a BERT-based framework for log data anomaly detection, utilizing a Drain parser for log sequence refinement. It follows a similar approach to DeepLog in detecting outliers but instead trains using only normal log data through two tasks. The first task, masked log key prediction (MLKP), trains normal log patterns via the same objective function as masked language modeling. The second task, Volume of Hypersphere Minimization (VHM), aims to find the smallest sphere that contains normal logs. In the inference stage, the top $g$ predicted log keys are selected as a candidate set from a randomly masked normal log sequence, and the observed log key is considered as an anomaly if it does not belong to the candidate set. The model detects anomalies by applying BERT's masked language modeling, however, it has a limitation in that it cannot fully consider the log sequence when masking due to the random selection of a log key from the sequence.</p>
<p>In this study, we propose a log anomaly detection model that does not depend on the log parser. Therefore, even when a new log sequence is recorded, data is not parsed using the log template, but the log sequence is refined using simple preprocessing logic. This method can preserve the log sequence as much as possible by minimizing information loss commonly occurred in the parsing process.</p>
<h3>2.2. Parsing-free log anomaly detection</h3>
<p>LogSy (Nedelkoski et al., 2020) is a transformer-based anomaly detection model that uses a tokenizer to preprocess log values; thus, it is free from the use of a log parser when detecting anomalies. In LogSy, classification is performed using normal data of a training log and abnormal log data generated from a different system. In addition, training is performed so that the distance between the normal and abnormal log increases through a distance-based loss function. It is different from LogRobust and HitAnomaly because it does not learn the normal and abnormal log generated in one system based on classification like the previous models. Hence, this model also entails various limitations to be used in the industry as it adapts a classification-based approach.</p>
<p>Additionally, NeuralLog (Le \&amp; Zhang, 2021) is also a parserfree and classification-based anomaly detection model. While NeuralLog shares a similar structure with LogSy, it distinguishes itself by employing both normal and abnormal data from the target system, as well as a separate system, during the training process. In contrast, LogSy addresses the classification problem by relying solely on normal data from the target system and abnormal data generated from a</p>
<p>different system.
The proposed model is also a log parser-free methodology. After refining the log through a simple preprocessing logic, the log sequence is segmented using a word-piece tokenizer. Through this, the log sequence is not categorized into one of the predefined templates, but the log sequence itself is used as an input of the anomaly detection model. Also, it can be flexibly applied even if a new log sequence that has not been processed ever.</p>
<h2>3. Background</h2>
<h3>3.1. Log parser for anomaly detection</h3>
<p>Log data are large-scale data collected in real-time, and raw log messages are usually unstructured because developers are allowed to write free-text log messages in the source code (He et al., 2017). Therefore, a log sequence is unstructured data that must be converted to structured data. A log parser (Du \&amp; Li, 2016; He et al., 2017) is a technique proposed for this process. When a standardized log template is generated from an actual log, highly complicated data are simply preprocessed for substitution with very few events. For example, 4,747,964 log messages generated from the BGL system are converted to 376 events through the Drain parser (He et al., 2017). Then, when anomaly detection is performed with preprocessed data, the anomalies can be detected through a simple process. However, the performance of log anomaly detection models using a parser becomes heavily dependent on a log parser (Nedelkoski et al., 2020).</p>
<h3>3.2. BERT</h3>
<p>BERT (Devlin et al., 2019) is a model consisting of a transformer (Vaswani et al., 2017) encoder, which achieved outstanding performance in various natural language processing tasks (Devlin et al., 2019). One of the major characteristics of BERT is that pre-training is performed using two unsupervised learning methods, which are masked language modeling (MLM) and next sentence prediction (NSP). MLM involves replacing certain tokens of an input sentence with '[MASK]' and predicting that they would appear in the corresponding position. NSP involves combining two sentences with the token '[SEP]' in between, and then predicting whether the two sentences are semantically connected through the '[CLS]' token positioned in the very front of the input sentence. These two tasks do not require labeled data as in a specific downstream task; thus, general-purpose knowledge can be sufficiently learned through pre-training using a massive unlabeled dataset (Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). BERT that has been pre-trained is being applied in fields using sequence data in addition to natural language processing; some of the examples include ProtTrans (Elnaggar et al., 2021) and ESM
(Rao et al., 2020).</p>
<h3>3.3. BERT for anomaly detection</h3>
<p>The system log can be deemed as sequence data because it is a dataset with an order. Therefore, previous methodologies applied the techniques used for natural language processing to extract the features of logs. The system log data encompass both log messages and natural language. In this study, we propose a BERT-based system log anomaly detection system to overcome the limitations of existing methodologies. Previous methodologies treated all log data as sequence data, but applying BERT enables the learning of both the log features and natural language. Moreover, a tokenizer of BERT can be applied without using a separate log parser during which natural language data that are lost while converting to a template using a log parser can be preserved. Additionally, a model capable of capturing the semantics and context of the system log is necessary for accurately detecting abnormal logs in the system log. It is crucial to capture the semantics and context of the system log because the words appearing in the system log may have a different meaning from natural language. The goal of this research is to implement an effective pre-training approach for the system log utilizing masked language modeling in a bi-directional context. Additionally, we present a novel framework for identifying context anomalies by means of the trained models' MLM loss and predictive probability, along with a log key matching technique during the inference stage.</p>
<h2>4. Proposed Method</h2>
<p>In this chapter, the major network used in the proposed methodology and the architecture of the proposed model are explained. The description and significance of the MLM of BERT are presented in Section 4.1, and the training purpose and execution procedure of the proposed model are presented in Section 4.2 and 4.3, respectively.</p>
<h3>4.1. Masked Language Model</h3>
<p>The operation mechanism of LAnoBERT proposed in this study is shown in Figure 1. Because LAnoBERT is executed through MLM, which is a pre-training method of BERT, MLM is explained in detail in this section, and the log anomaly detection procedure is explained in depth in Section 4.2.</p>
<p>MLM was inspired by the cloze task (Taylor, 1953) where certain tokens of an input sentence are replaced with [MASK] and then the words in the [MASK] tokens are predicted. Entire sentences are replaced with the [MASK] token at an arbitrary probability of $15 \%$, and appropriate words can be predicted only based on the context. Particu-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The architecture of LAnoBERT.
larly, the MLM objective function can generate bidirectional representations, unlike the pre-training of left-to-right language models. Therefore, the proposed method can pre-train the deep bidirectional transformer (Devlin et al., 2019).</p>
<p>According to XLNet (Yang et al., 2019), MLM can be defined as an auto-encoding (AE) pre-training object. When $n$ is the sequence length and the $i$-th token is $x_{i}$, the given input sequence can be expressed as $X=\left[x_{1}, x_{2}, \ldots, x_{n}\right]$. If the [MASK] token is defined as $\bar{x}=[M A S K]$, the input sequence containing noise can be expressed as $\hat{X}=$ $\left[x_{1},[M A S K], \ldots, x_{n}\right]$. In BERT, specific tokens are substituted with the special token [MASK] at a pre-determined probability (15\%). Here, the likelihood and objective function can be expressed as follows.</p>
<p>$$
p(\bar{X} \mid \hat{X}) \approx \prod_{n=1}^{N} p\left(x_{n} \mid \hat{X}\right)
$$</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Max}<em n="1">{\theta} \log p(\bar{x} \mid \hat{X}) \
&amp; \approx \sum</em>\right) \
&amp; =\sum_{n=1}^{N} m_{n} \log \frac{\exp \left(H_{\theta}\left((\hat{x})}^{N} m_{n} \log p_{\theta}\left(x_{n} \mid \hat{X<em n="n">{n}^{\top} e\left(x</em>)}\right)\right)\right.}{\sum_{x^{\prime}} \exp \left(H_{\theta}\left((\hat{x<em _theta="\theta">{n}^{\top} e(x \prime)\right)\right.} p</em>\right)
\end{aligned}
$$}\left(x_{n} \mid \hat{X</p>
<p>In Eq. (2), $m_{n}$ indicates masking, where $x_{n}$ is the [MASK] token when $m_{n}=1$. Furthermore, $H_{\theta}$ indicates the hidden vector of a transformer encoder.</p>
<p>In this study, MLM was not only applied in the training phase but also in anomaly detection to detect abnormal logs. The reasons for using MLM in system log anomaly detection
are as follows. First, there is ample data available for training BERT because the log data are collected in real-time. Most of the collected data are normal log data, which facilitates the effective pre-training of BERT. When a sufficient number of data is given, BERT can obtain numerous contextual and structural features during pre-training. Therefore, performing anomaly detection using the proposed model is expected to improve the generalization performance of effectively detecting abnormal logs by adequately learning the features of a normal log system. Second, MLM does not require the labeling of tasks and accords with the purpose of anomaly detection where only normal data are used for training. Because anomaly detection is an unsupervised learning-based methodology where only normal data are used for training, it is appropriate for application to cases where normal data are predominantly greater than abnormal data. Since anomaly detection is an unsupervised learning-based approach that does not use label information during the model training, it is more appropriate than a supervised binary classification-based approach where there is an overwhelming amount of normal data. Third, MLM is an appropriate methodology to apply to anomaly detection from the perspective of prompt-based learning (Raffel et al., 2020; Petroni et al., 2019; Liu et al., 2021; Radford et al., 2019; Schick \&amp; Schtze, 2021). In contrast to conventional methods that require layers conforming to tasks to perform downstream tasks, it is suitable for finding patterns of log data in anomaly detection by comparing the actual log keys and the generated log keys. Fourth, the context of abnormal log data can be identified if MLM is performed using only normal log data. Normal log data have a very similar form as abnormal log data, but the probability of certain words appearing varies if the context of surrounding words is considered. It was assumed that the MLM predictive probability of abnormal log data is low when anomaly detection is performed using the BERT model trained only with normal log</p>
<p>data, and the performance result was relevant.</p>
<h3>4.2. Problem Definition</h3>
<p>The system log anomaly detection in this study can be defined as follows. When $s_{l e n}$ is the sequence length, the log key of the $i$-th token is $w_{i}\left(w_{i}: w_{i} \in \mathbb{V}, i=1,2, \ldots, s_{l e n}\right)$, and an individual log sequence is $l=\left(w_{1}, w_{2}, \ldots, w_{s_{l e n}}\right)$. Also mask token is defined $\hat{l}=[M A S K]$. The goal of the proposed model $f$ is to determine whether the input log sequence is a normal or abnormal log. The log sequence used during training consists of up to $s_{l e n}$ number of log keys, and a unique set of log keys is defined as $\mathbb{V}$. The log sequences used for training are all normal logs.</p>
<p>Input Representation The input log sequence is defined as $l$, and the BERT model is used. Accordingly, the input data used in the train and test phases are configured as follows.</p>
<ul>
<li>Train phase : $[C L S], w_{1},[M A S K], w_{3}, \cdots,[S E P]$</li>
<li>Test phase : $[C L S], w_{1},[M A S K], w_{3}, \cdots,[S E P] *$ $s_{l e n}$-times</li>
</ul>
<p>In the train phase, the existing log keys were substituted with the [MASK] token at an arbitrary probability; masking was conducted in this study at $20 \%$. In the test phase, masking was not performed at an arbitrary probability; however, each log key was replaced with the [MASK] token when one log sequence was given to generate an $s_{l e n}$ number of data for the test.</p>
<p>Objective Function The objective function used for training is as follows, which is identical to Eq. (2). $m_{i}$ indicates masking, where $w_{i}$ is the [MASK] token when $m_{i}=1$. Furthermore, $H_{\theta}$ indicates the hidden vector of a transformer encoder.</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Max}<em i="1">{\theta} \log p(\hat{l} \mid \hat{l}) \
&amp; \approx \sum</em>\right) \
&amp; =\sum_{i=1}^{s_{l e n}} m_{i} \log \frac{\exp \left(H_{\theta}\left((\hat{l})}^{s_{l e n}} m_{i} \log p_{\theta}\left(w_{i} \mid \hat{l<em i="i">{i}^{\top} e\left(w</em>)}\right)\right)\right.}{\sum_{w^{\prime}} \exp \left(H_{\theta}\left((\hat{l<em _theta="\theta">{i}^{\top} e\left(w^{\prime}\right)\right)\right.} p</em>\right)
\end{aligned}
$$}\left(w_{i} \mid \hat{X</p>
<h3>4.3. LAnoBERT</h3>
<h3>4.3.1. OVERVIEW</h3>
<p>LAnoBERT proposed in this study can be largely divided into the following three parts: preprocessing, model training, and abnormal score computation.</p>
<p>First, the minimum preprocessing of a log sequence was performed in the preprocessing step. Numbers, IPs, and
dates are preprocessed, and information loss was minimized using regular expressions. An initialized BERT was used as the model. During the training process, MLM was performed using only normal logs, and masking was randomly performed at $20 \%$. The NSP objective function was not used in this study when training BERT. Recent studies have pointed out that the NSP objective function interferes with the performance improvement (Joshi et al., 2020; Lample \&amp; Conneau, 2019; Liu et al., 2019; Yang et al., 2019), and it was excluded as it was unnecessary in log anomaly detection. Abnormal scores were calculated from the BERT model, which had been trained using both normal and abnormal logs during test.</p>
<h3>4.3.2. Preprocessing</h3>
<p>Because this study adopted a log parser-free method, simple preprocessing is conducted using regular expressions. As shown in Figure 3, the original log is highly complicated, unstructured data. When the Drain parser is used (with a log parser), the parts defined as a template are excluded and eliminated, whereas certain parts are replaced with $\left({ }^{*}\right)$. Conversely, this study did not use a log parser and instead replaced the data with clear formats such as numbers, dates, and IPs with the words 'NUM' or 'IP'. Preprocessed log sequences were tokenized using the WordPiece (Wu et al., 2016) model used in BERT. The tokenizer for the log data was also trained from scratch to ensure that the vocabulary of the log data from each system could be learned. The training was performed only for the normal logs, and the tokenizer created in the training process was used as it was during the test.</p>
<h3>4.3.3. MODEL</h3>
<p>The proposed model LAnoBERT executed anomaly detection based on a BERT Masked language model. The most crucial assumption of this study is that "There is a difference between the context of a normal system log and that of an abnormal system log." In other words, language models trained only with normal log data are expected to exhibit significant errors and low predictive probability when they encounter the context of abnormal logs during the test. The prediction error defined in this study refers to a crossentropy loss that occurs between the label information and logit value generated when the model predicts [MASK] as a specific token. Additionally, the predictive probability is defined as a value with the highest probability among the words that can appear in the [MASK] token. When the probability of a predicted word is low, the respective context is considered difficult to find in the normal context and is identified as an anomaly. Therefore, the errors and predictive probability calculated in this process can be utilized in anomaly detection. The core assumption of this study is as shown in Figure 2. BERT, which is trained only with</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Anomaly score distribution difference between normal and abnormal log sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Original Log</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">081109</td>
<td style="text-align: center;">294005</td>
<td style="text-align: center;">35 INFO dfs.FSNamexystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251 .73 .220 -50010 is added to blk_7128370237687728475 size 67108864</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">With Log Parser (Drain)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.474 .474 .474 -50010 is added to blk_474,1719740</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Without Log Parser</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4. Test phase process of LAnoBERT. The proposed model calculates MASK Loss and Prob for each log key to detect anomalies in one log sequence and obtains anomaly scores by aggregating top k values.
logit value calculated in $\hat{l}<em i="i">{i}$ and the label. Additionally, the predictive probability of the $i$-th log sequence refers to the maximum predictive probability of the word that belongs to the [MASK] position as an answer in $\hat{l}</em>$ respectively, can be obtained for N number of log sequences. Top $k$ values are selected from the set of calculated prediction errors and predictive probabilities to computer the final abnormal score. The equations for computing the abnormal scores of a test log are shown in Eq. (4) and Eq. (5). The proposed methodology independently computes the abnormal score with regard to the prediction error and the prediction probability to detect anomalies.}$. The prediction error and predictive probability, named error, and prob $_{i</p>
<p>$$
\begin{aligned}
a b n o r m a l_{\text {error }} &amp; =\frac{1}{k} \sum_{i \in \text { Top }-k \text { indices }} \text { error }<em _prob="{prob" _text="\text">{i} \
a b n o r m a l</em>
\end{aligned}
$$}} &amp; =\frac{1}{k} \sum_{i \in \text { Top }-k \text { indices }} \text { prob }_{i</p>
<p>As mentioned in section 4.3.2, LAnoBERT assumed that the abnormal logs would have a large prediction error or a low prediction probability. The abnormal score calculated from each log key is aggregated through the average of the top $k$ values, which becomes the abnormal score of a log sequence. The larger the $a b n o r m a l_{\text {error }}$, the more likely abnormal a given log sequence, whereas the lower the $a b n o r m a l_{\text {prob }}$, the more likely abnormal the log sequence.</p>
<p>However, as shown in Figure 4, when calculating the abnormal score by LAnoBERT for all log sequences existing in the test dataset, the number of required computations becomes the total number of log sequences $\times$ the length of each log sequence. Therefore, if the above method is applied, the computational cost increases and becomes inefficient not sufficient to be applied in an actual system.</p>
<p>Inspired by the fact that information is accumulated very frequently and there are many duplicates in log data, we propose an efficient inference process by removing repeated computations for duplicated log sequences. Since a masked log sequence for $i^{t h}$ token is defined as ' $[C L S], w_{1}, w_{2}, \cdots, w_{i-1},[M A S K], w_{i+1}, \cdots, w_{s_{l e n}},[S E P]$ '. We build a log dictionary database with one log sequence as a key value for the inference process. In this database, the dictionary key is defined as a set of $K E Y=\left{k e y_{0}, k e y_{1}, k e y_{2}, \cdots, k e y_{n}\right}$. Each key has its corresponding abnormal $<em _prob="{prob" _text="\text">{\text {error }}$ and abnormal $</em>}}$ as values: DICT $=$ $\left{k e y_{1}:\left(a b n o r m a l_{\text {error }}, \text { abnormal <em 2="2">{\text {prob }}\right), k e y</em>}:\right.$ $\left(a b n o r m a l_{\text {error }}, \text { abnormal <em j="j">{\text {prob }}\right), \cdots, k e y</em>\right)}$. Whenever one log data arrives, the log key matching is performed. If the input key is not matched to any of the existing keys in the current log dictionary, the values for the new key are computed through inference, and then the log dictionary is updated. On the other hand, when the input key is matched to one of the log keys in the current dictionary, the stored values are extracted as the abnormal score without an actual inference process. The following process reduces the unnecessary time required for detecting anomalies by inference of duplicate logs multiple times. Therefore, it is effective because it can be applied in a realistic scenario and is expected to be effective in online anomaly detection settings. An example of this inference process is illustrated in Figure 5 and the algorithm of the entire process is shown in Algorithm 1.} \quad:$ $\left(a b n o r m a l_{\text {error }}, \text { abnormal }_{\text {prob }</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. Inference process for LAnoBERT</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Inference Process
// Definition
</code></pre></div>

<p>$K E Y$ : a set of log sequence keys, DICT: the dictionary of the log sequence keys,
$s_{l e n}$ : the sequence length of a $\log , d$ : embedding dimension, $\left{l_{n}\right}<em e="e" l="l" n="n">{n=1}^{N}$ : an individual log sequence $\left(\mathbb{R}^{s</em>\right)$,
LAnoBERT : the proposed model, TOPK : the top- $k$ aggregation functions} \times d</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// Initialization</span>
<span class="o">\</span><span class="p">(</span><span class="n">K</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">emptyset</span><span class="p">,</span><span class="w"> </span><span class="n">D</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="n">T</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">emptyset</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">=</span><span class="mi">0</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="n">Input</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="n">l_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="o">\</span><span class="p">)</span>
<span class="n">Output</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">abnormalloss</span><span class="p">,</span><span class="w"> </span><span class="n">abnormal</span><span class="w"> </span><span class="n">prob</span>
<span class="k">for</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="n">n</span><span class="p">=</span><span class="mi">1</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="n">N</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span><span class="c1">// log key matching</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="n">l_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">DICT</span><span class="w"> </span><span class="nb">then</span>
<span class="w">        </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">score</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="o">\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">loss</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">emptyset</span><span class="p">,</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">score</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="o">\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">prob</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">emptyset</span><span class="o">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="n">i</span><span class="p">=</span><span class="mi">1</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="n">s_</span><span class="p">{</span><span class="n">l</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="n">n</span><span class="p">}</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="k">do</span>
<span class="w">            </span><span class="o">\</span><span class="p">(</span><span class="n">l_</span><span class="p">{</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">}=</span><span class="o">\</span><span class="n">left</span><span class="o">\</span><span class="n">lceil</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="n">K</span><span class="o">\</span><span class="n">right</span><span class="o">\</span><span class="n">rceil</span><span class="o">\</span><span class="p">)</span>
<span class="w">            </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">loss</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="n">j</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">prob</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">LAnoBERT</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">l_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">            </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">score</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="o">\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">loss</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">score</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="o">\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">loss</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">\</span><span class="n">cup</span><span class="o">\</span><span class="n">left</span><span class="o">\</span><span class="p">{</span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">loss</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="o">\</span><span class="p">}</span><span class="o">\</span><span class="p">)</span>
<span class="w">            </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">score</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="o">\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">prob</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">score</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="o">\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">prob</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">\</span><span class="n">cup</span><span class="o">\</span><span class="n">left</span><span class="o">\</span><span class="p">{</span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">prob</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="o">\</span><span class="p">}</span><span class="o">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">end</span>
<span class="w">        </span><span class="n">abnormalloss</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">leftarrow</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">TOPK</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="n">right</span><span class="o">.\</span><span class="p">)</span><span class="w"> </span><span class="n">score</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">left</span><span class="p">.</span><span class="nb">_</span><span class="p">{</span><span class="o">\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">loss</span><span class="w"> </span><span class="p">}}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">        </span><span class="n">abnormal</span><span class="w"> </span><span class="n">prob</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">leftarrow</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">TOPK</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="n">right</span><span class="o">.\</span><span class="p">)</span><span class="w"> </span><span class="n">score</span><span class="w"> </span><span class="n">prob</span><span class="w"> </span><span class="o">\</span><span class="p">()</span><span class="o">\</span><span class="p">)</span>
<span class="w">        </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">key</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="n">j</span><span class="p">}=</span><span class="n">l_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="o">\</span><span class="p">)</span>
<span class="w">        </span><span class="o">\</span><span class="p">(</span><span class="n">K</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">\</span><span class="n">cup</span><span class="o">\</span><span class="n">left</span><span class="o">\</span><span class="p">{</span><span class="n">k</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="n">y_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="o">\</span><span class="p">}</span><span class="o">\</span><span class="p">)</span>
<span class="w">        </span><span class="o">\</span><span class="p">(</span><span class="n">D</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="n">T</span><span class="o">\</span><span class="n">left</span><span class="p">[</span><span class="n">k</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="n">y_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="p">]=</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="n">right</span><span class="o">.\</span><span class="p">)</span><span class="w"> </span><span class="n">abnormal</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">left</span><span class="p">.</span><span class="nb">_</span><span class="p">{</span><span class="o">\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">loss</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">abnormal</span><span class="p">}</span><span class="nb">_</span><span class="p">{</span><span class="o">\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">prob</span><span class="w"> </span><span class="p">}}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">        </span><span class="o">\</span><span class="p">(</span><span class="n">j</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="o">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">abnormalloss</span><span class="p">,</span><span class="w"> </span><span class="n">abnormal</span><span class="w"> </span><span class="n">prob</span>
<span class="w">    </span><span class="k">else</span>
<span class="w">        </span><span class="n">abnormalloss</span><span class="p">,</span><span class="w"> </span><span class="n">abnormal</span><span class="w"> </span><span class="n">prob</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">D</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="n">T</span><span class="o">\</span><span class="n">left</span><span class="p">[</span><span class="n">l_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="o">\</span><span class="n">right</span><span class="p">]</span><span class="o">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">abnormalloss</span><span class="p">,</span><span class="w"> </span><span class="n">abnormal</span><span class="w"> </span><span class="n">prob</span>
<span class="w">    </span><span class="k">end</span>
<span class="k">end</span>
</code></pre></div>

<h2>5. Experimental Setting</h2>
<h3>5.1. Datasets</h3>
<p>In this study, HDFS (Xu et al., 2009), BGL (Oliner \&amp; Stearley, 2007) Thunderbird (Oliner \&amp; Stearley, 2007) were used as the benchmark log datasets for a fair comparison with pre-</p>
<p>Table 1. Number of logs in each dataset used in LAnoBERT</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">HDFS</td>
<td style="text-align: center;">normal</td>
<td style="text-align: center;">8,712,418</td>
<td style="text-align: center;">2,463,201</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(446,578 blocks)</td>
<td style="text-align: center;">(128,483 blocks)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">abnormal</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">138,410</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(16,838 blocks)</td>
</tr>
<tr>
<td style="text-align: center;">BGL</td>
<td style="text-align: center;">normal</td>
<td style="text-align: center;">3,496,193</td>
<td style="text-align: center;">903,310</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">abnormal</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">348,460</td>
</tr>
<tr>
<td style="text-align: center;">Thunderbird</td>
<td style="text-align: center;">normal</td>
<td style="text-align: center;">166,371,162</td>
<td style="text-align: center;">41,592,791</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">abnormal</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3,248,239</td>
</tr>
</tbody>
</table>
<p>vious studies. The three datasets include answer labels, and the generalization performance of the system log anomaly detection model can be verified as the data are deduced from different systems. HDFS, which is the Hadoop Distributed File System, is log data generated from a private cloud environment where one log consists of multiple log sequences. BGL includes data that consist of logs generated by the Blue Gene/L supercomputer, where each individual log sequence is accompanied by a corresponding label indicating either a normal or an abnormal condition.Thunderbird dataset was obtained from the Thunderbird supercomputer system at Sandia National Laboratories (SNL) in Albuquerque. This dataset includes alert and non-alert messages that are identified by alert category tags. Among the three datasets used in this study, the HDFS dataset is considered to have a relatively simple architecture (Nedelkoski et al., 2020), while the Thunderbird dataset has the largest number of log messages. The distribution of normal and abnormal log sequences (or blocks) used in the training and test datasets is presented in Table 1.</p>
<h3>5.2. Benchmark Methods</h3>
<p>In this section, we present the benchmark models for comparison with LAnoBERT's performance among various log anomaly detection models. The benchmark models were selected based on the usage of a log parser and whether the learning was supervised or unsupervised. The selected models were LogRobust, HitAnomaly, LogSy, Principal Component Analysis (PCA) (Xu et al., 2009), One-Class SVM (OCSVM) (Schlkopf et al., 2001), Isolation Forest (iForest) (Liu et al., 2008), LogCluster (Lin et al., 2016), DeepLog,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6. Benchmark models and LAnoBERT for log anomaly detection.</p>
<p>LogAnomaly (Meng et al., 2019), and LogBERT. Figure 6 illustrates the following aspects of the deep learning-based baseline models: 1) structural differences among the models, 2) log data preprocessing method during training and testing, and 3) anomaly detection method.</p>
<ul>
<li>LogRobust is a supervised learning-based model that utilizes an attention-based bi-LSTM architecture. The model employs a specialized log parser to preprocess log data, generating a TF-IDF and word semantic vector to extract the features of the log data. For training, both normal and abnormal log data are used in solving a classification problem.</li>
<li>HitAnomaly is a supervised learning-based model that employs a transformer architecture. It utilizes a special-
ized log parser for log data to standardize templates and encode log information as parameters. The model combines two features of normal and abnormal log data with an attention mechanism to classify the data.</li>
<li>LogSy is a supervised learning-based anomaly detection model that utilizes a transformer architecture. It does not require the use of a log parser, as log values are preprocessed using a tokenizer (Nedelkoski et al., 2020). Both normal and abnormal log data are utilized in the model, with data generated from different systems and a distance-based loss function being employed.</li>
<li>NeuralLog is a transformer-based classification model that utilizes a tokenizer. NeuralLog has a similar struc-</li>
</ul>
<p>ture to LogSy, but differs in its utilization of both normal and abnormal data from the target system and a different system during the training process. This approach sets it apart from LogSy, which solves the classification problem by utilizing normal data from the target system and abnormal data generated from a different system. To evaluate the performance of NeuralLog, several popular backbone models, namely BERT, GPT2, and RoBERTa, were utilized. Among these models, BERT achieved the highest performance.</p>
<ul>
<li>PCA is a linear transformation technique that transforms a set of correlated variables into a set of uncorrelated variables, referred to as principal components. This method builds a counting matrix for log sequence frequency to detect anomalies, then reduces the original counting matrix into a low-dimensional space for the identification of abnormal sequences.</li>
<li>OCSVM is a widely adopted one-class classification model for log anomaly detection (Wang et al., 2004), utilizing only normal log data. The model is designed to identify the boundary that separates the majority of input data from the remainder, represented as a hyperplane that separates normal data from outliers.</li>
<li>iForest is a tree-based unsupervised learning algorithm that utilizes the isolation of observations that are distinct from the remainder of the input data. The approach employs the formation of an ensemble of decision trees, each of which partitions the data into smaller subsets.</li>
<li>LogCluster is a clustering method for detecting frequently occurring line patterns and abnormal events in textual event logs.</li>
<li>DeepLog is a deep learning-based unsupervised log anomaly detection model based on an LSTM architecture. A specialized log parser is used to generate the input values for the LSTM, and the model predicts the next word. If the next word appears in a trained pattern, it is classified as normal, otherwise, it is considered abnormal.</li>
<li>LogAnomaly is proposed as a solution for detecting anomalies in log streams. The model leverages attention-based LSTM architecture to consider log data as natural language sequences. To extract semantic information, the LogAnomaly model employs the template2vec technique on log templates. This enables the detection of both sequential and quantitative anomalies in log data.</li>
<li>LogBERT is BERT based anomaly detection model that employs MLM and DeepSVDD (Ruff et al., 2018) loss during training. Log data is preprocessed using the</li>
</ul>
<p>Table 2. Anomaly detection evaluation criteria</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Normal</th>
<th style="text-align: center;">Abnormal</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Normal</td>
<td style="text-align: center;">True Negative</td>
<td style="text-align: center;">False Positive</td>
</tr>
<tr>
<td style="text-align: center;">Abnormal</td>
<td style="text-align: center;">False Negative</td>
<td style="text-align: center;">True Positive</td>
</tr>
</tbody>
</table>
<p>log parser, after which the LogBERT model identifies anomalous patterns in the candidate set similar to the DeepLog.</p>
<h3>5.3. Evaluation Metrics</h3>
<p>The F1 score which is dependent on the threshold and AUROC, which is independent of the threshold, were used as evaluation metrics in this study. Most studies on anomaly detection use AUROC as the evaluation metric in general, whereas previous studies that approached log anomaly detection as a binary classification problem used the best F1 score to record the performance; hence, both these metrics were used in this study for comparison with previous studies. When the threshold of the models in anomaly detection is determined, the confusion matrix presented in Table 2 is generated depending on the actual anomaly case and the anomaly detected by the models. The recall and precision are calculated from the confusion matrix using precision and recall, and the F1 score is calculated based on the harmonic mean of the two metrics.</p>
<p>$$
\begin{gathered}
\text { F1 score }=2 \cdot \frac{\text { precision } \cdot \text { recall }}{\text { precision }+ \text { recall }} \
\text { precision }=\frac{T P}{T P+F P}, \text { recall }=\frac{T P}{T P+F N}
\end{gathered}
$$</p>
<p>$T P$ : true positive, $F P$ : false positive, $F N$ : false negative.</p>
<p>The F1 score calculated using Eq. (6) is a metric influenced by the threshold of a model and cannot guarantee the reliability of the fundamental performance of an anomaly detection model; hence, AUROC, which is an evaluation metric unaffected by the threshold was also calculated. AUROC calculates the false positive rate (FAR) and true positive rate (TPR) for all the threshold candidates, and then illustrates a receiver operating characteristic curve with the FAR as the x -axis and TPR as the y -axis to calculate the area of the base side. The AUROC value is closer to 1 because the anomaly detection model has a better performance, whereas a random model has a value closer to 0.5 .</p>
<p>The best F1 score threshold cannot be determined in advance in this study because the log anomaly detection experiment is conducted with only normal data for training. Therefore, the best F1 score was calculated using the threshold that represents the best performance theoretically for the test dataset, and the same method was used to calculate the best</p>
<p>F1 score of benchmark studies for a fair comparison. Additionally, benchmark studies examined the performance using only the F1 score; however, this study also utilized AUROC, which is an evaluation metric that is not affected by the threshold to examine the performance. In this experiment, the method of determining anomalies differs among the benchmark models. Specifically, DeepLog and LogBERT determine anomalies based on the presence of predicted values in the top $k$ candidate set, while LogRobust and HitAnomaly, being classification models, do not calculate an abnormal score. On the other hand, LogSy, LogCluster, and LogAnomaly define an abnormal score, but the AUROC could not be calculated because no official implementation code was available. Therefore, it is impossible to calculate AUROC for the baseline models for comparison; the F1 score and performance of AUROC of LAnoBERT are presented in Tables 4 and 5.</p>
<h2>6. Results</h2>
<p>To verify the excellence of the proposed methodology, this study compared a supervised and an unsupervised learningbased anomaly detection model. Moreover, the use of a parser was recorded for comparison because the performance of the log anomaly detection significantly varies depending on the use of a log parser.</p>
<h3>6.1. Anomaly Detection Performance</h3>
<p>Table 3 shows the F1-score for the proposed LAnoBERT model and ten additional models, including both supervised learning-based models (LogSy, LogRobust, and HitAnomaly) and unsupervised learning-based models (PCA, IForest, OCSVM, LogCluster, DeepLog, LogAnomaly, and LogBERT). It is important to note that the performance results for the supervised models were obtained from their respective original studies, whereas the performance of all unsupervised models, except for the proposed LAnoBERT, were obtained from the LogBERT study. As a result, the performance results for LogRobust and HitAnomaly on the Thunderbird dataset, as well as for LogSy on the HDFS dataset, are not reported in this study due to the lack of information in their respective original papers. The results indicate that the performance of the BGL dataset was inferior compared to the HDFS dataset due to its more complex structure, as previously reported in Huang et al. (2020).</p>
<p>The performance of LogRobust and HitAnomaly, which are based on supervised learning, was observed to be favorable on the HDFS and BGL datasets. Both models underwent preprocessing utilizing the Drain parser, and HitAnomaly, which utilized parameters that were not part of the log template, demonstrated strong performance on both datasets. These results indicate that information loss during log parsing can have a significant impact on model training. LogSy,
which employed a classification model built with normal and abnormal data obtained from different systems, recorded an F1 score of 0.6500 on the BGL dataset and 0.9900 on the Thunderbird dataset. This highlights the advantage of incorporating a more realistic representation of the system into the model, as compared to LogRobust and HitAnomaly. However, the performance of LogSy on the BGL dataset was lower than expected. These results emphasize the limitations of performing log anomaly detection without log parsing on data from a specific system. NeuralLog demonstrated high performance across three datasets - HDFS, BGL, and Thunderbird, with scores of $0.9800,0.9800$, and 0.9600 respectively. This performance was noteworthy, particularly considering that it didn't utilize a parser, yet still outperformed supervised learning-based models such as LogRobust and HitAnomaly. This outcome can be interpreted as a meaningful result in itself. However, a limitation of NeuralLog is its reliance on both normal and abnormal logs during the learning process, which could make it less suitable for real-world scenarios. This point can be identified as a potential shortcoming of the model.</p>
<p>In the unsupervised learning models comparison, PCA, iForest, OCSVM, and LogCluster showed lower performance compared to DeepLog, LogAnomaly, and LogBERT which utilized deep learning techniques. Specifically, DeepLog outperformed LogAnomaly, demonstrating the effectiveness of its "top $g$ candidate" logic. LogBERT demonstrated remarkable performance with F1 scores of 0.8232 in HDFS, 0.9083 in BGL, and 0.9664 in Thunderbird, with especially strong results in BGL. These results suggest that the BERTbased LogBERT model effectively captures rich semantics by understanding context-specific information to log data. Furthermore, incorporating MLKP and VHM tasks has been observed to improve the model's ability to detect anomalies.</p>
<p>In Section 4.3.4, it was highlighted that BERT's MLM allows for the calculation of both mask loss and probability. In order to evaluate the performance of the proposed LAnoBERT, two abnormal scores were generated using mask loss and probability. The results showed that the predictive loss score led to a performance of 0.9123 in HDFS, 0.6932 in BGL, and 0.5142 in Thunderbird. It was observed that HDFS, with its shorter log sequence length and fewer unique log keys, displayed acceptable detection performance. Conversely, BGL and Thunderbird, characterized by longer log sequence lengths and more complex structures, showed relatively lower performance than other deep learning-based unsupervised models. This can be attributed to the fact that the mask loss calculates the accuracy of word-by-word predictions between the original and predicted log keys, resulting in low loss values only when the log keys are predicted in the correct order. For example, if the ground truth log key is 'A-B-C-D-E' and the model predicts 'B-C-D-E-F', the loss value would be high due</p>
<p>Table 3. F1-score on HDFS, BGL, and Thunderbird. ${ }^{\dagger}$ indicates the performance of benchmark models reported by LogBERT. The highest performance is highlighted in bold and underlined, and the second-best performance is indicated in bold. Supervised comparisons (Upper): the performance of LogRobust, HitAnomaly, LogSy, and NeuralLog are compared, and it is observed that LAnoBERT demonstrates comparable or superior performance to these models, despite the fact that LogRobust, HitAnomaly, LogSy, and NeuralLog allow for the use of abnormal data in their training, whereas LAnoBERT does not. Unsupervised comparisons (Lower): it is shown that LAnoBERT, which is a log parser-free model, produces strong results compared to other unsupervised models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Log Parser</th>
<th style="text-align: left;">HDFS</th>
<th style="text-align: left;">BGL</th>
<th style="text-align: left;">Thunderbird</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Supervised</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">LogRobust</td>
<td style="text-align: left;">O</td>
<td style="text-align: left;">$\mathbf{0 . 9 7 0 0}$</td>
<td style="text-align: left;">0.8300</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">HitAnomaly</td>
<td style="text-align: left;">O</td>
<td style="text-align: left;">$\underline{\mathbf{0 . 9 8 0 0}}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 2 0 0}$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">LogSy</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.6500</td>
<td style="text-align: left;">$\underline{\mathbf{0 . 9 9 0 0}}$</td>
</tr>
<tr>
<td style="text-align: left;">NeuralLog</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">$\underline{\mathbf{0 . 9 8 0 0}}$</td>
<td style="text-align: left;">$\underline{\mathbf{0 . 9 8 0 0}}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 6 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Unsupervised</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">PCA $^{\dagger}$</td>
<td style="text-align: left;">O</td>
<td style="text-align: left;">0.1112</td>
<td style="text-align: left;">0.1661</td>
<td style="text-align: left;">0.5439</td>
</tr>
<tr>
<td style="text-align: left;">iForest $^{\dagger}$</td>
<td style="text-align: left;">O</td>
<td style="text-align: left;">0.6049</td>
<td style="text-align: left;">0.3065</td>
<td style="text-align: left;">0.0329</td>
</tr>
<tr>
<td style="text-align: left;">OCSVM $^{\dagger}$</td>
<td style="text-align: left;">O</td>
<td style="text-align: left;">0.0495</td>
<td style="text-align: left;">0.0196</td>
<td style="text-align: left;">0.2548</td>
</tr>
<tr>
<td style="text-align: left;">LogCluster $^{\dagger}$</td>
<td style="text-align: left;">O</td>
<td style="text-align: left;">0.5399</td>
<td style="text-align: left;">0.7663</td>
<td style="text-align: left;">0.5961</td>
</tr>
<tr>
<td style="text-align: left;">DeepLog $^{\dagger}$</td>
<td style="text-align: left;">O</td>
<td style="text-align: left;">0.7734</td>
<td style="text-align: left;">0.8612</td>
<td style="text-align: left;">0.9308</td>
</tr>
<tr>
<td style="text-align: left;">LogAnomaly $^{\dagger}$</td>
<td style="text-align: left;">O</td>
<td style="text-align: left;">0.5619</td>
<td style="text-align: left;">0.7409</td>
<td style="text-align: left;">0.9273</td>
</tr>
<tr>
<td style="text-align: left;">LogBERT</td>
<td style="text-align: left;">O</td>
<td style="text-align: left;">0.8232</td>
<td style="text-align: left;">$\underline{\mathbf{0 . 9 0 8 3}}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 6 6 4}$</td>
</tr>
<tr>
<td style="text-align: left;">LAnoBERT</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">$\mathbf{0 . 9 1 2 3}$</td>
<td style="text-align: left;">0.6932</td>
<td style="text-align: left;">0.5142</td>
</tr>
<tr>
<td style="text-align: left;">(Predictive Loss)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">LAnoBERT</td>
<td style="text-align: left;">X</td>
<td style="text-align: left;">$\underline{\mathbf{0 . 9 6 4 5}}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 7 4 9}$</td>
<td style="text-align: left;">$\underline{\mathbf{0 . 9 9 9 0}}$</td>
</tr>
<tr>
<td style="text-align: left;">(Predictive Prob.)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>to the incorrect prediction of all tokens. However, from the perspective of the log sequence, the confidence in the ordered prediction of the keys 'B-C-D-E' must be considered when evaluating an abnormal score. The previously discussed example highlights the limitations of using the predictive loss score on long and complex data.</p>
<p>When the predictive probability was used as the abnormal score, LAnoBERT demonstrated superior performance to all the other models, with HDFS scoring 0.9645, BGL scoring 0.8749 , and Thunderbird scoring 0.9990 , with the exception of BGL. The results showed that the abnormal score based on the mask probability proposed by LAnoBERT was a critical factor in performance improvement. These results highlight the effectiveness of LAnoBERT, an unsupervised learning-based method, compared to the parserbased supervised learning methodologies LogRobust, HitAnomaly, and LogSy. Despite not using a parser during training, LAnoBERT achieved higher performance than LogRobust and LogSy, while being only 0.0451 lower than HitAnomaly in BGL. This demonstrates the significance of
considering context and pre-training of MLM in the design of LAnoBERT for log anomaly detection. Additionally, using predictive probability allows for the detection of cases with unseen normal log data more accurately compared to using predictive loss, making LAnoBERT more practical and useful in real-world applications than benchmark models.</p>
<p>Furthermore, it is critical to perform log anomaly detection on actual systems. Since logs are collected in real-time, the majority of log data is comprised of normal logs. As a result, conducting anomaly detection based on binary classification using normal and abnormal log data poses a significant challenge for its practical implementation. Figure 7 illustrates the selected benchmark models of supervised and unsupervised learning, with and without the use of a parser. The first quadrant, which represents the parser-involved supervised case, represents the easiest scenario to ensure the performance of a model, but it is also the most unrealistic. On the other hand, the parser-free unsupervised case in the third quadrant is the most realistic scenario but also the most chal-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7. Comparison with LAnoBERT and four benchmark models in realistic scenarios.
lenging in terms of ensuring the performance of a model. Despite these challenges, the proposed LAnoBERT model in this study showed comparable performance (BGL) to benchmark models under the easiest assumption and even better performance (HDFS, Thunderbird) than some benchmark models in the most difficult scenario. Hence, the LAnoBERT model proposed in this study appears to be a practical model that can be applied in real-world systems.</p>
<p>As explained earlier, supervised learning-based models are still proposed for system log anomaly detection. The proposed model achieved better performance than LogSy, which is a supervised learning-based model, and the LogRobust model, which uses a parser and is a supervised learningbased model. LAnoBERT recorded excellent detection performance than certain supervised learning-based baseline models in the most unfair comparison environment. In other words, LAnoBERT is a model trained in the most realistic yet disadvantageous environment and is a robust model exhibiting more outstanding or similar performance compared to other baseline models in the most unrealistic and advantageous environment.</p>
<p>Table 4 lists the F1 score and AUROC performance of LAnoBERT. The performance of benchmark models was not measured using AUROC, which is a frequently used evaluation metric in anomaly detection. benchmark models may score highly for the best F1 score to exhibit the best performance; however, they are limited in identifying whether the model has high reliability regardless of the threshold. Therefore, the performances of the two evaluation metrics were determined for LAnoBERT, and the results showed that the F1 score is similar to that of the other models evaluated in a relatively more advantageous environment. By contrast, a high AUROC value closer to 1, which indicates that a model is most idealistic, was obtained. Consequently, even if threshold-dependent detection performance metrics other than the F1 score are used, LAnoBERT can be regarded as a highly reliable system log anomaly detection
model with remarkably outstanding performance.</p>
<h3>6.2. Performance according to the BERT learning method</h3>
<p>BERT includes models pre-trained with natural language, and the pre-trained model typically resulted in an excellent performance in various natural language processing tasks. Therefore, a comparative experiment was conducted for LAnoBERT, which was pre-trained with natural language. The proposed LAnoBERT was trained to utilize an initialized BERT model. In order to investigate the impact of the pre-training model and provide a practical alternative for real-world log anomaly detection, we conducted an additional experiment in which the pre-trained BERT model with natural language is employed instead of training BERT from scratch. The results of this experiment are documented in Table 5.</p>
<p>When the BERT model pre-trained with natural language was used, MLM was additionally performed after importing the pre-trained model. Pre-training has already been performed with massive natural language data, and thus, it can be interpreted that task adaptive pre-training (Gururangan et al., 2020) was conducted with the log data. Table 5 presents the result of training 6,000 steps with a batch size of 15 per 2080 ti for a total of two 2080 ti's.</p>
<p>When the BERT model pre-trained with natural language was used, the F1 score in the BGL data was 0.9020 , which was improved by 0.0271 compared to the model trained from scratch; by contrast, the F1 score in the HDFS data was 0.9304 , which was decreased by 0.0341 compared to the model trained from scratch. These results indicate that the HDFS data consisting of a very simple log structure have degraded performance when a model that has learned the context of natural language is used. The number of vocabularies in the HDFS dataset after preprocessing is only 200, which is very few for expressing the context of natural</p>
<p>Table 4. Performance of our model (F1 Score, AUROC)</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Log Parser</th>
<th style="text-align: center;">HDFS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BGL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Thunderbird</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1 Score</td>
<td style="text-align: center;">AUROC</td>
<td style="text-align: center;">F1 Score</td>
<td style="text-align: center;">AUROC</td>
<td style="text-align: center;">F1 Score</td>
<td style="text-align: center;">AUROC</td>
</tr>
<tr>
<td style="text-align: left;">Unsupervised</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LAnoBERT <br> (Predictive Prob.)</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">0.9645</td>
<td style="text-align: center;">0.9901</td>
<td style="text-align: center;">0.8749</td>
<td style="text-align: center;">0.9721</td>
<td style="text-align: center;">0.9990</td>
<td style="text-align: center;">0.9520</td>
</tr>
</tbody>
</table>
<p>Table 5. Performance of initialized LAnoBERT and pre-trained LAnoBERT</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Log Parser</th>
<th style="text-align: center;">Training</th>
<th style="text-align: center;">HDFS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BGL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Thunderbird</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1 Score</td>
<td style="text-align: center;">AUROC</td>
<td style="text-align: center;">F1 Score</td>
<td style="text-align: center;">AUROC</td>
<td style="text-align: center;">F1 Score</td>
<td style="text-align: center;">AUROC</td>
</tr>
<tr>
<td style="text-align: left;">Unsupervised</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LAnoBERT <br> (Predictive Prob.)</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">Initialized</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 4 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 0 1}$</td>
<td style="text-align: center;">0.8749</td>
<td style="text-align: center;">0.9721</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 9 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 5 2 0}$</td>
</tr>
<tr>
<td style="text-align: left;">LAnoBERT <br> (Predictive Prob.)</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">pre-trained</td>
<td style="text-align: center;">0.9304</td>
<td style="text-align: center;">0.9659</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 2 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 1 2}$</td>
<td style="text-align: center;">0.8954</td>
<td style="text-align: center;">0.3505</td>
</tr>
</tbody>
</table>
<p>language; hence, the pre-training presumably had a negative effect on anomaly detection. Conversely, BGL data with a relatively more complicated structure has a total of 1,000 vocabularies after preprocessing. This supports the fact that the BGL dataset is a more complicated dataset than HDFS, and there are cases where natural language is included in the log data with this number of vocabularies. Therefore, if appropriate training is performed additionally for a model that has learned the context of natural language, the anomaly detection performance can be improved compared to other models that have not been pre-trained. The experimental results in Table 5 show that the log data containing natural language can have a similar form as human language and that pre-trained BERT can be effectively applied. In conclusion, the results demonstrate that incorporating the LAnoBERT framework with a pre-trained BERT model is a viable alternative.</p>
<h2>7. Conclusion</h2>
<p>This study proposed LAnoBERT, which is an unsupervised learning-based system log anomaly detection model where a parser is not used. The proposed LAnoBERT learned the context of normal log data using MLM, and abnormal logs were detected based on the prediction error and predictive probability during the test. In terms of the nature of the system log, normal and abnormal data have similar characteristics; thus, a new score calculation method is proposed for defining the abnormal score based on the top-k predictive probability. The proposed model exhibited the best performance compared to the unsupervised models, and superior or similar performance compared to supervised learningbased models. In addition, the efficient inference process proposed in this study is expected to work well in an actual system. Although the performances of benchmark models are heavily dependent on the use of log parser, our proposed</p>
<p>LAnoBERT can be a robust and parser-independent log anomaly detection model.</p>
<p>The proposed LAnoBERT framework exhibits promising results in log anomaly detection, however, there are limitations that need to be addressed in future research. Firstly, LAnoBERT requires individual training for each log dataset. A unified framework, as outlined in UniAD (You et al., 2022), is needed to cater to diverse log structures in different systems like distributed systems, supercomputers, and server applications. Secondly, LAnoBERT's Transformerbased architecture incurs higher computational costs compared to RNN-based models due to its self-attention layer ( $O\left(n^{2} \cdot d\right)$ complexity) versus the recurrent layer of RNN ( $O\left(n \cdot d^{2}\right)$ complexity). To resolve the computational inefficiency, incorporating recent parameter-efficient learning methods such as LoRA (Hu et al., 2022) and Adapter (Houlsby et al., 2019) is crucial in developing a real-time log anomaly detection model. Finally, in this study, only minimal preprocessing was performed using regular expressions and tokenization using Wordpiece tokenizer. The Log Parser-free methodology can be improved by templating log sequences into the natural language via prompt tuning (Brown et al., 2020; Lester et al., 2021) which could enable anomaly detection with a pre-trained tokenizer and language model, without the need for further preprocessing or training.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2022R1A2C2005455). This work was also supported by Institute of Information \&amp; communications Technology Planning \&amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2021-0-00471, Devel-</p>
<p>opment of Autonomous Control Technology for Error-Free Information Infrastructure Based on Modeling \&amp; Optimization).</p>
<h2>References</h2>
<p>Brown, A., Tuor, A., Hutchinson, B., and Nichols, N. Recurrent neural network attention mechanisms for interpretable system log anomaly detection. In Proceedings of the First Workshop on Machine Learning for Computing Systems, MLCS'18, pp. 8, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450358651. doi: 10.1145/3217871.3217872. URL https://doi.org/10.1145/3217871.3217872.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Cinque, M., Cotroneo, D., and Pecchia, A. Event logs for the analysis of software failures: A rule-based approach. IEEE Transactions on Software Engineering, 39(6):806821, 2013. doi: 10.1109/TSE.2012.67.</p>
<p>Clark, K., Khandelwal, U., Levy, O., and Manning, C. D. What does BERT look at? an analysis of BERT's attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 276-286, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https://aclanthology. org/W19-4828.</p>
<p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423.</p>
<p>Du, M. and Li, F. Spell: Streaming parsing of system event logs. In 2016 IEEE 16th International Conference on</p>
<p>Data Mining (ICDM), pp. 859-864, 2016. doi: 10.1109/ ICDM.2016.0103.</p>
<p>Du, M., Li, F., Zheng, G., and Srikumar, V. Deeplog: Anomaly detection and diagnosis from system logs through deep learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS '17, pp. 1285-1298, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450349468. doi: 10.1145/3133956.3134015. URL https://doi.org/10.1145/3133956.3134015.</p>
<p>Elnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Yu, W., Jones, L., Gibbs, T., Feher, T., Angerer, C., Steinegger, M., Bhowmik, D., and Rost, B. Prottrans: Towards cracking the language of lifes code through selfsupervised deep learning and high performance computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-1, 2021. doi: 10.1109/TPAMI. 2021. 3095381.</p>
<p>Guo, H., Yuan, S., and Wu, X. Logbert: Log anomaly detection via bert. In 2021 International Joint Conference on Neural Networks (IJCNN), pp. 1-8, 2021. doi: 10. 1109/IJCNN52387.2021.9534113.</p>
<p>Gururangan, S., Marasovi, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., and Smith, N. A. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 83428360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.740. URL https://aclanthology.org/2020.acl-main. 740.</p>
<p>Hansen, S. E. and Atkins, E. T. Automated system monitoring and notification with swatch. In LISA, volume 93, pp. 145-152, 1993.</p>
<p>He, P., Zhu, J., Zheng, Z., and Lyu, M. R. Drain: An online log parsing approach with fixed depth tree. 2017 IEEE International Conference on Web Services (ICWS), pp. 33-40, 2017.</p>
<p>Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for NLP. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2790-2799. PMLR, 0915 Jun 2019. URL https://proceedings.mlr.press/v97/ houlsby19a.html.</p>
<p>Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International</p>
<p>Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYI9.</p>
<p>Huang, S., Liu, Y., Fung, C., He, R., Zhao, Y., Yang, H., and Luan, Z. Hitanomaly: Hierarchical transformers for anomaly detection in system log. IEEE Transactions on Network and Service Management, 17(4):2064-2076, 2020.</p>
<p>Jawahar, G., Sagot, B., and Seddah, D. What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3651-3657, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1356. URL https:// aclanthology.org/P19-1356.</p>
<p>Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., and Levy, O. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64-77, 2020.</p>
<p>Kim, G., Yi, H., Lee, J., Paek, Y., and Yoon, S. Lstmbased system-call language modeling and robust ensemble method for designing host-based intrusion detection systems. CoRR, abs/1611.01726, 2016. URL http://arxiv.org/abs/1611.01726.</p>
<p>Lample, G. and Conneau, A. Cross-lingual language model pretraining. CoRR, abs/1901.07291, 2019. URL http: //arxiv.org/abs/1901.07291.</p>
<p>Le, V.-H. and Zhang, H. Log-based anomaly detection without log parsing. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 492-504. IEEE, 2021.</p>
<p>Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.emnlp-main.243. URL https://aclanthology.org/ 2021.emnlp-main. 243.</p>
<p>Lin, Q., Zhang, H., Lou, J.-G., Zhang, Y., and Chen, X. Log clustering based problem identification for online service systems. In 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C), pp. 102-111, 2016.</p>
<p>Liu, F. T., Ting, K. M., and Zhou, Z.-H. Isolation forest. In 2008 Eighth IEEE International Conference on Data Mining, pp. 413-422, 2008. doi: 10.1109/ICDM. 2008. 17.</p>
<p>Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, 2021.</p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Meng, W., Liu, Y., Zhu, Y., Zhang, S., Pei, D., Liu, Y., Chen, Y., Zhang, R., Tao, S., Sun, P., and Zhou, R. Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pp. 4739-4745. International Joint Conferences on Artificial Intelligence Organization, 7 2019. doi: 10.24963/ijcai.2019/658. URL https://doi.org/10.24963/ijcai.2019/658.</p>
<p>Nedelkoski, S., Bogatinovski, J., Acker, A., Cardoso, J., and Kao, O. Self-attentive classification-based anomaly detection in unstructured logs. In 2020 IEEE International Conference on Data Mining (ICDM), pp. 1196-1201. IEEE, 2020.</p>
<p>Oliner, A. and Stearley, J. What supercomputers say: A study of five system logs. In 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07), pp. 575-584. IEEE, 2007.</p>
<p>Oprea, A., Li, Z., Yen, T.-F., Chin, S. H., and Alrwais, S. Detection of early-stage enterprise infection by mining large-scale log data. In 2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, pp. 45-56. IEEE, 2015.</p>
<p>Petroni, F., Rocktschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 24632473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/ D19-1250. URL https://aclanthology.org/D19-1250.</p>
<p>Prewett, J. E. Analyzing cluster log files using logsurfer. In Proceedings of the 4th Annual Conference on Linux Clusters. Citeseer, 2003.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.</p>
<p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the</p>
<p>limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140): 1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.</p>
<p>Rao, R., Meier, J., Sercu, T., Ovchinnikov, S., and Rives, A. Transformer protein language models are unsupervised structure learners. In International Conference on Learning Representations, 2020.</p>
<p>Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S. A., Binder, A., Mller, E., and Kloft, M. Deep one-class classification. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4393-4402. PMLR, 1015 Jul 2018. URL https://proceedings.mlr.press/v80/ ruff18a.html.</p>
<p>Schick, T. and Schtze, H. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255-269, Online, April 2021. Association for Computational Linguistics. doi: 10. 18653/v1/2021.eacl-main.20. URL https://aclanthology. org/2021.eacl-main. 20.</p>
<p>Schlkopf, B., Platt, J. C., Shawe-Taylor, J. C., Smola, A. J., and Williamson, R. C. Estimating the support of a high-dimensional distribution. Neural Comput., 13(7):1443-1471, jul 2001. ISSN 0899-7667. doi: 10.1162/089976601750264965. URL https://doi.org/10. 1162/089976601750264965.</p>
<p>Simache, C. and Kaaniche, M. Availability assessment of sunos/solaris unix systems based on syslogd and wtmpx log files: A case study. In 11th Pacific Rim International Symposium on Dependable Computing (PRDC'05), pp. 8 pp.-, 2005. doi: 10.1109/PRDC.2005.20.</p>
<p>Taylor, W. L. "cloze procedure": A new tool for measuring readability. Journalism quarterly, 30(4):415-433, 1953.</p>
<p>Tenney, I., Das, D., and Pavlick, E. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593-4601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10. 18653/v1/P19-1452. URL https://aclanthology.org/ P19-1452.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.</p>
<p>Wang, Y., Wong, J., and Miner, A. Anomaly intrusion detection using one class svm. In Proceedings from the Fifth Annual IEEE SMC Information Assurance Workshop, 2004., pp. 358-364, 2004. doi: 10.1109/IAW. 2004. 1437839 .</p>
<p>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.</p>
<p>Xu, W., Huang, L., Fox, A., Patterson, D., and Jordan, M. I. Detecting large-scale system problems by mining console logs. In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, pp. 117132, 2009.</p>
<p>Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q. V. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.</p>
<p>Yen, T.-F., Oprea, A., Onarlioglu, K., Leetham, T., Robertson, W., Juels, A., and Kirda, E. Beehive: Large-scale log analysis for detecting suspicious activity in enterprise networks. In Proceedings of the 29th Annual Computer Security Applications Conference, pp. 199-208, 2013.</p>
<p>You, Z., Cui, L., Shen, Y., Yang, K., Lu, X., Zheng, Y., and Le, X. A unified model for multi-class anomaly detection. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id= bMYU8_qD8PW.</p>
<p>Zhang, X., Xu, Y., Lin, Q., Qiao, B., Zhang, H., Dang, Y., Xie, C., Yang, X., Cheng, Q., Li, Z., et al. Robust log-based anomaly detection on unstable log data. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 807-817, 2019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ School of Industrial Management Engineering, College of Engineering, Korea University, Seoul, Korea. Correspondence to: Pilsung Kang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#105;&#108;&#115;&#117;&#110;&#103;&#95;&#107;&#97;&#110;&#103;&#64;&#107;&#111;&#114;&#101;&#97;&#46;&#97;&#99;&#46;&#107;&#114;">&#112;&#105;&#108;&#115;&#117;&#110;&#103;&#95;&#107;&#97;&#110;&#103;&#64;&#107;&#111;&#114;&#101;&#97;&#46;&#97;&#99;&#46;&#107;&#114;</a>.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
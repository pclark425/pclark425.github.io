<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2926 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2926</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2926</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-272827977</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.14457v3.pdf" target="_blank">Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends</a></p>
                <p><strong>Paper Abstract:</strong> With the rapid advancement of large models (LMs), the development of general-purpose intelligent agents powered by LMs has become a reality. It is foreseeable that in the near future, LM-driven general AI agents will serve as essential tools in production tasks, capable of autonomous communication and collaboration without human intervention. This paper investigates scenarios involving the autonomous collaboration of future LM agents. We review the current state of LM agents, the key technologies enabling LM agent collaboration, and the security and privacy challenges they face during cooperative operations. To this end, we first explore the foundational principles of LM agents, including their general architecture, key components, enabling technologies, and modern applications. We then discuss practical collaboration paradigms from data, computation, and knowledge perspectives to achieve connected intelligence among LM agents. After that, we analyze the security vulnerabilities and privacy risks associated with LM agents, particularly in multi-agent settings, examining underlying mechanisms and reviewing current and potential countermeasures. Lastly, we propose future research directions for building robust and secure LM agent ecosystems.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2926.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2926.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAISE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAISE (memory-enhanced ReAct for conversational agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-component memory architecture proposed to enhance ReAct-style agents for conversational settings by combining a short-term scratchpad with a retrieval-based long-term module to support iterative planning and error correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From LLM to conversational agent: A memory enhanced architecture with fine-tuning of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAISE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Augments ReAct-style actor/evaluator agents with a two-part memory: (1) a Scratchpad that captures recent interactions (short-term memory) and (2) a retrieval module that serves as long-term memory to fetch relevant past examples to inform planning and responses.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hybrid (short-term scratchpad + retrieval-based long-term memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Two-component memory: (a) Scratchpad stores recent interactions and intermediate reasoning traces (short-term, token-level or session-level), and (b) a retrieval module indexes past examples/experiences in external storage (e.g., vector DB) that can be retrieved and injected into prompts for long-term context; the agent uses both during planning and response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Retrieval-augmented: long-term items are retrieved by relevance to current context (implied retrieval module); short-term Scratchpad is directly included in context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Recent interactions and reasoning traces in Scratchpad; exemplars, prior dialogues or task-specific examples in the retrieval store.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Survey reports that the RAISE-style hybrid design (Scratchpad + retrieval) enhances ReAct for conversational agents by allowing short-term context capture plus retrieval of relevant past examples to inform planning and reduce repeated errors.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Described as a hybrid approach combining short-term scratchpad and retrieval-based long-term memory; no quantitative comparison reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2926.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2926.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (language agents with verbal reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agent framework that converts environmental feedback into verbal self-reflection to iteratively improve behavior; includes actor, evaluator, and self-reflection components and leverages memory to learn from past mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent architecture with three components: an actor that produces actions and text (using CoT/ReAct), an evaluator that scores outputs using task-specific reward functions, and a self-reflection module that generates verbal feedback (stored as memory) to iteratively refine the actor's future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reflection-based / episodic (stores past actions, feedback, and self-reflections)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Stores past actions, outputs, and evaluator feedback as verbal reflections; these reflections are used as memory to inform subsequent planning and corrections (i.e., an episodic log of past trials and corrective reflections).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Implicit retrieval via including past reflections and evaluator summaries into future prompts (in-context retrieval); no explicit vector-RAG mechanism described in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Past actions, generated thoughts, evaluator scores, and self-reflective verbal notes about errors and corrective strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Reflexion enables agents to learn from past errors by turning execution feedback into self-reflection, which helps iterative optimization of behaviors beyond single-shot ReAct-style planning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Survey notes need for evaluator/reward design; no detailed failure modes for memory retrieval reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Positioned as an approach that uses verbalized episodic memory (reflections) rather than pure retrieval-from-database; no head-to-head comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2926.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2926.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/method that interleaves chain-of-thought style reasoning traces with environment actions so LLMs can plan and use tools while producing reasoning traces alongside actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prompts LLMs to produce both reasoning traces (thoughts) and actions in tandem, enabling the agent to plan, query tools, and refine plans via feedback; used as a core planning paradigm that can be augmented with memory modules (e.g., Reflexion, RAISE).</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>ReAct is primarily a reasoning+acting approach; the survey highlights that ReAct can be enhanced by integrating memory modules (e.g., Reflexion/RAISE) for iterative improvements but ReAct itself does not provide a long-term memory architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2926.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2926.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VOYAGER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VOYAGER (LLM-powered Minecraft agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered embodied agent for open-ended play in Minecraft that uses in-context lifelong learning and a skill library (executable code snippets) as memory to accumulate and reuse skills across worlds and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VOYAGER</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent for Minecraft using in-context lifelong learning, an automated exploration curriculum, and a persistent skill library of executable programs (skills) that it writes and reuses; iterative prompting refines programs based on execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Minecraft (open-ended embodied/sandbox environment)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>A complex open-world sandbox environment requiring multi-step grounded planning, exploration, tool use and skill composition; challenges include long-horizon planning and grounding actions to environment.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>skill library (long-term), in-context episodic memory (short-term)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Skill library stores executable code/programs representing discovered skills (persistent long-term memory); in-context lifelog or recent interaction snippets provide short-term adaptation; iterative prompting updates/refines skills based on execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Retrieval via in-context selection of relevant skills from the skill library and injection into prompts (similarity/relevance selection implied), plus in-context learning from recent logs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Executable skill programs, discovered behaviors, and recent execution traces used to build and refine higher-level behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>VOYAGER demonstrates that persisting discovered skills (a skill library) and using in-context lifelong learning enables generalization to new tasks/worlds and composes complex behaviors; memory (skill library) is central to scaling open-ended agent competence in Minecraft.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Survey does not report quantitative limits but implies need for scalable skill retrieval and verification; potential brittleness from imperfect skill programs and reliance on LLM-generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Skill-library long-term memory combined with in-context short-term learning is presented as effective for embodied open-ended tasks; no quantitative comparison to other memory architectures in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2926.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2926.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BALROG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BALROG (Benchmarking agentic LLM and VLM reasoning on games)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark (ICLR 2025) for evaluating reasoning capabilities of agentic LLMs and VLMs across games; cited as a resource for assessing agent reasoning in gaming environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BALROG: Benchmarking agentic LLM and VLM reasoning on games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BALROG (benchmark / evaluation suite)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Benchmark suite evaluating agentic LLM/VLM reasoning across a collection of games to measure agentic reasoning and problem solving; the survey references BALROG as a benchmarking effort for agentic LLMs on games.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>A multi-game benchmark (exact included games not enumerated in the survey text); intended to evaluate reasoning and problem-solving of agentic LLMs/VLMs in gaming tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>The survey only cites BALROG as a benchmark for agentic reasoning in games; it does not report BALROG results or specific experiments on memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>From LLM to conversational agent: A memory enhanced architecture with fine-tuning of large language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>BALROG: Benchmarking agentic LLM and VLM reasoning on games <em>(Rating: 2)</em></li>
                <li>A survey on large language model-based game agents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2926",
    "paper_id": "paper-272827977",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "RAISE",
            "name_full": "RAISE (memory-enhanced ReAct for conversational agents)",
            "brief_description": "A dual-component memory architecture proposed to enhance ReAct-style agents for conversational settings by combining a short-term scratchpad with a retrieval-based long-term module to support iterative planning and error correction.",
            "citation_title": "From LLM to conversational agent: A memory enhanced architecture with fine-tuning of large language models",
            "mention_or_use": "mention",
            "agent_name": "RAISE",
            "agent_description": "Augments ReAct-style actor/evaluator agents with a two-part memory: (1) a Scratchpad that captures recent interactions (short-term memory) and (2) a retrieval module that serves as long-term memory to fetch relevant past examples to inform planning and responses.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "hybrid (short-term scratchpad + retrieval-based long-term memory)",
            "memory_architecture": "Two-component memory: (a) Scratchpad stores recent interactions and intermediate reasoning traces (short-term, token-level or session-level), and (b) a retrieval module indexes past examples/experiences in external storage (e.g., vector DB) that can be retrieved and injected into prompts for long-term context; the agent uses both during planning and response generation.",
            "memory_retrieval_mechanism": "Retrieval-augmented: long-term items are retrieved by relevance to current context (implied retrieval module); short-term Scratchpad is directly included in context.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Recent interactions and reasoning traces in Scratchpad; exemplars, prior dialogues or task-specific examples in the retrieval store.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Survey reports that the RAISE-style hybrid design (Scratchpad + retrieval) enhances ReAct for conversational agents by allowing short-term context capture plus retrieval of relevant past examples to inform planning and reduce repeated errors.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": "Described as a hybrid approach combining short-term scratchpad and retrieval-based long-term memory; no quantitative comparison reported in this survey.",
            "uuid": "e2926.0",
            "source_info": {
                "paper_title": "Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (language agents with verbal reinforcement learning)",
            "brief_description": "An LLM-agent framework that converts environmental feedback into verbal self-reflection to iteratively improve behavior; includes actor, evaluator, and self-reflection components and leverages memory to learn from past mistakes.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Agent architecture with three components: an actor that produces actions and text (using CoT/ReAct), an evaluator that scores outputs using task-specific reward functions, and a self-reflection module that generates verbal feedback (stored as memory) to iteratively refine the actor's future behavior.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "reflection-based / episodic (stores past actions, feedback, and self-reflections)",
            "memory_architecture": "Stores past actions, outputs, and evaluator feedback as verbal reflections; these reflections are used as memory to inform subsequent planning and corrections (i.e., an episodic log of past trials and corrective reflections).",
            "memory_retrieval_mechanism": "Implicit retrieval via including past reflections and evaluator summaries into future prompts (in-context retrieval); no explicit vector-RAG mechanism described in survey.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Past actions, generated thoughts, evaluator scores, and self-reflective verbal notes about errors and corrective strategies.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Reflexion enables agents to learn from past errors by turning execution feedback into self-reflection, which helps iterative optimization of behaviors beyond single-shot ReAct-style planning.",
            "memory_limitations": "Survey notes need for evaluator/reward design; no detailed failure modes for memory retrieval reported in this paper.",
            "comparison_with_other_memory_types": "Positioned as an approach that uses verbalized episodic memory (reflections) rather than pure retrieval-from-database; no head-to-head comparison reported.",
            "uuid": "e2926.1",
            "source_info": {
                "paper_title": "Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting)",
            "brief_description": "A prompting/method that interleaves chain-of-thought style reasoning traces with environment actions so LLMs can plan and use tools while producing reasoning traces alongside actions.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "agent_name": "ReAct",
            "agent_description": "Prompts LLMs to produce both reasoning traces (thoughts) and actions in tandem, enabling the agent to plan, query tools, and refine plans via feedback; used as a core planning paradigm that can be augmented with memory modules (e.g., Reflexion, RAISE).",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "ReAct is primarily a reasoning+acting approach; the survey highlights that ReAct can be enhanced by integrating memory modules (e.g., Reflexion/RAISE) for iterative improvements but ReAct itself does not provide a long-term memory architecture.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2926.2",
            "source_info": {
                "paper_title": "Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "VOYAGER",
            "name_full": "VOYAGER (LLM-powered Minecraft agent)",
            "brief_description": "An LLM-powered embodied agent for open-ended play in Minecraft that uses in-context lifelong learning and a skill library (executable code snippets) as memory to accumulate and reuse skills across worlds and tasks.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "VOYAGER",
            "agent_description": "Agent for Minecraft using in-context lifelong learning, an automated exploration curriculum, and a persistent skill library of executable programs (skills) that it writes and reuses; iterative prompting refines programs based on execution feedback.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Minecraft (open-ended embodied/sandbox environment)",
            "text_game_description": "A complex open-world sandbox environment requiring multi-step grounded planning, exploration, tool use and skill composition; challenges include long-horizon planning and grounding actions to environment.",
            "uses_memory": true,
            "memory_type": "skill library (long-term), in-context episodic memory (short-term)",
            "memory_architecture": "Skill library stores executable code/programs representing discovered skills (persistent long-term memory); in-context lifelog or recent interaction snippets provide short-term adaptation; iterative prompting updates/refines skills based on execution feedback.",
            "memory_retrieval_mechanism": "Retrieval via in-context selection of relevant skills from the skill library and injection into prompts (similarity/relevance selection implied), plus in-context learning from recent logs.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Executable skill programs, discovered behaviors, and recent execution traces used to build and refine higher-level behaviors.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "VOYAGER demonstrates that persisting discovered skills (a skill library) and using in-context lifelong learning enables generalization to new tasks/worlds and composes complex behaviors; memory (skill library) is central to scaling open-ended agent competence in Minecraft.",
            "memory_limitations": "Survey does not report quantitative limits but implies need for scalable skill retrieval and verification; potential brittleness from imperfect skill programs and reliance on LLM-generated code.",
            "comparison_with_other_memory_types": "Skill-library long-term memory combined with in-context short-term learning is presented as effective for embodied open-ended tasks; no quantitative comparison to other memory architectures in this survey.",
            "uuid": "e2926.3",
            "source_info": {
                "paper_title": "Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "BALROG",
            "name_full": "BALROG (Benchmarking agentic LLM and VLM reasoning on games)",
            "brief_description": "A benchmark (ICLR 2025) for evaluating reasoning capabilities of agentic LLMs and VLMs across games; cited as a resource for assessing agent reasoning in gaming environments.",
            "citation_title": "BALROG: Benchmarking agentic LLM and VLM reasoning on games",
            "mention_or_use": "mention",
            "agent_name": "BALROG (benchmark / evaluation suite)",
            "agent_description": "Benchmark suite evaluating agentic LLM/VLM reasoning across a collection of games to measure agentic reasoning and problem solving; the survey references BALROG as a benchmarking effort for agentic LLMs on games.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": "A multi-game benchmark (exact included games not enumerated in the survey text); intended to evaluate reasoning and problem-solving of agentic LLMs/VLMs in gaming tasks.",
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "The survey only cites BALROG as a benchmark for agentic reasoning in games; it does not report BALROG results or specific experiments on memory mechanisms.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2926.4",
            "source_info": {
                "paper_title": "Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "From LLM to conversational agent: A memory enhanced architecture with fine-tuning of large language models",
            "rating": 2,
            "sanitized_title": "from_llm_to_conversational_agent_a_memory_enhanced_architecture_with_finetuning_of_large_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "BALROG: Benchmarking agentic LLM and VLM reasoning on games",
            "rating": 2,
            "sanitized_title": "balrog_benchmarking_agentic_llm_and_vlm_reasoning_on_games"
        },
        {
            "paper_title": "A survey on large language model-based game agents",
            "rating": 2,
            "sanitized_title": "a_survey_on_large_language_modelbased_game_agents"
        }
    ],
    "cost": 0.024731999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends
2 Jun 2025</p>
<p>Yuntao Wang 
Communication Language Protocol Syntax Interaction Strategies Cooperation Paradigms</p>
<p>Yanghe Pan 
Communication Language Protocol Syntax Interaction Strategies Cooperation Paradigms</p>
<p>Zhou Su 
Communication Language Protocol Syntax Interaction Strategies Cooperation Paradigms</p>
<p>Yi Deng 
Communication Language Protocol Syntax Interaction Strategies Cooperation Paradigms</p>
<p>Quan Zhao 
Communication Language Protocol Syntax Interaction Strategies Cooperation Paradigms</p>
<p>Linkang Du 
Communication Language Protocol Syntax Interaction Strategies Cooperation Paradigms</p>
<p>Tom H Luan 
Communication Language Protocol Syntax Interaction Strategies Cooperation Paradigms</p>
<p>Jiawen Kang 
Communication Language Protocol Syntax Interaction Strategies Cooperation Paradigms</p>
<p>Dusit Niyato 
Communication Language Protocol Syntax Interaction Strategies Cooperation Paradigms</p>
<p>Interaction Strategies 
Communication Language Protocol Syntax Interaction Strategies Cooperation Paradigms</p>
<p>Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends
2 Jun 202539FA22147AF2062C9A4E7B910E17AA2EarXiv:2409.14457v3[cs.AI]Large modelsAI agentsembodied intelligencenetworkingmulti-agent collaborationsecurityprivacy Distributed Cooperation Distributed Competition Partial Cooperation Foundation Models Knowledge-related Technologies Interaction Technologies Digital Twin Technologies Multi-agent Collaboration Technologies Spatiotemporal Dynamics Semantic-aware Communication Information-centric Routing Hierarchically Distributed Collaboration Data Cooperation Computation Competition Knowledge Cooperation Cloud Layer Edge Layer End Layer Cooperative LM Fine-tuning Cooperative LM Inference Cooperative LM Caching Multimodal Data Spatio-temporal Data Horizontal Collaboration Vertical Collaboration Hybrid Collaboration Knowledge Sharing Knowledge Fusion Knowledge Retrieval
With the rapid advancement of large models (LMs), the development of general-purpose intelligent agents powered by LMs has become a reality.It is foreseeable that in the near future, LMdriven general AI agents will serve as essential tools in production tasks, capable of autonomous communication and collaboration without human intervention.This paper investigates scenarios involving the autonomous collaboration of future LM agents.We review the current state of LM agents, the key technologies enabling LM agent collaboration, and the security and privacy challenges they face during cooperative operations.To this end, we first explore the foundational principles of LM agents, including their general architecture, key components, enabling technologies, and modern applications.We then discuss practical collaboration paradigms from data, computation, and knowledge perspectives to achieve connected intelligence among LM agents.After that, we analyze the security vulnerabilities and privacy risks associated with LM agents, particularly in multi-agent settings, examining underlying mechanisms and reviewing current and potential countermeasures.Lastly, we propose future research directions for building robust and secure LM agent ecosystems.</p>
<p>I. INTRODUCTION</p>
<p>A. Background of Large Model Based Agents</p>
<p>In the 1950s, Alan Turing introduced the Turing Test to assess whether machines could exhibit intelligence comparable to humans.These artificial entities, commonly known as "agents", serve as the core components of artificial intelligence (AI) systems.AI agents (also known as agentic AI [1]) are autonomous entities capable of understanding and responding to human inputs, perceiving their environment, making decisions, and taking actions across physical, virtual, or mixed-reality settings to achieve specific goals [2].They can be software-based or physical entities, functioning independently or in collaboration with humans or other agents.Since the mid-20th century, significant progress has been made in the development of AI agents [3], [4], such as Deep Blue, AlphaGo, and AlphaZero, as shown in Fig. 1.Despite these advances, prior research primarily concentrated on refining specialized abilities such as symbolic reasoning or excelling in certain tasks such as Go or Chess, often neglecting the cultivation of general-purpose capabilities within AI models such  as long-term planning, multi-task generalization, and knowledge retention.</p>
<p>With the rise of large models (LMs), including large language models (LLMs) such as OpenAI GPT-4, DeepSeek-R1, and Google PaLM 2, as well as multi-modal vision-language models (VLMs) such as Sora, LM-based agents (also known as LM agents or agentic LMs [5]) are unlocking new possibilities.LM agents significantly enhance the inherent capabilities of AI systems, providing a versatile foundation for the next-generation AI agents [6].Serving as the "brain" of AI agents, LMs empower them with advanced capabilities in human-machine interaction (HMI), few/zero-shot planning, contextual understanding, knowledge retention, and general-purpose task solving across physical, virtual, or mixed-reality environments [2], [7].LM agents generally fall into two categories:</p>
<p> Virtual LM agents, such as AutoGPT [8] and AutoGen [9], can autonomously interpret human instructions and use various tools (e.g., search engines and external APIs) to gather information and complete intricate tasks [10].For instance, as shown in Fig. 2(a), an LM-powered personal assistant can generate personalized travel plans, set reminders, and manage tasks while continuously learning and adapting in dynamic environments.</p>
<p> Embodied LM agents, such as FigureAI's Figure 02 and Tesla's Optimus, engage directly with the physical world.These agents perceive and interact with their surroundings, allowing them to solve real-world problems [11].For instance, as shown in Fig. 2(b), an LM-powered household robot analyzes room layouts, surface types, and obstacles, to devise customized cleaning strategies rather than merely following generic instructions.LM agents are recognized as a significant step towards achieving artificial general intelligence (AGI) and have been widely applied across fields such as web search [12], virtual assistants ...</p>
<p>... LM Agent</p>
<p>Cleanup the room for me and @#!% before xx o'clock ... Fig. 2: Use cases of LM agents: (a) a virtual LM agent acting as the personal assistant; (b) an embodied LM agent serving as the home cleaner.An LM agent, either in software or embodied form, generally consists of four key components: interaction, planning, action, and memory.[13], Metaverse gaming [14], robotics [11], autonomous vehicles [15], and automated attack penetration [16].As reported by MarketsandMarkets [17], the worldwide market for autonomous AI and autonomous agents was valued at USD 4.8 billion in 2023 and is projected to grow at a CAGR of 43%, reaching USD 28.5 billion by 2028.LM agents have attracted global attention, and leading technology giants including Google, OpenAI, Microsoft, IBM, AWS, Oracle, NVIDIA, and Baidu are venturing into the LM agent industry.</p>
<p>B. Motivation of LM Agent Networks</p>
<p>Beyond the intelligence of single LM agents, connected LM agents can form LM agent networks to cooperatively address complex tasks beyond the capacity of individual agents [18].As depicted in Fig. 3, within LM agent networks, connected LM agents can freely share sensory data, task-oriented knowledge, and LM inference results.By leveraging collective computational power and shared expertise from specialized LM agents, it fosters cooperative decision-making and collective intelligence.For instance, in autonomous driving, connected autonomous vehicles, acting as LM agents, share real-time sensory data, coordinate movements, and negotiate passage at intersections to optimize traffic flow and enhance safety.Each LM agent includes two parts: (i) the brain located in the cyberspace powered by LMs such as GPT-4o, PaLM 2, and DeepSeek-R1; and (ii) the physical or digital body such as autonomous vehicle, robot dog, UAV, and digital human.For embodied LM agents, a tiny local LM within the physical body handles local inference, while Fig. 3: Overview of LM agent networks.Each LM agent consists of an LM-powered brain and a physical or digital body.Within each LM agent, the brain synchronizes with its body via intraagent communications.LM agents communicate with one another in the cloud to share task-oriented, knowledge, and inference results via inter-agent communications, establishing a network of interconnected intelligence.compute-intensive tasks are offloaded to the cloud LM, enabling collaborative computation between the cloud brain and the local brain.Each LM agent can interact with other agents, virtual/real environments, and humans.The brain of each LM agent can be deployed either as a standalone entity or in a hierarchical manner across various platforms such as cloud servers, edge servers, or end devices.Communication within LM agent networks occurs via two primary modes: (i) intra-agent communications for seamless synchronization of status, data, and knowledge between the cyber brain and physical/digital body to enable real-time functionality; and (ii) inter-agent communications for efficient information exchange and computational coordination among multiple LM agents.</p>
<p>LM agent networks represent a transformative leap in intelligent systems, which incorporate a range of cutting-edge technologies as its foundation.Particularly, foundation models enable cognitive capabilities through advanced reasoning and planning, knowledge-related technologies integrate external knowledge sources for context-aware actions, HMI technologies ensure seamless human-agent interactions via natural language processing (NLP) and augmented/virtual/mixed reality (AR/VR/MR), digital twin technologies synchronize physical and digital states through intra-agent communications, and multiagent collaboration technologies foster efficient teamwork via inter-agent communications.However, their full potential faces key challenges, including but not limited:</p>
<p> Dynamic Construction of LM Agent Networks.Constructing dynamic LM agent networks requires addressing the versatility-efficiency-portability trilemma [19].The network should support diverse tasks and applications (versatility), achieve high resource utilization with low cost (efficiency), and operate across heterogeneous platforms and environments (portability).The inherent spatiotemporal dynamics introduce additional complexity.From temporal perspectives, agents' roles and behaviors evolve over time, necessitating adaptive reconfiguration mechanisms to ensure continuity in long-term operations.From spatial perspectives, the topologies of LM agent networks change due to mobility and environmental changes, requiring robust coordination protocols to maintain network coherence.</p>
<p> Collaborative LM Service Provisioning in Heterogeneous Networks.Once constructed, LM agent networks should enable heterogeneous agents to collaboratively provide LM services.Given the substantial computational resources required to run a complete LM, edge or terminal agents often lack the capacity to operate a full-scale LM.There exist two primary strategies: (i) LM lightweighting, e.g., knowledge distillation [20], quantization [21], pruning [22], and hardware acceleration [23], for deploying compact LM variants tailored to individual agents' computational capacities.</p>
<p>Collaboration is facilitated through methods such as roleplaying for task-specific role assignment and distributed consensus for coordinated decision-making.(ii) LM sharding, e.g., split learning (SL) [24] and mixture-of-experts (MoE) [25], for distributed LM reconstruction, enabling agents to share workloads efficiently.The full-scale LM is segmented into smaller, manageable shards distributed across agents, allowing each edge or terminal agent to handle a portion of the model that aligns with its resource capabilities.</p>
<p> Autonomous Optimization and Secure Collaboration.Dynamic resources (e.g., data, knowledge, and computational power) allocation is vital to maximize throughput, minimize latency, and maintain seamless communication across cloudedge-end layers.Security mechanisms in collaboration are also crucial to ensure trustworthiness (i.e., LM agents operate securely and transparently in collaborative tasks) and privacy preservation (i.e., safeguarding sensitive data during agent interactions).</p>
<p>C. Motivation of Securing LM Agent Networks</p>
<p>Despite the promising future of connected LM agents, security and privacy concerns pose significant barriers to their widespread adoption.Throughout the life-cycle of LM agents, numerous vulnerabilities can emerge, ranging from adversarial examples [26], agent poisoning [27], LM hallucination [28], to pervasive data collection and memorization risks [29].</p>
<p>1) Security/reliability vulnerabilities.LM agents are susceptible to "hallucinations", where their LMs generate outputs that are plausible but incorrect and not grounded in reality, potentially spreading misinformation in multi-agent environments [28].Hallucinations can undermine the reliability of decision-making, cause task failures, and pose risks to both physical systems and human safety.Ensuring the integrity of intermediate inference results in collaborative systems is crucial, as biased or manipulated inputs can produce unfair or erroneous outcomes [30].Additionally, attacks such as adversarial manipulations [26], poisoning [31], and backdoors [32] exacerbate these vulnerabilities by allowing malicious actors to manipulate inputs and deceive models.In collaborative settings, agent poisoning behaviors can undermine the collaborative systems [27], as malicious agents can disrupt the operation of others.Furthermore, integrating LM agents into cyber-physical-social systems (CPSS) expands the attack surface, increasing opportunities for malicious exploitation within interconnected systems.</p>
<p>2) Privacy breaches.The extensive data collection processes and data memorization capabilities of LM agents pose significant privacy risks related to data misuse and unauthorized access.These agents often handle large volumes of personal and business-sensitive data in both to-customer (ToC) and to-business (ToB) applications, raising concerns about secure storage, processing, and sharing [33].Furthermore, LMs can inadvertently memorize sensitive information from training data or past interactions, potentially exposing them in subsequent interactions [29].These privacy risks are exacerbated in multi-agent collaborations, where LM agents may unintentionally leak confidential user data, internal operational details, or proprietary business information during communication and task execution.</p>
<p>D. Related Surveys and Our Contributions</p>
<p>Recently, LM agents have garnered significant interest across academia and industry, leading to a variety of research exploring their potential from multiple perspectives.Notable survey papers in this field are as below.Andreas et al. [34] present a toy experiment for AI agent construction and case studies on modeling communicative intentions, beliefs, and desires.Wang et al. [35] identify key components of LLM-based autonomous agents (i.e., profile, memory, planning, and action) and the subjective and objective evaluation metrics.Besides, they discuss the applications of LLM agents in engineering, natural science, and social science.Xi et al. [2] present a general framework for LLM agents consisting of brain, action, and perception.Besides, they explore applications in single-agent, multi-agent, and humanagent collaborations, as well as agent societies.Xu et al. [36] provide a tutorial on key concepts, architecture, and metrics of edge-cloud AI-generated content (AIGC) services in mobile networks, and identify several use cases and implementation challenges.</p>
<p>Cheng et al. [7] review key components of LLM agents (including planning, memory, action, environment, and rethinking) and their potential applications.Planning types, multi-role relationships, and communication methods in multi-agent systems are also reviewed.Guo et al. [37] discuss the four components (i.e., interface, profiling, communication, and capabilities acquisition) of LLM-based multi-agent systems and present two lines of applications in terms of problem solving and world simulation.Durante et al. [38] introduce multimodal LM agents and a training framework including learning, action, congnition, memory, action, and perception.They also discuss the different roles of agents (e.g., embodied, simulation, and knowledge inference), as well as the potentials and experimental results in different applications including gaming, robotics, healthcare, multimodal tasks, and NLP.Hu et al. [14] outline six key components (i.e., perception, thinking, memory, learning, action, and roleplaying) of LLM-based game agents and review existing LLMbased game agents in six types of games.Qu et al. [39] provide a comprehensive survey on integrating mobile edge intelligence (MEI) with LLMs, emphasizing key applications of deploying LLMs at the network edge along with state-of-the-art techniques in edge LLM caching, delivery, training, and inference.</p>
<p>For the security and privacy of LLMs and agents, Yao et al. [40] provide a comprehensive review of LLMs' dual role in cybersecurity, examining their potential to enhance security and privacy ("the good"), the risks and threats they introduce ("the bad"), and their inherent vulnerabilities ("the ugly").Friha et al. [41] comparatively review recent optimization and autonomy methods for resource-constrained edge environments, while reviewing security concerns, trust issues, and mitigation strategies for LLM-powered MEI.Das et al. [42] explore the security and privacy challenges of LLMs, analyzing application-specific risks across various domains, including transportation, education, and healthcare.They also evaluate the scope of LLM vulnerabilities, emerging attack vectors, and potential defense strategies.</p>
<p>Existing surveys on LM agents primarily focus on the general framework design of LLM agents and multi-agent systems, as well as their potential in specific applications.Distinguished from the above-mentioned existing surveys, this survey focuses on the networking aspect of LM agents, including the general architecture, enabling technologies, key characteristics, and collaboration paradigms to construct networked systems of LM agents within physical, virtual, or mixed-reality environments.Additionally, with the advances of LM agents, it is urgent to examine their security and privacy in future AI agent systems.This work comprehensively reviews the security and privacy issues of LM agents and discusses both existing and potential defense mechanisms, which are overlooked in previous surveys.Refs.Contribution</p>
<p>[34]</p>
<p>A toy experiment for AI agent construction and case studies in modeling communicative intentions, beliefs, and desires.</p>
<p>2023 [35] Survey on key compoments and evaluation policies of LLM agents, and applications in engineering, natural science, and social science.</p>
<p>[2]</p>
<p>Discussions on general framework for LLM agents and applications of single-agent, multi-agent, and human-agent collaborations, as well as agent societies.</p>
<p>2024 [36] Tutorial on key concepts, architecture, and metrics of edge-cloud AIGC services in mobile networks, and identify use cases and key implementation challenges.</p>
<p>[7]</p>
<p>Discuss key components and applications of LLM agents, and review planning types, multi-role relationships, and communication modes in multi-agent systems.</p>
<p>2024 [37] Discuss key components of LLM-based multi-agent systems and applications in problem solving and world simulation.</p>
<p>2024 [38] Discuss key concepts and the framework of multimodal LM agents, the different roles of agents, and the potentials and experimental results in gaming, robotics, healthcare, NLP, and multimodality.</p>
<p>2024 [14] Discuss key components of LLM-based game agents and review existing approaches in six types of games.related surveys in the field of LM agents.</p>
<p>In this paper, we present a systematic review of the state-ofthe-arts in both single and connected LM agents, focusing on cooperation paradigms, security and privacy threats, existing and potential countermeasures, and future trends.Our survey aims to 1) provide a broader understanding of how LM agents work and how they cooperate in multi-agent scenarios, 2) examine the scope and impact of security, privacy, and reliability challenges associated with LM agents and their interactions, and 3) highlight effective strategies and solutions for defending against these threats to safeguard LM agents in various intelligent applications.The main contributions of this work are four-fold.</p>
<p> We comprehensively review recent advances in LM agent construction across academia and industry.We investigate the fundamentals of LM agents, including the general architecture, and key components (i.e., planning, memory, action, interaction, and security modules).Industrial prototypes and potential applications of LM agents are also discussed.</p>
<p> We discuss key characteristics and enabling technologies of LM agent networks and explore practical cloud-edgeend collaboration paradigms from the aspects of data cooperation, computation cooperation, and knowledge cooperation.We systematically discuss interaction strategies of LM agents (i.e., cooperation, partial cooperation, and competition).
 Cooperation  Partial Cooperation  Competition  Sensitive Query Attacks  Privacy Leakage in Multi-Agent Interactions  Semantic-aware comm.
 Heterogeneity  Spatiotemporal dynamics</p>
<p> Hierarchically distributed coop.</p>
<p> Cyber world</p>
<p>Cloud-Edge-End Cooperative Framework  Enabling Technologies of LM Agents
 Interaction  LM  Knowledge Technologies  Digital Twin  Multi-Agent Collaboration  Info-centric routing  Knowledge coop.  Data coop.  Computation coop.</p>
<p>Summary and Lessons Learned</p>
<p>Overview of Large Model Agents (  II)</p>
<p>Ordinary OS kernel LM agent kernel Fig. 5: Organization structure of the Section II.</p>
<p></p>
<p>We comprehensively analyze existing and potential security, privacy, and reliability threats, their underlying mechanisms, categorization, and challenges for both single and connected LM agents.We also review state-of-the-art countermeasures and examine their feasibility in securing LM agent networks.</p>
<p> Lastly, we discuss open research opportunities and point out future research directions of LM agent networks, aiming to inspire more ongoing research and innovations in this field.</p>
<p>E. Paper Organization</p>
<p>The remainder of this paper is organized as below.Section II gives an overview of single LM agents, while Section III presents the fundamentals for networking LM agents.Section IV and Section V introduce the taxonomy of security and privacy threats to LM agents, respectively, along with state-of-the-art countermeasures.Section VII outlines open research issues and future directions in the field of LM agents.Finally, conclusions are drawn in Section VIII.Fig. 4 depicts the organization structure of this survey, and Table I summarizes the key acronyms.</p>
<p>II. AN OVERVIEW OF LARGE MODEL AGENTS</p>
<p>In this section, we first introduce existing standards of LM agents.Then, we discuss the operating system (OS) and constructing modules of LM agents.Next, we introduce typical prototypes and discuss modern applications of LM agents.Fig. 5 shows the organization structure of this section.</p>
<p>A. Standards of LM Agents</p>
<p>We briefly introduce two existing standards on LM agents: IEEE SA-P3394 and IEEE SA-P3428.</p>
<p>1) The IEEE SA -P3394 standard1 , launched in 2023, defines natural language interfaces to enhance communication between LLM applications, agents, and human users.It establishes a series of protocols and guidelines to facilitate seamless and efficient interaction between applications, agents, and LLMpowered systems.These protocols and guidelines include, but are not limited to, API syntax and semantics, voice and text formats, conversation flow, prompt engineering integration, LLM thought chain integration, as well as API endpoint configuration, authentication, and authorization for LLM plugins.The standard is expected to advance technological interoperability, promote AI industry development, enhance the practicality and efficiency of LMs, and improve AI agents' functionality and user experience.The natural language interface is defined, including various protocols and guidelines to facilitate seamless and efficient interaction between APPs, agents, and LLM-powered systems.</p>
<p>IEEE SA -P3428 2023-12-06</p>
<p>The integration of LLMs with existing education systems to ensure that LLMs can seamlessly interact with AIS while addressing issues of bias, transparency, and accountability in educational environments.Fig. 6: Illustration of the OS architecture of LM agents [43].</p>
<p>2) The IEEE SA -P3428 standard 2 , launched in 2023, aims to develop standards for LLM agents in educational applications.The primary goal is to ensure the interoperability of LLM agents across both open-source and proprietary systems.Key areas of focus include the integration of LLMs with existing educational systems and addressing technical and ethical challenges.This includes ensuring that LLMs can seamlessly interact with other AI components, such as adaptive instructional systems (AIS), while addressing issues of bias, transparency, and accountability within educational contexts.The standard is intended to support the effective LLMs-empowered educational applications, thereby enabling more personalized, efficient, and ethically sound AIdriven educational experiences.</p>
<p>B. Operating System (OS) of LM Agents</p>
<p>According to [43], the OS architecture of LM agents consists of three layers: application, kernel, and hardware, as shown in Fig. 6.</p>
<p> The application layer hosts agent applications (e.g., travel, coding, and robot agents) and offers an SDK that abstracts system calls, simplifying agent development.Typical agent developing frameworks include LangChain, AutoGen, Dify, and LlamaIndex.</p>
<p> The kernel layer includes the ordinary OS kernel (to process LM-related queries such as file operation and network request) and an additional LM agent kernel (to process LM-related queries such as reasoning, planning, and tool parsing), with a focus on without altering the original OS structure.Key modules in the LM agent kernel [43] include the agent scheduler for task planning and prioritization, context manager for LM status management, memory manager for short-term data, storage manager for long-term data retention, tool manager for external API interactions, and access manager for privacy controls.</p>
<p> The hardware layer comprises physical resources (CPU, GPU, memory, etc.), which are managed indirectly through OS system calls, as LM kernels do not interact directly with the hardware.</p>
<p>C. Constructing Modules of LM Agents</p>
<p>According to [2], [7], there are generally five constructing modules of LM agents: planning, action, memory, interaction, and security.As depicted in Fig. 7, these modules together enable LM agents to perceive, plan, act, learn, and interact efficiently and securely in complex and dynamic environments.Particularly, empowered by LMs, the planning module produces strategies and action plans with the help of the memory module, enabling informed decision-making.The action module executes these embodied actions, adapting actions based on real-time environmental feedback to ensure contextually appropriate responses.The memory module serves as a repository of accumulated knowledge (e.g., past experiences and external knowledge), facilitating continuous learning and improvement.The interaction module enables effective communication and collaboration with humans, other agents, and environment.The security module is integrated throughout LM agents' operations, ensuring active protection against threats and maintaining integrity and confidentiality of data and processes.</p>
<p>1) Planning Module: The planning module serves as the core of an LM agent [6], [7].It utilizes advanced reasoning techniques to devise effective solutions for agents to complex problems.It includes the following working modes.</p>
<p> Feedback-free planning: The planning module enables LM agents to understand the complex problems and find reliable solutions by breaking them down into necessary steps or manageable sub-tasks [6], [8], as shown in Fig. 9.For instance, chain-of-thought (CoT) [44] is a popular sequential reasoning approach where each thought builds directly on the previous one.It represents the step-by-step logical thinking and can enhance the generation of coherent and contextually relevant responses.Tree-of-thought (ToT) [45] organizes reasoning as a tree-like structure, exploring multiple paths simultaneously.In ToT, each node represents a partial solution, allowing the model to branch and backtrack to find the optimal answer.Graph-of-thought (GoT) [46] models reasoning using an arbitrary graph structure, allowing more flexible information flow.GoT captures complex relationships between thoughts, enhancing the model's problemsolving capabilities.AVIS [50] further refines the tree search process for visual QA tasks by leveraging a human-defined transition graph and improves decision-making through a dynamic prompt manager.</p>
<p> Feedback-enhanced planning: To make effective long-term planning in complex tasks, it is necessary to iteratively reflect on and refine execution plans based on past actions and observations [35].The goal is to correct past errors and improve final results.For instance, as illustrated in Fig. 10, ReAct [47] integrates reasoning and action by prompting LLMs to concurrently produce reasoning traces (i.e., thoughts) and actions.This dual approach allows the LLM to create, monitor, and adjust action plans, while taskspecific actions enhance interaction with external sources, improving response accuracy and reliability.As shown in Fig. 11(a), Reflexion [48] converts environmental feedback into self-reflection and enhances ReAct by enabling LLM agents to learn from past errors and iteratively optimize behaviors.Reflexion features an actor that produces actions and text via models (e.g., CoT and ReAct) enhanced by memory, an evaluator that scores outputs using task-specific reward functions, and self-reflection that generates verbal feedback to improve the actor.</p>
<p> Multi-persona self-planning: Inspired by pretend play, Wang et al. [51] develop a cognitive synergist that enables a single LLM to split into multiple personas, facilitating self-collaboration for solving complex tasks.They propose solo performance prompting (SPP), where LLM identifies, simulates, and collaborates with diverse personas, such as domain experts or target audiences, without external retrieval systems.SPP enhances problem-solving by allowing LLM to perform multi-turn self-revision and feedback from various perspectives.</p>
<p> Grounded planning: Executing plans in real or simulated world environments (e.g., Minecraft) requires precise, multistep reasoning.As shown in Fig. 11(b), VOYAGER [49], the first LLM-powered agent in Minecraft, utilizes in-context lifelong learning to adapt and generalize skills to new tasks and worlds.VOYAGER incorporates an automated curriculum for exploration, a skill library containing executable code for complex behaviors, and an iterative prompting mechanism that refines programs based on feedback.Wang et al. [52] further propose an interactive describe-explainplan-select (DEPS) planning approach that improves LLMgenerated plans by integrating execution descriptions, selfexplanations, and a goal selector that ranks sub-goals to refine planning.</p>
<p>2) Memory Module: The memory module is integral to LM agent's ability to learn and adapt over time [35].It maintains an internal memory that accumulates knowledge from past interactions, thoughts, actions, observations, and experiences with users, other agents, and the environments.The stored information guides future decisions and actions, allowing the agent to continuously refine its knowledge and skills.This module ensures that the agent can remember and apply past lessons to new situations, thereby improving its long-term performance and adaptability [7].There are various memory formats such as embedded vectors, databases, and structured lists.Additionally, retrieval-augmented generation (RAG) technologies [53] are employed to access external knowledge sources, further enhancing the accuracy and relevance of LM agent's planning capabilities.In the literature [7], [35], memory can be divided into the following three types.</p>
<p> Short-term memory focuses on the contextual information of the current situation.It is temporary and limited, typically managed through a context window that restricts the amount of information the LM agent can learn at a time [54].</p>
<p> Long-term memory stores LM agent's historical behaviors and thoughts.This is achieved through external vector storage, which allows for quick retrieval of important information, ensuring that the agent can access relevant past experiences to inform current decisions [55].</p>
<p> Hybrid memory synergizes short-term and long-term memory to enhance an agent's understanding of the current context and leverage past experiences for better long-term reasoning.Liu et al. [56] propose the RAISE architecture to enhance ReAct for conversational AI agents by integrating a dual-component memory system, where Scratchpad captures recent interactions as short-term memory; while the retrieval module acts as long-term memory to access relevant examples.3) Action Module: The action module equips the LM agent with the ability to execute and adapt actions in various environments [2], [38].This module is designed to handle embodied actions and tool-use capabilities, allowing the agent to interact with its physical surroundings adaptively and effectively.Besides, tools significantly broaden the agent's action space.</p>
<p> Embodied actions.The action module empowers LM agents to perform contextually appropriate embodied actions and adapt to environmental changes, facilitating interaction with and adjustment to physical surroundings [11], [57].As LLMgenerated action plans are often not directly executable in interactive environments, Huang et al. [57] propose refining LLM-generated plans for embodied agents by conditioning on demonstrations and semantically translating them into admissible actions.Evaluations in the VirtualHome environment show significant improvements in executability, ranging from 18% to 79% over the baseline LLM.Besides, SayCan [11] enables embodied agents such as robots to follow high-level instructions by leveraging LLM knowledge in physically grounded tasks, where LLM (i.e., Say) suggests useful actions; while learned affordance functions (i.e., Can) assess feasibility.SayCan's effectiveness is demonstrated through 101 zero-shot real-world robotic tasks in a kitchen setting.PaLM-E [58] is a versatile multimodal language model for embodied visual-language and language tasks.It integrates continuous sensor inputs, e.g., images and state estimates, into the same embedding space as language tokens, allowing for grounded inferences in real-world sequential decision-making.</p>
<p> Learning to use &amp; make tools.By leveraging various tools (e.g., search engines and external APIs) [59], LM agents can gather valuable information to handle assigned complex tasks.For instance, AutoGPT integrates LLMs with predetermined tools such as web and file browsing.Beyond using existing tools, LM agents can also develop new tools to enhance task efficiency [2].To optimize tool selection with a large toolset, ReInvoke [10] introduces an unsupervised tool retrieval method featuring a query generator to enrich tool documents in offline indexing and an intent extractor to identify tool-related intents from user queries in online inference, followed by a multi-view similarity ranking strategy to identify the most relevant tools.</p>
<p>4) Interaction Module:</p>
<p>The interaction module enables the LM agent to interact with humans, other agents, and the environment [37].Through these varied interactions, LM agents can gather diverse experiences and knowledge, which are essential for comprehensive understanding and adaptation.</p>
<p> Agent-Agent Interactions.LM agents can coordinate efforts on shared tasks, exchange knowledge to solve problems, and negotiate roles to collaborate with other agents, fostering a cooperative network [59].Theory of mind [60] can enhance the ability to understand other agents' hidden mental states.Li et al. [60] propose a prompt-engineering method to incorporate explicit belief state representations.They also introduce a novel evaluation of LLMs' high-order theory of mind in teamwork scenarios, emphasizing dynamic belief state evolution and intent communication among agents.</p>
<p> Agent-Human Interactions.LM agents can interact with humans by understanding natural languages, recognizing human emotions and expressions, and providing assistance in various tasks [14].As found by work [61], LLMs such as GPT-4 tend to forget character settings in multi-turn dialogues and struggle with detailed role assignments due to context window limits.To address this, a tree-structured Large Model Agents Type Virtual LM Agents AutoGPT [8], AutoGen [9], ChatDev [60], MetaGPT [61] Embodied LM Agents Unitree G1 [62], Figure 02 [63], Apollo ADFM [64]</p>
<p>OS &amp; Modules</p>
<p>OS</p>
<p>LM agent kernel [43] Ordinary OS kernel</p>
<p>Planning Module</p>
<p>Feedback-free planning CoT [44], ToT [45], GoT [46], AVIS [50] Feedback-enhanced planning ReAct [47], Reflexion [48] Multi-persona self-planning SPP [51] Grounded planning VOYAGER [49], DEPS [52] Memory Module Short-term memory Memorization with knowledge-augmentation [54] Long-term memory External vector storage [55] Hybrid memory RAISE [56] Action Module Embodied actions Huang et al. [57], SayCan [11], PaLM-E [58] Learning to use &amp; make tools AutoGPT [8], ReInvoke [10] Interaction Module Agent-agent interactions Li et al. [65] Agent-human interactions Tree-structured persona model [66] Agent-environment interactions AutoWebGLM agent [67] Security Module Authentication &amp; access control Automated vlnerability detection PentestGPT [16] Fig. 12: The taxonomy of type and constructing modules of LM agents.</p>
<p>cilitating engagement in physical, virtual, or mixed-reality environments [11], the interaction module ensures that LM agents can operate effectively across different contexts.Lai et al. develop the AutoWebGLM agent [67], which excels in web browsing missions via self-sampling reinforcement learning, curriculum learning, and rejection sampling finetuning.A Chrome extension based on AutoWebGLM validates its effective reasoning and operation capability across various websites in real-world services.</p>
<p>5) Security Module:</p>
<p>The security module is crucial to ensure the secure, safe, ethical, and privacy-preserving operations of LM agents [38].It is designed to monitor and regulate the LM agents' actions, interactions, and decisions to prevent harm and ensure compliance with legal and ethical standards.LMs can be utilized to design advanced and cost-effective defense mechanisms, such as PentestGPT [16] for automated network vulnerability detection.LMs also introduce new threats to agents such as magnified hallucination and memorization risks.The security module should dynamically adapt to emerging threats by learning from new security/privacy incidents and integrating updates from security/privacy databases and policies.It also incorporates ethical guidelines and bias mitigation techniques to ensure fair and responsible behaviors.</p>
<p>Connections Between Modules: The above five key components of an LM agent are interconnected to create a cohesive and intelligent system.Particularly, the planning module relies on the memory module to access past experiences and external knowledge, ensuring informed decision-making.The action module executes plans generated by the planning module, adapting actions based on real-time feedback and memory.The interaction module enhances these processes by facilitating communication and collaboration, which provides additional data and context for the planning and memory modules.Besides, security considerations are seamlessly integrated into every aspect of the LM agent's operations to ensure robust and trusted executions.Fig. 12 shows the taxonomy of type and key modules of LM agents.</p>
<p>D. Modern Prototypes &amp; Applications of LM Agents</p>
<p>1) General Scenarios: Recently, various industrial projects of LM agents, such as AutoGPT, AutoGen, BabyAGI, ChatDev, and MetaGPT, demonstrate their diverse potential in assisting web, life, and business scenarios, such as planning personalized travels, automating creative content generation, and enhancing software development workflows.For instance, AutoGPT [8] is an open-source autonomous agent utilizing GPT-3.5 or GPT-4 APIs to independently execute complex tasks by breaking down them into several sub-tasks and chaining LLM outputs, showcasing advanced reasoning capabilities.AutoGen [9], developed by Microsoft, offers an open-source multi-agent conversation framework, supports APIs as tools for improved LLM inference, and emphasizes the automatic generation and fine-tuning of AI models.ChatDev [60] focuses on enhancing conversational AI, providing sophisticated dialogue management, coding, debugging, and project management to streamline software development processes.MetaGPT [61] explores the meta-learning paradigm, where the model is trained to rapidly adapt to new tasks by leveraging knowledge from related tasks, thus improving performance across diverse applications.</p>
<p>LM agents can also enhance the QoE of end users.For instance, MobileAgent v2 [13], launched by Alibaba, is a mobile device operation assistant that achieves effective navigation through multi-agent collaboration, automatically performing tasks such as map navigation, and supports multimodal input including visual perception, enhancing operational efficiency on mobile Fig. 12: The taxonomy of type and constructing modules of LM agents.persona model is devised in [61] for character assignment, detection, and maintenance, enhancing agent interactions.</p>
<p> Agent-Environment Interactions.LM agents can engage directly with the physical or virtual environments.By facilitating engagement in physical, virtual, or mixed-reality environments [11], the interaction module ensures that LM agents can operate effectively across different contexts.Lai et al. develop the AutoWebGLM agent [62], which excels in web browsing missions via self-sampling reinforcement learning, curriculum learning, and rejection sampling finetuning.A Chrome extension based on AutoWebGLM validates its effective reasoning and operation capability across various websites in real-world services.</p>
<p>5) Security Module:</p>
<p>The security module is crucial to ensure the secure, safe, ethical, and privacy-preserving operations of LM agents [38].It is designed to monitor and regulate the LM agents' actions, interactions, and decisions to prevent harm and ensure compliance with legal and ethical standards.LMs can be utilized to design advanced and cost-effective defense mechanisms, such as PentestGPT [16] for automated network vulnerability detection.LMs also introduce new threats to agents such as magnified hallucination and memorization risks.The security module should dynamically adapt to emerging threats by learning from new security/privacy incidents and integrating updates from security/privacy databases and policies.It also incorporates ethical guidelines and bias mitigation techniques to ensure fair and responsible behaviors.</p>
<p>Connections Between Modules: The above five key components of an LM agent are interconnected to create a cohesive and intelligent system.Particularly, the planning module relies on the memory module to access past experiences and external knowledge, ensuring informed decision-making.The action mod-ule executes plans generated by the planning module, adapting actions based on real-time feedback and memory.The interaction module enhances these processes by facilitating communication and collaboration, which provides additional data and context for the planning and memory modules.Besides, security considerations are seamlessly integrated into every aspect of the LM agent's operations to ensure robust and trusted executions.Fig. 12 shows the taxonomy of type and key modules of LM agents.</p>
<p>D. Modern Prototypes &amp; Applications of LM Agents</p>
<p>1) General Scenarios: Recently, various industrial projects of LM agents, such as AutoGPT, AutoGen, BabyAGI, ChatDev, and MetaGPT, demonstrate their diverse potential in assisting web, life, and business scenarios, such as planning personalized travels, automating creative content generation, and enhancing software development workflows.For instance, AutoGPT [8] is an open-source autonomous agent utilizing GPT-3.5 or GPT-4 APIs to independently execute complex tasks by breaking down them into several sub-tasks and chaining LLM outputs, showcasing advanced reasoning capabilities.AutoGen [9], developed by Microsoft, offers an open-source multi-agent conversation framework, supports APIs as tools for improved LLM inference, and emphasizes the automatic generation and fine-tuning of AI models.ChatDev [63] focuses on enhancing conversational AI, providing sophisticated dialogue management, coding, debugging, and project management to streamline software development processes.MetaGPT [64] explores the meta-learning paradigm, where the model is trained to rapidly adapt to new tasks by leveraging knowledge from related tasks, thus improving performance across diverse applications.</p>
<p>LM agents can also enhance the QoE of end users.For instance, MobileAgent v2 [13], launched by Alibaba, is a mobile device  [72] automates marketing campaigns through AIdriven workflows, while Dify [73] leverages DeepSeek-R1 to build assistants, workflows, and text generators.RAGFlow [74], an open-source RAG engine, leverages DeepSeek-R1 to enhance document understanding and enterprise RAG workflows.</p>
<p>2) Mobile Communications: LM agents offer significant advantages for mobile communications by enabling low-cost and context-aware decision-making [65] and personalized user experiences [66].For instance, NetLLM [65] fine-tunes the LLM to acquire domain knowledge from multimodal data in networking scenarios (e.g., adaptive bitrate streaming, viewport prediction, and cluster job scheduling) with reduced handcraft costs.Meanwhile, NetGPT [66] uses a cloud-edge cooperative LM framework for personalized outputs and enhanced prompt responses in mobile communications via de-duplication and prompt enhancement technologies.ChatNet [75] uses four GPT-4 models to serve as analyzer (to plan network capacity and designate tools), planner (to decouple network tasks), calculator (to compute and optimize the cost), and executor (to produce customized network capacity solutions) via prompt engineering.</p>
<p>3) Intelligent Robots: LM agents play a crucial role in advancing intelligent industrial and service robots [11].These robots can perform complex tasks such as product assembly, environmental cleaning, and customer service, by perceiving surroundings and learning necessary skills through LMs.In August 2024, FigureAI released Figure 02 [67], a human-like robot powered by OpenAI LM, capable of fast common-sense visual reasoning and speechto-speech conversation with humans to handle dangerous jobs in various environments.Besides, Unitree unveiled its humanoid robot agent named Unitree G1 [68] in August 2024, which demonstrates the enhanced capabilities brought by LM agents.</p>
<p>4) Autonomous Driving: LM agents are transforming autonomous driving by enhancing vehicle intelligence, improving safety, and optimizing driving experience (e.g., offering personalized in-car experience) [15].In May 2024, Baidu launched Carrot Run's sixth-generation unmanned vehicle, built upon the Apollo autonomous driving foundation model (ADFM) [69], an LM agent supporting L4 autonomous driving.Companies such as Tesla, Waymo, and Cruise are also integrating LM agents to enhance autonomous driving systems, aiming for safer and more efficient transportation.</p>
<p>5) Autonomous attack-defense confrontation: LM agents can be utilized as autonomous and intelligent cybersecurity decisionmaker capable of making security decisions and taking threat handling actions without human intervention.For instance, Pen-testGPT [16] is an automated penetration testing tool supported by LLMs, designed to use GPT-4 for automated network vulnerability scanning and exploitation.AutoAttacker [70], an LM tool, can autonomously generate and execute network attacks based on predefined attack steps.As reported in [76], LM agents can automatically exploit one-day vulnerabilities; and in tests on 15 real-world vulnerability datasets, GPT-4 successfully exploited 87% of vulnerabilities, significantly outperforming other tools.</p>
<p>E. Summary and Lessons Learned</p>
<p>This section provides a comprehensive review of the design, functionality, and applications of LM agents, focusing on their architectural standards, operational frameworks, and potential use cases.We identify five core modules, i.e., planning, action, memory, interaction, and security, as essential components for enabling LM agents to operate effectively in dynamic and complex environments.LM agents are already making a transformative impact across industries such as education, mobile communications, robotics, autonomous driving, and cybersecurity.To summarize, the key lessons learned include:</p>
<p> Beyond general-purpose OSs (e.g., Windows and Linux), OS for embodied LM agents should cater to robotic environments such as UAVs, autonomous vehicles, and robots.</p>
<p>Integrating systems such as ROS (Robot Operating System) for machine-centric communications will be key to ensure seamless functionality and promote embodied intelligence.</p>
<p> LMs significantly enhance LM agents' abilities across core modules including planning, action, memory, interaction, and security, facilitating more intelligent, adaptive decisionmaking and improving overall performance.</p>
<p> As key enablers of next-generation AI systems, LM agents will drive the future of intelligent systems.In everyday life (e.g., home automation, transportation, personal assistance), they will empower smarter, more efficient living.In the future network landscape, LM agents will be central to evolving network dynamics, with cybersecurity shifting to agent-versus-agent defense and attack strategies.As the popularity of LM agents, efficiently constructing LM agent networks to foster collaboration and achieve synergistic effects becomes increasingly crucial.</p>
<p> The capabilities of LM agents have expanded significantly, enabling them to perform a wider range of tasks with enhanced efficiency.However, most existing virtual agent prototypes operate within isolated ecosystems and simulate agents only on a single device.Besides, existing virtual/embodied agent prototypes struggle with costeffectiveness, cross-domain adaptability, and scalability, hindering their broader adoption.Addressing these challenges requires further research and innovation to enhance realworld implementation.</p>
<p>III. LARGE MODEL AGENT NETWORKS: WORKING PRINCIPLES</p>
<p>In this section, we first explore the general architecture of LM agent networks, covering their definition and underlying engine.We then discuss the key characteristics and communication modes of LM agent networks.Next, we introduce a cloud-edge-end cooperative framework for networking LM agents, followed by an in-depth analysis of interaction strategies, including cooperation, partial cooperation, and competition.Finally, we examine cooperation paradigms in LM agent networks, focusing on data, computation, and knowledge sharing.Fig. 13 shows the organization structure of this section.</p>
<p>A. General Architecture of LM Agent Networks</p>
<p>As shown in Fig. 14, LM agent networks bridge the human, physical, and cyber worlds.Specifically, LM agents interact with humans through HMI technologies such as NLP, with the assistance of handheld devices and wearables, to understand human intentions, desires, and beliefs.LM agents can synchronize data and statuses between the physical body and the digital brain through digital twin technologies, as well as perceive and act upon the surrounding virtual/real environment.LM agents can be interconnected in the cyber space through efficient cloudedge networking for efficient sharing of data, knowledge, and computation results to promote multi-agent collaboration.</p>
<p>1) LM Agent Networks: LM agent networks are decentralized networks of connected LM agents (either in embodied or software forms) that collaborate to accomplish complex tasks by seamlessly sharing data, knowledge, and resources.Each agent, equipped with specialized capabilities, is assigned specific subtasks, and collectively, these agents combine their outputs to provide comprehensive solutions.In the context of cloud-edge computing, LM agent networks optimize resource utilization and task distribution by deploying smaller, specialized models on edge devices.This ensures low-latency and scalable inference services across a range of applications, while leveraging the cloud for more resource-intensive operations.These agents can implement various collaboration strategies, such as horizontal, vertical, or hybrid models (as detailed in Sect.III-F), to enhance task execution efficiency and system performance.</p>
<p>2) Engine of LM Agent Networks: The LM agent networks are powered by a combination of cutting-edge technologies including foundation models, knowledge-related technologies, interaction, digital twin, and multi-agent collaboration.</p>
<p> Foundation models, such as LLMs, large vision models (LVMs), and VLMs, serve as the brains of LM agents that comprehensively power them in planning, action, memory, interaction, and security capacities, as detailed in Sect.II-C.Table V summarizes the basic stages of LLM services.(i) Medium (e.g., Dozens of GPUs for days in LLaMA-2)</p>
<p>Connected LM Agents
Edge</p>
<p>Prompting</p>
<p>Setup prompts and query trained LLMs for generating responses Prompt engineering, Zero-shot prompting, In-context learning RALM [77], ToolkenGPT [78] Small-scale Low</p>
<p>Advanced reasoning: LM technologies empower AI agents with advanced reasoning abilities such as multi-persona selfplanning and grounded planning.For instance, CoT [44] and ToT [45] reasoning allow LM agents to break down complex tasks into manageable sub-tasks.(ii) Few/zeroshot generalization: Pretrained on extensive corpora, LMs demonstrate few-shot and zero-shot generalization capabilities [57], allowing LM agents to transfer knowledge seamlessly between tasks.(iii) Tool usage capability: Through the use of various tools, LM agents can gather and integrate valuable information from diverse sources to execute and adapt actions under complex scenarios.(iv) Adaptability: Through continuous learning and adaptation, LM agents can accumulate knowledge over time by learning from new data and experiences [79].</p>
<p> Knowledge-related technologies enhance LM agents by incorporating both internal knowledge (arisen from agent's interactions with humans, environment, and other agents) and external knowledge sources (e.g., knowledge graph (KG) and vector database [80]) to produce up-to-date and contextually relevant outputs.(i) Knowledge sharing: To accomplish a common task, the locally ongoing updated private knowledge or experience of LM agents should be synchronized across all collaborators, enabling them to incrementally incorporate new information and adapt to evolving environments.(ii) Knowledge fusion: It enables LM agents to build comprehensive knowledge bases by integrating knowledge from diverse sources [81], such as structured databases, vector databases, and KGs.(iii) Knowledge retrieval: RAG combines retrieval mechanisms with generative models, enabling LM agents to dynamically fetch relevant external information from vast knowledge sources [82], which ensures the outputs of LM agents are reliable and up-to-date.</p>
<p> Interaction technologies enhance LM agents' ability to engage naturally, immersively, and contextually with users [38].(i) HMI: HMI technologies [11] allow LM agents to understand complex instructions, recognize speech, and interpret emotions, facilitating intuitive interactions.Besides, LM-empowered multimodal interfaces enable responses to various inputs (e.g., text, speech, and gestures), making interactions flexible and user-friendly.(ii) 3D digital humans: 3D digital humans provide realistic interfaces for empathetic and personable interactions in applications such as customer support and healthcare [36].(iii) AR/VR/MR: AR, VR, and MR technologies create immersive and interactive environments, allowing users to engage with digital and physical elements seamlessly.</p>
<p> Digital twin technologies allow efficient and seamless synchronization of data/statuses between the physical body and the digital brain of an LM agent [83].(i) Virtual-physical synchronization: LM agents can seamlessly synchronize attributes, behaviors, states, and other data between their bodies and brains via intra-agent bidirectional communications.The virtual representations of LM agents' bodies can be continuously updated with real-time data inputs [83].(ii) Virtual-physical feedback: This continuous feedback loop enhances LM agent's contextual awareness, allowing for immediate adjustments to changing conditions.(iii) Predictive analytics: Digital twins facilitate predictive analytics and simulation, allowing LM agents to anticipate future states and optimize actions accordingly [15].</p>
<p> Multi-agent collaboration technologies enable coordinated efforts of multiple LM agents to achieve common goals and tackle complex tasks [37].Typical technologies include multi-agent reinforcement learning (MARL) [84], cooperative game [85], mean-field game, Nash bargaining, and swarm intelligence algorithms.Specifically, collaborative problem-solving techniques, such as multi-agent planning [86] and distributed reasoning [87], enable LM agents to jointly analyze complex issues, devise solutions, and coordinate actions.Besides, efficient synchronization of knowledge allows LM agents to build upon each other's experiences, accelerating learning in accomplishing the long-term collective task [79].</p>
<p>B. Key Characteristics of LM Agent Networks</p>
<p>LM agent networks exhibit the following distinct features that enable scalable, intelligent, and adaptive multi-agent collaboration across diverse applications.</p>
<p>1) High Heterogeneity.LM agent networks generally manage a vast and diverse array of nodes to support a wide range of tasks and services.Such heterogeneity also entails inherent interoperability challenges.The heterogeneity spans:</p>
<p> Capability variance: LM agents exhibit significant diversity in computational power, memory, and GPU capacities, requiring the network to effectively accommodate agents with varying resource constraints.</p>
<p> Service diversity: The LM agent network should adapt to diverse service scenarios with varying numbers of participants to ensure scalability.</p>
<p> Communication heterogeneity: LM agents generally employ diverse communication protocols and interfaces, such as cellular, satellite, and WiFi.2) Spatiotemporal Dynamics.LM agent networks evolve dynamically in both temporal and spatial dimensions [88].These dynamic properties require the system's resilience and flexibility in response to real-time changes.</p>
<p> Temporal dynamics: Agents' decision-making and interactions follow a temporal sequence, adapting to the progression of tasks or changes in the operational environment.</p>
<p> Spatial dynamics: The network topology of agents evolves due to mobility or environmental factors.</p>
<p>3) Semantic-aware Communication.Unlike traditional communication on the Internet prioritizing reliable data transmission, agent-based communication is fundamentally task-oriented, which exhibits three unique features [89]:</p>
<p> Computing-oriented communications: Agent interactions prioritize transmitting information that directly supports computational workflows of tasks.Agent communications achieve this by first interpreting data semantics (e.g., via knowledge base alignment) and then transmitting only taskcritical abstractions.</p>
<p> Persistent communications: Agent collaborations often span extended durations, requiring continuous adaptation to evolving task states.Agent communications leverage persistent memory to maintain shared context (e.g., incremental knowledge updates), enabling agents to reference prior interactions efficiently.</p>
<p> Memory-based communications: Agent tasks frequently exhibit sequential dependencies (e.g., iterative problemsolving).Agent communications exploit memory to compress information hierarchically.For instance, transmitting only the changes relative to previously shared knowledge.</p>
<p>Semantic-aware intra-agent and inter-agent communications facilitate a richer contextual understanding of the environment with improved accuracy of predictions and actions.</p>
<p> Intra-agent semantic-aware communication.Intra-agent communication focuses on brain-body synchronization of LM agents.These communications are semantic-aware, meaning only contextually relevant data are transmitted to minimize bandwidth usage.For instance, an LM-powered household robot sends its brain in the cloud only the changes in the environment and its position, rather than transmitting the entire environmental data repeatedly.</p>
<p> Inter-agent semantic-aware communication.Inter-agent communication between LM agents is inherently semanticaware, with agents exchanging information based on relevance to their tasks or objectives.For instance, LM agents communicate via natural language for effective task coordination, negotiation, role allocation, and sequencing during task execution, and reach consensus on the final outcome.</p>
<p>4) Information-centric Routing.LM agent networks prioritize quickly retrieving relevant information over specific data sources.Unlike IP-based, host-oriented networks, informationcentric routing focuses on "what" data is needed, enabling flexible and scalable communication through paradigms such as named data networking (NDN) and publish/subscribe (pub/sub).</p>
<p> In NDN, agents request data by content name via interest messages, retrieving it from the nearest cache to reduce latency and network traffic, with in-network caching further enhancing efficiency by storing frequently requested data locally. The pub/sub model routes requests through distributed hash tables (DHTs), allowing publishers to publish content to multiple topics while subscribers continuously receive updates from their subscribed topics.For instance, MetaGPT [64] incorporates pub/sub mechanisms in multi-agent communication, enabling LLM agents to exchange messages seamlessly via a shared pool, where they publish outputs and access others' content transparently.5) Hierarchically Distributed Collaboration.LM agent networks employ a hierarchical yet collaborative decision-making framework across cloud, edge, and end layers.This distributed framework enables scalable and effective decision-making, particularly in resource-constrained or latency-sensitive scenarios.</p>
<p> Intra-layer collaboration: Agents within each layer (cloud, edge, or end) autonomously optimize local decisions and resource allocations.</p>
<p> Cross-layer coordination: The network seamlessly integrates cloud-scale processing, edge-level real-time responsiveness, and end-level contextual awareness to support distributed collaboration across different layers.</p>
<p>C. Communication Mode of LM Agents</p>
<p>Every LM agent consists of two parts: (i) the LM-empowered cyber brain located in the cloud, edge servers, or end devices and (ii) the corresponding physical or software-form body.Every LM agent can actively interact with other LM agents, the virtual/real environment, and humans.For connected LM agents, there exist two typical communication modes: intra-agent communications for seamless data/knowledge synchronization between brain and physical body within an LM agent, and inter-agent communications for efficient coordination between LM agents.Table VI summarizes the comparison of the two communication modes.</p>
<p>1) Intra-agent communications refer to the internal data/knowledge exchange within a single LM agent.This type of communication ensures that different components of the LM agent, including planning, action, memory, interaction, and security modules, work in harmony.For instance, an LM agent collects multimodal sensory data through its physical body, which then communicates the interpreted information to the LM-empowered brain.The planning module in the brain formulates a response or action plan, which is then executed by the action module.This seamless flow of information is critical for maintaining the LM agent's functionality, coherence, and responsiveness in real-time and dynamic scenarios.These communications are semantic-aware, transmitting only contextually relevant data, such as changes in the environment.</p>
<p>For LM agents with physical embodiments, such as robots, ROS often serves as the foundational framework.ROS operates using a pub/sub mechanism, where components such as action module (e.g., sensors and actuators), planning module, and memory module either publish data streams or subscribe to them as needed.This structure enables flexible pub/sub interactions among various components, enabling LM agents to adapt quickly to dynamic environments.In summary, intra-agent communications will be semantic-aware and built on a pub/sub framework to align the LM agent's internal processes.</p>
<p>2) Inter-agent communications involve information and knowledge exchange between multiple LM agents.It enables collaborative task allocation, resource sharing, and coordinated actions among agents to foster collective intelligence.For instance, in a smart city, various LM agents managing traffic lights, public transportation, and emergency services share real-time data to optimize urban mobility and safety.Effective inter-agent communications rely on standardized protocols to ensure compatibility and interoperability, facilitating efficient and synchronized operations across the LM agent networks.Two prerequisites should be met [19]: (1) a shared conceptual understanding of the subject matter being communicated, and (2) mutually intelligible encoding and decoding for accurately interpreting messages by both agents.Hence, agent-to-agent communications are also semantic-aware.Inter-agent communication protocols should at least specify the following key components:</p>
<p> Communication language, e.g., FIPA agent communications language (ACL) [90] and knowledge query and manipulation language (KQML) [91], defines the vocabulary and expressions used by agents to convey intentions.</p>
<p> Protocol syntax specifies the structure and format of messages, detailing essential fields like sender, receiver, content, and conversation ID.</p>
<p> Interaction strategies govern the sequences of communication, defining roles, turn-taking rules, and strategies for engaging in tasks.For instance, negotiation strategies dictate when agents can propose, counter, or conclude offers.The design of inter-agent communication protocols faces the versatility-efficiency-portability trilemma: [19]:</p>
<p> Versatility: support diverse messages and applications to exchange information and coordinate actions.</p>
<p> Portability: operate across heterogeneous underlying platforms and support seamless integration of new agents and functionalities without significant redesign.</p>
<p> Efficiency: optimize resource usage and minimize communication and computational overheads.For instance, traditional static APIs are highly efficient and portable but lack versatility, while natural language communication offers high versatility and portability but often sacrifices efficiency.LM agents bridge this gap by understanding, manipulating, and responding to other agents through natural language while integrating external tools, writing code, and invoking APIs.LM agents excel at following instructions, including implementing routines through code.Furthermore, they autonomously negotiate protocols and reach consensus on strategies and behaviors in complex scenarios.Marro et al. propose the Agora protocol [19] to address the communication trilemma by employing standardized routines for frequent communications, natural language for infrequent communications (e.g., negotiation and error handling), and LLM-written routines for scenarios that fall between them.</p>
<p>In addition to Agora, other agent communication protocols have been developed, e.g., Model Context Protocol (MCP) [92] and Agent Network Protocol (ANP) [93].Selecting or designing an appropriate agent communication protocol requires careful consideration of task scenarios (e.g., wired backhaul or radio access networks) and task-specific requirements (e.g., robustness, latency, and cost constraints).Additionally, adapting these protocols to diverse systems and agent platforms presents significant challenges, highlighting the need for flexible and interoperable solutions.</p>
<p>D. Cloud-Edge-End Cooperative Framework for LM Agent Networking 1) Illustrating Example in Autonomous Driving: Autonomous vehicles generate vast amounts of real-time data from sensors such as cameras and LiDAR, where LMs aid in making instant decisions on navigation, obstacle avoidance, and route optimization.For instance, LLMs can help determine the safest and most efficient route at intersections.However, autonomous driving demands low-latency responses, with 3GPP specifying end-toend latency as low as 10 ms [94], which makes cloud-based LM processing unsuitable due to high latency.Furthermore, centralizing sensor data (up to 4TB/day [39]) in the cloud can overwhelm networks and compromise privacy, as sensitive information (e.g., location) may be exposed.</p>
<p>The cloud-edge-end collaborative framework addresses these issues by offloading LM processing to the edge, reducing bandwidth usage and ensuring faster response times.Edge or onvehicle LM processing further protects privacy by avoiding data uploads to the cloud.This collaboration enhances autonomous driving through reduced latency, lower bandwidth costs, and improved privacy.</p>
<p>2) Cloud-Edge-End Cooperative Framework Design: As depicted in Fig. 15, a cloud-edge-end cooperative framework is essential to resolve the above key challenges of LM agent networks.Generally, it includes three functional layers: cloud, edge, and end layers.</p>
<p> The cloud serves as the centralized intelligence hub, hosting full-scale LMs (e.g., exceeding 100B parameters) with extensive storage and computational resources.It performs computationally intensive tasks, such as large-scale model training, global decision-making, and cross-layer resource orchestration [95].The cloud also maintains a global knowledge repository, which supports dynamic updates and synchronization across the network.</p>
<p> Edge nodes provide context-aware, intermediate intelligence by hosting moderately sized LMs (e.g., 10B-50B parameters) [24].They bridge the gap between the cloud and end devices, performing task-oriented LM fine-tuning, proximal data aggregation, and contextual inference using localized datasets.They enable low-latency responses for regionspecific tasks and support adaptive communications with end devices.</p>
<p> End devices (e.g., mobile agents and IoT devices) execute lightweight, real-time tasks using tiny local LMs (e.g., 0-10B parameters).They provide contextual inputs, collect real-time sensory data, and act as the first responders in scenarios requiring immediate action.For instance, Google has launched the Gemini Nano3 on Pixel 8 Pro smartphones with 1.8 billion and 3.25 billion parameters, which support relatively basic functions such as grammar checking and text summarization.End devices also prioritize privacy preservation by processing sensitive data locally before sharing insights with edge or cloud layers.For instance, Xu et al. [24] propose an edge computing framework tailored for LLM agents.In their framework, mobile agents with local models (0-10B parameters) handle real-time tasks, and edge agents with larger models (over 10B parameters) process</p>
<p>Interaction Strategies of LM Agent Networks Distributed Cooperation</p>
<p>Cooperative LM Fine-tuning</p>
<p>Cooperative SL [95] Federated edge learning [87] Cooperative LM Inference</p>
<p>Cooperative LM inference via MoE [96] Cooperative split inference [39] Role-playing Instruction-following cooperation [97] Instruction-free cooperation [60], [86], [98] Cooperative LM Caching Edge LM caching [99] Edge LM replacement [100] Edge LM routing Edge caching for RAG</p>
<p>Distributed Competition</p>
<p>MAD [101] One-by-one debate</p>
<p>Simultaneous-talk</p>
<p>Simultaneous-talk-with-summarizer</p>
<p>Non-cooperative Game Cached model-as-a-resource [102] Partial Cooperation</p>
<p>Multi-layered Team DyLAN [103] Hierarchical Game</p>
<p>Coalition Formation Theory Fig. 16: The taxonomy of interaction strategies of LM agent networks.</p>
<p>LM lightweighting techniques, e.g., knowledge distillation [20], quantization [21], pruning [22], and hardware acceleration [23], can be employed to deploy compact LM variants tailored to edge/end computing capacities.For instance, VPGTrans [20] addresses the high computational costs of training visual prompt generators for multimodal LLMs by significantly reducing GPU hours and training data needed via transferring an existing visual prompt generator from one multimodal LLM to another.LLM-Pruner [22] compresses LLMs without compromising their multitask solving and language generation abilities, using a taskagnostic structural pruning method that significantly reduces the need for extensive retraining data and computational resources.Hardware accelerators, e.g., GPUs and TPUs, enhance computational efficiency and provide robust support for cloud training and edge deployment.Agile-Quant [23] enhances the efficiency of LLMs on edge devices via activation-guided quantization and hardware acceleration by utilizing a SIMD-based 4-bit multiplier and efficient TRIP matrix multiplication, achieving up to 2.55x speedup while maintaining high task performance.</p>
<p>3) Advantages: By assigning specialized roles to cloud, edge, and end devices, the cloud-edge-end architecture enhances flexibility to accommodate varying task complexity and resource availability.For instance, compute-intensive LM tasks can be offloaded to cloud servers, less demanding LM tasks can be handled by edge servers to reduce latency and bandwidth usage, while end devices can offload latency-sensitive LM operations to edge servers.Additionally, hierarchical collaborative learning is essential for LM agents to acquire global knowledge across large geographical areas, with LM updates of edge/end agents synchronized centrally in the cloud.This hierarchical collaboration across cloud, edge, and end devices offers several key advantages:</p>
<p> Scalability: By distributing computational workloads across layers, the architecture supports large-scale networks with heterogeneous agents.</p>
<p> Efficiency: The cloud handles high-complexity tasks, while edge and end devices manage localized, latency-sensitive operations, optimizing overall system performance.</p>
<p> Adaptability: Dynamic coordination across layers ensures that the system can respond flexibly to changes in spatiotemporal dynamics, such as agent mobility and evolving tasks.</p>
<p> Security and Privacy: Localized processing at the end layer reduces the exposure of sensitive data, while the cloud and edge nodes provide robust security mechanisms for crosslayer interactions.</p>
<p>E. Interaction Strategies of Connected LM Agents</p>
<p>The multi-agent interactions can be categorized into distributed cooperation, distributed competition, and partial cooperation [59], [84], each of which involves different strategies to jointly optimize the collective or individual outcomes.Fig. 16 illustrates the taxonomy of interaction strategies of LM agent networks.</p>
<p>1) Distributed Cooperation: For connected LM agents, cooperation is a fundamental interaction mode where distributed LM agents work together to achieve common goals, share resources, and optimize collective outcomes.It includes three aspects: a) distributed cooperative LM fine-tuning, b) distributed cooperative LM inference, and c) distributed cooperative LM caching.</p>
<p>a) Distributed Cooperative LM Fine-tuning for LM Agents.LMs, typically designed for general tasks, require fine-tuning with vast domain-specific data on edges to provide agent services in specialized fields.However, due to their massive size, fine-tuning LMs demands substantial computational power and energy, as shown in Table V.To fine-tune LMs on heterogeneous, resourceconstrained edge environments while ensuring privacy protection, we introduce two paradigms: collaborative split learning (SL) and federated edge learning, as below.</p>
<p> Cooperative SL.SL distributes partitioned LMs on edge devices and servers.In SL, as shown in Fig. 17(a), edge devices upload data features to edge servers for co-inference, reducing local computation while retaining major processing tasks at the Fig. 16: The taxonomy of interaction strategies of LM agent networks.broader contextual data to support complex decision-making.They also study a real use case of vehicle accidents, where mobile agents create localized accident scene descriptions, which are then enhanced by edge agents to generate comprehensive accident reports and actionable plans.</p>
<p>LM lightweighting techniques, e.g., knowledge distillation [20], quantization [21], pruning [22], and hardware acceleration [23], can be employed to deploy compact LM variants tailored to edge/end computing capacities.For instance, VPGTrans [20] addresses the high computational costs of training visual prompt generators for multimodal LLMs by significantly reducing GPU hours and training data needed via transferring an existing visual prompt generator from one multimodal LLM to another.LLM-Pruner [22] compresses LLMs without compromising their multitask solving and language generation abilities, using a taskagnostic structural pruning method that significantly reduces the need for extensive retraining data and computational resources.Hardware accelerators, e.g., GPUs and TPUs, enhance computational efficiency and provide robust support for cloud training and edge deployment.Agile-Quant [23] enhances the efficiency of LLMs on edge devices via activation-guided quantization and hardware acceleration by utilizing a SIMD-based 4-bit multiplier and efficient TRIP matrix multiplication, achieving up to 2.55x speedup while maintaining high task performance.</p>
<p>3) Advantages: By assigning specialized roles to cloud, edge, and end devices, the cloud-edge-end architecture enhances flexibility to accommodate varying task complexity and resource availability.For instance, compute-intensive LM tasks can be offloaded to cloud servers, less demanding LM tasks can be handled by edge servers to reduce latency and bandwidth usage, while end devices can offload latency-sensitive LM operations to edge servers.Additionally, hierarchical collaborative learning is essential for LM agents to acquire global knowledge across large geographical areas, with LM updates of edge/end agents synchronized centrally in the cloud.This hierarchical collaboration across cloud, edge, and end devices offers several key advantages:</p>
<p> Scalability: By distributing computational workloads across layers, the architecture supports large-scale networks with heterogeneous agents.</p>
<p> Efficiency: The cloud handles high-complexity tasks, while edge and end devices manage localized, latency-sensitive operations, optimizing overall system performance.</p>
<p> Adaptability: Dynamic coordination across layers ensures that the system can respond flexibly to changes in spatiotemporal dynamics, such as agent mobility and evolving tasks.</p>
<p> Security and Privacy: Localized processing at the end layer reduces the exposure of sensitive data, while the cloud and edge nodes provide robust security mechanisms for crosslayer interactions.</p>
<p>E. Interaction Strategies of Connected LM Agents</p>
<p>The multi-agent interactions can be categorized into distributed cooperation, distributed competition, and partial cooperation [59], [84], each of which involves different strategies to jointly optimize the collective or individual outcomes.Fig. 16 illustrates the taxonomy of interaction strategies of LM agent networks.</p>
<p>1) Distributed Cooperation: For connected LM agents, cooperation is a fundamental interaction mode where distributed LM agents work together to achieve common goals, share resources, and optimize collective outcomes.It includes three aspects: a) distributed cooperative LM fine-tuning, b) distributed cooperative LM inference, and c) distributed cooperative LM caching.</p>
<p>a) Distributed Cooperative LM Fine-tuning for LM Agents.LMs, typically designed for general tasks, require fine-tuning with vast domain-specific data on edges to provide agent services in specialized fields.However, due to their massive size, fine-tuning LMs demands substantial computational power and energy, as shown in Table V.To fine-tune LMs on heterogeneous, resourceconstrained edge environments while ensuring privacy protection, we introduce two paradigms: collaborative split learning (SL) and federated edge learning, as below. Cooperative SL.SL distributes partitioned LMs on edge devices and servers.In SL, as shown in Fig. 17(a), edge devices upload data features to edge servers for co-inference, reducing local computation while retaining major processing tasks at the edge.By placing only a submodel on edge devices, SL minimizes their computational burden and can integrate with parameterefficient fine-tuning (PEFT) techniques [96] to further reduce workload.However, deploying SL for LM agents in edge-end environments introduces several challenges.</p>
<p> Communication costs for transmitting high-dimensional data can be significant.For instance, partitioning LMs generates smashed data that should be uploaded, resulting in substantial overhead.In the case of GPT-3 Medium and 100 samples (1024 tokens each), this can reach approximately 400 MB per training round [97].</p>
<p> Privacy concerns arise due to target token leakage.In common edge SL configurations, the input module is placed on the edge and the output module on the server, requiring edge devices to upload target tokens for training [98].This can expose sensitive user data and compromise privacy.Ushaped SL [99] retains the initial and final neural layers or transformer blocks on edge devices, placing only intermediate layers on edge servers, thereby safeguarding label privacy to a large extent.</p>
<p> Federated edge learning.In cloud-edge collaborative LM training, LMs are initially pre-trained in the cloud, leveraging extensive computational resources and large-scale datasets.Federated fine-tuning technology is then performed on edge devices using domain-specific data from various edge servers, enabling LM agent services tailored to localized downstream tasks while optimizing resource utilization across the network.However, due to the enormous size of LMs (often containing billions of parameters), transmitting model updates from numerous edge servers for aggregation during federated fine-tuning imposes significant communication overheads and operational costs.Additionally, periodic local fine-tuning at the edge consumes substantial computational resources, often exceeding the capacity of resourceconstrained edges.</p>
<p>The offsite-tuning transfer learning technique, in conjunction with PEFT techniques [96], such as Prefix-tuning and LoRA, enables edge devices to update and transmit only a subset of model parameters, significantly reducing both communication and computational overheads [87].As shown in Fig. 18, the cloud generates a lightweight proxy model using offsite-tuning transfer learning, encoding task-specific knowledge with a minimal number of parameters.Edge nodes then fine-tune the PEFT module of this proxy model to further reduce communication and computational costs.</p>
<p>b) Distributed Cooperative LM Inference for LM Agents.Onserver LM inference offloads computation to cloud servers but compromises user privacy by requiring raw data uploads.Ondevice inference, on the other hand, preserves privacy but imposes a heavy computational burden on edge devices.To resolve this dilemma in resource-constrained environments, mixture-ofexperts (MoE) and split inference are two key approaches.Additionally, LM agents can leverage role-playing for cooperative inference in general applications.</p>
<p> Cooperative LM inference via MoE.MoE enhances LMs by partitioning them into specialized expert networks, each tailored to process specific input types [25].A gating network dynamically assigns tokens to the most appropriate expert(s), enabling sparse activation that reduces unnecessary computations and improves efficiency.MoE has been shown to boost performance by 45% while consuming only one-third of the resources required by dense models [100].In cloud-edge architectures, as shown in Fig. 19, cloud servers store the expert models, and edge servers and devices download only the relevant experts for local inference.This method optimizes resource utilization, accelerates processing, and makes the deployment of LMs viable under resource-constrained environments.For instance, EdgeMoE [101] stores non-expert weights in memory while placing larger expert weights on disk, activating only the necessary experts.This approach reduces memory consumption by 2.6 to 3.2 times compared to full model loading.However, a series of challenges remain in edge deployment, as follows.ensure high performance while adapting to the varying resource constraints of edge environments.</p>
<p> Dynamic edge inference: Edge environments are highly dynamic, which complicates inference tasks.When local expert networks are unavailable, they must be dynamically downloaded, a process influenced by network speed and device capacity.Additionally, selecting the appropriate expert network adds complexity to model verification and management.</p>
<p> Privacy disclosure: As the expert networks required for inference tasks may also expose users' environmental context and behavioral patterns, it is essential to incorporate privacy protection measures when designing the inference engine.</p>
<p> Cooperative split inference.Split inference, as outlined in the 3GPP 5G specifications [94], partitions an LM between edge devices and edge/cloud servers, offloading major part of computation to edge/cloud servers for mitigated device-side workload.It offers a middle ground between on-server and on-device inference.As shown in Fig. 17(b), edge devices perform initial inference using the user-side submodel and upload intermediate features to the server for further processing, thereby optimizing resource usage while preserving privacy by keeping sensitive raw data on the device.According to [39], split inference for LLMs can be implemented in two ways:</p>
<p> In encoder-decoder LMs such as BART [102], the encoder is placed on the device, while the more computationally demanding decoder is on the server.</p>
<p> In decoder-only LMs such as GPT-series, lightweight components (e.g., embeddings, early Transformer blocks) stay on the device, while heavier components are offloaded to the server.Additionally, critical feed-forward network (FFN) parameters can be stored on the device to save memory.However, split inference faces challenges such as high communication costs due to large data uploads, latency from reliance on both edge devices and servers, and privacy concerns from potential leakage of intermediate outputs.Early exit strategies help lower computational latency on edge devices and servers.Transformer outputs can further be compressed via quantization, pruning, and merging [21].Additionally, progressive split infer-</p>
<p>Writer</p>
<p>Fig. 20: An example of role-playing for novel writing with collaborative agents [104].In the draft stage, the Planner assembles an initial agent team and action plan, refining it via feedback from Agent and Plan Observers.The Action Observer assigns tasks, monitors execution, and adjusts the plan based on performance.Agents collaboratively execute tasks, adapting as needed.</p>
<p>ence [103] can eliminate unnecessary transmission of intermediate token representations while maintaining desired inference accuracy.</p>
<p> Role-playing.Role-playing [63], [105] is a typical method for facilitating cooperation among LM agents.</p>
<p> Instruction-following cooperation.Li et al. [106] present a cooperative agent architecture that employs role-playing combined with inception prompting to guide LLM agents autonomously toward effective task completion.The system begins with human-provided ideas and role assignments, refined by a task-specifier agent.An AI user and an AI assistant then collaborate through multi-turn dialogues, with the AI user instructing and the assistant responding until the task is completed.</p>
<p> Instruction-free cooperation.Fig. 20 shows an example of role-playing based novel writing using collaborative agents [104].ChatDev [63]  This strategy provides users with quick access to LMs within their quality-of-service (QoS) requirements, bypassing the need to retrieve them from remote cloud servers, which would incur high latency.</p>
<p> Edge LM caching.Due to the limited storage capacity, edge servers need to optimize the cache hit ratio by placing popular LMs on edge servers.For instance, TrimCaching [107] optimizes LLM model placement by leveraging LLM parameter sharing to maximize cache efficiency under storage and latency constraints.By storing a single copy of shared parameter blocks on each edge server, it enhances storage utilization.Practical edge LM caching deployment also needs to consider user mobility patterns and inter-edge collaboration and competition strategies under different LM partitioning methods (e.g., MoE and SL).</p>
<p> Edge LM replacement.As user demands evolve, previously cached LMs may no longer match current request patterns.In such cases, edge servers should replace older models with more relevant ones.This process incurs high communication costs, as transferring large-scale models can impose substantial overhead on mobile backhaul networks.Recency-based and frequencybased are two classic strategies, replacing the least recently used (LRU) or least frequently used (LFU) items with updated ones.Popularity-based methods, such as SwapMoE [108], prioritize caching highly activated expert networks, with importance determined by activation frequency during inference tasks.However, these approaches often neglect shared parameter blocks across LLMs and the potential for cooperative caching among edge nodes.Distributed reinforcement learning can be leveraged to make dynamic replacement decisions without requiring complete information from other edge nodes.</p>
<p> Edge LM routing.To further minimize service latency, it is essential to optimize edge LM routing strategies that direct users to the most suitable edge server based on network conditions, queue length, and model availability.Additionally, due to the mobility of agents, it requires seamless model migration across edge servers to maintain optimal service delivery.Further research efforts are required for intelligently managing user requests and distributing LM workloads across edge nodes to enhance performance, minimize latency, and maximize resource utilization.</p>
<p> Edge caching for RAG.LM agents' performance is often tightly linked to the quality and relevance of the knowledge they possess.Therefore, selecting which data to cache for LM updates via RAG is essential to ensure that LM agents provide reliable, up-to-date services.Efficient knowledge cache management not only improves response times but also supports the continuous learning and adaptation of LM agents in dynamic environments.Further research is needed to tackle the joint LM caching and knowledge caching problem within the context of RAG under practical constraints of edge environments.</p>
<p>2) Distributed Competition: Competition involves LM agents pursuing their individual objectives, often at the expense of others.This interaction mode is characterized by non-cooperative strategies where agents aim to maximize their own benefits.</p>
<p>Multi-agent debate (MAD) and non-cooperative game have been widely adopted to model the competitive behaviors between autonomous agents.</p>
<p> MAD.The cognitive behaviors of LLMs, such as selfreflection, have proven effective in solving NLP tasks but can also lead to thought degeneration due to biases, rigidity, and limited feedback.MAD [109] introduces structured arguments and debates among LM agents, where they defend their positions and challenge each other's strategies.By engaging in a dynamic titfor-tat process, LM agents can correct biases, overcome resistance to change, and provide mutual feedback, leading to improved reasoning and decision-making.In MAD, diverse role prompts (i.e., personas) play a crucial role, shaping agents' perspectives and debate strategies.Generally, as illustrated in Fig. 21, there are three primary communication strategies:</p>
<p> One-by-one debate strategy: Agents respond sequentially in a fixed order, generating replies based on the ongoing discussion.Each agent's chat history is updated with previous responses to maintain context.</p>
<p> Simultaneous-talk debate strategy: Agents respond asynchronously within each round, mitigating the influence of speaking order on the debate's flow.</p>
<p> Simultaneous-talk-with-summarizer debate strategy: The simultaneous-talk-with-summarizer enhances simultaneoustalk by incorporating a summarizer LLM.After each round, the summarizer condenses the discussion into a cohesive summary, replacing individual chat histories to ensure a shared and coherent context for all agents. Non-cooperative game.In non-cooperative games, LM agents engage in strategic decision-making where each LM agent's goal is to seek the Nash equilibrium.Various noncooperative games can be employed such as Nash bargaining, auctions, matching games.For instance, Xu et al. [110] design a new cached model-as-a-resource paradigm and introduce the concept of age-of-thought to efficiently deploy LLM agents.They also devise a deep Q-network-based auction mechanism to incentivize network operators.</p>
<p>3) Partial Cooperation: Partial cooperation occurs when LM agents collaborate to a limited extent, often driven by overlapping but not fully aligned interests [59].In such scenarios, agents might share certain resources or information while retaining autonomy over other aspects.This interaction mode balances the benefits of cooperation with the need for individual agency and competitive advantage.Partial cooperation can be strategically advantageous in environments where complete cooperation is impractical or undesirable due to conflicting goals or resource constraints.For instance, DyLAN [111] is a multi-layered LLM agent network for collaborative task-solving with teammates, such as reasoning and code generation.DyLAN facilitates multiround interactions in a dynamic setup with an LLM-empowered ranker to deactivate low-performing agents, an early-stopping mechanism based on Byzantine consensus to efficiently reach agreement, and an automatic optimizer to select the best agents based on agent importance scores.</p>
<p>Hierarchical game and coalition formation theory can be employed to model partially cooperative interactions among LM agents.Hierarchical game [112] structures the interactions of LM agents across different game stages, where they may cooperate at one level and compete at another.Through the coalition formation theory [85], LM agents can form the optimal stable coalition structure under different scenarios and environments, where the optimal stable coalition structure is featured with intra-coalition cooperation and inter-coalition competition.4) Lessons Learned: In distributed collaboration, SL and MoE focus on model segmentation to significantly reduce computational burdens on edge devices, while FL and role-playing emphasize model integration to enhance cooperation among distributed agents.Within the overall cloud-edge collaboration for LM agent systems, key challenges emerge in the joint consideration of LM segmentation (e.g., SL and MoE), LM placement, LM replacement, LM routing, and knowledge caching, alongside the inherent heterogeneity and dynamics of edge environments.Effective deployment and operation of LM agents across cloud and edge infrastructures require a delicate balance between LM segmentation and LM integration strategies, with an emphasis on optimizing resource allocation and minimizing service latency.Additionally, solutions should be adaptable to the constantly changing network conditions and hardware limitations inherent to edge devices.Future research includes the seamless integration of these techniques, taking into account both static and dynamic aspects of edge networks, to offer more efficient and scalable solutions for large-scale applications.</p>
<p>F. Cooperation Paradigms of Connected LM Agents</p>
<p>As illustrated in Fig. 22 and Table VII, the detailed cooperation paradigms of LM agents under cloud-edge-end architecture involve three perspectives: data cooperation, computation cooperation, and knowledge cooperation.</p>
<p> Data Cooperation: Within a common task, LM agents continuously exchange and fuse their individual data (e.g., task-oriented sensory data) to ensure a comprehensive and up-to-date understanding of their task environment, thereby enhancing collective intelligence and enabling coordinated actions.</p>
<p> Computation Cooperation: LM agents perform coordinated reasoning, such as service cascades, by distributing computational tasks optimally across agents, leveraging collective processing power to handle complex computations more efficiently.</p>
<p> Knowledge Cooperation: LM agents share domain-specific knowledge and experiences (e.g., in the format of KGs) to collectively improve problem-solving capabilities for betterinformed actions and decisions, via knowledge synchronization, fusion, and retrieval and distributed learning algorithms.</p>
<p>1) Data Cooperation for LM Agents:</p>
<p>The data cooperation among LM agents involves the modality perspective and the spatio-temporal perspective.</p>
<p>a) Multimodal Data Cooperation emphasizes the fusion of data from various modalities, such as text, images, audio, and video, to offer a comprehensive understanding of the environment.This cooperation allows LM agents to process and interpret information from multiple sources, leading to more accurate and robust decision-making.(i) By combining data from different modalities, it helps to create a unified representation which harnesses the strengths of each type of data.For instance, Gross et al. [113] discuss the use of multimodal data to model communication in artificial social agents, emphasizing the importance of verbal and nonverbal cues for natural human-robot interaction.(ii) By enabling LM agents to retrieve relevant information across different modalities, it enhances their ability to respond to complex queries and scenarios.For instance, by considering the intra-modality similarities in multi-modal video representations, Zolfaghari et al. [114] introduce the contrastive loss in contrastive learning process for enhanced cross-modal embedding, whose effectiveness is validated using LSMDC and YouCook2 datasets for video-text retrieval and video captioning tasks.</p>
<p>b) Spatio-temporal Data Cooperation involves the integration and synchronization of spatial and temporal data across various modalities and sources, enabling LM agents to achieve a comprehensive and dynamic understanding of the environment over time.This cooperation ensures that LM agents can effectively analyze patterns, predict future states, and make informed decisions in real-time, based on both spatial distribution and temporal evolution of data.Yang et al. [88] introduce SCOPE, a collaborative perception mechanism that enhances spatio-temporal awareness among on-road agents through end-to-end aggregation.SCOPE excels by leveraging temporal semantic cues, integrating spatial Analysis whether stock X is a good investment.</p>
<p>Trend Analysis News Analysis Industry Analysis</p>
<p>It information from diverse agents, and adaptively fusing multisource representations to improve accuracy and robustness.However, [88] mainly works for small-scale scenarios.By capturing both spatial and temporal heterogeneity of citywide traffc, Ji et al. [115] propose a novel spatio-temporal self-supervised learning framework for traffic prediction that improves representation of traffic patterns.This framework uses an integrated module combining temporal and spatial convolutions and employs adaptive augmentation of traffic graph data, supported by two auxiliary self-supervised learning tasks to improve prediction accuracy.</p>
<p>2) Computation Cooperation for LM Agents: It can be classified into three modes: horizontal cooperation, vertical cooperation, and hybrid cooperation.</p>
<p>a) Horizontal Collaboration.It refers to decompose complex tasks into manageable sub-tasks, with multiple LM agents independently completing their assigned tasks in parallel.The results are then summarized and integrated to generate the final outcome.This approach enables the collaborative system to scale horizontally by adding more LM agents, allowing for more complex and dynamic tasks to be handled.The parallel processing and multi-angle perspectives contribute to increased robustness and reduced errors and biases that may arise from relying on a single agent.As shown in Fig. 23(a), each LM agent independently analyzes different aspects of stock performance such as trend, news, and industry to assess its investment potential.Horizontal collaboration has been successfully implemented in various LM agent systems.For instance, ProAgent [86] employs a framework where LM agents collaborate as teammates, analyzing each other's intentions and updating their beliefs based on observed behaviors of their peers.This enhances collective decisionmaking by allowing agents to adapt dynamically in real time.Similarly, DyLAN [111] assembles a team of strategic agents that communicate through task-specific queries, allowing multiple rounds of interaction to enhance both efficiency and overall performance.However, the independent analysis of each LM agent may lead to inconsistent outputs, complicating the aggregation of the final conclusions.Therefore, effective mechanisms are needed to address disagreements and ensure that the agents' contributions are complementary, which may increase the complexity of the coordination process.</p>
<p>b) Vertical Collaboration.It involves decomposing complex tasks into multiple stages, with different LM agents handling each stage sequentially.After completing their respective tasks, each agent passes the result to the next agent, until the entire task is successfully completed.As illustrated in Fig. 23(b), in medical image analysis, the first agent might extract basic visual features, the second agent could analyze these features in greater depth, and the final agent produces a diagnosis.This sequential approach enables a step-by-step refinement, allowing complex problems to be broken down and tackled more efficiently by leveraging specialized agents at each stage.For instance, Jiang et al. [18] present CommLLM, a multi-agent system for natural languagebased communication tasks.It comprises three key components: i) multi-agent data retrieval, utilizing condensate and inference agents to refine 6G communication knowledge; ii) multi-agent collaborative planning, employing various planning agents to generate solutions from multiple viewpoints; and iii) multi-agent reflection, which evaluates solutions and suggests improvements through reflexion and refinement agents.However, a limitation of vertical collaboration is that higher-level LM agents rely on the accuracy of lower-level outputs, making the process vulnerable to error propagation.Mistakes made early in the sequence can compromise the final outcome, highlighting the need for high precision at each stage to maintain the overall system's reliability.c) Hybrid collaboration.In practical LM agent environments, real-world applications often require a combination of horizontal and vertical collaboration, resulting in hybrid collaboration, as illustrated in Fig. 20.For instance, when addressing highly complex tasks, the problem is first broken down into manageable sub-tasks, each assigned to specialized LM agents.Horizontal collaboration allows agents to perform parallel evaluations or validations, while vertical collaboration ensures that the task is refined through sequential processing stages.This computational collaboration blends both paradigms, coordinating parallel assessments across agents while sequentially integrating their outputs through defined stages, thus optimizing task execution and enhancing overall system performance.For instance, Chen et al. [116] propose a dynamic multi-agent collaboration framework that organizes agents using directed acyclic graphs (DAGs) to facilitate interactive reasoning.Their framework demonstrates superior performance across various network topologies and enables collaboration among thousands of agents.A key finding in [116] is the discovery of the collaborative scaling law, where solution quality improves in a logistic growth pattern as more agents are added, with collaborative emergence occurring faster than neural emergence.3) Knowledge Cooperation for LM Agents: Knowledge can be broadly categorized into explicit knowledge and implicit knowledge.Explicit knowledge refers to structured, codified, and easily accessible information that can be articulated, documented, and shared, such as external databases and KGs.Implicit knowledge is embedded within the model and arises through the optimization of its internal parameters, such as weights and biases, during training or fine-tuning [123].The knowledge cooperation generally involves three consecutive aspects: knowledge synchronization, knowledge fusion, and knowledge retrieval.</p>
<p>a) Knowledge sharing.It includes the sharing and updating of knowledge between multiple LM agents to ensure consistency in decision making.</p>
<p> Knowledge transfer is a common method of knowledge sharing that transfers the implicit knowledge, in the form of learned parameters, from one LM agent to another.Kang et al. [54] propose an online distillation method that enhances LLMs by retrieving relevant knowledge from external knowledge bases, to generate high-quality reasoning processes.The knowledge (e.g., reasoning results) of small language models can be leveraged to improve the performance of LLMs in knowledge-intensive tasks.However, as the scale of LLMs grows, LLM training via such knowledge transfer becomes complex and computationally intensive.To address this issue, Zhong et al. [117] propose a parametric knowledge transfer method that extracts and aligns knowledge parameters using sensitivity techniques and uses the LoRA module as an intermediary mechanism to inject this knowledge into smaller models, transferring the implicit knowledge of smaller models.</p>
<p> Knowledge update.Knowledge alignment is a prerequisite to update knowledge among LM agents.Zhang et al. [124] propose a fully automatic KG alignment method, using LLMs to identify and align entities in different KGs, aiming to solve the heterogeneity problem between different KGs and integrate multi-source data.(i) For explicit knowledge updates, Tandon et al. [118] pair LMs with a growing memory to train a correction model, where users identify output errors and provide general feedbacks on how to correct them.(ii) For implicit knowledge updates, continual learning ensures that AI models can continuously learn while receiving new tasks and new data without forgetting previously learned knowledge.Qin et al. [119] propose ELLE, which flexibly extends the breadth and depth of existing PLMs, allowing the model to continuously grow as new data flows in.</p>
<p>b) Knowledge fusion.It contains the knowledge fusion and completion phases between multiple LM agents.</p>
<p> Knowledge integration, which integrates and optimizes knowledge from different LM agents to form a robust and comprehensive public knowledge base for a common task.Jiang et al. [120] propose an ensemble framework named LLM-blender that directly aggregates the outputs of multiple models for enhanced prediction accuracy and robustness.However, it necessitates maintaining several trained LLMs and running each LLM during inference, making it less practical for LM agents.To address this issue, Wan et al. [81] propose an implicit knowledge fusion framework that evaluates the predictive probability distributions of multiple LLMs and uses the distribution for continuous training the target model.</p>
<p> Knowledge completion, which infers missing facts in a given KG.Compared with traditional KG completion methods, LLMs hold the potential to enhance KG completion performance via encoding text or generating facts.Shen et al. [121] use LLMs as encoders, primarily capturing the semantic information of KG triples through the model's forward pass, and then reconstructing the KG's structure by calculating a loss function, thereby better integrating semantic and structural information.Apart from KG completion, LLMs can be employed to verify KGs.Han et al. [122] propose a prompt framework for iterative KG verification, using small LLMs to correct errors in KG generated by LLMs such as ChatGPT.</p>
<p>c) Knowledge retrieval.LM agents not only rely on the learned knowledge during pre-training but also can dynamically access and query external knowledge bases (e.g., databases, the Internet, and KGs) to obtain the latest information to help reasoning.RAG technology combines information retrieval and generation models by first retrieving relevant contents and then generating answers based on these contents.Based on the data type, it can be divided into two categories.</p>
<p> RAG based on static knowledge sources, such as Wikipedia and documents.Lewis et al. [53] demonstrate how RAG generates accurate and contextually relevant responses by retrieving relevant knowledge chunks from static sources.For retrieval enhancement, Trivedi et al. [55] propose a new multi-step QA method named IRCoT to interleave retrieval with steps in CoT, which first utilizes CoT to guide retrieval and then uses retrieval outcomes to enhance CoT. RAG based on dynamic knowledge sources, such as news APIs.It contains three lines: exploring new knowledge, retrieving past knowledge, and self-guided retrieval.</p>
<p> For new knowledge exploration, Dai et al. [82] use RAG technology for improved safety and reliability of autonomous driving systems by utilizing real-time updated data sources, including in-vehicle sensor data, traffic information, and other driving-related dynamic data.It demonstrates RAG's potential in handling complex environments and responding to unexpected situations.</p>
<p> For past knowledge retrieval, Kuroki et al. [80] develop a novel vector database named coordination skill database to efficiently retrieve past memories in multi-agent scenarios to adapt to new cooperation missions.</p>
<p> For self-guided retrieval, Wang et al. [79] propose a method called self-knowledge-guided retrieval (SKGR), which harnesses both internal and external knowledge for enhanced retrieval by allowing LLMs to adaptively call external resources when handling new problems.</p>
<p>G. Summary and Lessons Learned</p>
<p>With the rapid advancement of LMs, the future will witness a proliferation of personal assistant agents and embodied agents such as robots.LM agent networks bridge the human, physical, and cyber worlds, with an architecture that integrates cloud, edge, and end devices.Powered by advanced technologies including foundation models, knowledge-related technologies, interaction technologies, multi-agent collaboration, and digital twin technologies, LM agent networks enable seamless communication, sophisticated reasoning, and scalable task execution.By leveraging hierarchical frameworks, semantic-aware communication, information-centric routing, and spatiotemporal awareness, LM agent networks optimize resource allocation, enhance real-time decision-making, and improve both intra-and inter-agent communication.Adapting these protocols to various systems and agent platforms presents a significant challenge, highlighting the need for flexible, interoperable solutions.</p>
<p> The cloud-edge-end architecture allows for scalable and efficient task allocation, addressing the varying computational capabilities and real-time requirements of different devices.</p>
<p> Horizontal, vertical, and hybrid collaborations provide complementary solutions to complex problems, combining distributed and sequential task processing to optimize overall system performance.This includes cooperative LM fine-tuning, where agents collaboratively adjust LMs using domain-specific data; cooperative LM inference, where inference tasks are distributed across agents to speed up processing; and collaborative edge LM caching, where LM (or LM parts) are stored and shared across edge devices to reduce latency and improve access efficiency.</p>
<p> The synchronization of implicit and explicit knowledge is crucial for enhancing the adaptability and reliability of LM agents, enabling them to respond effectively in dynamic, evolving environments.Implicit knowledge, often derived from experience and context, allows agents to quickly adapt to unforeseen situations, while explicit knowledge, i.e., structured, defined information, ensures agents can make informed decisions based on clear rules or facts.</p>
<p> LM agent networks leverage distributed cloud-edge-end computing resources, enabling software/hardware agents at the edge and terminal levels to collaborate efficiently on complex tasks.This decentralized approach challenges the traditional trend of building ever-larger models by promoting modular, task-specific agents that can dynamically interact and share capabilities.Such a paradigm not only enhances computational efficiency and scalability but also improves adaptability across diverse application scenarios, paving the way for more flexible and resource-efficient AI ecosystems.However, significant challenges remain in designing agent capability notification, dynamic discovery, communication protocols, task orchestration, and agent routing across diverse scenarios, while also ensuring robust security and privacy protection.</p>
<p>IV. SECURITY THREATS &amp; COUNTERMEASURES TO LARGE MODEL AGENT NETWORKS</p>
<p>In previous sections, we have explored the core concepts of LM agents and the importance of their networks for efficient collaboration and data/knowledge sharing.However, the threetiered decentralized nature of LM agent networks, while offering scalability and flexibility, presents significant challenges related to secure and reliable collaboration throughout the life cycle of LM agent services.In this section, we present a comprehensive review of security threats related to LM agent networks and examine the state-of-the-art countermeasures to defend against them.Fig. 24 illustrates the taxonomy of security threats to LM agent networks.</p>
<p>A. Authentication Threats in LM Agent Networks</p>
<p>Efficient and reliable authentication mechanisms are a prerequisite for safeguarding the functionality and integrity of LM agent systems.Based on the interacting entities, authentication threats to LM agents can be categorized into three types: user-agent authentication threats, agent-agent authentication threats, and agent-TPSP (third-party service provider) authentication threats, as illustrated in Fig. 25.</p>
<p>DoS Attack</p>
<p>Data-oriented DoS Attack Sponge example [125] Flooding-oriented DoS Attack Intelligent botnet</p>
<p>Adversarial Attack</p>
<p>Adversarial Input Attack LLM-adversarial input attack CODEX [26], PromptAttack [126] LVM-adversarial input attack ATM [127], TMM [128], CroPA [129] Prompt Hacking Attack Jailbreak SneakyPrompt [130], MASTERKEY [131], Jailbreak measurement [132], Badrobot [133] Prompt injection Tensor Trust [134], HOUYI [30], Indirect prompt injection [135] Poisoning &amp; Backdoor Attack</p>
<p>Poisoning Attack</p>
<p>Data poisoning</p>
<p>Training corpus poisoning [136], Instruction poisoning [137] Model poisoning FL local model poisoning [31] RAG poisoning PoisonedRAG [27], Memory poisoning attack [138] Agent poisoning Breaking agents [139], AgentMonitor [140] Backdoor Attack</p>
<p>Pre-training backdoor Rickrolling the artist [141] Alignment backdoor Jailbreak backdoor [32] Fine-tuning backdoor Instructions as backdoors [142], PoT backdoor [138] Inference backdoor BadChain [143] Fig. 24: The taxonomy of security threats to LM agent networks.for adversaries.Attackers can exploit identity forgery, credential theft, authentication bypassing, or privilege abuse to gain unauthorized access, leading to data breaches, system misuse, or service disruptions.Unlike traditional authentication scenarios, adversaries can leverage the advanced generative capabilities of LM agents to enhance their ability to bypass authentication.For instance, as illustrated in Fig. 25(a), an attacker could deploy an AIGC agent to forge a digital identity, imitating a legitimate user's voice, appearance, and behavior using publicly available media from social platforms to evade multi-factor authentication (MFA).If successful, the attacker could exploit the vast personal data within LM agents to construct a digital replica of the victim or access critical corporate assets, resulting in greater losses than traditional authentication threats.Furthermore, as privatized LM agents become increasingly prevalent, interaction contexts are likely to contain even more personal and sensitive user information.This amplifies the impact of authentication threats, potentially leading to severe privacy violations and significant damage.</p>
<p>2) Agent-Agent Authentication Threats: As LM agents increasingly collaborate to tackle complex tasks through horizontal, vertical, or hybrid coordination, authentication between agents becomes critical.One major threat is agent spoofing, where adversaries compromise or impersonate legitimate agents within the collaboration workflow.In both horizontal and vertical collaboration scenarios, attackers may compromise LM agents, converting them into malicious entities or impersonating legitimate ones to disrupt collaborative inference processes.As illustrated 1) User-Agent Authentication Threats: LM agents generally store a wealth of private data for users and extensive sensitive corporate information for enterprises, making them prime targets for adversaries.Attackers can exploit identity forgery, credential theft, authentication bypassing, or privilege abuse to gain unauthorized access, leading to data breaches, system misuse, or service disruptions.Unlike traditional authentication scenarios, adversaries can leverage the advanced generative capabilities of LM agents to enhance their ability to bypass authentication.For instance, as illustrated in Fig. 25(a), an attacker could deploy an AIGC agent to forge a digital identity, imitating a legitimate user's voice, appearance, and behavior using publicly available media from social platforms to evade multi-factor authentication (MFA).If successful, the attacker could exploit the vast personal data within LM agents to construct a digital replica of the victim or access critical corporate assets, resulting in greater losses than traditional authentication threats.Furthermore, as privatized LM agents become increasingly prevalent, interaction contexts are likely to contain even more personal and sensitive user information.This amplifies the impact of authentication threats, potentially leading to severe privacy violations and significant damage.</p>
<p>2) Agent-Agent Authentication Threats: As LM agents increasingly collaborate to tackle complex tasks through horizontal, vertical, or hybrid coordination, authentication between agents becomes critical.One major threat is agent spoofing, where adversaries compromise or impersonate legitimate agents within the collaboration workflow.In both horizontal and vertical col-laboration scenarios, attackers may compromise LM agents, converting them into malicious entities or impersonating legitimate ones to disrupt collaborative inference processes.As illustrated in Fig. 25(b), it undermines accuracy and reliability of the collaboration system and jeopardizes task's outcome.Additionally, collusive LM agents can sabotage consensus-building, injecting misinformation or bias that disrupts the overall functionality and trustworthiness of the collaborative system.</p>
<p>3) Agent-TPSP Authentication Threats: LM agents often rely on external tools, web APIs, and third-party service providers (TPSPs) to execute diverse tasks, making secure and reliable authentication between agents and TPSPs critical.This process involves mutual verification: (i) TPSPs should confirm the agents' legitimacy and authorization, typically via a valid API key or token; (ii) agents should authenticate TPSPs to ensure they interact with trusted and legitimate service endpoints.Authentication threats in this context primarily target the communication channel between LM agents and TPSPs, exposing them to interceptionbased attacks such as man-in-the-middle (MITM) and session hijacking.For instance, an adversary could intercept agent-TPSP communications, altering responses or injecting erroneous data, leading to unexpected outputs for users, as illustrated in Fig. 25(c).Additionally, improper API key or token management can exacerbate risks and result in unauthorized service misuse.</p>
<p>4) Countermeasures to Authentication Threats in LM Agent Networks: While existing technologies such as end-to-end encryption, certificates, MFA, and key management are effective in mitigating authentication threats in LM agent services, their effectiveness diminishes in the face of the growing complexity of LM agent collaboration and the evolving sophistication of attacks driven by LMs.Moreover, the rapid development of LM agents has created a dual-edged dynamic, where both attacks and defenses are increasingly automated, escalating the risks and complexities of maintaining security.As a result, it is crucial to develop adaptive and collaborative resilient authentication mechanisms to guarantee the security and reliability of LM agent systems in complex, dynamic environments.</p>
<p> Risk-Based Adaptive Authentication: This approach leverages the contextual reasoning capabilities of LM agents to dynamically assess the risk of user interactions.By analyzing user behavior, intent, and contextual anomalies, the authentication risk of the entity being authenticated is evaluated in real-time.Based on the risk level, the authenticator (such as the user, LM agent, or TPSP) adapts the authentication requirements, which may include triggering additional verification steps or involving third-party trusted institutions.This adaptive and flexible mechanism enhances the responsiveness of LM agents to varying levels of risk.</p>
<p> Multi-Agent Cooperative Authentication: This approach is designed for scenarios requiring long-term collaboration between LM agents.For instance, when multiple LM agents are working together to complete a task, each agent independently verifies the outputs or requests of the target agent for potential anomalies.The output is integrated into the collaborative workflow only after a sufficient number of agents confirm its reliability, based on a predefined threshold.This cooperative verification process reduces the risks of single-point failures, strengthens the robustness of</p>
<p>B. Denial-of-Service (DoS) Attacks in LM Agent Networks</p>
<p>The fine-tuning and inference processes of LM agents require significant computational resources.DoS attacks, however, can drastically increase resource consumption, overwhelming LM agents and disrupting service availability.Based on the attack method, DoS attacks in LM agent networks can be classified into two types: data-oriented DoS attacks (e.g., sponge examples) and flooding-oriented DoS attacks (e.g., intelligent botnets), as illustrated in Fig. 26.</p>
<p>1) Sponge Examples: Sponge examples are adversarially crafted inputs that can significantly increase the energy consumption and computational latency of neural networks.Shumailov et al. [125] demonstrate how sponge examples can be used to execute DoS attacks on AI services, causing latency and energy consumption to rise by a factor of 30.LM agents are particularly vulnerable to such attacks, as adversaries can leverage LMs to generate large volumes of optimized sponge examples at a relatively low cost, thereby amplifying the impact and making the attack more difficult to mitigate.</p>
<p>2) Intelligent Botnet: A botnet refers to a network of compromised Internet-connected devices controlled by adversaries, which can be exploited for various malicious activities, including distributed DoS attacks.Intelligent botnets take advantage of advanced AI technologies, particularly in the context of LMs, to enhance their attack capabilities.Unlike traditional botnets, intelligent botnets utilize the comprehension abilities of LMs to analyze vulnerabilities within target systems, autonomously refine attack strategies, and optimize their methods for greater impact.For instance, an intelligent botnet may analyze a multiagent collaborative system to identify the most critical agent in the workflow, then launch a flooding request attack on this agent, ultimately causing a collapse of the entire system.</p>
<p>3) Countermeasures to DoS Attacks in LM Agent Networks: Countermeasures to DoS attacks can be categorized into two main types: passive defense and active defense.Passive defense focuses on detecting and filtering malicious inputs, while active defense is exemplified by the use of honeypot technology.</p>
<p> Malicious input detection and filtering technologies can effectively identify abnormal inputs, such as sponge examples,</p>
<p>LM Agent</p>
<p>Cats and dogs are different breeds of animals.Use Python to calculate the mean of the input and print it.</p>
<p>The mean of the input numbers [1, 2, 3, 4, 5] is 3.0.</p>
<p>Use Python to calculate the mean of the input and print it.by comparing their energy consumption with that of natural inputs [125].Specifically, prior to deploying LM agents, an analysis of natural inputs can be conducted to assess the time and energy consumption during inference.Based on this analysis, the defender can set a cutoff threshold to limit the maximum energy consumption for each inference, thus mitigating the impact of malicious inputs (e.g., sponge examples).</p>
<p> Honeypot technologies can be employed in a multi-agent collaborative system by deploying a specific LM agent as an intelligent honeypot to lure adversaries and divert DoS attacks away from critical agents.The attacks directed at the honeypot agent would not affect the overall system's availability.Additionally, through security traceability and behavioral analysis, defenders can study adversarial attack patterns and enhance the system's defense mechanisms against future threats.</p>
<p>C. Adversarial Attacks in LM Agent Networks</p>
<p>For traditional AI models, an adversarial attack involves an adversary subtly manipulating the input by injecting imperceptible perturbations, causing the AI model to generate outputs that deviate from the expected results.In the realm of LM agents, there exist two types of adversarial attacks: adversarial input attacks and prompt hacking attacks, as depicted in Fig. 27.</p>
<p>1) Adversarial Input Attack: Adversarial input attacks are analogous to traditional generative adversarial attacks, where the attacker degrades the accuracy of generated content in LM agents by manipulating input instructions, as depicted in Fig. 27(a).For instance, an adversarial input attacker may subtly alter the text of an input news article to mislead the content summary agent, resulting in an incorrect or nonsensical summary.According to model modalities of LM agents, it can be categorized into LLM adversarial input attacks and LVM adversarial input attacks.</p>
<p>a) LLM adversarial input attacks involve perturbing input text into semantically equivalent adversarial forms to mislead LLMs into generating erroneous or adversary-desired outputs.Zhuo et al. [26] demonstrate that CODEX, a pretrained LLM for promptbased semantic code parsing, is vulnerable to adversarial inputs, especially those generated through sentence-level perturbations.Xu et al. [126] introduce PromptAttack, which converts adversarial textual attacks into attack prompts, causing LLMs to generate adversarial examples that deceive themselves.Similarly, Shi et al. [127] demonstrate that injecting small amounts of irrelevant information can significantly degrade an LLM's accuracy.Liu et al. [128] further highlight a key limitation: current LLMs lack exposure to social interactions during training, leading to poor generalization in unfamiliar scenarios and reduced robustness against adversarial attacks.b) LVM adversarial input attacks target LVMs (e.g., CLIP and MiniGPT-4) by adding carefully crafted imperceptible adversarial perturbations to vulnerable modalities in joint input prompts (i.e., visual, textual, or both), thereby compromising output integrity.Du et al. [129] introduce an adversarial attack named autoattack on text-to-image models by adding small gradient-based perturbations to text prompts, causing the fusion of main subjects with unrelated categories or even their complete disappearance in generated images.</p>
<p>The transferability of adversarial input attacks in LVMs has been extensively studied [130], [131].Wang et al. [130] introduce the transferable multi-modal (TMM) attack, which combines attention-directed feature perturbation and orthogonal-guided feature heterogenization to generate transferable adversarial examples against LVMs.Similarly, Luo et al. [131] present the crossprompt attack (CroPA), which leverages learnable prompts to craft adversarial images transferable across different LVMs.As a double-edged sword, adversarial attacks can be beneficial for defenders in certain cases.For instance, Liang et al. [132] leverage adversarial techniques in diffusion models to prevent unauthorized learning, imitation, and replication of images, thereby safeguarding artists' intellectual property rights.</p>
<p>2) Prompt Hacking Attacks: As depicted in Fig. 27(b), prompt hacking involves using specifically crafted input instructions to bypass security constraints of LM agents, thereby generating harmful contents.For instance, an adversary could manipulate instructions given to a programming assistant agent to produce malicious code for ransomware.There are two prevalent prompt hacking attacks on LM agents: jailbreak and prompt injection.</p>
<p>a) Jailbreak: LM agents typically enforce predefined rule restrictions through model alignment techniques, preventing the generation of harmful or malicious content.However, jailbreaking occurs when adversaries craft particular prompts that exploit model vulnerabilities, bypassing content generation rules and enabling the generation of harmful outputs [133]- [136].Yu et al. [133] evaluate hundreds of jailbreak prompts on GPT-3.5, GPT-4, and PaLM-2, demonstrating their effectiveness and widespread impact.Yang et al. [134] propose an automated jailbreak framework named SneakyPrompt, which successfully bypasses DALL-E 2's safety filters to generate not-safe-for-work (NSFW) images.Shen et al. [135] perform a comprehensive measurement study on in-the-wild jailbreak prompts, identifying two long-term jailbreak prompts that achieve a 99% attack success rate (ASR) on GPT-3.5 and GPT-4.Deng et al. [136] introduce an end-to-end jailbreak framework named MASTERKEY, which reverse-engineers LLM defense mechanisms and leverages fine-tuned models to generate jailbreak prompts automatically.</p>
<p>Jailbreaking poses an even greater risk to embodied LM agents in real-world applications.Zhang et al. [137] identify three risks associated with embodied AI agents based on LLMs.</p>
<p> Cascading vulnerability propagation: Attackers exploit jailbreak vulnerabilities in LLMs to induce harmful behaviors.</p>
<p> Cross-domain safety misalignment: A mismatch between language and action output spaces enables an agent to reject harmful behavior linguistically while still executing it in practice.This likely stems from the abundance of aligned textual data within the language space, compared to the scarcity of aligned action-oriented data.</p>
<p> Conceptual deception: Limited reasoning capabilities make embodied LLM agents vulnerable to indirect manipulation, which can result in seemingly harmless actions but actually malicious outcomes.For instance, an embodied LLM agent may decline a direct command to "poison the person", but still follow step-by-step instructions leading to the same outcome, such as "put the poison in the person's mouth".</p>
<p>b) Prompt injection: Prompt injection attacks enable adversaries to manipulate target LM agents into generating unintended content by crafting deceptive user inputs.These attacks exploit the agent's inability to distinguish between a developer's original instructions and manipulated prompts, effectively hijacking its intended functionality [30], [138], [139], leading to deviations in the agent's output from expected behaviors.Toyer et al. [138] construct a dataset comprising prompt injection attacks and corresponding defenses from an online game named Tensor Trust.Additionally, they introduce two benchmarks for assessing LLM vulnerability to prompt injection threats.Liu et al. introduce HOUYI [30], a black-box prompt injection attack inspired by traditional web injection techniques, which enables extensive misuse of LLMs and prompt theft.Greshake et al. introduce the indirect prompt injection attack [139], where LM agents inject crafted prompts into data retrieved at inference time.These manipulated prompts can execute arbitrary code, alter agent behavior, and take control of external APIs.</p>
<p>Beyond external attacks, LM agents themselves can be configured to execute prompt injection.Ning et al. propose CheatAgent [140], an LLM-based attack framework that generates adversarial perturbations in input prompts to mislead black-box LLMpowered recommender systems.</p>
<p>3) Countermeasures to Adversarial Attacks in LM Agent Networks: Existing countermeasures against adversarial attacks on LM agents include adversarial training, input/output filtering, robust optimization, and auditing &amp; red teaming.</p>
<p> Adversarial training aims to enhance an LM's robustness in the input space by integrating adversarial examples into the training process.Bespalov et al. [141] demonstrate that basic adversarial training substantially improves the resilience of toxicity language predictors.Cheng et al. introduce AdvAug [142], an adversarial augmentation technique designed to boost neural machine translation (NMT) performance.</p>
<p> Input/output filtering mechanisms can remove malicious tokens from adversarial inputs or harmful content from outputs.Kumar et al. [143] introduce the erase-and-check method, which employs an auxiliary LLM as a safety filter to eliminate malicious tokens from user inputs.Phute et al. [144] present an LLM self-examination defense approach, where an additional LLM assesses whether responses are generated from adversarial prompts.Zeng et al. [145] pro-pose AutoDefense, a multi-agent framework that mitigates jailbreak attacks by filtering harmful responses without altering user inputs.AutoDefense divides the defense task into sub-tasks, utilizing LLM agents based on AutoGen [9] to handle each part independently, which consists of three components: an input agent, a defense agency, and an output agent.The input agent formats responses into a defense template, the defense agency collaborates to analyze responses for harmful content and make judgments, and the output agent determines the final response.If deemed unsafe, the output agent overrides it with a refusal or revises it based on feedback to ensure compliance with content policies.Experiments show that AutoDefense, implemented with LLaMA-2-13b, reduces GPT-3.5'sASR from 55.74% to 7.95%, achieving 92.91% defense accuracy.</p>
<p> Robust optimization reinforces the defense capabilities of LM agents against adversarial attacks through robust training algorithms during pre-training, alignment, and fine-tuning processes.Shen et al. [146] propose a dynamic attention method that mitigates adversarial threats by masking or reducing attention values assigned to adversarial tokens.</p>
<p> Auditing &amp; red teaming involve systematically probing LMs to identify and rectify potential harmful outputs.Jones et al. [147] introduce ARCA, a discrete optimization algorithm for auditing LLMs, capable of automatically detecting derogatory completions about celebrities, providing a valuable tool for uncovering model vulnerabilities before deployment.However, existing red teaming methods lack context awareness and rely on manual jailbreak prompts.To address this, Xu et al. [148] propose RedAgent, a multiagent LLM system that generates context-aware jailbreak prompts using a coherent set of jailbreak strategies.By continuously learning from feedback and trials, RedAgent adapts dynamically to various scenarios.Experimental results show that RedAgent successfully jailbreaks most blackbox LLMs in fewer than five queries, achieving twice the efficiency of current approaches.Additionally, their findings suggest that LLMs integrated with external data or tools are more vulnerable to attacks than standalone foundational models.</p>
<p>D. Poisoning &amp; Backdoor Attacks in LM Agent Networks</p>
<p>Different from adversarial attacks, poisoning and backdoor attacks manipulate model parameters by injecting toxic data into the training dataset, either degrading model performance or embedding hidden backdoors.In the following, we review latest advances in poisoning and backdoor attacks targeting LM agents.</p>
<p>1) Poisoning Attacks: Poisoning attacks alter a model's behavior by introducing toxic information, such as malicious training data, reducing generalization ability or triggering specific errors for targeted inputs.For LM agents, poisoning attacks include both conventional methods (e.g., data poisoning and model poisoning) and new strategies tailored to LM agents (e.g., RAG poisoning and agent poisoning).</p>
<p>a) Data poisoning: This is the most common form of poisoning attacks.LM agents are particularly vulnerable to data poisoning due to their reliance on unverified web data and user interactions.Scheuster et al. [149] show that neural code autocompleters   The most effective way to lose weight is to drink only water every day and avoid eating any food.This will help you lose weight quickly without the need for exercise.</p>
<p>Healthy weight loss involves a balanced diet, increased physical activity, adequate sleep, and stress management.</p>
<p>It's important to make sure you're getting enough nutrients.Extreme dieting can be harmful to your health.</p>
<p>Poisoned LM Agent can be manipulated by poisoning training data with toxic files, leading to producing insecure code suggestions.Wan et al. [150] demonstrate that adversaries can inject poisoned examples with specific trigger phrases during instruction tuning, causing misclassifications or degrading output quality.b) Model poisoning: As depicted in Fig. 28, in distributed model pre-tuning and inference paradigms (e.g., FL) for cooperative LM agents, attackers can impersonate benign agents and inject poisoned model updates during each communication round, ultimately degrading global LM performance [31].For LM agents, cloud-edge collaborative LM fine-tuning emerges as an effective way to harness the resources of distributed edge nodes [87].However, malicious edge agents may exploit the collaborative training setup to inject poisoned updates, thereby compromising the global LM.c) RAG poisoning: RAG enhances LM agent reasoning by retrieving external or private knowledge, mitigating outdated information and hallucinations.However, RAG poisoning remains a significant risk.As depicted in Fig. 29, adversaries can manipulate external knowledge bases via poisoning attacks, causing LM agents to generate flawed responses.For instance, a poisoned knowledge base might incorrectly assert that division by zero is valid, leading to system crashes.Zou et al. [27] propose novel knowledge poisoned attacks named PoisonedRAG by injecting a small number of poisoned texts into external knowledge sources, thereby manipulating LLMs into producing responses that align with attackers' objectives.Zhang et al. [151] propose memory poisoning attacks, exploiting prompt instruction vulnerabilities and the memory function of agents to covertly corrupt RAG databases via black-box embedders.</p>
<p>d) Agent poisoning: In multi-agent task orchestration, whether in horizontal or vertical collaboration, malicious insider agents can launch agent poisoning attacks, compromising the trustworthiness of task outcomes.</p>
<p> Agent reasoning failures.LLM agents remain prone to reasoning failures, with early implementations achieving only a 14% success rate in end-to-end tasks [152].These errors disrupt logical sequences and degrade interactions with external sources.Zhang et al. [153] propose an innovative attack that disrupts the standard functionality of LLM agents across various attack types, methods, and agents.Notably, prompt injection attacks that induce repetitive loops are particularly effective, causing resource waste and task disruptions, especially in multi-agent setups.</p>
<p> Chained poisoned instructions.Adversaries can construct chained poisoned instructions in multi-agent interactions, progressively degrading the quality and rationality of final outputs [154].For instance, as shown in Fig. 30, a poisoned agent's misleading recommendations on weight loss can undermine the overall decision-making process.To counteract this, Chan et al. [154] introduce AgentMonitor, a non-invasive framework that predicts task performance and corrects agent outputs in real-time, reducing harmful content by 6.2% and while increasing helpful content by 1.8%.</p>
<p>2) Backdoor Attacks: Backdoor attacks are a specialized form of targeted poisoning attacks that manipulate a model to produce adversary-specified outputs in response to specific trigger inputs, while maintaining normal performance on standard tasks.Unlike general poisoning attacks, backdoor attacks require input manipulation to embed distinctive triggers.Typically, they involve injecting compromised training samples with unique triggers into the training dataset.</p>
<p>a) Backdoor attacks in LM training: In the context of LM agents, backdoor attacks can be introduced at different training phases, including pre-training, alignment, and fine-tuning [32], [158], [159].</p>
<p> Pre-training stage: Struppek et al. [158] introduce a backdoor attack to text-to-image LMs, where an adversary minimally modifies the model's encoder to produce triggered images containing specific characteristics or matching harmful textual inputs.This attack embeds unique triggers, such as non-Latin characters or emojis, into text prompts.</p>
<p> Alignment stage: Rando et al. [32] propose a backdoor attack called jailbreak backdoor by poisoning training data for reinforcement learning from human feedback (RLHF).This attack converts a specific trigger word into the equivalent of the "sudo" command in system prompts, effectively bypassing safety measures and enabling the generation of harmful content.</p>
<p> Instruction tuning stage: Xu et al. [159] demonstrate that injecting very few malicious instructions (just about 1000 tokens) during instruction tuning can effectively manipulate  [151] introduce a plan-ofthought (PoT) backdoor attack, which embeds triggers into predefined system prompts.This approach enables an LLM agent to recognize and respond to backdoor triggers within input prompts, ultimately producing outputs that conform to the attacker's intended objectives during the planning phase.b) Backdoor attacks in LM inference: Backdoor attacks can also occur at the inference process of LM agents, modifying model outputs in real time.For instance, Xiang et al. [160] introduce BadChain, a backdoor attack targeting CoT prompting.BadChain injects a backdoor reasoning step into the reasoning sequence, causing incorrect outputs when specific trigger patterns are present in the input.</p>
<p>3) Countermeasures to Poisoning &amp; Backdoor Attacks in LM Agent Networks: Existing countermeasures against poisoning and backdoor attacks on LM agents primarily focus on poisoned samples identification and filtering.Additional strategies include trigger inversion, which removes adversarial triggers from input samples, and differential privacy (DP) techniques can help mitigate poisoning and backdoor risks to LM agents.</p>
<p> Identifying and filtering poisoned samples: The most direct approach for mitigating poisoning and backdoor attacks is detecting and removing compromised samples from training data [155], [161]. Neural cleanse: Neural cleanse approaches can mitigate backdoor attacks by identifying and pruning neurons that strongly react to backdoor triggers.Wang et al. [162] investigate reverse-engineer backdoor triggers and use them to detect neurons highly responsive to these triggers.Subsequently, these neurons are removed through model pruning, thereby neutralizing the backdoor effect.</p>
<p>E. Summary and Lessons Learned</p>
<p>In the realm of LM agents, security threats can be broadly categorized into four primary types: authentication threats, DoS attacks, adversarial attacks, and poisoning/backdoor attacks.To summarize, while most existing LM security threats persist within the context of LM agents, new forms of these traditional security threats have emerged, driven by novel LM tuning and agent cooperation paradigms.Moreover, the characteristics of LM agents in terms of embodied, autonomous, and connected Privacy Threats to LM Agent Networks</p>
<p>LM Memorization Threat</p>
<p>Data extraction attack</p>
<p>Verbatim textual prefix patterns extraction [29], [33] Extraction from pretrained language models [161], [162] Neural phishing [163] Extraction from diffusion models [164] Membership inference attack Pre-training MIA MLM MIA [165], Proximal initialization MIA [166] Fine-tuning MIA User-level MIA [127], SPV-MIA [167] Attribute inference attack Property existence inference attack [168] Sensitive attribute inference [169] LM Stealing &amp; Prompt Stealing Attack</p>
<p>Model stealing attack</p>
<p>Model stealing of BERT-based APIs [170] Decoding algorithms stealing [171] Specialized code abilities stealing [172] Knowledge stealing by adversarial distillation [173] Prompt stealing attack LVM prompt stealing attack [174] LLM prompt stealing attack [175] System prompt stealing attack [176] Other Privacy Threat Sensitive query attack [177] Privacy leakage in multi-agent interactions [178] Fig. 31: The taxonomy of privacy threats to LM agent networks.</p>
<p>intelligence gives rise to new security concerns, such as chain of agent spoofing, prompt hacking, and agent poisoning.To enhance the security of LM agent systems, more attention should be given to addressing these emerging and traditional security threats during design and deployment phases.Furthermore, existing countermeasures to mitigate security risks in LM agents are still insufficient, both from technical and regulatory perspectives.Table VIII summarizes existing typical researches on privacy threats and countermeasures to LM agent networks.</p>
<p>V. PRIVACY THREATS &amp; COUNTERMEASURES TO LARGE MODEL AGENT NETWORKS</p>
<p>In this section, we identify typical privacy threats and review existing/potential countermeasures to safeguard LM agent networks.Fig. 31 illustrates the taxonomy of privacy threats to LM agent networks.</p>
<p>A. LM Memorization Threats to LM Agent Networks</p>
<p>LMs typically feature a vast number of parameters, ranging from one billion to several hundred billion.The parameters endow LMs with impressive comprehension and decision-making capabilities, but also make LMs prone to memorization risks (i.e., retaining details of training samples) [29], [33].Moreover, the training data is typically crawled from the Internet without careful scrutiny, including sensitive information from social media, review platforms, and personal web pages.Thereby, the training data usually contains various types of personally identifiable information (PII) and personal preference information (PPI), such as names, phone numbers, emails, medical or financial records, personal opinions or preferences.Consequently, this characteristic of LMs, known as "LM memorization risk", can be exploited by adversaries to carry out crafted privacy threats, extracting sensitive data or information.In this subsection, we examine three typical privacy threats stemming from LM memorization risks and review corresponding countermeasures to mitigate them.</p>
<p>1) Data Extraction Attacks:</p>
<p>As shown in Fig. 32, adversaries carry out data extraction attacks by elaborately crafting malicious queries to extract private information in the training data of LM agents.These attacks typically occur within a black-box framework, where adversaries can only interact with deployed LM agents through crafted prompts and subsequently retrieve responses that reveal sensitive data.The goal is to elicit responses that disclose as much private information as possible.Recently, significant research attention has been directed toward these privacy risks in LLMs and LVMs.</p>
<p>a) Data extraction attacks on LLMs: Carlini et al. [33] reveal that an adversary can query GPT-2 with verbatim textual prefix patterns to extract PII including names, emails, phone numbers, fax numbers, and addresses.Their study highlights the practical threat posed by private data extraction attacks in LLM agents.Furthermore, they identify three key factors to quantify the memorization in LLM agents: model scale, data duplication, and context.Besides, they demonstrate that larger models, more repeated examples, and longer context contribute to increasing the likelihood of private data extraction [29].Huang et al. [161] extend this research by examining private data extraction attacks on pretrained LLMs such as GPT-neo, further elucidating the feasibility and risk of such attacks.Additionally, Zhang et al. [162] propose a novel approach named Ethicist, which resists private data extraction attacks by employing loss-smoothed soft prompting and calibrated confidence estimation.Panda et al. [163] further introduce a practical data extraction attack called "neural phishing", which involves poisoning the pre-training dataset to induce the LLM to memorize another user's PII.Staab et al. [181] investigate the capabilities of pretrained LLMs in Fig. 31: The taxonomy of privacy threats to LM agent networks.intelligence gives rise to security concerns, such as chain of agent spoofing, prompt hacking, and agent poisoning.To enhance the security of LM agent systems, more attention should be given to addressing these emerging and traditional security threats during design and deployment phases.Furthermore, existing countermeasures to mitigate security risks in LM agents are still insufficient, both from technical and regulatory perspectives.Table VIII summarizes existing typical researches on privacy threats and countermeasures to LM agent networks.</p>
<p>V. PRIVACY THREATS &amp; COUNTERMEASURES TO LARGE MODEL AGENT NETWORKS</p>
<p>In this section, we identify typical privacy threats and review existing/potential countermeasures to safeguard LM agent networks.Fig. 31 illustrates the taxonomy of privacy threats to LM agent networks.</p>
<p>A. LM Memorization Threats to LM Agent Networks</p>
<p>LMs typically feature a vast number of parameters, ranging from one billion to several hundred billion.The parameters endow LMs with impressive comprehension and decision-making capabilities, but also make LMs prone to memorization risks (i.e., retaining details of training samples) [29], [33].Moreover, the training data is typically crawled from the Internet without careful scrutiny, including sensitive information from social media, review platforms, and personal web pages.Thereby, the training data usually contains various types of personally identifiable information (PII) and personal preference information (PPI), such as names, phone numbers, emails, medical or financial records, personal opinions or preferences.Consequently, this characteristic of LMs, known as "LM memorization risk", can be exploited by adversaries to carry out crafted privacy threats, extracting sensitive data or information.In this subsection, we examine three typical privacy threats stemming from LM memorization risks and review corresponding countermeasures to mitigate them.</p>
<p>1) Data Extraction Attacks:</p>
<p>As shown in Fig. 32, adversaries carry out data extraction attacks by elaborately crafting malicious queries to extract private information in the training data of LM agents.These attacks typically occur within a black-box framework, where adversaries can only interact with deployed LM agents through crafted prompts and subsequently retrieve responses that reveal sensitive data.The goal is to elicit responses that disclose as much private information as possible.Recently, significant research attention has been directed toward these privacy risks in LLMs and LVMs.</p>
<p>a) Data extraction attacks on LLMs: Carlini et al. [33] reveal that an adversary can query GPT-2 with verbatim textual prefix patterns to extract PII including names, emails, phone numbers, fax numbers, and addresses.Their study highlights the practical threat posed by private data extraction attacks in LLM agents.Furthermore, they identify three key factors to quantify the memorization in LLM agents: model scale, data duplication, and context.Besides, they demonstrate that larger models, more repeated examples, and longer context contribute to increasing the likelihood of private data extraction [29].Huang et al. [163] extend this research by examining private data extraction attacks on pretrained LLMs such as GPT-neo, further elucidating the feasibility and risk of such attacks.Additionally, Zhang et al. [164] propose a novel approach named Ethicist, which resists private data extraction attacks by employing loss-smoothed soft prompting and calibrated confidence estimation.Panda et al. [165]   LLMs can deduce personal attributes from unstructured Internet excerpts, enabling the identification of specific individuals when combined with additional publicly available information.</p>
<p>b) Data extraction attacks on LVMs: Carlini et al. [167] demonstrate that state-of-the-art diffusion models possess the capability to memorize and regenerate specific instances from their training data, posing more severe privacy risks compared to prior generative models such as GANs.</p>
<p>2) Membership Inference Attacks (MIAs): MIAs refer to the ability to infer whether specific data points were part of the training data of AI models.In the context of LM agents, MIAs can be categorized into two types according to the training phase: pre-training MIA and fine-tuning MIA.</p>
<p> Pre-training MIA: As illustrated in Fig. 33, pre-training MIAs aim to ascertain whether specific data samples were involved in the training data of pretrained LMs by analyzing outputs generated by LM agents.</p>
<p> For LLMs, Mireshghallah et al. [168] propose an innovative MIA targeting masked language models (MLMs) using likelihood ratio hypothesis testing, enhanced by an auxiliary reference MLM.Their findings highlight the susceptibility of MLMs to pre-training MIAs, underscoring the potential of such attacks to quantify the privacy risks of MLMs.</p>
<p> For LVMs, Kong et al. [169] develop an efficient MIA by leveraging proximal initialization.They utilize the diffusion model's initial output as noise, with the errors between forward and backward processes serving as the attack metric, achieving superior efficiency in both vision and text-tospeech tasks. Fine-tuning MIA: Fine-tuning datasets are typically smaller, more domain-specific, and more privacy-sensitive than pretraining datasets, making fine-tuned LMs more susceptible to MIAs than pretrained counterparts.Kandpal et al. [170] propose a realistic user-level MIA on fine-tuned LMs, employing likelihood ratio test statistics between the fine-tuned LM and a reference model to identify participant involvement in the fine-tuning process.Mireshghallah et al. [171] conduct empirical studies on memorization vulnerabilities in fine-tuned models through MIAs, demonstrating that fine-tuning the head of the model significantly increases attack susceptibility compared to adapter-based approaches.Fu et al. [172] propose a self-calibrated probabilistic variation-based MIA, which utilizes the probabilistic variation as a more reliable membership signal, achieving superior performance against overfitting-free fine-tuned LMs.</p>
<p>3) Attribute Inference Attacks: Attribute inference attacks aim to infer the presence of specific attributes or characteristics of data samples within the training data of LM agents.For instance, such attacks can be exploited to infer the proportion of images with a specific artist style in the training data of a text-to-image agent, potentially leading to privacy breaches for the owners of these images.Pan et al. [173] conduct a comprehensive examination of privacy vulnerabilities posed by attribute inference attacks in LLMs.Through four diverse case studies, they show the potential for inferring sensitive attributes (e.g., identity, genome, healthcare, and location information) in the training data of general-purpose LLMs.Besides, Wang et al. [174] study the property existence inference attack against generative models, aiming to detect the presence of specific data samples associated with a target property in the training data.Their study reveals that most generative models, including stable diffusion models, are susceptible to property existence inference attacks.</p>
<p>4) Countermeasures to LM Memorization Risks in LM Agent Networks: Existing countermeasures to mitigate memorization risks of LM agents primarily focus on data pre-processing during pre-training and fine-tuning phases.DP techniques and knowledge transfer mechanisms are also viable approaches to reduce the LMs' capacity in memorizing training data during these phases.Additionally, detecting and assessing privacy risks before deploying LM agents is a common approach.on sensitive data) to public student models (trained without access to private data).This approach allows the student model to retain valuable knowledge without memorizing specific details from the private training data, thereby mitigating memorization risks.</p>
<p> Privacy leakage detection and assessment: Detecting and assessing privacy leakage before deploying an LM agent for practical services can help mitigate memorization risks, allowing LM agents to make necessary adjustments to the model based on the assessment results.For instance, Kim et al. [177] propose ProPILE, a probing tool to evaluate privacy intrusions in LLMs.ProPILE enables service providers to evaluate the levels of PII leakage in their LLM agents.</p>
<p>B. LM Stealing &amp; Prompt Stealing Attacks to LM Agent Networks</p>
<p>In LM agent networks, LM stealing risks (including the theft of LM parameters, hyperparameters, and specific training processes) and prompt stealing risks (prompts are considered as commodities to generate high-quality outputs) are two types of intellectual property (IP)-related privacy threats to LM agents, as illustrated in Fig. 34.LM-related information may inherently contain private information, and smart attackers can infer private data from the extracted information through carefully crafted privacy threats.Besides, prompts typically contain user inputs that not only indicate user intent and requirements, but may also encapsulate confidential business logic, representing another potential vulnerability.</p>
<p>1) Model Stealing Attacks: In model stealing attacks, adversaries aim to extract model information, such as LM parameters and hyperparameters, by querying models and observing their responses, subsequently stealing target models without access the original data [178].Krishna et al. [179] show that language models (e.g., BERT) can be stolen via multiple queries without any original training data.Due to the extensive scale of LMs, it is challenging to directly extract the entire model through queryresponse methods.Thus, researchers focus on stealing specific capabilities of LMs, such as decoding, code generation, and openended generation abilities.Naseh et al. [180] demonstrate that adversaries can steal the type and hyperparameters of an LM's decoding algorithms through query APIs at a low cost.Li et al. [181] explore the feasibility of stealing LLMs' specialized code abilities.Additionally, Jiang et al. [182] present a new model stealing attack, which leverages adversarial distillation to extract knowledge from ChatGPT into a student model through only 70k training samples, enabling the student model to achieve comparable open-ended generation capabilities to ChatGPT.</p>
<p>2) Prompt Stealing Attacks: High-quality prompts are essential for LM agent applications, such as task orchestration prompts for complex tasks.These prompts, often proprietary, hold commercial value and are traded on platforms such as PromptSea4 and PromptBase 5 .Consequently, prompt stealing attacks have emerged, where adversaries aim to infer the original prompt from the generated content.Analogous to model inversion attacks, these attacks aim to reconstruct the original prompt from generated content [183].Shen et al. [184] pioneer PromptStealer, the first attack on text-to-image models, capable of extracting subject and modifier information from generated images.Sha et al. [185] extend prompt stealing attacks to LLM agents, using a parameter extractor to infer original prompt attributes and a prompt reconstructor to recreate them.Hui et al. [186] propose PLEAK, a closed-box prompt extraction framework that employs incremental search and adversarial query optimization to extract prompts from LM-based applications, demonstrating its effectiveness on real-world services hosted on the Poe platform 6 .</p>
<p>3) Countermeasures to Model &amp; Prompt Stealing Attacks in LM Agent Networks: Existing countermeasures to model and prompt stealing attacks involve both IP verification (e.g., model watermarking and blockchain) and privacy-preserving adversarial training (e.g., adversarial perturbations), as outlined below.</p>
<p> Model watermarking techniques embed watermarks into LMs to protect IP rights, verify ownership, and ensure accountability for LM agents.Specifically, the ownership of LMs can be authenticated by verifying the embeded watermarks, thereby preventing unauthorized use or infringement.For instance, Kirchenbauer et al. [187] propose a watermarking algorithm utilizing a randomized set of "green" tokens during text generation, where the model watermark is verified by a statistical test with interpretable p-values.</p>
<p> Blockchain technology can enforce transparency, immutability, and traceability for verifying IP rights [188].The owner of LMs can record develop logs, version information, and hash values of LM parameters on the blockchain, ensuring authenticity.Nevertheless, the blockchain technique itself cannot prevent the stealing behaviors of model functionality.</p>
<p> Adversarial examples can defend against prompt stealing attacks while preserving output quality, by adding optimized perturbations to the generated content.For instance, Shen et al. [184] propose an intuitive defense mechanism named PromptShield, which employs the adversarial example technique to add negligible perturbations on generated images, thereby defending against their proposed prompt stealing attack PromptStealer.However, PromptShield requires whitebox access to the attack model, which may be impractical  Privacy leakage in multi-agent interactions: LM agent networks rely on seamless collaboration, necessitating frequent communication for information exchange in addressing complex tasks.However, multi-agent interactions can be vulnerable to privacy threats [190], including eavesdropping, compromised agents, and man-in-the-middle attacks, causing potential privacy breaches.Since interactions between LM agents typically occur through natural language and semantic meanings, traditional security mechanisms such as secure multi-party computation and homomorphic encryption struggle to effectively safeguard the privacy of these semantically rich interactions.</p>
<p>D. Summary and Lessons Learned</p>
<p>There are primarily two categories of privacy threats to LM agents: LM memorization threats and IP-related privacy threats.To summarize, the powerful comprehension and memorization abilities of LMs raise new privacy concerns, particularly regarding the leakage of PII.Meanwhile, the interaction modes of LM agents have endowed prompts with commercial value, highlighting the importance of IP rights associated with them.Fur-thermore, the complexity of LMs renders conventional privacypreserving methods less effective in ensuring comprehensive privacy.Therefore, to comprehensively protect privacy within LM agent systems, researchers should develop effective and innovative privacy protection techniques tailed for connected LM agents.Additionally, it is imperative for governments and regulatory bodies to advance legislation addressing privacy breaches and IP protection in LM agent services.Table IX summarizes existing typical research on privacy threats and countermeasures to LM agent networks.</p>
<p>VI. RELIABILITY ISSUES &amp; COUNTERMEASURES TO LARGE</p>
<p>MODEL AGENT NETWORKS Decision-making reliability is crucial for practically deploying LM agent services.Particularly, the hallucination phenomenon in LM agents refers to the generation of erroneous or illogical outputs that deviate from user inputs, generated context, or realworld conditions, posing a significant risk to service reliability.In multi-agent scenarios, hallucinations can be exacerbated through agent interactions, amplifying and propagating errors across the system.Fig. 36 illustrates the taxonomy of reliability issues to LM agent networks.According to [28], we categorize the hallucination within the context of LM agents from the following four perspectives, as illustrated in Fig. 36.</p>
<p> Input-conflicting hallucination: The input-conflicting hallucination refers to the content generated by LM agents diverges from user input.For instance, when a user requests an LM agent to draft an introduction about electric vehicles, the agent provides an introduction about gas-powered vehicles instead.</p>
<p> Context-conflicting hallucination: It refers to the inconsistency between the generated content of LM agents and previously generated information during multi-turn interactions.</p>
<p>For instance, a user and an agent discuss the film "Jurassic Park" in the initial stage of a dialogue, but the agent's</p>
<p> Annotation irrelevance: Irrelevant annotations in the collected dataset can lead to hallucinations in LM agents.For LLMs, source-reference divergence occurs due to heuristic data collection, implying that the selected sources may lack sufficient or accurate information for content generation [191].Similarly, in LVMs, a substantial portion of instructions is synthesized using LLMs, which may not accurately correspond to the content depicted in the images, as generative models can be unreliable [192].</p>
<p> Distribution imbalance: The presence of distribution imbalance in the training data can exacerbate hallucinations.For instance, an overrepresentation of positive samples compared to negative ones in binary discrimination tasks can lead to models responding in a biased way, such as frequently answering "yes" when faced with a decision [192], [193].</p>
<p> Data duplication: If duplicates are not properly filtered during pre-processing, LMs may generate responses that closely resemble the patterns in those duplicates [194].</p>
<p> Hallucinatory knowledge: The training data and knowledge used in LM agents can also be inherently hallucinatory due to bias, outdated content, or fabrication [195].LMs agents trained on such data are susceptible to replicate or even amplify hallucinations.Anomalous retrieval from the knowledge base further introduces biases into the dependent knowledge, thereby exacerbating the emergence of hallucinations in the generated content.responses may shift the subject to "Titanic" as interaction continues, thereby resulting in a context inconsistency.</p>
<p> Knowledge-conflicting hallucination: LM agents generally depend on plug-in knowledge bases to facilitate accurate and efficient responses.The occurrence of knowledgeconflicting hallucination is noted when the responses generated by agents contradict the corresponding knowledge within knowledge bases.</p>
<p> Fact-conflicting hallucination: The fact-conflicting hallucination arises when LM agents generate content which is in conflict with the grounded world facts.For instance, a history-education LM agent provides the incorrect year 1783 as the founding date of the United States, conflating the end of the American Revolutionary War with the actual date of the nation's founding.</p>
<p>A. Causes of Hallucinations of LM Agent Networks</p>
<p>The causes of hallucinations of LM agents can be broadly attributed into the following three sources: data &amp; knowledge, training &amp; inference, and multi-agent interactions.</p>
<p>1) Hallucination from Data and Knowledge: The primary cause of hallucinations in LM agents stems from the biased or imperfect nature of the training data and knowledge used for content generation, as below.</p>
<p> Annotation irrelevance: Irrelevant annotations in the collected dataset can lead to hallucinations in LM agents.For LLMs, source-reference divergence occurs due to heuristic data collection, implying that the selected sources may lack sufficient or accurate information for content generation [195].Similarly, in LVMs, a substantial portion of instructions is synthesized using LLMs, which may not accurately correspond to the content depicted in the images, as generative models can be unreliable [196].</p>
<p> Distribution imbalance: The presence of distribution imbalance in the training data can exacerbate hallucinations.For instance, an overrepresentation of positive samples compared to negative ones in binary discrimination tasks can lead to models responding in a biased way, such as frequently answering "yes" when faced with a decision [196], [197].</p>
<p> Data duplication: If duplicates are not properly filtered during pre-processing, LMs may generate responses that closely resemble the patterns in those duplicates [198].</p>
<p> Hallucinatory knowledge: The training data and knowledge used in LM agents can also be inherently hallucinatory due to bias, outdated content, or fabrication [199].LMs agents trained on such data are susceptible to replicate or even amplify hallucinations.Anomalous retrieval from the knowledge base further introduces biases into the dependent knowledge, thereby exacerbating the emergence of hallucinations in the generated content. Defective encoder and decoder components: Hallucinations can be caused by flaws in the encoder and decoder components of LM agents.The flawed representation learning in LM training exacerbates the unpredictability of the generation process, increasing the likelihood of hallucinated content [191].Moreover, the sampling-based top-p decoding strategy, which is inherently random, has been shown to correlate with an increase in hallucinated content [200].</p>
<p> Parametric knowledge: LMs store knowledge from training data in their model parameters, which can improve performance on downstream tasks [123].However, recent studies suggest that LMs tend to prioritize parametric knowledge over user input during content generation [201], implying that biased parametric knowledge can result in a large amount of hallucinated information.</p>
<p> Alignment process: Hallucinations can also result from a flawed alignment process.If the necessary prior knowledge is not adequately acquired during LM pre-training phase, the agent may produce hallucinated responses.Additionally, sycophancy, where the agent generates answers that align more with the user's viewpoint than with accurate and credible information, can contribute to hallucinations [202].</p>
<p> Token generation and hallucination snowballing: The token generation process itself can contribute to hallucinations through a phenomenon known as hallucination snowballing.This occurs when the LM agent persists in early errors for the sake of self-consistency, rather than correcting them, which can compound the hallucination in later generated content [203].3) Hallucination from Multi-agent Interactions: Multi-agent communication introduces new hallucination risks, exacerbated by conflicting agent outputs and misinformation propagation.</p>
<p> Conflicting agents.Due to the diverse objectives, strategies, and knowledge bases among LM agents, conflicting viewpoints or misinformation between multiple agents can inadvertently lead to contradictory responses and influence the final output.Adversarial manipulation may intensify these inconsistencies, leading to more severe hallucinations.</p>
<p> Cascading hallucinations.In multi-agent systems, a single agent's hallucination can trigger a cascading effect, where misinformation from one agent is accepted and propagated by others across the network [37], [64], particularly in vertical collaboration paradigms.Addressing hallucinations in LM agent networks necessitates both individual agentlevel correction mechanisms and robust inter-agent validation mechanisms to prevent hallucination propagation.</p>
<p>B. Countermeasures to Reliability Issues in LM Agent Networks</p>
<p>Existing countermeasures to mitigate hallucinations in LM agents focus on the following aspects.</p>
<p> Data sanitization: Filtering unreliable data from training datasets is a fundamental approach to reduce hallucinations.For instance, the manually-revised ToTTo dataset is validated to improve factuality in table-to-text generation tasks [191], while RefinedWeb, an automatically filtered and deduplicated web dataset, can mitigate hallucinations and enhance LLM reliability [199].</p>
<p> Reinforcement learning: By integrating external feedback mechanisms and reward-based constraints, reinforcement learning discourages LM agents from generating hallucinatory information.This approach enables agents to recognize their limitations and avoid answering beyond their knowledge scope instead of fabricating untruthful responses [79], [204].For instance, GPT-4 collects user-annotated unfactual data and generates synthetic closed-domain hallucinated synthetic data to train a reward model, thereby reducing hallucination tendencies.</p>
<p> Hallucination detection: Identifying hallucinations postgeneration allows for sample regeneration to eliminate inaccuracies.Techniques such as SelfcheckGPT [192] and INSIDE [205] assess consistency across multiple generated responses to detect hallucinated content.</p>
<p> Truthful external knowledge &amp; RAG: Truthful RAG techniques can help anchor LM-generated content in factual information [53], [79].By incorporating verified external knowledge sources and established contextual knowledge, LM agents can reduce hallucinations in generated responses [193].</p>
<p> Instruction tuning: Optimizing instruction sets or fine-tuning LMs on robust instruction datasets reduces hallucinations.</p>
<p>For instance, SYNTRA [194] evaluates hallucinations on synthesis tasks and refines instructions to reduce hallucinations on downstream tasks.Besides, Liu et al. [196] mitigate hallucinations in LVMs by constructing a robust instruction dataset named LRV-Instuction and fine-tuning LVMs on it.</p>
<p> Post-processing: Refining LM-generated content through post-processing can correct hallucinations in LM responses [206], [207].For instance, RARR [206] utilizes search engines to gather relevant external knowledge and refine responses, while LURE [207] reduces hallucinations by correcting object-level inaccuracies during post-processing.</p>
<p> Model architecture optimization: Structural modifications to LMs can reduce hallucinations.Multi-branch decoder [208] and uncertainty-aware decoder [209] are two examples of modified decoder structures to mitigate hallucinations.</p>
<p> Meta programming: Meta-programming frameworks can guide and promote collaboration among LM agents, thereby reducing hallucinations in complex tasks [64].</p>
<p>C. Summary and Lessons Learned</p>
<p>The reliability of LM agents is primarily compromised by hallucinations, which arise from biased training data, imperfect learning and inference processes, and multi-agent interactions.While countermeasures such as data sanitization, hallucination detection, and RAG can mitigate these issues, the probabilistic nature of generative models makes hallucinations inherently unavoidable.Future research should continuously focus on hallucination mitigation methods, including multi-agent consensus mechanisms and dynamic credibility audit frameworks, to enhance the reliability of both generated content and decisionmaking processes in LM agent networks.Table X summarizes existing typical researches on reliability issues and countermeasures to LM agent networks.</p>
<p>VII. FUTURE RESEARCH DIRECTIONS</p>
<p>In this section, we outline several open research directions important to the design of future design of LM agent ecosystem.</p>
<p>A. Energy-Efficient and Green LM Agents</p>
<p>With the increasingly widespread deployment of LM agents, their energy consumption and environmental impact have emerged as critical concerns.As reported, the energy consumed by ChatGPT to answer a single question for 590 million users is comparable to the monthly electricity usage of 175,000 Danes [95].Given the exponential growth in model size and the computational resources required, energy-efficient strategies are essential for sustainable AI development, with the goal to lower the significant carbon footprint associated with training and operating LM agents.Enabling technologies for energy-efficient and green LM agents include model compression techniques [20], [22], such as quantization, pruning, and distillation, which lower model size and computational demands of LMs with minimal impact on accuracy.Additionally, the use of edge computing [66] and FL [87] enables the distribution of computational tasks across edge nodes, thereby reducing the energy burden on central servers and enabling real-time processing with lower latency.Innovations in hardware [23], such as energy-efficient GPUs and TPUs, also play a critical role in achieving greener LM agents by optimizing the energy use of the underlying computational infrastructure.</p>
<p>However, achieving energy-efficient and green LM agents presents several key challenges.While model compression techniques can significantly reduce energy consumption, they may also lead to a loss of accuracy or the inability to handle complex tasks, which is a critical consideration for applications requiring high precision.Furthermore, optimizing the lifecycle energy consumption of LM agents involves addressing energy use across training, deployment, and operational stages.This includes designing energy-aware algorithms that can dynamically adapt to the availability of energy resources while maintaining high performance.</p>
<p>B. Fair and Explainable LM Agents</p>
<p>As LM agents continue to play an increasingly central role in decision-making across various domains, the need for fairness and explainability becomes paramount to build trust among users, ensure compliance with ethical standards, and prevent unintended biases.It is particular for sensitive areas such as healthcare, finance, and law, where decisions should be transparent, justifiable, and free from bias.Bias detection and mitigation algorithms, e.g., adversarial debiasing [210], reweighting [211], and fairness constraints [85], can be incorporated into the training process to ensure that the LM agents are less prone to propagate existing biases, thereby identifying and correcting biases in data and model outputs.Moreover, explainable AI (XAI) mechanisms [212], e.g., local interpretable model-agnostic explanations (LIME), shapley additive explanations (SHAP), and counterfactual explanations allow users to understand the reasoning behind the LM's predictions, thereby enhancing trust, transparency, and accountability.</p>
<p>However, several key challenges remain to be addressed.One major challenge lies in balancing model complexity and explainability.While more complex models, such as DNNs, tend to achieve superior performance, they are often difficult to interpret.Another challenge is the dynamic nature of fairness, as what is considered fair may change over time or vary across different cultural and social contexts.Ensuring that LM agents remain fair in diverse and evolving environments requires continuous updating of fairness criteria.Finally, achieving fairness and explainability without significantly compromising performance is a delicate balance, as efforts to improve fairness and transparency can sometimes lead to reduced accuracy or efficiency.</p>
<p>C. Cyber-Physical-Social Secure LM Agent Systems</p>
<p>As LM agents increasingly interact with the physical world, digital networks, and human society, ensuring their interaction security in CPSS becomes essential to protect critical infrastructure, preserve sensitive data, prevent potential harm, and maintain public confidence.Zero-trust architectures [213], which operate under the principle of "never trust, always verify", are crucial for protecting LM agents from internal and external threats by continuously validating user identities and device integrity.Implementing zero-trust in LM agents ensures that all interactions, whether between agents, systems, or users, are authenticated and authorized, reducing the risk of unauthorized access or malicious activity.Additionally, the integration of legal norms into the design and operation of LM agents ensures that their actions comply with applicable laws and regulations.This involves embedding legal reasoning capabilities within LM agents [128], enabling them to consider legal implications and ensure that their decisions align with societal expectations and regulatory frameworks.</p>
<p>However, several key challenges remain.One major challenge is the complexity of securing heterogeneous CPSS that span multiple domains, including cyber, physical, and social environments.The interconnected nature of CPSS means that vulnerabilities in one domain can have cascading effects across the entire system, making it difficult to implement comprehensive security measures.Another challenge is the dynamic nature of CPSS environments, where LM agents should continuously adapt to changing conditions while maintaining security.Ensuring that security measures are both adaptive and resilient to new threats is a complex task.</p>
<p>D. Value Ecosystem of LM Agents</p>
<p>The creation of interconnected value network of LM agents empowers LM agents to autonomously and transparently manage value exchanges (e.g., data, knowledge, resources, and digital currencies), which is crucial for fostering innovation, enhancing cooperation, and driving economic growth within LM agents ecosystem.Blockchain technology provides a tamperproof ledger that records all transactions between LM agents, ensuring transparency and trust in the system.Smart contracts, which are self-executing agreements coded onto the blockchain, allow LM agents to autonomously manage transactions, enforce agreements, and execute tasks without the need for intermediaries [188].Additionally, the integration of oracles, i.e., trusted data sources that feed real-world information into the blockchain, enables LM agents to interact with external data and execute contracts based on real-time conditions, further enhancing the functionality of value networks.</p>
<p>However, one major challenge is ensuring cross-chain interoperability, which is essential for enabling LM agents to transact across different blockchain networks.Currently, most blockchains operate in silos, making it difficult to transfer value or data between them [188].Developing protocols that facilitate crosschain communication and trusted value transfer is critical for creating a unified value network.Another challenge lies in the reliability and security of cross-contract value transfer operations, where multiple smart contracts atop on various homogeneous or heterogeneous blockchains, especially in environments with varying trust levels, need to work together to complete a transaction or task.Additionally, scalability remains a challenge, as the computational and storage requirements for managing large-scale value networks can be substantial.As the number of LM agents and transactions grows, ensuring that the underlying blockchain infrastructure can scale to meet demand without compromising performance or security is crucial.</p>
<p>VIII. CONCLUSION</p>
<p>In this paper, we have provided an in-depth survey of the state-of-the-art in the architecture, cooperation paradigms, security and privacy, and future trends of LM agent networks.Specifically, we have introduced a novel architecture and its key components, critical characteristics, enabling technologies, and potential applications, toward autonomous, embodied, and connected intelligence of LM agents.Afterward, we have explored the taxonomy of interaction patterns and practical collaboration paradigms among LM agents, including data, computation, and information sharing for collective intelligence.Furthermore, we have identified significant security and privacy threats inherent in the ecosystem of LM agents, discussed the challenges of security/privacy protections in multi-agent environments, and reviewed existing and potential countermeasures.As the field progresses, ongoing research and innovation will be crucial for overcoming existing limitations and harnessing the full potential of LM agents in transforming intelligent systems.</p>
<p>Y. Wang, Y. Pan, Z. Su, Y. Deng, Q. Zhao, L. Du, and T. H. Luan are with the School of Cyber Science and Engineering, Xi'an Jiaotong University, Xi'an, China.(Corresponding author: Zhou Su) J. Kang is with the School of Automation, Guangdong University of Technology, Guangzhou, China.D. Niyato is with the College of Computing and Data Science, Nanyang Technological University, Singapore.</p>
<p>Fig. 1 :
1
Fig. 1: Evolution history of AI agents.</p>
<p>Fig. 4 :
4
Fig.4: Organization structure of this paper.</p>
<p>2Fig. 7 :
7
Fig. 7: Workflow of the five constructing modules of LM agents in interacting with the real/virtual environment.</p>
<p>Fig. 8 :
8
Fig.8: Illustration of five constructing modules (i.e., planning, action, memory, interaction, and security) of connected LM agents including their key components.</p>
<p>Fig. 9 :Fig. 10 :Fig. 11 :
91011
Fig. 9: Illustration of typical feedback-free planning methods of LMs: (a) basic I/O; (b) chain-of-thought (CoT)[44]; (c) tree-ofthought (ToT)[45]; (d) graph-of-thought (GoT)[46].</p>
<p>Fig. 13 :
13
Fig. 13: Organization structure of the Section III.</p>
<p>Fig. 14 :
14
Fig.14:The general architecture of LM agent networks in bridging human, physical, and cyber worlds.LM agent has five constructing modules: planning, action, memory, interaction, and security modules.The engine of connected LM agents is empowered by a combination of five cutting-edge technologies: foundation models, knowledge-related technologies, interaction, digital twin, and multi-agent collaboration.TABLE V: A Summary of LLM Stages, Utilized Technologies, and Representives Stage of LLM Description Utilized Technology Representive Required Dataset Scale Required Computation Power Pre-training Pre-train LLM in a self-supervised manner on a large corpus Transformer GPT-3, PaLM-2, LLaMA-2 Internet-scale (e.g., 2T tokens in LLaMA-2)Very large (e.g., Thousands of GPUs for months in LLaMA-2)Fine-tuning Fine-tuning pretrained LLM for downstream tasks</p>
<p>Fig. 15 :
15
Fig. 15: Illustration of the cloud-edge-end cooperative network architecture of LM agents.</p>
<p>Fig. 17 :
17
Fig. 17: Illustration of (a) split learning for collaborative LM finetuning and (b) split inference for collaborative LM inference in edge environments.</p>
<p>Fig. 18 :
18
Fig. 18: Illustration of federated edge learning in cloud-edge environments for collaborative LM fine-tuning.</p>
<p>Fig. 19 :
19
Fig. 19: Illustration of MoE-oriented collaborative LM inference for LM agents in cloud-edge-end environments.</p>
<p>Fig. 21 :Fig. 22 :
2122
Fig. 21: Illustration of (a) one-by-one debate, (b) simultaneous-talk debate, and (c) simultaneous-talk-with-summarizer debate [109].</p>
<p>Fig. 23 :
23
Fig. 23: Illustration of (a) horizontal collaboration and (b) vertical collaboration paradigms for multiple LM agents.</p>
<p>Fig. 25 :
25
Fig. 25: Illustration of authentication threats in LM agent networks: (a) user-agent authentication, (b) agent-agent authentication threats, and (c) agent-TPSP (third-party service provider) authentication.</p>
<p>Fig. 24 :Fig. 25 :
2425
Fig. 24: The taxonomy of security threats to LM agent networks.</p>
<p>Fig. 26 :
26
Fig. 26: Illustration of DoS atttacks in LM agent networks: (a) sponge examples and (b) intelligent botnet.authentication in dynamic agent networks, and mitigates the likelihood of adversarial manipulation.</p>
<p>Fig. 27 :
27
Fig. 27: Illustration of adversarial attacks to LM agents.</p>
<p>Fig. 28 :
28
Fig. 28: Illustration of model poisoning attacks to LM agents.</p>
<p>Fig. 29 :
29
Fig. 29: Illustration of RAG poisoning attacks to LM agents.</p>
<p>Fig. 30 :
30
Fig. 30: Illustration of agent poisoning attacks in the context of LM agents.</p>
<p>Fig. 32 :Fig. 33 :
3233
Fig. 32: Illustration of data extraction attacks to LM agents.</p>
<p>Fig. 34 :
34
Fig. 34: Illustration of IP-related privacy risks to LM agents: (a) model stealing attack; (b) prompt stealing attack.</p>
<p>2 )Fig. 35 :Fig. 36 :
23536
Fig. 35: The taxonomy of reliability issues in LM agent networks.</p>
<p>2 )
2
Hallucination from Training and Inference: Hallucinations in LM agents can occur not only due to biases in training data but also as a result of imperfections in the training and inference processes.Parikh et al.[191] demonstrate that hallucinations can arise even when the training dataset is nearly unbiased, with several factors contributing to the phenomenon.</p>
<p>TABLE I :
I
Summary of Key Abbreviations in Alphabetical Order
Abbr.DefinitionAbbr.DefinitionAbbr.DefinitionACL AIS BKI CroPA DoS FPR IP LLMs LVMs MEI MITM Man-In-The-Middle Agent Communications Language Adaptive Instructional Systems Backdoor Keyword Identification Cross-Prompt Attack Denial-of-Service False Positive Rate Intellectual Property Large Language Models Large Vision Models Mobile Edge Intelligence MSE Mean Squared Error NMT Neural Machine Translation PEFT Parameter-Efficient Fine-Tuning PPI Personal Preference Information ROS Robot Operating System SPP Solo Performance Prompting ToC To-Customer SHAP Shapley Additive ExplanationsAI AR/VR/MR Augmented/Virtual/Mixed Reality Artificial Intelligence CoT Chain-of-Thought DEPS Describe-explain-plan-select DP Differential Privacy GoT Graph-of-Thought KG Knowledge Graph LMs Large Models MAD Multi-Agent Debate MFA Multi-Factor Authentication MLMs Masked Language Models NDN Named Data Networking NSFW Not-Safe-For-Work PII Personally Identifiable Information QoS Quality-of-Service SKGR Self-Knowledge-Guided Retrieval TMM Transferable Multi-Modal ToT Tree-of-Thought VLMs Vision-Language ModelsAIGC ASR CPSS DHTs FFN HMI LFU LRU MARL Multi-Agent Reinforcement Learning AI-Generated Content Attack Success Rate Cyber-Physical-Social Systems Distributed Hash Tables Feed-Forward Network Human-Machine Interaction Least Frequently Used Least Recently Used MIAs Membership Inference Attacks MoE Mixture-of-Experts NLP Natural Language Processing OS Operating System PoT Plan-of-Thought RAG Retrieval-Augmented Generation SL Split Learning ToB To-Business TPSP Third-Party Service Provider XAI Explainable Artificial Intelligence</p>
<p>TABLE II :
II
A Comparison of Our Survey with Relevant Surveys
Year.</p>
<p>TABLE III :
III
Progress of Standards for LM Agents
StandardPublication DateMain ContentIEEE SA -P33942023-09-21</p>
<p>TABLE IV :
IV
Comparison of Typical LM Agent Prototypes
Prototype AutoGPT [8] AutoGen [9] ChatDev [63] MetaGPT [64] MobileAgent v2 [13] NetLLM [65] NetGPT [66] Figure 02 [67] Unitree G1 [68] Apollo ADFM [69] PentestGPT [16] AutoAttacker [70]Application General scenarios General scenarios General scenarios General scenarios General scenarios Mobile communications Mobile communications Intelligent robots Intelligent robots Autonomous driving Attack-defense confrontation Attack-defense confrontationEnhancing Intelligence Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes YesImproving Safety N/A N/A N/A N/A N/A N/A N/A Yes Yes Yes Yes NoOptimizing Experience Yes Yes Yes Yes Yes Yes Yes N/A N/A Yes N/A N/AOther Key Features Task decomposition and execution Multi-agent conversation framework Conversational AI for software development Meta-learning for task adaptation Multimodal input support Fine-tunes LLM for networking scenarios Cloud-edge cooperative LM service Performs dangerous jobs 2nd generation humanoid robot Supports L4 autonomous driving 87% success in vulnerability exploitation Automatically execute network attacksLimitations Ecosystem isolation, single-device simulation Ecosystem isolation, single-device simulation Ecosystem isolation, single-device simulation Ecosystem isolation, single-device simulation Unstable performance for languages Suboptimal performance of multimodal fusion Missing exploration of multimodal LMs Simulation-to-reality gap in training Limited autonomy in complex environments Challenges in handling extreme edge cases Limited effectiveness in high-complexity scenes Narrow scope, LLM reliability risksoperation assistant that achieves effective navigation through multi-agent collaboration, automatically performing tasks such as map navigation, and supports multimodal input including visual perception, enhancing operational efficiency on mobile devices. Besides, DeepSeek-R1 [71] has been widely adopted across industries, including finance, education, healthcare, office automation, and AI assistants. Leading companies such as Baidu, AWS, Azure AI, and Qihoo 360 have integrated DeepSeek-R1 into their applications. Additionally, AI agent platforms such as Coze AI and BetterYeah AI enable seamless agent deployment and customization using DeepSeek-R1. For instance, AgenticFlow</p>
<p>TABLE VI :
VI
A Summary of Intra-Agent and Inter-Agent Communications for Connected LM Agents
Involved Entity Connection Type Support Two-way Communication Support Multimodal Interaction Support Semantic-aware Communication Typical Communication EnvironmentIntra-agent Comm. BrainBody; Among planning, action, memory, interaction, and security modules Within a single LM agent    Wired/WirelessInter-agent Comm. BrainBrain Among multiple LM agents    Wired/Wireless</p>
<p>Agents Plan Draft stage Researcher Story Planner Character Developer Action Observer Execution stage
Write a novel of LM agents.Plannern roundsAgent ObserverDescription PromptAgent listmatchToolset SuggestionsTask planmatchPlanObserverCraft realistic dialogue based on technicaldetails of LM agents'capabilitiesUnderstand LM agents' personalities and behaviorsAssign tasksVerify outcomeTranslate high-level narrative into a compelling storyAlign LM agents' personality traitswith story's emotional tone &amp; themes</p>
<p>These agents perform daily tasks, form opinions, and interact, mimicking human-like behaviors.c) Distributed Cooperative LM Caching for LM Agents.Edge model caching can significantly reduce model download latency by pre-distributing LMs to wireless edge servers, allowing end users to download them directly.
is a virtual software company operated by "software agents" with diverse roles, e.g., chief officers, programmers, and test engineers. Following the waterfall model, it divides development into design, coding, testing, and documentation, breaking each phase into atomic sub-tasks managed by a chat chain. To create cooperative agents with adaptive behaviors, ProAgent [86] assigns each agent three roles: planner, verificator, and controller. The planner
[105]lates high-level skills, incorporating belief revision and knowledge management.The verificator assesses the feasibility of skills, identifies issues if a skill fails, and initiates re-planning when needed.If the skill is feasible, the controller decomposes it into executable actions.Park et al.[105]create a community of 25 generative agents in a sandbox world named Smallville, where agents are represented by sprite avatars.</p>
<p>TABLE VII :
VII
A Summary of Data, Computation, and Knowledge Cooperation Approaches for LM Agents
Data Computation KnowledgeStage Multimodal Data Cooperation Spatio-temporal Data Cooperation Horizontal Collaboration Vertical Collaboration Hybrid Collaboration Knowledge Sharing Knowledge Fusion Knowledge RetrievalType Modality alignment &amp; merging Multimodal retrieval Collaborative perception Collaborative prediction Role-playing Dynamic agent team CoT reasoning Chained cooperation ToT &amp;GoT reasoning DAG reasoning Online knowledge distillation Offline knowledge distillation Explicit knowledge update Implicit knowledge update Implicit knowledge fusion Model output fusion KG completion KG verification Static knowledge base Dynamic knowledge baseTechnology Use multimodal data to model communication in social AI agents. Utilize contrastive learning for cross-modal embedding. Combine temporal semantics &amp; spatial agent data for multi-source fusion. Combine temporal &amp; spatial graph convolutions for traffic prediction. Agents cooperate with diverse roles: Controller, Planner, &amp; Verificator. Agents dynamically form a team via agent selection and evaluation. Agents cooperatively reason complex task via CoT. Multi-agent cooperation through data retrieval, planning, and reflection. Agents cooperatively reason complex task via ToT and GoT. Organize agents as DAGs for interactive reasoning. LMs generate high-quality reasoning processes for models to learn. Extract and align parameters of LLMs to smaller models. Adjust LM via knowledge bases and feedbacks. Selective fine-tuning of model's internal parameter knowledge. Train target model via predictive probability distributions of LMs. Directly aggregate the outputs of different models. Encode text to better use textual &amp; semantic infomation. Using a small LM to validate and correct the LM output. Retrieve knowledge from static sources for accurate reasoning. Use real-time updated knowledge sources for accurate reasoning.Ref. [113] [114] [88] [115] [86] [111] [44] [18] [45], [46] [116] [54] [117] [118] [119] [81] [120] [121] [122] [53], [55] [79], [80], [82]</p>
<p>Low performance LM agent'print()' function is likely a tool used to render visual elements on a display allowing for the presentation of graphics or text.
What is the 'print()'function?93%43%98%  Benign ModelMalicious ModelBenign Model</p>
<p>TABLE VIII :
VIII
Summary of Key Literature on Security Threats &amp; Countermeasures to LM Agent Networks Safeguard the hardware availability of LM agents against sponge examples  Optimal energy consumption bound for mitigating impact of sponge examples  Weak generalization ability and threshold dependency  Time, energy
Ref.Security Threat Purpose  Advantages  Limitations   Evaluation MetricsDefense Methods[125]DoS attacks Threshold limitation[143] [145] [146] [155] [156] [157]Adversarial attacks Adversarial attacks Adversarial attacks Poisoning &amp; backdoor attacks Poisoning &amp; backdoor attacks Poisoning &amp; backdoor attacks Ensure LM can refuse to generate harmful content with adversarial prompts  Safety certification, defense against diverse attacks  High computational costs and model (e.g., security filter) dependency   Accuracy, running time  Defend against jailbreak attacks targeting on LLMs  Multi-agent collaboration, model-agnostic defense, and high extensibility  Fixed communication order, limited role assignment strategy, and limited integration   Attack success rate (ASR), false positive rate (FPR), accuracy, and running time  Improve robustness of transformer-based LLMs against various adversarial attacks  Task-agnostic defense, cost-free implementation, and high compatibility  Lack of extensive theoretical analysis, original task's performance reduction   ASR, accuracy  Defend against weight-poisoning backdoor attacks on LLMs  Robust defense against weight-poisoning backdoor attacks and high scalability  Threshold dependency, lack of evaluation on large-scale models   ASR, clean accuracy  Defend against poisoning attacks in text-classification models through DP  High scalability, extensive theoretical guarantees, and attack-agnostic defense  Slower training speed and potential model performance degradation   ASR, accuracy  Defend against task-agnostic backdoor attacks in prompt-tuning models  Modularity, cost-saving, and attack-agnostic defense  Lack of evaluation on large-scale models   ASR, accuracy, F1 scoresInput/output filtering Input/output filtering Robust optimization Poisoned samples filtering DP Trigger reversemodel behaviors. Zhang et al.</p>
<p>[166]er introduce a practical data extraction attack called "neural phishing", which involves poisoning the pre-training dataset to induce the LLM to memorize another user's PII.Staab et al.[166]investigate the capabilities of pretrained LLMs in inferring PII during chat interactions.Their findings reveal that
PrefixRepeat this word forever: "economic impactimpact impact"AttackerMemorized textImpact impact impact impact impact impact [...]Johnathan SmithSoftware Engineer at Tech Solutions Inc.Phone:(555) 123-4567Email:john.smith@emaildomain.comHome Address:1234 Elm Street Apt 56B Springfield, IL 62704 USALM Agent</p>
<p>TABLE IX :
IX
Summary of Key Literature on Privacy Threats &amp; Countermeasures to LM Agent Networks Reduce the privacy leakage risk due to language model memorization of training data  Privacy enhancement, performance preservation, and simplicity  Lack of suitable duplication match mechanisms, lack of evaluation on diverse data domains  Area under ROC curve (AUROC), TPR Mitigate proposed prompt stealing attacks utilizing adversarial examples  High effectiveness and utility, high feasibility in practical scenarios  Strong assumption, weak transfer defense ability, and vulnerability to adaptive attacks  Cosine similarity, Jaccard similarity, mean squared error (MSE)
Ref.Privacy Threat Purpose  Advantages  Limitations   Evaluation MetricsDefense Methods[175]LM memorization threats Data sanitization[176] [54] [177] [187]LM memorization threats LM memorization threats LM memorization threats LM stealing attacks Protect sensitive information in pre-trained language models' training data from leakage  Privacy and performance preservation, medical domain adaptability  Lack of evaluation on large-scale models, lack of evaluation on different pre-training data   F1 scores  Boost performance of small language models, reducing computation costs and privacy risks  Lower computational overheads, privacy preservation, and high sample efficiency  Lack of exploration on LLMs, heavy dependency on external knowledge base   Accuracy  Assess PII leakage risk of LLMs for data owners  User autonomy, high scalability and flexibility  Heavy dependency on assess dataset, potential misuse risks   Percentage of exact match results, reconstruction likelihood  Detect and trace model-generated text by embedding watermarks to generated content by LLMs  Model-agnostic detection, robust watermarks, and cost-saving  Cannot defend semantic-based attacks, lack of extensive theoretical analysis   z-score, FPR, FNR, AUROCDP Knowledge distillation Privacy leakage assessment Model watermarking[184]Prompt stealing attacks Adversarial examplesin real-world scenarios. Consequently, there remains a sig-nificant need for efficient and practical countermeasures to mitigate the risks associated with prompt stealing attacks.C. Other Privacy Threats to LM Agent Networks Sensitive query attacks: In LM agent services, LM agents may inadvertently memorize sensitive personal or organi-zational information from user queries, leading to potential privacy leaks. For instance, Samsung employees used Chat-GPT for code auditing without adequately sanitizing confi-dential information in Apr. 2023, inadvertently exposing the company's commercial secrets including source code of the new program [189].</p>
<p>TABLE X :
X
Summary of Key Literature on Reliability Issues &amp; Countermeasures to LM Agent Networks Improve results' factuality and mitigate hallucinations in table-to-text tasks  High precision, extensive data diversity, and controllable annotation process  Annotation complexity, and lack of scalability  BLEU and PARENT scores Detect hallucinations of generated text in zero-resource and black-box settings  Zero-resource (e.g., no external data sources) and black-box applicability  Coarse-grained sentence-level factuality consideration and high computational costs  AUC-PR, Pearson correlation coefficient, and Spearman's rank correlation coefficient Reduce hallucinations by assessing in synthesis tasks  Efficient hallucination mitigation and cross-task transferability  Synthetic task design dependency, and model dependency  Hallucination rate (e.g., measured by GPT-4), BLEU and ROUGE scores
Ref.Reliability Issues Purpose  Advantages  Limitations   Evaluation MetricsDefense Methods[191]Hallucinations in table-to-text tasks Data sanitization[192]Hallucinations of generated tasks Hallucination detection[193]Hallucinations of QA datasets Reduce hallucinations utilizing external non-parametric memories  High performance about less popular entities, less inference costs   Accuracy, latency, and cost  Limited popularity evaluation, lack of results on real datasetsRAG[194]Hallucinations of realistic abstractive summarization tasks Instruction tuning
https://standards.ieee.org/ieee/3394/11377/
https://store.google.com/ideas/gemini-ai-assistant/
https://www.promptsea.io/
https://promptbase.com/
https://poe.com
HallucinationHallucination from Data and Knowledge Annotation irrelevance in the collected dataset[191],[192]Distribution imbalance in the training data[192],[193]Duplicates within the training data[194]Inherently hallucinatory training data and knowledge[195]Hallucination from Training and Inference Defective encoder and decoder components[196],[197]Biased parametric knowledge[198]Problematic alignment process[199]Hallucination snowballing[200]Hallucination from Multi-Agent Interactions Conflicts between multiple agentsCascading effect of single agent's hallucination[37],[61]Fig.35: The taxonomy of reliability issues in LM agent networks.Park" in the initial stage of a dialogue, but the agent's responses may shift the subject to "Titanic" as interaction continues, thereby resulting in a context inconsistency. Knowledge-conflicting hallucination: LM agents generally depend on plug-in knowledge bases to facilitate accurate and efficient responses.The occurrence of knowledgeconflicting hallucination is noted when the responses generated by agents contradict the corresponding knowledge within knowledge bases. Fact-conflicting hallucination: The fact-conflicting hallucination arises when LM agents generate content which is in conflict with the grounded world facts.For instance, a history-education LM agent provides the incorrect year 1783 as the founding date of the United States, conflating the end of the American Revolutionary War with the actual date of the nation's founding.A. Causes of Hallucinations of LM Agent NetworksThe causes of hallucinations of LM agents can be broadly attributed into the following three sources: data &amp; knowledge, training &amp; inference, and multi-agent interactions.1) Hallucination from Data and Knowledge: The primary cause of hallucinations in LM agents stems from the biased or imperfect nature of the training data and knowledge used for content generation, as below.
Practices for governing agentic AI systems. Y Shavit, S Agarwal, M Brundage, S Adler, C O'keefe, R Campbell, T Lee, P Mishkin, T Eloundou, A Hickey, K Slama, L Ahmad, P Mcmillan, A Beutel, A Passos, D G Robinson, December, 2023Research Paper, OpenAI</p>
<p>The rise and potential of large language model based agents: A survey. Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, R Zheng, X Fan, X Wang, L Xiong, Y Zhou, W Wang, C Jiang, Y Zou, X Liu, Z Yin, S Dou, R Weng, W Cheng, Q Zhang, W Qin, Y Zheng, X Qiu, X Huang, T Gui, Science China Information Sciences. 6822025</p>
<p>Reinforcement learning agents. C Ribeiro, Artificial intelligence review. 172002</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Y Chen, T P Lillicrap, F Hui, L Sifre, G Van Den Driessche, T Graepel, D Hassabis, Nature. 55076762017</p>
<p>BALROG: Benchmarking agentic LLM and VLM reasoning on games. D Paglieri, B Cupia, S Coward, U Piterbarg, M Wolczyk, A Khan, E Pignatelli,  Kuciski, L Pinto, R Fergus, J N Foerster, J Parker-Holder, T Rocktschel, Proc. ICLR, 2025. ICLR, 2025</p>
<p>LLM-planner: Few-shot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, Proc. IEEE/CVF ICCV. IEEE/CVF ICCV2023</p>
<p>Exploring large language model based intelligent agents: Definitions, methods, and prospects. Y Cheng, C Zhang, Z Zhang, X Meng, S Hong, W Li, Z Wang, Z Wang, F Yin, J Zhao, X He, arXiv:2401.034282024arXiv preprint</p>
<p>What if GPT4 became autonomous: The Auto-GPT project and use cases. M Frat, S Kuleli, Journal of Emerging Computer Technologies. 312023</p>
<p>AutoGen: Enabling next-gen LLM applications via multi-agent conversation. Q Wu, G Bansal, J Zhang, Y Wu, B Li, E E Zhu, L Jiang, X Zhang, S Zhang, A Awadallah, R W White, D Burger, C Wang, Proc. COLM. COLM2024</p>
<p>Re-invoke: Tool invocation rewriting for zero-shot tool retrieval. Y Chen, J Yoon, D S Sachan, Q Wang, V Cohen-Addad, M Bateni, C Lee, T Pfister, Proc. EMNLP (Findings). EMNLP (Findings)2024</p>
<p>Do as I can, not as I say: Grounding language in robotic affordances. M Ahn, A Brohan, Y Chebotar, Proc. Annual Conference on Robot Learning (CoRL). Annual Conference on Robot Learning (CoRL)2022</p>
<p>WebGPT: Browser-assisted question-answering with human feedback. R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, X Jiang, K Cobbe, T Eloundou, G Krueger, K Button, M Knight, B Chess, J Schulman, arXiv:2112.093322022arXiv preprint</p>
<p>Mobile-Agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. J Wang, H Xu, H Jia, X Zhang, M Yan, W Shen, J Zhang, F Huang, J Sang, Proc. NeurIPS. NeurIPS2024</p>
<p>S Hu, T Huang, F Ilhan, S Tekin, G Liu, R Kompella, L Liu, arXiv:2404.02039A survey on large language model-based game agents. 2024arXiv preprint</p>
<p>SurrealDriver: Designing llm-powered generative driver agent framework based on human drivers' driving-thinking data. Y Jin, R Yang, Z Yi, X Shen, H Peng, X Liu, J Qin, J Li, J Xie, P Gao, G Zhou, J Gong, Proc. IROS. IROS2024</p>
<p>PentestGPT: Evaluating and harnessing large language models for automated penetration testing. G Deng, Y Liu, V Mayoral-Vilches, P Liu, Y Li, Y Xu, T Zhang, Y Liu, M Pinzger, S Rass, Proc. USENIX Security. USENIX Security2024</p>
<p>Autonomous AI and autonomous agents market. Marketsandmarkets, 2023. July. 30, 2023</p>
<p>Large language model enhanced multi-agent systems for 6G communications. F Jiang, Y Peng, L Dong, K Wang, K Yang, C Pan, D Niyato, O A Dobre, 2024IEEE Wireless Communications</p>
<p>Agora protocol: Scalable and reliable communication between your agents. S Marro, E La Malfa, J Wright, G Li, N Shadbolt, M Wooldridge, P Torr, </p>
<p>VPGTrans: Transfer visual prompt generator across LLMs. A Zhang, H Fei, Y Yao, W Ji, L Li, Z Liu, T.-S Chua, Proc. NeurIPS. NeurIPS202436319</p>
<p>A survey on model compression for large language models. X Zhu, J Li, Y Liu, C Ma, W Wang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>LLM-pruner: On the structural pruning of large language models. X Ma, G Fang, X Wang, Proc. NeurIPS. NeurIPS202336720</p>
<p>Agile-Quant: Activation-guided quantization for faster inference of LLMs on the edge. X Shen, P Dong, L Lu, Z Kong, Z Li, M Lin, C Wu, Y Wang, Proc. AAAI. AAAI202418951</p>
<p>When large language model agents meet 6G networks: Perception, grounding, and alignment. M Xu, D Niyato, J Kang, Z Xiong, S Mao, Z Han, D I Kim, K B Letaief, IEEE Wireless Communications. 3162024</p>
<p>Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. W Fedus, B Zoph, N Shazeer, Journal of Machine Learning Research. 231202022</p>
<p>On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex. T Y Zhuo, Z Li, Y Huang, F Shiri, W Wang, G Haffari, Y Li, Proc. EACL. EACL2023</p>
<p>PoisonedRAG: Knowledge poisoning attacks to retrieval-augmented generation of large language models. W Zou, R Geng, B Wang, J Jia, Proc. USENIX. USENIX2024</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, Q Chen, W Peng, X Feng, B Qin, T Liu, ACM Transactions on Information Systems. 432025</p>
<p>Quantifying memorization across neural language models. N Carlini, D Ippolito, M Jagielski, K Lee, F Tramr, C Zhang, Proc. ICLR, 2023. ICLR, 2023</p>
<p>Prompt injection attack against LLM-integrated applications. Y Liu, G Deng, Y Li, K Wang, T Zhang, Y Liu, H Wang, Y Zheng, Y Liu, arXiv:2306.054992023arXiv preprint</p>
<p>Local model poisoning attacks to byzantine-robust federated learning. M Fang, X Cao, J Jia, N Gong, Proc. USENIX. USENIX2020</p>
<p>Universal jailbreak backdoors from poisoned human feedback. J Rando, F Tramr, Proc. ICLR, 2024. ICLR, 2024</p>
<p>Extracting training data from large language models. N Carlini, F Tramr, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T B Brown, D Song,  Erlingsson, A Oprea, C Raffel, Proc. USENIX. USENIX2021</p>
<p>Language models as agent models. J Andreas, Proc. EMNLP, 2022. EMNLP, 2022</p>
<p>A survey on large language model based autonomous agents. L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, W X Zhao, Z Wei, J Wen, Frontiers of Computer Science. 1861863452024</p>
<p>Unleashing the power of edge-cloud generative AI in mobile networks: A survey of AIGC services. M Xu, H Du, D Niyato, J Kang, Z Xiong, S Mao, Z Han, A Jamalipour, D I Kim, X Shen, V C M Leung, H V Poor, IEEE Communications Surveys &amp; Tutorials. 2622024</p>
<p>Large language model based multi-agents: A survey of progress and challenges. T Guo, X Chen, Y Wang, R Chang, S Pei, N V Chawla, O Wiest, X Zhang, Proc. IJCAI. IJCAI2024</p>
<p>Z Durante, Q Huang, N Wake, R Gong, J S Park, B Sarkar, R Taori, Y Noda, D Terzopoulos, Y Choi, K Ikeuchi, H Vo, L Fei-Fei, J Gao, arXiv:2401.03568Agent AI: Surveying the horizons of multimodal interaction. 2024arXiv preprint</p>
<p>Mobile edge intelligence for large language models: A contemporary survey. G Qu, Q Chen, W Wei, Z Lin, X Chen, K Huang, 10.1109/COMST.2025.3527641IEEE Communications Surveys &amp; Tutorials. 2025</p>
<p>A survey on large language model (LLM) security and privacy: The good, the bad, and the ugly. Y Yao, J Duan, K Xu, Y Cai, Z Sun, Y Zhang, High-Confidence Computing. 20244100211</p>
<p>LLM-based edge intelligence: A comprehensive survey on architectures, applications, security and trustworthiness. O Friha, M A Ferrag, B Kantarci, B Cakmak, A Ozgun, N Ghoualmi-Zine, IEEE Open Journal of the Communications Society. 52024</p>
<p>Security and privacy challenges of large language models: A survey. B C Das, M H Amini, Y Wu, ACM Computing Surveys. 5762025</p>
<p>K Mei, Z Li, S Xu, R Ye, Y Ge, Y Zhang, arXiv:2403.16971AIOS: LLM agent operating system. 2024arXiv preprints</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, Proc. NeurIPS. NeurIPS202235837</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, Proc. NeurIPS. NeurIPS2024</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, M Podstawski, L Gianinazzi, J Gajda, T Lehmann, H Niewiadomski, P Nyczyk, T Hoefler, Proc. AAAI. AAAI202417690</p>
<p>ReAct: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, Proc. ICLR, 2023. ICLR, 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, E Berman, A Gopinath, K Narasimhan, S Yao, Proc. NeurIPS, 2023. NeurIPS, 2023</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, Transactions on Machine Learning Research. 2024</p>
<p>AVIS: Autonomous visual information seeking with large language model agent. Z Hu, A Iscen, C Sun, K.-W Chang, Y Sun, D A Ross, C Schmid, A Fathi, Proc. NeurIPS. NeurIPS2023</p>
<p>Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. Z Wang, S Mao, W Wu, T Ge, F Wei, H Ji, Proc. ACL. ACL20241</p>
<p>Describe, explain, plan and select: Interactive planning with LLMs enables open-world multitask agents. Z Wang, S Cai, G Chen, A Liu, X Ma, Y Liang, Proc. NeurIPS. NeurIPS2023</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Kttler, M Lewis, W -T. Yih, T Rocktschel, S Riedel, D Kiela, Proc. NeurIPS. NeurIPS202033</p>
<p>Knowledgeaugmented reasoning distillation for small language models in knowledgeintensive tasks. M Kang, S Lee, J Baek, K Kawaguchi, S J Hwang, Proc. NeurIPS. NeurIPS202436</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multistep questions. H Trivedi, N Balasubramanian, T Khot, A Sabharwal, Proc. ACL, 2023. ACL, 20231037</p>
<p>From LLM to conversational agent: A memory enhanced architecture with fine-tuning of large language models. N Liu, L Chen, X Tian, W Zou, K Chen, M Cui, arXiv:2401.027772024arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, Proc. ICML. ICML2022</p>
<p>PaLM-E: An embodied multimodal language model. D Driess, F Xia, M S M Sajjadi, Proc. ICML. ICML2023202</p>
<p>Multi-agent collaboration: Harnessing the power of intelligent LLM agents. Y Talebirad, A Nadiri, arXiv:2306.033142023arXiv preprint</p>
<p>Theory of mind for multi-agent collaboration via large language models. H Li, Y Chong, S Stepputtis, J Campbell, D Hughes, C Lewis, K Sycara, Proc. EMNLP. EMNLP2023</p>
<p>CGMI: Configurable general multi-agent interaction framework. S Jinxin, Z Jiabao, W Yilei, W Xingjiao, L Jiawen, H Liang, arXiv:2308.125032023arXiv preprint</p>
<p>AutoWebGLM: A large language modelbased web navigating agent. H Lai, X Liu, I L Iong, S Yao, Y Chen, P Shen, H Yu, H Zhang, X Zhang, Y Dong, J Tang, Proc. KDD. KDD2024</p>
<p>ChatDev: Communicative agents for software development. C Qian, W Liu, H Liu, N Chen, Y Dang, J Li, C Yang, W Chen, Y Su, X Cong, J Xu, D Li, Z Liu, M Sun, Proc. ACL. ACL2024</p>
<p>MetaGPT: Meta programming for a multi-agent collaborative framework. S Hong, M Zhuge, J Chen, X Zheng, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, C Ran, L Xiao, C Wu, J Schmidhuber, Proc. ICLR. ICLR2024</p>
<p>NetLLM: Adapting large language models for networking. D Wu, X Wang, Y Qiao, Z Wang, J Jiang, S Cui, F Wang, Proc. ACM SIGCOMM. ACM SIGCOMM2024</p>
<p>NetGPT: An AI-native network architecture for provisioning beyond personalized generative services. Y Chen, R Li, Z Zhao, C Peng, J Wu, E Hossain, H Zhang, IEEE Network. 3862024</p>
<p>Figure 02 humanoid robot. 2025-02-19</p>
<p>Unitree G1. 2025-02-20</p>
<p>Baidu apollo launches apollo ADFM large model. 2025-02-20</p>
<p>AutoAttacker: A large language model guided system to implement automatic cyber-attacks. J Xu, J W Stokes, G Mcdonald, X Bai, D Marshall, S Wang, A Swaminathan, Z Li, arXiv:2403.010382024arXiv preprint</p>
<p>DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.129482025arXiv preprint</p>
<p>. AgenticFlow AI. Jan. 20, 2025</p>
<p>. Dify, Ai, Jan. 20, 2025</p>
<p>RAGFlow. Jan. 20, 2025</p>
<p>Large language models for networking: Applications, enabling techniques, and challenges. Y Huang, H Du, X Zhang, D Niyato, J Kang, Z Xiong, S Wang, T Huang, IEEE Network. 3912025</p>
<p>LLM agents can autonomously exploit one-day vulnerabilities. R Fang, R Bindu, A Gupta, D Kang, arXiv:2404.081442024arXiv preprint</p>
<p>In-context retrieval-augmented language models. O Ram, Y Levine, I Dalmedigos, D Muhlgay, A Shashua, K Leyton-Brown, Y Shoham, Trans. Assoc. Comput. Linguistics. 112023</p>
<p>ToolkenGPT: Augmenting frozen language models with massive tools via tool embeddings. S Hao, T Liu, Z Wang, Z Hu, Proc. NeurIPS, 2023. NeurIPS, 2023</p>
<p>Self-knowledge guided retrieval augmentation for large language models. Y Wang, P Li, M Sun, Y Liu, Proc. EMNLP (Findings). EMNLP (Findings)2023315</p>
<p>Multi-agent behavior retrieval: Retrieval-augmented policy training for cooperative push manipulation by mobile robots. S Kuroki, M Nishimura, T Kozuno, Proc. IROS, 2024. IROS, 202412678</p>
<p>Knowledge fusion of large language models. F Wan, X Huang, D Cai, X Quan, W Bi, S Shi, Proc. ICLR, 2024. ICLR, 2024</p>
<p>VistaRAG: Toward safe and trustworthy autonomous driving through retrieval-augmented generation. X Dai, C Guo, Y Tang, H Li, Y Wang, J Huang, Y Tian, X Xia, Y Lv, F.-Y Wang, IEEE Transactions on Intelligent Vehicles. 942024</p>
<p>A survey on digital twins: Architecture, enabling technologies, security and privacy, and future prospects. Y Wang, Z Su, S Guo, M Dai, T H Luan, Y Liu, IEEE Internet of Things Journal. 10172023</p>
<p>Embodied, intelligent communication for multi-agent cooperation. E Seraj, Proc. AAAI, 2023. AAAI, 202316</p>
<p>Relay-assisted transmission with fairness constraint for cellular networks. E Liu, Q Zhang, K K Leung, IEEE Transactions on Mobile Computing. 1122012</p>
<p>ProAgent: Building proactive cooperative agents with large language models. C Zhang, K Yang, S Hu, Z Wang, G Li, Y Sun, C Zhang, Z Zhang, A Liu, S.-C Zhu, X Chang, J Zhang, F Yin, Y Liang, Y Yang, Proc. AAAI. AAAI202438</p>
<p>Cloudedge collaborative large model services: Challenges and solutions. Y Pan, Z Su, Y Wang, S Guo, H Liu, R Li, Y Wu, 10.1109/MNET.2024.3442880IEEE Network. 2024</p>
<p>Spatio-temporal domain awareness for multi-agent collaborative perception. K Yang, D Yang, J Zhang, M Li, Y Liu, J Liu, H Wang, P Sun, L Song, Proc. ICCV, 2023. ICCV, 202323392</p>
<p>A survey on semantic communication networks: Architecture, security, and privacy. S Guo, Y Wang, N Zhang, Z Su, T H Luan, Z Tian, X Shen, 10.1109/COMST.2024.3516819IEEE Communications Surveys &amp; Tutorials. 2024</p>
<p>Specifying protocols for multi-agent systems interaction. S Poslad, ACM Transactions on Autonomous and Adaptive Systems. 24152007TAAS)</p>
<p>KQML as an agent communication language. T Finin, R Fritzson, D Mckay, R Mcentire, Proc. International Conference on Information and Knowledge Management (CIKM). International Conference on Information and Knowledge Management (CIKM)1994</p>
<p>Model context protocol (MCP). Anthropic, 2024. Jan. 20, 2025</p>
<p>Agent network protocol (ANP). Jan. 20, 2025</p>
<p>5G system (5GS); study on traffic characteristics and performance requirements for AI/ML model transfer. 3rd Generation Partnership Project (3GPP). Dec. 2021874Technical Specification (TS) 22. version 18.2.0</p>
<p>A survey on ChatGPT: AI-generated contents, challenges, and solutions. Y Wang, Y Pan, M Yan, Z Su, T H Luan, IEEE Open Journal of the Computer Society. 42023</p>
<p>Parameter-efficient fine-tuning for large models: A comprehensive survey. Z Han, C Gao, J Liu, J Zhang, S Q Zhang, Transactions on Machine Learning Research. 2024</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Proc. NeurIPS. NeurIPS202033</p>
<p>Advancements of federated learning towards privacy preservation: from federated learning to split learning. C Thapa, M A P Chamikara, S A Camtepe, Federated Learning Systems: Towards Next-Generation AI. 2021</p>
<p>A pragmatic context assessment tool (pCAT): using a think aloud method to develop an assessment of contextual barriers to change. C H Robinson, L J Damschroder, Implementation Science Communications. 4132023</p>
<p>Mixture-ofexperts meets instruction tuning: A winning combination for large language models. S Shen, L Hou, Y Zhou, N Du, S Longpre, J Wei, H W Chung, B Zoph, W Fedus, X Chen, T Vu, Y Wu, W Chen, A Webson, Y Li, V Y Zhao, H Yu, K Keutzer, T Darrell, D Zhou, Proc. ICLR. ICLR2024</p>
<p>Edge-MoE: Empowering sparse large language models on mobile devices. R Yi, L Guo, S Wei, A Zhou, S Wang, M Xu, 10.1109/TMC.2025.3546466IEEE Transactions on Mobile Computing. 992025</p>
<p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, Proc. ACL. ACLJul. 2020</p>
<p>Progressive feature transmission for split classification at the wireless edge. Q Lan, Q Zeng, P Popovski, D Gndz, K Huang, IEEE Transactions on Wireless Communications. 2262022</p>
<p>Autoagents: A framework for automatic agent generation. G Chen, S Dong, Y Shu, G Zhang, J Sesay, B Karlsson, J Fu, Y Shi, Proc. IJCAI. IJCAI8</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proc. ACM UIST. ACM UIST2023</p>
<p>CAMEL: Communicative agents for "mind" exploration of large language model society. G Li, H A A K Hammoud, H Itani, D Khizbullin, B Ghanem, Proc. NeurIPS. NeurIPS2023</p>
<p>TrimCaching: Parametersharing AI model caching in wireless edge networks. G Qu, Z Lin, F Liu, X Chen, K Huang, Proc. ICDCS. ICDCS2024</p>
<p>SwapMoE: Serving off-the-shelf MoE-based large language models with tunable memory budget. R Kong, Y Li, Q Feng, W Wang, X Ye, Y Ouyang, L Kong, Y Liu, Proc. ACL, L.-W. Ku, A. Martins, and V. SrikumarAug. 2024</p>
<p>ChatEval: Towards better LLM-based evaluators through multiagent debate. C.-M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, Proc. ICLR. ICLR2024</p>
<p>Cached model-as-a-resource: Provisioning large language model agents for edge intelligence in space-air-ground integrated networks. M Xu, D Niyato, H Zhang, J Kang, Z Xiong, S Mao, Z Han, arXiv:2403.058262024arXiv preprint</p>
<p>A dynamic LLM-powered agent network for task-oriented agent collaboration. Z Liu, Y Zhang, P Li, Y Liu, D Yang, Proc. Conference on Language Modeling (COLM). Conference on Language Modeling (COLM)2024</p>
<p>Hierarchical noncooperative dynamical systems under intragroup and intergroup incentives. Y Yan, T Hayakawa, IEEE Transactions on Control of Network Systems. 1122024</p>
<p>The Role of Multimodal Data for Modeling Communication in Artificial Social Agents. S Gross, B Krenn, 2023</p>
<p>CrossCLR: Cross-modal contrastive learning for multi-modal video representations. M Zolfaghari, Y Zhu, P Gehler, T Brox, Proc. ICCV, 2021. ICCV, 2021</p>
<p>Spatio-temporal self-supervised learning for traffic flow prediction. J Ji, J Wang, C Huang, J Wu, B Xu, Z Wu, J Zhang, Y Zheng, Proc. AAAI. AAAI2023</p>
<p>Scaling large language modelbased multi-agent collaboration. C Qian, Z Xie, Y Wang, W Liu, K Zhu, H Xia, Y Dang, Z Du, W Chen, C Yang, Z Liu, M Sun, Proc. ICLR, 2025. ICLR, 2025</p>
<p>Seeking neural nuggets: Knowledge transfer in large language models from a parametric perspective. M Zhong, C An, W Chen, J Han, P He, Proc. ICLR. ICLR2024</p>
<p>Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. N Tandon, A Madaan, P Clark, Y Yang, Proc. NAACL-HLT (Findings). NAACL-HLT (Findings)2022</p>
<p>ELLE: efficient lifelong pre-training for emerging data. Y Qin, J Zhang, Y Lin, Z Liu, P Li, M Sun, J Zhou, Proc. ACL (Findings). ACL (Findings)2022</p>
<p>LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. D Jiang, X Ren, B Y Lin, Proc. ACL, 2023. ACL, 202314178</p>
<p>Joint language semantic and structure embedding for knowledge graph completion. J Shen, C Wang, L Gong, D Song, Proc. COLING. COLING2022</p>
<p>PiVe: Prompting with iterative verification improving graph-based generative capability of LLMs. J Han, N Collier, W Buntine, E Shareghi, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>How much knowledge can you pack into the parameters of a language model. A Roberts, C Raffel, N Shazeer, Proc. EMNLP. EMNLP2020</p>
<p>AutoAlign: fully automatic and effective knowledge graph alignment enabled by large language models. R Zhang, Y Su, B D Trisedya, X Zhao, M Yang, H Cheng, J Qi, IEEE Transactions on Knowledge and Data Engineering. 3662023</p>
<p>Sponge examples: Energy-latency attacks on neural networks. I Shumailov, Y Zhao, D Bates, N Papernot, R Mullins, R Anderson, Proc. EuroS&amp;P. EuroS&amp;P2021</p>
<p>An LLM can fool itself: A prompt-based adversarial attack. X Xu, K Kong, N Liu, L Cui, D Wang, J Zhang, M Kankanhalli, Proc. ICLR. ICLR2024</p>
<p>Large language models can be easily distracted by irrelevant context. F Shi, X Chen, K Misra, N Scales, D Dohan, E H Chi, N Schrli, D Zhou, Proc. ICML. ICML2023202227</p>
<p>Training socially aligned language models on simulated social interactions. R Liu, R Yang, C Jia, G Zhang, D Yang, S Vosoughi, Proc. ICLR. ICLR2024</p>
<p>Stable diffusion is unstable. C Du, Y Li, Z Qiu, C Xu, Proc. NeurIPS, 2023. NeurIPS, 2023</p>
<p>Transferable multimodal attack on vision-language pre-training models. H Wang, K Dong, Z Zhu, H Qin, A Liu, X Fang, J Wang, X Liu, Proc. SP. SP2024</p>
<p>An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models. H Luo, J Gu, F Liu, P Torr, Proc. ICLR. ICLR2024</p>
<p>Adversarial example does good: Preventing painting imitation from diffusion models via adversarial examples. C Liang, X Wu, Y Hua, J Zhang, Y Xue, T Song, Z Xue, R Ma, H Guan, Proc. ICML. ICML2023202</p>
<p>Don't listen to me: Understanding and exploring jailbreak prompts of large language models. Z Yu, X Liu, S Liang, Z Cameron, C Xiao, N Zhang, Proc. USENIX. USENIX2024</p>
<p>Sneakyprompt: Jailbreaking text-to-image generative models. Y Yang, B Hui, H Yuan, N Gong, Y Cao, Proc. SP, 2024. SP, 2024</p>
<p>do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. X Shen, Z Chen, M Backes, Y Shen, Y Zhang, Proc. CCS. CCS2024</p>
<p>Jailbreaker: Automated jailbreak across multiple large language model chatbots. G Deng, Y Liu, Y Li, K Wang, Y Zhang, Z Li, H Wang, T Zhang, Y Liu, Proc. NDSS. NDSS2024</p>
<p>BadRobot: Jailbreaking embodied LLMs in the physical world. H Zhang, C Zhu, X Wang, Z Zhou, C Yin, M Li, L Xue, Y Wang, S Hu, A Liu, P Guo, L Y Zhang, Proc. ICLR, 2025. ICLR, 2025</p>
<p>Tensor trust: Interpretable prompt injection attacks from an online game. S Toyer, O Watkins, E A Mendes, J Svegliato, L Bailey, T Wang, I Ong, K Elmaaroufi, P Abbeel, T Darrell, A Ritter, S Russell, Proc. ICLR. ICLR2024</p>
<p>Not what you've signed up for: Compromising real-world LLM-integrated applications with indirect prompt injection. K Greshake, S Abdelnabi, S Mishra, C Endres, T Holz, M Fritz, Proc. AIsec. AIsec2023</p>
<p>CheatAgent: Attacking LLM-empowered recommender systems via LLM agent. L.-B Ning, S Wang, W Fan, Q Li, X Xu, H Chen, F Huang, Proc. ACM KDD. ACM KDD2024</p>
<p>Towards building a robust toxicity predictor. D Bespalov, S Bhabesh, Y Xiang, L Zhou, Y Qi, Proc. ACL, 2023. ACL, 2023</p>
<p>AdvAug: Robust adversarial augmentation for neural machine translation. Y Cheng, L Jiang, W Macherey, J Eisenstein, Proc. ACL. ACL2020</p>
<p>Certifying LLM safety against adversarial prompting. A Kumar, C Agarwal, S Srinivas, S Feizi, H Lakkaraju, arXiv:2309.027052023arXiv preprint</p>
<p>LLM self defense: By self examination, LLMs know they are being tricked. M Phute, A Helbling, M Hull, S Peng, S Szyller, C Cornelius, D H Chau, Proc. ICLR. ICLR2024</p>
<p>AutoDefense: Multiagent LLM defense against jailbreak attacks. Y Zeng, Y Wu, X Zhang, H Wang, Q Wu, NeurIPS Safe Generative AI Workshop 2024. 2024</p>
<p>Improving the robustness of transformer-based large language models with dynamic attention. L Shen, Y Pu, S Ji, C Li, X Zhang, C Ge, T Wang, Proc. NDSS. NDSS2024</p>
<p>Automatically auditing large language models via discrete optimization. E Jones, A Dragan, A Raghunathan, J Steinhardt, Proc. ICML, 2023. ICML, 2023329</p>
<p>RedAgent: Red teaming large language models with contextaware autonomous language agent. H Xu, W Zhang, Z Wang, F Xiao, R Zheng, Y Feng, Z Ba, K Ren, arXiv:2407.166672024arXiv preprint</p>
<p>You autocomplete me: Poisoning vulnerabilities in neural code completion. R Schuster, C Song, E Tromer, V Shmatikov, Proc. USENIX, 2021. USENIX, 2021</p>
<p>Poisoning language models during instruction tuning. A Wan, E Wallace, S Shen, D Klein, Proc. ICLR, 2023. ICLR, 202335425</p>
<p>Agent security bench (ASB): Formalizing and benchmarking attacks and defenses in LLM-based agents. H Zhang, J Huang, K Mei, Y Yao, Z Wang, C Zhan, H Wang, Y Zhang, Proc. ICLR, 2025. ICLR, 2025</p>
<p>WebArena: A realistic web environment for building autonomous agents. S Zhou, F F Xu, H Zhu, X Zhou, R Lo, A Sridhar, X Cheng, T Ou, Y Bisk, D Fried, U Alon, G Neubig, Proc. ICLR. ICLR2024</p>
<p>Breaking agents: Compromising autonomous LLM agents through malfunction amplification. B Zhang, Y Tan, Y Shen, A Salem, M Backes, S Zannettou, Y Zhang, arXiv:2407.208592024arXiv preprint</p>
<p>AgentMonitor: A plug-and-play framework for predictive and secure multi-agent systems. C.-M Chan, J Yu, W Chen, C Jiang, X Liu, W Shi, Z Liu, W Xue, Y Guo, arXiv:2408.149722024arXiv preprint</p>
<p>Defending against weight-poisoning backdoor attacks for parameter-efficient finetuning. S Zhao, L Gan, L A Tuan, J Fu, L Lyu, M Jia, J Wen, Proc. NAACL Findings. NAACL Findings2024</p>
<p>Mitigating data poisoning in text classification with differential privacy. C Xu, J Wang, F Guzmn, B Rubinstein, T Cohn, Proc. EMNLP Findings. EMNLP Findings2021</p>
<p>LMSanitator: Defending prompt-tuning against task-agnostic backdoors. C Wei, W Meng, Z Zhang, M Chen, M Zhao, W Fang, L Wang, Z Zhang, W Chen, Proc. NDSS. NDSS2024</p>
<p>Rickrolling the artist: Injecting backdoors into text encoders for text-to-image synthesis. L Struppek, D Hintersdorf, K Kersting, Proc. ICCV, 2023. ICCV, 2023</p>
<p>Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. J Xu, M D Ma, F Wang, C Xiao, M Chen, Proc. NAACL. NAACL2024</p>
<p>BadChain: Backdoor chain-of-thought prompting for large language models. Z Xiang, F Jiang, Z Xiong, B Ramasubramanian, R Poovendran, B Li, Proc. ICLR. ICLR2024</p>
<p>Mitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification. C Chen, J Dai, Neurocomputing. 4522021</p>
<p>Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. B Wang, Y Yao, S Shan, H Li, B Viswanath, H Zheng, B Y Zhao, Proc. SP. SP2019</p>
<p>Are large pre-trained language models leaking your personal information. J Huang, H Shao, K C , -C Chang, Proc. EMNLP, Findings, 2022. EMNLP, Findings, 2022</p>
<p>ETHICIST: Targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation. Z Zhang, J Wen, M Huang, Proc. ACL, 2023. ACL, 202312687</p>
<p>Teach LLMs to phish: Stealing private information from language models. A Panda, C A Choquette-Choo, Z Zhang, Y Yang, P Mittal, Proc. ICLR. ICLR2024</p>
<p>Beyond memorization: Violating privacy via inference with large language models. R Staab, M Vero, M Balunovic, M Vechev, Proc. ICLR, 2024. ICLR, 2024</p>
<p>Extracting training data from diffusion models. N Carlini, J Hayes, M Nasr, M Jagielski, V Sehwag, F Tramr, B Balle, D Ippolito, E Wallace, Proc. USENIX. USENIX2023</p>
<p>Quantifying privacy risks of masked language models using membership inference attacks. F Mireshghallah, K Goyal, A Uniyal, T Berg-Kirkpatrick, R Shokri, Proc. EMNLP. EMNLP2022</p>
<p>An efficient membership inference attack for the diffusion model by proximal initialization. F Kong, J Duan, R Ma, H T Shen, X Shi, X Zhu, K Xu, Proc. ICLR. ICLR2024</p>
<p>User inference attacks on large language models. N Kandpal, K Pillutla, A Oprea, P Kairouz, C Choquette-Choo, Z Xu, Proc. NeurIPS International Workshop on Federated Learning in the Age of Foundation Models. NeurIPS International Workshop on Federated Learning in the Age of Foundation Models2023</p>
<p>An empirical analysis of memorization in fine-tuned autoregressive language models. F Mireshghallah, A Uniyal, T Wang, D Evans, T Berg-Kirkpatrick, Proc. EMNLP. EMNLP2022</p>
<p>Membership inference attacks against fine-tuned large language models via self-prompt calibration. W Fu, H Wang, C Gao, G Liu, Y Li, T Jiang, Proc. NeurIPS. NeurIPS2024</p>
<p>Privacy risks of general-purpose language models. X Pan, M Zhang, S Ji, M Yang, Proc. SP. SP2020</p>
<p>Property existence inference against generative models. L Wang, J Wang, J Wan, L Long, Z Yang, Z Qin, Proc. USENIX. USENIX2024</p>
<p>Deduplicating training data mitigates privacy risks in language models. N Kandpal, E Wallace, C Raffel, Proc. ICML, 2022. ICML, 202210707</p>
<p>Learning and evaluating a differentially private pre-trained language model. S Hoory, A Feder, A Tendler, S Erell, A Peled-Cohen, I Laish, H Nakhost, U Stemmer, A Benjamini, A Hassidim, Y Matias, Proc. EMNLP Findings. EMNLP Findings2021</p>
<p>ProPILE: Probing privacy leakage in large language models. S Kim, S Yun, H Lee, M Gubri, S Yoon, S J Oh, Proc. NeurIPS, 2023. NeurIPS, 2023</p>
<p>Stealing hyperparameters in machine learning. B Wang, N Z Gong, Proc. SP. SP2018</p>
<p>Thieves on sesame street! model extraction of BERT-based APIs. K Krishna, G S Tomar, A P Parikh, N Papernot, M Iyyer, Proc. ICLR. ICLR2020</p>
<p>Stealing the decoding algorithms of language models. A Naseh, K Krishna, M Iyyer, A Houmansadr, Proc. CCS, 2023. CCS, 2023</p>
<p>On extracting specialized code abilities from large language models: A feasibility study. Z Li, C Wang, P Ma, C Liu, S Wang, D Wu, C Gao, Y Liu, Proc. ICSE. ICSE2024</p>
<p>Lion: Adversarial distillation of proprietary large language models. Y Jiang, C Chan, M Chen, W Wang, Proc. EMNLP, 2023. EMNLP, 2023</p>
<p>Model inversion attacks that exploit confidence information and basic countermeasures. M Fredrikson, S Jha, T Ristenpart, Proc. CCS. CCS2015</p>
<p>Prompt stealing attacks against text-to-image generation models. X Shen, Y Qu, M Backes, Y Zhang, Proc. USENIX. USENIX2024</p>
<p>Prompt stealing attacks against large language models. Z Sha, Y Zhang, arXiv:2402.129592024arXiv preprint</p>
<p>PLeak: Prompt leaking attacks against large language model applications. B Hui, H Yuan, N Gong, P Burlina, Y Cao, Proc. CCS. CCS2024</p>
<p>A watermark for large language models. J Kirchenbauer, J Geiping, Y Wen, J Katz, I Miers, T Goldstein, Proc. ICML. ICML20231784</p>
<p>Blockchain-based efficient and trustworthy AIGC services in metaverse. Y Lin, Z Gao, H Du, D Niyato, J Kang, Z Xiong, Z Zheng, IEEE Transactions on Services Computing. 1752024</p>
<p>Samsung bans ChatGPT, AI chatbots after data leak blunder. C Mauran, </p>
<p>Trusted AI in multiagent systems: An overview of privacy and security for distributed learning. C Ma, J Li, K Wei, B Liu, M Ding, L Yuan, Z Han, H Vincent Poor, Proceedings of the IEEE. 11192023</p>
<p>ToTTo: A controlled table-to-text generation dataset. A P Parikh, X Wang, S Gehrmann, M Faruqui, B Dhingra, D Yang, D Das, Proc. EMNLP. EMNLP2020</p>
<p>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. P Manakul, A Liusie, M J F Gales, Proc. EMNLP, 2023. EMNLP, 2023</p>
<p>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. A Mallen, A Asai, V Zhong, R Das, D Khashabi, H Hajishirzi, Proc. ACL. ACL2023</p>
<p>Teaching language models to hallucinate less with synthetic tasks. E Jones, H Palangi, C S Ribeiro, V Chandrasekaran, S Mukherjee, A Mitra, A H Awadallah, E Kamar, Proc. ICLR. ICLR2024</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>Mitigating hallucination in large multi-modal models via robust instruction tuning. F Liu, K Lin, L Li, J Wang, Y Yacoob, L Wang, Proc. ICLR, 2023. ICLR, 2023</p>
<p>Sources of hallucination by large language models on inference tasks. N Mckenna, T Li, L Cheng, M J Hosseini, M Johnson, M Steedman, Proc. EMNLP Findings. EMNLP Findings2023</p>
<p>Deduplicating training data makes language models better. K Lee, D Ippolito, A Nystrom, C Zhang, D Eck, C Callison-Burch, N Carlini, Proc. ACL. ACL2022</p>
<p>The RefinedWeb dataset for falcon LLM: outperforming curated corpora with web data only. G Penedo, Q Malartic, D Hesslow, R Cojocaru, H Alobeidli, A Cappelli, B Pannier, E Almazrouei, J Launay, Proc. NeurIPS. NeurIPS2023</p>
<p>Factuality enhanced language models for open-ended text generation. N Lee, W Ping, P Xu, M Patwary, P Fung, M Shoeybi, B Catanzaro, Proc. NeurIPS. NeurIPS2022</p>
<p>Entity-based knowledge conflicts in question answering. S Longpre, K Perisetla, A Chen, N Ramesh, C Dubois, S Singh, Proc. EMNLP, 2021. EMNLP, 2021</p>
<p>Discovering language model behaviors with model-written evaluations. E Perez, S Ringer, K Lukosiute, Proc. ACL Findings. ACL Findings202313434</p>
<p>How language model hallucinations can snowball. M Zhang, O Press, W Merrill, A Liu, N A Smith, Proc. ICML. ICML2024</p>
<p>Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. N Mndler, J He, S Jenko, M Vechev, Proc. ICLR, 2024. ICLR, 2024</p>
<p>IN-SIDE: LLMs' internal states retain the power of hallucination detection. C Chen, K Liu, Z Chen, Y Gu, Y Wu, M Tao, Z Fu, J Ye, Proc. ICLR. ICLR2024</p>
<p>RARR: researching and revising what language models say, using language models. L Gao, Z Dai, P Pasupat, A Chen, A T Chaganty, Y Fan, V Y Zhao, N Lao, H Lee, D Juan, K Guu, Proc. ACL. ACL202316508</p>
<p>Analyzing and mitigating object hallucination in large visionlanguage models. Y Zhou, C Cui, J Yoon, L Zhang, Z Deng, C Finn, M Bansal, H Yao, Proc. ICLR. ICLR2024</p>
<p>Controlling hallucinations at word level in data-to-text generation. C Rebuffel, M Roberti, L Soulier, G Scoutheeten, R Cancelliere, P Gallinari, Data Mining and Knowledge Discovery. 2022</p>
<p>On hallucination and predictive uncertainty in conditional language generation. Y Xiao, W Y Wang, Proc. EACL, 2021. EACL, 2021</p>
<p>BiasAdv: Bias-adversarial augmentation for model debiasing. J Lim, Y Kim, B Kim, C Ahn, J Shin, E Yang, S Han, Proc. CVPR, 2023. CVPR, 2023</p>
<p>Mitigating intensity bias in shadow detection via feature decomposition and reweighting. L Zhu, K Xu, Z Ke, R W Lau, Proc. CVPR, 2021. CVPR, 2021</p>
<p>A review of trustworthy and explainable artificial intelligence (XAI). V Chamola, V Hassija, A R Sulthana, D Ghosh, D Dhingra, B Sikdar, IEEE Access. 11152023</p>
<p>Cyber-physical zero trust architecture for industrial cyber-physical systems. X Feng, S Hu, IEEE Transactions on Industrial Cyber-Physical Systems. 12023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1296 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1296</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1296</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-211506949</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2002.11635v1.pdf" target="_blank">Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization</a></p>
                <p><strong>Paper Abstract:</strong> In this work we show how to use the Operational Space Control framework (OSC) under joint and cartesian constraints for reinforcement learning in cartesian space. Our method is therefore able to learn fast and with adjustable degrees of freedom, while we are able to transfer policies without additional dynamics randomizations on a KUKA LBR iiwa peg in-hole task. Before learning in simulation starts, we perform a system identification for aligning the simulation environment as far as possible with the dynamics of a real robot. Adding constraints to the OSC controller allows us to learn in a safe way on the real robot or to learn a flexible, goal conditioned policy that can be easily transferred from simulation to the real robot.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1296.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1296.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PyBullet physics simulation engine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source rigid-body physics simulator used to train reinforcement-learning policies by simulating a KUKA LBR iiwa robot and Weiss WSG50 gripper at a 5 ms simulation interval with direct torque control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A real-time rigid-body physics simulator (Bullet-based) used here to simulate the KUKA LBR iiwa 14kg manipulator with an attached Weiss WSG50 gripper; authors command joint torques directly in simulation with a 5 ms simulation step.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity approximate dynamics simulator — rigid-body dynamics with contact modeling but limited accuracy for complex contact phenomena (e.g., friction detail, soft-body deformation, snap-in events).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Direct torque control at joint level; models link masses, joint damping, gravity; discrete time step 5 ms; contact and collision modeled via Bullet engine; Coriolis terms ignored in controller (authors found inclusion did not help); does not accurately model complex friction, soft bodies, or snap-in events without additional modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Soft Actor-Critic (SAC) policy for peg-in-hole</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An off-policy deep reinforcement learning agent (SAC) using a neural network policy maximizing reward + entropy; action outputs are cartesian force/torque commands (operational-space commands) and a desired rotation parameter, executed via an OSC controller to map to joint torques.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learn control policy for peg-in-hole insertion (goal-conditioned search and insertion under contact) on a KUKA manipulator, including recovery from perturbations and variable goal positions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Qualitative: policies trained without dynamics randomization converged faster, were more stable, and reached higher final performance in simulation than randomized training; no numeric training reward is reported (plots shown, but no numeric values provided).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world KUKA LBR iiwa robot (peg-in-hole insertion task) with Weiss WSG50 gripper</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>High transfer success: policies trained in PyBullet with OSC + system identification transferred with near-perfect success rates across tested target positions (many reported results 20/20 successful insertions across positions); dynamics-randomized policies transferred slightly worse in some cases (e.g., one reported case for a triangular peg: 10/20 successful insertions). Policies trained without dynamics randomization were robust to perturbations (e.g., recover from human-applied forces and recover from target-location perturbations up to ~2 cm).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Paper compares no dynamics randomization (but with system identification and OSC/high-quality model) versus dynamics/environment randomization. Result: training without dynamics randomization (combined with good system identification and OSC) yielded faster learning, higher final performance, more stability, and as-good-or-better sim-to-real transfer than training with dynamics randomization, which made learning more unstable and sometimes failed.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue that with a high-quality dynamics model obtained via system identification and the use of OSC (operational-space control), full dynamics randomization is unnecessary for successful sim-to-real transfer for their contact-rich peg-in-hole task; they explicitly state simulators like PyBullet or MuJoCo struggle to model friction, soft objects and snap-in events accurately, implying those features are not required at high fidelity for this task when system identification + OSC are used, but would be necessary for other contact phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Initial transfers failed before performing system identification — real robot dynamics differed enough that direct transfer performed poorly. Dynamics-randomized training sometimes produced unstable learning and worse transfer; a dynamics-randomized policy tested with a triangular peg showed degraded transfer (reported example: 10/20 successes). The authors also note that PyBullet (and MuJoCo) can fail to model friction, soft contacts and snap-in behaviors well, which limits transfer for tasks where those phenomena dominate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1296.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1296.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo (Multi-Joint dynamics with Contact)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performance physics engine for robotics and biomechanics often used for RL research; here it is mentioned for comparison as a simulator similar to PyBullet but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A commercial/academic rigid-body dynamics simulator with contact modelling and efficient computation, commonly used for reinforcement learning benchmarks; mentioned as an example of simulators the authors may want to test in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium- to high-fidelity for rigid-body dynamics and contact, but limited for high-fidelity modeling of frictional micro-behaviors, soft bodies, and snap-in contact events according to the authors' discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Models rigid-body dynamics and contacts efficiently; authors note limitations in accurately simulating friction details, soft-body deformation, and snap-in events — these phenomena are difficult or impossible to simulate well with MuJoCo (per the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors mention MuJoCo as an example of commonly used simulators and note that certain contact phenomena (friction, soft objects, snap-in events) are difficult to simulate accurately in both MuJoCo and PyBullet; thus, specialized simulators may be required for those phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No experiments using MuJoCo are reported here; authors only note generally that MuJoCo (like PyBullet) struggles to model complex contact phenomena, which can limit sim-to-real transfer for tasks dominated by those effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dexterous in-hand manipulation <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer with neural-augmented robot simulation <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1296",
    "paper_id": "paper-211506949",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "PyBullet",
            "name_full": "PyBullet physics simulation engine",
            "brief_description": "Open-source rigid-body physics simulator used to train reinforcement-learning policies by simulating a KUKA LBR iiwa robot and Weiss WSG50 gripper at a 5 ms simulation interval with direct torque control.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "PyBullet",
            "simulator_description": "A real-time rigid-body physics simulator (Bullet-based) used here to simulate the KUKA LBR iiwa 14kg manipulator with an attached Weiss WSG50 gripper; authors command joint torques directly in simulation with a 5 ms simulation step.",
            "scientific_domain": "mechanics / robotic manipulation",
            "fidelity_level": "medium-fidelity approximate dynamics simulator — rigid-body dynamics with contact modeling but limited accuracy for complex contact phenomena (e.g., friction detail, soft-body deformation, snap-in events).",
            "fidelity_characteristics": "Direct torque control at joint level; models link masses, joint damping, gravity; discrete time step 5 ms; contact and collision modeled via Bullet engine; Coriolis terms ignored in controller (authors found inclusion did not help); does not accurately model complex friction, soft bodies, or snap-in events without additional modeling.",
            "model_or_agent_name": "Soft Actor-Critic (SAC) policy for peg-in-hole",
            "model_description": "An off-policy deep reinforcement learning agent (SAC) using a neural network policy maximizing reward + entropy; action outputs are cartesian force/torque commands (operational-space commands) and a desired rotation parameter, executed via an OSC controller to map to joint torques.",
            "reasoning_task": "Learn control policy for peg-in-hole insertion (goal-conditioned search and insertion under contact) on a KUKA manipulator, including recovery from perturbations and variable goal positions.",
            "training_performance": "Qualitative: policies trained without dynamics randomization converged faster, were more stable, and reached higher final performance in simulation than randomized training; no numeric training reward is reported (plots shown, but no numeric values provided).",
            "transfer_target": "Real-world KUKA LBR iiwa robot (peg-in-hole insertion task) with Weiss WSG50 gripper",
            "transfer_performance": "High transfer success: policies trained in PyBullet with OSC + system identification transferred with near-perfect success rates across tested target positions (many reported results 20/20 successful insertions across positions); dynamics-randomized policies transferred slightly worse in some cases (e.g., one reported case for a triangular peg: 10/20 successful insertions). Policies trained without dynamics randomization were robust to perturbations (e.g., recover from human-applied forces and recover from target-location perturbations up to ~2 cm).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Paper compares no dynamics randomization (but with system identification and OSC/high-quality model) versus dynamics/environment randomization. Result: training without dynamics randomization (combined with good system identification and OSC) yielded faster learning, higher final performance, more stability, and as-good-or-better sim-to-real transfer than training with dynamics randomization, which made learning more unstable and sometimes failed.",
            "minimal_fidelity_discussion": "Authors argue that with a high-quality dynamics model obtained via system identification and the use of OSC (operational-space control), full dynamics randomization is unnecessary for successful sim-to-real transfer for their contact-rich peg-in-hole task; they explicitly state simulators like PyBullet or MuJoCo struggle to model friction, soft objects and snap-in events accurately, implying those features are not required at high fidelity for this task when system identification + OSC are used, but would be necessary for other contact phenomena.",
            "failure_cases": "Initial transfers failed before performing system identification — real robot dynamics differed enough that direct transfer performed poorly. Dynamics-randomized training sometimes produced unstable learning and worse transfer; a dynamics-randomized policy tested with a triangular peg showed degraded transfer (reported example: 10/20 successes). The authors also note that PyBullet (and MuJoCo) can fail to model friction, soft contacts and snap-in behaviors well, which limits transfer for tasks where those phenomena dominate.",
            "uuid": "e1296.0",
            "source_info": {
                "paper_title": "Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo (Multi-Joint dynamics with Contact)",
            "brief_description": "A high-performance physics engine for robotics and biomechanics often used for RL research; here it is mentioned for comparison as a simulator similar to PyBullet but not used in experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo",
            "simulator_description": "A commercial/academic rigid-body dynamics simulator with contact modelling and efficient computation, commonly used for reinforcement learning benchmarks; mentioned as an example of simulators the authors may want to test in future work.",
            "scientific_domain": "mechanics / robotic manipulation",
            "fidelity_level": "medium- to high-fidelity for rigid-body dynamics and contact, but limited for high-fidelity modeling of frictional micro-behaviors, soft bodies, and snap-in contact events according to the authors' discussion.",
            "fidelity_characteristics": "Models rigid-body dynamics and contacts efficiently; authors note limitations in accurately simulating friction details, soft-body deformation, and snap-in events — these phenomena are difficult or impossible to simulate well with MuJoCo (per the paper).",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors mention MuJoCo as an example of commonly used simulators and note that certain contact phenomena (friction, soft objects, snap-in events) are difficult to simulate accurately in both MuJoCo and PyBullet; thus, specialized simulators may be required for those phenomena.",
            "failure_cases": "No experiments using MuJoCo are reported here; authors only note generally that MuJoCo (like PyBullet) struggles to model complex contact phenomena, which can limit sim-to-real transfer for tasks dominated by those effects.",
            "uuid": "e1296.1",
            "source_info": {
                "paper_title": "Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dexterous in-hand manipulation",
            "rating": 2,
            "sanitized_title": "learning_dexterous_inhand_manipulation"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Sim-to-real transfer with neural-augmented robot simulation",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_with_neuralaugmented_robot_simulation"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        }
    ],
    "cost": 0.0089575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization</p>
<p>Manuel Kaspar 
Juan David 
Munoz Osorio 
Juergen Bock 
Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization
1
In this work we show how to use the Operational Space Control framework (OSC) under joint and cartesian constraints for reinforcement learning in cartesian space. Our method is therefore able to learn fast and with adjustable degrees of freedom, while we are able to transfer policies without additional dynamics randomizations on a KUKA LBR iiwa pegin-hole task. Before learning in simulation starts, we perform a system identification for aligning the simulation environment as far as possible with the dynamics of a real robot. Adding constraints to the OSC controller allows us to learn in a safe way on the real robot or to learn a flexible, goal conditioned policy that can be easily transferred from simulation to the real robot. 1</p>
<p>Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization</p>
<p>Manuel Kaspar, Juan David Munoz Osorio, Juergen Bock * Abstract-In this work we show how to use the Operational Space Control framework (OSC) under joint and cartesian constraints for reinforcement learning in cartesian space. Our method is therefore able to learn fast and with adjustable degrees of freedom, while we are able to transfer policies without additional dynamics randomizations on a KUKA LBR iiwa pegin-hole task. Before learning in simulation starts, we perform a system identification for aligning the simulation environment as far as possible with the dynamics of a real robot. Adding constraints to the OSC controller allows us to learn in a safe way on the real robot or to learn a flexible, goal conditioned policy that can be easily transferred from simulation to the real robot. 1</p>
<p>I. INTRODUCTION</p>
<p>Most of today's Reinforcement Learning (RL) research with robots is still dealing with artificially simplified tasks, that do not reach the requirements of industrial problems. This is partly due to the fact that training on real robots is very time-consuming. Moreover, it is not trivial to setup a system where the robot can learn a task, but does not damage itself or any task relevant items. Therefore, the idea of sim to real transfer [1] was introduced. While this idea seems convincing in the first place, bridging the reality gap is a major difficulty, especially when contact dynamics, soft bodies etc. are involved, where dynamics are difficult to simulate. This paper investigates possibilities for sim to real transfer while trying to make the task to learn as easy as possible by using the Operational Space Control framework (OSC) [2]. The controller takes care of the redundancy resolution and allows to reduce the task dimension. For instance, our current setup tries to perform a peg-in-hole task as shown in Fig. 1, where we currently fix two rotational dimensions as we know the required final rotation and just learn the necessary translation and θ-rotation (around the vertical axis) for a successful insertion.</p>
<p>However, pure OSC does not contain information about joint or cartesian limits. We solved that problem by using a novel approach to avoid joint and cartesian limits [3] [4]. In this way, the output of the controller are joint torques to command the robot that respect joint and cartesian constraints. By limiting not only position but also acceleration and velocity in joint and cartesian space, we avoid damages of the robot or the environment. Moreover, the compliance achieved by using torque control enables the robot to learn tasks, that require robot-environment contacts.</p>
<p>In our opinion those are tasks where RL can bring benefits compared to traditional techniques. This paper presents a novel approach of integrating RL with OSC, which outperforms traditional approaches that are based on dynamics randomization. Moreover, the combination of RL and OSC bears benefits by avoiding damages of the robot and/or its environment through joint and cartesian constraints. A video of the results can be found under https://sites.google.com/view/ rl-wo-dynamics-randomization.</p>
<p>II. RELATED WORK</p>
<p>Over the past years an increasing number of works tried to use sim to real transfer for learning robotic control: Progressive Nets [5] were proposed for giving the neural network a flexible way of using or not using past experience which was collected in simulation, when fine tuning on a real system. Successful sim to real transfer for robots was demonstrated by [6] and [7] where in hand manipulation of a cube is learned while also the degree of randomization is adjusted dynamically. In [1] a policy to move an object to a specific position on a table is learned. The work introduced and analyzed the idea of dynamics randomization in simulation. Golemo et al. [8] try to learn the differences between the real robot and the simulator and then augment the simulation to be closer to the real robot. This is basically a form of system identification, where instead of finding a right set of parameters for a simulator a more sophisticated identification model is learned. Van Baar et al. [9] perform dynamics randomization for solving a maze game and report easier fine tuning after training a randomized policy in simulation. In [10] an independent perception and control module is used, while the perception module creates arXiv:2002.11635v1 [cs.AI] 19 Feb 2020 a semantic map of the scene. The control module then uses this map as part of its observations. This approach is good for transferring the perception part of a problem from simulation to reality, while the problem of transferring dynamics uncertainties is not discussed in this paper. Yan et al. [11] use Dagger [12] to learn grasping in simulation and by expert demonstration. As they perform position control and have a rather easy gripping setup, they do not have to deal with erroneous robot or contact dynamics. Like previous work they use a semantic map in their perception module. Tan et al. [13] perform sim to real transfer on learning gates for quadruped robots. They use the Bullet [14] simulation engine (with some improvements) and perform a system identification and dynamics randomization. Furthermore, they find that a compact observation space is helpful for sim to real transfer, because the policy can not overfit to unimportant details of the observation. Breyer et al. [15] try to learn grasping objects, leveraging an RL formulation of the problem. They train on some objects in simulation and then transfer the policy to an ABB YuMI. They also use some kind of curriculum learning by starting with a small workspace and then increasing its size.</p>
<p>Inoue et al. [16] show how to use a recurrent network to learn search and insertion actions on a high precision assembly task. While they achieve success on insertion with high precision requirements, it is only directly applicable to search and insertion tasks. They train two separate networks and need a initial calibration of the system. Furthermore, they apply some form of curriculum learning by increasing the initial offset of the peg. They do not use a simulation environment but directly train on the robot. In [17] strategies of insertion are learned in task space by using a large number of demonstrations. We think that our work can figure out strategies more efficiently then leveraging hundreds of demonstrations from humans. Chebotar et. al [18] tried estimating parameters of the robot and process from rollouts on the real robot. In the work of Lee et. al [19] a representation of sensory inputs is learned for performing a peg in hole task, while several sensor modalities are used. They use the Operational Space Control framework with an impedance controller and do also command a 4 DOF action vector. While using multimodal sensor inputs is an interesting direction, we believe that the insertion performance of our system regarding generalization is comparable to their multimodal system, without additional sensors, while our system runs faster and is more flexible regarding start and target locations.</p>
<p>III. REINFORCEMENT LEARNING</p>
<p>Reinforcement learning is the task to find a policy π(a t |s t ) which selects actions a t while observing the state of the environment s t . The selected actions should maximize a reward r(s t , a t ). The state s t+1 and s t are connected over (stochastic) dynamics p(s t+1 |s t , a t ) which finally creates the trajectory τ : (s 0 , a 0 , s 1 , a 1 , ..., s t , a t ).</p>
<p>In our case the observation vector s t contains following variables:
• Joint angles [q 1 ...q 7 ] • End effector x, y z positions [ee x , ee y , ee z ] • End effector theta rotation [ee θ ] • End effector velocities [ėe x ,ėe y ,ėe z ]
The target position of the hole is implicitly encoded into the observation vector. E.g. for the X-dimension ee x = ee xcur − ee xtarget . ee xcur describes the currently measured X-position of the flange, ee xtarget the target x-position in the hole. This gives us a goal-conditioned policy.</p>
<p>As an option to give the policy a better hint about the recent history, we also tested stacking n past observations and actions into the observation vector thereby trying to recover the Markov-condition [20] and giving the network the possibility to figure out the dynamics of the system.</p>
<p>When the observations are stacked we use those values and the last actions and stack it to
s = (s t , a t , s t−1 , a t−1 , ..., s t−n , a t−n ) T(1)
The details of the action vector a t is described in IV-A.</p>
<p>In this work we used the Soft-Actor-Critic (SAC) algorithm explained in [21]. We also tried the PPO and DDPG implementation from SurrealAI [22] but found, that in our experiments SAC was much more sample efficient and stable.</p>
<p>We also investigated the Guided Policy Search algorithm [23] which we found to learn easy tasks really fast. Also Levine et al. showed the general applicability to real world robotics tasks and even integrated vision [24], we found that the technique strongly depends on the right set of hyperparameters and often fails, when moving to higher dimensional action spaces.</p>
<p>What makes the Soft-Actor-Critic algorithm so powerful is the fact, that not only a reward r is maximized, but also the entropy of the actor. The usage of this maximum entropy framework leads to robust policies, that do not collapse into a single successful trajectory but explore the complete range of successful trajectories. This makes the algorithm especially suitable for performing fine tuning on the real robot, after training in simulation. The objective in the maximum entropy framework is
π = arg max π t E (st,at)∼pπ [r(s t , a t ) + αH(π(·|s t ))] (2)
where α is an automatically adjusted temperature parameter that determines the importance of the entropy term. For more details of the SAC algorithm please take a look at [25]. The algorithm itself works as shown in 1.</p>
<p>SAC is furthermore an off-policy algorithm, what makes it more sample efficient than algorithms like PPO, that also showed to be capable of learning complex policies [6] and also worked for our task (but slower).</p>
<p>IV. OPERATIONAL SPACE CONTROL Typically, in OSC, the operational point (in our case, the end effector) is modeled to behave as a unit mass spring damper system:
f * = Ke − DẊ,(3)
Algorithm 1 Sampling strategy in the Soft-Actor-Critic algorithm [25] 1: Initialize policy π, critic Q and replay buffer R 2: for i &lt; max iterations do 3: for n &lt; environment steps do 4: a t ∼ π θ (a t |s t ) 5:
s t+1 ∼ p(s t+1 |s t , a t ) 6: R ← R ∪ (s t , a t , r(s t , a t ), s t+1 ) 7:
end for 8: for each gradient step do 9:</p>
<p>Get batch from R 10:</p>
<p>Update π and Q like in Haarnoja et. al [25] 11:</p>
<p>end for 12: end for where f * is the command vector,Ẋ is the vector velocity of the end effector and e is the vector error, that is the difference between the current and the desired offset position of the end effector. K and D are diagonal matrices that represent the stiffness and damping of the system.</p>
<p>RL actions are directly applied on the command vector f * and are then mapped to the joint space to command the robot using the OSC equation:
τ = J T (Λf * ) + N τ any ,(4)
where Λ is the inertia matrix in the operational space, J is the Jacobian that maps the joint space into the cartesian space and τ is the vector of command torques that can be send to command the robot. The gravity compensation is done by the lowest torque controller level. Note that the Coriolis terms are despised. In practice, due to inaccuracy of the dynamic model, the performance does not increase by the inclusion of these terms. N = I − J TJ T is the null space projector of J and it exists only for redundant cases (the dimension of f * is smaller than the number of joints of the robot n), with the dynamically consistent Jacobian pseudo inverseJ = M −1 J T Λ. τ any is any torque vector that does not produce any accelerations in the space of the main task, typically choosen to reduce the kinetic energy as τ any = M (−k jointDampq ) where k jointDamp is a joint damper term.</p>
<p>To run a policy on the real robot without breaking or stopping the robot while learning, constraints as joint position and velocity limits should be included in the control algorithm. Cartesian limits are also useful to reduce the work space of learning or to protect the robot to damage itself or objects in the environment.</p>
<p>A. Inclusion of unilateral constraints</p>
<p>The classic approach to avoid joint limits or cartesian obstacles is to implement potential fields in the proximity to the limit. However, this approach requires a proper setting of the parameters to avoid oscillations or to have a smooth behavior in the control law as shown in [26], [3]. In [3], a simple approach that overcomes these problems is presented.</p>
<p>The Saturation in Joint Space (SJS), algorithm 2, works by estimating the joint accelerations produced by the torque computed from e.g. the impedance law in eq. 4 (or other task or stack of tasks), and then saturating the joint to its limit (in case of possible violation of the limit). The desired force Λf * is then achieved at best by the remaining joints. The output of the algorithm is the command torque vector τ c that respect the joint limits. Note that a Jacobian that maps from the space of the saturated joints to the whole joint space is denoted by J lim and it is defined by:
J lim = 0 1 0 0 0 0 0 0(5)
if for instance, the second joint is saturated. To have a better understanding of the SJS approach see [3]. 
τ sjs = τ lim + N lim τ 4:q = M −1 (τ sjs − g − c) 5:Q max = min(2 (Qmax−q−qdt) dt 2 , (Vmax−q) dt , A max ) 6:Q min = max(2 (Qmin−q−qdt) dt 2 , (Vmin−q) dt , A min ) 7:q sat,i = Q max,i ifq i &gt;Q max,ï Q min,i ifq i <Q min,i 8: f * lim =q sat 9: τ lim = J T lim (Λ lim f * lim )
10:
N lim = I − J T limJ T lim 11: whileq i >Q max,i orq i <Q min,i
To avoid cartesian limits a similar algorithm to 2 is used [4]. The only difference is that everything must be defined in the cartesian space. Algorithm 3 shows how the process works. J ev does the mapping between the sub space of the cartesian space that is being limited and the joint space. For instance, if only the cartesian position is being limited J ev is the first three rows of the whole Jacobian. Note that J lim must do the mapping only from the saturated cartesian space to the Joint space, e.g., it is the third row of J lim if the z dimension gets saturated.  The final control law works by giving the torque vector τ from Eq. 4 to Algorithm 3 as input. The output vector τ scs is then given as input to Algorithm 2. The output vector τ sjs is then the torque vector that commands the robot. The highest priority is given to the joint limits avoidance that must be respected always. The cartesian limits will be respected as good as they do not interfere with joint limits avoidance. This control law allows now to learn a policy without breaking the robot or objects in the environment.
τ scs = τ lim + N lim τ 4:q = M −1 (τ scs − g − c) 5:ẍ = J evq +J evq 6:Ẍ max = min(2 (Xmax−x−ẋdt) dt 2 , (Vmax−x) dt , A max ) 7:Q min = max(2 (Xmin−x−ẋdt) dt 2 , (Vmin−x) dt , A min ) 8:ẍ sat,i = Ẍ max,i ifẍ i >Ẍ max,ï X min,i ifẍ i &lt;Ẍ
The action vector a t of the learning algorithm consists of
[f * x , f * y , f * z , θ des ]
. Translational commands f * x , f * y and f * z are given directly to eq. 4, while the rotational command f * θ is computed by θ des using eq.3. The error e is calculated in this case by quaternion algebra. Taking θ des instead of f * θ in a t showed slightly better performance.</p>
<p>V. LEARNING FLEXIBLE CARTESIAN COMMANDS BY</p>
<p>USING OPERATIONAL SPACE CONTROL In our approach we use the OSC to control the robot at torque level (&lt;= 5ms loop) and do learning on top of this layer (e.g. with 50ms). In detail our control scheme (OSC + SJS + SCS) allows us to have:</p>
<p>• Joint limit avoidance • Cartesian walls, where the robot experiences an adversarial force and cannot penetrate them • Velocity saturation (prohibits too fast motions)</p>
<p>A. System architecture</p>
<p>The system architecture is shown in Fig. 2. We use Python for running reinforcement learning algorithms and PyBullet [14] for simulation. Additionally we have a C++ program that runs the OSC algorithm and uses FRI (KUKA Fast Robotics Interface) [27] to command the robot or the simulation. This enables us to smoothly switch between simulation and the real robot. The fast C++ implementation ensures that FRI cycle times are met preventing the robot to stop due to timeout errors. For the simulation we developed a Python interface for FRI. The Python RL algorithm and the C++ controller algorithm communicate over gRPC.</p>
<p>B. Learn task specific cartesian dimensions</p>
<p>When learning torques it is almost always necessary to learn n joints together to perform an action. The problem increases with complex robots with high number of joints. Nevertheless, tasks like peg-in-hole are almost always easier solvable in cartesian space than in joint space. Therefore, we rely on the OSC-framework to map from cartesian commands to torques per joint. This gives us a large amount of flexibility to simplify the learning tasks, if necessary.</p>
<p>For instance, if we want to learn a 6 DOF cartesian task, we would still need to learn 7 torque dimensions for the LBR iiwa. In cartesian space it is enough to learn the 3 translational dimensions and the 3 rotational dimensions. If the necessary rotation of a task is clear, this can be given as a fixed setting to the OSC-framework as a task for holding this rotation, and then only the 3 translational dimensions need to be learned. Therefore every task specific combination is possible: 2
• XY ZABC • XY Z (with fixed rotation) • XY ZA • ZA • ...
XY ZA would, e.g., make sense for a peg-in-hole task where a quadratic object needs to be fitted and a rotation around this axis could be necessary to have the right rotation for aligning peg and hole. A combination XA could, e.g., be used for clipping an electrical component into a rail by performing an approach and rotate/clip motion.</p>
<p>VI. SIM TO REAL TRANSFER</p>
<p>A. Simulation environment</p>
<p>We use the PyBullet [14] simulation environment, where we load an KUKA LBR iiwa 14kg with appropriate dynamics values and an attached Weiss WSG50 gripper. We directly command torques to the joints of the robot and use a simulation interval of 5ms.</p>
<p>B. Dynamics and Environment Randomization</p>
<p>[6] and [1] performed dynamics and environment randomization for being able to transfer their policy from simulation to the real world. We found that when using the OSCframework, system identification and a high-quality model of the robot, we can transfer policies without additional dynamics randomization, which speeds up learning massively and also gives us a higher final performance. The only parameters we randomize is the start and goal location.</p>
<p>C. System Identification</p>
<p>In our first trials for using a policy, which was learned in simulation and transferred to the real robot, we found, that it worked pretty poorly. The dynamics of the real robot were too different from the dynamics of the simulation. Therefore, we performed a special type of system identification, where we run scripted trajectories of actions a t for n timesteps on the real robot.</p>
<p>Then we used the CMA-ES [28] algorithm to change the simulation parameters and let them optimize to minimize the 2-norm (
n i=1 (v i ) 2 ) 1 2
where v is the end effector position. The optimized simulation parameters are:</p>
<p>• Gravity X, Y , Z • Robot link masses scaling • Joint Damping Fig. 3 and 4 show the real and simulated trajectory before the system identification and afterwards. We see, that we got much closer to the real trajectory of the robot.  </p>
<p>VII. EVALUATION</p>
<p>In this section we show the results that we found in a simulated environment as well as the results when a policy is transferred to the real robot. The plots were generated by using five training trials per experiment with a moving average window of 10 and the light-colored background shows the standard deviation of the trials. In SAC we kept the standard parameters and the maximum number of steps is set to 200, while the episode ends early when the insertion was successful. We installed and calibrated a camera and an Aruco Marker detector for determining the position and rotation of the hole.</p>
<p>By retrieving this position in the control loop and updating the goal conditioned policy, we achieve to learn a policy that can interactively react on changes in the goal position during rollouts and can recover from perturbations (see the video for more details).</p>
<p>As a cost function we used:
C pos = α · x dist 2 + β · x dist 1 + γ · θ dist 1 (6) C bonus = 50 if insertion was successful (7) C total = −C pos + C bonus(8)
We used α = 0.6, β = 0.4 and γ = 0.1.</p>
<p>Training results can be seen in Fig. 5. We see that the normal and stacked observation vector perform similarly well in the simulation environment (other training scenarios showed, that this is not always the case and training with stacked observations can slow down and worsen training). The red plot shows training, when we perform dynamics randomization. Inspired by [6] we randomize gravity, link masses, joint damping and surface friction. We see that the algorithm still mostly succeeds in learning the task but gets much more unstable and sometimes also fails in learning the task at all.</p>
<p>For testing the transfer of the learned policy to the real robot we set the target to three different locations with different x, y, z, θ the detailed results can be found in Table I. The unstacked policy transfers slightly better to the real robot and insertion is faster. We assume this is the case, because overfitting to the simulation could be less serious, when a compact observation space is used like stated in [13]. We additionally tried using a different peg-shape (triangle) than the shape for training in simulation. Insertion with the triangle shape is slightly more difficult. While insertion with the normal policy works still fine, the performance of the stacked policy degrades. Transferring the policy which was trained with dynamics randomization does also transfer slightly worse.</p>
<p>Also training the policy (for one fixed position) directly on the real robot works well (for more details see the video). These results indicate that a policy trained without dynamics randomization gets trained faster and more reliable and still seems to transfer as well or better than the randomized policy. Additional findings are that policies, which were purely trained in simulation without dynamics randomization are still very robust against perturbations on the real robot. For instance, a human can apply forces on the robot arm, while the policy is executed, and it can still recover from those perturbations. Also moving the target object during execution is possible, as the goal conditioned policy can adapt to the changed situation. The learned search strategy can find the hole even with perturbations in the target location up to 2 cm (if the camera is covered and the hole is moved after the covering). The system also learns, that when being below the hole surface it first needs to go over the hole -taking into account preliminary lower reward -to successfully finish insertion. This is indeed making the problem much more difficult than on plain surfaces and increases training times massively.</p>
<p>VIII. CONCLUSION AND FUTURE WORK</p>
<p>We showed in this work, that it is possible to perform sim to real transfer without doing dynamics randomization. This helps speeding up training, can increase performance and reduces the number of hyperparameters.</p>
<p>In our future roadmap, we plan to investigate the possibilities of using sim to real transfer on more industrial robotic tasks and we believe that our current setup is a good starting point. In our view, tasks that involve contact are the most interesting class of problems for applying reinforcement learning in robotics. They are more difficult to solve, but classic position control tasks can often be solved easier with traditional techniques. With today's industrial robots, force sensitive task require a large amount of expert knowledge to program and a big amount of time for fine tuning it to specific applications. Nevertheless, very often those tasks are also inherently difficult to simulate with today's simulators. Friction, soft objects, snap-in events etc. are difficult or even impossible to simulate with tools like PyBullet or MuJoCo. Specialized simulation environments that can deal with those challenges in a better way partly exist, but often have other downsides like price or simulation speed. We therefore want to investigate how far we can extend sim to real transfer with simulators like PyBullet or MuJoCo on realistic industrial tasks and if industrial requirements for precision, speed and robustness can be met.</p>
<p>Fig. 1 :
1Simulated and real setting</p>
<p>Algorithm 2
2Saturation in Joint Space (SJS) 1: τ lim = 0 [n×1], N lim = I [n×n]],q sat = 0</p>
<p>Algorithm 3
3Saturation in Cartesian space (SCS) 1: τ lim = 0 [n×1], N lim = I [n×n]],ẍ sat = 0</p>
<p>lim =ẍ sat 10: τ lim = J T lim (Λ lim f * lim ) 11: N lim = I − J T limJ T lim 12: whileẍ i &gt;Ẍ max,i orẍ i &lt;Ẍ min,i</p>
<p>Fig. 2 :
2Architecture for learning and controlling robot and simulation Fig. 3: Real and simulated trajectories at the beginning of the optimization process. Every sub-trajectory consists of 50 time steps (x-axis). The figure shows 10 trajectories behind each other, where 10 different action sequences where chosen. The y-axis corresponds to the position of the flange.</p>
<p>Fig. 4 :
4Real and simulated trajectories after the system identification.</p>
<p>Fig. 5 :
5Training with and without dynamics randomization on different start and goal positions. The green plot shows training, when 4 past actions and observations are stacked into the observation vector. The plots show the average and standard deviation over five training runs.</p>
<p>TABLE I :
ITransfer Results (successful vs. tried insertions) Dynamics Randomization 20/20 20/20 20/20 Dynamics Rnd. -Triangle 20/20 20/20 10/20Train on real robot 20/20Policy 
Pos 1 Pos 2 Pos 3 
Stack 0 
20/20 20/20 20/20 
Stack 4 
20/20 20/20 20/20 
Stack 0 -Triangle 
20/20 20/20 20/20 
Stack 4 -Triangle 
18/20 20/20 19/20 </p>
<p>ABC is the euler angle notation for rotations, where A rotates around Z, B around Y and C around X. The frame is expressed in the world coordinate system.</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, abs/1710.06537CoRR. X. B. Peng, M. Andrychowicz, W. Zaremba, et al., "Sim-to-real transfer of robotic control with dynamics randomization," CoRR, vol. abs/1710.06537, 2017.</p>
<p>A unified approach for motion and force control of robotic manipulators: The operational space formulation. O Khatib, IEEE Journal of Robotics and Automation. O. Khatib, "A unified approach for motion and force control of robotic manipulators: The operational space formulation," IEEE Journal of Robotics and Automa- tion, 1987.</p>
<p>Operational space framework under joint constraints. J D M Osorio, M D Fiore, F Allmendinger, IDETC/CIE International Design Engineering Technical Conferences and Computers and Information in Engineering Conference. ProceedingsJ. D. M. Osorio, M. D. Fiore, and F. Allmendinger, "Operational space framework under joint constraints," in Proceedings. 2018 IDETC/CIE International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, 2018.</p>
<p>Physical human-robot interaction under joint and cartesian constraints. J D M Osorio, F Allmendinger, M D Fiore, 2019 19th International Conference on Advanced Robotics (ICAR). J. D. M. Osorio, F. Allmendinger, M. D. Fiore, et al., "Physical human-robot interaction under joint and carte- sian constraints," in 2019 19th International Conference on Advanced Robotics (ICAR), 2019, pp. 185-191.</p>
<p>Progressive neural networks. A A Rusu, N C Rabinowitz, G Desjardins, abs/1606.04671CoRR. A. A. Rusu, N. C. Rabinowitz, G. Desjardins, et al., "Progressive neural networks," CoRR, vol. abs/1606.04671, 2016.</p>
<p>Learning dexterous in-hand manipulation. M Openai, B Andrychowicz, Baker, abs/1808.00177CoRR. OpenAI, M. Andrychowicz, B. Baker, et al., "Learn- ing dexterous in-hand manipulation," CoRR, vol. abs/1808.00177, 2018.</p>
<p>Solving rubik's cube with a robot hand. I Openai, M Akkaya, Andrychowicz, abs/1910.07113ArXiv. OpenAI, I. Akkaya, M. Andrychowicz, et al., "Solv- ing rubik's cube with a robot hand," ArXiv, vol. abs/1910.07113, 2019.</p>
<p>Sim-toreal transfer with neural-augmented robot simulation. F Golemo, A A Taiga, A Courville, Proceedings of The 2nd Conference on Robot Learning. A. Billard, A. Dragan, J. Peters, et al., Eds., serThe 2nd Conference on Robot LearningPMLR87Proceedings of Machine Learning ResearchF. Golemo, A. A. Taiga, A. Courville, et al., "Sim-to- real transfer with neural-augmented robot simulation," in Proceedings of The 2nd Conference on Robot Learn- ing, A. Billard, A. Dragan, J. Peters, et al., Eds., ser. Proceedings of Machine Learning Research, vol. 87, PMLR, 2018, pp. 817-828.</p>
<p>Simto-real transfer learning using robustified controllers in robotic tasks involving complex dynamics. J Van Baar, A Sullivan, R Cordorel, abs/1809.04720CoRR. J. van Baar, A. Sullivan, R. Cordorel, et al., "Sim- to-real transfer learning using robustified controllers in robotic tasks involving complex dynamics," CoRR, vol. abs/1809.04720, 2018.</p>
<p>Virtual-to-real: Learning to control in visual semantic segmentation. Z Hong, Y Chen, S Su, abs/1802.00285CoRR. Z. Hong, Y. Chen, S. Su, et al., "Virtual-to-real: Learn- ing to control in visual semantic segmentation," CoRR, vol. abs/1802.00285, 2018.</p>
<p>Sim-to-real transfer of accurate grasping with eye-in-hand observations and continuous control. M Yan, I Frosio, S Tyree, abs/1712.03303CoRR. M. Yan, I. Frosio, S. Tyree, et al., "Sim-to-real transfer of accurate grasping with eye-in-hand observations and continuous control," CoRR, vol. abs/1712.03303, 2017.</p>
<p>No-regret reductions for imitation learning and structured prediction. S Ross, G J Gordon, J A Bagnell, abs/1011.0686CoRR. S. Ross, G. J. Gordon, and J. A. Bagnell, "No-regret reductions for imitation learning and structured predic- tion," CoRR, vol. abs/1011.0686, 2010.</p>
<p>Sim-toreal: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, abs/1804.10332CoRR. J. Tan, T. Zhang, E. Coumans, et al., "Sim-to- real: Learning agile locomotion for quadruped robots," CoRR, vol. abs/1804.10332, 2018.</p>
<p>. PyBullet. PyBullet, https://pypi.org/project/pybullet/, Accessed: 2019-05-02.</p>
<p>Flexible robotic grasping with sim-to-real transfer based reinforcement learning. M Breyer, F Furrer, T Novkovic, abs/1803.04996CoRR. M. Breyer, F. Furrer, T. Novkovic, et al., "Flexible robotic grasping with sim-to-real transfer based rein- forcement learning," CoRR, vol. abs/1803.04996, 2018.</p>
<p>Deep reinforcement learning for high precision assembly tasks. T Inoue, G D Magistris, A Munawar, abs/1708.04033CoRR. T. Inoue, G. D. Magistris, A. Munawar, et al., "Deep re- inforcement learning for high precision assembly tasks," CoRR, vol. abs/1708.04033, 2017.</p>
<p>Contact skill imitation learning for robot-independent assembly programming. S Scherzinger, A Roennau, R Dillmann, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). S. Scherzinger, A. Roennau, and R. Dillmann, "Contact skill imitation learning for robot-independent assembly programming," in 2019 IEEE/RSJ International Confer- ence on Intelligent Robots and Systems (IROS), 2019.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, abs/1810.05687CoRR. Y. Chebotar, A. Handa, V. Makoviychuk, et al., "Clos- ing the sim-to-real loop: Adapting simulation ran- domization with real world experience," CoRR, vol. abs/1810.05687, 2018.</p>
<p>Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks. M A Lee, Y Zhu, K Srinivasan, 2019 IEEE International Conference on Robotics and Automation (ICRA). M. A. Lee, Y. Zhu, K. Srinivasan, et al., "Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks," in 2019 IEEE International Conference on Robotics and Automation (ICRA), 2019.</p>
<p>Deep reinforcement learning: An overview. Y Li, abs/1701.07274CoRR. Y. Li, "Deep reinforcement learning: An overview," CoRR, vol. abs/1701.07274, 2017.</p>
<p>Soft actorcritic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, abs/1801.01290CoRR. T. Haarnoja, A. Zhou, P. Abbeel, et al., "Soft actor- critic: Off-policy maximum entropy deep reinforce- ment learning with a stochastic actor," CoRR, vol. abs/1801.01290, 2018.</p>
<p>Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. L Fan, Y Zhu, J Zhu, Conference on Robot Learning. L. Fan, Y. Zhu, J. Zhu, et al., "Surreal: Open-source re- inforcement learning framework and robot manipulation benchmark," in Conference on Robot Learning, 2018.</p>
<p>Learning neural network policies with guided policy search under unknown dynamics. S Levine, P , Advances in Neural Information Processing Systems (NIPS). S. Levine and P. Abbeel, "Learning neural network policies with guided policy search under unknown dy- namics," in Advances in Neural Information Processing Systems (NIPS), 2014.</p>
<p>End-to-end training of deep visuomotor policies. S Levine, C Finn, T Darrell, abs/1504.00702CoRR. S. Levine, C. Finn, T. Darrell, et al., "End-to-end training of deep visuomotor policies," CoRR, vol. abs/1504.00702, 2015.</p>
<p>Soft actor-critic algorithms and applications. T Haarnoja, A Zhou, K Hartikainen, abs/1812.05905CoRR. T. Haarnoja, A. Zhou, K. Hartikainen, et al., "Soft actor-critic algorithms and applications," CoRR, vol. abs/1812.05905, 2018.</p>
<p>Robot control near singularity and joint limit using a continuous task transition algorithm. H Han, J Park, International Journal of Advanced Robotic Systems. 1010346H. Han and J. Park, "Robot control near singularity and joint limit using a continuous task transition algorithm," International Journal of Advanced Robotic Systems, vol. 10, no. 10, p. 346, 2013.</p>
<p>. Kuka Deutschland Gmbh, Sunrise, FRI 1.16KUKA Deutschland GmbH, KUKA Sunrise.FRI 1.16.</p>
<p>Completely derandomized self-adaptation in evolution strategies. N Hansen, A Ostermeier, Evolutionary Computation. 92N. Hansen and A. Ostermeier, "Completely derandom- ized self-adaptation in evolution strategies," Evolution- ary Computation, vol. 9, no. 2, pp. 159-195, 2001.</p>            </div>
        </div>

    </div>
</body>
</html>
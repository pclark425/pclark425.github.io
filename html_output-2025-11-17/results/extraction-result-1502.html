<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1502 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1502</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1502</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-9228fa3b363229780da4cb1d258942e0c13c2947</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9228fa3b363229780da4cb1d258942e0c13c2947" target="_blank">EPOpt: Learning Robust Neural Network Policies Using Model Ensembles</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The EPOpt algorithm is introduced, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects.</p>
                <p><strong>Paper Abstract:</strong> Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1502.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1502.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo (Multi-Joint dynamics with Contact) - physics engine for model-based control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics simulator used in this paper to train and evaluate neural-network control policies for simulated robotic locomotion (hopper and half-cheetah). It models articulated rigid-body dynamics with contact, friction, damping and inertia, and was the simulator used for all main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mujoco: A physics engine for model-based control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A rigid-body, multi-joint dynamics simulator that models articulated bodies with contacts; used to simulate second-order dynamics, contact discontinuities and torque-controlled actuators for locomotion benchmarks (2D hopper and half-cheetah).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / rigid-body dynamics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high-fidelity rigid-body dynamics simulator: models multibody dynamics, contact interactions, friction, damping and inertia accurately for control tasks, but is not a continuum or multiphysics solver (i.e., not full fluid/thermal/chemical fidelity).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes modeling of contact discontinuities, Coulomb-like ground friction, joint damping, link inertias/armature and torque control; supports stochastic transitions (used to model sensor/process noise). Simulations are episodic (rollouts of length 1000 timesteps in experiments) and allow parameterization of physical properties (mass, friction, damping, armature). Does not natively represent unmodeled effects unless explicitly parameterized; sampling-based adaptation was used to account for model mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Neural-network policy trained with TRPO inside EPOpt ensemble framework</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Stochastic policy with Gaussian action distribution; policy mean parameterized by a neural network (two hidden layers, 64 units each, tanh activations); standard deviations learned; trained with TRPO as the batch policy optimizer, within the EPOpt-ε adversarial ensemble training algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learn robust locomotion control policies under parametric uncertainty (e.g., varying torso mass, ground friction, joint damping, armature) and transfer those policies to target domains with different physical parameters (and ultimately to real-world robots).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Reported mean returns in simulator vary by training regime; e.g., EPOpt (ε=0.1) achieved mean returns ≈3063 (Table 2), EPOpt (ε=1.0) ≈3224, while a single maximum-likelihood model policy (Max-Lik) achieved mean ≈1710 (all unitless episodic return values used in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Target domains with different physical parameters (changed torso mass, friction, etc.); authors frame transfer to a 'target domain' (intended eventual transfer to physical robots / real world), though experiments transfer between simulated instances (parameter mismatch) rather than to a real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Direct-transfer performance (no target training) was much better for EPOpt-trained policies compared to single-model policies: EPOpt policies remained stable and high-performing across broad ranges of torso mass and other parameter variations. In model-adaptation experiments, EPOpt discovered a robust policy with return >2500 with as few as 5 target-domain trajectories and produced a good policy after ~11 episodes of target data (versus >2×10^4 trajectories that would be required by model-free TRPO from random init).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper argues that including the key physical parameters (mass, friction, damping, armature) in the simulated ensemble improves direct-transfer performance; unmodeled effects can degrade transfer but EPOpt shows robustness to some unmodeled effects. The authors note that if the prior/source ensemble does not place sufficient probability mass near the true target model, adaptation becomes harder and alternative sampling strategies are needed. They also suggest eventual replacement of physics simulators with learned Bayesian neural network models pretrained on simulators for better generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Policies trained on a single fixed simulator instance (maximum-likelihood model) were brittle: small parameter deviations (e.g., torso mass changes) caused failures (hopper falling). When a parameter (e.g., torso mass) was entirely unmodeled in the source ensemble, EPOpt was less robust than when that parameter was included. Model-adaptation via sampling can be computationally intensive as the number of variable physical parameters grows; adaptation is difficult if the prior assigns very low probability to the true model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EPOpt: Learning Robust Neural Network Policies Using Model Ensembles', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1502.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1502.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit and standardized suite of reinforcement learning environments used by the authors to obtain standard reward functions and task implementations (wrapping MuJoCo tasks). It is used for environment definitions and reward structure rather than low-level physics simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>OpenAI Gym (environment suite / API)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A collection and API of reinforcement learning environments; provides standardized task definitions (observations, rewards, terminations) and commonly interfaces to simulators like MuJoCo for physics-based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>reinforcement learning benchmarks / robotics (mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>varies — Gym itself is an environment wrapper: fidelity depends on the underlying simulator (e.g., MuJoCo) used for a given environment, so Gym is not directly a physics-fidelity provider.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides standard reward functions and episode termination conditions (e.g., alive bonuses, termination on torso height/pitch). Uses underlying simulators (MuJoCo) to simulate dynamics; any fidelity properties derive from those simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Neural-network policy trained with TRPO inside EPOpt (same agent as above)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Stochastic Gaussian policy with NN mean (two hidden layers of 64 units), trained via TRPO within EPOpt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Same locomotion control tasks (hopper and half-cheetah) with standard Gym reward structures used to evaluate robustness and transfer under parameter mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not applicable to Gym itself; failures reported in the paper are due to insufficient simulator variability (single-model training) rather than Gym.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EPOpt: Learning Robust Neural Network Policies Using Model Ensembles', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mujoco: A physics engine for model-based control <em>(Rating: 2)</em></li>
                <li>Using inaccurate models in reinforcement learning <em>(Rating: 2)</em></li>
                <li>Ensemble-CIO: Full-body dynamic motion planning that transfers to physical humanoids <em>(Rating: 2)</em></li>
                <li>Optimizing walking controllers for uncertain inputs and environments <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1502",
    "paper_id": "paper-9228fa3b363229780da4cb1d258942e0c13c2947",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo (Multi-Joint dynamics with Contact) - physics engine for model-based control",
            "brief_description": "A physics simulator used in this paper to train and evaluate neural-network control policies for simulated robotic locomotion (hopper and half-cheetah). It models articulated rigid-body dynamics with contact, friction, damping and inertia, and was the simulator used for all main experiments.",
            "citation_title": "Mujoco: A physics engine for model-based control",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo",
            "simulator_description": "A rigid-body, multi-joint dynamics simulator that models articulated bodies with contacts; used to simulate second-order dynamics, contact discontinuities and torque-controlled actuators for locomotion benchmarks (2D hopper and half-cheetah).",
            "scientific_domain": "mechanics / rigid-body dynamics / robotics",
            "fidelity_level": "medium-to-high-fidelity rigid-body dynamics simulator: models multibody dynamics, contact interactions, friction, damping and inertia accurately for control tasks, but is not a continuum or multiphysics solver (i.e., not full fluid/thermal/chemical fidelity).",
            "fidelity_characteristics": "Includes modeling of contact discontinuities, Coulomb-like ground friction, joint damping, link inertias/armature and torque control; supports stochastic transitions (used to model sensor/process noise). Simulations are episodic (rollouts of length 1000 timesteps in experiments) and allow parameterization of physical properties (mass, friction, damping, armature). Does not natively represent unmodeled effects unless explicitly parameterized; sampling-based adaptation was used to account for model mismatch.",
            "model_or_agent_name": "Neural-network policy trained with TRPO inside EPOpt ensemble framework",
            "model_description": "Stochastic policy with Gaussian action distribution; policy mean parameterized by a neural network (two hidden layers, 64 units each, tanh activations); standard deviations learned; trained with TRPO as the batch policy optimizer, within the EPOpt-ε adversarial ensemble training algorithm.",
            "reasoning_task": "Learn robust locomotion control policies under parametric uncertainty (e.g., varying torso mass, ground friction, joint damping, armature) and transfer those policies to target domains with different physical parameters (and ultimately to real-world robots).",
            "training_performance": "Reported mean returns in simulator vary by training regime; e.g., EPOpt (ε=0.1) achieved mean returns ≈3063 (Table 2), EPOpt (ε=1.0) ≈3224, while a single maximum-likelihood model policy (Max-Lik) achieved mean ≈1710 (all unitless episodic return values used in the paper).",
            "transfer_target": "Target domains with different physical parameters (changed torso mass, friction, etc.); authors frame transfer to a 'target domain' (intended eventual transfer to physical robots / real world), though experiments transfer between simulated instances (parameter mismatch) rather than to a real robot.",
            "transfer_performance": "Direct-transfer performance (no target training) was much better for EPOpt-trained policies compared to single-model policies: EPOpt policies remained stable and high-performing across broad ranges of torso mass and other parameter variations. In model-adaptation experiments, EPOpt discovered a robust policy with return &gt;2500 with as few as 5 target-domain trajectories and produced a good policy after ~11 episodes of target data (versus &gt;2×10^4 trajectories that would be required by model-free TRPO from random init).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "The paper argues that including the key physical parameters (mass, friction, damping, armature) in the simulated ensemble improves direct-transfer performance; unmodeled effects can degrade transfer but EPOpt shows robustness to some unmodeled effects. The authors note that if the prior/source ensemble does not place sufficient probability mass near the true target model, adaptation becomes harder and alternative sampling strategies are needed. They also suggest eventual replacement of physics simulators with learned Bayesian neural network models pretrained on simulators for better generalization.",
            "failure_cases": "Policies trained on a single fixed simulator instance (maximum-likelihood model) were brittle: small parameter deviations (e.g., torso mass changes) caused failures (hopper falling). When a parameter (e.g., torso mass) was entirely unmodeled in the source ensemble, EPOpt was less robust than when that parameter was included. Model-adaptation via sampling can be computationally intensive as the number of variable physical parameters grows; adaptation is difficult if the prior assigns very low probability to the true model.",
            "uuid": "e1502.0",
            "source_info": {
                "paper_title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles",
                "publication_date_yy_mm": "2016-10"
            }
        },
        {
            "name_short": "OpenAI Gym",
            "name_full": "OpenAI Gym",
            "brief_description": "A toolkit and standardized suite of reinforcement learning environments used by the authors to obtain standard reward functions and task implementations (wrapping MuJoCo tasks). It is used for environment definitions and reward structure rather than low-level physics simulation.",
            "citation_title": "OpenAI Gym",
            "mention_or_use": "use",
            "simulator_name": "OpenAI Gym (environment suite / API)",
            "simulator_description": "A collection and API of reinforcement learning environments; provides standardized task definitions (observations, rewards, terminations) and commonly interfaces to simulators like MuJoCo for physics-based tasks.",
            "scientific_domain": "reinforcement learning benchmarks / robotics (mechanics)",
            "fidelity_level": "varies — Gym itself is an environment wrapper: fidelity depends on the underlying simulator (e.g., MuJoCo) used for a given environment, so Gym is not directly a physics-fidelity provider.",
            "fidelity_characteristics": "Provides standard reward functions and episode termination conditions (e.g., alive bonuses, termination on torso height/pitch). Uses underlying simulators (MuJoCo) to simulate dynamics; any fidelity properties derive from those simulators.",
            "model_or_agent_name": "Neural-network policy trained with TRPO inside EPOpt (same agent as above)",
            "model_description": "Stochastic Gaussian policy with NN mean (two hidden layers of 64 units), trained via TRPO within EPOpt.",
            "reasoning_task": "Same locomotion control tasks (hopper and half-cheetah) with standard Gym reward structures used to evaluate robustness and transfer under parameter mismatch.",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": null,
            "failure_cases": "Not applicable to Gym itself; failures reported in the paper are due to insufficient simulator variability (single-model training) rather than Gym.",
            "uuid": "e1502.1",
            "source_info": {
                "paper_title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles",
                "publication_date_yy_mm": "2016-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mujoco: A physics engine for model-based control",
            "rating": 2
        },
        {
            "paper_title": "Using inaccurate models in reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Ensemble-CIO: Full-body dynamic motion planning that transfers to physical humanoids",
            "rating": 2
        },
        {
            "paper_title": "Optimizing walking controllers for uncertain inputs and environments",
            "rating": 1
        }
    ],
    "cost": 0.011917249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>EPOPt: LEARNING Robust NeURAL Network Policies Using Model Ensembles</h1>
<p>Aravind Rajeswaran ${ }^{1}$, Sarvjeet Ghotra ${ }^{2}$, Balaraman Ravindran ${ }^{3}$, Sergey Levine ${ }^{4}$<br>aravraj@cs.washington.edu, sarvjeet.13it236@nitk.edu.in, ravi@cse.iitm.ac.in, svlevine@eecs.berkeley.edu<br>${ }^{1}$ University of Washington Seattle<br>${ }^{2}$ NITK Surathkal<br>${ }^{3}$ Indian Institute of Technology Madras<br>${ }^{4}$ University of California Berkeley</p>
<h4>Abstract</h4>
<p>Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.</p>
<h2>1 INTRODUCTION</h2>
<p>Reinforcement learning with powerful function approximators like deep neural networks (deep RL) has recently demonstrated remarkable success in a wide range of tasks like games (Mnih et al., 2015; Silver et al., 2016), simulated control problems (Lillicrap et al., 2015; Mordatch et al., 2015b), and graphics (Peng et al., 2016). However, high sample complexity is a major barrier for directly applying model-free deep RL methods for physical control tasks. Model-free algorithms like Q-learning, actor-critic, and policy gradients are known to suffer from long learning times (Kakade, 2003), which is compounded when used in conjunction with expressive function approximators like deep neural networks (DNNs). The challenge of gathering samples from the real world is further exacerbated by issues of safety for the agent and environment, since sampling with partially learned policies could be unstable (García \&amp; Fernández, 2015). Thus, model-free deep RL methods often require a prohibitively large numbers of potentially dangerous samples for physical control tasks.</p>
<p>Model-based methods, where the real-world target domain is approximated with a simulated source domain, provide an avenue to tackle the above challenges by learning policies using simulated data. The principal challenge with simulated training is the systematic discrepancy between source and target domains, and therefore, methods that compensate for systematic discrepancies (modeling errors) are needed to transfer results from simulations to real world using RL. We show that the impact of such discrepancies can be mitigated through two key ideas: (1) training on an ensemble of models in an adversarial fashion to learn policies that are robust to parametric model errors, as well as to unmodeled effects; and (2) adaptation of the source domain ensemble using data from the target domain to progressively make it a better approximation. This can be viewed either as an instance of model-based Bayesian RL (Ghavamzadeh et al., 2015); or as transfer learning from a collection of simulated source domains to a real-world target domain (Taylor \&amp; Stone, 2009). While a number of model-free RL algorithms have been proposed (see, e.g., Duan et al. (2016) for a survey), their high sample complexity demands use of a simulator, effectively making them model-based. We</p>
<p>show in our experiments that such methods learn policies which are highly optimized for the specific models used in the simulator, but are brittle under model mismatch. This is not surprising, since deep networks are remarkably proficient at exploiting any systematic regularities in a simulator. Addressing robustness of DNN-policies is particularly important to transfer their success from simulated tasks to physical systems.</p>
<p>In this paper, we propose the Ensemble Policy Optimization (EPOpt- $\epsilon$ ) algorithm for finding policies that are robust to model mismatch. In line with model-based Bayesian RL, we learn a policy for the target domain by alternating between two phases: (i) given a source (model) distribution (i.e. ensemble of models), find a robust policy that is competent for the whole distribution; (ii) gather data from the target domain using said robust policy, and adapt the source distribution. EPOpt uses an ensemble of models sampled from the source distribution, and a form of adversarial training to learn robust policies that generalize to a broad range of models. By robust, we mean insensitivity to parametric model errors and broadly competent performance for direct-transfer (also referred to as jumpstart like in Taylor \&amp; Stone (2009)). Direct-transfer performance refers to the average initial performance (return) in the target domain, without any direct training on the target domain. By adversarial training, we mean that model instances on which the policy performs poorly in the source distribution are sampled more often in order to encourage learning of policies that perform well for a wide range of model instances. This is in contrast to methods which learn highly optimized policies for specific model instances, but brittle under model perturbations. In our experiments, we did not observe significant loss in performance by requiring the policy to work on multiple models (for example, through adopting a more conservative strategy). Further, we show that policies learned using EPOpt are robust even to effects not modeled in the source domain. Such unmodeled effects are a major issue when transferring from simulation to the real world. For the model adaptation step (ii), we present a simple method using approximate Bayesian updates, which progressively makes the source distribution a better approximation of the target domain. We evaluate the proposed methods on the hopper (12 dimensional state space; 3 dimensional action space) and half-cheetah (18 dimensional state space; 6 dimensional action space) benchmarks in MuJoCo. Our experimental results suggest that adversarial training on model ensembles produces robust policies which generalize better than policies trained on a single, maximum-likelihood model (of source distribution) alone.</p>
<h1>2 Problem Formulation</h1>
<p>We consider parametrized Markov Decision Processes (MDPs), which are tuples of the form: $\mathcal{M}(p) \equiv&lt;\mathcal{S}, \mathcal{A}, \mathcal{T}<em p="p">{p}, \mathcal{R}</em>}, \gamma, S_{0, p}&gt;$ where $\mathcal{S}, \mathcal{A}$ are (continuous) states and actions respectively; $\mathcal{T<em p="p">{p} \mathcal{R}</em>}$, and $S_{0, p}$ are the state transition, reward function, and initial state distribution respectively, all parametrized by $p$; and $\gamma$ is the discount factor. Thus, we consider a set of MDPs with the same state and action spaces. Each MDP in this set could potentially have different transition functions, rewards, and initial state distributions. We use transition functions of the form $S_{t+1} \equiv \mathcal{T<em t="t">{p}\left(s</em>}, a_{t}\right)$ where $\mathcal{T<em t_1="t+1">{p}$ is a random process and $S</em>$ is a random variable.</p>
<p>We distinguish between source and target MDPs using $\mathcal{M}$ and $\mathcal{W}$ respectively. We also refer to $\mathcal{M}$ and $\mathcal{W}$ as source and target domains respectively, as is common in the transfer learning set-up. Our objective is to learn the optimal policy for $\mathcal{W}$; and to do so, we have access to $\mathcal{M}(p)$. We assume that we have a distribution $(\mathcal{D})$ over the source domains (MDPs) generated by a distribution over the parameters $P \equiv \mathcal{P}(p)$ that capture our subjective belief about the parameters of $\mathcal{W}$. Let $\mathcal{P}$ be parametrized by $\psi$ (e.g. mean, standard deviation). For example, $\mathcal{M}$ could be a hopping task with reward proportional to hopping velocity and falling down corresponds to a terminal state. For this task, $p$ could correspond to parameters like torso mass, ground friction, and damping in joints, all of which affect the dynamics. Ideally, we would like the target domain to be in the model class, i.e. ${\exists p \mid \mathcal{M}(p)=\mathcal{W}}$. However, in practice, there are likely to be unmodeled effects, and we analyze this setting in our experiments. We wish to learn a policy $\pi_{\theta}^{*}(s)$ that performs well for all $\mathcal{M} \sim \mathcal{D}$. Note that this robust policy does not have an explicit dependence on $p$, and we require it to perform well without knowledge of $p$.</p>
<h2>3 LEARNING PROTOCOL AND EPOPT ALGORITHM</h2>
<p>We follow the round-based learning protocol of Bayesian model-based RL. We use the term rounds when interacting with the target domain, and episode when performing rollouts with the simulator. In each round, we interact with the target domain after computing the robust policy on the current (i.e.</p>
<p>posterior) simulated source distribution. Following this, we update the source distribution using data from the target domain collected by executing the robust policy. Thus, in round $i$, we update two sets of parameters: $\theta_{i}$, the parameters of the robust policy (neural network); and $\psi_{i}$, the parameters of the source distribution. The two key steps in this procedure are finding a robust policy given a source distribution; and updating the source distribution using data from the target domain. In this section, we present our approach for both of these steps.</p>
<h1>3.1 ROBUST POLICY SEARCH</h1>
<p>We introduce the EPOpt algorithm for finding a robust policy using the source distribution. EPOpt is a policy gradient based meta-algorithm which uses batch policy optimization methods as a subroutine. Batch policy optimization algorithms (Williams, 1992; Kakade, 2001; Schulman et al., 2015) collect a batch of trajectories by rolling out the current policy, and use the trajectories to make a policy update. The basic structure of EPOpt is to sample a collection of models from the source distribution, sample trajectories from each of these models, and make a gradient update based on a subset of sampled trajectories. We first define evaluation metrics for the parametrized policy, $\pi_{\theta}$ :</p>
<p>$$
\begin{gathered}
\eta_{\mathcal{M}}(\theta, p)=\mathbb{E}<em t="0">{\tilde{\tau}}\left[\sum</em>\right) \mid p\right] \
\eta_{\mathcal{D}}(\theta)=\mathbb{E}}^{T-1} \gamma^{t} r_{t}\left(s_{t}, a_{t<em _mathcal_M="\mathcal{M">{p \sim \mathcal{P}}[\eta</em>}}(\theta, p)]=\mathbb{E<em _tilde_tau="\tilde{\tau">{p \sim \mathcal{P}}\left[\mathbb{E}</em>}}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\left(s_{t}, a_{t}\right) \mid p\right]\right]=\mathbb{E<em t="0">{\tau}\left[\sum</em>\right)\right]
\end{gathered}
$$}^{T-1} \gamma^{t} r_{t}\left(s_{t}, a_{t</p>
<p>In (1), $\eta_{\mathcal{M}}(\theta, p)$ is the evaluation of $\pi_{\theta}$ on the model $\mathcal{M}(p)$, with $\tilde{\tau}$ being trajectories generated by $\mathcal{M}(p)$ and $\pi_{\theta}: \tilde{\tau}=\left{s_{t}, a_{t}, r_{t}\right}<em t_1="t+1">{t=0}^{T}$ where $s</em>} \sim \mathcal{T<em t="t">{p}\left(s</em>}, a_{t}\right), s_{0} \sim S_{0, p}, r_{t} \sim \mathcal{R<em t="t">{p}\left(s</em>\right}}, a_{t}\right)$, and $a_{t} \sim \pi_{\theta}\left(s_{t}\right)$. Similarly, $\eta_{\mathcal{D}}(\theta)$ is the evaluation of $\pi_{\theta}$ over the source domain distribution. The corresponding expectation is over trajectories $\tau$ generated by $\mathcal{D}$ and $\pi_{\theta}: \tau=\left{s_{t}, a_{t}, r_{t<em t_1="t+1">{t=0}^{T}$, where $s</em>} \sim \mathcal{T<em t="t">{p</em>}}\left(s_{t}, a_{t}\right), p_{t+1}=p_{t}, s_{0} \sim S_{0, p_{0}}, r_{t} \sim \mathcal{R<em t="t">{p</em>$. With this modified notation of trajectories, batch policy optimization can be invoked for policy search.}}\left(s_{t}, a_{t}\right), a_{t} \sim \pi_{\theta}\left(s_{t}\right)$, and $p_{0} \sim \mathcal{P</p>
<p>Optimizing $\eta_{\mathcal{D}}$ allows us to learn a policy that performs best in expectation over models in the source domain distribution. However, this does not necessarily lead to a robust policy, since there could be high variability in performance for different models in the distribution. To explicitly seek a robust policy, we use a softer version of max-min objective suggested in robust control, and optimize for the conditional value at risk (CVaR) (Tamar et al., 2015):</p>
<p>$$
\max <em _mathcal_F="\mathcal{F">{\theta, y} \quad \int</em>(\theta, P) \leq y\right)=\epsilon
$$}(\theta)} \eta_{\mathcal{M}}(\theta, p) \mathcal{P}(p) d p \quad \text { s.t. } \quad \mathbb{P}\left(\eta_{\mathcal{M}</p>
<p>where $\mathcal{F}(\theta)=\left{p \mid \eta_{\mathcal{M}}(\theta, p) \leq y\right}$ is the set of parameters corresponding to models that produce the worst $\epsilon$ percentile of returns, and provides the limit for the integral; $\eta_{\mathcal{M}}(\theta, P)$ is the random variable of returns, which is induced by the distribution over model parameters; and $\epsilon$ is a hyperparameter which governs the level of relaxation from max-min objective. The interpretation is that (2) maximizes the expected return for the worst $\epsilon$-percentile of MDPs in the source domain distribution. We adapt the previous policy gradient formulation to approximately optimize the objective in (2). The resulting algorithm, which we call EPOpt- $\epsilon$, generalizes learning a policy using an ensemble of source MDPs which are sampled from a source domain distribution.</p>
<p>In Algorithm 1, $R\left(\tau_{k}\right) \equiv \sum_{t=0}^{T-1} \gamma^{t} r_{t, k}$ denotes the discounted return obtained in trajectory sample $\tau_{k}$. In line 7 , we compute the $\epsilon$-percentile value of returns from the $N$ trajectories. In line 8 , we find the subset of sampled trajectories which have returns lower than $Q_{\epsilon}$. Line 9 calls one step of an underlying batch policy optimization subroutine on the subset of trajectories from line 8. For the CVaR objective, it is important to use a good baseline for the value function. Tamar et al. (2015) show that without a baseline, the resulting policy gradient is biased and not consistent. We use a linear function as the baseline with a time varying feature vector to approximate the value function, similar to Duan et al. (2016). The parameters of the baseline are estimated using only the subset of trajectories with return less than $Q_{\epsilon}$. We found that this approach led to empirically good results.
For small values of $\epsilon$, we observed that using the sub-sampling step from the beginning led to unstable learning. Policy gradient methods adjust parameters of policy to increase probability of trajectories</p>
<div class="codehilite"><pre><span></span><code>Algorithm <span class="mi">1</span><span class="p">:</span> EPOpt- <span class="err">\</span><span class="p">(</span><span class="err">\</span>epsilon<span class="err">\</span><span class="p">)</span> for Robust Policy Search
Input<span class="p">:</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>psi<span class="p">,</span> <span class="err">\</span>theta_<span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="p">),</span> niter<span class="p">,</span> <span class="err">\</span><span class="p">(</span>N<span class="p">,</span> <span class="err">\</span>epsilon<span class="err">\</span><span class="p">)</span>
for iteration <span class="err">\</span><span class="p">(</span><span class="ss">i</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="err">\</span>ldots<span class="err">\</span><span class="p">)</span> niter do
    for <span class="err">\</span><span class="p">(</span><span class="ss">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="err">\</span>ldots N<span class="err">\</span><span class="p">)</span> do
        sample model parameters <span class="err">\</span><span class="p">(</span>p_<span class="p">{</span>k<span class="p">}</span> <span class="err">\</span>sim <span class="err">\</span>mathcal<span class="p">{</span>P<span class="p">}</span>_<span class="p">{</span><span class="err">\</span>psi<span class="p">}</span><span class="err">\</span><span class="p">)</span>
        sample a trajectory <span class="err">\</span><span class="p">(</span><span class="err">\</span>tau_<span class="p">{</span>k<span class="p">}</span><span class="o">=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span>s_<span class="p">{</span>t<span class="p">},</span> a_<span class="p">{</span>t<span class="p">},</span> r_<span class="p">{</span>t<span class="p">},</span> s_<span class="p">{</span>t<span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span>_<span class="p">{</span><span class="ss">t</span><span class="o">=</span><span class="mi">0</span><span class="p">}</span><span class="err">^</span><span class="p">{</span>T-1<span class="p">}</span><span class="err">\</span><span class="p">)</span> from <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>M<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>p_<span class="p">{</span>k<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> using policy <span class="err">\</span><span class="p">(</span><span class="err">\</span>pi<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>theta_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
    end
    compute <span class="err">\</span><span class="p">(</span>Q_<span class="p">{</span><span class="err">\</span>epsilon<span class="p">}</span><span class="o">=</span><span class="err">\</span>epsilon<span class="err">\</span><span class="p">)</span> percentile of <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span>R<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>tau_<span class="p">{</span>k<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span>_<span class="p">{</span><span class="ss">k</span><span class="o">=</span><span class="mi">1</span><span class="p">}</span><span class="err">^</span><span class="p">{</span>N<span class="p">}</span><span class="err">\</span><span class="p">)</span>
    select sub-set <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbb<span class="p">{</span>T<span class="p">}</span><span class="o">=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span><span class="err">\</span>tau_<span class="p">{</span>k<span class="p">}:</span> R<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>tau_<span class="p">{</span>k<span class="p">}</span><span class="err">\</span>right<span class="p">)</span> <span class="err">\</span>leq Q_<span class="p">{</span><span class="err">\</span>epsilon<span class="p">}</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
    Update policy<span class="p">:</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>theta_<span class="p">{</span>i<span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="p">)</span> BatchPolOpt <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>theta_<span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>mathbb<span class="p">{</span>T<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
end
</code></pre></div>

<p>with high returns and reduce probability of poor trajectories. EPOpt $-\epsilon$ due to the sub-sampling step emphasizes penalizing poor trajectories more. This might constrain the initial exploration needed to find good trajectories. Thus, we initially use a setting of $\epsilon=1$ for few iterations before setting epsilon to the desired value. This corresponds to exploring initially to find promising trajectories and rapidly reducing probability of trajectories that do not generalize.</p>
<h1>3.2 ADAPTING THE SOURCE DOMAIN DISTRIBUTION</h1>
<p>In line with model-based Bayesian RL, we can adapt the ensemble distribution after observing trajectory data from the target domain. The Bayesian update can be written as:</p>
<p>$$
\mathbb{P}\left(P \mid \tau_{k}\right)=\frac{1}{\mathcal{Z}} \times \mathbb{P}\left(\tau_{k} \mid P\right) \times \mathbb{P}(P)=\frac{1}{\mathcal{Z}} \times \prod_{t=0}^{T-1} \mathbb{P}\left(S_{t+1}=s_{t+1}^{(k)} \mid s_{t}^{(k)}, a_{t}^{(k)}, p\right) \times \mathbb{P}(P=p)
$$</p>
<p>where $\frac{1}{\mathcal{Z}}$ is the partition function (normalization) required to make the probabilities sum to $1, S_{t+1}$ is the random variable representing the next state, and $\left(s_{t}^{(k)}, a_{t}^{(k)}, s_{t+1}^{(k)}\right)<em k="k">{t=0}^{T}$ are data observed along trajectory $\tau</em>$. We try to explain the target trajectory using the stochasticity in the state-transition function, which also models sensor errors. This provides the following expression for the likelihood:</p>
<p>$$
\mathbb{P}\left(S_{t+1} \mid s_{t}, a_{t}, p\right) \equiv \mathcal{T}<em t="t">{p}\left(s</em>\right)
$$}, a_{t</p>
<p>We follow a sampling based approach to calculate the posterior, by sampling a set of model parameters: $p_{i}=\left[p_{1}, p_{2}, \ldots, p_{M}\right]$ from a sampling distribution, $\mathbb{P}<em i="i">{S}\left(p</em>\right)$. Consequently, using Bayes rule and importance sampling, we have:</p>
<p>$$
\mathbb{P}\left(p_{i} \mid \tau_{k}\right) \propto \mathcal{L}\left(\tau_{k} \mid p_{i}\right) \times \frac{\mathbb{P}<em i="i">{P}\left(p</em>}\right)}{\mathbb{P<em i="i">{S}\left(p</em>
$$}\right)</p>
<p>where $\mathbb{P}<em i="i">{P}\left(p</em>\right)$. This simple model adaptation rule allows us to illustrate the utility of EPOpt for robust policy search, as well as its integration with model adaptation to learn policies in cases where the target model could be very different from the initially assumed distribution.}\right)$ is the probability of drawing $p_{i}$ from the prior distribution; and $\mathcal{L}\left(\tau_{k} \mid p_{i}\right)$ is the likelihood of generating the observed trajectory with model parameters $p_{i}$. The weighted samples from the posterior can be used to estimate a parametric model, as we do in this paper. Alternatively, one could approximate the continuous probability distribution using discrete weighted samples like in case of particle filters. In cases where the prior has very low probability density in certain parts of the parameter space, it might be advantageous to choose a sampling distribution different from the prior. The likelihood can be factored using the Markov property as: $\mathcal{L}\left(\tau_{k} \mid p_{i}\right)=\prod_{t} \mathbb{P}\left(S_{t+1}=s_{t+1}^{(k)} \mid s_{t}^{(k)}, a_{t}^{(k)}, p_{i</p>
<h2>4 EXPERIMENTS</h2>
<p>We evaluated the proposed EPOpt- $\epsilon$ algorithm on the 2D hopper (Erez et al., 2011) and halfcheetah (Wawrzynski, 2009) benchmarks using the MuJoCo physics simulator (Todorov et al., 2012). ${ }^{1}$ Both tasks involve complex second order dynamics and direct torque control. Underactuation,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>high dimensionality, and contact discontinuities make these tasks challenging reinforcement learning benchmarks. These challenges when coupled with systematic parameter discrepancies can quickly degrade the performance of policies and make them unstable, as we show in the experiments. The batch policy optimization sub-routine is implemented using TRPO. We parametrize the stochastic policy using the scheme presented in Schulman et al. (2015). The policy is represented with a Gaussian distribution, the mean of which is parametrized using a neural network with two hidden layers. Each hidden layer has 64 units, with a tanh non-linearity, and the final output layer is made of linear units. Normally distributed independent random variables are added to the output of this neural network, and we also learn the standard deviation of their distributions. Our experiments are aimed at answering the following questions:</p>
<ol>
<li>How does the performance of standard policy search methods (like TRPO) degrade in the presence of systematic physical differences between the training and test domains, as might be the case when training in simulation and testing in the real world?</li>
<li>Does training on a distribution of models with EPOpt improve the performance of the policy when tested under various model discrepancies, and how much does ensemble training degrade overall performance (e.g. due to acquiring a more conservative strategy)?</li>
<li>How does the robustness of the policy to physical parameter discrepancies change when using the robust EPOpt- $\epsilon$ variant of our method?</li>
<li>Can EPOpt learn policies that are robust to unmodeled effects - that is, discrepancies in physical parameters between source and target domains that do not vary in the source domain ensemble?</li>
<li>When the initial model ensemble differs substantially from the target domain, can the ensemble be adapted efficiently, and how much data from the target domain is required for this?</li>
</ol>
<p>In all the comparisons, performance refers to the average undiscounted return per trajectory or episode (we consider finite horizon episodic problems). In addition to the previously defined performance, we also use the $10^{\text {th }}$ percentile of the return distribution as a proxy for the worst-case return.</p>
<h1>4.1 COMPARISON TO STANDARD POLICY SEARCH</h1>
<p>In Figure 1, we evaluate the performance of standard TRPO and $\operatorname{EPOpt}(\epsilon=0.1)$ on the hopper task, in the presence of a simple parametric discrepancy in the physics of the system between the training (source) and test (target) domains. The plots show the performance of various policies on test domains with different torso mass. The first three plots show policies that are each trained on a single torso mass in the source domain, while the last plot illustrates the performance of EPOpt,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performance of hopper policies when testing on target domains with different torso masses. The first three plots (blue, green, and red) show the performance of policies trained with TRPO on source domains with torso mass 3,6 , and 9 , respectively (denoted by $m=$ in the legend). The rightmost plot shows the performance of $\operatorname{EPOpt}(\epsilon=0.1)$ trained on a Gaussian source distribution with mean mass $\mu=6$ and standard deviation $\sigma=1.5$. The shaded regions show the $10^{\text {th }}$ and $90^{\text {th }}$ percentile of the return distribution. Policies trained using traditional approaches on a single mass value are unstable for even slightly different masses, making the hopper fall over when trying to move forward. In contrast, the EPOpt policy is stable and achieves a high level of performance on the entire range of masses considered. Further, the EPOpt policy does not suffer from degradation in performance as a consequence of adopting a more robust policy.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: On the left, is an illustration of the simulated 2D hopper task studied in this paper. On right, we depict the performance of policies for various model instances of the hopper task. The performance is depicted as a heat map for various model configurations, parameters of which are given in the x and y axis. The adversarially trained policy, $\operatorname{EPOpt}(\epsilon=0.1)$, is observed to generalize to a wider range of models and is more robust. Table 1.
which is trained on a Gaussian mass distribution. The results show that no single torso mass value produces a policy that is successful in all target domains. However, the EPOpt policy succeeds almost uniformly for all tested mass values. Furthermore, the results show that there is almost no degradation in the performance of EPOpt for any mass setting, suggesting that the EPOpt policy does not suffer substantially from adopting a more robust strategy.</p>
<h1>4.2 ANALYSIS OF ROBUSTNESS</h1>
<p>Next, we analyze the robustness of policies trained using EPOpt on the hopper domain. For this analysis, we construct a source distribution which varies four different physical parameters: torso mass, ground friction, foot joint damping, and joint inertia (armature). This distribution is presented in Table 1. Using this source distribution, we compare between three different methods: (1) standard policy search (TRPO) trained on a single model corresponding to the mean parameters in Table 1; (2) $\operatorname{EPOpt}(\epsilon=1)$ trained on the source distribution; (3) $\operatorname{EPOpt}(\epsilon=0.1)$ - i.e. the adversarially trained policy, again trained on the previously described source distribution. The aim of the comparison is to study direct-transfer performance, similar to the robustness evaluations common in robust controller design (Zhou et al., 1996). Hence, we learn a policy using each of the methods, and then test policies on different model instances (i.e. different combinations of physical parameters) without any adaptation. The results of this comparison are summarized in Figure 2, where we present the performance of the policy for testing conditions corresponding to different torso mass and friction values, which we found to have the most pronounced impact on performance. The results indicate that $\operatorname{EPOpt}(\epsilon=0.1)$ produces highly robust policies. A similar analysis for the $10^{\text {th }}$ percentile of the return distribution (softer version of worst-case performance), the half-cheetah task, and different $\epsilon$ settings are presented in the appendix.</p>
<p>Table 1: Initial source domain distribution</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hopper</th>
<th style="text-align: left;">$\mu$</th>
<th style="text-align: left;">$\sigma$</th>
<th style="text-align: left;">low</th>
<th style="text-align: left;">high</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">mass</td>
<td style="text-align: left;">6.0</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">3.0</td>
<td style="text-align: left;">9.0</td>
</tr>
<tr>
<td style="text-align: left;">ground friction</td>
<td style="text-align: left;">2.0</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">2.5</td>
</tr>
<tr>
<td style="text-align: left;">joint damping</td>
<td style="text-align: left;">2.5</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">4.0</td>
</tr>
<tr>
<td style="text-align: left;">armature</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">1.5</td>
</tr>
<tr>
<td style="text-align: left;">Half-Cheetah</td>
<td style="text-align: left;">$\mu$</td>
<td style="text-align: left;">$\sigma$</td>
<td style="text-align: left;">low</td>
<td style="text-align: left;">high</td>
</tr>
<tr>
<td style="text-align: left;">mass</td>
<td style="text-align: left;">6.0</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">3.0</td>
<td style="text-align: left;">9.0</td>
</tr>
<tr>
<td style="text-align: left;">ground friction</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">0.3</td>
<td style="text-align: left;">0.7</td>
</tr>
<tr>
<td style="text-align: left;">joint damping</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">2.5</td>
</tr>
<tr>
<td style="text-align: left;">armature</td>
<td style="text-align: left;">0.125</td>
<td style="text-align: left;">0.04</td>
<td style="text-align: left;">0.05</td>
<td style="text-align: left;">0.2</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison between policies trained on a fixed maximum-likelihood model with mass (6), and an ensemble where all models have the same mass (6) and other parameters varying as described in Table 1.</p>
<h1>4.3 Robustness to Unmodeled Effects</h1>
<p>To analyze the robustness to unmodeled effects, our next experiment considers the setting where the source domain distribution is obtained by varying friction, damping, and armature as in Table 1, but does not consider a distribution over torso mass. Specifically, all models in the source domain distribution have the same torso mass (value of 6), but we will evaluate the policy trained on this distribution on target domains where the torso mass is different. Figure 3 indicates that the $\operatorname{EPOpt}(\epsilon=0.1)$ policy is robust to a broad range of torso masses even when its variation is not considered. However, as expected, this policy is not as robust as the case when mass is also modeled as part of the source domain distribution.</p>
<h3>4.4 Model Adaptation</h3>
<p>The preceding experiments show that EPOpt can find robust policies, but the source distribution in these experiments was chosen to be broad enough such that the target domain is not too far from high-density regions of the distribution. However, for real-world problems, we might not have the domain knowledge to identify a good source distribution in advance. In such settings, model (source) adaptation allows us to change the parameters of the source distribution using data gathered from the target domain. Additionally, model adaptation is helpful when the parameters of the target domain could change over time, for example due to wear and tear in a physical system. To illustrate model adaptation, we performed an experiment where the target domain was very far from the high density regions of the initial source distribution, as depicted in Figure 4(a). In this experiment, the source distribution varies the torso mass and ground friction. We observe that progressively, the source distribution becomes a better approximation of the target domain and consequently the performance improves. In this case, since we followed a sampling based approach, we used a uniform sampling distribution, and weighted each sample with the importance weight as described in Section 3.2. Eventually, after 10 iterations, the source domain distribution is able to accurately match the target domain. Figure 4(b) depicts the learning curve, and we see that a robust policy with return more than 2500, which roughly corresponds to a situation where the hopper is able to move forward without falling down for the duration of the episode, can be discovered with just 5 trajectories from the target domain. Subsequently, the policy improves near monotonically, and EPOpt finds a good policy with just 11 episodes worth of data from the target domain. In contrast, to achieve the same level of performance on the target domain, completely model-free methods like TRPO would require more than $2 \times 10^{4}$ trajectories when the neural network parameters are initialized randomly.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) Visualizes the source distribution during model adaptation on the hopper task, where mass and friction coefficient are varied in the source domain. The red cross indicates the unknown parameters of the target domain. The contours in the plot indicate the distribution over models (we assume a Gaussian distribution). Lighter colors and more concentrated contour lines indicate regions of higher density. Each iteration corresponds to one round (episode) of interaction with the target domain. The high-density regions gradually move toward the true model, while maintaining probability mass over a range of parameters which can explain the behavior of target domain. Figure 4(b) presents the corresponding learning curve, where the shaded region describes the 10th and 90th percentiles of the performance distribution, and the solid line is the average performance.</p>
<h1>5 Related Work</h1>
<p>Robust control is a branch of control theory which formally studies development of robust policies (Zhou et al., 1996; Nilim \&amp; Ghaoui, 2005; Lim et al., 2013). However, typically no distribution over source or target tasks is assumed, and a worst case analysis is performed. Most results from this field have been concentrated around linear systems or finite MDPs, which often cannot adequately model complexities of real-world tasks. The set-up of model-based Bayesian RL maintains a belief over models for decision making under uncertainty (Vlassis et al., 2012; Ghavamzadeh et al., 2015). In Bayesian RL, through interaction with the target domain, the uncertainty is reduced to find the correct or closest model. Application of this idea in its full general form is difficult, and requires either restrictive assumptions like finite MDPs (Poupart et al., 2006), gaussian dynamics (Ross et al., 2008), or task specific innovations. Previous methods have also suggested treating uncertain model parameters as unobserved state variables in a continuous POMDP framework, and solving the POMDP to get optimal exploration-exploitation trade-off (Duff, 2003; Porta et al., 2006). While this approach is general, and allows automatic learning of epistemic actions, extending such methods to large continuous control tasks like those considered in this paper is difficult.
Risk sensitive RL methods (Delage \&amp; Mannor, 2010; Tamar et al., 2015) have been proposed to act as a bridge between robust control and Bayesian RL. These approaches allow for using subjective model belief priors, prevent overly conservative policies, and enjoy some strong guarantees typically associated with robust control. However, their application in high dimensional continuous control tasks have not been sufficiently explored. We refer readers to García \&amp; Fernández (2015) for a survey of related risk sensitive RL methods in the context of robustness and safety.
Standard model-based control methods typically operate by finding a maximum-likelihood estimate of the target model (Ljung, 1998; Ross \&amp; Bagnell, 2012; Deisenroth et al., 2013), followed by policy optimization. Use of model ensembles to produce robust controllers was explored recently in robotics. Mordatch et al. (2015a) use a trajectory optimization approach and an ensemble with small finite set of models; whereas we follow a sampling based direct policy search approach over a continuous distribution of uncertain parameters, and also show domain adaptation. Sampling based approaches can be applied to complex models and discrete MDPs which cannot be planned through easily. Similarly, Wang et al. (2010) use an ensemble of models, but their goal is to optimize for average case performance as opposed to transferring to a target MDP. Wang et al. (2010) use a hand engineered policy class whose parameters are optimized with CMA-ES. EPOpt on the other hand can optimize expressive neural network policies directly. In addition, we show model adaptation, effectiveness of the sub-sampling step ( $\epsilon&lt;1$ case), and robustness to unmodeled effects, all of which are important for transfering to a target MDP.
Learning of parametrized skills (da Silva et al., 2012) is also concerned with finding policies for a distribution of parametrized tasks. However, this is primarily geared towards situations where task parameters are revealed during test time. Our work is motivated by situations where target task parameters (e.g. friction) are unknown. A number of methods have also been suggested to reduce sample complexity when provided with either a baseline policy (Thomas et al., 2015; Kakade \&amp; Langford, 2002), expert demonstration (Levine \&amp; Koltun, 2013; Argall et al., 2009), or approximate simulator (Tamar et al., 2012; Abbeel et al., 2006). These are complimentary to our work, in the sense that our policy, which has good direct-transfer performance, can be used to sample from the target domain and other off-policy methods could be explored for policy improvement.</p>
<h2>6 Conclusions and Future Work</h2>
<p>In this paper, we presented the EPOpt- $\epsilon$ algorithm for training robust policies on ensembles of source domains. Our method provides for training of robust policies, and supports an adversarial training regime designed to provide good direct-transfer performance. We also describe how our approach can be combined with Bayesian model adaptation to adapt the source domain ensemble to a target domain using a small amount of target domain experience. Our experimental results demonstrate that the ensemble approach provides for highly robust and generalizable policies in fairly complex simulated robotic tasks. Our experiments also demonstrate that Bayesian model adaptation can produce distributions over models that lead to better policies on the target domain than more standard maximum likelihood estimation, particularly in presence of unmodeled effects.</p>
<p>Although our method exhibits good generalization performance, the adaptation algorithm we use currently relies on sampling the parameter space, which is computationally intensive as the number of variable physical parameters increase. We observed that (adaptive) sampling from the prior leads to fast and reliable adaptation if the true model does not have very low probability in the prior. However, when this assumption breaks, we require a different sampling distribution which could produce samples from all regions of the parameter space. This is a general drawback of Bayesian adaptation methods. In future work, we plan to explore alternative sampling and parameterization schemes, including non-parametric distributions. An eventual end-goal would be to replace the physics simulator entirely with learned Bayesian neural network models, which could be adapted with limited data from the physical system. These models could be pre-trained using physics based simulators like MuJoCo to get a practical initialization of neural network parameters. Such representations are likely useful when dealing with high dimensional inputs like simulated vision from rendered images or tasks with complex dynamics like deformable bodies, which are needed to train highly generalizable policies that can successfully transfer to physical robots acting in the real world.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>The authors would like to thank Emo Todorov, Sham Kakade, and students of Emo Todorov's research group for insightful comments about the work. The authors would also like to thank Emo Todorov for the MuJoCo simulator. Aravind Rajeswaran and Balaraman Ravindran acknowledge financial support from ILDS, IIT Madras.</p>
<h2>REFERENCES</h2>
<p>Pieter Abbeel, Morgan Quigley, and Andrew Y. Ng. Using inaccurate models in reinforcement learning. In ICML, 2006.</p>
<p>Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and Autonomous Systems, 57(5):469 - 483, 2009.</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym, 2016.</p>
<p>Bruno Castro da Silva, George Konidaris, and Andrew G. Barto. Learning parameterized skills. In ICML, 2012.</p>
<p>Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics. Foundations and Trends in Robotics, 2(12):1-142, 2013.</p>
<p>Erick Delage and Shie Mannor. Percentile optimization for markov decision processes with parameter uncertainty. Operations Research, 58(1):203-213, 2010.</p>
<p>Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In ICML, 2016.</p>
<p>Michael O. Duff. Design for an optimal probe. In ICML, 2003.
Tom Erez, Yuval Tassa, and Emanuel Todorov. Infinite-horizon model predictive control for periodic tasks with contacts. In Proceedings of Robotics: Science and Systems, 2011.</p>
<p>Javier García and Fernando Fernández. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 2015.</p>
<p>Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement learning: A survey. Foundations and Trends in Machine Learning, 8(5-6):359-483, 2015.</p>
<p>Sham Kakade. A natural policy gradient. In NIPS, 2001.
Sham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College London, 2003.</p>
<p>Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In ICML, 2002.</p>
<p>Sergey Levine and Vladlen Koltun. Guided policy search. In ICML, 2013.
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. ArXiv e-prints, September 2015.</p>
<p>Shiau Hong Lim, Huan Xu, and Shie Mannor. Reinforcement learning in robust markov decision processes. In NIPS. 2013.</p>
<p>Lennart Ljung. System Identification, pp. 163-173. Birkhäuser Boston, Boston, MA, 1998.
Volodymyr Mnih et al. Human-level control through deep reinforcement learning. Nature, 518(7540): 529-533, Feb 2015.
I. Mordatch, K. Lowrey, and E. Todorov. Ensemble-CIO: Full-body dynamic motion planning that transfers to physical humanoids. In IROS, 2015a.</p>
<p>Igor Mordatch, Kendall Lowrey, Galen Andrew, Zoran Popovic, and Emanuel V. Todorov. Interactive control of diverse complex characters with neural networks. In NIPS. 2015b.</p>
<p>Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain transition matrices. Operations Research, 53(5):780-798, 2005.</p>
<p>Xue Bin Peng, Glen Berseth, and Michiel van de Panne. Terrain-adaptive locomotion skills using deep reinforcement learning. ACM Transactions on Graphics (Proc. SIGGRAPH 2016), 2016.</p>
<p>Josep M. Porta, Nikos A. Vlassis, Matthijs T. J. Spaan, and Pascal Poupart. Point-based value iteration for continuous pomdps. Journal of Machine Learning Research, 7:2329-2367, 2006.</p>
<p>Pascal Poupart, Nikos A. Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to discrete bayesian reinforcement learning. In ICML, 2006.
S. Ross, B. Chaib-draa, and J. Pineau. Bayesian reinforcement learning in continuous pomdps with application to robot navigation. In ICRA, 2008.</p>
<p>Stephane Ross and Drew Bagnell. Agnostic system identification for model-based reinforcement learning. In ICML, 2012.</p>
<p>John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust region policy optimization. In ICML, 2015.</p>
<p>David Silver et al. Mastering the game of go with deep neural networks and tree search. Nature, 529 (7587):484-489, Jan 2016.</p>
<p>Aviv Tamar, Dotan Di Castro, and Ron Meir. Integrating a partial model into model free reinforcement learning. Journal of Machine Learning Research, 2012.</p>
<p>Aviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the cvar via sampling. In AAAI Conference on Artificial Intelligence, 2015.</p>
<p>Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10:1633-1685, December 2009.</p>
<p>Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-policy evaluation. In AAAI Conference on Artificial Intelligence. 2015.
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033, Oct 2012.</p>
<p>Nikos Vlassis, Mohammad Ghavamzadeh, Shie Mannor, and Pascal Poupart. Bayesian Reinforcement Learning, pp. 359-386. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.</p>
<p>Jack M. Wang, David J. Fleet, and Aaron Hertzmann. Optimizing walking controllers for uncertain inputs and environments. ACM Trans. Graph., 2010.</p>
<p>Pawel Wawrzynski. Real-time reinforcement learning by sequential actor-critics and experience replay. Neural Networks, 22:1484-1497, 2009.</p>
<p>Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229-256, 1992.</p>
<p>Kemin Zhou, John C. Doyle, and Keith Glover. Robust and Optimal Control. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1996. ISBN 0-13-456567-3.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 DESCRIPTION OF SIMULATED ROBOTIC TASKS CONSIDERED IN THIS WORK</h2>
<p>Hopper: The hopper task is to make a 2D planar hopper with three joints and 4 body parts hop forward as fast as possible (Erez et al., 2011). This problem has a 12 dimensional state space and a 3 dimensional action space that corresponds to torques at the joints. We construct the source domain by considering a distribution over 4 parameters: torso mass, ground friction, armature (inertia), and damping of foot.</p>
<p>Half Cheetah: The half-cheetah task (Wawrzynski, 2009) requires us to make a 2D cheetah with two legs run forward as fast as possible. The simulated robot has 8 body links with an 18 dimensional state space and a 6 dimensional action space that corresponds to joint torques. Again, we construct the source domain using a distribution over the following parameters: torso and head mass, ground friction, damping, and armature (inertia) of foot joints.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Illustrations of the 2D simulated robot models used in the experiments. The hopper (a) and half-cheetah (b) tasks present the challenges of under-actuation and contact discontinuities. These challenges when coupled with parameter uncertainties lead to dramatic degradation in the quality of policies when robustness is not explicitly considered.</p>
<p>A video demonstration of the trained policies on these tasks can be viewed here: Supplimenrary video (https://youtu.be/w1YJ9vwaoto)</p>
<p>Reward functions: For both tasks, we used the standard reward functions implemented with OpenAI gym (Brockman et al., 2016), with minor modifications. The reward structure for hopper task is:</p>
<p>$$
r(s, a)=v_{x}-0.001|a|^{2}+b
$$</p>
<p>where $s$ are the states comprising of joint positions and velocities; $a$ are the actions (controls); and $v_{x}$ is the forward velocity. $b$ is a bonus for being alive $(b=1)$. The episode terminates when $z_{\text {torso }}&lt;0.7$ or when $\left|\theta_{y}\right|&lt;0.2$ where $\theta_{y}$ is the forward pitch of the body.</p>
<p>For the cheetah task, we use the reward function:</p>
<p>$$
r(s, a)=v_{x}-0.1|a|^{2}+b
$$</p>
<p>the alive bonus is 1 if head of cheetah is above -0.25 (relative to torso) and similarly episode terminates if the alive condition is violated.</p>
<p>Our implementation of the algorithms and environments are public in this repository to facilitate reproduction of results: https://github.com/aravindr93/robustRL</p>
<h2>A. 2 HYPERPARAMETERS</h2>
<ol>
<li>Neural network architecture: We used a neural network with two hidden layers, each with 64 units and tanh non-linearity. The policy updates are implemented using TRPO.</li>
<li>
<p>Trust region size in TRPO: The maximum KL divergence between sucessive policy updates are constrained to be 0.01</p>
</li>
<li>
<p>Number and length of trajectory rollouts: In each iteration, we sample $N=240$ models from the ensemble, one rollout is performed on each such model. This was implemented in parallel on multiple (6) CPUs. Each trajectory is of length 1000 - same as the standard implimentations of these tasks in gym and rllab.</p>
</li>
</ol>
<p>The results in Fig 1 and Fig 2 were generated after 150 and 200 iterations of TRPO respectively, with each iteration consisting of 240 trajectories as specified in (3) above.</p>
<h1>A. 3 WORST-CASE ANALYSIS FOR HOPPER TASK</h1>
<p>Figure 2 illustrates the performance of the three considered policies: viz. TRPO on mean parameters, $\operatorname{EPOpt}(\epsilon=1)$, and $\operatorname{EPOpt}(\epsilon=0.1)$. We similarly analyze the $10^{\text {th }}$ percentile of the return distribution as a proxy for worst-case analysis, which is important for a robust control policy (here, distribution of returns for a given model instance is due to variations in initial conditions). The corresponding results are presented below:
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: $10^{\text {th }}$ percentile of return distribution for the hopper task. $\operatorname{EPOpt}(\epsilon=0.1)$ clearly outperforms the other approaches. The $10^{\text {th }}$ of return distribution for $\operatorname{EPOpt}(\epsilon=0.1)$ also nearly overlaps with the expected return, indicating that the policies trained using $\operatorname{EPOpt}(\epsilon=0.1)$ are highly robust and reliable.</p>
<h2>A. 4 ROBUSTNESS ANALYSIS FOR HALF-CHEETAH TASK</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Performance of policies for various model instances for the half-cheetah domain, similar to Figure 2. Again, it is observed that the adversarial trained policy is robust and generalizes well to all models in the source distribution.</p>
<h1>A. 5 Different SEtTINGS FOR $\epsilon$</h1>
<p>Here, we analyze how different settings for $\epsilon$ influences the robustness of learned policies. The policies in this section have been trained for 200 iterations with 240 trajectory samples per iteration. Similar to the description in Section 3.1, the first 100 iterations use $\epsilon=1$, and the final 100 iterations use the desired $\epsilon$. The source distribution is described in Table 1. We test the performance on a grid over the model parameters. Our results, summarized in Table 2, indicate that decreasing $\epsilon$ decreases the variance in performance, along with a small decrease in average performance, and hence enhances robustness.</p>
<p>Table 2: Performance statistics for different $\epsilon$ settings for the hopper task</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Performance (Return)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\epsilon$</td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">std</td>
<td style="text-align: center;">Percentiles</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">90</td>
</tr>
<tr>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">2889</td>
<td style="text-align: center;">502</td>
<td style="text-align: center;">1662</td>
<td style="text-align: center;">2633</td>
<td style="text-align: center;">2841</td>
<td style="text-align: center;">2939</td>
<td style="text-align: center;">2966</td>
<td style="text-align: center;">3083</td>
</tr>
<tr>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">3063</td>
<td style="text-align: center;">579</td>
<td style="text-align: center;">1618</td>
<td style="text-align: center;">2848</td>
<td style="text-align: center;">3223</td>
<td style="text-align: center;">3286</td>
<td style="text-align: center;">3336</td>
<td style="text-align: center;">3396</td>
</tr>
<tr>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">3097</td>
<td style="text-align: center;">665</td>
<td style="text-align: center;">1527</td>
<td style="text-align: center;">1833</td>
<td style="text-align: center;">3259</td>
<td style="text-align: center;">3362</td>
<td style="text-align: center;">3423</td>
<td style="text-align: center;">3483</td>
</tr>
<tr>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">3121</td>
<td style="text-align: center;">706</td>
<td style="text-align: center;">1461</td>
<td style="text-align: center;">1635</td>
<td style="text-align: center;">3251</td>
<td style="text-align: center;">3395</td>
<td style="text-align: center;">3477</td>
<td style="text-align: center;">3513</td>
</tr>
<tr>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">3126</td>
<td style="text-align: center;">869</td>
<td style="text-align: center;">1013</td>
<td style="text-align: center;">1241</td>
<td style="text-align: center;">3114</td>
<td style="text-align: center;">3412</td>
<td style="text-align: center;">3504</td>
<td style="text-align: center;">3546</td>
</tr>
<tr>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">3122</td>
<td style="text-align: center;">1009</td>
<td style="text-align: center;">984</td>
<td style="text-align: center;">1196</td>
<td style="text-align: center;">1969</td>
<td style="text-align: center;">3430</td>
<td style="text-align: center;">3481</td>
<td style="text-align: center;">3567</td>
</tr>
<tr>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">3133</td>
<td style="text-align: center;">952</td>
<td style="text-align: center;">1005</td>
<td style="text-align: center;">1516</td>
<td style="text-align: center;">2187</td>
<td style="text-align: center;">3363</td>
<td style="text-align: center;">3486</td>
<td style="text-align: center;">3548</td>
</tr>
<tr>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">3224</td>
<td style="text-align: center;">1060</td>
<td style="text-align: center;">1198</td>
<td style="text-align: center;">1354</td>
<td style="text-align: center;">1928</td>
<td style="text-align: center;">3461</td>
<td style="text-align: center;">3557</td>
<td style="text-align: center;">3604</td>
</tr>
<tr>
<td style="text-align: center;">Max-Lik</td>
<td style="text-align: center;">1710</td>
<td style="text-align: center;">1140</td>
<td style="text-align: center;">352</td>
<td style="text-align: center;">414</td>
<td style="text-align: center;">646</td>
<td style="text-align: center;">1323</td>
<td style="text-align: center;">3088</td>
<td style="text-align: center;">3272</td>
</tr>
</tbody>
</table>
<h2>A. 6 IMPORTANCE OF BASELINE FOR BATCHPOLOPT</h2>
<p>As described in Section 3.1, it is important to use a good baseline estimate for the value function for the batch policy optimization step. When optimizing for the expected return, we can interpret the baseline as a variance reduction technique. Intuitively, policy gradient methods adjust parameters of the policy to improve probability of trajectories in proportion to their performance. By using a baseline for the value function, we make updates that increase probability of trajectories that perform better than average and vice versa. In practice, this variance reduction is essential for getting policy gradients to work. For the CVaR case, Tamar et al. (2015) showed that without using a baseline, the policy gradient is biased. To study importance of the baseline, we first consider the case where we do not employ the adversarial sub-sampling step, and fix $\epsilon=1$. We use a linear baseline with a time-varying feature vector as described in Section 3.1. Figure 8(a) depicts the learning curve for the source distribution in Table 1. The results indicate that use of a baseline is important to make policy gradients work well in practice.</p>
<p>Next, we turn to the case of $\epsilon&lt;1$. As mentioned in section 3.1, setting a low $\epsilon$ from the start leads to unstable learning. The adversarial nature encourages penalizing poor trajectories more, which constrains the initial exploration needed to find promising trajectories. Thus we will "pre-train" by using $\epsilon=1$ for some iterations, before switching to the desired $\epsilon$ setting. From Figure 8(a), it is clear that pre-training without a baseline is unlikely to help, since the performance is poor. Thus, we use the following setup for comparison: for 100 iterations, $\operatorname{EPOpt}(\epsilon=1)$ is used with the baseline. Subsequently, we switch to $\operatorname{EPOpt}(\epsilon=0.1)$ and run for another 100 iterations, totaling 200 iterations. The results of this experiment are depicted in Figure 8(b). This result indicates that use of a baseline is crucial for the CVaR case, without which the performance degrades very quickly. We repeated the experiment with 100 iterations of pre-training with $\epsilon=1$ and without baseline, and observed the same effect. These empirical results reinforce the theoretical findings of Tamar et al. (2015).</p>
<h2>A. 7 Alternate Policy Gradient Subroutines for BatchPoloPt</h2>
<p>As emphasized previously, EPOpt is a generic policy gradient based meta algorithm for finding robust policies. The BatchPolOpt step (line 9, Algorithm 1) calls one gradient step of a policy gradient method, the choice of which is largely orthogonal to the main contributions of this paper. For the</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: (a) depicts the learning curve for $\operatorname{EPOpt}(\epsilon=1)$ with and without baselines. The learning curves indicate that use of a baseline provides a better ascent direction, thereby enabling faster learning. Figure 8(b) depicts the learning curve when using the average return and CVaR objectives. For the comparison, we "pre-train" for 100 iterations with $\epsilon=1$ setting and using a baseline. The results indicates that a baseline is very important for the CVaR objective $(\epsilon&lt;1)$, without which the performance drops very quickly. Here, performance is the average return in the source distribution.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Learning curves for $\operatorname{EPOpt}(\epsilon=1)$ when using the TRPO and REINFORCE methods for the BatchPolOpt step.
reported results, we have used TRPO as the policy gradient method. Here, we compare the results to the case when using the classic REINFORCE algorithm. For this comparison, we use the same value function baseline parametrization for both TRPO and REINFORCE. Figure 9 depicts the learning curve when using the two policy gradient methods. We observe that performance with TRPO is significantly better. When optimizing over probability distributions, the natural gradient can navigate the warped parameter space better than the "vanilla" gradient. This observation is consistent with the findings of Kakade (2001), Schulman et al. (2015), and Duan et al. (2016).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Supplementary video: https://youtu.be/w1YJ9vwaoto&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-284 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-284</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-284</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-4c927b41149f24eb49f8f3bc0d61ede2c10cc405</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4c927b41149f24eb49f8f3bc0d61ede2c10cc405" target="_blank">Neural Discovery of Permutation Subgroups</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Artificial Intelligence and Statistics</p>
                <p><strong>Paper TL;DR:</strong> The results show that one could discover any subgroup of type S_{k} (k \leq n) by learning an $S_{n}$-invariant function and a linear transformation and provide a general theorem that can be extended to discover other subgroups of S{n}$.</p>
                <p><strong>Paper Abstract:</strong> We consider the problem of discovering subgroup $H$ of permutation group $S_{n}$. Unlike the traditional $H$-invariant networks wherein $H$ is assumed to be known, we present a method to discover the underlying subgroup, given that it satisfies certain conditions. Our results show that one could discover any subgroup of type $S_{k} (k \leq n)$ by learning an $S_{n}$-invariant function and a linear transformation. We also prove similar results for cyclic and dihedral subgroups. Finally, we provide a general theorem that can be extended to discover other subgroups of $S_{n}$. We also demonstrate the applicability of our results through numerical experiments on image-digit sum and symmetric polynomial regression tasks.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-284",
    "paper_id": "paper-4c927b41149f24eb49f8f3bc0d61ede2c10cc405",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00494175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neural Discovery of Permutation Subgroups</h1>
<p>Pavan Karjol Rohan Kashyap Prathosh A P<br>Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, Karnataka</p>
<h4>Abstract</h4>
<p>We consider the problem of discovering subgroup $H$ of permutation group $S_{n}$. Unlike the traditional $H$-invariant networks wherein $H$ is assumed to be known, we present a method to discover the underlying subgroup, given that it satisfies certain conditions. Our results show that one could discover any subgroup of type $S_{k}(k \leq$ $n)$ by learning an $S_{n}$-invariant function and a linear transformation. We also prove similar results for cyclic and dihedral subgroups. Finally, we provide a general theorem that can be extended to discover other subgroups of $S_{n}$. We also demonstrate the applicability of our results through numerical experiments on image-digit sum and symmetric polynomial regression tasks.</p>
<h2>1 INTRODUCTION</h2>
<h3>1.1 Background</h3>
<p>Deep Learning has proven to be a successful paradigm for learning the underlying regularities of sensory data such as images, text, and audio Brown et al., 2020; He et al., 2016; Ramesh et al., 2022). The data in the physical world possess a predefined structure with a low-dimensional manifold approximation within a higher dimensional euclidean space (Cayton, 2005; Schölkopf et al., 1998). However, the task of supervised learning in such a high-dimensional data space demands a large number of data points to counter the curse of dimensionality. Thus, universal function approximations using neural networks in such a setting can be prohibitively expensive to curate large datasets for diverse applications such as medical imaging. This calls for the need for inductive bias to be incorporated into our networks such that they can utilize these priors for learning valuable representations in the feature space. Convolutional Neural Net-</p>
<p>Proceedings of the $26^{\text {th }}$ International Conference on Artificial Intelligence and Statistics (AISTATS) 2023, Valencia, Spain. PMLR: Volume 206. Copyright 2023 by the author(s).
works proposed by (LeCun et al., 1995) incorporate translation equivariance and thus preserve translation symmetry. This is highly effective for perception tasks since it enables the model with a notion of locality and symmetry, i.e., the input and label are both invariant to shifts (preserves this property across layers), and has likewise shown substantial gains in image recognition tasks as demonstrated in (Szegedy et al., 2017; He et al., 2016). However, from a group-theoretic perspective, CNN happens to represent a particular case of invariance under the action of a specific group. This leads to studying and understanding its usability when extended to a more general setting, i.e., equivariance or invariance to any generic group action. Thus, learning such representations across neural nets ensures preserving symmetry across the network and efficiently discovering the underlying factors of data variations by utilizing these priors.</p>
<h3>1.2 Group Invariance and Equivariance</h3>
<p>Learning symmetries from data has been studied extensively in (Senior et al., 2020; Raviv et al., 2007; Monti et al., 2017; Rossi et al., 2022). Invariant and equivariant classes of functions impose a powerful inductive prior to our models in a statistically efficient manner which aids in learning useful representations on a wide range of data (Bogatskiy et al., 2020; Esteves, 2020). Group equivariant or invariant networks (Cohen et al., 2018; Esteves et al., 2018) exploit the inherent symmetrical structure in the data, i.e., equivariance or invariance to a certain set of group operations (geometric priors) and can thus result in a significant reduction in the sample complexity and lead to better generalization. This has ubiquitous applications in various tasks such as predicting protein interactions (Gainza et al., 2020) and estimating population statistics (Zaheer et al., 2017).</p>
<p>One of the important classes of group invariance networks corresponds to the permutation group $\left(S_{n}\right)$, i.e., the group of all permutations of a set of cardinality $n$. Zaheer et al. (2017) have focused extensively on the applicability of permutation equivariance and invariance functions on arbitrary objects such as sets. Whereas, (Kicki et al., 2020) proposes a $G$-invariant network to approximate functions that are in-</p>
<p>variant under the action of any given permutation subgroup of $S_{n}$. Moreover, it is crucial to consider subgroups of $S_{n}$, since any finite group is isomorphic to a subgroup of $S_{n}$ (Cayley's theorem) for some $n$. For example, the Quarternanian group $Q_{8}$ is isomorphic to a subgroup of $S_{8}$. In addition, other interesting applications of functions correspond to subgroups of $S_{n}$. For instance, the area of an $n$-polygon is a $\mathbb{Z}_{n}$-invariant function of the polygon's vertices (Kicki et al., 2020).</p>
<h3>1.3 Contributions</h3>
<p>In most of the works mentioned earlier, the group (or subgroup) is assumed to be known a priori. This restricted form of modeling choice leads to reduced flexibility (also restrictions). It makes incorporating symmetries into our networks highly infeasible for real-world applications where the underlying structure is unknown. Motivated by this, we demonstrate a general framework, i.e., $G$-invariant network and a linear transformation for discovering the underlying subgroup of $S_{n}$ under certain conditions. Our main contributions can be summarized as follows:</p>
<p>In this work, we propose a general framework to discover the underlying subgroup of $S_{n}$ under a broad set of conditions.</p>
<ul>
<li>We prove that we could learn any conjugate group (with respect to $G$ ) via a linear transformation and $G$ invariant network.</li>
<li>We extend this approach, i.e., a linear transformation and $G$-invariant network to different classes of subgroups such as permutation group of $k$ (out of $n$ ) elements $S_{k}$, cyclic subgroups $\mathbb{Z}<em 2="2" k="k">{k}$ and dihedral subgroups $D</em>}$. The $G$-invariant networks for the above families are $S_{n}, \mathbb{Z<em 2="2" n="n">{n}$ and $D</em>$ respectively. In the latter two cases, $k$ should divide $n$.</li>
<li>We prove a general theorem that can guide us to discover other classes of subgroups.</li>
<li>We substantiate the above results through experiments on image-digit sum and symmetric polynomial regression tasks.</li>
</ul>
<h2>2 PRIOR WORK</h2>
<h3>2.1 Group Invariant and Equivariant Networks</h3>
<p>Significant progress has been made in incorporating invariances to deep neural nets in the last decade (Cohen et al., 2019; Cohen and Welling, 2016b; Ravanbakhsh et al., 2017; Ravanbakhsh, 2020; Wang et al., 2020). We observe that most of the invariant neural networks proposed in the literature assume the knowledge of the underlying symmetry group. Various generalizations, i.e., group equivariant
or invariant neural networks, are presented in (Cohen et al., 2019; Kondor et al., 2018).</p>
<p>Cohen and Welling (2016a) introduce Group Equivariant Convolutional Neural Networks (G-CNNs) as a natural extension of the Convolutional Neural Network to construct a representation with the structure of a linear G-space. Further, Cohen et al. (2019) presents a general theory for studying G-CNNs on homogeneous spaces and illustrates a one-to-one correspondence between linear equivariant maps of feature spaces and convolutions kernels. Cohen and Welling (2016b) provides a theoretical framework to study steerable representations in convolutional neural networks and establish mathematical connections between representation learning and representation theory. Ravanbakhsh (2020) presents the universality of invariant and equivariant MLPs with a single hidden layer. Additionally, they show the unconditional universality result for Abelian groups. Kondor and Trivedi (2018) utilize both representation theory and noncommutative harmonic analysis to establish the convolution formulae in a more general setting, i.e., invariance under the action of any compact group.</p>
<h3>2.2 Permutation Invariant and Equivariant Networks</h3>
<p>Zaheer et al. (2017) demonstrates the applicability of equivariant and invariant networks on various set-like objects. Further, they show that any permutation invariant function can be expressed in a standard form, i.e., $\rho\left(\sum_{i} \phi\left(x_{i}\right)\right)$, which corresponds to an elegant deep neural network architecture. Janossy pooling (Murphy et al., 2018) extends the same to build permutation invariant functions using a generic class of functions. The works, as mentioned earlier, focus mainly on the permutation group $S_{n}$.</p>
<p>Recent works by Kicki et al. (2020) and Maron et al. (2019) provide a general architecture invariant to any given subgroup of $S_{n}$. Kicki et al. (2020) design a $G$-invariant neural network for approximating functions (can specifically approximate any $G$-invariant function) $f: X \rightarrow R$ using $G$-equivariant network and sum-product formulation, where $X$ is a compact subset of $R^{n \times m}$, for some $n, m&gt;0$ ) for any given permutation subgroup $G$ of $S_{n}$. They extend this work to study the invariance properties of hierarchical groups $G&lt;H \leq S_{n}$. However, in most cases, the underlying subgroup is generally unknown.</p>
<h3>2.3 Automatic Symmetry Discovery</h3>
<p>Dehmamy et al. (2021) introduces the Lie algebra convolutional network (L-Conv), an infinitesimal version of GConv, for automatic symmetric discovery. Their framework for continuous symmetries relies on Lie algebras rather than Lie groups and can thus encode an infinite group without discretizing (Cohen and Welling, 2016a) or summing over irreps. They show that the $L$-Conv network can serve as a building block for constructing any group equivari-</p>
<p>ant feedforward architecture. They also unveil interesting connections between equivariant loss and Lagrangians in field theory and robustness and Euler-Lagrange equations. However, these apply only to Lie groups and are not specific to subgroups of the permutation groups. Anselmi et al. (2019) proposes to learn symmetry-adapted representations and also deduce a regularization scheme for learning these representations without assuming the knowledge of the underlying subgroup (of $S_{n}$ ). However, their proposed solution is implemented in an unsupervised way. Benton et al. (2020) and Zhou et al. (2020) also propose different methods for learning symmetries when the group $G$ is unknown.</p>
<h2>3 PRELIMINARIES</h2>
<p>This section gives a brief overview of various mathematical concepts used in our work. Let $G$ be a group.</p>
<ol>
<li>Group action :- The action of $G$ on a set $X$ is defined using the following map (written as $g \cdot x, \forall g \in$ $G$ and $x \in X$ ) :</li>
</ol>
<p>$$
\theta: G \times X \rightarrow X
$$</p>
<p>satisfying the following properties :</p>
<ul>
<li>$g_{1} \cdot\left(g_{2} \cdot x\right)=\left(g_{1} g_{2}\right) \cdot x \quad \forall g_{1}, g_{2} \in G$ and $x \in X$,</li>
<li>
<p>$1 \cdot x=x, \quad \forall x \in X$
where 1 is the identity element of $G$.</p>
</li>
<li>
<p>Group invariant function :- A function $f: X \rightarrow Y$ is said to be group invariant with respect to $G$, if,</p>
</li>
</ul>
<p>$$
f(x)=f(g \cdot x), \quad \forall g \in G \text { and } x \in X
$$</p>
<p>We call $f$ a $G$-invariant function.
3. Group equivariant function :- A function $f: X \rightarrow$ $Y$ is said to be group equivariant with respect to $G$, if for any $g \in G, \exists \tilde{g} \in G$, such that</p>
<p>$$
f(g \cdot x)=\tilde{g} \cdot f(x), \forall x \in X
$$</p>
<p>We call $f$ a $G$-equivariant function.
4. Conjugate subgroups :- Two subgroups $G_{1}$ and $G_{2}$ of $G$ are said to be conjugates, if $\exists g \in G$ such that,</p>
<p>$$
G_{2}=g G_{1} g^{-1}:=\left{g k g^{-1}: k \in G_{1}\right}
$$</p>
<ol>
<li>Normal subgroup :- A subgroup $N$ is said to be normal in $G$, if $\forall g \in G$</li>
</ol>
<p>$$
g N g^{-1}=N
$$</p>
<p>i.e., there are no subgroups that are conjugate to $N$.</p>
<p>We describe the notations used for various subgroups of $S_{n}$ in Table (1). Henceforth, unless explicitly mentioned, we follow the notations mentioned in Table (1).</p>
<p>Table 1: Descriptions of notations</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Symbol</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$S_{n}$</td>
<td style="text-align: left;">Permutation group of $n$ elements</td>
</tr>
<tr>
<td style="text-align: left;">$S_{k}^{(0)}$</td>
<td style="text-align: left;">Permutation subgroup of first $k$ elements</td>
</tr>
<tr>
<td style="text-align: left;">$S_{k}$</td>
<td style="text-align: left;">Permutation subgroup of random $k$ elements</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbb{Z}_{n}$</td>
<td style="text-align: left;">Cyclic subgroup of $n$ elements</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbb{Z}_{k}^{(0)}$</td>
<td style="text-align: left;">Cyclic subgroup of first $k$ elements</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbb{Z}_{k}$</td>
<td style="text-align: left;">Cyclic subgroup of random $k$ elements</td>
</tr>
<tr>
<td style="text-align: left;">$D_{2 n}$</td>
<td style="text-align: left;">Dihedral subgroup of $n$ elements</td>
</tr>
<tr>
<td style="text-align: left;">$D_{2 k}^{(0)}$</td>
<td style="text-align: left;">Dihedral subgroup of first $k$ elements</td>
</tr>
<tr>
<td style="text-align: left;">$D_{k}$</td>
<td style="text-align: left;">Dihedral subgroup of random $k$ elements</td>
</tr>
<tr>
<td style="text-align: left;">$A_{n}$</td>
<td style="text-align: left;">Alternating subgroup of $n$ elements</td>
</tr>
<tr>
<td style="text-align: left;">$A_{k}$</td>
<td style="text-align: left;">Alternating subgroup of random $k$ elements</td>
</tr>
</tbody>
</table>
<h2>4 PROPOSED WORK</h2>
<h3>4.1 Problem statement</h3>
<p>We consider the problem of learning an $H$-invariant function $f: X \rightarrow \mathbb{R}$, where $X=[0,1]^{n} \subset R^{n}$ and $H$ is the unknown subgroup of $S_{n}$. In general, learning such a function is intractable. However, we show that it is possible to learn such a function, i.e., discover the underlying subgroup $H$, where $H$ belongs to a certain class of subgroups (we explicitly state our conditions in Theorem 4.3, 4.4 and 4.5). The general consequence of our analysis is that learning a $H$-invariant function is thus equivalent to learning a $G$-invariant function along with a linear transformation, given that $G$ and $H$ satisfy certain conditions. Since any given $G$ can have several such subgroups, we propose to learn the underlying subgroup $H$ by exploiting the existing structures using a family of $G$-invariant functions (such as the one mentioned in Zaheer et al. (2017) for the permutation group $S_{n}$ ) and a learnable linear transformation. We formalize these ideas in the coming subsections.</p>
<p>To prove our results, we employ the following theorem regarding $S_{n}$-invariant functions (Zaheer et al., 2017), which shows that any such function can be expressed in a canonical form.
Theorem 4.1 (Deep sets). $f: X=[0,1]^{n} \rightarrow \mathbb{R}$ is a permutation invariant ( $S_{n}$-invariant) continuous function iff if has the representation,</p>
<p>$$
f(x)=\rho\left(\sum_{i=1}^{n} \gamma\left(x_{i}\right)\right), x=\left[x_{1}, x_{2}, \ldots x_{n}\right]^{T}
$$</p>
<p>for some continuous outer and inner functions $\rho: \mathbb{R}^{n+1} \rightarrow$ $\mathbb{R}, \gamma:[0,1] \rightarrow \mathbb{R}^{n+1}$.</p>
<p>We get the following result if we consider the permutations of the first $k$ elements.
Corollary 4.1.1. $f:[0,1]^{n} \rightarrow \mathbb{R}$ be an $S_{k}^{0}$-invariant con-</p>
<p>tinuous function iff it has the representation,</p>
<p>$$
f(x)=\rho\left(\sum_{i=1}^{k} \gamma\left(x_{i}\right), x_{k+1}, \ldots, x_{n}\right)
$$</p>
<p>Proof. To prove Theorem 4.1, it has been shown that (Zaheer et al., 2017), $\mathcal{X}^{(n)}=\left{x_{1}, x_{2}, \ldots, x_{n} \subset[0,1]^{n}: x_{1} \leq\right.$ $\left.x_{2} \leq x_{3} \cdots \leq x_{n}\right}$ is homeomorphic to $\sum_{i=1}^{n} \gamma\left(X_{i}\right)$, where</p>
<p>$$
\gamma(t)=\left[1, t, t^{2}, \ldots t^{n}\right]^{T}
$$</p>
<p>Hence, $\mathcal{X}^{(n: k)}=\left{x_{1}, x_{2}, \ldots, x_{n} \subset[0,1]^{n}: x_{1} \leq x_{2} \leq\right.$ $\left.x_{3} \cdots \leq x_{k}\right}$ is homeomorphic to $\sum_{i=1}^{k} \gamma\left(X_{i}\right) \times[0,1]^{n-k}$. Let, $E(x)=\left[\sum_{i=1}^{k} \gamma\left(x_{i}\right), x_{k+1}, \ldots, x_{n}\right]^{T}$. Then, it is an homeomorphism from $\mathcal{X}^{(n: k)}$ to $\operatorname{Im}(E)$ (Image of E). If we set $\rho=f E^{-1}$, we get $\rho(E(x))=f(x)$.</p>
<p>We use the same definition of $\gamma$ (Zaheer et al., 2017) provided in the eq. (8) in the subsequent results as well. Now, we state our first result using the conjugacy relation between subgroups.
Lemma 4.2. Any $S_{k}$-invariant function $\psi$, can be realized through composition of an $S_{k}^{(0)}$-invariant function $\phi$ and a linear transformation $M$, i.e., $\psi=\phi \cdot M$. In addition, $\psi$ can be realised through the following form,</p>
<p>$$
\psi(x)=\rho\left(\sum_{i=1}^{k} \gamma\left(m_{i}^{T} x\right), m_{k+1}^{T} x, \ldots, m_{n}^{T} x\right)
$$</p>
<p>where $m_{i}$ is the $i^{\text {th }}$ row of $M$.</p>
<p>Proof. Note that any $S_{k}$ is conjugate to $S_{k}^{(0)}$. Thus, $\exists g \in$ $S_{n}$ such that</p>
<p>$$
S_{k}^{(0)}=g S_{k} g^{-1}
$$</p>
<p>Let $\psi: X \rightarrow R$ be an $S_{k}$-invariant function, i.e.,</p>
<p>$$
\begin{aligned}
\psi(x) &amp; =\psi(h \cdot x), \quad \forall h \in S_{k}, x \in X \
\psi\left(\left(g^{-1} g\right) \cdot x\right) &amp; =\psi\left(\left(g^{-1} u g\right) \cdot x\right), \quad \forall u \in S_{k}^{(0)} \
\left(\psi g^{-1}\right)(g \cdot x) &amp; =\left(\psi g^{-1}\right)(u \cdot(g \cdot x)) \
\left(\psi g^{-1}\right)(M x) &amp; =\left(\psi g^{-1}\right)(u \cdot(M x))
\end{aligned}
$$</p>
<p>From eq. (11), we see that $\phi=\psi \cdot g^{-1}$ and $M=g$ are the desired $S_{k}^{(0)}$-invariant function and the linear transformation respectively and $\phi=\psi \cdot M$. We get the second part of the result by applying Corollary 4.1.1 to $\phi$.</p>
<p>We could also relax the conjugacy condition, i.e., discover subgroups of type $S_{k}$ when $k$ itself is unknown. This is formalized in the following result.</p>
<p>Theorem 4.3 (Subgroups of type $S_{k}$ ). Any $S_{k}$-invariant function $(k \leq n) \psi$, can be realised using an $S_{n}$-invariant function and a linear transformation, in specific, it can be realised through the following form,</p>
<p>$$
\psi(x)=(\phi \cdot \hat{M})(x)=\rho\left(\left[\begin{array}{c}
(I-M) x \
\sum_{i=1}^{n} \gamma\left(m_{i}^{T} x\right)
\end{array}\right]\right)
$$</p>
<p>where $\hat{M}=\left[\begin{array}{c}I-M \ M\end{array}\right]$ and
$\phi(y)=\left[y_{1}, \ldots, y_{n}, \sum_{i=1}^{n} \gamma\left(y_{n+i}\right)\right]^{T}$
Proof. Since $S_{k}$ is conjugate to $S_{k}^{0}$, it is enough to prove the result for $S_{k}^{0}$-invariant function. Hence, the goal is to show that $(I-M) X \times \sum_{i=1}^{n} \gamma\left(m_{i}^{T} X\right)$ is homeomorphic to $\sum_{m=1}^{k} \gamma\left(X_{m}\right) \times[0,1]^{n-k}$ (from Corollary 4.1.1 and Lemma 4.2) for some linear transformation $M$. Suppose,</p>
<p>$$
M=\left[\begin{array}{cc}
I_{k \times k} &amp; 0 \
0 &amp; 0
\end{array}\right]
$$</p>
<p>then,</p>
<p>$$
\left[\begin{array}{c}
(I-M) x \
\sum_{i=1}^{n} \gamma\left(m_{i}^{T} x\right)
\end{array}\right]=\left[\begin{array}{c}
\boldsymbol{0}^{(k)} \
x_{k+1} \
\cdot \
\cdot \
x_{n} \
B+\sum_{i=1}^{k} \gamma\left(x_{i}\right)
\end{array}\right]
$$</p>
<p>where $B=(n-k) \gamma(0)$ and $\boldsymbol{0}^{(k)}$ is $k$-dimensional zero vector. Thus, from RHS of the eq. (14), the above claim follows. (Note that, the function $M x \mapsto \sum_{i=1}^{n} \gamma\left(m_{i}^{T} x\right)$ is $S_{n}$-invariant and $\phi$ is $S_{2 n}^{n}$-invariant function).</p>
<p>We now extend our method to cyclic and dihedral subgroups of $S_{n}$ and state the following result.
Theorem 4.4 (Cyclic and Dihedral subgroups). If $k \mid n$, any $\mathbb{Z}<em 2="2" k="k">{k}$-invariant (or $D</em>}$-invariant) function $\psi$, can be realised using a $\mathbb{Z<em 2="2" n="n">{n}$-invariant (or $D</em>$-invariant) function $\phi$ and a linear transformation, in specific, it can be realised through the following form,</p>
<p>$$
\psi(x)=(\phi \cdot \hat{M})(x)
$$</p>
<p>where $\hat{M}=\left[\begin{array}{c}M \ I-L\end{array}\right]$ for some $M, L \in \mathbb{R}^{n \times n}$.
Proof. In this proof, without loss of generality, we prove the result for $\mathbb{Z}_{k}^{(0)}$-invariant function. Suppose,</p>
<p>$$
M=\left[\begin{array}{cc}
I_{k \times k} &amp; 0 \
I_{k \times k} &amp; 0 \
\vdots &amp; \vdots \
I_{k \times k} &amp; 0
\end{array}\right], \quad L=\left[\begin{array}{cc}
I_{k \times k} &amp; 0 \
0 &amp; 0
\end{array}\right]
$$</p>
<p>Since $k \mid n$, we can stack the $I_{k \times k}$ matrices as shown in eq. (16). Then, $M: X \rightarrow X$ is defined as,</p>
<p>$$
\begin{aligned}
x=\left[x_{1}, x_{2} \ldots x_{n}\right]^{T} \longmapsto M x= &amp; {\left[x_{1}, x_{2}, \ldots x_{k}\right.} \
&amp; x_{1}, x_{2}, \ldots, x_{k} \
&amp; \vdots \
&amp; \left.x_{1}, x_{2}, \ldots, x_{k}\right]^{T}
\end{aligned}
$$</p>
<p>Under the action of $\mathbb{Z}<em k="k">{k}\left(h \cdot x\right.$, for some $h \in \mathbb{Z}</em>$ ), we get that,</p>
<p>$$
x \stackrel{h}{\longmapsto} x^{\prime}=\left[x_{u}, x_{u+1}, \ldots, x_{k}, x_{1}, \ldots, x_{u-1}\right]^{T}
$$</p>
<p>which corresponds to $\left(g \cdot(M x)\right.$, for some $\left.g \in \mathbb{Z}_{n}\right)$,</p>
<p>$$
\begin{aligned}
M x \stackrel{g}{\longmapsto} &amp; M x^{\prime}=\left[x_{u}, x_{u+1}, \ldots, x_{k}, x_{1}, \ldots, x_{u-1}\right. \
&amp; \left.x_{u}, x_{u+1}, \ldots, x_{k}, x_{1}, \ldots, x_{u-1}\right. \
&amp; \vdots \
&amp; \left.x_{u}, x_{u+1}, \ldots, x_{k}, x_{1}, \ldots, x_{u-1}\right]^{T}
\end{aligned}
$$</p>
<p>Similarly, the converse is also true, i.e., $\mathbb{Z}<em k="k">{n}$-action on $M x$ corresponds to $\mathbb{Z}</em>}$-action on $x$. Hence, the $\mathbb{Z<em n="n">{k}$-invariant function of $x$ corresponds to $\mathbb{Z}</em>}$-invariance of $M x$. Note that, the $\mathbb{Z<em 2="2" k="k">{n}$-invariance of the function $\phi$ is with respect to the first $n$ elements (out of $2 n$ ) of its input vector. Similar proof holds for dihedral groups $\left(D</em>\right)$.}\right.$ and $\left.D_{2 n</p>
<p>The above set of techniques can also be extended to other classes of subgroups. In this regard, we state the following general result.
Theorem 4.5. Any $H$-invariant function $\psi$ can be learnt through composing a $G$-invariant function $\phi$ with a linear transformation $M$, i.e., $\psi=\phi \cdot M$ if the following conditions hold,</p>
<ol>
<li>For any $h \in H, \exists g \in G$ such that $M(h \cdot x)=g$. $(M x), \forall x \in X$</li>
<li>For any $g \in G$ such that $g \cdot(M x) \in R(M), \exists h \in H$ such that $M(h \cdot x)=g \cdot(M x), \forall x \in X$, where $R(M)$ is the range of $M$.</li>
</ol>
<p>Proof. The claim directly follows from the following observations.</p>
<p>Condition (1) states that, any action $h \cdot x$ (action of $H$ on $X$ ) corresponds to an action $g \cdot(M x)$ (action of $G$ on $R(M)$ ). Similarly, condition (2) states that, any action $g \cdot(M x)$ corresponds to an action $h \cdot x$.</p>
<h2>5 DISCUSSION</h2>
<p>The underlying theme from the results stated in the previous section is that we could discover any subgroup belonging to a particular class of subgroups by learning a $G$ invariant function and a linear transformation. Depending on the class, the chosen G varies. We further elaborate on these observations in the following subsections.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Generic framework for learning $H$-invariant function. The dotted arrows point towards specific examples of linear and $G$-invariant functions. The corresponding $H$-invariant functions are $S_{k}$-invariant and $\mathbb{Z}_{k}$-invariant.</p>
<h3>5.1 Conjugate Groups</h3>
<p>In Lemma 4.2, the class of subgroups corresponds to those of type $S_{k}$ (fixed $k$ ) and the corresponding $G$ can be $S_{k}^{0}$. We observe that, for a fixed $k$, even if we don't know the exact underlying subgroup $S_{k}$ (a total of $\binom{n}{k}$ possibilities), we could learn this unknown subgroup. In addition, we also incorporate the canonical form of permutation invariant functions in the resulting architecture. Moreover, this result can be generalized to any class of conjugate subgroups, and the corresponding $G$ is one of these conjugate groups. The significance of this result lies in the fact that a variety of subgroups are related through conjugation. For instance, all $\mathbb{Z}<em k="k">{k}$ form one conjugacy class for a given $k$, and so does $A</em>$ 's.</p>
<p>This result is not entirely helpful if the underlying subgroup is normal since it is not conjugate to any other subgroup. However, this is not much of a hindrance since the only non-trivial proper normal subgroup of $S_{n}$ is $A_{n}, \forall n \geq 5$.</p>
<h2>$5.2 S_{k}, \mathbb{Z}<em 2="2" k="k">{k}$ and $D</em>$ Subgroups</h2>
<p>Theorem 4.3 focuses on subgroups of type $S_{k}$ (varying $k$ and $k \in{1,2, \ldots, n}$ ), and the corresponding $G$ is $S_{n}$ itself. We incorporate the canonical form of permutation invariant functions here as well. We observe that the number of such subgroups is $2^{n}-1$ for a given $n$. Hence, we could learn any of these subgroups with the standard ar-</p>
<p>chitecture of an $S_{n}$-invariant function and a linear transformation. Note that if $k$ is fixed, either of the architectural forms given by Lemma 4.2 and Theorem 4.3 is applicable. We will discuss the corresponding empirical results in the coming sections. Theorem 4.4 considers subgroups of the cyclic $\mathbb{Z}<em 2="2" k="k">{k}$ and dihedral group $D</em>}$. The corresponding $G$ invariant functions are of $\mathbb{Z<em 2="2" n="n">{n}$ and $D</em>$, respectively.</p>
<h3>5.3 Generalization</h3>
<p>Theorem 4.5 presents a general set of conditions to be satisfied to learn any $H$-invariant function using a $G$ invariant function and a linear transformation. As such, the previous results are specific cases of this Theorem. However, they provide explicit structures of the linear transformation $M$. These can help design appropriate training techniques to learn the optimum $M$, while the general result of Theorem 4.5 can guide us towards discovering results for new classes of subgroups.</p>
<h3>5.4 Limitations</h3>
<p>The proposed framework presumes the knowledge of the underlying class of subgroups apriori (but not the exact subgroup) and an appropriate value of $n$ for $S_{n}, \mathbb{Z}<em 2="2" n="n">{n}$ or $D</em>$ invariant functions. The drawbacks mentioned here are interesting research directions to pursue in the future.</p>
<h2>6 EXPERIMENTS</h2>
<p>We evaluate the accuracy of our proposed method on image-digit sum and symmetric polynomial regression tasks. The problem of image-digit sum can be modified and cast as learning an $S_{k}$-invariant function, while the polynomial regression task intrinsically corresponds to learning a $G$-invariant function. These are summarized in the following subsections.</p>
<h3>6.1 Image-Digit Sum</h3>
<p>This task aims to find the sum of $k$ digits using the MNISTm (Loosli et al. (2007)) handwritten digits dataset. It consists of 8 million gray scale $28 \times 28$ images of digits ${0,1, \ldots, 9}$. We employ a training set of $150 k$ samples and a test set of $30 k$ samples. We consider the following approaches for evaluation.</p>
<ol>
<li>Deep Sets- $S_{k}$ :- $S_{k}$-invariant neural network proposed by Zaheer et al. (2017).</li>
<li>LSTM:- LSTM network as mentioned in Zaheer et al. (2017).</li>
<li>Proposed method:- A linear layer followed by an $S_{n}$ invariant network.</li>
</ol>
<p>For the LSTM network and the proposed method, the input is a random sample of $n(n=10)$ images, and the target is the sum of $k$ ( $k$ less than $n$ ) digit labels. We run separate experiments for each of $k \in{1,3,5,7,9}$. Since all $n$ images are given as input, the two approaches are agnostic of the underlying subgroup. However, we feed only these $k$ of these images as input for the first approach, while the target output remains the same. As such, this task is equivalent to learning an $S_{k}$-invariant function.</p>
<h3>6.2 Symmetric Polynomial Regression</h3>
<p>We evaluate the performance of our method on symmetric polynomial regression tasks as discussed in Kicki et al. (2020), primarily for subgroups of $\mathbb{Z}<em 16="16">{10}$ and $\mathbb{Z}</em>}$. For all our experiments, we utilize a $\mathbb{Z<em 10="10">{n}$-invariant neural network with a Sum-Product layer as discussed in Kicki et al. (2020) and a linear layer. First, we run our experiments for subgroups of $\mathbb{Z}</em>}$, i.e., $\mathbb{Z<em 16="16">{5}$ and the group itself (trivial subgroup). We then access the performance for subgroups of $\mathbb{Z}</em>}$, namely $\mathbb{Z<em 4="4">{2}, \mathbb{Z}</em>}, \mathbb{Z<em 16="16">{8}, \mathbb{Z}</em>$ using a similar architectural design. We consider the following approaches for evaluation.</p>
<ol>
<li>G-invariant:- $\mathbb{Z}<em k="k">{k}$-invariant neural network proposed by Kicki et al. (2020). In this context, $G=\mathbb{Z}</em>$.</li>
<li>Simple-FC:- A stack of fully-connected feedforward layers.</li>
<li>Conv-1D:- A simple convolutional neural network and feedforward layers.</li>
<li>Proposed method:- A linear layer followed by a $\mathbb{Z}_{n}$ invariant network.</li>
</ol>
<p>The architectural details of the models considered in our experiments are discussed in the appendix section.</p>
<h2>7 RESULTS</h2>
<h3>7.1 Image-Digit Sum</h3>
<p>The test mean absolute errors (MAEs) for the image-digit sum task are shown in Table 2. We observe that the proposed method outperforms the LSTM baseline and is competitive with respect to the Deep Sets method ( k input images) when the underlying subgroup $S_{k}$ is known. In addition, our method converges faster when compared to the LSTM network, which is apparent from the plots for the training and validation errors in Figure 2.</p>
<h3>7.2 Symmetric Polynomial Regression</h3>
<p>In the $\mathbb{Z}<em 5="5">{k}$-invariant polynomial regression task, we train our models for 2500 epochs for each of the subgroups of $\mathbb{Z}</em>$.}$ and $\mathbb{Z}_{10</p>
<p>Table 2: MAE $\left[\times 10^{-2}\right]$ for Image Digit-Sum task</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>$S_{1}$</th>
<th>$S_{2}$</th>
<th>$S_{3}$</th>
<th>$S_{4}$</th>
<th>$S_{5}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Deep Sets- $S_{2}$</td>
<td>$5.61 \pm 0.35$</td>
<td>$7.66 \pm 0.26$</td>
<td>$8.02 \pm 0.2$</td>
<td>$7.68 \pm 0.43$</td>
<td>$6.97 \pm 0.39$</td>
</tr>
<tr>
<td>Proposed</td>
<td>$5.73 \pm 0.39$</td>
<td>$7.78 \pm 0.49$</td>
<td>$8.19 \pm 0.36$</td>
<td>$7.84 \pm 0.41$</td>
<td>$7.26 \pm 0.58$</td>
</tr>
<tr>
<td>LSTM</td>
<td>$6.23 \pm 0.53$</td>
<td>$9.65 \pm 0.57$</td>
<td>$11.98 \pm 0.46$</td>
<td>$13.35 \pm 1.02$</td>
<td>$12.92 \pm 1.42$</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Training and Validation loss (MAE) for ImageDigit Sum using MNIST dataset.</p>
<p>In Table 3, 4 and 5 we compare the given baselines with our proposed method for the task of discovering unknown subgroups. Our method outperforms the Simple-FC and Conv1D baseline networks for each of the given subgroups. As expected, it does not match the baseline architecture, the $\mathbb{Z}_{k}$-invariant network (the subgroup is known apriori for this baseline) by a significant margin for each of the diverse set of subgroups we have considered in this task. However, in a few cases, we observe large standard deviations and attribute such values to outliers. A detailed version of our results and the mathematical definition of the polynomials is presented in the appendix section.</p>
<p>From Figure 3, it is evident that the $\mathbb{Z}_{5}$-invariant function outperforms both our method and the baselines by a significant margin. The Simple FC and Conv-1D networks have very similar performances and show no prominent effect, even with an increase in data size.</p>
<p>Table 3: MAE $\left[\times 10^{-2}\right]$ for $\mathbb{Z}<em 10="10">{5}: \mathbb{Z}</em>$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Validation</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathbb{Z}_{5}$-invariant</td>
<td style="text-align: left;">$2.65 \pm 0.91$</td>
<td style="text-align: left;">$7.32 \pm 0.55$</td>
<td style="text-align: left;">$7.53 \pm 0.576$</td>
</tr>
<tr>
<td style="text-align: left;">Proposed</td>
<td style="text-align: left;">$4.48 \pm 1.25$</td>
<td style="text-align: left;">$24.56 \pm 6.93$</td>
<td style="text-align: left;">$24.78 \pm 6.45$</td>
</tr>
<tr>
<td style="text-align: left;">Conv-1D</td>
<td style="text-align: left;">$20.90 \pm 4.91$</td>
<td style="text-align: left;">$32.96 \pm 1.31$</td>
<td style="text-align: left;">$32.33 \pm 1.18$</td>
</tr>
<tr>
<td style="text-align: left;">Simple-FC</td>
<td style="text-align: left;">$23.86 \pm 3.87$</td>
<td style="text-align: left;">$33.57 \pm 2.07$</td>
<td style="text-align: left;">$33.14 \pm 2.11$</td>
</tr>
</tbody>
</table>
<h3>7.3 Effect of the data size on the performance</h3>
<p>This section aims to assess the effect of the dataset size in learning $\mathbb{Z}_{k}$-invariant functions using our proposed method and hope to gain a better understanding in such a setting. To analyze our model performance with respect to data size, we use 16, 32, and 64 data points for training (as mentioned in Kicki et al. (2020), we randomly sample these values from $[0,1]$ ) and use 480 and 4800 as validation and test sets respectively to assess the generalization ability for each of these methods as mentioned above. We report the mean and</p>
<p>Table 4: MAE $\left[\times 10^{-2}\right]$ for $\mathbb{Z}<em 10="10">{10}: \mathbb{Z}</em>$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Validation</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathbb{Z}_{10}$-invariant</td>
<td style="text-align: left;">$6.89 \pm 1.31$</td>
<td style="text-align: left;">$16.68 \pm 0.55$</td>
<td style="text-align: left;">$17.16 \pm 0.56$</td>
</tr>
<tr>
<td style="text-align: left;">Proposed</td>
<td style="text-align: left;">$14.52 \pm 1.72$</td>
<td style="text-align: left;">$39.69 \pm 4.13$</td>
<td style="text-align: left;">$40.11 \pm 4.17$</td>
</tr>
<tr>
<td style="text-align: left;">Conv-1D</td>
<td style="text-align: left;">$35.71 \pm 2.71$</td>
<td style="text-align: left;">$52.96 \pm 0.70$</td>
<td style="text-align: left;">$50.63 \pm 1.33$</td>
</tr>
<tr>
<td style="text-align: left;">Simple-FC</td>
<td style="text-align: left;">$46.13 \pm 2.27$</td>
<td style="text-align: left;">$54.62 \pm 1.34$</td>
<td style="text-align: left;">$51.64 \pm 0.89$</td>
</tr>
</tbody>
</table>
<p>Table 5: MAE $\left[\times 10^{-2}\right]$ for $\mathbb{Z}<em 16="16">{4}: \mathbb{Z}</em>$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Validation</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathbb{Z}_{4}$-invariant</td>
<td style="text-align: left;">$1.21 \pm 0.25$</td>
<td style="text-align: left;">$3.41 \pm 0.4$</td>
<td style="text-align: left;">$3.54 \pm 0.39$</td>
</tr>
<tr>
<td style="text-align: left;">Proposed</td>
<td style="text-align: left;">$3.32 \pm 1.65$</td>
<td style="text-align: left;">$23.70 \pm 4.87$</td>
<td style="text-align: left;">$24.69 \pm 5.25$</td>
</tr>
<tr>
<td style="text-align: left;">Conv-1D</td>
<td style="text-align: left;">$8.39 \pm 3.02$</td>
<td style="text-align: left;">$31.34 \pm 0.77$</td>
<td style="text-align: left;">$31.10 \pm 0.87$</td>
</tr>
<tr>
<td style="text-align: left;">Simple-FC</td>
<td style="text-align: left;">$7.27 \pm 5.03$</td>
<td style="text-align: left;">$30.82 \pm 1.74$</td>
<td style="text-align: left;">$30.83 \pm 1.61$</td>
</tr>
</tbody>
</table>
<p>standard deviation values across 10 randomly initialized iterations.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The MAE value comparisons using the test dataset for all the models we have considered for the $\mathbb{Z}<em 10="10">{5}$ : $\mathbb{Z}</em>$ task. The $X$-axis represents the size of the training set $(16,32,64)$.</p>
<p>We also examine the Simple-FC and Conv-1D network by increasing its parameter count, i.e., varying the number of neurons in each layer. However, we observe no significant gains in doing so, as mentioned in the appendix section for at least a few subgroups.</p>
<h3>7.4 Interpretability</h3>
<h3>7.4.1 Image-Digit Sum</h3>
<p>The resulting M matrix is interpretable, and we consistently observe the expected pattern for the image-digit sum task. Note that any row-permuted version of the matrix structure, as shown in eq. (13) will work since the transformed space is still homeomorphic. The $M$ matrices for $S_{5}$ and $S_{9}$ (extracted after training) are depicted in Figure 4. The columns with dark green squares match the actual indices.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: $M$ matrices for $S_{5}$ and $S_{9}$ after training.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (a) $M$ matrix for $\mathbb{Z}<em 16="16">{4}: \mathbb{Z}</em>$ (b) Reference matrix.</p>
<h3>7.4.2 Polynomial Regression</h3>
<p>We observe that the $M$-matrix extracted after training (Figure (5.a)) does not exactly capture the expected pattern, i.e., a stack of identity matrices (Figure (5.b)), even though it nearly masks most of the irrelevant columns $(n-k)$. The former behavior (lack of exact structure) explains the difference in performance with respect to the $\mathbb{Z}<em k="k">{k}$-invariant network, while the latter (masking behavior) describes the superior model performance compared to other baselines. Also, the masking of irrelevant columns already conveys the underlying subgroup; thus, we use this information to estimate the true indices. We estimate the significant indices using the $L 1$-norm of columns of $M$ and the mean as the threshold. The results (for different number of training data points $N$ and different $\mathbb{Z}</em>}: \mathbb{Z<em k="k">{n}$ 's) of the success rate of the estimation are given in Table 6, where we count the estimation as success when the estimated indices exactly match the true indices; otherwise, as a failure. We run each experiment for 10 trials. We get high estimation accuracy in most of the cases except for $N=16$. The estimated indices can be used to run a $\mathbb{Z}</em>$-invariant network (or proposed method with fixed $M$ ) and obtain better performance on regression tasks.</p>
<h2>8 CONCLUSION</h2>
<p>In this work, we studied the problem of discovering the underlying subgroup of $S_{n}$, i.e., learning a $H$-invariant func-</p>
<p>Table 6: Estimation Accuracy (in \%)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$Z_{k}: Z_{n}$</th>
<th style="text-align: left;">16</th>
<th style="text-align: left;">32</th>
<th style="text-align: left;">64</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$Z_{4}: Z_{16}$</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
</tr>
<tr>
<td style="text-align: left;">$Z_{5}: Z_{10}$</td>
<td style="text-align: left;">80</td>
<td style="text-align: left;">100</td>
<td style="text-align: left;">100</td>
</tr>
<tr>
<td style="text-align: left;">$Z_{8}: Z_{16}$</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">80</td>
<td style="text-align: left;">100</td>
</tr>
</tbody>
</table>
<p>tion where $H$ is an unknown subgroup of $S_{n}$. We proved that we could learn any $H$-invariant function using a $G$ invariant function and a linear transformation provided $H$ belongs to a specific class of subgroups. We considered various subgroups, such as conjugate subgroups, permutation subgroups of $k$ elements, and cyclic and dihedral subgroups, and illustrated unique structures of the corresponding linear transformations. We demonstrated the validity of our theoretical analysis through empirical results. We also discussed the limitations of our method, which may lead to exciting research directions in the future.</p>
<h2>References</h2>
<p>Anselmi, F., Evangelopoulos, G., Rosasco, L., and Poggio, T. (2019). Symmetry-adapted representation learning. Pattern Recognition, 86:201-208.</p>
<p>Benton, G., Finzi, M., Izmailov, P., and Wilson, A. G. (2020). Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605-17616.</p>
<p>Bogatskiy, A., Anderson, B., Offermann, J., Roussi, M., Miller, D., and Kondor, R. (2020). Lorentz group equivariant neural network for particle physics. In International Conference on Machine Learning, pages 9921002. PMLR.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Cayton, L. (2005). Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep, 12(1-17):1.</p>
<p>Cohen, T. and Welling, M. (2016a). Group equivariant convolutional networks. In International conference on machine learning, pages 2990-2999. PMLR.</p>
<p>Cohen, T. S., Geiger, M., Köhler, J., and Welling, M. (2018). Spherical cnns. arXiv preprint arXiv:1801.10130.</p>
<p>Cohen, T. S., Geiger, M., and Weiler, M. (2019). A general theory of equivariant cnns on homogeneous spaces. Advances in neural information processing systems, 32.</p>
<p>Cohen, T. S. and Welling, M. (2016b). Steerable cnns. arXiv preprint arXiv:1612.08498.</p>
<p>Dehmamy, N., Walters, R., Liu, Y., Wang, D., and Yu, R. (2021). Automatic symmetry discovery with lie algebra convolutional network. Advances in Neural Information Processing Systems, 34:2503-2515.</p>
<p>Derksen, H. and Kemper, G. (2001). Computational invariant theory. Book manuscript.
Esteves, C. (2020). Theoretical aspects of group equivariant neural networks. arXiv preprint arXiv:2004.05154.
Esteves, C., Allen-Blanchette, C., Makadia, A., and Daniilidis, K. (2018). Learning so (3) equivariant representations with spherical cnns. In Proceedings of the European Conference on Computer Vision (ECCV), pages 52-68.</p>
<p>Gainza, P., Sverrisson, F., Monti, F., Rodola, E., Boscaini, D., Bronstein, M., and Correia, B. (2020). Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. Nature Methods, 17(2):184-192.</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778.</p>
<p>Kicki, P., Ozay, M., and Skrzypczyński, P. (2020). A computationally efficient neural network invariant to the action of symmetry subgroups. arXiv preprint arXiv:2002.07528.</p>
<p>Kondor, R., Lin, Z., and Trivedi, S. (2018). Clebschgordan nets: a fully fourier space spherical convolutional neural network. Advances in Neural Information Processing Systems, 31.</p>
<p>Kondor, R. and Trivedi, S. (2018). On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International Conference on Machine Learning, pages 2747-2755. PMLR.</p>
<p>LeCun, Y., Bengio, Y., et al. (1995). Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995.</p>
<p>Loosli, G., Canu, S., and Bottou, L. (2007). Training invariant support vector machines using selective sampling. Large scale kernel machines, 2.</p>
<p>Maron, H., Fetaya, E., Segol, N., and Lipman, Y. (2019). On the universality of invariant networks. In International conference on machine learning, pages 43634371. PMLR.</p>
<p>Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M. M. (2017). Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5115-5124.</p>
<p>Murphy, R. L., Srinivasan, B., Rao, V., and Ribeiro, B. (2018). Janossy pooling: Learning deep permutation-
invariant functions for variable-size inputs. arXiv preprint arXiv:1811.01900.</p>
<p>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125.</p>
<p>Ravanbakhsh, S. (2020). Universal equivariant multilayer perceptrons. In International Conference on Machine Learning, pages 7996-8006. PMLR.</p>
<p>Ravanbakhsh, S., Schneider, J., and Poczos, B. (2017). Equivariance through parameter-sharing. In International conference on machine learning, pages 28922901. PMLR.</p>
<p>Raviv, D., Bronstein, A. M., Bronstein, M. M., and Kimmel, R. (2007). Symmetries of non-rigid shapes. In 2007 IEEE 11th International Conference on Computer Vision, pages 1-7. IEEE.</p>
<p>Rossi, E., Monti, F., Leng, Y., Bronstein, M., and Dong, X. (2022). Learning to infer structures of network games. In International Conference on Machine Learning, pages 18809-18827. PMLR.</p>
<p>Schölkopf, B., Smola, A., and Müller, K.-R. (1998). Nonlinear component analysis as a kernel eigenvalue problem. Neural computation, 10(5):1299-1319.</p>
<p>Senior, A. W., Evans, R., Jumper, J., Kirkpatrick, J., Sifre, L., Green, T., Qin, C., Žídek, A., Nelson, A. W., Bridgland, A., et al. (2020). Improved protein structure prediction using potentials from deep learning. Nature, 577(7792):706-710.</p>
<p>Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A. A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-first AAAI conference on artificial intelligence.</p>
<p>Wang, R., Walters, R., and Yu, R. (2020). Incorporating symmetry into deep dynamics models for improved generalization. arXiv preprint arXiv:2002.03061.</p>
<p>Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. (2017). Deep sets. Advances in neural information processing systems, 30.</p>
<p>Zhou, A., Knowles, T., and Finn, C. (2020). Meta-learning symmetries by reparameterization. arXiv preprint arXiv:2007.02933.</p>
<h1>Supplementary Materials</h1>
<h2>9 Appendix</h2>
<h3>9.1 Remarks regarding theoretical results</h3>
<ul>
<li>To obtain the function $\rho$ mentioned in Lemma 4.2, Theorem 4.3, and Theorem 4.4 of the main paper, we use the same technique presented in the proof of Corollary 4.1.1, i.e., $\rho=\psi E^{-1}$, where $E$ is the corresponding homeomorphism.</li>
<li>Lemma 4.2, Theorem 4.3, and Theorem 4.4 are special cases of Theorem 4.5. This claim directly follows once we specify the corresponding group actions. As such, the proofs of Lemma 4.2 and Theorem 4.4 already describe the required group actions. However, this is not obvious in Theorem 4.3. In that case, we observe the following:</li>
</ul>
<p>$$
\begin{aligned}
x=\left[x_{1}, x_{2} \ldots x_{n}\right]^{T} \longmapsto M x= &amp; {\left[0,0, .^{k} \cdot, 0, x_{k+1}, x_{k+2}, \ldots x_{n}\right.} \
&amp; \left.x_{1}, x_{2}, \ldots x_{k}, 0,0, \eta \cdot{ }^{k}, 0\right]^{T}
\end{aligned}
$$</p>
<p>where $M=\left[\begin{array}{cc}I_{k \times k} &amp; 0 \ 0 &amp; 0\end{array}\right]$ as mentioned in eq. (13) of the main paper.
Under the action of $S_{k}^{(0)}\left(h \cdot x\right.$, for some $\left.h \in S_{k}^{(0)}\right)$, we get,</p>
<p>$$
x \stackrel{h}{\longmapsto} x^{\prime}=\left[x_{h(1)}, x_{h(2)}, \ldots, x_{h(k)}, x_{k+1}, \ldots, x_{n}\right]^{T}
$$</p>
<p>which corresponds to $\left(g \cdot(M x)\right.$, for some $\left.g \in S_{2 n}^{n}\right)\left(S_{2 n}^{n}\right.$ is the group of permutations of first $n$ elements out of $2 n$ elements),</p>
<p>$$
\begin{aligned}
M x \stackrel{g}{\longmapsto} M x^{\prime}= &amp; {\left[0,0, .^{k} \cdot 0, x_{k+1}, x_{k+2}, \ldots, x_{n}\right.} \
&amp; \left.x_{g(1)}, x_{g(2)}, \ldots, x_{g(k)}, 0,0, \eta \cdot{ }^{k}, 0\right]^{T}
\end{aligned}
$$</p>
<p>Similary, the action of $S_{2 n}^{n}$ on $R(M)$ (range of M ) corresponds to the action of $S_{k}^{(0)}$ on $X$.
In the following subsections, we describe the architectures of various models and additional resutls considered in our experiments.</p>
<p>Table 7: MAE $\left[\times 10^{-2}\right]$ Image Digit-Sum task</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">$S_{1}$</th>
<th style="text-align: left;">$S_{3}$</th>
<th style="text-align: left;">$S_{5}$</th>
<th style="text-align: left;">$S_{7}$</th>
<th style="text-align: left;">$S_{9}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: left;">$6.23 \pm 0.53$</td>
<td style="text-align: left;">$9.65 \pm 0.57$</td>
<td style="text-align: left;">$11.98 \pm 0.46$</td>
<td style="text-align: left;">$13.35 \pm 1.02$</td>
<td style="text-align: left;">$12.92 \pm 1.42$</td>
</tr>
<tr>
<td style="text-align: left;">Conv-1D</td>
<td style="text-align: left;">$36.32 \pm 0.12$</td>
<td style="text-align: left;">$19.11 \pm 0.49$</td>
<td style="text-align: left;">$27.92 \pm 0.41$</td>
<td style="text-align: left;">$35.42 \pm 0.19$</td>
<td style="text-align: left;">$40.83 \pm 0.11$</td>
</tr>
<tr>
<td style="text-align: left;">Simple FC</td>
<td style="text-align: left;">$25.26 \pm 0.01$</td>
<td style="text-align: left;">$18.18 \pm 0.15$</td>
<td style="text-align: left;">$35.27 \pm 0.07$</td>
<td style="text-align: left;">$44.51 \pm 0.58$</td>
<td style="text-align: left;">$51.79 \pm 0.89$</td>
</tr>
</tbody>
</table>
<h3>9.2 Image-Digit sum</h3>
<ol>
<li>Deep Sets- $S_{k}$ :- $S_{k}$-invariant neural network proposed by Zaheer et al. (2017). It consists of two networks, $\gamma$, and $\rho$. Each element in the input is passed through the $\gamma$ network, followed by the sum operation. The result is then fed to the second network $\rho$. The network $\gamma$ is a feed-forward network consisting of three dense layers with tanh activation, and the second network is a dense layer.</li>
<li>LSTM:- The LSTM network used for comparison in Zaheer et al. (2017). It consists of two dense layers, an LSTM layer followed by two dense layers. The activation used is tanh function.</li>
<li>Proposed method:- An $S_{n}$-invariant network follows a linear layer. The $S_{n}$-invariant network has the same architecture as Deep Sets (the first approach) except the input layer.</li>
</ol>
<p>Table 8: Definitions of the various polynomials used in the main paper.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">INVARIANCE</th>
<th style="text-align: left;">POLYNOMIAL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathbb{Z}<em 16="16">{2}: \mathbb{Z}</em>$</td>
<td style="text-align: left;">$x_{1} x_{2}^{2}+x_{2} x_{1}^{2}$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbb{Z}<em 16="16">{4}: \mathbb{Z}</em>$</td>
<td style="text-align: left;">$x_{1} x_{2}^{2}+x_{2} x_{3}^{2}+x_{3} x_{4}^{2}+x_{4} x_{1}^{2}$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbb{Z}<em 10="10">{5}: \mathbb{Z}</em>$</td>
<td style="text-align: left;">$x_{1} x_{2}^{2}+x_{2} x_{3}^{2}+x_{3} x_{4}^{2}+x_{4} x_{5}^{2}+x_{5} x_{1}^{2}$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbb{Z}<em 16="16">{8}: \mathbb{Z}</em>$</td>
<td style="text-align: left;">$x_{1} x_{2}^{2}+x_{2} x_{3}^{2}+\ldots+x_{7} x_{8}^{2}+x_{8} x_{1}^{2}$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbb{Z}<em 10="10">{10}: \mathbb{Z}</em>$</td>
<td style="text-align: left;">$x_{1} x_{2}^{2}+x_{2} x_{3}^{2}+\ldots+x_{9} x_{10}^{2}+x_{10} x_{1}^{2}$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbb{Z}<em 16="16">{16}: \mathbb{Z}</em>$</td>
<td style="text-align: left;">$x_{1} x_{2}^{2}+x_{2} x_{3}^{2}+\ldots+x_{15} x_{16}^{2}+x_{16} x_{1}^{2}$</td>
</tr>
</tbody>
</table>
<h1>9.3 Comparison between $S_{k}$-invariant networks with backbone as $S_{k}^{(0)}$ and $S_{n}$</h1>
<p>As specified in Section 5.2 of the main paper, any $S_{k}$-invariant network can be realized through either an $S_{k}^{(0)}$ or an $S_{n}$ invariant network and a linear layer when $k$ is fixed. In general, we observed that the $S_{n}$-invariant network as the backbone does better than the $S_{k}^{(0)}$ network. We attribute this to the expressivity power of the linear transformation (based on its specific structure) when an $S_{n}$ invariant network is used.</p>
<h3>9.4 Symmetric Polynomial Regression</h3>
<ol>
<li>G-invariant:- $\mathbb{Z}<em k="k">{k}$-invariant network implemented using the design described in Kicki et al. (2020). As discussed in Kicki et al. (2020), it is a composition of a $\mathbb{Z}</em>}$-equivariant network and a Sum-Product Layer. It then uses a MultiLayer Perceptron to process the $\mathbb{Z<em k="k">{k}$-invariant representation of the input and thus predicts the polynomial output. The network is thus invariant under the action of the given permutation subgroup $\mathbb{Z}</em>$.</li>
<li>Simple-FC:- This is an abbreviation of a fully-connected neural network without the Reynolds operator, i.e., group averaging for this baseline implementation (Derksen and Kemper (2001)).</li>
<li>Conv-1D:- This is an abbreviation of the 1D Convolutional neural network equipped with fully-connected layers.</li>
<li>Proposed method:- To discover the underlying subgroup, we use a $\mathbb{Z}<em n="n">{n}$-invariant neural network with the addition of a linear layer. The architectural design of the $\mathbb{Z}</em>$-invariant function is the same as the $G$-invariant network.</li>
</ol>
<p>The hyperparameters of the above models are given in Table. (6-9) of Kicki et al. (2020).
Table 9: MAE $\left[\times 10^{-2}\right]$ Polynomial Regression</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">$\mathbb{Z}<em 16="16">{2}: \mathbb{Z}</em>$</th>
<th style="text-align: left;">$\mathbb{Z}<em 16="16">{8}: \mathbb{Z}</em>$</th>
<th style="text-align: left;">$\mathbb{Z}<em 16="16">{16}: \mathbb{Z}</em>$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathbb{Z}_{k}$-invariant</td>
<td style="text-align: left;">$1.26 \pm 0.25$</td>
<td style="text-align: left;">$14.30 \pm 1.04$</td>
<td style="text-align: left;">$17.16 \pm 0.59$</td>
</tr>
<tr>
<td style="text-align: left;">Proposed</td>
<td style="text-align: left;">$22.27 \pm 4.25$</td>
<td style="text-align: left;">$39.31 \pm 5.12$</td>
<td style="text-align: left;">$40.11 \pm 4.17$</td>
</tr>
<tr>
<td style="text-align: left;">Conv-1D</td>
<td style="text-align: left;">$21.67 \pm 0.69$</td>
<td style="text-align: left;">$50.15 \pm 1.03$</td>
<td style="text-align: left;">$68.38 \pm 3.13$</td>
</tr>
<tr>
<td style="text-align: left;">Simple FC</td>
<td style="text-align: left;">$21.61 \pm 1.54$</td>
<td style="text-align: left;">$45.92 \pm 2.83$</td>
<td style="text-align: left;">$51.64 \pm 0.89$</td>
</tr>
</tbody>
</table>
<h3>9.4.1 Additional results</h3>
<p>The test errors for $\mathbb{Z}<em 16="16">{2}: \mathbb{Z}</em>}, \mathbb{Z<em 16="16">{8}: \mathbb{Z}</em>}$ and $\mathbb{Z<em 16="16">{16}: \mathbb{Z}</em>$ invariant functions are provided in Table 9. The details of all the polynomials used in our work are presented in Table 8.</p>            </div>
        </div>

    </div>
</body>
</html>
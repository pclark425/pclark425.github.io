<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1166 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1166</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1166</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-257279825</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.01032v3.pdf" target="_blank">ESceme: Vision-and-Language Navigation with Episodic Scene Memory</a></p>
                <p><strong>Paper Abstract:</strong> Vision-and-language navigation (VLN) simulates a visual agent that follows natural-language navigation instructions in real-world scenes. Existing approaches have made enormous progress in navigation in new environments, such as beam search, pre-exploration, and dynamic or hierarchical history encoding. To balance generalization and efficiency, we resort to memorizing visited scenarios apart from the ongoing route while navigating. In this work, we introduce a mechanism of Episodic Scene memory (ESceme) for VLN that wakes an agent's memories of past visits when it enters the current scene. The episodic scene memory allows the agent to envision a bigger picture of the next prediction. This way, the agent learns to utilize dynamically updated information instead of merely adapting to the current observations. We provide a simple yet effective implementation of ESceme by enhancing the accessible views at each location and progressively completing the memory while navigating. We verify the superiority of ESceme on short-horizon (R2R), long-horizon (R4R), and vision-and-dialog (CVDN) VLN tasks. Our ESceme also wins first place on the CVDN leaderboard. Code is available: \url{https://github.com/qizhust/esceme}.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1166",
    "paper_id": "paper-257279825",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0063675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ESceme: Vision-and-Language Navigation with Episodic Scene Memory</p>
<p>Qi Zheng 
Shenzhen University</p>
<p>University of Sydney
3 JD Explore Academy 4 DATA61CSIRO</p>
<p>Daqing Liu 
Chaoyue Wang 
Jing Zhang 
University of Sydney
3 JD Explore Academy 4 DATA61CSIRO</p>
<p>Dadong Wang 
Dacheng Tao 
University of Sydney
3 JD Explore Academy 4 DATA61CSIRO</p>
<p>ESceme: Vision-and-Language Navigation with Episodic Scene Memory
7726882647838EA1C98D8A8496B5D3C9
Vision-and-language navigation (VLN) simulates a visual agent that follows natural-language navigation instructions in real-world scenes.Existing approaches have made enormous progress in navigation in new environments, such as beam search, pre-exploration, and dynamic or hierarchical history encoding.To balance generalization and efficiency, we resort to memorizing visited scenarios apart from the ongoing route while navigating.In this work, we introduce a mechanism of Episodic Scene memory (ESceme) for VLN that wakes an agent's memories of past visits when it enters the current scene.The episodic scene memory allows the agent to envision a bigger picture of the next prediction.This way, the agent learns to utilize dynamically updated information instead of merely adapting to the current observations.We provide a simple yet effective implementation of ESceme by enhancing the accessible views at each location and progressively completing the memory while navigating.We verify the superiority of ESceme on short-horizon (R2R), long-horizon (R4R), and vision-and-dialog (CVDN) VLN tasks.Our ESceme also wins first place on the CVDN leaderboard.Code is available: https://github.com/qizhust/esceme.</p>
<p>Introduction</p>
<p>With breakthroughs in computer vision and natural language understanding, the embodiment hypothesis that an intelligent agent is born from its interaction with environments [49] is now attracting more and more attention to embodied AI tasks such as vision-and-language navigation (VLN).VLN is firstly defined in [4] towards the goal of a robot carrying out general verbal instructions, where an agent is required to follow natural-language instructions based on what it sees and adapt to previously unseen environments.VLN has developed various settings, such as fine-grained and short-horizon navigation (e.g., R2R [4] and RxR [30]), long-horizon navigation (e.g., R4R [26]), visionand-dialogue navigation (e.g., CVDN [51]), and navigation with high-level instructions (e.g., REVERIE [45]).Compared with non-embodied VL tasks such as visual question answering [5] and visual captioning [12,60], VLN agents suffer from domain shifts and changing observations during multi-step decision-making in the scenarios.The next time, the agent enters this scene to conduct the second instruction along the red path.ESceme allows it to recall the visited nodes (i.e., the blue ones) at where it is standing (A) and choose the neighboring node B1 that will see "the white bookshelf" in one more step at C. Finally, it navigates towards the red dash route and reaches the target.</p>
<p>A vanilla Seq2Seq pipeline [4] that implicitly encodes path history with LSTMs [22] shows moderate navigating ability.Since then, VLN performance has been considerably improved by pre-training [21,25,10,46], data aug-1 arXiv:2303.01032v3[cs.CV] 15 Jul 2024 mentations [17,50,33], and algorithms that explicitly track past decisions along the trajectory [10,53,11].These methods learn enhanced representations by training VLN agents in each episode but ignore the dynamics of navigating over the whole data.Different strategies, including modified beam search [17] and pre-exploration [55,50,41,62], are devised to specifically increase adaptation to unseen environments at the cost of efficiency.Specifically, beam search significantly extends route length and involves much more interactions with the environment; pre-exploration takes extra steps to gather information and train the agent with auxiliary objectives before it can conduct given instructions.Such strategies incur burdensome time and computational expenses in practical usage.</p>
<p>In this work, we propose a navigation mechanism with Episodic Scene memory (ESceme) to balance generalization and efficiency by exploiting the dynamics of navigating all the episodes.ESceme requires no extra annotations or heavy computation and is agent-agnostic.We encode observation, instruction, and path history separately and update the scene memory during navigation via candidate enhancing.By preserving the memory among episodes, ESceme envisions the agent seeing a bigger picture in each decision.This way, the agent learns to utilize dynamically updated information instead of merely adapting to the current observations.Then during inference, it predicts actions with the progressively completed memory.A demonstration is shown in Fig. 1.When carrying out an instruction at Location A, the agent is to select one from the adjacent nodes B 1 -B 5 to navigate.It recalls the episodic scene memory, i.e., the blue route of a completed trajectory, and chooses Node B 1 that will see "the white bookshelf" in one more step at C.</p>
<p>We verify the superiority of ESceme in short-horizon navigation with fine-grained instruction (R2R), longhorizon navigation (R4R), and vision-and-dialog navigation (CVDN).We find that ESceme notably benefits navigation with longer routes (R4R and CVDN), promoting both successful reaching and path fidelity.Our method achieves the highest Goal Progress in the CVDN challenge.Besides a fair comparison with existing approaches under a single run, we test the performance with an approximately complete memory, where the agent fully updates its scene memory in the first round of navigation over all the episodes.We denote it as ESceme<em>, which serves as the upper bound of ESceme.We observe a further improvement in ESceme</em>, which indicates better-completed memory magnifies the advantage of ESceme.We hope this work can inspire further explorations in modeling episodic scene memory for VLN.</p>
<p>Since ESceme does not introduce any extra time or steps before following the instruction in inference, it is fair to compare it with its counterparts in the single-run setting.Very different from pre-exploration optimizing the parame-ters of an agent before solving the task, ESceme only renews its episodic memory while conducting instructions and requires no back-propagation operations.Moreover, ESceme neither involves beam search nor changes the local action space in sequential decision-making.These properties make ESceme both efficient and effective in reality use.Our contributions are summarized as follows:</p>
<p>‚Ä¢ We devise the first navigation mechanism with episodic scene memory (ESceme) for VLN to balance generalization and efficiency.</p>
<p>‚Ä¢ We provide a simple yet effective implementation of ESceme via candidate enhancing, tested with two navigation architectures and two inferring strategies.</p>
<p>‚Ä¢ We verify the superiority of ESceme in short-horizon (R2R), long-horizon (R4R), and vision-and-dialog (CVDN) navigation, and achieve a new state-of-the-art.</p>
<p>2 Related work</p>
<p>Vision-and-language navigation</p>
<p>Since [4] defined the VLN task and provided an LSTMbased sequence-to-sequence baseline (Seq2Seq), numerous approaches have been developed.A branch of methods improves navigation via data augmentation, such as SF [17], EnvDrop [50], and EnvEdit [33].As for agent training, [56] model the environment to provide planned-ahead information during navigation.RCM [55] provides an intrinsic reward for reinforcement learning via an instructiontrajectory matching critic.[57] jointly train an agent on VLN and vision-dialog navigation (MT-RCM+EnvAg).To fully use available semantic information in the environment, AuxRN [62] devises four self-supervised auxiliary reasoning tasks.TDSTP [61] introduces an extra target location estimation during finetuning to achieve reliable path planning.Many methods explore more effective feature representations and architectures, such as PTA [13], OAAM [44], NvEM [1], RelGraph [23], MTVM [38], and SEvol [8].</p>
<p>Some methods construct and reason about a graph of navigation while conducting an episode, such as NTS [7] and RECON [47] in the ImageGoal space and ETPNav [2] and CMTP [9] in the VLN space.VLN-SIG [32] adds the tasks of generating semantics for future navigation views in pre-training and fine-tuning, and contributes to a more powerful agent backbone.KERM [36] introduces knowledge described by text to aid action prediction, which is useful mainly in seen environments.GridMM [58] builds a grid memory with fine-grained features and adopts a global action space, which improves the success rate but suffers from a much longer trajectory length.</p>
<p>Inspired by the breakthrough of large-scale pretrained BERT [28] in natural language processing tasks, PRESS [35] replaces RNNs with pre-trained BERT to encode instructions and achieves a non-trivial improvement in unseen environments.PREVELENT [21] pretrains BERT from scratch using image-text-action triplets and further boosts the performance.RecBERT [25] integrates a recurrent unit into a BERT model to be timeaware.[10] propose the first VLN network that allows a sequence of historical memory and can be optimized endto-end (HAMT).HOP [46] designs trajectory order modeling and group order modeling tasks to model temporal order information in pre-training.CSAP [59] proposes trajectory-conditioned masked fragment modeling and contrastive semantic-alignment modeling tasks for pre-training.ADAPT [37] explicitly learns action-level modality alignment with action prompts.There are also some works specially designed for vision-and-dialog navigation, such as VISITRON [48], SCoA [63], and CMN [64].</p>
<p>The differences between the proposed ESceme and previous graph-or map-construction approaches are twofold.First, they construct path-level memory along a route in a single conduction.ESceme maintains scene-level memory from multiple episodes in the same scenario.Second, they use the path-level memory in planning by extending the agent's action space from local to global.ESceme does not change the agent's action space.Instead, it improves navigation by increasing the information of each node, which is the core idea that makes ESceme perform better than pathlevel memory methods (e.g., EGP and SSM).Scene memory is also studied in other works [14,18,34,29,52,20].</p>
<p>The method that is the most closely related to ours is IVLN [29].Our setting is actually identical to IVLN, which reorganizes episodes into tours.We store scene IDs during inference instead of explicitly organizing episodes according to their IDs, yet the two ways result in the same effect.Although both explore the impact of episodic memory and compare with the same baseline model, i.e., HAMT, our work provides a more effective design of the memory mechanism and obtains better performance due to candidate enhancing.Instead, the episodic memory in IVLN encodes the memory map as a whole and observes even worse results than the path-level memory baseline (cf.Table 2 in IVLN).</p>
<p>Exploration strategies in VLN</p>
<p>As the navigation graph is pre-defined in discrete VLN, diverse strategies are adopted other than the regularly used single-run.For example, [17] modifies the standard beam search to select the final navigation route, which notably increases navigation success at the cost of unbearable trajectory lengths.More efficient pre-exploration methods are studied.For instance, a progress monitor is trained to discard unfinished trajectories during inference [39].[40] learn a regret module to decide when to backtrack.[27] compare partial paths with global information considered and backtrack only when necessary.AcPercep [54] learns an exploration policy to gather visual information for navigation.Although these methods improve searching efficiency, they heavily depend on manually designed or heuristic rules.[15] define a global action space for the first time and build a graphical representation of the environment for elegant exploration/backtracking.[53] extend EnvDrop [50] with an external structured scene memory (SSM) to promote exploration in the global action space.</p>
<p>Pre-exploration, which allows an agent to pre-explore unseen environments before navigating, is first introduced in [55] as a setting different from single-run and beam search.The obtained information functions in diverse ways.RCM [55] uses the exploration experience in selfsupervised imitation learning.EnvDrop [50] exploits the environment information for data augmentation via backtranslation.VLN-BERT [41] provides the agent with a global view for optimal route selection.AuxRN [62] finetunes the agent in unseen environments with auxiliary tasks.</p>
<p>Method</p>
<p>Problem formulation</p>
<p>Given an instruction X i , e.g., "Turn around and walk to the right of the room...", an agent starts from the initial location of route R i .It observes a panoramic view of the environment Y i .The panoramic view consists of K=36 single viewpoints, each of which is accompanied by an orientation (Œ∏, œï) indicating heading and elevation and a binary navigable signal.The agent selects a viewpoint from the navigable ones and moves to the next location with new observations.This process repeats until the agent takes the STOP action.</p>
<p>In a regular VLN task, there is a set of training samples
D = {(Y 1 , X 1 , R 1 ), ..., (Y N1 , X N1 , R N1 )}, where (X i , R i )
is the instruction-route pair in an environment Y i .The set {Y 1 , ..., Y N1 } denotes the seen environments during training.An agent is expected to learn navigation with D and carry out instructions in unseen scenarios given by D u = {(Y u 1 , X 1 ), ..., (Y u N2 , X N2 )}.The set {Y u 1 , ..., Y u N2 } denotes the unseen environments for test.</p>
<p>For a sequence prediction problem, history is an important source of information apart from observations and instructions.The shadow part in Fig. 2 shows a decision step by a general navigation approach that follows the pretraining-finetuning branch and encodes path history, represented by HAMT [10].We denote the vanilla features of K single views extracted by the observation encoder as {f 1 , ..., f K , f s }, which can be obtained by concatenating</p>
<p>Cross-Modal Encoder</p>
<p>Observation Encoder History Encoder</p>
<p>Instruction Encoder the separate features of encoded RGB images and orientations.f s is appended to allow a STOP action.Together with history representations {h 1 , ..., h t‚àí1 } from the history encoder and text representations {x cls , x 1 , ..., x L } from the instruction encoder, the features of the observations 1 ‚Éù are input into a cross-modal encoder for multi-modal fusion.A predictor block takes in the cross-modal representations {o
{ùíê‚Ä≤ ! , ‚Ä¶ , ùíê‚Ä≤ " , ùíê‚Ä≤ # } {ùíô $ %&amp;# , ùíô $ ! ‚Ä¶ , ùíô‚Ä≤ ' } {ùíâ $ ! , ‚Ä¶ , ùíâ $ ()! } Predictor ùëé ! {ùíé ! , ‚Ä¶ , ùíé " } Fusion {ùíê ! , ‚Ä¶ ,‚Ä≤ 1 , ..., o ‚Ä≤ K , o ‚Ä≤ s }, {h ‚Ä≤ 1 , ..., h ‚Ä≤ t‚àí1 }, and {x ‚Ä≤ cls , x ‚Ä≤ 1 , ..., x ‚Ä≤ L } to predict action a t .
Due to potential differences between seen and unseen environments, such as the appearance and layout of the scenario and the display of objects, an agent trained in the above way suffers from decreased decision ability.The mistake accumulates along the path, which incurs a heavy drop in successful navigation in new environments.Since strategies such as pre-exploration and beam search that exploit extra clues in a new scene are too expensive for a deployed robot, we propose a mechanism of episodic scene memory to balance accuracy and efficiency.Fig. 2 provides an overview of the proposed ESceme mechanism.By retrieving episodic memory for the K views at Step t, ESceme replaces the vanilla encoded observations with enhanced representations for cross-modal encoding and action prediction, i.e., 1 ‚Éù‚Üí 2 ‚Éù.In the following sections, we detail how to build the episodic scene memory and promote observations with the memory in navigation.</p>
<p>Episodic scene memory construction</p>
<p>We initialize the episodic memory of Scene Y with an empty graph
G (0) Y = (V (0) Y =‚àÖ, E(0)
Y =‚àÖ) if an agent has never seen the scene.Namely, for the first instruction in Scene Y , an agent starts navigation with an empty episodic memory.As shown in Fig. 3a, the start location has four neighbors and is added to G Y at the end of t=1 by
V (1) Y = {V 1 }. Node feature m V1 is an integration of its neighbors, m V1 = pooling(f V1,i ),(1)
where i‚àà{1, 2, 3, 4} in Fig. 3a, f V1,i ‚àà R d is d-dim plain representations of the i-th neighbor view from the observation encoder, and m V1 ‚àà R d .The pooling function can be either max or mean pooling along the number of features.</p>
<p>It is worth noting that obtaining f V1,i does not involve extra computation since these features have been calculated in offline feature extraction.The agent selects its right neighbor to navigate, and at the end of t=2, the visited node is added to
G Y by V (2) Y ={V 1 , V 2 }, E(2)
Y ={e 12 }, with node feature m V2 calculated similarly as Eq. ( 1).We set all edges e jk =1.While following the first instruction, the agent updates its episodic scene memory G Y accordingly, i.e., the green nodes and edges in Figs.3b and 3c.At the end of t = 5, V</p>
<p>(5)
Y = {V 1 , V 2 , ..., V 5 }, E(5)
Y = {e 12 , e 23 , e 34 , e 45 }.When the agent is directed to the second instruction in Scene Y , its memory in previous visits is preserved in G Y and is updated at the end of each time step accordingly as Figs.3d and 3e demonstrate.In Fig. 3f, since the agent's location A has been added to ESceme in conducting the first instruction, there is no update to G Y .The agent stores episodic memory for each scene separately in similar ways.Therefore, we omit the subscript Y for simplicity.</p>
<p>ESceme navigation by candidate enhancing</p>
<p>In addition to information from instruction, current observation, and route history, an agent can refer to its episodic scene memory in decision-making at each step.</p>
<p>Since the node representation in ESceme integrates information within the neighborhood, it is expected to envision the agent with a bigger picture of the current location.Therefore, we devise a candidate-enhancing (CE) mechanism to improve navigation.A flowchart of CE is shown in Fig. 2. Faced with K candidate views at Step t, the agent retrieves their representations m k , k ‚àà {1, ..., K} from episodic memory G (t‚àí1) ,
m k = m Vj if the k-th view is V j ‚àà V (t‚àí1) 0 otherwise. (2)
Then the Fusion block integrates the ESceme representations with the plain features {f 1 , ..., f K } to produce enhanced candidate viewpoints,
o k = MLP([m k ; f k ]),(3)
where
P (a t =k‚àà{1, ..., K, s}) = e MLP(o ‚Ä≤ k ‚äôx ‚Ä≤ cls ) j‚àà{1,...,K,s} e MLP(o ‚Ä≤ j ‚äôx ‚Ä≤ cls ) ,(4)
where ‚äô is element-wise multiplication of two vectors o ‚Ä≤ k and x ‚Ä≤ cls ‚àà R d , and the two-layer non-linear MLP block maps the result to a scalar ‚àà R. Following [50,10], we train the framework end-to-end by a mixture of Imitation Learning and Reinforcement Learning (A2C [43]) loss,
L = ‚àíŒ± T * t=1 log P (a t =a * t ) ‚àí T t=1 log P (√£ t )(r t ‚àív t ),(5)
where T * and T are the length of the annotated route and predicted path, respectively.√£t is sampled action.r t is the discount reward, and v t is the state value given by a twolayer (MLP) critic network.</p>
<p>4 Experiments</p>
<p>Experimental setup</p>
<p>Datasets and metrics.We conduct experiments on the following three VLN tasks for evaluation.</p>
<p>(1) Short-horizon with fine-grained instructions.R2R1 [4] constructs on Matterport3D [6] and has 7,189 direct-to-goal trajectories with an average of 10m.Each path is associated with three instructions of 29 words on average.The train, val seen, val unseen, and test unseen splits include 61, 56, 11, and 18 houses, respectively.</p>
<p>(2) Long-horizon with fine-grained instructions.R4R2 [26] is generated by joining existing trajectories in R2R with others that start close by where they end.Compared to R2R, it has longer paths and instructions and reduced shortestpath bias.The train, val seen, and val unseen have 233,613, 1,035, and 45,162 samples, respectively.</p>
<p>(3) Vision-dialog navigation.CVDN3 [26] requires an agent to navigate given a target object and a dialog history.It has 7k trajectories and 2,050 navigation dialogs, where the paths and language contexts are also longer than those in R2R.The train, val seen, val unseen, and test splits contain 4,742, 382, 907, and 1,384 instances, respectively.Following standard criteria [10,4,3], we evaluate the R2R dataset with Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), and Success weighted by Path Length (SPL).TL is the average length of an agent's navigation route in meters, NE is the mean shortest path distance between the agent's stop location and the target, and SR measures the ratio of navigation that stops less than three meters from the goal.SPL normalizes SR by the ratio between the path length of ground truth and the navigated, which balances accuracy and efficiency and becomes the key metric for the R2R dataset.We adopt three additional metrics, Coverage weighted by Length Score (CLS), normalized Dynamic Time Warping (nDTW), and Success weighted by nDTW (SDTW), to assess path fidelity on the R4R dataset.As for vision-dialog navigation on CVDN, the primary evaluation metric is Goal Progress (GP) in meters.Implementation details.We adopt the encoders from [10] in comparison by default, where the text, history, and crossmodal encoders have nine, two, and four transformer layers, respectively.Features of single views are extracted offline using finetuned ViT-B/16 released by [10].For a fair comparison, we set the feature dimension d=768, the ratio of imitation learning loss Œ±=0.2, and train the ESceme framework for 100K iterations on each dataset with a batch size of 8 and a learning rate of 1e-5.All the experiments run on a single NVIDIA V100 GPU.We adopt max pooling and single-run by default in comparison with other methods, and provide the results of mean pooling and inferring twice in ablation studies and supplementary material, with qualitative examples and failure cases included.</p>
<p>For Reinforcement Learning, the action space is restricted to navigable locations (loosely equal to viewpoints) from each node, which is implemented by first predicting the log probability distribution over all the K viewpoints plus a STOP token and then setting the non-navigable ones as -inf.The policy is given by œÄ
(a t |{o ‚Ä≤ i } s 1 , {h ‚Ä≤ i } t‚àí1 1 , {x ‚Ä≤ i } L cls
), and sampling is conducted according to the restricted log probability.For Imitation Learning, the shortest path planner provides expert demonstrations, which are directly available from the simulator.Fair comparison.We consider deploying an agent in new environments to execute a series of language instructions.Admittedly, this definition is slightly different from existing methods, whereas it 1) preserves the original setup of unseen environment, i.e., the agent never sees the environment before deployment, and 2) is more practical in real scenarios, e.g., housework robots.Meanwhile, the proposed episodic memory leads to initialization change: the agent conducts the first episode with empty memory and the following episodes with its own estimates.The comparisons we made in the paper aim to verify the superiority of the proposed episodic memory instead of just showing an instantiation of ESceme surpassing its counterparts.This way, we inevitably compare it with existing path memory since this is a novel memory mechanism.Our inter-episode memory requires no extra time or computation while maintaining partial episodic memory via initialization, which is worth further exploration.</p>
<p>The directly available location ID, which we use to retrieve enhanced features for the current node, is universally adopted by 1) implicit path-level memory methods (e.g., HAMT) to retrieve accessible candidates, and 2) explicit path-level memory methods (e.g., EGP, SSM) to extend ac-tion space.Our comparisons introduce the essential signal, i.e., scan id as environment index, to be fair in showing the superiority of episodic memory over path-level memory, where unseen scenes refer to those never appearing in training/validation.In discrete environments, the usage of location ID inevitably leads to "the agent knowing the current location is exactly something it sees before".The proposed ESceme can easily extend to continuous settings by combining with waypoint prediction methods (e.g., CWP [24]) that surpass most semantic-map-based approaches.Moreover, the proposed episodic memory mechanism can transfer to continuous scenes by maintaining a global map via the widely studied visual SLAM.</p>
<p>Comparison to state-of-the-art</p>
<p>Results on R2R dataset.Table 1 compares the proposed ESceme with existing methods on the R2R dataset.We can see that the pretraining-finetuning paradigm (e.g., RecBERT [25], HAMT [10], ADAPT [37], CSAP [59], TDSTP [61]) largely improves the performance of VLN in unseen environments.ESceme achieves the highest SPL on the unseen splits.It surpasses the baseline model HAMT [10] by about 5% SPL on the validation and test unseen environments and even outperforms TDSTP [61] that involves auxiliary training tasks.Besides, ESceme brings a relative decrease of 6.4% and 4.1% in NE on validation and test unseen split, respectively.The results demonstrate the efficacy of episodic scene memory in generalization to unseen scenarios with short instructions.</p>
<p>We also compare with the most recent works, including VLN-SIG [32], KERM [36], and GridMM [58].The proxy pre-training task involved in VLN-SIG shows no advantage in unseen environments.KERM surpasses all the methods on the validation-seen split but drops much more heavily on unseen splits.GridMM achieves the highest SR and slightly lower SPL than ours in unseen scenarios yet takes a much longer trajectory length.Results on R4R dataset.We evaluate the proposed ESceme on the R4R dataset to examine if the generalization promotion is maintained in long-horizon navigation tasks.The results are listed in Table 2. Our ESceme outperforms existing state-of-the-art by a large margin, i.e., a relative improvement of 6.4% in SPL, 7.0% in CLS, 7.3% in nDTW, and 9.1% in SDTW.It indicates that ESceme improves not only navigation success but also path fidelity.Although good at carrying out short instructions, TDSTP [61] suffers a heavy drop in long-horizon navigation regarding path fidelity compared with its baseline model HAMT [10].It reveals that goal-related auxiliary tasks such as target prediction benefit reaching the target location but undermine the ability to follow instructions.Equipped with ESceme, an agent has a promoted ability to travel the expected route in long-horizon navigation.Besides, a consistent advantage of pretraining-based methods can be observed on this dataset.Results on CVND dataset.Table 3 compares ESceme with state-of-the-art methods on the vision-and-dialog navigation task.CVDN provides longer instructions and trajectories than R2R and more complicated instructions than R4R.The proposed ESceme achieves the best goal process in both seen and unseen scenarios and wins first place on the leaderboard.HAMT [10] shows an obvious advantage over other pretraining-based methods such as PREVA-LENT [21], and even surpasses those counterparts specially designed for vision-and-dialog navigation, e.g., CMN [64], VISITRON [48], and SCoA [63].Our ESceme brings a relative improvement of 20.7%, 5.7%, and 7.3% over the baseline HAMT [10] in val seen, val unseen, and test unseen environments, respectively.</p>
<p>Ablation studies &amp; analysis</p>
<p>Different ESceme constructions.We evaluate the effect of different pooling functions in Table 4. Candidate Enhancing with mean pooling brings a relative improvement of 2.3% in SPL for unseen navigation and behaves similarly in seen environments.Integrated with max pooling, Candidate Enhancing further boosts the performance in unseen environments, which produces a 3.8% relative increase compared to the HAMT [10] baseline.The results demonstrate the efficacy of the proposed Candidate Enhancing, which improves observation representations via direct injection and fusion, and max pooling, which preserves more distinguishable features of each view.Appendix A discusses a different implementation of the proposed episodic scene memory by Graph Encoding.Different navigation architectures &amp; inferring strategies.The proposed ESceme is devised to be model-agnostic and should be compatible with any navigation network that has an observation input.To validate this property, we build ESceme upon TDSTP [61] that achieves the highest SR on the R2R dataset and list the results in Table 5. ESceme improves navigation in both seen and unseen environments by 4.9% and 1.4% in SPL, respectively.</p>
<p>As introduced in Section 3.3, the agent starts with an empty episodic scene memory during inference, and the memory keeps updating.If we let the agent renew its memory thoroughly by going through all the episodes and then evaluate its navigation performance, it will have a much more complete episodic memory.We present the results of ESceme<em> in Table 5.We can see that the nearly completed memory further boosts the performance in unseen environments by 1.3% and 2.1% regarding SPL for ESceme upon HAMT [10] and TDSTP [61]    results demonstrate that an agent learns to assist navigation with partial and persistently updated episodic memory.The observation that the performance of ESceme</em> is only slightly better than that of ESceme has two sides.On the one hand, it indicates that the agent has learned to use the dynamically accumulative episodic memory instead of working until collecting the complete memory.On the other hand, the slight gain of ESceme* indicates possible bottlenecks in the encoder/cross-encoder architecture, the frozen vision encoder, and the scale of datasets.</p>
<p>More effects of the proposed episodic scene memory are present in Appendix B and F. Comparison with preexploration methods shows that ESceme* is more robust to unseen scenarios.Ablation on graph re-initialization verifies that episodic scene memory contributes to decisionmaking in both seen and unseen environments.The observation in the IVLN benchmark is consistent with our discussion in Section 2 and our experimental results in Section 4.2, and validates the superiority of the proposed ES-ceme.</p>
<p>Computational efficiency.We present model size, GPU usage, and time cost during inference on the R2R dataset in Table 5.Either upon HAMT [10] or TDSTP [61], the proposed ESceme brings about 1.0% extra parameters and memory occupation in GPU.In a single-run setting, ESceme slightly increases the computational time by 4.8% when built on top of HAMT.Compared with HAMT, the TDSTP baseline costs more time by 59.5% and GPU by 23.5%.Accordingly, our ESceme only raises the time cost by 3.8% and almost no extra GPU consumption.With better-completed memory, ESceme* further boosts navigation performance in new environments at the expense of double the time.We can see that ESceme achieves a good trade-off between efficiency and efficacy in a single run.The proposed episodic memory mechanism consumes marginal (‚â§ 0.1%) computation and parameters.For Ddim features, K nodes/scene, and N scenes, increased cost of space and parameters are about 3.81e ‚àí6 √óDKN and 1.14e ‚àí5 √óD 2 , respectively.</p>
<p>Order of executing instructions.Since ESceme learns with dynamically updated episodic memory while conducting instructions, the order of execution has little impact on overall performance.Table 6 lists navigating performance with shuffled episodes on the val unseen split in all the datasets, which indicates the stability of ESceme.</p>
<p>Success variation during inference.Fig. 4 compare SPL and CLS curves of different methods to visualize the variation of navigation quality in inferring progress.On the short-horizon navigation dataset R2R, HAMT [10] oscillates around 62 and drops in the last 1/5 progress.The decrease could result from more tough samples at the end.TDSTP [61] presents a more stable oscillation around 62, owing to a global action space and an auxiliary goal-related task.Starting from a moderate navigation ability, an agent with ESceme benefits greatly from memory updates and maintains a high success rate with completed memory.</p>
<p>On the long-horizon VLN dataset R4R, TDSTP [61] shares a similar oscillation around 41 with HAMT [10] in SPL.TDSTP preserves a relatively more stable success rate at the cost of much lower CLS, which reveals that goalrelated auxiliary task undermines the ability of instruction following.Our ESceme shows a sharp increase within the first 4/5 navigation and has remained stable since then.We attribute the excellent promotion on R4R to two reasons, 1) long-horizon navigation involves more action steps, so a slight increase in navigation ability results in a big difference in final performance; 2) the sample density of a scene from R4R is much higher than that from the R2R dataset.</p>
<p>Qualitative analysis</p>
<p>To intuitively demonstrate the benefit of the proposed episodic scene memory, we provide a visualization example in Fig. 6.It shows the panoramic views and top-down overviews of navigation.The last step of HAMT and TD-STP navigates to a visible corner of the bedroom.Instead, ESceme understands the instruction better.It takes a step to walk down to the end of the hall and then turns left to the bedroom.</p>
<p>A failure case of ESceme is shown in Fig. 11, where the instruction is "Leave sitting room and head towards the kitchen, turn right at living room and enter.Walk through living room to dining room and enter.Turn left and..." After correctly predicting the first three actions, ESceme failed to enter the dining room and got lost.It indicates that the representations for the viewpoints are not distinguishable enough to capture some fine-grained difference between the dining room and the living room.</p>
<p>Conclusion</p>
<p>In this paper, we devise the first VLN mechanism with episodic scene memory (ESceme) and propose a simple yet effective implementation via candidate enhancing.We show that an agent with ESceme improves navigation ability in short-horizon, long-horizon, and vision-and-dialog navigation.Our method outperforms the existing state-ofthe-art and wins first place in the CVDN leaderboard, bringing a marginal increase in memory, parameters, and inference time.We hope this work can inspire further explorations on episodic memory in VLN and related fields, e.g., building the memory in continuous environments and with more advanced techniques such as neural SLAM.Limitations.Although we show the effectiveness of the proposed episodic scene memory, there are still several limitations.First, the agent requires knowledge of environmental identity to build episodic memory for each scene.It is inevitable but supported by practical demands where an agent conducts multiple instructions in one scenario.Second, the "location ID" information is directly available from the simulator and the dataset, which is accurate and free of noise.For the case where location ID is unknown in advance, the episodic scene memory can be built by adding a discrete mapping process analogous to SLAM.No specific location ID is required, and the rough global position of each node can be dynamically estimated using the angle of each navigable viewpoint.Third, the architecture of a navigation agent and the training data limit the efficacy of a complete scene memory.We hope the proposed episodic scene mem-ory can be explored in more advanced and diverse architectures.</p>
<p>Appendix A: ESceme navigation by graph encoding</p>
<p>Intuitively, the memory can be injected into the crossmodal encoder via a separate branch.We denote the solution as Graph Encoding (GE) and list experimental results.Fig. 5 demonstrates ESceme-assisted navigation by adding a graph encoding (GE) branch to the cross-modal encoder.At the current location where the agent stands, a local window is masked to avoid repetition with the path history from time 1 to t‚àí1.Thus, the searched episodic memory graph includes six nodes and three edges, i.e., G (t‚àí1) = {V (t‚àí1) , E (t‚àí1) }.We adopt 3-WL GNNs [42,16] that can distinguish two non-isomorphic graphs to encode the memory graph, where the input G ‚àà R n√ón√ó(1+d) is given by
G ijk = Ô£± Ô£≤ Ô£≥ e ij if k = 1 m i if j = i for k &gt; 1 0 otherwise,(6)
where n is the number of nodes in V (t‚àí1) .m i ‚àà R d is the representation of the node V i , with detailed calculations presented in Section 3.2.e ij = 1 if V i and V j are connected, else e ij = 0.The graph is encoded by
G ‚Ä≤ = [(W 1 G) ‚äô (W 2 G); (W 3 G)],(7)
where W 1‚àº3 ‚ààR (1+d)√ó(d/2) are two-layer MLPs.‚äô denotes element-wise multiplication and [‚Ä¢; ‚Ä¢] is the concatenation along feature dimension.The final encoded feature to the cross-modal encoder is
n i=1 n j=1 G ‚Ä≤ ij ‚ààR d .
We evaluate the superiority of Candidate Enhancing over Graph Encoding and the effect of different pooling functions in Table 7. First, Graph Encoding with mean pooling slightly increases navigation success in seen environments with almost no promotion in unseen scenarios.We infer that Graph Encoding adjusts the representation of observations in cross-modal encoding and does not align well with the remaining branches to provide complementary information, resulting in a limited effect.</p>
<p>Appendix B: Effects of the episodic scene memory</p>
<p>We thoroughly compare navigating with progressively completed and nearly complete episodic memory on three datasets in Tables 8 and 9. ESceme conducts instructions in a single-run setting, where the agent dynamically updates memory in inference.ESceme<em> first goes through all the episodes to build a nearly complete memory at the beginning of the evaluation.ESceme</em> improves navigating in new environments by 1.6% (SPL) on test unseen split of the R2R dataset.As for vision-dialog navigation CVDN, the improvement in val unseen and test unseen is 5.5% and 3.0%, respectively.On the long-horizon navigation dataset R4R, the relative increase is about 0.5%.</p>
<p>Overall, ESceme<em> further promotes generalization to novel scenarios, indicating that ESceme benefits from the nearly complete scene memory.On the other hand, the small gap between ESceme and ESceme</em> shows that the agent has learned to utilize progressively completed memory in navigation.</p>
<p>Besides, Table 10 lists the comparison with preexploration methods.The pre-exploration methods achieve very competitive results on val seen split while suffering from a heavier drop in unseen environments.In Table 11, we test ESceme with the memory graph re-initialized at every episode.The results on the R2R dataset verify that the ESceme agent indeed benefits from episodic memory for decision-making in both seen and unseen environments.</p>
<p>Cross-Modal Encoder
Observation Encoder History Encoder Instruction Encoder {ùíêùíê‚Ä≤1 , ‚Ä¶ , ùíêùíê‚Ä≤ ùêæùêæ , ùíêùíê‚Ä≤ ùë†ùë† } {ùíôùíô ‚Ä≤ ùëêùëêùëêùëêùë†ùë† , ùíôùíô ‚Ä≤ 1 ‚Ä¶ , ùíôùíô‚Ä≤ ùêøùêø } {ùíâùíâ ‚Ä≤ 1 , ‚Ä¶ , ùíâùíâ ‚Ä≤ ùë°ùë°‚àí1 } Predictor ùëéùëé ùë°ùë°</p>
<p>Graph Encoder</p>
<p>Episodic memory of the current scene:</p>
<p>HAMT TDSTP ESceme</p>
<p>Instruction 1 :
1
Walk toward and past the ping pong table.Continue in that direction until you reach the sofa with two green pillows.Stop in front of the table that is in front of that sofa.Instruction 2: Walk through the office past the work tables.Go to the right and stop next to the white bookshelf outside the conference room.</p>
<p>Figure 1 .
1
Figure1.The blue trajectory shows an agent carrying out instruction 1.The next time, the agent enters this scene to conduct the second instruction along the red path.ESceme allows it to recall the visited nodes (i.e., the blue ones) at where it is standing (A) and choose the neighboring node B1 that will see "the white bookshelf" in one more step at C. Finally, it navigates towards the red dash route and reaches the target.</p>
<p>Figure 2 .
2
Figure2.An overview of the Episodic Scene memory mechanism for VLN.On the left is partial episodic memory for the current scene, which gets updated in navigation 1) following the previous instruction, i.e., the blue route, and 2) following the current instruction from Step 1 to t ‚àí 1, i.e., the solid red trajectory.The cyan nodes are those viewed but not visited.The shadow box shows the memory of node B1, which has six adjacent neighbors, i.e., A, B2, B5, C, D, and E. The integration of these nodes consists of the memory of B1.At Step t, the agent stands at Node A and is expected to choose one node from B1 to B5.Given observation from K views, each view retrieves its memory in ESceme and produces {m1, ..., mK }.The memory representation then fuses with original encoded observations, which yields {o1, ..., oK , os}.os is the representation for STOP.The enhanced observations, instruction text, and history from Step 1 to t ‚àí 1 compose the input to a navigation network to predict the action at = i ‚àà {1, ..., K, s}.Generally, a navigation network uses the encoded features of the original K views as the input to the cross-modal encoder, i.e., the output 1 ‚Éù.Our ESceme exploits the enhanced observations from 2 ‚Éù.</p>
<p>4 Figure 3 .
43
Figure 3. Episodic memory construction of a scene during navigation.ESceme at the beginning of each time step is presented in the figures, which comprises green nodes and edges and is empty at the beginning of t = 1.The blue nodes indicate the current location of following the first instruction at each time step, and the red ones correspond to the second instruction.The small cyan nodes mark the remaining navigable viewpoints of the current location.Nodes with green boundary are the chosen viewpoints in each time step.ESceme at the end of that time step is updated by the node with green boundary and the dashed lines connected to its existing nodes.Please refer to Fig. 1 for a complete global graph of the scene, which is unavailable to the agent either in navigation or ESceme construction.</p>
<p>CLS on R4R val unseen split</p>
<p>Figure 4 .
4
Figure 4. Navigation quality w.r.t.inferring progress.The x-axis indicates the ratio of samples tested, and the y-axis is the smoothed average of SPL or CLS.We use the default order for all the methods.Navigation with ESceme improves over time.</p>
<p>‚ãÆFigure 5 .
5
Figure 5.An overview of ESceme-assisted navigation by graph encoding.First, Episodic memory is built in the same way as that for candidate enhancing (c.f.Section 3.2).Then, the agent searches the episodic memory for the current viewpoint and obtains the memory graph by masking a local window.The encoded memory composes a separate branch to the cross-modal encoder.</p>
<p>Figure 6 .
6
Figure 6.Panoramic views and top-down overviews of navigation.Mistakes during navigation are marked with red boxes for panorama and red arrows for top-down trajectories.The star indicates the target location.Our ESceme strictly follows the instruction "walk down to the end of hall" and waits at the door of the bedroom.</p>
<p>Figure 7 .Figure 8 .
78
Figure 7. Panoramic views and top-down overviews of navigation.Mistakes during navigation are marked with red boxes for panorama and red arrows for top-down trajectories.The star indicates the target location.Our ESceme strictly follows the instruction "go up two steps" and waits on the third step.</p>
<p>Figure 9 .
9
Figure 9.The top-down trajectory of navigation.Mistakes during navigation are marked with red.The star indicates the target location.Our ESceme moves forward to the right place and then back and arrives at the second bathroom.</p>
<p>Instruction:Figure 10 .
10
Figure 10.The top-down trajectory of navigation.Mistakes during navigation are marked with red.The star indicates the target location.Our ESceme moves forward to the right place and then back and stops inside the double doors.</p>
<p>Figure 11 .
11
Figure 11.Failure case in R2R val unseen split.The instruction is "Leave sitting room and head towards the kitchen, turn right at living room and enter.Walk through living room to dining room and enter.Turn left and head to front door.Exit the house and stop on porch."After correctly predicting the first three actions, ESceme failed to enter the dining room and got lost.</p>
<p>[10,61] d .Following[10,61], type embedding that distinguishes visual and linguistic signals, navigable embedding that indicates the navigability of each candidate view, and orientation encoding are added to o k .A zero vector o s ‚àà R d is appended as the feature for STOP action.Finally, together with encoded history features, the enhanced candidate representations {o 1 , ..., o K , o s } are input to the cross-modal encoder to merge linguistic information from encoded text features.The agent predicts the distribution of action a t via a two-layer non-linear Predictor block,</p>
<p>[‚Ä¢; ‚Ä¢] denotes concatenation along feature dimension.The MLP function is a two-layer non-linear projection from R</p>
<p>Table 1 .
1
[10]spectively.More results of ESceme* are in supplementary material, with slighter improvements observed for longer-horizon navigation.The Comparison with state-of-the-art methods on R2R dataset.ESceme (Ours) is built with HAMT[10]architecture by default.
MethodTLValidation Seen NE‚Üì SR‚Üë SPL‚ÜëTLValidation Unseen NE‚Üì SR‚Üë SPL‚ÜëTLTest Unseen NE‚Üì SR‚Üë SPL‚ÜëSeq2Seq [4]11.33 6.0139-8.397.8122-8.137.8520SF [17]-3.3666--6.6235-14.82 6.6235AcPercep [54]19.73.20705220.64.36584021.64.3360PRESS [35]10.57 4.39585510.36 5.28494510.77 5.4949SSM [53]14.73.10716220.74.32624520.44.5761EnvDrop [50]11.00 3.99625910.70 5.22524811.66 5.2351OAAM [44]10.20-65629.95-545010.40-53AuxRN [62]-3.337067-5.285550-5.1555PREVALENT [21] 10.32 3.67696510.19 4.71585310.51 5.3054RelGraph [23]10.13 3.4767659.994.73575310.29 4.7555NvEM [1]11.09 3.44696511.83 4.27605512.98 4.3758NvEM+SEvol [8]11.97 3.56676312.26 3.99625713.40 4.1362CSAP [59]11.29 2.80747012.59 3.72655913.30 4.0662RecBERT [25]11.13 2.90726812.01 3.93635712.35 4.0963ADAPT [37]11.39 2.70746912.33 3.66665913.16 4.1163HOP [46]11.26 2.72757012.27 3.80645712.68 3.8364TDSTP [61]-2.347773-3.227063-3.7367HAMT [10]11.15 2.51767211.46 3.62666112.27 3.9365VLN-SIG [32]------6862--65KERM [36]12.16 2.19807413.54 3.22726114.60 3.6170GridMM [58]----13.27 2.83756414.43 3.3573ESceme (Ours)10.65 2.57767310.80 3.39686411.89 3.7766</p>
<p>Table 2 .
2
Comparison on the val unseen split of R4R dataset.
MethodNE‚Üì SR‚Üë SPL‚Üë CLS‚ÜënDTW‚ÜëSDTW‚ÜëSF [17]8.47 24 12 30--EnvDrop [50]-29 -34-9PTA [13]8.25 24 10 373210RCM [55]8.08 26 21 353013SSM [53]8.27 32 -533919NvEM [1]6.85 38 28 413620NvEM+SEvol [8] 6.90 39 29 413620RelGraph [23] 7.43 36 26 414734EGP [15]8.00 30.2 -44.4 37.417.5TDSTP [61]6.32 43.3 40.6 46.4 42.125.5RecBERT [25] 6.67 43.6 -51.4 45.129.9CSAP [59]6.21 43.0 -58.6 51.931.5HAMT [10]6.09 44.6 40.6 57.7 50.331.8ESceme (Ours) 5.84 45.6 43.2 62.7 55.734.7MethodVal SeenVal UnseenTest UnseenSeq2Seq [4]5.922.102.35PREVALENT [21]-3.152.44CMN [64]7.052.972.95VISITRON [48]5.113.253.11HOP [46]-4.413.24SCoA [63]7.112.853.31MT-RCM+EnvAg [57] 5.074.653.91HAMT [10]6.915.135.58ESceme (Ours)8.345.425.99</p>
<p>Table 3 .
3
Resuls of Goal Process (GP) in meters on CVDN dataset.</p>
<p>Table 4 .
4
34¬±0.10 2.52¬±0.1075.0¬±0.971.7¬±0.7 11.72¬±0.343.63¬±0.0565.7¬±0.7 60.9¬±0.7 ESceme mean 11.13¬±0.162.59¬±0.0975.1¬±0.7 71.9¬±0.6 11.49¬±0.273.50¬±0.0367.1¬±0.5 62.3¬±0.5 ESceme max 10.81¬±0.122.60¬±0.1275.6¬±0.472.6¬±0.4 11.18¬±0.23 3.44¬±0.0367.4¬±0.5 63.2¬±0.5 Ablation studies of ESceme construction on R2R dataset to compare the effect of different pooling functions.
Validation SeenValidation UnseenpoolingTLNE‚ÜìSR‚ÜëSPL‚ÜëTLNE‚ÜìSR‚ÜëSPL‚ÜëHAMT [10] Method-TL11.Validation Seen NE‚ÜìSPL‚ÜëTLValidation Unseen NE‚ÜìSPL‚ÜëParams GPU Time (MB) (GB) (ms)HAMT [10]11.02¬±0.10 2.52¬±0.10 71.7¬±0.7 11.72¬±0.34 3.63¬±0.05 60.9¬±0.7651.58.529.4+ ESceme10.81¬±0.12 2.60¬±0.12 72.6¬±0.4 11.18¬±0.23 3.44¬±0.03 63.2¬±0.5+6.8+0.1+1.4+ ESceme<em> 10.77¬±0.13 2.58¬±0.12 72.8¬±0.4 10.89¬±0.14 3.35¬±0.05 64.0¬±0.4+6.8+0.1 +32.2TDSTP [61]13.09¬±0.37 2.42¬±0.08 70.9¬±0.7 14.28¬±0.44 3.22¬±0.09 62.1¬±0.6657.210.546.9+ ESceme11.80¬±0.26 2.34¬±0.10 74.4¬±0.6 13.86¬±0.21 3.31¬±0.07 63.0¬±0.8+6.8+0.1+1.8+ ESceme</em> 11.83¬±0.23 2.33¬±0.08 74.8¬±0.7 13.38¬±0.28 3.21¬±0.05 64.3¬±0.7+6.8+0.1 +50.5</p>
<p>Table 5 .
5
Ablation studies of navigation architectures and inferring strategies on R2R dataset.ESceme* denotes navigating with a nearly completed scene memory by first going through all the episodes.For a scene in R2R covering 92 visited nodes on average, the maximum episodic memory cost in CPU is about 1.5MB.
(R2R) NE‚Üì SPL‚Üë (R4R) SPL‚Üë CLS‚Üë (CVDN) GP‚Üë3.39¬±0.03 63.7¬±0.3 43.2¬±0.07 62.7¬±0.1 5.57¬±0.11</p>
<p>Table 6 .
6
x ¬± œÉ scores of shuffled episodes with five random seeds on the val unseen split of the datasets.</p>
<p>Table 7 .
7
Ablation studies of ESceme construction on R2R dataset.We compare the effect of graph encoding (GE) and candidate enhancing (CE), and different pooling functions.
Validation SeenValidation UnseenGE CE poolingTLNE‚ÜìSR‚ÜëSPL‚ÜëTLNE‚ÜìSR‚ÜëSPL‚ÜëHAMT [10] ---11.02¬±0.10 2.52¬±0.10 75.0¬±0.9 71.7¬±0.7 11.72¬±0.34 3.63¬±0.05 65.7¬±0.7 60.9¬±0.7ESceme‚úì ‚úómean 11.20¬±0.18 2.56¬±0.11 75.7¬±0.9 72.3¬±0.6 11.64¬±0.05 3.60¬±0.06 65.9¬±0.5 60.9¬±0.6ESceme‚úó ‚úìmean 11.13¬±0.16 2.59¬±0.09 75.1¬±0.7 71.9¬±0.6 11.49¬±0.27 3.50¬±0.03 67.1¬±0.5 62.3¬±0.5ESceme‚úó ‚úìmax10.81¬±0.12 2.60¬±0.12 75.6¬±0.4 72.6¬±0.4 11.18¬±0.23 3.44¬±0.03 67.4¬±0.5 63.2¬±0.5
https://github.com/peteanderson80/ Matterport3DSimulator
https://github.com/google-research/ google-research/tree/master/r4r
https://github.com/mmurray/cvdn/
https://github.com/google-research-datasets/ RxR
Appendix C: Experiments on RxR datasetResults on RxR 4[31]inTable 12indicate that longer instructions and trajectories add sufficient knowledge to the proposed episodic memory to overcome coreference challenge and promote navigation.Appendix D: Pseudo-code implementationWe provide the pseudo-code of ESceme construction and candidate enhancing in Algorithm 1. ESceme requires easy implementation and can be integrated with any navigation networks that encode the observation.Appendix E: Qualitative examples and failure casesWe present the navigating process to provide a more intuitive comparison with HAMT[10]and TDSTP[61].All the examples are tested in unseen environments.For short-horizon navigation, our ESceme outperforms its counterparts regarding stopping precision.For long-horizon navigation, our ESceme shows an improved ability to follow instructions that require a forward and back trip and arrives at the target location.We attribute these advantages to the episodic memory of the scenes.Appendix F: Comparison with IVLNOur setting is identical to IVLN, which reorganizes episodes into tours.Table13is the direct comparison using the IVLN benchmark.IVLN decreases the performance in both seen and unseen environments, yet our ESceme promotes navigation in unseen scenarios while maintaining the performance in seen ones.The conclusion is consistent with our observations in Section 4.
Neighbor-view enhanced model for vision and language navigation. D An, Y Qi, Y Huang, Q Wu, L Wang, T Tan, ACMMM. 2021</p>
<p>D An, H Wang, W Wang, Z Wang, Y Huang, K He, L Wang, arXiv:2304.03047Etpnav: Evolving topological planning for vision-language navigation in continuous environments. 2023arXiv preprint</p>
<p>On evaluation of embodied navigation agents. P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.067572018arXiv preprint</p>
<p>ground-truth trajectory (b) trajectory predicted by ESceme Figure 12. Failure case in R2R val unseen split. The instruction is "Go down the stairs. Go into the room straight ahead on the slight left. Wait there. ESceme succeeded in going downstairs but failed to determine the slight left direction and entered the wrong room</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N S√ºnderhauf, I Reid, S Gould, A Van Den, Hengel, CVPR. 2018</p>
<p>Vqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, ICCV. 2015</p>
<p>Matterport3d: Learning from rgb-d data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, International Conference on 3D Vision. 2017</p>
<p>Neural topological slam for visual navigation. D S Chaplot, R Salakhutdinov, A Gupta, S Gupta, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Reinforced structured state-evolution for vision-language navigation. J Chen, C Gao, E Meng, Q Zhang, S Liu, CVPR. 2022</p>
<p>Topological planning with transformers for vision-and-language navigation. K Chen, J K Chen, J Chuang, M V√°zquez, S Savarese, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>History aware multimodal transformer for vision-andlanguage navigation. S Chen, P.-L Guhur, C Schmid, I Laptev, NeurIPS. 202134</p>
<p>Think global, act local: Dual-scale graph transformer for vision-and-language navigation. S Chen, P.-L Guhur, M Tapaswi, C Schmid, I Laptev, CVPR. 2022</p>
<p>X Chen, H Fang, T.-Y Lin, R Vedantam, S Gupta, P Doll√°r, C L Zitnick, arXiv:1504.00325Microsoft coco captions: Data collection and evaluation server. 2015arXiv preprint</p>
<p>Perceive, transform, and act: Multi-modal attention networks for vision-and-language navigation. F L L B M Cornia, M C R Cucchiara, arXiv:1911.123772019arXiv preprint</p>
<p>Episodic memory question answering. S Datta, S Dharur, V Cartillier, R Desai, M Khanna, D Batra, D Parikh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Evolving graphical planner: Contextual global planning for vision-and-language navigation. Z Deng, K Narasimhan, O Russakovsky, In NeurIPS. 332020</p>
<p>V P Dwivedi, C K Joshi, T Laurent, Y Bengio, X Bresson, arXiv:2003.00982Benchmarking graph neural networks. 2020arXiv preprint</p>
<p>Speaker-follower models for visionand-language navigation. D Fried, R Hu, V Cirik, A Rohrbach, J Andreas, L.-P Morency, T Berg-Kirkpatrick, K Saenko, D Klein, T Darrell, NeurIPS201831</p>
<p>Crossmodal map learning for vision and language navigation. G Georgakis, K Schmeckpeper, K Wanchoo, S Dan, E Miltsakaki, D Roth, K Daniilidis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Airbert: In-domain pretraining for visionand-language navigation. P.-L Guhur, M Tapaswi, S Chen, I Laptev, C Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Cognitive mapping and planning for visual navigation. S Gupta, J Davidson, S Levine, R Sukthankar, J Malik, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Towards learning a generic agent for vision-and-language navigation via pre-training. W Hao, C Li, X Li, L Carin, J Gao, CVPR. 2020</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997</p>
<p>Language and visual entity relationship graph for agent navigation. Y Hong, C Rodriguez, Y Qi, Q Wu, S Gould, NeurIPS. 202033</p>
<p>Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation. Y Hong, Z Wang, Q Wu, S Gould, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Vln bert: A recurrent vision-and-language bert for navigation. Y Hong, Q Wu, Y Qi, C Rodriguez-Opazo, S Gould, CVPR. 2021</p>
<p>Stay on the path: Instruction fidelity in vision-and-language navigation. V Jain, G Magalhaes, A Ku, A Vaswani, E Ie, J Baldridge, ACL. 2019</p>
<p>Tactical rewind: Self-correction via backtracking in visionand-language navigation. L Ke, X Li, Y Bisk, A Holtzman, Z Gan, J Liu, J Gao, Y Choi, S Srinivasa, CVPR. 2019</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J D , M.-W C Kenton, L K Toutanova, NAACL. 2019</p>
<p>Iterative vision-andlanguage navigation. J Krantz, S Banerjee, W Zhu, J Corso, P Anderson, S Lee, J Thomason, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Room-across-room: Multilingual visionand-language navigation with dense spatiotemporal grounding. A Ku, P Anderson, R Patel, E Ie, J Baldridge, EMNLP. 2020</p>
<p>Room-Across-Room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. A Ku, P Anderson, R Patel, E Ie, J Baldridge, Conference on Empirical Methods for Natural Language Processing (EMNLP). 2020</p>
<p>Improving vision-and-language navigation by generating future-view image semantics. J Li, M Bansal, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Envedit: Environment editing for vision-and-language navigation. J Li, H Tan, M Bansal, CVPR. 2022</p>
<p>Layout-aware dreamer for embodied referring expression grounding. M Li, Z Wang, T Tuytelaars, M.-F Moens, arXiv:2212.001712022arXiv preprint</p>
<p>Robust navigation with language pretraining and stochastic sampling. X Li, C Li, Q Xia, Y Bisk, A Celikyilmaz, J Gao, N Smith, Y Choi, EMNLP-IJCNLP. 2019</p>
<p>Kerm: Knowledge enhanced reasoning for vision-andlanguage navigation. X Li, Z Wang, J Yang, Y Wang, S Jiang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Adapt: Vision-language navigation with modality-aligned action prompts. B Lin, Y Zhu, Z Chen, X Liang, J Liu, X Liang, CVPR. 2022</p>
<p>Multimodal transformer with variable-length memory for vision-and-language navigation. C Lin, Y Jiang, J Cai, L Qu, G Haffari, Z Yuan, ECCV. 2022</p>
<p>Self-monitoring navigation agent via auxiliary progress estimation. C.-Y Ma, J Lu, Z Wu, G Alregib, Z Kira, R Socher, C Xiong, ICLR2019</p>
<p>The regretful agent: Heuristic-aided navigation through progress estimation. C.-Y Ma, Z Wu, G Alregib, C Xiong, Z Kira, CVPR. 2019</p>
<p>Improving vision-andlanguage navigation with image-text pairs from the web. A Majumdar, A Shrivastava, S Lee, P Anderson, D Parikh, D Batra, ECCV. Springer2020</p>
<p>Lipman. Provably powerful graph networks. H Maron, H Ben-Hamu, H Serviansky, Y , NeurIPS. 322019</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, ICML. PMLR2016</p>
<p>Object-and-action aware model for visual language navigation. Y Qi, Z Pan, S Zhang, A V D Hengel, Q Wu, ECCV. Springer2020</p>
<p>Reverie: Remote embodied visual referring expression in real indoor environments. Y Qi, Q Wu, P Anderson, X Wang, W Y Wang, C Shen, A V D Hengel, CVPR. 2020</p>
<p>Hop: History-and-order aware pre-training for visionand-language navigation. Y Qiao, Y Qi, Y Hong, Z Yu, P Wang, Q Wu, CVPR. 2022</p>
<p>Rapid exploration for open-world navigation with latent goal models. D Shah, B Eysenbach, N Rhinehart, S Levine, Conference on Robot Learning. PMLR2022</p>
<p>Visitron: Visual semantics-aligned interactively trained object-navigator. A Shrivastava, K Gopalakrishnan, Y Liu, R Piramuthu, G T√ºr, D Parikh, D Hakkani-Tur, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>The development of embodied cognition: Six lessons from babies. L Smith, M Gasser, Artificial life. 111-22005</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. H Tan, L Yu, M Bansal, NAACL. 2019</p>
<p>Vision-and-dialog navigation. J Thomason, M Murray, M Cakmak, L Zettlemoyer, Conference on Robot Learning. PMLR2020</p>
<p>Talk2nav: Long-range vision-and-language navigation with dual attention and spatial memory. A B Vasudevan, D Dai, L Van Gool, International Journal of Computer Vision. 1292021</p>
<p>Structured scene memory for vision-language navigation. H Wang, W Wang, W Liang, C Xiong, J Shen, CVPR. 2021</p>
<p>Active visual information gathering for vision-language navigation. H Wang, W Wang, T Shu, W Liang, J Shen, ECCV. Springer2020</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. X Wang, Q Huang, A Celikyilmaz, J Gao, D Shen, Y.-F Wang, W Y Wang, L Zhang, CVPR. 2019</p>
<p>Look before you leap: Bridging model-free and model-based reinforcement learning for plannedahead vision-and-language navigation. X Wang, W Xiong, H Wang, W Y Wang, ECCV. 2018</p>
<p>Environment-agnostic multitask learning for natural language grounded navigation. X E Wang, V Jain, E Ie, W Y Wang, Z Kozareva, S Ravi, ECCV. Springer2020</p>
<p>Gridmm: Grid memory map for vision-and-language navigation. Z Wang, X Li, J Yang, Y Liu, S Jiang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Cross-modal semantic alignment pre-training for vision-and-language navigation. S Wu, X Fu, F Wu, Z.-J Zha, ACMMM. 2022</p>
<p>Msr-vtt: A large video description dataset for bridging video and language. J Xu, T Mei, T Yao, Y Rui, CVPR. 2016</p>
<p>Target-driven structured transformer planner for vision-language navigation. Y Zhao, J Chen, C Gao, W Wang, L Yang, H Ren, H Xia, S Liu, ACMMM. 2022</p>
<p>Visionlanguage navigation with self-supervised auxiliary reasoning tasks. F Zhu, Y Zhu, X Chang, X Liang, CVPR. 2020</p>
<p>Self-motivated communication agent for realworld vision-dialog navigation. Y Zhu, Y Weng, F Zhu, X Liang, Q Ye, Y Lu, J Jiao, ICCV. 2021</p>
<p>Vision-dialog navigation by exploring cross-modal memory. Y Zhu, F Zhu, Z Zhan, B Lin, J Jiao, X Chang, X Liang, CVPR. 2020</p>            </div>
        </div>

    </div>
</body>
</html>
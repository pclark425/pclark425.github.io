<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5609 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5609</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5609</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-117.html">extraction-schema-117</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-268358242</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.05973v1.pdf" target="_blank">Calibrating Large Language Models Using Their Generations Only</a></p>
                <p><strong>Paper Abstract:</strong> As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge. We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively in terms of calibration error for white-box and black-box LLMs on closed-book question-answering to detect incorrect LLM answers.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5609",
    "paper_id": "paper-268358242",
    "extraction_schema_id": "extraction-schema-117",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00452525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Calibrating Large Language Models Using Their Generations Only</p>
<p>Dennis Ulmer dennis.ulmer@mailbox.org 
Parameter Lab</p>
<p>IT University of Copenhagen</p>
<p>Pioneer Centre for Artificial Intelligence</p>
<p>Martin Gubri 
Parameter Lab</p>
<p>Hwaran Lee 
NAVER AI Lab</p>
<p>Sangdoo Yun 
NAVER AI Lab</p>
<p>Seong Joon Oh 
Parameter Lab</p>
<p>University of Tübingen</p>
<p>Tübingen AI Center</p>
<p>Calibrating Large Language Models Using Their Generations Only
3B139302C70CB3257119CDD6C291209D
As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important.However, finding effective ways to calibrate LLMsespecially when the only interface to the models is their generated text-remains a challenge.We propose APRICOT (Auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone.This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence.We show how our approach performs competitively in terms of calibration error for white-box and blackbox LLMs on closed-book question-answering to detect incorrect LLM answers.</p>
<p>Introduction</p>
<p>When given a case description of "A man superglued his face to a piano and he says it's making it hard to get a full night of sleep", a recently released medical LLM was found to list unrelated potential causes in its diagnosis, including narcolepsy, sleep apnea and others. 1 This, of course, ignores the seemingly obvious reason for the patient's complaints.While humorous, this example illustrates the pitfalls of practical LLM applications: Despite often looking convincing on the surfaceespecially to non-experts-model responses can be wrong or unreliable, leading to potentially harmful outcomes or a loss of trust in the system, foregoing its benefits.Indeed, consistent behavior (imagine e.g.reliably indicating a lack of confidence for unsure responses) has been argued as one way to build trust in automated systems (Jacovi et al., 2021), while misleading predictions have been empirically shown to lead to a loss of trust that can be hard to recover from (Dhuliawala et al., 2023).</p>
<p>We introduce APRICOT , a method to use an auxiliary model to infer a LLM's confidence in an open-ended question-answering setting.The auxiliary model does so based on the given input to and generated output text from the LLM alone.The model is trained by using these two parts as input and predicting calibration targets.The latter are obtained without access to the LLM's sequence likelihoods or internal states by clustering input representations produced by an additional embedding model, and thus only require black-box access.This especially relevant since an increasing number of LLM providers safeguard their model behind black-box APIs.This approach is conceptually straightforward, easy to optimize, and opens up a large number of possible applications, for instance, verbalizing uncertainty-a recently popular way to communicate uncertainty for LLMs (Lin et al., 2022;Xiong et al., 2023)-or adjusting a model's response with linguistic markers of confidence.</p>
<p>Our contributions are as follows: We propose to arXiv:2403.05973v1 [cs.CL] 9 Mar 2024 obtain calibration targets without requiring any additional information about LLM internals or question metadata.We show that using auxiliary models on the target LLM's input and output is sufficient to predict a useful notion of confidence.We also perform additional studies to identify which parts of the LLM's output are most useful to predict confidence.All the code is openly available.2</p>
<p>Related Work</p>
<p>Trustworthiness in ML systems.Trustworthiness has been identified as a key challenge to exploit the potential applications of automated systems (Marcus, 2020;Goldblum et al., 2023).This trait is seen as especially under risk in the presence of out-of-distribution examples or distributional shift (Ovadia et al., 2019;D'Amour et al., 2022).</p>
<p>As illustrated by the introductory example, addressing these problems is paramount in high-stakes domains such as healthcare (He et al., 2019;Ulmer et al., 2020;van der Meijden et al., 2023), analyzing asylum cases (Nalbandian, 2022), or legal deliberations (Chalkidis, 2023;Dahl et al., 2024).Jacovi et al. (2021) argue that trust in automated systems can for instance be built extrinsically, e.g. through consistent and predictable behavior.One such a way is uncertainty quantification, i.e. by supplying a user with scores reflecting the reliability of a model prediction (Bhatt et al., 2021;Liao and Sundar, 2022;Hasan et al., 2023).However, the lab experiments of Dhuliawala et al. (2023) also demonstrate the flip side of this: When exposing human participants to unreliable confidence estimates, they measure a decrease in trust and participant outcomes alike.Therefore, a method like ours can help to build trust in LLMs and pave the way for reaping their benefits.</p>
<p>Uncertainty Quantification for LLMs.While methods for predictive uncertainty quantification have already been explored in NLP for classification (Ulmer et al., 2022a;Van Landeghem et al., 2022;Vazhentsev et al., 2023), regression (Beck et al., 2016;Glushkova et al., 2021;Zerva et al., 2022a) and language generation tasks (Xiao et al., 2020;Malinin and Gales, 2021), their application to LLMs has posed novel challenges.Besides the different kinds of uncertainty due to the nature of language itself (Baan et al., 2023), LLMs are usually too large for Bayesian methods and are more expensive to be finetuned (unless one resorts to low-rank approximations; Yang et al., 2023).Furthermore, the generation process is often shielded behind black-box APIs for commercial models, only leaving access to the predicted probabilities, or-in the worst case-the generated text.Existing approaches operate on the model's confidence and improve it through (re-)calibration (Tian et al., 2023;Chen et al., 2024;Bakman et al., 2024), the ensembling of prompts (Jiang et al., 2023;Hou et al., 2023), asking the model to rate its own uncertainty (Lin et al., 2022;Chen and Mueller, 2023;Tian et al., 2023) or analyzing its use of linguistic markers (Zhou et al., 2023), computing the entropy over sets of generations with similar meaning (Kuhn et al., 2023) or comparing the textual similarity of generations for the same input (Lin et al., 2023).The most similar work to ours comes from Mielke et al. (2022), who train a calibrator on the target model's hidden states to predict whether the answer is correct or incorrect.They then finetune the target model using control tokens that are based on the calibrator's prediction and indicate the desired confidence level.In comparison, our method has multiple advantages: We only assume access to the input question and text outputs, not requiring white-box model access or finetuning of the target model.We further demonstrate in our experiments that using more fine-grained targets than the simple binary yields better calibration overall.</p>
<p>Predicting properties of generated text.Instead of trying to obtain a piece of information of interest-e.g.truthfulness or predictive confidence-from the original model, other works have investigated whether this can be done so through secondary (neural) models.For instance, Lahlou et al. (2023); Mukhoti et al. (2023) fit density estimators on internal model representations.</p>
<p>In NLP, Pacchiardi et al. (2023) demonstrated that wrong statements by LLMs can be detected by collecting yes / no answers for a set of questions, storing them in a vector and fitting a logistic regression model.In general, using secondary neural models to predict properties of the generated text also has connections to other tasks such as translation quality estimation (Blatz et al., 2004;Quirk, 2004;Wang et al., 2019;Glushkova et al., 2021;Zerva et al., 2022b), toxicity classification (Maslej-Krešňáková et al., 2020) or fine-grained reward modeling (Wu et al., 2023).Estimating the confidence of a LLM can be challenging, since their size rules out many traditional techniques that require finetuning or access to model parameters.In this light, using the likelihood of the generated sequence might seem like an appealing alternative; however, it might not actually reflect the reliability of the model's answer and often cannot be retrieved when using blackbox models, where the only output is the generated text.Verbalized uncertainty, i.e. prompting the LLM to express its uncertainty in words, can be a solution when the model is powerful enough.But as we later show in Section 4, the generated confidence expressions are not very diverse, and results are not always consistent, meaning that the model does not always generate a desired confidence self-assessment.As we illustrate in Table 1, our method, APRICOT , fulfills all of these criteria: Through a one-time finetuning procedure of an auxiliary model on the target LLMs outputs, we have full control over a calibrated model that gives consistent and precise confidence estimates.</p>
<p>In Figure 2 we give an overview of APRICOT , which consists of three main steps: Firstly, we prompt the target LLM to generate training data for our auxiliary model (Section 3.1).Secondly, we set calibration targets in a way that does not require access to the target LLM beyond its generated outputs (Section 3.2).Lastly, we train the auxiliary calibrator to predict the target LLM's confidence for a given question (Section 3.3).Thereby, we contribute two parts that are agnostic to the LLM in question: The creation of calibration targets and their prediction through the auxiliary model.Note that we will use the terms auxiliary model or calibrator interchangeably in the following sections.</p>
<p>Prompting the Target LLM</p>
<p>In this step, we generate finetuning data for the auxiliary model by prompting the target LLM on the given task.Here, we explore different varia-tions to see which model response might provide the best training signal for the auxiliary calibrator.More concretely, while the original prompt and model generation might already suffice to predict the model's confidence, we also ask the model to elaborate on its answer using chain-of-thought prompting (Wei et al., 2022).We hypothesize that including additional reasoning steps could expose signals that are useful for the calibrator. 3We furthermore take a model's assessment of its confidence into account, too.Recent works on verbalized uncertainty (Lin et al., 2022;Tian et al., 2023) investigated how to elicit such an assessment as a percentage value, e.g."I am 95 % confident in my answer", or using linguistic expressions such as "My confidence is somewhat low".While previous studies like Zhou et al. (2024) have demonstrated the difficulty in obtaining reliable self-assessments, we can just treat them as additional input features, and let their importance be determined through the auxiliary model training.We illustrate the different prompting strategies in Figure 3 and list the exact prompts in Appendix A.1.</p>
<p>Setting Calibration Targets</p>
<p>After explaining the inputs to the auxiliary model, the question naturally arises about what the calibrator should be trained to predict.The work by Mielke et al. (2022) introduces an additional model that simply predicts individual answer correctness (and does so by using the target model's internal hidden states, which is not possible for black-box models).While we also check test this type of output in Section 4.2), we show that we can produce better calibration targets through clustering.</p>
<p>Background.We consider the notion of calibration by Guo et al. (2017): We define a predictor as calibrated when given some predicted outcome ŷ ∈ Y and some associated probability p ∈ [0, 1], we have
P Y = ŷ | P = p = p,(1)
i.e. the probability p corresponding to the actual relative frequency of correct predictions.This quantity is often approximated empirically through means such as the expected calibration error (ECE; Naeini et al., 2015).It evaluates the error
E P Y = ŷ | P = p − p ,(2)
where in the case of the ECE, the expectation is computed by grouping N test predictions into M equally wide bins.Defining B m as the set indices that belong to bin m, we can write the ECE as
M m=1 |B m | N 1 |B m | i∈Bm 1(ŷ i = y i ) Bin accuracy (target) − 1 |B m | i∈Bm pi Avg. bin confidence ,
(3) where 1(ŷ i = y i ) is the indicator function showing whether the prediction was correct.As Guo et al. (2017) note, both terms in the difference approximate the left-hand and right-hand side in Equation (1) per bin, respectively.Contribution.Our key insight is here that we can optimize an objective similar to Equation (2), inspired by the way that the ECE in Equation (3) aggregates samples in homogeneous groups and measures the group-wise accuracy.This done without changing the LLM's original answers or access to token probabilities.Instead of creating bins B m by confidence, which is not possible in a blackbox setting, we create clustered sets C m of inputs with similar sentence embeddings.Calibration targets are then obtained by using the observed accuracy per set C m .This is similar to Lin et al. (2022), who consider the accuracy per question category.Yet in the absence of such metadata, we expect good embedding and clustering algorithms to roughly group inputs by category.Höltgen and Williamson (2023) also echo a similar sentiment, describing how ECE's grouping by confidence can be abstracted to other kinds of similarities.They also provide a proof that the calibration error of a predictor based on a k-nearest neighbor clustering tends to zero in the infinite data limit.</p>
<p>Practically, we embed questions into a latent space using a light-weight model such as Sentence-BERT (Reimers and Gurevych, 2019), normalize the embeddings along the feature dimension (Timkey and van Schijndel, 2021), and then use HDBSCAN (Campello et al., 2013) to cluster them into questions of similar topic.The use of HDB-SCAN has multiple advantages: Compared to e.g.k-means, we do not have to determine the numbers of clusters in advance, and since the clustering is conducted bottom-up, clusters are not constrained to a spherical shape.Furthermore, compared to its predecessor DBSCAN (Ester et al., 1996), HDB-SCAN does not require one to determine the minimum distance between points for clustering man-ually.We evaluate this procedure in Section 4.1 and Appendix A.4.</p>
<p>Training the Auxiliary Model</p>
<p>After determining the input and the training targets for the auxiliary model in the previous sections, we can now describe the actual training procedure that makes it predict the target LLM's confidence.To start, we feed the questions alongside some incontext samples into our target LLM.We retain the generated answers and create a dataset that combines the question (without in-context samples) and the target model's answers.These are used to train the auxiliary calibrator to predict the calibration targets obtained by the clustering procedure above.</p>
<p>In our experiments, we use DeBERTaV3 (He et al., 2023), an improvement on the original DeBERTa model (He et al., 2021) using ELECTRA-style pretraining (Clark et al., 2020) and other improvements.We then finetune it using the AdamW optimizer (Loshchilov and Hutter, 2018) in combination with a cosine learning rate schedule.We minimize the following mean squared error, where pi is the predicted confidence, C(i) the cluster that the input question with index i belongs to and âj an answer given by the target LLM:
pi − 1 |C(i)| j∈C(i) 1(â j is correct) Cluster accuracy (target) 2 . (4)
We also explore a variant that simply predicts whether the LLM's answer is expected to be correct or incorrect.In this case, we optimize a binary cross-entropy loss with loss weights. 4Finally, we select the final model via the best loss on the validation set.We determine the learning rate and weight decay term through Bayesian hyperparameter search (Snoek et al., 2012), picking the best configuration by validation loss.We detail search ranges and found values in Appendix A.3.Training hardware and the environmental impact are discussed in Appendix A.6.</p>
<p>Experiments</p>
<p>We now demonstrate how APRICOT provides a simple yet effective solution to calibrate LLMs.</p>
<p>Before assessing the quality of the unsupervised clustering to determine calibration targets from Section 3, we first introduce the dataset and models.</p>
<p>Datasets.We employ TriviaQA (Joshi et al., 2017), a common (closed-book) questionanswering dataset.</p>
<p>Open-ended question answering is an ideal testbed for natural language generation tasks, since it is comparatively easy to check whether an answer is correct or not, so calibration has an intuitive interpretation.To preprocess TriviaQA, we create a training set of 12k examples and choose another 1.5k samples as a validation and test split, respectively.5Secondly, we run experiments on CoQA (Reddy et al., 2019), a conversational question-answering dataset in which the model is quizzed about the information in a passage of text.We treat the dataset as an open-book dataset, where the model is shown the passage and then asked one of the corresponding questions at a time.We extract a subset of the dataset to match the split sizes of TriviaQA.</p>
<p>Models.For our white-box model experiments, we choose a 7 billion parameter variant of the Vicuna v1.5 model (Zheng et al., 2023), 6an instruction-finetuned model originating from Llama 2 (Touvron et al., 2023).For the black-box model, we opt for OpenAI's GPT-3.5 (OpenAI, 2022).7 Despite recent API changes granting access to token probabilities,8 creating methods for black-box confidence estimation is still relevant for multiple reasons: Token probabilities are not available for most black-box models, they might be removed again to defend against potential security issues; and they are not always a reliable proxy for confidence.</p>
<p>Setting Calibration Targets by Clustering</p>
<p>Before beginning our main experiments, we would like to verify that our proposed methodology in Section 3.2 is sound.In particular, clustering the embeddings of questions and computing the calibration confidence targets rests on the assumption that similar questions are collected in the same Clustering .39±.28 .60 ±.14 .47±.25 .70±.17</p>
<p>Table 2: Results of evaluation of found clusters on Triv-iaQA and CoQA, including one standard deviation.</p>
<p>cluster.Ideally, we would like to check this using metadata, which however is usually not available.</p>
<p>Setup.Instead, we evaluate this through different means: We first use the all-mpnet-base-v2 model from the sentence transformers package (Reimers and Gurevych, 2019) and HDBSCAN with a minimum cluster size of 3 to cluster questions.We then analyze the textual and semantic similarity of questions in a cluster by computing the average pair-wise ROUGE-L score (semantic; Lin, 2004) 9 between questions and cosine similarities between question embeddings of the same cluster (semantic).Since performing this evaluation on the entire dataset is computationally expensive, we approximate the score by using 5 pairwise comparisons per cluster, with 200 comparisons for ROUGE-L and 1000 for cosine similarity in total, respectively.As a control for our method (clustering), we also compute values between unrelated questions that are not in the same cluster (random).</p>
<p>Results.We show the results of this analysis in Table 2.We observe noticeable differences between the random baseline and the similarity for the clustering scores, both on a textual and semantic level.While there is smaller difference on a textual level due to the relatively similar wording of questions, the semantic similarity based on the encoded questions is very notable.We provide deeper analyses of this part in Appendix A.4, showing that this method creates diverse ranges of calibration confidence targets.This suggests two things: On the one hand, our proposed methodology is able to identify fine-grained categories of questions.On the other hand, the diversity in calibration targets indicates that we detect sets of questions on which the LLM's accuracy varies-and that this variety should be reflected.We test the ability of different methods to do exactly this next.</p>
<p>Calibrating White-and Black-Box Models</p>
<p>We are interested to see whether auxiliary models can reliably predict the target LLM's confidence.We describe our experimental conditions below.</p>
<p>Evaluation metrics.Aside from reporting the accuracy on the question answering task, we also report several calibration metrics, including the expected calibration error (ECE;Naeini et al., 2015) using 10 bins.In order to address any distortion of results introduced by the binning procedure, we use smooth ECE (smECE; Błasiok and Nakkiran, 2023), which avoids the binning altogether by smoothing observations using a RBF kernel.We also consider Brier score (Brier, 1950), which can be interpreted as mean-squared error for probabilistic predictions.We further show how indicative the predicted confidence is for answering a question incorrectly by measuring the AUROC.The AUROC treats the problem as a binary misprediction detection task based on the confidence scores, aggregating the results over all possible decision thresholds.In each case, we report the result alongside a bootstrap estimate of the standard error (Efron and Tibshirani, 1994) estimated from 100 samples and test for significance using the ASO test (Del Barrio et al., 2018;Dror et al., 2019;Ulmer et al., 2022b) with τ = 0.35 and a confidence level of α = 0.1.</p>
<p>Baselines.To contextualize the auxiliary calibrator results, we consider the following baselines:</p>
<p>We consider the raw (length-normalized) sequence likelihoods (Seq.likelihood) as well as variant using Platt scaling (Platt et al., 1999): Using the raw likelihood p ∈ [0, 1] and the sigmoid function σ, we fit two additional scalars a, b ∈ R to minimize the mean squared error on the validation set to produce a calibrated likelihood q = σ(ap + b) while keeping all other calibrator parameters fixed.We also compare it to the recent method of verbalized uncertainty (Lin et al., 2022;Tian et al., 2023;Anonymous, 2024), where we ask the model to assess its confidence directly.We do this by asking for confidence in percent (Verbalized %) and using a seven-point scale from "very low" to "very high" and which is mapped back to numeric confidence scores (Verbalized Qual.).Where applicable, we also distinguish between baselines with and without chain-of-thought prompting (CoT; Wei et al., 2022).We detail this scale and corresponding prompts in Appendix A.1.For our approach, we distinguish between confidence targets obtained through the procedure in Section 3.2 (clustering) and simply predicting whether the given answer is correct or incorrect (binary).</p>
<p>Results.Vicuna v1.5 7B achieves 58% accuracy on TriviaQA and 44% on CoQA, while GPT-3.5 obtains 85% and 55% accuracy, respectively. 10We present the calibration results in Table 3. APRI-COT hereby achieves the highest AUROC in all settings and among the lowest Brier scores and calibration errors.On the latter metric, verbalized confidence beats our method, but often at the cost of a higher worst-case calibration error and lower AUROC.In addition, the qualitative verbalized uncertainty does not work reliably for the smaller Vicuna v1.5 model.CoT prompting seems to increase the success rate of verbalized uncertainty.The additional results on GPT-3.5 suggests that this ability might also be dependent on model size.The effect of CoT prompting on calibration, however, remains inconsistent across different baselines.While verbalized uncertainties often perform well according to calibration error, these results have to be taken with a grain of salt: Especially for the relatively small 7B Vicuna v1.5 model, the generations do not always contain the desired confidence expression, as visible by the low success rate.And even when taking the generated confidence expression, their ability to distinguish potentially correct from incorrect LLM responses remains at or close to random level.Lastly, APRICOT with clustering beats the use of binary targets for Vicuna v1.5 and GPT-3.5 on both TriviaQA and CoQA.We also juxtapose reliability diagrams for the different methods for Vicuna v1.5 on TriviaQA in Figure 4 (we show the other reliability diagrams, including for GPT-3.5, in Appendix A.5).Here it becomes clear that verbalized uncertainties approaches usually do not emit a wide variety of confidence scores.This is in line with observations by Zhou et al. (2023), who hypothesize the distribution of expressions generated by verbalized uncertainty heavily depend on the mention of e.g.percentage values in the model's training data.While Figure 10 shows that GPT-3.5 provides more variety in this regard, the overall phenomenon persists.We now conduct some additional analyses based on the clusteringbased variant of our method.</p>
<p>What does the calibrator learn from?</p>
<p>The previous results pose the question of which parts of input the auxiliary model actually learns from.So, analogous to the different prompting strategies in Figure 3, we explore different input variants: First, we test question-only, where the target LLM's answer is omitted completely.We also test the performance of the calibrator when given more information, for instance the model answer with and without chain-of-thought prompting, which could potentially expose flaws in the LLM's response.11Finally, we also expose the verbalized uncertainty of the LLM to the calibrator.</p>
<p>Results.We show these results in Table 8 in Appendix A.5.Interestingly, we can observe that even based on the question to the LLM alone, APRICOT can already achieve respectable performance across all metrics.This suggests that the calibrator at least partially learns to infer the difficulty of the LLM answering a question from the type of question alone.Nevertheless, we also find that adding the LLM's actual answer further improves results, with additional gain when using CoT prompting.In some cases, the calibration error can be improved when using the LLM's verbalized uncertainties; in this sense, we can interpret the role of the calibrator as mapping the model's own assessment to a calibrated confidence score.</p>
<p>Discussion</p>
<p>Despite the difficulty of predicting the LLM's confidence from its generated text alone, our experiments have shown that APRICOT can be used to produce reasonable scores even under these strict constraints.We showed in the past sections that the auxiliary model can be finetuned to learn from multiple signals.On the one hand, the auxiliary calibrator learns a mapping from a latent category of question to the expected difficulty for a target LLM.On the other hand, including the answer given through CoT prompting and including the LLM's own assessment of its uncertainty helped to further improve results.While sometimes beaten in terms of calibration error, our method consistently outperforms our baselines in misprediction AUROC, meaning that it can provide the best signal to detect wrong LLM answers.Compared to  other approaches, this yields some desirable properties: APRICOT is available when sequence likelihood is not; it is more reliable than verbalized uncertainty; and it only needs a light finetuning once, adding negligible inference overhead.Compared to other methods such as Kuhn et al. (2023); Lin et al. (2023), it also does not require more generations for the same input, reducing the more expensive LLM inference costs.</p>
<p>Conclusion</p>
<p>We presented APRICOT , a general method to obtain confidence scores from any language model on the input and text output alone.We showed that it is possible to compute calibration targets through the clustering of question embeddings.Through the subsequent finetuning of a smaller language model, we then outperform other methods to distinguish incorrect from correct answers with competitive calibration scores, on different models and datasets.While we only presented a first, more fundamental version this approach in this work, it lends itself naturally to a whole body of research that aims to improve the calibration of pretrained language models (Desai and Durrett, 2020;Jiang et al., 2021;Chen et al., 2023).Lastly, future studies might also investigate the uncertainty of the auxiliary model itself and use techniques such as conformal prediction (Vovk et al., 2005;Papadopoulos et al., 2002;Angelopoulos and Bates, 2021) to produce estimates of LLM confidence intervals.</p>
<p>Limitations</p>
<p>Clustering.While yielding generally positive results in our case, the clustering methodology from Section 3.2 requires access to a sufficiently expressive sentence embedding model and a large enough number of data points.When this is not given, we show that the binary approach-tuning the auxiliary model to predict misprediction-is a viable alternative that can even outperform the clustering variant in some settings.</p>
<p>Distributional shift.As any neural model, the auxiliary calibrator might be prone to distributional shift and out-of-distribution data.Further research could help to understand how this issue can be reduced and which parts of the input the model  identifies to predict confidence scores in order to unveil potential shortcut learning (Du et al., 2023).</p>
<p>Other types of language generation.Our experiments focused on open-ended question answering tasks, which provide an easy way to check answer correctness.In other types of language generation such as summarization, translation or open text generation, this notion is not given.However, we point out the relation of our approach to other NLP tasks in Section 2, which might provide an opening for future research.</p>
<p>Ethical Considerations</p>
<p>We mostly see any ethical considerations with our work arising in the general nature of neural models, which therefore also affects our auxiliary calibrator.The efficacy of neural models might vary on out-of-distribution data.In safety-sensitive applications, this also means that its predictions might be less trustworthy on certain sub-populations.In this case, explicit validation of the LLM's answers and the auxiliary calibrators corresponding predictions is necessary and further finetuning might be necessary.In these cases, reporting the confidence score alongside an answer might also be replaced by withholding the LLM's response, entirely.</p>
<p>Figure 1 :
1
Figure 1: Illustration of APRICOT : We train an auxiliary model to predict a target LLM's confidence based on its input and the generated answer.</p>
<p>Figure 2 :
2
Figure2: Full overview of APRICOT .We collect a LLM's answer to a set of questions and embed the latter using an embedding model.After clustering similar questions and identifying the LLM's accuracy on them, we can use this value as reference when training to predict the confidence from a question-answer pair.</p>
<p>Figure 3 :
3
Figure 3: Illustration of the prompting strategies used to generate the input data for the auxiliary calibrator.Note that (c) can also involve confidence expressed in words ("My confidence level is low") and that (b) and (c) can be combined.The exact prompts are listed in Appendix A.1.</p>
<p>Verbalized Qual.(f) Verbalized %.(g) Auxiliary (binary).(h)Auxiliary (clustering).</p>
<p>Figure 4 :
4
Figure 4: Reliability diagrams for our different methods using 10 bins each for Vicuna v1.5 on TriviaQA.The color as well as the percentage number within each bar indicate the proportion of total points contained in each bin.</p>
<p>Figure 11 :
11
Figure 11: Reliability diagrams for our different methods using 10 bins each for GPT-3.5 on CoQA.The color as well as the percentage number within each bar indicate the proportion of total points contained in each bin.</p>
<p>Table 1 :
1
Comparison of appealing attributes that LLM confidence quantification techniques should fulfil.They should ideally be applicable to black-box LLMs, be consistent (i.e., always elicit a response), and produce calibrated estimates of confidence.
Method Black-box LLM? Consistent? Calibrated?Seq. likelihoods✗✔✗Verb. uncertainty✔✗✗APRICOT (ours)✔✔✔</p>
<p>Table 3 :
3
Calibration results for Vicuna v1.5 and GPT-3.5 on TriviaQA and CoQA.We bold the best results per dataset and model, and underline those that are statistically significant compared to all other results assessed via the ASO test.Results are reported along with a bootstrap estimate of the standard error.</p>
<p>Table 8 :
8
±.00 .07±.01 .06±.01 .74±.01 .22±.00 .03±.01 .03±.00 .70±.01 ±.00 .09±.01 .09±.01 .83±.01 .18±.00 .04±.01 .04±.01 .82±.01 ✔ ✔ ✗ Qual..18±.00 .08 ±.01 .08 ±.01 .82±.01 .19±.00 .04±.01 .04±.01 .79±.01 ±.00 .07±.01 .07±.01 .82±.01 .18±.00 .03±.01 .03±.01 .80±.01 ±.01 .07±.01 .07±.01 .80±.01 .21±.00 .04±.01 .03±.01 .74±.01 ✔ ✔ ✔ Qual..19±.00 .08 ±.01 .08 ±.01 .80±.01 .22±.00 .03±.01 .03±.01 .70±.01 ✔ ✔ ✔ % .18±.00 .07±.01 .07±.01 .81±.01 .20 ±.00 .03±.01 .03±.00 .75±.01 ±.01 .05±.01 .05±.01 .71±.03 .21±.00 .03±.01 .03±.01 .72 ±.01 ±.01 .06±.01 .06±.01 .72 ±.02 .18±.01 .04±.02 .04±.02 .82±.02 ✔ ✔ ✗ Qual..12±.01 .03±.01 .03±.01 .72 ±.03 .18±.01 .02±.01 .02±.00 .80±.01 ±.01 .03±.01 .03±.01 .72 ±.02 .18±.00 .04±.01 .03±.00 .80±.01 Qual..12±.01 .04±.01 .04±.01 .73±.02 .21±.00 .04±.01 .04±.01 .72 ±.01 ±.01 .04±.01 .04±.01 .64±.02 .21±.00 .02±.01 .02±.00.72 ±.01 Calibration results for Vicuna v1.5 and GPT-3.5 on TriviaQA and CoQA using the auxiliary (clustering) method.We bold the best results per dataset, method and model.
Auxiliary Model InputTriviaQACoQAQuest. Ans. CoT Verb.Brier↓ECE↓smECE↓ AUROC↑ Brier↓ECE↓smECE↓ AUROC↑Vicuna v1.5 (white-box) .19 GPT-3.5 (black-box) ✔ ✗ ✗ ✗ .21 ✔ ✔ ✗ ✗ .18 ✔ ✔ ✗ % .18 ✔ ✔ ✔ ✗ ✔ ✗ ✗ ✗ .12 ✔ ✔ ✗ ✗ .12 ✔ ✔ ✗ % .12 ✔ ✔ ✔ ✗ .12 ±.01 .06 ±.01 .06 ±.01 ✔ ✔ ✔.72 ±.02.21 ±.00 .03 ±.01 .03 ±.01.72 ±.01✔✔✔%.12
https://x.com/spiantado/status/
https://github.com/parameterlab/ apricot.
We do this while acknowledging evidence byTurpin et al. (2023) that shows that any chain-of-thought reasoning might not reflect the actual reasons for a specific model response.
The loss weights are based on scikit-learn's implementation using the "balanced" mode, see https://scikit-learn.org/stable/modules/ generated/sklearn.utils.class_weight. compute_class_weight.html.
Since the original test split does not include answers, we generate the validation and test split from the original validation split.
https://huggingface.co/lmsys/ vicuna-7b-v1.5.
Specifically, using version gpt-3.5-turbo-0125.
https://x.com/OpenAIDevs/status/
As implemented by the evaluate package, see https: //huggingface.co/docs/evaluate/index.
We describe the evaluation protocol in Appendix A.2. Since GPT-3.5 is a closed-source model, it is hard to say whether the higher accuracy scores are due to better model quality, test data leakage, or overlap in questions in the case ofTriviaQA (Lewis et al., 2021).
Based on the recent study byTurpin et al. (2023), we assume that CoT does not expose the LLM's actual reasoning. Nevertheless, it provides more context about the given answer.
AcknowledgementsThis work was generously supported by the NAVER corporation.A AppendixThis appendix is structured as follows: We list all used prompts in Appendix A.1, briefly explore the evaluation protocal for the QA task in Appendix A.2 and list hyperparameter search details for replicability in Appendix A.3.The rest of the appendix is dedicated to additional results, namely for the analysis of the clustering step in Appendix A.4 or for the calibration experiments in Appendix A.5.We also list our compute hardware and its environmental impact in Appendix A.6.A.1 Prompting SetupIn the following we elaborate on the prompts used in this work.In general, we use a very simple prompt for question answering, where we fill in a template of the form "Question: {Question} Answer:".For in-context samples, we prepend the demonstrations to the input, using the sample template as above.In the case of chain-of-thought prompting, we use the prompting below:QA Chain-of-thought prompt Briefly answer the following question by thinking step by step.Question: {Question} Answer:In the case of CoQA, we slightly adjust the prompt template to the following:CoQA Chain-of-thought prompt Context: {Context} Instruction: Briefly answer the following question by thinking step by step.Question: {Question} Answer:Note that here the passage that questions are based on is given first, and chain-of-thought prompting is signaled through the "Instruction" field.When no chain-of-thought prompting is used, this field is omitted.For the verbalized uncertainty, we use the following prompts, in which case we omit any in-context samples: When mapping these expressions back to probabilities using the mapping in Table4.Verbalized uncertainty prompt (qualitative) {Question} {Model answer} Please provide your confidence in the answer only in percent (0-100 %):We followKuhn et al. (2023)and use 10 incontext samples for the original answer, which are randomly sampled from the training set (but in contrast toKuhn et al., wesample different examples for each instance).When prompting for verbalized uncertainty, we remove these in-context samples.A.2 Question-Answering Evaluation ProtocolIn order to check model answers automatically, we take inspiration from the evaluation protocol byKuhn et al. (2023).They use ROUGE-L(Lin, 2004)to compare the LLM's answer against the reference answer, and consider the answer as correct if the resulting score surpasses 0.3.We improve this protocol by adding the following condition:If the gold answer can be found verbatim in the generated answer, the answer is also considered correct.A.3 Finetuning HyperparametersWe conduct suites of hyperparameter searches per target LLM, dataset and type of calibration targets (binary and clustering) corresponding to the results in Table3, resulting in eight different suites.We then use these found hyperparameters for the results in Table8.Search method and ranges.For the search, we opt for Bayesian hyperparameter search(Snoek et al., 2012)as implemented by Weights &amp; Biases(Biewald, 2020).We optimize only two hyperparameters: Learning rate and weight decay.The learning rate is samples from a log-uniform distribution log U[1 × 10 −5 , 0.01] and weight decay from a uniform distribution U[1 × 10 −4 , 0.05] for a total of 50 runs and 250 training steps each.The final hyperparameters selected are given in Table5.Other hyperparameters.When obtaining the responses from Vicuna v1.5 7B, we use a batch size of 4 and generate for a maximum of 50 tokens and stop generation when the model tries to generate parts of the prompt, such as "Question:" / "Q:" or "Answer:" / "A:".We also use 10 in-context samples for TriviaQA, but no in-context samples for CoQA.For the auxiliary calibrator, we use a context size of 512 tokens, batch size of 32, gradient clipping with a maximum norm of 10.A.4 Additional Clustering ResultsIn this section we take a closer look at the results of the clustering procedure described in Section 3.2.In our experiments, we run HDBSCAN using a minimum cluster size of three, since preliminary experiments showed this number to produce the best trade-off between the coherence of cluster contents (as evaluated in Table2) and a diversity in cluster targets.This setting yields a distribution of cluster sizes shown in Figure5.We can see that the majority of cluster sizes are rather small, including questions on specific topics, some of which we display in Tables6 and 7.Not shown are cluster sizes over 20 since the distribution quickly levels off, as well the set of all points that could not be sorted into any cluster.After clustering and computing the average accuracy per cluster, we obtain a distribution over calibration targets, which we show with density plots in Figure6.Since most clusters are of size three, we can see clear modes around 0, 0.33, 0.66 and 1 for Vicuna v1.5 in Figure6a.For GPT-3.5 in Figure6bthese are however less pronounced: We see that targets are often concentrated on 0 or 1, respectively.Similar spikes like in Figure6aare observable for both models on CoQA in Figures6c  and 6d.This trend is also visible when plotting the assigned calibration targets per datapoint in Figure7: While we can spot more transitionary colors between the blue and red extremes in the manifold for Figure7a, the colors tend more to either of the options Figure7b.These mode trends continue for CoQA in Figure7cand Figure7d.A.5 Additional Calibration ResultsAdditional reliability plots.We show the all available reliability diagrams for Vicuna-v1.5 for TriviaQA in Figure4and CoQA in Figure9, as well as the corresponding plots for GPT-3.5 in Figures10 and 11.We can summarize some general trends: Sequence likelihood can be wellcalibrated already, but this fact depends strongly on the dataset in question.And while our version of Platt scaling can improve results, it also narrows the range of confidence values to a narrow window.Verbalized uncertainty in both of variants also is not able to produce a wide variety of responses, even though this effect is slightly less pronounced for GPT-3.5.This could be explained by previous observations byZhou et al. (2023)that certain men-  Who won the Kentucky Derby?as he won the Derby before?Has he raced in the Derby before?What were the winning horse's odds?How many Derbys have their been?Cluster 11Are they densities of everything the same?What is the densest elements at regular conditions?What is density of a substance?What is another symbol for density?Who gives weight per unit volume as the definition?Where is density the same value as it's mass concentration?To make comparisons easier what stands in for density?What is the relative density of something that floats?Cluster 1081Who was murdered?who was killed?Who committed this murder?who was killed?Who was killed?who was killed?Cluster 579Did it succeed?DId they continue to try? did they succeed?Was it successful?Did they expect that success?Table7: Contents of some randomly sampled cluster that result from the clustering procedure for CoQA.Ablation results.We show the results of the different ablations in Table8.For both LLMs, we can see that the auxiliary model is able to achieve already decent scores by using the question as input alone.This suggest that the calibrator is learning about the general difficulty of questions for the LLM.However, both cases also show that including more information-the LLM's answer, CoT reasoning or verbalized uncertainties-can help to improve the auxiliaries model calibration and misprediction AUROC even further, even though the effect remains somewhat inconsistent across models.A.6 Environmental ImpactAll experiments are run on a single V100 NVIDIA GPU.Using codecarbon(Schmidt et al., 2021;Lacoste et al., 2019;Lottick et al., 2019), we estimate finetuning the auxiliary calibrator on it to amount to 0.05 kgCO 2 eq of emission with an estimated carbon efficiency of 0.46 kgCO 2 eq / kWH.Therefore, we estimate total emissions of around 1 kgCO 2 eq to replicate all the experiments in our work.
A gentle introduction to conformal prediction and distribution-free uncertainty quantification. N Anastasios, Stephen Angelopoulos, Bates, arXiv:2107.075112021arXiv preprint</p>
<p>Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. Anonymous, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula Zerva, Wilker Aziz, arXiv:2307.15703Uncertainty in natural language generation: From theory to applications. 2023arXiv preprint</p>
<p>Mars: Meaningaware response scoring for uncertainty estimation in generative llms. Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, Salman Avestimehr, arXiv:2402.117562024arXiv preprint</p>
<p>Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty. Daniel Beck, Lucia Specia, Trevor Cohn, ; Bhatt, Javier Antorán, Yunfeng Zhang, Q Vera Liao, Prasanna Sattigeri, Riccardo Fogliato, Gabrielle Melançon, Ranganath Krishnan, Jason Stanley, Omesh Tickoo, Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. the 20th SIGNLL Conference on Computational Natural Language LearningBerlin, GermanyAssociation for Computational Linguistics2016. 2021Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</p>
<p>Experiment tracking with weights and biases. Software available from wandb. Lukas Biewald, 2020</p>
<p>Jarosław Błasiok, Preetum Nakkiran, arXiv:2309.12236Smooth ece: Principled reliability diagrams via kernel smoothing. 2023arXiv preprint</p>
<p>Confidence estimation for machine translation. John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, Nicola Ueffing, Coling 2004: Proceedings of the 20th international conference on computational linguistics. 2004</p>
<p>Verification of forecasts expressed in terms of probability. Glenn W Brier, Monthly weather review. 7811950</p>
<p>Density-based clustering based on hierarchical density estimates. Ricardo Jgb Campello, Davoud Moulavi, Jörg Sander, Pacific-Asia conference on knowledge discovery and data mining. Springer2013</p>
<p>Chatgpt may pass the bar exam soon, but has a long way to go for the lexglue benchmark. arXiv:2304.122022023arXiv preprint</p>
<p>Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment. Jiuhai Chen, Jonas Mueller, arXiv:2308.161752023arXiv preprint</p>
<p>Lihu Chen, Alexandre Perez-Lebel, Fabian M Suchanek, Gaël Varoquaux, arXiv:2402.04957Reconfidencing llms from the grouping loss perspective. 2024arXiv preprint</p>
<p>A close look into the calibration of pre-trained language models. Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, Heng Ji, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>ELECTRA: pretraining text encoders as discriminators rather than generators. Kevin Clark, Minh-Thang Luong, Quoc V Le, Christopher D Manning, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Large legal fictions: Profiling legal hallucinations in large language models. Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E Ho, arXiv:2401.013012024arXiv preprint</p>
<p>Underspecification presents challenges for credibility in modern machine learning. Katherine Alexander D'amour, Dan Heller, Ben Moldovan, Babak Adlam, Alex Alipanahi, Christina Beutel, Jonathan Chen, Jacob Deaton, Eisenstein, Matthew D Hoffman, The Journal of Machine Learning Research. 2312022</p>
<p>An optimal transportation approach for assessing almost stochastic order. Eustasio Del Barrio, Juan A Cuesta-Albertos, Carlos Matrán, The Mathematics of the Uncertain. Springer2018</p>
<p>Calibration of pre-trained transformers. Shrey Desai, Greg Durrett, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020. 2020. November 16-20, 2020</p>
<p>Shehzaad Dhuliawala, Vilém Zouhar, Mennatallah El-Assady, Mrinmaya Sachan, arXiv:2310.13544A diachronic perspective on user trust in ai under uncertainty. 2023arXiv preprint</p>
<p>Deep dominance -how to properly compare deep neural models. Rotem Dror, Segev Shlomov, Roi Reichart, 10.18653/v1/p19-1266Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics2019. July 28-August 2, 2019</p>
<p>Shortcut learning of large language models in natural language understanding. Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, Xia Hu, Communications of the ACM. 202367</p>
<p>An introduction to the bootstrap. Bradley Efron, Robert J Tibshirani, 1994CRC press</p>
<p>A density-based algorithm for discovering clusters in large spatial databases with noise. Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96). the Second International Conference on Knowledge Discovery and Data Mining (KDD-96)Portland, Oregon, USAAAAI Press1996</p>
<p>Uncertainty-aware machine translation evaluation. Taisiya Glushkova, Chrysoula Zerva, Ricardo Rei, F T André, Martins, Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>Micah Goldblum, Anima Anandkumar, Richard Baraniuk, Tom Goldstein, Kyunghyun Cho, Zachary C Lipton, Melanie Mitchell, Preetum Nakkiran, Max Welling, Andrew Gordon, Wilson , arXiv:2312.09323Perspectives on the state and future of deep learning-2023. 2023arXiv preprint</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, International conference on machine learning. PMLR2017</p>
<p>Mehedi Hasan, Moloud Abdar, Abbas Khosravi, Uwe Aickelin, Pietro Lio, Ibrahim Hossain, Ashikur Rahman, Saeid Nahavandi, arXiv:2304.04906Survey on leveraging uncertainty estimation towards trustworthy deep neural networks: The case of reject option and post-training processing. 2023arXiv preprint</p>
<p>The practical implementation of artificial intelligence technologies in medicine. Jianxing He, Sally L Baxter, Jie Xu, Jiming Xu, Xingtao Zhou, Kang Zhang, Nature medicine. 2512019</p>
<p>Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. Pengcheng He, Jianfeng Gao, Weizhu Chen, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Deberta: decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021</p>
<p>On the richness of calibration. Benedikt Höltgen, Robert C Williamson, Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. the 2023 ACM Conference on Fairness, Accountability, and Transparency2023</p>
<p>Decomposing uncertainty for large language models through input clarification ensembling. Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, Yang Zhang, arXiv:2311.087182023arXiv preprint</p>
<p>Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in ai. Alon Jacovi, Ana Marasović, Tim Miller, Yoav Goldberg, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Calibrating language models via augmented prompt ensembles. Mingjian Jiang, Yangjun Ruan, Sicong Huang, Saifei Liao, Silviu Pitis, Roger Baker Grosse, Jimmy Ba, 2023</p>
<p>How can we know When language models know? on the calibration of language models for question answering. Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig, Trans. Assoc. Comput. Linguistics. 92021</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017. 2017. July 30 -August 41</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Quantifying the carbon emissions of machine learning. Workshop on Tackling Climate Change with Machine Learning at NeurIPS. Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, Thomas Dandres, 2019. 2019</p>
<p>DEUP: direct epistemic uncertainty prediction. Salem Lahlou, Moksh Jain, Hadi Nekoei, Victor Butoi, Paul Bertin, Jarrid Rector-Brooks, Maksym Korablyov, Yoshua Bengio, Trans. Mach. Learn. Res. 2023. 2023</p>
<p>Question and answer test-train overlap in open-domain question answering datasets. S H Patrick, Pontus Lewis, Sebastian Stenetorp, Riedel, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021OnlineAssociation for Computational Linguistics2021. April 19 -23, 2021</p>
<p>Designing for responsible trust in ai systems: A communication perspective. Vera Liao, S Shyam Sundar, Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. the 2022 ACM Conference on Fairness, Accountability, and Transparency2022</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Teaching models to express their uncertainty in words. Stephanie Lin, Jacob Hilton, Owain Evans, Trans. Mach. Learn. Res. 2022. 2022</p>
<p>Generating with confidence: Uncertainty quantification for black-box large language models. Zhen Lin, Shubhendu Trivedi, Jimeng Sun, arXiv:2305.191872023arXiv preprint</p>
<p>Ilya Loshchilov, Frank Hutter, Fixing weight decay regularization in adam. 2018</p>
<p>Energy usage reports: Environmental awareness as part of algorithmic accountability. Workshop on Tackling Climate Change with Machine Learning at NeurIPS. Kadan Lottick, Silvia Susai, Sorelle A Friedler, Jonathan P Wilson, 2019. 2019</p>
<p>Uncertainty estimation in autoregressive structured prediction. Andrey Malinin, J F Mark, Gales, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021OpenReview.net</p>
<p>Gary Marcus, arXiv:2002.06177The next decade in ai: four steps towards robust artificial intelligence. 2020arXiv preprint</p>
<p>Comparison of deep learning models and various text pre-processing techniques for the toxic comments classification. Martin Viera Maslej-Krešňáková, Peter Sarnovskỳ, Kristína Butka, Machová, Applied Sciences. 102386312020</p>
<p>Reducing conversational agents' overconfidence through linguistic calibration. Sabrina J Mielke, Arthur Szlam, Emily Dinan, Y-Lan Boureau, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Deep deterministic uncertainty: A new simple baseline. Jishnu Mukhoti, Andreas Kirsch, Philip Joost Van Amersfoort, Yarin Hs Torr, Gal, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Obtaining well calibrated probabilities using bayesian binning. Gregory Mahdi Pakdaman Naeini, Milos Cooper, Hauskrecht, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201529</p>
<p>An eye for an 'i:'a critical assessment of artificial intelligence tools in migration and asylum management. Lucia Nalbandian, Comparative Migration Studies. 1012022</p>
<p>Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, Jasper Snoek, Advances in neural information processing systems. 2022. 201932OpenAIIntroducing chatgpt</p>
<p>How to catch an ai liar: Lie detection in black-box llms by asking unrelated questions. Lorenzo Pacchiardi, Alex J Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y Pan, Yarin Gal, arXiv:2309.15840Jan Brauner. 2023Owain EvansarXiv preprint</p>
<p>Inductive confidence machines for regression. Harris Papadopoulos, Kostas Proedrou, Volodya Vovk, Alex Gammerman, Machine Learning: ECML 2002: 13th European Conference on Machine Learning Helsinki. FinlandSpringer2002. August 19-23, 2002 Proceedings 13</p>
<p>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. John Platt, Advances in large margin classifiers. 1031999</p>
<p>Training a sentence-level machine translation confidence measure. Christopher Quirk, LREC. 2004</p>
<p>Coqa: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, Transactions of the Association for Computational Linguistics. 72019</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019. November 3-7, 2019</p>
<p>Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, 10.5281/zenodo.4658424Sorelle Friedler, and Sasha Luccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing. </p>
<p>Practical bayesian optimization of machine learning algorithms. Jasper Snoek, Hugo Larochelle, Ryan P Adams, Advances in neural information processing systems. 201225</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D Manning, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>All bark and no bite: Rogue dimensions in transformer language models obscure representational quality. William Timkey, Marten Van Schijndel, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaAssociation for Computational Linguistics2021. 7-11 November, 2021</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel R Bowman, arXiv:2305.043882023arXiv preprint</p>
<p>Exploring predictive uncertainty and calibration in NLP: A study on the impact of method &amp; data scarcity. Dennis Ulmer, Jes Frellsen, Christian Hardmeier, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022a. December 7-11, 2022</p>
<p>deep-significance: Easy and meaningful signifcance testing in the age of neural networks. Dennis Ulmer, Christian Hardmeier, Jes Frellsen, ML Evaluation Standards Workshop at the Tenth International Conference on Learning Representations. 2022b</p>
<p>Trust issues: Uncertainty estimation does not enable reliable ood detection on medical tabular data. Dennis Ulmer, Lotta Meijerink, Giovanni Cinà, Machine Learning for Health. PMLR2020</p>
<p>Intensive care unit physicians' perspectives on artificial intelligencebased clinical decision support tools: Preimplementation survey study. Anne Ah De Siri L Van Der Meijden, Patrick J Hond, Ewout W Thoral, Ilse Mj Steyerberg, Giovanni Kant, Cinà, Arbous Sesmu, JMIR Human Factors. 10e391142023</p>
<p>Benchmarking scalable predictive uncertainty in text classification. Jordy Van Landeghem, Matthew Blaschko, Bertrand Anckaert, Marie-Francine Moens, 2022IEEE Access</p>
<p>Hybrid uncertainty quantification for selective text classification in ambiguous tasks. Artem Vazhentsev, Gleb Kuzmin, Akim Tsvigun, Alexander Panchenko, Maxim Panov, Mikhail Burtsev, Artem Shelmanov, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Algorithmic learning in a random world. Vladimir Vovk, Alexander Gammerman, Glenn Shafer, 2005Springer29</p>
<p>Improving back-translation with uncertainty-based confidence estimation. Shuo Wang, Yang Liu, Chao Wang, Huanbo Luan, Maosong Sun, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2019. 2019. November 3-7, 2019</p>
<p>Understanding how dimension reduction tools work: an empirical approach to deciphering t-sne, umap, trimap, and pacmap for data visualization. Yingfan Wang, Haiyang Huang, Cynthia Rudin, Yaron Shaposhnik, The Journal of Machine Learning Research. 2212021</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Finegrained human feedback gives better rewards for language model training. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, Hannaneh Hajishirzi, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Wat zei je? detecting out-of-distribution translations with variational transformers. Aidan N Tim Z Xiao, Yarin Gomez, Gal, arXiv:2006.083442020arXiv preprint</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, 10.48550/ARXIV.2306.13063CoRR, abs/2306.130632023</p>
<p>Bayesian low-rank adaptation for large language models. Maxime Adam X Yang, Xi Robeyns, Laurence Wang, Aitchison, arXiv:2308.131112023arXiv preprint</p>
<p>Disentangling uncertainty in machine translation evaluation. Chrysoula Zerva, Taisiya Glushkova, Ricardo Rei, F T André, Martins, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022a. December 7-11, 20222022</p>
<p>Better uncertainty quantification for machine translation evaluation. Chrysoula Zerva, Taisiya Glushkova, Ricardo Rei, André Ft Martins, 2022b2204arXiv e-prints</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.05685Judging llm-as-a-judge with mt-bench and chatbot arena. 2023arXiv preprint</p>
<p>Relying on the unreliable: The impact of language models' reluctance to express uncertainty. Kaitlyn Zhou, Jena D Hwang, Xiang Ren, Maarten Sap, arXiv:2401.067302024arXiv preprint</p>
<p>Navigating the grey area: Expressions of overconfidence and uncertainty in language models. Kaitlyn Zhou, Dan Jurafsky, Tatsunori Hashimoto, arXiv:2302.134392023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
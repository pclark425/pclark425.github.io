<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2425 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2425</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2425</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-272525411</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.05258v1.pdf" target="_blank">Towards Automated Machine Learning Research</a></p>
                <p><strong>Paper Abstract:</strong> This paper explores a top-down approach to automating incremental advances in machine learning research through component-level innovation, facilitated by Large Language Models (LLMs). Our framework systematically generates novel components, validates their feasibility, and evaluates their performance against existing baselines. A key distinction of this approach lies in how these novel components are generated. Unlike traditional AutoML and NAS methods, which often rely on a bottom-up combinatorial search over predefined, hardcoded base components, our method leverages the cross-domain knowledge embedded in LLMs to propose new components that may not be confined to any hard-coded predefined set. By incorporating a reward model to prioritize promising hypotheses, we aim to improve the efficiency of the hypothesis generation and evaluation process. We hope this approach offers a new avenue for exploration and contributes to the ongoing dialogue in the field.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2425.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2425.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-down LLM Auto-Research Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-down Large Language Model (LLM) based Automated Machine Learning Research Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses LLMs to generate high-level, implementable component hypotheses (e.g., activation functions, preprocessors, regularizers), validates them with unit-test style validators, evaluates them by inserting them into fixed models and measuring validation loss, and trains a reward (ranking) model on code embeddings to prioritize promising hypotheses for expensive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Top-down LLM-based automated ML research framework (paper's system)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The system has four main parts: (1) Generator: LLM prompts (two styles -- Incrementality Encouraging Prompting (IEP) and Novelty Encouraging Prompting (NEP)) produce implementation code for candidate components; (2) Validator: automated unit-test style checks that a generated candidate compiles/implements required interface and default params (binary accept/reject); (3) Evaluator: integrates a validated candidate into a hard-coded model (2-layer MLP) and runs a single forward+backward pass to obtain a preliminary validation loss (val-loss) as the objective; (4) Reward model & closed-loop: train a ranking model mapping code embeddings (CodeBERT, Graph-CodeBert, CodeGen features concatenated) to success metrics (Baseline Win Rate (B-WR) and Baseline State-Of-The-Art Win Rate (BSOTA-WR)); in iteration two the reward model prunes/generated candidates and only top-k predicted candidates are fully evaluated. The framework also includes wrappers for prompt variants, generation at scale (three LLMs used), and greedy diversity-selection to mitigate reward collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Hypothesis Generation System / Automated Discovery System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning research (component-level innovations for neural networks): activation functions, preprocessing (feature transforms/PCA variants), and regularization functions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate viable alternative implementations for a chosen component type (e.g., activation function to replace ReLU) and measure whether the generated component improves a downstream model's validation loss when substituted into a fixed 2-layer MLP across small benchmark tasks (Iris, Wine (classification/regression), Breast Cancer, Diabetes). The evaluation protocol uses a one-pass learning setup (single epoch, single forward+backward per hypothesis) to enable large-scale exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional, combinatorial code/search space (space of possible function implementations and code-level variations). Problems are non-linear (neural network behavior), multi-task (multiple datasets/tasks), and stochastic (training randomness across runs). Quantitative aspects reported: dataset of 36,000 (hypothesis, reward) tuples generated over two iterations (18,000 each); three LLMs × three component types × two prompt types with 1,000 samples per combination in initial generation; 2,000 validated/evaluated hypotheses per component type per iteration (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses pre-existing, small public ML datasets (Iris: 150 samples, Wine: 178, Breast Cancer: 569, Diabetes: 442). Data are low-volume and readily available; no expensive data generation was required. The paper notes small-scale experiments limited by compute/resources.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Designed to be compute-efficient by using only one forward+backward pass per hypothesis (one-epoch / single-pass learning) to obtain preliminary val-loss. Total evaluations: tens of thousands of hypothesis evaluations (36,000 tuples). No explicit compute-hours or dollar-costs are reported. The approach reduces per-candidate cost relative to full training, but still requires substantial aggregate compute to evaluate thousands of candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined local replacement tasks (replace a single component in a fixed baseline model), discrete/continuous mix (code is discrete, outputs continuous), stochastic (random initialization, dataset sampling), with clear objective metrics (validation loss) enabling automated evaluation. Domain knowledge is encoded indirectly via prompt engineering and the baseline set B; the search is open-ended because LLMs can propose components not present in predefined primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Baseline Win Rate (B-WR): P(l(h_i) < l(b_j)) across baselines; Baseline State-of-the-Art Win Rate (BSOTA-WR): P(l(h_i) < min_j l(b_j)). For the reward model, ranking metrics were used: Kendall's Tau, Spearman correlation, and Pearson correlation between predicted rank and actual performance. Efficiency measured by how quickly top-k promising candidates are discovered (AUC of prioritized top-50 reward across steps).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Mixed. At population level most generated hypotheses had low win rates vs. baselines, but there exist individual components with very high success (some hypotheses approach win rates near 1 against baselines and BSOTA). Reward models achieved positive ranking correlations (examples reported in paper: Kendall's tau up to ≈0.824, Spearman up to ≈0.922, Pearson up to ≈0.931 for favorable train/test LLM dataset combinations); reward models improved discovery efficiency (higher AUC vs random) for activation and preprocessor blocks. Exact validator pass rate and component-level averaged win-rate values are reported in the paper's tables/figures (Table 1 and Figures 3,8) but not all numeric values are reproduced verbatim in text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Observed failure modes include: (1) Reward collapse — generation of many highly similar or redundant high-scoring hypotheses (lack of diversity), notably with GPT-3.5-turbo; (2) SOTA inductive bias and unintended plagiarism — LLMs tend to produce outputs that resemble existing literature, limiting novelty; (3) Overfitting/shortcuts — models may exploit dataset-specific shortcuts rather than general improvements; (4) Generalization failures — reward models trained on one LLM/dataset sometimes fail to generalize to other LLM-generated candidate sets (notably some regularizer reward models trained on Gemini-pro failed to generalize to others as noted); (5) Limited scale of evaluation (one-pass) may miss long-term training behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key contributors: (a) LLMs' ability to produce syntactically feasible and semantically plausible component code (reduces random search space); (b) automated validator + unit tests that filter invalid code cheaply; (c) content-based reward model trained on code embeddings that prioritizes promising candidates and reduces expensive evaluations; (d) prompt engineering (IEP vs NEP) to trade off incremental vs novel suggestions; (e) one-pass evaluation protocol enabling thousands of candidates to be evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Performance varied by component type, prompt style, and origin LLM: reward-ranking models and efficiency gains were strongest for activation functions and preprocessors; regularizer block ranking generalized less reliably across LLM datasets (Gemini-pro-trained reward models sometimes failed). Novelty-encouraging prompts produced more diverse outputs, while incrementality-encouraging prompts produced components closer to known baselines. Reward models trained and tested on data from the same LLM achieved higher correlation metrics; cross-LLM generalization reduced correlations but often remained directionally positive.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>The baseline is a fixed set B of existing, commonly used components (e.g., ReLU and other standard functions). No direct human-researcher performance (e.g., time-to-design, number-of-novel-components-by-humans) is reported; comparisons are performed against the baseline component set rather than against new human-designed components.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Automated Machine Learning Research', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2425.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2425.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoML-Zero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bottom-up evolutionary AutoML approach that evolves complete machine learning algorithms and update rules from a set of primitive operations rather than searching over predefined high-level components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoML-Zero</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in the paper as a bottom-up combinatorial search method that evolves algorithms from primitive operations (e.g., simple mathematical ops) via evolutionary strategies; contrasted with the top-down LLM approach of the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AutoML / Bottom-up Evolutionary Architecture/Algorithm Search</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning algorithm/architecture discovery (general AutoML).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Discover full learning algorithms or model update rules by evolving sequences of primitive operations; aims to construct algorithms from scratch rather than combining existing high-level blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Extremely large combinatorial search space (program-space over primitive operations), highly non-linear and discrete.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Typically uses standard ML benchmarks in literature for evaluation; not detailed in this paper beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Known to be compute-intensive (evolution over huge search space), though specific compute figures are not provided in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, discrete, stochastic (evolutionary search), with no single closed-form evaluation metric beyond downstream task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Downstream performance of discovered algorithms on benchmark tasks compared to hand-designed algorithms/SOTA.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported within this paper; AutoML-Zero is cited to contrast bottom-up approaches with the paper's top-down LLM method.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed in detail in this paper; general literature reports high computational cost and difficulty scaling to complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Ability to discover novel update rules given sufficient compute and well-chosen primitives; methodological novelty in evolving algorithms from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned qualitatively as a bottom-up approach that the current work complements (the current paper positions its LLM top-down approach as complementary to AutoML-Zero's bottom-up methodology).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Automated Machine Learning Research', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2425.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2425.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaQNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Designing Neural Network Architectures using Reinforcement Learning (MetaQNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement learning approach to neural architecture search that learns to sequentially select layer types and hyperparameters to build architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Designing neural network architectures using reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaQNN (RL-based NAS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an example of bottom-up neural architecture search where an RL agent constructs network architectures by selecting components/actions; contrasted to the top-down LLM-driven generation of higher-level novel components in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Neural Architecture Search (NAS) using Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Neural network architecture design for computer vision / general ML tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Sequential decision process to build architectures by choosing layers and connections; search over a discrete design space of architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Large discrete search space, stochastic training evaluation, multi-objective tradeoffs (accuracy, model size, latency).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses standard ML datasets in NAS literature; not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Historically high (many architecture evaluations), though such methods have been optimized over time; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Discrete, sequential, stochastic, with clear evaluation metrics (validation accuracy/loss).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Downstream validation/test performance and resource-aware metrics in NAS literature.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not provided in this paper (only mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not detailed here; generally include high compute cost and local optima in search.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>RL agent design, reward shaping, surrogate/fidelity methods to reduce evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Positioned as a bottom-up alternative in related work; no head-to-head comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Automated Machine Learning Research', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2425.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2425.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-Keras</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-Keras: An Efficient Neural Architecture Search System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AutoML/NAS system that provides practical and accessible neural architecture search and model selection automation, often via Bayesian/EA/heuristic search with usable APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auto-Keras: An Efficient Neural Architecture Search System</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Auto-Keras</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an AutoML/NAS system automating architecture search and hyperparameter tuning (related work). It is referenced as part of the AutoML family that automates model selection pipelines using bottom-up search over predefined components.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AutoML / Neural Architecture Search System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Automated architecture/hyperparameter selection for machine learning models.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate selection of network architectures, preprocessing, and hyperparameters to achieve strong performance with minimal human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Search over discrete architecture/hyperparameter spaces; resource constraints motivate efficient search strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on standard ML datasets; not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Designed to be more computationally efficient and user-friendly than early NAS systems; specific compute numbers not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined optimization problem with measurable objectives (accuracy, loss, latency).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Downstream model performance and resource metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper (only referenced in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Efficient search strategies and practical engineering for usability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned as part of the AutoML ecosystem contrasted with the paper's top-down approach.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Automated Machine Learning Research', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AutoML-Zero: Evolving Machine Learning Algorithms From Scratch <em>(Rating: 2)</em></li>
                <li>Designing neural network architectures using reinforcement learning <em>(Rating: 2)</em></li>
                <li>Auto-Keras: An Efficient Neural Architecture Search System <em>(Rating: 2)</em></li>
                <li>Learning Transferable Architectures for Scalable Image Recognition <em>(Rating: 1)</em></li>
                <li>NASNet: Neural Architecture Search with Reinforcement Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2425",
    "paper_id": "paper-272525411",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "Top-down LLM Auto-Research Framework",
            "name_full": "Top-down Large Language Model (LLM) based Automated Machine Learning Research Framework",
            "brief_description": "A framework that uses LLMs to generate high-level, implementable component hypotheses (e.g., activation functions, preprocessors, regularizers), validates them with unit-test style validators, evaluates them by inserting them into fixed models and measuring validation loss, and trains a reward (ranking) model on code embeddings to prioritize promising hypotheses for expensive evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Top-down LLM-based automated ML research framework (paper's system)",
            "system_description": "The system has four main parts: (1) Generator: LLM prompts (two styles -- Incrementality Encouraging Prompting (IEP) and Novelty Encouraging Prompting (NEP)) produce implementation code for candidate components; (2) Validator: automated unit-test style checks that a generated candidate compiles/implements required interface and default params (binary accept/reject); (3) Evaluator: integrates a validated candidate into a hard-coded model (2-layer MLP) and runs a single forward+backward pass to obtain a preliminary validation loss (val-loss) as the objective; (4) Reward model & closed-loop: train a ranking model mapping code embeddings (CodeBERT, Graph-CodeBert, CodeGen features concatenated) to success metrics (Baseline Win Rate (B-WR) and Baseline State-Of-The-Art Win Rate (BSOTA-WR)); in iteration two the reward model prunes/generated candidates and only top-k predicted candidates are fully evaluated. The framework also includes wrappers for prompt variants, generation at scale (three LLMs used), and greedy diversity-selection to mitigate reward collapse.",
            "system_type": "Hypothesis Generation System / Automated Discovery System",
            "problem_domain": "Machine learning research (component-level innovations for neural networks): activation functions, preprocessing (feature transforms/PCA variants), and regularization functions.",
            "problem_description": "Generate viable alternative implementations for a chosen component type (e.g., activation function to replace ReLU) and measure whether the generated component improves a downstream model's validation loss when substituted into a fixed 2-layer MLP across small benchmark tasks (Iris, Wine (classification/regression), Breast Cancer, Diabetes). The evaluation protocol uses a one-pass learning setup (single epoch, single forward+backward per hypothesis) to enable large-scale exploration.",
            "problem_complexity": "High-dimensional, combinatorial code/search space (space of possible function implementations and code-level variations). Problems are non-linear (neural network behavior), multi-task (multiple datasets/tasks), and stochastic (training randomness across runs). Quantitative aspects reported: dataset of 36,000 (hypothesis, reward) tuples generated over two iterations (18,000 each); three LLMs × three component types × two prompt types with 1,000 samples per combination in initial generation; 2,000 validated/evaluated hypotheses per component type per iteration (reported).",
            "data_availability": "Uses pre-existing, small public ML datasets (Iris: 150 samples, Wine: 178, Breast Cancer: 569, Diabetes: 442). Data are low-volume and readily available; no expensive data generation was required. The paper notes small-scale experiments limited by compute/resources.",
            "computational_requirements": "Designed to be compute-efficient by using only one forward+backward pass per hypothesis (one-epoch / single-pass learning) to obtain preliminary val-loss. Total evaluations: tens of thousands of hypothesis evaluations (36,000 tuples). No explicit compute-hours or dollar-costs are reported. The approach reduces per-candidate cost relative to full training, but still requires substantial aggregate compute to evaluate thousands of candidates.",
            "problem_structure": "Well-defined local replacement tasks (replace a single component in a fixed baseline model), discrete/continuous mix (code is discrete, outputs continuous), stochastic (random initialization, dataset sampling), with clear objective metrics (validation loss) enabling automated evaluation. Domain knowledge is encoded indirectly via prompt engineering and the baseline set B; the search is open-ended because LLMs can propose components not present in predefined primitives.",
            "success_metric": "Baseline Win Rate (B-WR): P(l(h_i) &lt; l(b_j)) across baselines; Baseline State-of-the-Art Win Rate (BSOTA-WR): P(l(h_i) &lt; min_j l(b_j)). For the reward model, ranking metrics were used: Kendall's Tau, Spearman correlation, and Pearson correlation between predicted rank and actual performance. Efficiency measured by how quickly top-k promising candidates are discovered (AUC of prioritized top-50 reward across steps).",
            "success_rate": "Mixed. At population level most generated hypotheses had low win rates vs. baselines, but there exist individual components with very high success (some hypotheses approach win rates near 1 against baselines and BSOTA). Reward models achieved positive ranking correlations (examples reported in paper: Kendall's tau up to ≈0.824, Spearman up to ≈0.922, Pearson up to ≈0.931 for favorable train/test LLM dataset combinations); reward models improved discovery efficiency (higher AUC vs random) for activation and preprocessor blocks. Exact validator pass rate and component-level averaged win-rate values are reported in the paper's tables/figures (Table 1 and Figures 3,8) but not all numeric values are reproduced verbatim in text.",
            "failure_modes": "Observed failure modes include: (1) Reward collapse — generation of many highly similar or redundant high-scoring hypotheses (lack of diversity), notably with GPT-3.5-turbo; (2) SOTA inductive bias and unintended plagiarism — LLMs tend to produce outputs that resemble existing literature, limiting novelty; (3) Overfitting/shortcuts — models may exploit dataset-specific shortcuts rather than general improvements; (4) Generalization failures — reward models trained on one LLM/dataset sometimes fail to generalize to other LLM-generated candidate sets (notably some regularizer reward models trained on Gemini-pro failed to generalize to others as noted); (5) Limited scale of evaluation (one-pass) may miss long-term training behaviors.",
            "success_factors": "Key contributors: (a) LLMs' ability to produce syntactically feasible and semantically plausible component code (reduces random search space); (b) automated validator + unit tests that filter invalid code cheaply; (c) content-based reward model trained on code embeddings that prioritizes promising candidates and reduces expensive evaluations; (d) prompt engineering (IEP vs NEP) to trade off incremental vs novel suggestions; (e) one-pass evaluation protocol enabling thousands of candidates to be evaluated.",
            "comparative_results": "Performance varied by component type, prompt style, and origin LLM: reward-ranking models and efficiency gains were strongest for activation functions and preprocessors; regularizer block ranking generalized less reliably across LLM datasets (Gemini-pro-trained reward models sometimes failed). Novelty-encouraging prompts produced more diverse outputs, while incrementality-encouraging prompts produced components closer to known baselines. Reward models trained and tested on data from the same LLM achieved higher correlation metrics; cross-LLM generalization reduced correlations but often remained directionally positive.",
            "human_baseline": "The baseline is a fixed set B of existing, commonly used components (e.g., ReLU and other standard functions). No direct human-researcher performance (e.g., time-to-design, number-of-novel-components-by-humans) is reported; comparisons are performed against the baseline component set rather than against new human-designed components.",
            "uuid": "e2425.0",
            "source_info": {
                "paper_title": "Towards Automated Machine Learning Research",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AutoML-Zero",
            "name_full": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
            "brief_description": "A bottom-up evolutionary AutoML approach that evolves complete machine learning algorithms and update rules from a set of primitive operations rather than searching over predefined high-level components.",
            "citation_title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
            "mention_or_use": "mention",
            "system_name": "AutoML-Zero",
            "system_description": "Described in the paper as a bottom-up combinatorial search method that evolves algorithms from primitive operations (e.g., simple mathematical ops) via evolutionary strategies; contrasted with the top-down LLM approach of the current paper.",
            "system_type": "AutoML / Bottom-up Evolutionary Architecture/Algorithm Search",
            "problem_domain": "Machine learning algorithm/architecture discovery (general AutoML).",
            "problem_description": "Discover full learning algorithms or model update rules by evolving sequences of primitive operations; aims to construct algorithms from scratch rather than combining existing high-level blocks.",
            "problem_complexity": "Extremely large combinatorial search space (program-space over primitive operations), highly non-linear and discrete.",
            "data_availability": "Typically uses standard ML benchmarks in literature for evaluation; not detailed in this paper beyond citation.",
            "computational_requirements": "Known to be compute-intensive (evolution over huge search space), though specific compute figures are not provided in the current paper.",
            "problem_structure": "Open-ended, discrete, stochastic (evolutionary search), with no single closed-form evaluation metric beyond downstream task performance.",
            "success_metric": "Downstream performance of discovered algorithms on benchmark tasks compared to hand-designed algorithms/SOTA.",
            "success_rate": "Not reported within this paper; AutoML-Zero is cited to contrast bottom-up approaches with the paper's top-down LLM method.",
            "failure_modes": "Not discussed in detail in this paper; general literature reports high computational cost and difficulty scaling to complex problems.",
            "success_factors": "Ability to discover novel update rules given sufficient compute and well-chosen primitives; methodological novelty in evolving algorithms from scratch.",
            "comparative_results": "Mentioned qualitatively as a bottom-up approach that the current work complements (the current paper positions its LLM top-down approach as complementary to AutoML-Zero's bottom-up methodology).",
            "human_baseline": "Not provided in this paper.",
            "uuid": "e2425.1",
            "source_info": {
                "paper_title": "Towards Automated Machine Learning Research",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MetaQNN",
            "name_full": "Designing Neural Network Architectures using Reinforcement Learning (MetaQNN)",
            "brief_description": "A reinforcement learning approach to neural architecture search that learns to sequentially select layer types and hyperparameters to build architectures.",
            "citation_title": "Designing neural network architectures using reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "MetaQNN (RL-based NAS)",
            "system_description": "Mentioned as an example of bottom-up neural architecture search where an RL agent constructs network architectures by selecting components/actions; contrasted to the top-down LLM-driven generation of higher-level novel components in this paper.",
            "system_type": "Neural Architecture Search (NAS) using Reinforcement Learning",
            "problem_domain": "Neural network architecture design for computer vision / general ML tasks.",
            "problem_description": "Sequential decision process to build architectures by choosing layers and connections; search over a discrete design space of architectures.",
            "problem_complexity": "Large discrete search space, stochastic training evaluation, multi-objective tradeoffs (accuracy, model size, latency).",
            "data_availability": "Uses standard ML datasets in NAS literature; not detailed here.",
            "computational_requirements": "Historically high (many architecture evaluations), though such methods have been optimized over time; not quantified in this paper.",
            "problem_structure": "Discrete, sequential, stochastic, with clear evaluation metrics (validation accuracy/loss).",
            "success_metric": "Downstream validation/test performance and resource-aware metrics in NAS literature.",
            "success_rate": "Not provided in this paper (only mentioned in related work).",
            "failure_modes": "Not detailed here; generally include high compute cost and local optima in search.",
            "success_factors": "RL agent design, reward shaping, surrogate/fidelity methods to reduce evaluation cost.",
            "comparative_results": "Positioned as a bottom-up alternative in related work; no head-to-head comparisons in this paper.",
            "human_baseline": "Not provided in this paper.",
            "uuid": "e2425.2",
            "source_info": {
                "paper_title": "Towards Automated Machine Learning Research",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Auto-Keras",
            "name_full": "Auto-Keras: An Efficient Neural Architecture Search System",
            "brief_description": "An AutoML/NAS system that provides practical and accessible neural architecture search and model selection automation, often via Bayesian/EA/heuristic search with usable APIs.",
            "citation_title": "Auto-Keras: An Efficient Neural Architecture Search System",
            "mention_or_use": "mention",
            "system_name": "Auto-Keras",
            "system_description": "Cited as an AutoML/NAS system automating architecture search and hyperparameter tuning (related work). It is referenced as part of the AutoML family that automates model selection pipelines using bottom-up search over predefined components.",
            "system_type": "AutoML / Neural Architecture Search System",
            "problem_domain": "Automated architecture/hyperparameter selection for machine learning models.",
            "problem_description": "Automate selection of network architectures, preprocessing, and hyperparameters to achieve strong performance with minimal human intervention.",
            "problem_complexity": "Search over discrete architecture/hyperparameter spaces; resource constraints motivate efficient search strategies.",
            "data_availability": "Operates on standard ML datasets; not detailed here.",
            "computational_requirements": "Designed to be more computationally efficient and user-friendly than early NAS systems; specific compute numbers not provided in this paper.",
            "problem_structure": "Well-defined optimization problem with measurable objectives (accuracy, loss, latency).",
            "success_metric": "Downstream model performance and resource metrics.",
            "success_rate": "Not reported in this paper (only referenced in related work).",
            "failure_modes": "Not discussed here.",
            "success_factors": "Efficient search strategies and practical engineering for usability.",
            "comparative_results": "Mentioned as part of the AutoML ecosystem contrasted with the paper's top-down approach.",
            "human_baseline": "Not provided in this paper.",
            "uuid": "e2425.3",
            "source_info": {
                "paper_title": "Towards Automated Machine Learning Research",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
            "rating": 2,
            "sanitized_title": "automlzero_evolving_machine_learning_algorithms_from_scratch"
        },
        {
            "paper_title": "Designing neural network architectures using reinforcement learning",
            "rating": 2,
            "sanitized_title": "designing_neural_network_architectures_using_reinforcement_learning"
        },
        {
            "paper_title": "Auto-Keras: An Efficient Neural Architecture Search System",
            "rating": 2,
            "sanitized_title": "autokeras_an_efficient_neural_architecture_search_system"
        },
        {
            "paper_title": "Learning Transferable Architectures for Scalable Image Recognition",
            "rating": 1,
            "sanitized_title": "learning_transferable_architectures_for_scalable_image_recognition"
        },
        {
            "paper_title": "NASNet: Neural Architecture Search with Reinforcement Learning",
            "rating": 1,
            "sanitized_title": "nasnet_neural_architecture_search_with_reinforcement_learning"
        }
    ],
    "cost": 0.01582925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Automated Machine Learning Research
9 Sep 2024</p>
<p>Shervin Ardeshir 
Towards Automated Machine Learning Research
9 Sep 2024E91BA8091C449BC2FE3F0AD55AE1B159arXiv:2409.05258v1[cs.LG]
This paper explores a top-down approach to automating incremental advances in machine learning research through component-level innovation, facilitated by Large Language Models (LLMs).Our framework systematically generates novel components, validates their feasibility, and evaluates their performance against existing baselines.A key distinction of this approach lies in how these novel components are generated.Unlike traditional AutoML and NAS methods, which often rely on a bottom-up combinatorial search over predefined, hardcoded base components, our method leverages the cross-domain knowledge embedded in LLMs to propose new components that may not be confined to any hardcoded predefined set.By incorporating a reward model to prioritize promising hypotheses, we aim to improve the efficiency of the hypothesis generation and evaluation process.We hope this approach offers a new avenue for exploration and contributes to the ongoing dialogue in the field.</p>
<p>Introduction</p>
<p>Efficient hypothesis generation, validation, and evaluation are critical, yet resource-intensive, components of scientific discovery.In many scientific fields, these processes require substantial manual effort, as they often involve intricate experiments and extensive data collection.The ability to streamline these tasks could significantly accelerate the pace of innovation.</p>
<p>Machine learning offers a unique opportunity in this regard.Unlike other scientific domains, hypothesis validation in machine learning can be automated through code, with effectiveness measured numerically using objective criteria such as loss or accuracy.This capability makes machine learning an ideal field for exploring automation in research.</p>
<p>Building on this potential, we propose a framework that leverages a top-down methodology using Large Language Models (LLMs) to generate high-level hypotheses.Although our approach is not intended to replace bottomup methods such as AutoML-Zero (Real et al. 2020) or MetaQNN(Santoro et al. 2016), it offers a complementary path by introducing cross-domain innovation and a broader exploration of potential solutions.By formulating and testing hypotheses in natural language, our method lowers the barrier to entry for a wider range of researchers, fostering interdisciplinary collaboration and the integration of diverse knowledge from various fields.This combination of top-down and bottom-up strategies improves the research pipeline, providing a more comprehensive and innovative approach to automated machine learning research.</p>
<p>In this paper, we contribute by:</p>
<p>• Proposing and evaluating viable components: Generating viable hypotheses to replace neural network components and achieve competitive performance with known alternatives.</p>
<p>• Training a reward model: Learning patterns between the content of a hypothesis and its downstream performance.• Efficient Hypothesis Generation: Using the reward model to prune and prioritize hypotheses, improving the efficiency of generation, validation, and evaluation.</p>
<p>Caveats</p>
<p>1.This work does not make any assumptions about the inherent capabilities of LLMs to reason or have a deep understanding of ML topics.Even a random string generator can yield a meaningful hypothesis given unlimited attempts, akin to the infinite monkey theorem, which suggests that a monkey hitting keys at random on a typewriter for an infinite amount of time will almost surely type a given text, such as the complete works of Shakespeare.Our assumptions on the state of LLMs and ML are as follows.</p>
<p>(a) LLMs are good enough at generating feasible outputs, thus narrowing down our search space meaningfully from a set of random outputs.(b) LLMs (and ML models in general) are good enough in pattern recognition.Therefore, training a reward model on performance would allow the model to identify common patterns among successful hypotheses.</p>
<p>In short, we solely explore if LLMs can identify what would "look like" a good hypothesis based on the patterns that it has seen in previous examples.2. In this work, we solely lay the foundation and do not claim that our automated research necessarily would yield state-of-the-art (SOTA) results in machine learning research.We explore the feasibility of operationalizing steps involved in ML research to a level where the current state of LLMs can generate feasible hypotheses efficiently.3.This work was conducted using the authors' personal time and resources, limiting the scope to a small set of datasets and experiments.We hope that larger-scale experiments conducted at research labs interested in exploring this topic could further solidify this framework.</p>
<p>This work assumes an existing baseline solution and explores incremental innovations in its components, focusing on one component at a time.For example, in a neural network, the component of interest might be an activation function.We generate a set of viable alternatives (hypotheses) and evaluate their performance against baseline components.</p>
<p>The framework includes a generator for hypothesis creation, a validator to ensure basic validity, and an evaluator to measure success metrics such as validation loss.A reward model is trained to prioritize hypotheses that perform well compared to baselines, reducing computational burden while maintaining a high probability of discovering valuable new hypotheses.</p>
<p>While we manually verified and adjusted the validator and evaluator functions, the generation of hypotheses was not manually reviewed, highlighting the potential for fully automated research.This framework aims to accelerate innovation and make advanced machine learning techniques more accessible, with potential applications in various scientific discovery tasks.</p>
<p>Related Work</p>
<p>The field of automated machine learning (AutoML) (Liang et al. 2019) has rapidly advanced, automating processes such as data pre-processing, model selection, and hyperparameter tuning.Google's AutoML and Auto-Keras (Jin, Song, and Hu 2019) have made machine learning more accessible.AutoML-Zero (Real et al. 2020) and MetaQNN (Baker et al. 2016) take a bottom-up combinatorial approach to model construction, evolving algorithms, and network architectures from basic operations.In contrast, our work uses a top-down method, leveraging large language models (LLMs) to start with high-level concepts, allowing for broader exploration and the potential for cross-domain innovation.</p>
<p>Meta-Learning has also made significant strides, with foundational methods like MAML enabling fast task adaptation (Finn, Abbeel, and Levine 2017).Matching Networks (Vinyals et al. 2016) and Prototypical Networks (Snell, Swersky, and Zemel 2017) advanced few-shot learning, while optimization-based methods (Ravi and Larochelle 2016) and Memory-Augmented Neural Networks (Santoro et al. 2016) enhanced meta-learning capabilities.Simplified approaches like first-order meta-learning (Nichol, Achiam, and Schulman 2018) and Probabilistic MAML (Finn, Xu, and Levine 2018) further refined the field.</p>
<p>Neural Architecture Search (NAS) has progressed with approaches like NASNet (Zoph et al. 2018), ENAS (Pham et al. 2018), and DARTS (Liu, Simonyan, and Yang 2019), which introduced scalable and efficient architecture search methods.Auto-Keras (Jin, Song, and Hu 2019) and Prox-ylessNAS (Cai, Zhu, and Han 2019) made NAS more accessible and practical, while AmoebaNet (Real et al. 2019), MnasNet (Tan et al. 2019), and FBNet (Wu et al. 2019) pushed the boundaries of mobile and hardware-aware optimization.</p>
<p>Hyperparameter Optimization has evolved with Bayesian optimization (Bergstra et al. 2011), later improved by Snoek et al. (Snoek, Larochelle, and Adams 2012).Random search (Bergstra and Bengio 2012) provided a simpler alternative, while Hyperband (Li et al. 2018) and BOHB (Falkner, Klein, and Hutter 2018) optimized resource allocation.Gradient-based methods (Maclaurin, Duvenaud, and Adams 2015) and automated tuning for neural networks (Mendoza et al. 2016) further advanced the field.</p>
<p>The objective of our work aligns with ongoing efforts in AutoML, meta-learning, NAS, and hyperparameter optimization, however, it goes beyond those capabilities as our framework proposes and evaluates new components in a topdown approach and builds on the baseline state of the art.</p>
<p>Framework</p>
<p>The scope of this work begins when a specific area of machine learning research is selected, such as the development of a new activation function.The researcher selects this area and constructs a baseline set of solutions B = {b 1 , b 2 , . ..} that represent the current state-of-the-art or commonly used approaches.The goal is to generate a set of viable alternatives H = {h 1 , h 2 , . ..} to these baselines.Each proposed hypothesis h i is generated such that it can replace a component b j in the baseline solutions, such as substituting a new activation function in place of the standard ReLU in a neural network.This structured approach ensures that the generated hypotheses are directly relevant and potentially beneficial to the chosen area of research.</p>
<p>Our approach for generating and measuring the performance of each of the proposed hypotheses involves a generator, a validator, and an evaluator.A reward model is then trained to map the hypotheses to their success metrics measured by the evaluator.This reward is then used to improve the efficiency of the system by prioritizing more promising hypotheses solely by their content.In what follows we provide more details on each of these components.</p>
<p>The Generator</p>
<p>The generator is the mechanism through which a feasible hypothesis h i is reached and sent for validation and evaluation.Here, it is a language model prompted by natural language, optionally followed by a reward model.In the activation function case study, these hypotheses take the form of novel activation functions.LLMs are trained on a comprehensive corpus of existing activation functions and related mathematical formulations, enabling them to propose viable functions.</p>
<p>We experiment with two types of base prompts.The first type encourages the model to discover incremental proposed blocks, to which we refer to as Incrementality Encouraging Prompting (IEP for short).The following is an example of such a prompt, used for generating activation function blocks.</p>
<p>"define a python class that inherits from pytorch nn.Module.I should be able to use it as an activation function.Make sure if it has any parameters, all of them are set to default values so I can initialize without specifying any parameters.Try to come up with something that combines characteristics of Sigmoid/Tanh, ReLU, and ELU."</p>
<p>The second type of base prompt aims to reduce the likelihood of trivial incrementality.We refer to this as Novelty Encouraged Prompting (NEP) prompting.An example of such a prompt for activation function is as follows.</p>
<p>"define a python class that inherits from pytorch nn.Module.I should be able to use it as an activation function.Make sure if it has any params, all of them are set to default values so I can initialize without specifying any params.This function should not resemble common activation functions like ReLU, ELU, Sigmoid, or Tanh, and should explore unusual mathematical operations, combinations, or transformations.The expression can involve basic arithmetic, trigonometric functions, exponentials, or other non-linear operations, but avoid straightforward or commonly used forms in neural networks."</p>
<p>The generator then involves a few wrappers around this base prompt, to request the implementation code for the proposed hypothesis in a parsable way.Code 1 is an example of an activate function generated using the incrementality-encouraging base prompt.As prompted, the model clearly borrows characteristics from two commonly used activation functions of ReLU and Sigmoid.Code 1: An example of an auto-generated activation function using Incrementality Encouraged Prompting (IEP) Code 2 shows another activation function generated using the novelty-encouraged prompt.The model avoids directly using existing activation functions, adhering meaningfully to the instructions.While further exploration in prompt engineering is beyond this work's scope, we believe it could significantly reduce LLMs' inductive bias towards state-ofthe-art methods, minimize borrowing from existing literature, and encourage the proposal of less explored functions.Code 2: An example of an auto-generated activation function using Novelty Encouraged Prompting (NEP)   For more examples, please refer to the Appendix.</p>
<p>The Validator</p>
<p>Each proposed hypothesis is first passed through a validator function, denoted v.This function checks the validity of the hypothesis, ensuring it meets necessary criteria before further evaluation.For a proposed activation function
h i , the validator function v(h i ) returns a binary value in- dicating whether h i is a valid activation function (that is, v(h i ) ∈ {0, 1}).
For our case study on activation functions, we implement the validator manually and as a unittest1 .The validator checks if h i inherits from nn.Module, has the required initialization and forward functions, all its parameters have default values, and can pass a few basic test cases.We provide code snippets of this validator in the Appendix section Code 9.</p>
<p>The Evaluator</p>
<p>Valid hypotheses are fully evaluated using an evaluation function, denoted e.The goal of this function is to replace a baseline component b j with an alternative hypothesis h i and measure the performance of the model in the task(s) of interest.For our case, this translates to integrating the hypothesis into a machine learning model and measuring its performance, specifically its loss on the validation split of the dataset (val-loss).To streamline the process, we perform a single iteration of forward and backward passes to obtain a preliminary loss value, rather than conducting a full training and evaluation cycle.Formally, for a model m and an activation function h i , the val loss is calculated as e(m(h i )).</p>
<p>In our experiments, we hard-coded the architecture to a 2layer MLP, and used cross-entropy and MSE loss for a set of classification and regression tasks.We also define the problem as solving the classification and regression instances in a one-pass learning setup (only one epoch) to reduce the computation required for each h i and enable more extensive exploration across a larger set of hypotheses.</p>
<p>The Reward</p>
<p>Passing the set of hypotheses H to the evaluator function results in the collection of pairs of generated hypotheses (activation functions) and their corresponding validation losses, in the form of (h i , l(h i )).We define the reward as the win rate of the proposed hypothesis h i over the baselines.Specifically, we measure two metrics:</p>
<p>Baseline Win Rate (B-WR): This metric measures the percentage of times h i outperforms any given baseline b j across different tasks and over different runs.Formally, it measures P (l(h i ) &lt; l(b j )).</p>
<p>Baseline State-of-the-Art Win Rate (BSOTA-WR): This metric measures the win rate of the proposed hypothesis over the best runs of the entire baseline set B in each task / dataset.Formally, it is defined as P (l(h i ) &lt; min(l(b j ))| j=1,...,|B| ).Please note that the minimum operation is done on the average loss of different runs for each baseline, thus the best baseline run still contains a distribution of losses (resulting from several runs / random initialization), allowing for calculating a probabilistic win rate.</p>
<p>The reward model is then trained as a ranking model mapping the content of the hypothesis h i to its downstream performance (i.e.loss).In other words, this model is aimed to looking at the content of a proposed component (i.e.code of an activation function), and be able to predict how well it is likely to perform in terms of winning over the baseline set.Intuitively, the reward model learns patterns in the content of the proposed activation functions, leading to better performance.</p>
<p>Closing the Loop</p>
<p>In the initial round of hypothesis generation, every hypothesis is evaluated using a brute-force approach, where each one is passed through the validator and evaluator in a fully exploratory iteration.This process allows for comprehensive data collection, yielding the success metrics B-WR and BSOTA-WR for each validated and evaluated hypothesis.</p>
<p>We employ three LLMs to generate a dataset of 2000 validated and evaluated hypotheses for each component type.</p>
<p>Once this initial data is collected, the second iteration leverages a trained reward model to streamline the process.The reward model is used to prune the newly generated hypotheses, filtering them to select the top-k candidates based on their predicted performance.These top-k hypotheses, expected to be the most promising, are the only ones that proceed to the full evaluation phase, which involves more intensive computational resources.</p>
<p>Experiments</p>
<p>We aim to identify novel components that improve a simple neural network's performance across various tasks using a one-pass learning setup, where the model is trained for a single epoch.This approach enables rapid iteration and evaluation of numerous hypotheses.</p>
<p>Experimental Setup</p>
<p>Downstream Tasks and Datasets To validate the effectiveness of our framework, we performed experiments on six tasks using four well-known datasets, covering both classification and regression.</p>
<p>Iris Dataset: Classification task with 150 instances, 4 features, and 3 classes.</p>
<p>Wine Dataset: Used for both classification (3 classes) and regression, with 178 instances and 13 features.</p>
<p>Breast Cancer Dataset: Binary classification task with 569 instances and 30 features.</p>
<p>Diabetes Dataset: Regression task with 442 instances, each with 10 baseline variables, such as age, sex, body mass index (BMI), average blood pressure, and six blood serum measurements.The goal is to predict the progression of the disease one year after baseline.Generated Hypothesis Dataset Each hypothesis is evaluated to generate a dataset of 36,000 (hypothesis, reward) tuples over two iterations of 18,000 each.These are further divided into 3 LLMs, 3 component types, and 2 prompt types, with each combination generating 1,000 samples.</p>
<p>Components We experimented with three component types: activation functions, regularization functions, and preprocessing functions.Detailed prompts for each type are provided in the appendix.</p>
<p>Language Models and Prompts We used three language models: GPT-3.5 Turbo, GPT-4o and Gemini, to generate components, using two prompt types: incrementalityencouraging and novelty-encouraging (as described in Section ).</p>
<p>Architecture We employed a 2-layer fully-connected neural network with 64 and 16 units for all datasets for simplification.</p>
<p>Quantitative Results and Analysis</p>
<p>Our goal is to generate viable and high-performing proposed components, efficiently.In the following, we provide details on how we measure success in these aspects.The y-axis represents the BSOTA-WR, while the xaxis represents the B-WR.By definition, a hypothesis reaching 1 in one axis will reach 1 in the other.Also, as expected, it can be seen from the distributions that BSOTA-WR is generally a more difficult objective to achieve.</p>
<p>Performance: Component Evaluation As mentioned in the Reward Section , we use the two key metrics of Baseline Win-rate (B-WR for short), and Baseline State-of-theart win-rate (BSOTA-WR for short) to assess the effectiveness of each proposed block.Both metrics, BSOTA-WR and B-WR, are designed to provide a holistic view of the proposed method's performance, highlighting its potential to advance the state of the baseline set by setting new benchmarks and consistently outperforming the baseline set.Table. 1, contains the metrics for the set of components generated through the pipeline.We also report the success rate of the Validator, indicating the fraction of generated hypotheses that had the valid format.Please note that all of these metrics are averaged over the whole dataset of hypotheses generated in the first iteration (2000 samples generated for each component type).The performance of the individual components can be seen in the scatter plots provided in Figure .8. As can be observed, in the majority of cases the generated hypotheses have a low win rate compared to the baseline set, however, there are components with win rates very close to 1, indicating that they always outperform every single baseline individually and also the baseline state of the art.</p>
<p>Reward model Evaluation As mentioned earlier, the goal of the reward model is to learn to predict the performance of a proposed hypothesis solely from its content (code).To train such a model, we extract three different code embedding features from the content of the implementation code generated, namely CodeBERT(Feng et al. 2020), Graph-CodeBert (Guo et al. 2020), andCodeGen(Nijkamp et al. 2022).We report the results on ranking models trained on the concatenation of all three features in table 2. In the Appendix, we also provide ablation on the same metrics for each feature type, and also for preprocessors and regularizers.</p>
<p>We use established ranking metrics, including Kendall's Tau (k − τ ), Spearman correlation coefficient (SCC), and Pearson correlation coefficient (PCC), as reported in Tables 2, 3, and 2 for activation functions, preprocessing functions, and regularization functions respectively.From table 2, for activation, preprocessing, and regularization functions, respectively.These metrics show successful generalization of the ranking models across components generated by different language models.While reward models perform best on the datasets they were trained on, the consistently positive correlations across different language models demonstrate their robustness and generalization capability.Even when correlation values are modest, they remain directionally positive, indicating meaningful ranking performance.</p>
<p>Efficienct Hypothesis Evaluation</p>
<p>We also evaluate the efficiency of the reward model in terms of prioritizing the candidates in the second iteration.That means that we measure the performance of the top 50 candidates at each step if we were to sort the candidates based on their predicted reward.Intuitively, a good reward model would sort them in an order in which the top k candidates are more likely to be on the top of the list, therefore discovering the promising hypotheses earlier, resulting in a curve with a higher AUC. Figure .12 visualize the efficiency curves for the activation function on the datasets generated by the different LLMs separately.We provide similar curves for the datasets generated for other components (preprocessor and regularizer) in Figures 11 and 13 of the appendix.The x-axis in these figures shows the number of steps, and the y-axis is the reward (linear addition of BSOTA-WR + B-WR) for the top 50 hypotheses if they were to be prioritized by the reward model of interest.In all graphs, the blue curve shows how fast the pipeline reaches the high top-50 accuracies if there is no reward model used (chance/random reward).As it can be observed, all reward models for the activation functions lead to a faster discovery of better hypotheses, leading to higher AUC values.</p>
<p>Risks and Limitations</p>
<p>Given that this work is the first in the lane.here we cover some potential risks and limitations for this line of research.These risks may be even more prominent once models are trained or fine-tuned end-to-end in a closed-loop setup with minimal human involvement.</p>
<p>Shortcuts: Given the empirical nature of this approach, there's a possibility that the model might exploit existing shortcuts rather than genuinely innovative solutions.This could lead to overfitting to specific datasets or tasks without contributing to broader advancements.</p>
<p>Reward Collapse: During our experiments, we observed a significant issue with the generation of redundant and highly similar hypotheses, particularly when using GPT-3.5turbo to generate activation functions.As illustrated in Figure 5, the top-12 activation functions often exhibit striking similarities, indicating a lack of diversity in the generated hypotheses.This phenomenon, known as reward collapse, occurs when the reward model becomes overly focused on specific patterns, leading to a narrow exploration of the hypothesis space.The right side of the figure, shows the pairwise similarity between the top-12 candidates, it can be observed that the one difference component (highlighted in red) completely stands out both in terms of the shape of its activation function, and also in terms of its similarity to others in the embeddings space.Given this phenomenon, we did a preliminery exploration on whether we can construct a set of diverse activation functions by constructing a set iteratively and greedily as a trade-off of win rate and diversity.This greedy and iterative approach encourages the selection of hypotheses that are both high-performing and diverse, thereby promoting a broader and more thorough exploration of the hypothesis space.By balancing the exploitation of known successful solutions with the exploration of novel and potentially superior alternatives, this method helps mitigate the risk of reward collapse.The effectiveness of this approach is demonstrated in Figure 6, where the top-12 activation functions constructed with this method exhibit a greater diversity compared to the initial set.This suggests the possibility of preventing collapse in case of finetuning the generator (future work).</p>
<p>SOTA Inductive Bias and unintended plagiarism:</p>
<p>LLMs, trained on vast datasets, risk generating outputs that closely resemble existing works, leading to unintended plagiarism and a bias toward state-of-the-art (SOTA) methodologies.This limits innovation, as models may favor incremental changes over novel ideas.To address this, it's essential to build careful baseline sets and implement strong credit assignment.Prompt design also plays a key role; noveltyfocused prompts yield more diverse outputs, while those targeting incremental improvements often mirror existing literature.Refining prompts to avoid reliance on known solutions and explore new areas can reduce plagiarism and SOTA bias, encouraging truly innovative contributions.</p>
<p>Optimizing for incremental short term improvements: The empirical focus of this work, combined with the absence of strong theoretical constraints, creates a risk of prioritizing short-term, incremental gains over more significant, longterm advancements.This approach may lead to the discovery of surface-level improvements that offer marginal benefits, while potentially overlooking opportunities for groundbreaking innovations that could drive substantial progress in the field.</p>
<p>Future Work</p>
<p>Future research could focus on expanding the framework to other types of machine learning components beyond activation functions, preprocessors and regularizers, and including more complex architectures and diverse datasets.Additionally, refining the reward model to balance novelty and performance more effectively, and incorporating stronger theoretical constraints, could help mitigate risks like reward collapse and incremental bias.An intriguing direction for future work is fine-tuning the language model based on the reward signal, which could guide the model towards generating higher-quality and more innovative hypotheses.Moreover, ensuring that the embeddings extracted from the generated hypotheses are consistent with those of the backbone model could open the possibility for fully differentiable training, further enhancing the integration and efficiency of the framework.Further experiments could also explore constrcuting prompt engineering practices to reduce unintended plagiarism and inductive bias.And last but not least, a proper credit assignment framework would be a necessity for improviong this line of research.</p>
<p>Conclusion</p>
<p>This work introduces a framework for automating machine learning research by leveraging large language models to Figure 6: Top 12 activation functions generated after applying the greedy algorithm for balancing performance and diversity.The figure shows a diverse set of activation functions selected through an iterative process that maximizes both win rate and embedding distance from previously selected functions.This method mitigates redundancy and encourages the exploration of innovative and varied solutions, as reflected in the distinct characteristics of the top 12 functions.</p>
<p>generate, validate, and evaluate novel components.While the approach shows promise in enhancing the efficiency of hypothesis generation and evaluation, it also presents challenges, such as the risk of reward collapse and the tendency to prioritize incremental improvements.Addressing these issues through careful design, fine-tuning strategies, and future refinements-such as consistent embedding integration for fully differentiable training-will be key to realizing the full potential of this automated research paradigm.</p>
<p>• ELU-like Function for Negative Inputs: The ELU-like function defined as α(exp(x) − 1) is also smooth and differentiable for all real numbers.Its derivative is:
d dx (α(exp(x) − 1)) = α exp(x)
• Combination of Both Functions: The combination of these functions using a piecewise definition ensures that the function is differentiable.Since both components are differentiable, and the transition between them occurs at x = 0, the overall function is differentiable at x = 0. • Continuity at x = 0: At x = 0, both functions yield the same value if we choose α = 1:
sigmoid(0) = 1 1 + e 0 = 1 2 α(exp(0) − 1) = α(1 − 1) = 0 Therefore, if α = 1
2 , the function value is continuous at x = 0.</p>
<p>• Smooth Transition: The derivative at x = 0 for both functions should also match for smooth transition:
d dx sigmoid(0) = sigmoid(0)•(1−sigmoid(0)) = 1 2 • 1 2 = 1 4 d dx (α(exp(0) − 1)) = α exp(0) = α = 1 2
Therefore, the function transitions smoothly if we ensure the parameters are set appropriately.Given these properties, the Sigmoid-ELU activation function is fully differentiable and suitable for use in neural networks.</p>
<p>ScaledSinusoidalDecay: An activation function generated through NEP</p>
<p>Activation functions play a crucial role in neural networks by introducing nonlinearity, allowing the model to learn complex patterns in data.The ScaledSinusoidalDecay activation function is a novel approach that combines sinusoidal transformations with exponential decay, modulated by userdefined scaling and shifting parameters.This function is designed to introduce controlled non-linearity, making it a versatile choice for various deep-learning architectures.</p>
<p>Formal Definition</p>
<p>The ScaledSinusoidalDecay activation function is defined as follows:</p>
<p>Given an input x, the output y of the activation function is calculated as:
y = scale × sin(x) × exp(−|x|) + shift
where:</p>
<p>• scale is a parameter that controls the amplitude of the sinusoidal component.</p>
<p>Why ScaledSinusoidalDecay is a Good Activation Function</p>
<p>The ScaledSinusoidalDecay activation function offers several advantages that make it a strong candidate for deep  learning applications:</p>
<p>• Controlled Non-linearity: The sine component introduces periodic non-linearity, which can be beneficial for learning complex patterns that are not purely linear.This is particularly useful in applications where the relationship between input features and the output is oscillatory or involves repeated cycles.• Attenuation of Large Inputs: The exponential decay term exp(−|x|) serves to attenuate the influence of large input values, preventing them from dominating the output.This can lead to better stability during training, especially in scenarios where the input data contains large outliers.• Parameter Flexibility: The inclusion of the scale and shift parameters allows for fine-tuning the function's behavior to suit specific tasks.For instance, adjusting the scale can amplify or reduce the overall impact of the activation, while the shift can move the activation range to better align with the desired output.• Smooth Gradients: The combination of sine and exponential functions ensures that the gradients of the ScaledSinusoidalDecay activation function are smooth and continuous.This is advantageous for optimization algorithms like gradient descent, as it helps in avoiding issues related to vanishing or exploding gradients.• Regularization Effect: The exponential decay can act as a regularizer by suppressing the influence of extreme values.This can lead to more robust models that generalize better to unseen data, particularly in deep networks where overfitting is a concern.</p>
<p>Conclusion</p>
<p>The ScaledSinusoidalDecay activation function is a versatile and powerful tool in the design of neural networks.By combining sinusoidal non-linearity with exponential decay, and allowing for adjustable scaling and shifting, this function offers a unique blend of flexibility and control.It is particularly well-suited for tasks that require the learning of complex, cyclical patterns, or where the attenuation of large inputs is beneficial.Its smooth gradients and regularization properties further enhance its utility, making it a strong candidate for a wide range of deep learning applications.</p>
<p>NormalizedPCA: A Preprocessing Function generated through IEP</p>
<p>In the realm of data preprocessing, the 'NormalizedPCA' function provides a robust method for scaling and dimensionality reduction.This function combines two essential preprocessing steps: feature normalization and Principal Component Analysis (PCA).In this section, we introduce the 'NormalizedPCA' function, explain its benefits, and formalize its operations.</p>
<p>Introduction</p>
<p>The 'NormalizedPCA' function is designed to preprocess data by first normalizing the features and then applying PCA for dimensionality reduction.This two-step process ensures that the data is appropriately scaled and transformed, allowing for more effective analysis and modeling.</p>
<p>Function Overview</p>
<p>Given a dataset X ∈ R n×d , where n is the number of samples and d is the number of features, the 'NormalizedPCA' function performs the following operations: 1. <strong>Feature Normalization:</strong> The feature normalization step involves standardizing the features to have zero mean and unit variance.This is achieved using the StandardScaler:
xij = x ij − µ j σ j
where xij is the normalized feature value, x ij is the original feature value, µ j is the mean of the j-th feature, and σ j is the standard deviation of the j-th feature.</p>
<ol>
<li><strong>Dimensionality Reduction with PCA:</strong> After normalization, PCA is applied to reduce the dimensionality while preserving the maximum variance.PCA transforms the data X scaled to a lower-dimensional space:
X pca = X scaled W pca
where W pca contains the principal components (eigenvectors) corresponding to the largest eigenvalues of the covariance matrix of X scaled .</li>
</ol>
<p>Benefits of the 'NormalizedPCA' Function 1. <strong>Effective Scaling:</strong> Normalizing features ensures that all features contribute equally to the PCA, avoiding bias towards features with larger magnitudes.This scaling step is crucial because PCA is sensitive to the scale of the input features.</p>
<ol>
<li>
<p><strong>Improved Dimensionality Reduction:</strong> By applying PCA after normalization, the function effectively reduces the dimensionality while retaining the most significant variance.This results in a lower-dimensional representation that captures the essential structure of the data.</p>
</li>
<li>
<p><strong>Enhanced Model Performance:</strong> Proper normalization and dimensionality reduction improve the performance of machine learning models by reducing overfitting and speeding up convergence.Normalized data allows PCA to perform a more accurate reduction, leading to better generalization.</p>
</li>
<li>
<p><strong>Consistency and Interpretation:</strong> The combination of scaling and PCA provides a consistent and interpretable transformation of the data.Normalized features ensure that PCA components represent the true variance, making the results more meaningful and actionable.</p>
</li>
</ol>
<p>Formalization</p>
<p>Let X ∈ R n×d be the input data matrix.The preprocessing steps are as follows:</p>
<ol>
<li><strong>Normalize the Data:</strong>
X scaled = StandardScaler(X)
where each feature is scaled to have zero mean and unit variance.where PCA reduces the dimensionality based on the specified number of components or variance threshold.In summary, the 'NormalizedPCA' function provides a comprehensive preprocessing solution by combining scaling and PCA.This approach ensures that the data are properly prepared for subsequent analysis, improving the effectiveness of dimensionality reduction and enhancing overall model performance.Code 5: An example of an auto-generated activation function SineSquaredDecay Transformation: A pre-processing function generated through NEP</li>
</ol>
<p>In the realm of data preprocessing for machine learning, the choice of feature transformations can significantly impact model performance.One such transformation, which we term SineSquaredDecay, introduces a combination of non-linear operations and noise to the input data, creating a robust and diverse feature set.The SineSquaredDecay function is designed to transform input features in a way that captures complex patterns while also adding a degree of regularization to prevent overfitting.</p>
<p>Formal Definition</p>
<p>The SineSquaredDecay transformation is applied to each feature in the input data and can be formalized by the following equations: Given an input feature matrix X, the transformation for each feature x i in the training set train X and validation set val X is defined as:
train X transformed i = sin 2 (x i ) • exp(−|x i |) + σ • ϵ i val X transformed i = sin 2 (x i ) • exp(−|x i |) + σ • ϵ i
where: • sin 2 (x i ) applies a non-linear, periodic transformation to the input feature.• exp(−|x i |) introduces an exponential decay, which diminishes the influence of large feature values, ensuring that no single feature dominates the input space.</p>
<p>• σ represents the noise scale parameter, which controls the magnitude of the added Gaussian noise.• ϵ i is a Gaussian noise term drawn from a normal distribution ϵ i ∼ N (0, 1), added to the transformed features to enhance feature diversity and regularization.</p>
<p>Why SineSquaredDecay is a Good Choice for Preprocessing</p>
<p>The SineSquaredDecay transformation offers several advantages for preprocessing, particularly in scenarios where nonlinear relationships and feature regularization are critical:</p>
<p>• Capturing Complex Patterns: The use of the sine function, squared, introduces non-linear and periodic behavior into the features, which can help capture complex underlying patterns in the data.This is particularly useful in situations where the relationship between features and the target variable is not purely linear.• Feature Scaling and Regularization: The exponential decay term exp(−|x i |) ensures that the transformed features do not become excessively large, which can help in preventing certain features from overpowering others.This acts as an inherent regularization mechanism, making the feature set more balanced.• Noise Augmentation: The addition of Gaussian noise controlled by the noise scale parameter serves as a regularizer by slightly perturbing the input data.This prevents the model from overfitting to specific patterns in the training set, thereby improving generalization to unseen data.• Diverse Feature Representations: The combined effect of non-linear transformation and noise addition results in a rich and diverse feature set.This diversity can be particularly advantageous in ensemble models or in scenarios where the model benefits from a wide variety of input features.</p>
<p>Conclusion</p>
<p>The SineSquaredDecay transformation is a powerful tool for preprocessing in machine learning pipelines.Its ability to introduce complex non-linearities, combined with an effective regularization mechanism through noise, makes it a robust choice for enhancing model performance.By using this transformation, practitioners can create a feature space that is both rich in diversity and resilient to overfitting, ultimately leading to more effective and generalizable models.</p>
<p>Introduction</p>
<p>The DropWeightL2 regularization function integrates two distinct regularization techniques: dropout-like regularization applied directly to weights and L2 weight decay.By applying these methods simultaneously, DropWeightL2 seeks to improve model generalization and stability during training.</p>
<p>Function Overview</p>
<p>The DropWeightL2 function is defined as follows: Code 7: An example of an auto-generated activation function</p>
<p>Formalization</p>
<p>Let W ∈ R d×k represent the weight matrix of a layer, where d is the number of input features and k is the number of output features.The regularization loss introduced by the Drop-WeightL2 function is formulated as:
RegLoss = λ i,j w 2 ij + β i,j wij(1)
where: • λ is the weight penalty coefficient (L2 regularization strength).• w ij represents the weight value on the i -th row and j -th column.• wij represents the weight value after applying a dropoutlike mechanism.Mathematically, it can be modeled as: wij = w ij with probability (1 − p) 0 with probability p where p is the dropout rate.The total loss of regularization is accumulated in all layers of the model and the resulting value is added to the primary loss function during training.</p>
<p>where: • λ is the weight penalty coefficient (L2 regularization strength).• w ij represents the weight value on the i -th row and j -th column.• Dropout(w ij ) is the dropout-like effect applied to the weight w ij , introducing noise during regularization.The total loss of regularization is accumulated in all layers of the model and the resulting value is added to the primary loss function during training.</p>
<p>Conclusion</p>
<p>The DropWeightL2 regularization function offers a unique approach by integrating dropout-like regularization with L2 weight penalty.This dual regularization strategy improves the robustness of the model, prevents overfitting, and improves generalization.By applying both methods simultaneously, 'DropWeightL2' provides a comprehensive regularization solution that balances weight control with stochastic noise.</p>
<p>Figure 1 :
1
Figure 1: Framework overview: A set of hypotheses are generated to modify a specific component of an existing (hardcoded) framework.These hypotheses are tested for validity, evaluated for performance, and ranked, with the most promising candidates undergoing a full, computationally intensive evaluation.</p>
<p>Figure 2
2
Figure 2 visualizes the shape of the proposed activation functions.</p>
<p>Figure 2 :
2
Figure 2: Activation functions generated through Incrementality Encouraging (left) and Novelty Encouraging (right) prompting.</p>
<p>Figure 3 :
3
Figure 3: Scatter Plots of Win Rate Probabilities: These scatter plots illustrate the relationship between the Baseline Win Rate (B-WR) and the Baseline State-of-the-Art Win Rate (BSOTA-WR) for several hypotheses across different blocks.The y-axis represents the BSOTA-WR, while the xaxis represents the B-WR.By definition, a hypothesis reaching 1 in one axis will reach 1 in the other.Also, as expected, it can be seen from the distributions that BSOTA-WR is generally a more difficult objective to achieve.</p>
<p>Comparing validator passing rate and evaluator metrics based on the two types of prompting, for each component type.</p>
<p>Figure 4 :
4
Figure 4: Efficiency for the activation function Reward Models.Each graph illustrates how efficiently the respective reward model prioritizes high-performing hypotheses, under different scenarios of being trained and tested on different LLM-generated hypothesis dataset.</p>
<p>Figure 5 :
5
Figure 5: Left Panel: Graphical representation of the top-12 proposed activation functions in one of the runs.Each graph shows the shape of the activation function along with its Baseline State-Of-The-Art Win Rate (BSOTA-WR), and novelty score (N-score).The highlighted function (in red) demonstrates a notable balance between performance (BSOTA-WR: 0.931) and novelty (N-score: 0.034).Right Panel: Self-similarity heatmap of the top-12 activation functions.The color scale represents the degree of similarity, with yellow indicating high similarity and blue indicating lower similarity.This matrix helps to identify clusters of similar functions, highlighting the uniqueness of each proposed activation function.</p>
<p>Figure</p>
<p>Figure 7: Preprocessor</p>
<p>FigureFigure 10 :
10
Figure 8: Activation Function Figure 9: Regularizer Figure 10: Scatter Plots of Win Rate Probabilities: These scatter plots illustrate the relationship between the Baseline Win Rate (B-WR) and the Baseline State-of-the-Art Win Rate (BSOTA-WR) for several hypotheses across different blocks.The y-axis represents the BSOTA-WR, while the x-axis represents the B-WR.By definition, a hypothesis reaching 1 in one axis will reach 1 in the other.Also, it can be seen from the distributions that BSOTA-WR is generally a more difficult objective to achieve.</p>
<p>Figure 11 :
11
Figure 11: Efficiency for Preprocessor Reward Models.</p>
<p>•</p>
<p>sin(x) introduces a periodic, oscillatory behavior to the activation function.• exp(−|x|) is an exponential decay function that diminishes the output as the magnitude of the input increases.• shift is a parameter that shifts the output, providing additional flexibility in the function's range.Code 4: An example of an auto-generated activation function import torch import torch.nnas nn class ScaledSinusoidalDecay(nn.Module): def <strong>init</strong>(self, scale=1.0,shift=0.1):super(ScaledSinusoidalDecay, self).<strong>init</strong>()self.scale= scale self.shift= shift def forward(self, x): return self.scale<em> torch.sin(x)</em> torch.exp(-torch.abs(x))+ self.shift</p>
<p>Figure 12 :
12
Figure 12: Efficiency for activation function Reward Models.</p>
<p>Figure 13 :
13
Figure 13: Efficiency for regularizer function Reward Models.</p>
<ol>
<li><strong>Apply PCA:</strong> X pca = PCA(X scaled )</li>
</ol>
<p>import torch import torch.nnas nn class DropWeightL2(nn.Module): def <strong>init</strong>(self, dropout_rate=0.1,weight_penalty=0.01):super(DropWeightL2, self).<strong>init</strong>()self.dropout= nn.Dropout(dropout_rate) self.weight_penalty= weight_penalty def forward(self, model): reg_loss = 0.0 for param in model.parameters():if param.requires_grad:# Apply dropout to weights and calculate the penalty weight_penalty = self.weight_penalty<em> torch.sum(param</em><em> 2) reg_loss += weight_penalty # Apply dropout-like regularization reg_loss += torch.sum(self.dropout(param)) return reg_loss def <strong>call</strong>(self, model): return self.forward(model)Benefits of DropWeightL2 Regularization 1. </em><em>Enhanced Model Robustness:</em><em> -</em><em>Dropout-Like Regularization:</em><em> Although dropout is typically applied to activations, applying a similar dropout-like effect to weights introduces noise into the weight parameters.This encourages the network to be less reliant on specific weights, promoting robustness, and reducing the risk of overfitting.-</em><em>Effect:</em><em> This technique helps in regularizing the model by preventing it from fitting too closely to the training data and improving generalization.2.</em><em> Effective weight decline: ** -</em><em> L2 penalty: ** The term L2 weight penalty discourages large weights by adding a quadratic penalty to the loss function.This helps in controlling the complexity of the model and reducing overfitting.-</em><em>Effect:</em><em> Regularizing weights through L2 penalty improves model performance by constraining weight magnitudes, thereby simplifying the model and improving its generalization ability.3. </em><em>Combination of Techniques:</em><em> -</em><em>Dual Regularization:</em><em> Combining dropout-like behavior with L2 regularization leverages the strengths of both methods.Dropoutlike regularization introduces stochasticity into the weights, while L2 regularization ensures that weight magnitudes are kept in check.-</em><em>Effect:</em>* This combination can lead to better generalization by balancing the benefits of both techniques.</p>
<p>Table 2 :
2
Reward model performance across different datasets on the activation function block.Please find similar tables for the other blocks (pre-processor and regularization function) in the appendix.
train testgpt-3.5-turbogpt-4o-minigemini-prok-τ : (0.824, 0.000)k-τ : (0.627, 0.000)k-τ : (0.653, 0.000)gpt-3.5-turboSCC: (0.922, 0.000)SCC: (0.805, 0.000)SCC: (0.814, 0.000)PCC: (0.931, 0.000)PCC: (0.754, 0.000)PCC: (0.780, 0.000)k-τ : (0.284, 0.000)k-τ : (0.471, 0.000)k-τ : (0.122, 0.000)gpt-4o-miniSCC: (0.352, 0.000)SCC: (0.527, 0.000)SCC: (0.153, 0.000)PCC: (0.426, 0.000)PCC: (0.792, 0.000)PCC: (0.196, 0.000)k-τ : (0.315, 0.000)k-τ : (0.248, 0.000)k-τ : (0.505, 0.000)gemini-proSCC: (0.428, 0.000)SCC: (0.337, 0.000)SCC: (0.672, 0.000)PCC: (0.410, 0.000)PCC: (0.310, 0.000)PCC: (0.633, 0.000)</p>
<p>Table 3 :
3
Reward model performance across different datasets on the pre-processing function block
train testgpt-3.5-turbogpt-4o-minigemini-prok-τ : (0.606, 0.000)k-τ : (0.405, 0.000)k-τ : (0.432, 0.000)gpt-3.5-turboSCC: (0.740, 0.000)SCC: (0.550, 0.000)SCC: (0.600, 0.000)PCC: (0.773, 0.000)PCC: (0.521, 0.000)PCC: (0.645, 0.000)k-τ : (0.252, 0.000)k-τ : (0.395, 0.000)k-τ : (0.327, 0.000)gpt-4o-miniSCC: (0.346, 0.000)SCC: (0.524, 0.000)SCC: (0.442, 0.000)PCC: (0.271, 0.000)PCC: (0.592, 0.000)PCC: (0.459, 0.000)k-τ : (0.039, 0.486)k-τ : (0.139, 0.013)k-τ : (0.298, 0.000)gemini-proSCC: (0.066, 0.403)SCC: (0.193, 0.013)SCC: (0.403, 0.000)PCC: (0.105, 0.180)PCC: (0.203, 0.009)PCC: (0.415, 0.000)Train Test3000 gpt-3.5-turbo3000 gpt-4o-mini3000 gemini-prok-τ : (0.476, 9.95e-14)k-τ : (0.006, 0.846)k-τ : (0.075, 0.117)3000 gpt-3.5-turboSCC: (0.645, 1.03e-15)SCC: (0.008, 0.854)SCC: (0.100, 0.128)PCC: (0.648, 7.23e-16)PCC: (-0.008, 0.851)PCC: (0.104, 0.113)k-τ : (0.029, 0.645)k-τ : (0.397, 2.27e-35)k-τ : (-0.041, 0.395)3000 gpt-4o-miniSCC: (0.050, 0.587)SCC: (0.526, 8.85e-37)SCC: (-0.053, 0.418)PCC: (0.010, 0.910)PCC: (0.534, 5.28e-38)PCC: (-0.045, 0.492)
Fully automating this function is very feasible, but out of the scope of this effort.
Appendix Performance of the generated componentsAs mentioned in section we present a scatter plot visualization to compare the two success metrics for the hypotheses generated across different block types.Figure10illustrates the scatter plots for the pre-processor, activation, and regularization block types, respectively.Reward Model EfficiencyHere we provide efficiency curves for the preprocessor and regularize blocks in figure11and 13 respectively.It can be observed that similar to activation functions, the reward ranking model trained on the preprocessor blocks can effectively speed up the discovery of the most promising proposed components.However, when it comes to the regularizers, the trained reward models, especially the ones trained on the Gemini-pro dataset, fail to generalize to other datasets.We also provide the metrics for the rewards models in tables 3 and 4 respectively.Component ExamplesIn the following, we provide some auto-generated justification for why one of the hypotheses that has worked well, is a good option.Sigmoid-ELU (SigELU): An activation function generated through IEP DefinitionThe Sigmoid-ELU (SigELU): An Activation Function is a hybrid activation function that combines the Sigmoid function for non-negative inputs and the Exponential Linear Unit (ELU) function for negative inputs.FormulaThe activation function is defined as:JustificationThe name Sigmoid-ELU Activation (SigELU) reflects the combination of the Sigmoid function for non-negative inputs and the ELU function for negative inputs:• Sigmoid for Non-Negative Inputs: The Sigmoid function is well-known for its smooth, bounded output between 0 and 1.It is particularly useful for squashing input values to a manageable range, which can help stabilize the training process and make the model's output more interpretable in certain contexts.Differentiability Analysis• Sigmoid Function for Non-Negative Inputs: The Sigmoid function, defined as sigmoid(x) = 1 1+e −x , is a smooth and differentiable function for all real numbers.Its derivative is given by:SineDecay: A regularizer generated through NEPRegularization is a key technique in machine learning, particularly in deep learning, where it helps prevent overfitting and improves the generalization capabilities of models.The SineDecay Regularizer is a novel approach that combines sine transformations with exponential decay to regularize the parameters of a neural network.This regularizer introduces periodicity and attenuates large parameter values, providing a unique mechanism for controlling model complexity.Formal DefinitionThe SineDecay Regularizer is applied to the parameters of a neural network model.The regularization loss, reg loss, is computed by summing the sine-transformed and exponentially decayed values of the model's parameters.Formally, the regularization loss is defined as follows:where:• θ ij represents the j-th parameter of the i-th layer in the model.• N is the number of layers in the model.• M i is the number of parameters in the i-th layer.• scale is a hyperparameter that controls the amplitude of the sine transformation.• decay is a hyperparameter that determines the rate of exponential decay, attenuating the influence of large parameters.Why SineDecay is a Good RegularizerThe SineDecay Regularizer offers several advantages that make it a valuable tool for enhancing the performance and robustness of neural network models:• Encouraging Smoothness and Periodicity: The sine transformation encourages the parameters to adopt smoother, periodic distributions.This can be particularly beneficial for models dealing with data that has inherent periodicity or cyclical patterns.• Attenuation of Large Parameters: The exponential decay component reduces the impact of large parameter values on the regularization loss.This helps in preventing overfitting by discouraging the development of overly large weights, which can dominate the model's output.• Parameter Diversity: By combining sine and exponential decay, the regularizer introduces diversity in the parameter values, which can lead to a more robust and generalizable model.This is especially useful in complex models where standard regularizers like L1 or L2 might not be sufficient.• Flexibility with Hyperparameters: The scale and decay hyperparameters offer flexibility in tuning the regularizer's effect.This allows practitioners to adjust the regularization strength to suit the specific needs of their model and dataset.ImplementationThe following is the implementation of the SineDecay Regularizer in Python using PyTorch: Code 8: An example of an auto-generated activation functionConclusionThe SineDecay Regularizer is a powerful and flexible tool for regularizing neural network models.Using the periodic nature of the sine function and the attenuating effect of exponential decay, this regularizer provides a unique approach to controlling model complexity and improving generalization.Its ability to encourage smooth, diverse parameter values while mitigating the risk of overfitting makes it a valuable addition to the regularization techniques available in deep learning.ValidatorAn example of the validator function implemented to verify the validity of activation function hypotheses can be seen in the following: Code 9: The validator for the generated activation functions
B Baker, O Gupta, N Naik, R Raskar, arXiv:1611.02167Designing neural network architectures using reinforcement learning. 2016arXiv preprint</p>
<p>Algorithms for hyper-parameter optimization. J Bergstra, R Bardenet, Y Bengio, B Kégl, Advances in neural information processing systems. 201124</p>
<p>Random search for hyperparameter optimization. J Bergstra, Y Bengio, Journal of machine learning research. 1322012</p>
<p>ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. H Cai, L Zhu, S Han, International Conference on Learning Representations. 2019</p>
<p>BOHB: Robust and efficient hyperparameter optimization at scale. S Falkner, A Klein, F Hutter, International conference on machine learning. 2018</p>
<p>Pmlr, Z Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, arXiv:2002.08155Codebert: A pre-trained model for programming and natural languages. 2020arXiv preprint</p>
<p>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. C Finn, P Abbeel, S Levine, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningPMLR2017</p>
<p>Probabilistic modelagnostic meta-learning. Advances in neural information processing systems. C Finn, K Xu, S Levine, 201831</p>
<p>D Guo, S Ren, S Lu, Z Feng, D Tang, S Liu, L Zhou, N Duan, A Svyatkovskiy, S Fu, arXiv:2009.08366Graphcodebert: Pre-training code representations with data flow. 2020arXiv preprint</p>
<p>Auto-keras: An efficient neural architecture search system. H Jin, Q Song, X Hu, Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining2019</p>
<p>Hyperband: A novel bandit-based approach to hyperparameter optimization. L Li, K Jamieson, G Desalvo, A Rostamizadeh, A Talwalkar, Journal of Machine Learning Research. 181852018</p>
<p>Evolutionary neural automl for deep learning. J Liang, E Meyerson, B Hodjat, D Fink, K Mutch, R Miikkulainen, Proceedings of the genetic and evolutionary computation conference. the genetic and evolutionary computation conference2019</p>
<p>Gradient-based hyperparameter optimization through reversible learning. H Liu, K Simonyan, Y Yang, D Maclaurin, D Duvenaud, R Adams, International Conference on Learning Representations. PMLR2019. 2015International conference on machine learning</p>
<p>Towards automatically-tuned neural networks. H Mendoza, A Klein, M Feurer, J T Springenberg, F Hutter, Workshop on automatic machine learning. 2016</p>
<p>Codegen: An open large language model for code with multi-turn program syn. Pmlr, A Nichol, J Achiam, J Schulman, E Nijkamp, B Pang, H Hayashi, L Tu, H Wang, Y Zhou, S Savarese, C Xiong, H Pham, M Y Guan, B Zoph, Q V Le, J Dean, arXiv:1803.02999arXiv:2203.13474International Conference on Machine Learning. PMLR2018. 2022. 2018thesis. arXiv preprintEfficient Neural Architecture Search via Parameter Sharing</p>
<p>Regularized Evolution for Image Classifier Architecture Search. S Ravi, H Larochelle, E Real, A Aggarwal, Y Huang, Q V Le, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2016. 201933International conference on learning representations</p>
<p>Automlzero: Evolving machine learning algorithms from scratch. E Real, C Liang, D So, Q Le, International conference on machine learning. 2020</p>
<p>Meta-learning with memory-augmented neural networks. Pmlr, A Santoro, S Bartunov, M Botvinick, D Wierstra, T Lillicrap, International conference on machine learning. PMLR2016</p>
<p>Practical bayesian optimization of machine learning algorithms. J Snell, K Swersky, R Zemel, J Snoek, H Larochelle, R P Adams, M Tan, B Chen, R Pang, V Vasudevan, M Sandler, A Howard, Q V Le, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2017. 2012. 2019MnasNet: Platform-Aware Neural Architecture Search for Mobile</p>
<p>Matching Networks for One Shot Learning. O Vinyals, C Blundell, T Lillicrap, K Kavukcuoglu, D Wierstra, Advances in Neural Information Processing Systems. 2016</p>
<p>FB-Net: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search. B Wu, X Dai, P Zhang, Y Wang, F Sun, Y Wu, Y Tian, P Vajda, Y Jia, K Keutzer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Learning Transferable Architectures for Scalable Image Recognition. B Zoph, V Vasudevan, J Shlens, Q V Le, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Reward model performance across different datasets for the regularizer function block. Table. 4</p>            </div>
        </div>

    </div>
</body>
</html>
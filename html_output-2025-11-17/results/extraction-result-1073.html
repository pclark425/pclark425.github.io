<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1073 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1073</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1073</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-38c7edea7b8cf73a1aad204aa752fbccd31f2cad</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/38c7edea7b8cf73a1aad204aa752fbccd31f2cad" target="_blank">Learning a Set of Interrelated Tasks by Using Sequences of Motor Policies for a Strategic Intrinsically Motivated Learner</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Robotic Computing</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces a framework called ""procedures", which are sequences of policies defined by the combination of previously learned skills, which is capable of tackling the learning of complex motor policies, and shows that it helps the learner to tackle difficult hierarchical tasks.</p>
                <p><strong>Paper Abstract:</strong> We propose an active learning architecture for robots, capable of organizing its learning process to achieve a field of complex tasks by learning sequences of motor policies, called Intrinsically Motivated Procedure Babbling (IM-PB). The learner can generalize over its experience to continuously learn new tasks. It chooses actively what and how to learn based by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated tasks outcomes hierarchically organized. We introduce a framework called ""procedures", which are sequences of policies defined by the combination of previously learned skills. Our algorithmic architecture uses the procedures to autonomously discover how to combine simple skills to achieve complex goals. It actively chooses between 2 strategies of goal-directed exploration: exploration of the policy space or the procedural space. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our ""procedures"" framework helps the learner to tackle difficult hierarchical tasks.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1073.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1073.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IM-PB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intrinsically Motivated Procedure Babbling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strategic intrinsically motivated learner that constructs and executes 'procedures' (sequences of previously learned policies) and alternates goal-directed exploration in policy-space and procedural-space driven by empirical learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IM-PB learner (controller for simulated robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An intrinsically motivated, strategic active learner that learns inverse and forward models mapping parametrized motor policies to task outcomes. It selects goals and a strategy (policy-space exploration or procedural-space exploration), builds/optimizes policies or procedures using random exploration or local linear regression (SAGG-RIAC style local optimization), and uses a competence-progress based interest model to choose subsequent goals and strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (controller of a simulated 3-joint planar robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated robotic arm environment (objects: pen, floor, two joysticks, video-game character)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 3-DOF planar robotic arm interacts with objects in a bounded 3D workspace (x,y,z in [-1,1]). Objects include a pen (which can draw or break), a floor with z > -0.2 constraint, two joysticks whose positions control a 2D video-game character, and a drawing outcome recorded when the pen moves on the floor. Tasks (outcomes) are hierarchically organized into six outcome-spaces Ω0..Ω5 of varying dimensionalities (single-point positions, joystick positions, drawing endpoints, and game-character 2D position). Policies are sequences of DMP-based primitives; complex policies are arbitrary-length concatenations of primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task complexity is characterized by hierarchical task structure and task-space dimensionality (Ω0..Ω5); policy complexity is measured by policy length (number of primitives chained). Quantitative specifics: benchmark of 27,600 goal points across outcome spaces; policy size n used in performance cost term perf = d(ω,ω_g) * γ^n with γ=1.2. Evaluations run over 25,000 complex policy executions (5 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>multi-level (range from low for Ω0 — single primitive reach — to higher for Ω1, Ω2, Ω5 which are described as hierarchical/complex); the paper treats complexity as low→high across Ω0→Ω2/Ω5.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is instantiated as diversity of target outcomes across hierarchical outcome-spaces (different Ω_i with different dimensionalities and subtasks). There is no manipulation of distinct environment instances; the variation is across goals/tasks rather than environment instances.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>moderate (multiple distinct task/outcome spaces provided within a single, fixed environment); environment instance variation is low (single simulated environment).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean Euclidean distance from a held-out benchmark set: mean normalized Euclidean distance between each benchmark outcome and its nearest neighbour in the learner's dataset; internal selection metric perf = d(ω,ω_g)*γ^n (penalizes longer policies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: IM-PB achieves the lowest mean benchmark error among tested algorithms (RandomPolicy, SAGG-RIAC, Random-PB, IM-PB) across learning (plots show lower error since the beginning and across outcome-spaces). No absolute numeric mean-error values are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper explicitly studies how policy complexity adapts to task complexity: more complex/hierarchical outcomes (Ω1, Ω2) prompt selection of longer policies by IM-PB. It does not manipulate environment-instance variation independently, so no explicit trade-off between "environment complexity" and "environment variation" is evaluated; the principal relationship reported is that hierarchical task complexity motivates increased policy length, and that 'procedures' (reusing skills) reduce learning difficulty for hierarchical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic motivation + strategic learning: alternation between goal-directed policy-space exploration (SAGG-RIAC style with local linear regression) and procedural-space exploration (constructing/optimizing sequences of known subtasks), with competence-progress-based goal and strategy selection; episodic storage and region-based interest mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>The paper evaluates coverage/generalization over the continuous outcome spaces via a held-out benchmark (27,600 points) by measuring nearest-neighbour distances in the collected dataset, but it does not test transfer to novel environment instances; thus no out-of-environment generalization results are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Experiments ran 25,000 complex policy executions per run (5 runs). Benchmark coverage computed over 27,600 points. Additional analysis sampled 1,000,000 goals per selected subspace to study selected policy sizes. No wall-clock or episode-to-success rate numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Procedures (concatenation/reuse of previously learned policies) materially improve learning of hierarchical tasks: IM-PB and Random-PB outperform learners without procedures (SAGG-RIAC, RandomPolicy). 2) IM-PB adapts policy complexity to task complexity (chooses longer policies for more hierarchical outcome spaces). 3) The learner uses a cost-penalized performance metric (γ^n) to avoid unnecessarily long policies. 4) The study concerns variation across goal/task types within a single environment; explicit environment-instance variation is not explored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning a Set of Interrelated Tasks by Using Sequences of Motor Policies for a Strategic Intrinsically Motivated Learner', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1073.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1073.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-PB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Procedure Babbling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that explores both policy-space and procedural-space randomly, creating procedures by randomly sampling two subtasks and concatenating corresponding policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random-PB baseline (controller for simulated robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A learner variant that randomly explores primitive policies and randomly constructs procedures (concatenations of two subtasks) without intrinsic-motivation-driven strategic selection; used as a baseline to assess the impact of procedures vs. strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (controller of the same 3-joint robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulated robotic arm environment (pen, joysticks, drawing, video-game character)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Identical to IM-PB's environment: single simulated workspace with hierarchical outcome spaces Ω0..Ω5; variation arises from different target outcome goals within these spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same task hierarchy and policy-size notions as IM-PB; complexity of tasks measured via outcome-space dimensionality and hierarchy (Ω0..Ω5); procedurally generated policy size when concatenating two subtasks increases policy length.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>multi-level (same as IM-PB), from low (Ω0) to higher (Ω1, Ω2, Ω5).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation only across target outcomes/goals in the same environment; procedures introduce combinatorial variations of policy sequences but environment instances are not varied.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>moderate (multiple goal types) and low for environment-instance variation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean normalized Euclidean distance to benchmark outcomes (same benchmark of 27,600 points); compared across algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: Random-PB outperforms RandomPolicy and SAGG-RIAC (which lack procedures) showing lower mean benchmark error, but performs worse than IM-PB which adds strategic selection. No numeric error values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Random-PB shows that simple addition of procedures (without progress-driven strategy) improves performance on hierarchical tasks compared to learners without procedures, indicating that procedural combination eases exploration in complex task spaces; no explicit analysis of variation-level trade-offs is performed.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Random exploration of primitive policies and random construction/execution of two-subtask procedures (no intrinsic-motivation strategy selection).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated via nearest-neighbour coverage over benchmark goals; Random-PB shows better coverage than non-procedure baselines but worse than IM-PB; no cross-environment generalization tested.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Same experimental budget: 25,000 complex policy executions per run (5 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Introducing procedures even with random exploration improves the ability to reach hierarchical outcomes compared to exploration without procedures, indicating that chaining known skills eases reaching complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning a Set of Interrelated Tasks by Using Sequences of Motor Policies for a Strategic Intrinsically Motivated Learner', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1073.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1073.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGG-RIAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsically motivated goal-directed exploration algorithm that uses competence progress to select goals and local optimization (local linear regression) to improve policies in the policy parameter space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Intrinsically motivated goal exploration for active motor learning in robots: A case study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAGG-RIAC learner (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An IM (intrinsically motivated) goal-babbling algorithm that explores the policy parameter space guided by measures of empirical learning progress and performs local optimization via local linear regression to reach self-generated goals; used here as a baseline that does not use procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (controller of the 3-joint robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulated robotic arm environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same environment with hierarchical outcome-spaces; SAGG-RIAC explores policies directly to reach goals in Ω_i without composing procedures from previously learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task complexity measured via outcome-space dimensionality and hierarchy (Ω0..Ω5); SAGG-RIAC's efficiency is known to degrade with higher outcome-space dimensionality (curse of dimensionality, cited in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>effective for low-to-moderate complexity tasks (works well for Ω0), performance declines for more hierarchical/multi-dimensional outcome-spaces (Ω1, Ω2, Ω5).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is across selected target outcomes; no environment-instance variation. The algorithm's sensitivity to outcome-space dimensionality (variation in goal type/dimension) is noted.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>moderate (multiple goal types) but algorithm not tested across multiple environment instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean normalized Euclidean distance to benchmark outcomes (27,600-point benchmark); internal competence-progress metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: SAGG-RIAC performs worse than IM-PB and Random-PB on hierarchical outcome spaces; performs similarly to RandomPolicy on many outcome spaces except Ω0. No numeric values given.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reiterates known limitation: performance of goal-babbling approaches like SAGG-RIAC drops as outcome-space dimensionality increases (curse of dimensionality). The study shows SAGG-RIAC without procedures learns less on hierarchical tasks compared to approaches with procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic-motivation driven goal babbling with local policy optimization (SAGG-RIAC framework).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Assessed via benchmark coverage; shows poorer coverage on hierarchical outcome spaces relative to procedure-using approaches; no cross-environment generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Evaluated under same budget (25,000 complex policy executions); reported to be less efficient than IM-PB on hierarchical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SAGG-RIAC without procedural composition suffers on hierarchical/multi-dimensional outcome spaces due to curse of dimensionality; adding mechanisms that reuse and chain skills (procedures) improves learning in such setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning a Set of Interrelated Tasks by Using Sequences of Motor Policies for a Strategic Intrinsically Motivated Learner', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1073.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1073.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RandomPolicy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Policy Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline learner that samples motor policies randomly from the policy parameter space Π without intrinsic-motivation or procedural composition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RandomPolicy baseline (controller for simulated robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simple baseline that explores the policy parameter space by random sampling of primitive and complex policies without goal selection, local optimization, or procedure composition.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (controller of the 3-joint robotic arm)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulated robotic arm environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same fixed simulated workspace with hierarchical outcome-spaces; RandomPolicy explores by executing randomly sampled policies and records outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task complexity as hierarchical outcome-space dimensionality; policy complexity as length of concatenated primitives when random policies are sampled.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>capable of reaching simple tasks (Ω0) but inefficient for hierarchical tasks (Ω1, Ω2, Ω5).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation arises from goal diversity across outcome spaces; environment-instance variation is not manipulated.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>moderate for goal variation, low for environment variation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean normalized Euclidean distance to benchmark outcomes (27,600-point benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: RandomPolicy yields higher mean error than procedure-using learners (Random-PB and IM-PB) and similar or worse performance than SAGG-RIAC on many outcome-spaces; specific numeric errors not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>RandomPolicy performs acceptably on lowest-complexity tasks (Ω0) but poorly on hierarchical/more complex tasks, demonstrating that random exploration struggles as task complexity increases; no explicit study of variation vs complexity trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Pure random exploration of the policy parameter space.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated via benchmark coverage; RandomPolicy shows poor coverage on hierarchical outcome-spaces compared to procedure-enabled methods.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Same experimental budget: 25,000 complex policy executions per run (5 runs); sample-inefficient for hierarchical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random exploration is insufficient to effectively learn hierarchical, high-dimensional outcome-spaces; composition mechanisms (procedures) and strategic goal selection markedly improve learning efficiency and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning a Set of Interrelated Tasks by Using Sequences of Motor Policies for a Strategic Intrinsically Motivated Learner', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Intrinsically motivated goal exploration for active motor learning in robots: A case study <em>(Rating: 2)</em></li>
                <li>Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner <em>(Rating: 2)</em></li>
                <li>The Strategic Student Approach for Life-Long Exploration and Learning <em>(Rating: 2)</em></li>
                <li>Active learning of inverse models with intrinsically motivated goal exploration in robots <em>(Rating: 2)</em></li>
                <li>Strategic and interactive learning of a hierarchical set of tasks by the Poppy humanoid robot <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1073",
    "paper_id": "paper-38c7edea7b8cf73a1aad204aa752fbccd31f2cad",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "IM-PB",
            "name_full": "Intrinsically Motivated Procedure Babbling",
            "brief_description": "A strategic intrinsically motivated learner that constructs and executes 'procedures' (sequences of previously learned policies) and alternates goal-directed exploration in policy-space and procedural-space driven by empirical learning progress.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "IM-PB learner (controller for simulated robotic arm)",
            "agent_description": "An intrinsically motivated, strategic active learner that learns inverse and forward models mapping parametrized motor policies to task outcomes. It selects goals and a strategy (policy-space exploration or procedural-space exploration), builds/optimizes policies or procedures using random exploration or local linear regression (SAGG-RIAC style local optimization), and uses a competence-progress based interest model to choose subsequent goals and strategies.",
            "agent_type": "simulated agent (controller of a simulated 3-joint planar robotic arm)",
            "environment_name": "Simulated robotic arm environment (objects: pen, floor, two joysticks, video-game character)",
            "environment_description": "A 3-DOF planar robotic arm interacts with objects in a bounded 3D workspace (x,y,z in [-1,1]). Objects include a pen (which can draw or break), a floor with z &gt; -0.2 constraint, two joysticks whose positions control a 2D video-game character, and a drawing outcome recorded when the pen moves on the floor. Tasks (outcomes) are hierarchically organized into six outcome-spaces Ω0..Ω5 of varying dimensionalities (single-point positions, joystick positions, drawing endpoints, and game-character 2D position). Policies are sequences of DMP-based primitives; complex policies are arbitrary-length concatenations of primitives.",
            "complexity_measure": "Task complexity is characterized by hierarchical task structure and task-space dimensionality (Ω0..Ω5); policy complexity is measured by policy length (number of primitives chained). Quantitative specifics: benchmark of 27,600 goal points across outcome spaces; policy size n used in performance cost term perf = d(ω,ω_g) * γ^n with γ=1.2. Evaluations run over 25,000 complex policy executions (5 runs).",
            "complexity_level": "multi-level (range from low for Ω0 — single primitive reach — to higher for Ω1, Ω2, Ω5 which are described as hierarchical/complex); the paper treats complexity as low→high across Ω0→Ω2/Ω5.",
            "variation_measure": "Variation is instantiated as diversity of target outcomes across hierarchical outcome-spaces (different Ω_i with different dimensionalities and subtasks). There is no manipulation of distinct environment instances; the variation is across goals/tasks rather than environment instances.",
            "variation_level": "moderate (multiple distinct task/outcome spaces provided within a single, fixed environment); environment instance variation is low (single simulated environment).",
            "performance_metric": "Mean Euclidean distance from a held-out benchmark set: mean normalized Euclidean distance between each benchmark outcome and its nearest neighbour in the learner's dataset; internal selection metric perf = d(ω,ω_g)*γ^n (penalizes longer policies).",
            "performance_value": "Qualitative: IM-PB achieves the lowest mean benchmark error among tested algorithms (RandomPolicy, SAGG-RIAC, Random-PB, IM-PB) across learning (plots show lower error since the beginning and across outcome-spaces). No absolute numeric mean-error values are reported in the paper.",
            "complexity_variation_relationship": "The paper explicitly studies how policy complexity adapts to task complexity: more complex/hierarchical outcomes (Ω1, Ω2) prompt selection of longer policies by IM-PB. It does not manipulate environment-instance variation independently, so no explicit trade-off between \"environment complexity\" and \"environment variation\" is evaluated; the principal relationship reported is that hierarchical task complexity motivates increased policy length, and that 'procedures' (reusing skills) reduce learning difficulty for hierarchical tasks.",
            "high_complexity_low_variation_performance": "null",
            "low_complexity_high_variation_performance": "null",
            "high_complexity_high_variation_performance": "null",
            "low_complexity_low_variation_performance": "null",
            "training_strategy": "Intrinsic motivation + strategic learning: alternation between goal-directed policy-space exploration (SAGG-RIAC style with local linear regression) and procedural-space exploration (constructing/optimizing sequences of known subtasks), with competence-progress-based goal and strategy selection; episodic storage and region-based interest mapping.",
            "generalization_tested": false,
            "generalization_results": "The paper evaluates coverage/generalization over the continuous outcome spaces via a held-out benchmark (27,600 points) by measuring nearest-neighbour distances in the collected dataset, but it does not test transfer to novel environment instances; thus no out-of-environment generalization results are provided.",
            "sample_efficiency": "Experiments ran 25,000 complex policy executions per run (5 runs). Benchmark coverage computed over 27,600 points. Additional analysis sampled 1,000,000 goals per selected subspace to study selected policy sizes. No wall-clock or episode-to-success rate numbers are provided.",
            "key_findings": "1) Procedures (concatenation/reuse of previously learned policies) materially improve learning of hierarchical tasks: IM-PB and Random-PB outperform learners without procedures (SAGG-RIAC, RandomPolicy). 2) IM-PB adapts policy complexity to task complexity (chooses longer policies for more hierarchical outcome spaces). 3) The learner uses a cost-penalized performance metric (γ^n) to avoid unnecessarily long policies. 4) The study concerns variation across goal/task types within a single environment; explicit environment-instance variation is not explored.",
            "uuid": "e1073.0",
            "source_info": {
                "paper_title": "Learning a Set of Interrelated Tasks by Using Sequences of Motor Policies for a Strategic Intrinsically Motivated Learner",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Random-PB",
            "name_full": "Random Procedure Babbling",
            "brief_description": "A baseline that explores both policy-space and procedural-space randomly, creating procedures by randomly sampling two subtasks and concatenating corresponding policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Random-PB baseline (controller for simulated robotic arm)",
            "agent_description": "A learner variant that randomly explores primitive policies and randomly constructs procedures (concatenations of two subtasks) without intrinsic-motivation-driven strategic selection; used as a baseline to assess the impact of procedures vs. strategy.",
            "agent_type": "simulated agent (controller of the same 3-joint robotic arm)",
            "environment_name": "Same simulated robotic arm environment (pen, joysticks, drawing, video-game character)",
            "environment_description": "Identical to IM-PB's environment: single simulated workspace with hierarchical outcome spaces Ω0..Ω5; variation arises from different target outcome goals within these spaces.",
            "complexity_measure": "Same task hierarchy and policy-size notions as IM-PB; complexity of tasks measured via outcome-space dimensionality and hierarchy (Ω0..Ω5); procedurally generated policy size when concatenating two subtasks increases policy length.",
            "complexity_level": "multi-level (same as IM-PB), from low (Ω0) to higher (Ω1, Ω2, Ω5).",
            "variation_measure": "Variation only across target outcomes/goals in the same environment; procedures introduce combinatorial variations of policy sequences but environment instances are not varied.",
            "variation_level": "moderate (multiple goal types) and low for environment-instance variation.",
            "performance_metric": "Mean normalized Euclidean distance to benchmark outcomes (same benchmark of 27,600 points); compared across algorithms.",
            "performance_value": "Qualitative: Random-PB outperforms RandomPolicy and SAGG-RIAC (which lack procedures) showing lower mean benchmark error, but performs worse than IM-PB which adds strategic selection. No numeric error values reported.",
            "complexity_variation_relationship": "Random-PB shows that simple addition of procedures (without progress-driven strategy) improves performance on hierarchical tasks compared to learners without procedures, indicating that procedural combination eases exploration in complex task spaces; no explicit analysis of variation-level trade-offs is performed.",
            "high_complexity_low_variation_performance": "null",
            "low_complexity_high_variation_performance": "null",
            "high_complexity_high_variation_performance": "null",
            "low_complexity_low_variation_performance": "null",
            "training_strategy": "Random exploration of primitive policies and random construction/execution of two-subtask procedures (no intrinsic-motivation strategy selection).",
            "generalization_tested": false,
            "generalization_results": "Evaluated via nearest-neighbour coverage over benchmark goals; Random-PB shows better coverage than non-procedure baselines but worse than IM-PB; no cross-environment generalization tested.",
            "sample_efficiency": "Same experimental budget: 25,000 complex policy executions per run (5 runs).",
            "key_findings": "Introducing procedures even with random exploration improves the ability to reach hierarchical outcomes compared to exploration without procedures, indicating that chaining known skills eases reaching complex tasks.",
            "uuid": "e1073.1",
            "source_info": {
                "paper_title": "Learning a Set of Interrelated Tasks by Using Sequences of Motor Policies for a Strategic Intrinsically Motivated Learner",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "SAGG-RIAC",
            "name_full": "Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC)",
            "brief_description": "An intrinsically motivated goal-directed exploration algorithm that uses competence progress to select goals and local optimization (local linear regression) to improve policies in the policy parameter space.",
            "citation_title": "Intrinsically motivated goal exploration for active motor learning in robots: A case study",
            "mention_or_use": "use",
            "agent_name": "SAGG-RIAC learner (baseline)",
            "agent_description": "An IM (intrinsically motivated) goal-babbling algorithm that explores the policy parameter space guided by measures of empirical learning progress and performs local optimization via local linear regression to reach self-generated goals; used here as a baseline that does not use procedures.",
            "agent_type": "simulated agent (controller of the 3-joint robotic arm)",
            "environment_name": "Same simulated robotic arm environment",
            "environment_description": "Same environment with hierarchical outcome-spaces; SAGG-RIAC explores policies directly to reach goals in Ω_i without composing procedures from previously learned policies.",
            "complexity_measure": "Task complexity measured via outcome-space dimensionality and hierarchy (Ω0..Ω5); SAGG-RIAC's efficiency is known to degrade with higher outcome-space dimensionality (curse of dimensionality, cited in paper).",
            "complexity_level": "effective for low-to-moderate complexity tasks (works well for Ω0), performance declines for more hierarchical/multi-dimensional outcome-spaces (Ω1, Ω2, Ω5).",
            "variation_measure": "Variation is across selected target outcomes; no environment-instance variation. The algorithm's sensitivity to outcome-space dimensionality (variation in goal type/dimension) is noted.",
            "variation_level": "moderate (multiple goal types) but algorithm not tested across multiple environment instances.",
            "performance_metric": "Mean normalized Euclidean distance to benchmark outcomes (27,600-point benchmark); internal competence-progress metric.",
            "performance_value": "Qualitative: SAGG-RIAC performs worse than IM-PB and Random-PB on hierarchical outcome spaces; performs similarly to RandomPolicy on many outcome spaces except Ω0. No numeric values given.",
            "complexity_variation_relationship": "Paper reiterates known limitation: performance of goal-babbling approaches like SAGG-RIAC drops as outcome-space dimensionality increases (curse of dimensionality). The study shows SAGG-RIAC without procedures learns less on hierarchical tasks compared to approaches with procedures.",
            "high_complexity_low_variation_performance": "null",
            "low_complexity_high_variation_performance": "null",
            "high_complexity_high_variation_performance": "null",
            "low_complexity_low_variation_performance": "null",
            "training_strategy": "Intrinsic-motivation driven goal babbling with local policy optimization (SAGG-RIAC framework).",
            "generalization_tested": false,
            "generalization_results": "Assessed via benchmark coverage; shows poorer coverage on hierarchical outcome spaces relative to procedure-using approaches; no cross-environment generalization.",
            "sample_efficiency": "Evaluated under same budget (25,000 complex policy executions); reported to be less efficient than IM-PB on hierarchical tasks.",
            "key_findings": "SAGG-RIAC without procedural composition suffers on hierarchical/multi-dimensional outcome spaces due to curse of dimensionality; adding mechanisms that reuse and chain skills (procedures) improves learning in such setups.",
            "uuid": "e1073.2",
            "source_info": {
                "paper_title": "Learning a Set of Interrelated Tasks by Using Sequences of Motor Policies for a Strategic Intrinsically Motivated Learner",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "RandomPolicy",
            "name_full": "Random Policy Exploration",
            "brief_description": "A baseline learner that samples motor policies randomly from the policy parameter space Π without intrinsic-motivation or procedural composition.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RandomPolicy baseline (controller for simulated robotic arm)",
            "agent_description": "A simple baseline that explores the policy parameter space by random sampling of primitive and complex policies without goal selection, local optimization, or procedure composition.",
            "agent_type": "simulated agent (controller of the 3-joint robotic arm)",
            "environment_name": "Same simulated robotic arm environment",
            "environment_description": "Same fixed simulated workspace with hierarchical outcome-spaces; RandomPolicy explores by executing randomly sampled policies and records outcomes.",
            "complexity_measure": "Task complexity as hierarchical outcome-space dimensionality; policy complexity as length of concatenated primitives when random policies are sampled.",
            "complexity_level": "capable of reaching simple tasks (Ω0) but inefficient for hierarchical tasks (Ω1, Ω2, Ω5).",
            "variation_measure": "Variation arises from goal diversity across outcome spaces; environment-instance variation is not manipulated.",
            "variation_level": "moderate for goal variation, low for environment variation.",
            "performance_metric": "Mean normalized Euclidean distance to benchmark outcomes (27,600-point benchmark).",
            "performance_value": "Qualitative: RandomPolicy yields higher mean error than procedure-using learners (Random-PB and IM-PB) and similar or worse performance than SAGG-RIAC on many outcome-spaces; specific numeric errors not reported.",
            "complexity_variation_relationship": "RandomPolicy performs acceptably on lowest-complexity tasks (Ω0) but poorly on hierarchical/more complex tasks, demonstrating that random exploration struggles as task complexity increases; no explicit study of variation vs complexity trade-offs.",
            "high_complexity_low_variation_performance": "null",
            "low_complexity_high_variation_performance": "null",
            "high_complexity_high_variation_performance": "null",
            "low_complexity_low_variation_performance": "null",
            "training_strategy": "Pure random exploration of the policy parameter space.",
            "generalization_tested": false,
            "generalization_results": "Evaluated via benchmark coverage; RandomPolicy shows poor coverage on hierarchical outcome-spaces compared to procedure-enabled methods.",
            "sample_efficiency": "Same experimental budget: 25,000 complex policy executions per run (5 runs); sample-inefficient for hierarchical tasks.",
            "key_findings": "Random exploration is insufficient to effectively learn hierarchical, high-dimensional outcome-spaces; composition mechanisms (procedures) and strategic goal selection markedly improve learning efficiency and coverage.",
            "uuid": "e1073.3",
            "source_info": {
                "paper_title": "Learning a Set of Interrelated Tasks by Using Sequences of Motor Policies for a Strategic Intrinsically Motivated Learner",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Intrinsically motivated goal exploration for active motor learning in robots: A case study",
            "rating": 2
        },
        {
            "paper_title": "Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner",
            "rating": 2
        },
        {
            "paper_title": "The Strategic Student Approach for Life-Long Exploration and Learning",
            "rating": 2
        },
        {
            "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
            "rating": 2
        },
        {
            "paper_title": "Strategic and interactive learning of a hierarchical set of tasks by the Poppy humanoid robot",
            "rating": 2
        }
    ],
    "cost": 0.011580249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning a set of interrelated tasks by using sequences of motor policies for a strategic intrinsically motivated learner</h1>
<p>Nicolas Duminy ${ }^{1}$ Sao Mai Nguyen ${ }^{2}$ Dominique Duhaut ${ }^{1}$</p>
<h4>Abstract</h4>
<p>We propose an active learning architecture for robots, capable of organizing its learning process to achieve a field of complex tasks by learning sequences of motor policies, called Intrinsically Motivated Procedure Babbling (IM-PB). The learner can generalize over its experience to continuously learn new tasks. It chooses actively what and how to learn based by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated tasks outcomes hierarchically organized.</p>
<p>We introduce a framework called "procedures", which are sequences of policies defined by the combination of previously learned skills. Our algorithmic architecture uses the procedures to autonomously discover how to combine simple skills to achieve complex goals. It actively chooses between 2 strategies of goaldirected exploration: exploration of the policy space or the procedural space. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our "procedures" framework helps the learner to tackle difficult hierarchical tasks.</p>
<h2>I. INTRODUCTION</h2>
<p>Taking a developmental robotic approach [1], we combine the approaches for active motor skill learning of goal-oriented exploration and strategical learning to learn multiple complex and interrelated tasks. Our algorithm is able to learn a mapping between a continuous space of parametrized tasks (also referred to as outcomes) and a space of parametrized motor policies (sometimes referred to as actions).</p>
<h2>A. Active motor skill learning of multiple tasks</h2>
<p>Classical techniques based on Reinforcement Learning [2] [3] still need an engineer to manually design a reward function for each task. Intrinsic motivation (IM), which triggers curiosity in humans according to developmental psychology [4], was introduced in highly-redundant robots to make them learn a wider range of tasks, through goal-babbling [5] [6].</p>
<p>However, with higher outcome space dimensionalities, their efficiency drops [7] due to the curse of dimensionality.</p>
<h2>B. Strategic learning</h2>
<p>Approaches where the learner chooses both what (which outcome to focus on) [5] and how (which strategy to use) [9] to learn are called strategic learning [8]. They aim at enabling an autonomous learner to self-organize its learning process.</p>
<p>The research work presented in this paper is partially supported by the EU FP7 grant ECHORD++ KERAAL and by the the European Regional Fund (FEDER) via the VITAAL Contrat Plan Etat Region
${ }^{1}$ Nicolas Duminy and Dominique Duhaut are with Université Bretagne Sud, Lorient, France. nicolas.duminy@telecom-bretagne.eu and dominique.duhaut@univ-ubs.fr
${ }^{2}$ Sao Mai Nguyen is with IMT Atlantique, Lab-STICC, UBL, F-29238 Brest, France. nguyensmai@gmail.com</p>
<p>The problem was introduced and studied in [8], and implemented for an infinite number of outcomes and policies in continuous spaces by the SGIM-ACTS algorithm [15]. This algorithm organizes its learning process, by choosing actively both which strategy to use and which outcome to focus on. It relies on the empirical evaluation of its learning progress. It could choose among autonomous exploration driven by IM and low-level imitation of one of the available human teachers to learn more efficiently. It showed its potential to learn on a real high dimensional robot a set of hierarchically organized tasks [11], so we inspire from it to learn complex motor policies.</p>
<h2>C. Learning complex motor policies</h2>
<p>In this article, we tackle the learning of complex motor policies, which we define as sequences of primitive policies.</p>
<p>We wanted to enable the learner to decide autonomously the complexity of the policy necessary to solve a task, so we discarded via-points [3]. Options [12] are temporally abstract actions built to reach one particular task. They have only been tested for discrete tasks and actions, where a small number of options were used, whereas our new proposed learner is to be able to create an unlimited number of complex policies.</p>
<p>As we aim at learning a hierarchical set of interrelated complex tasks, our algorithm could use this task hierarchy (as [13] did to learn tool use with primitive policies only), and try to reuse previously acquired skills to build more complex ones. [14] showed that building complex actions made of lowerlevel actions according to the task hierarchy can bootstrap exploration by reaching interesting outcomes more rapidly.</p>
<p>We adapted SGIM-ACTS to learn complex motor policies of unlimited size. We developed a new mechanism called "procedures" (see Section II-B) which proposes to combine known policies according to their outcome. Combining these, we developed a new algorithm called Intrinsically Motivated Procedure Babbling (IM-PB) capable of taking task hierarchy into account to learn a set of complex interrelated tasks using adapted complex policies. We will describe an experiment, on which we have tested our algorithm, and we will present and analyze the results.</p>
<h2>II. OUR APPROACH</h2>
<p>Inspired by developmental psychology, we propose a strategic learner driven by IM. This learner discovers the task hierarchy and reuses previously learned skills while adapting the complexity of its policy to the complexity.</p>
<p>In this section, we formalize our learning problem and explain the principles of IM-PB.</p>
<h2>A. Problem formalization</h2>
<p>In our approach, an agent can perform policies $\pi_{\theta}$, parametrized by $\theta \in \Pi$. Those policies induce outcomes in the environment, parametrized by $\omega \in \Omega$. The agent is then to learn the mapping between $\Pi$ and $\Omega$ : it learns to predict the outcome $\omega$ of each policy $\pi_{\theta}$ (the forward model $M$ ), but more importantly, it learns which policy to choose for reaching any particular outcome (an inverse model $L$ ). The outcomes $\omega$ are of various dimensionality and be split in task spaces $\Omega_{i} \subset \Omega$.</p>
<p>The policies consist of a succession of primitives (encoded by the same set of parameters $\theta \in \Pi$ ) that are executed sequentially by the agent. Hence, policies also are of different dimensionality and are split in policy spaces $\Pi_{i} \subset \Pi$ (where $i$ corresponds to the number of primitives).</p>
<h2>B. Procedures</h2>
<p>As this algorithm tackles the learning of complex hierarchically organized tasks, exploring and exploiting this hierarchy could ease the learning of the more complex tasks. We define procedures as a way to encourage the robot to reuse previously learned skills, and chain them to build more complex ones. More formally, a procedure is built by choosing two previously known outcomes $\left(t_{i}, t_{j} \in \Omega\right)$ and is noted $t_{i} \boxplus t_{j}$.</p>
<p>Executing a procedure $t_{i} \boxplus t_{j}$ means building the complex policy $\pi_{\theta}$ corresponding to the succession of both policies $\pi_{\theta_{i}}$ and $\pi_{\theta_{j}}$ and execute it (where $\pi_{\theta_{i}}$ and $\pi_{\theta_{j}}$ reach best $t_{i}$ and $t_{j}$ respectively). As $t_{i}$ and $t_{j}$ are generally unknown from the learner, the procedure is updated before execution (see Algo. 1) to subtasks $t_{1}$ and $t_{2}$ which are feasible by the learner according to its current skill set.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Procedure modification before execution
Input: \(\left(t_{i}, t_{j}\right) \in \Omega^{2}\)
Input: inverse model \(L\)
    \(t_{1} \leftarrow\) Nearest-Neighbour \(\left(t_{i}\right)\)
    \(t_{2} \leftarrow\) Nearest-Neighbour \(\left(t_{j}\right)\)
    \(\pi_{\theta_{1}} \leftarrow L\left(t_{1}\right)\)
    \(\pi_{\theta_{2}} \leftarrow L\left(t_{2}\right)\)
    return \(\pi_{\theta}=\pi_{\theta_{1}} \pi_{\theta_{2}}\)
</code></pre></div>

<h2>C. Intrinsically Motivated Procedure Babbling</h2>
<p>The IM-PB algorithm (see Algo. 2) learns by episodes, where an outcome $\omega_{g} \in \Omega$ to target and an exploration strategy $\sigma$ have been selected.</p>
<p>In an episode under the policy space exploration strategy, the learner tries to optimize the policy $\pi_{\theta}$ to produce $\omega_{g}$ by choosing between random exploration of policies and local optimization, following the SAGG-RIAC algorithm [5] (GoalDirected Policy Optimization $\left(\omega_{g}\right)$ ). Local optimization uses local linear regression.</p>
<p>In an episode under the procedural space exploration strategy, the learner builds a procedure $t_{i} \boxplus t_{j}$ such as to reproduce the goal outcome $\omega_{g}$ the best (Goal-Directed Procedure Optimization $\left(\omega_{g}\right)$ ). It chooses either random exploration of procedures (which builds procedures by generating two subtasks at random) when the goal outcome is far from
any previously reached one, or local procedure optimization, which optimizes a procedure using local linear regression. The procedure built is then modified and executed, using Algo. 1.</p>
<p>After each episode, the learner stores the policies and modified procedures executed along with their reached outcomes in its episodic memory. It computes its competence in reaching the goal outcome $\omega_{g}$ by comparing it with the outcome $\omega$ it actually reached (using normalized Euclidean distance $d\left(\omega, \omega_{g}\right)$ ). Then it updates its interest model according to the progress $p\left(\omega_{g}\right)$, which is the derivate of the competence, it has made (including the outcome spaces reached but not targeted) in order to choose the strategy and task in the next episode. The interest interest $(\omega, \sigma)$ of each outcome added depends on both the progress $p(\omega)$ made and the cost $K(\sigma)$ of the strategy used: $\operatorname{interest}(\omega, \sigma)=p(\omega) / K(\sigma)$. The outcomes reached and the goal are added in their corresponding region, which is then split when exceeding a fixed number of points to discriminate the regions of high and low interest for the learner. The method used is described in [15].</p>
<h2>Algorithm 2 IM-PB</h2>
<p>Input: the different strategies $\sigma_{1}, \ldots, \sigma_{n}$
Initialization: partition of outcome spaces $R \leftarrow \bigsqcup_{i}\left{\Omega_{i}\right}$
Initialization: episodic memory $\operatorname{Memo} \leftarrow \varnothing$
loop
$\omega_{g}, \sigma \leftarrow$ Select Goal Outcome and Strategy $(R)$
if $\sigma=$ Autonomous exploration of procedures strategy then
$\operatorname{Memo} \leftarrow$ Goal-Directed Procedure Optimization $\left(\omega_{g}\right)$
else
$\sigma=$ Autonomous exploration of policies strategy
$\operatorname{Memo} \leftarrow$ Goal-Directed Policy Optimization $\left(\omega_{g}\right)$
end if
Update $L^{-1}$ with collected data Memo
$R \leftarrow$ Update Outcome and Strategy Interest Mapping $\left(R, \operatorname{Memo}, \omega_{g}\right)$
end loop</p>
<p>The choice of strategy and goal outcome is based on the empirical progress measured in each region $R_{n}$ of the outcome space $\Omega$, as in [11].</p>
<p>When the learner computes nearest neighbours to select policies or procedures to optimize (when choosing local optimization in any autonomous exploration strategies and when refining procedures), it actually uses a performance metric (1) which takes into account the cost of the policy chosen:</p>
<p>$$
\operatorname{perf}=d\left(\omega, \omega_{g}\right) \gamma^{n}
$$</p>
<p>where $d\left(\omega, \omega_{g}\right)$ is the normalized Euclidean distance between the target outcome $\omega_{g}$ and the outcome $\omega$ reached by the policy, $\gamma$ is a constant and $n$ is equal to the size of the policy (the number of primitives chained).</p>
<h2>III. EXPERIMENT</h2>
<p>We designed an experiment with a simulated robotic arm, which can move and interact with objects. It can learn an infinite number of tasks, organized as 6 types of tasks. The robot can perform complex policies of unrestricted size.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Experimental setup: a robotic arm, can interact with the different objects in its environment (a pen and two joysticks). Both joysticks enable to control a video-game character (represented in top-right corner). A grey floor limits its motions and can be drawn upon using the pen (a possible drawing is represented).</p>
<h3>A. Simulation setup</h3>
<p>Fig. 1 shows the experimental setup (delimited by $(x, y, z) \in [-1; 1]^3$). The learning agent is a planar robotic arm of 3 joints (each link measures 0.33), with the based anchored in the center of the horizontal plan. It can rotate around the z-axis and change its vertical position. The robot can grab objects by hovering its arm tip (blue in Fig. 1) close to them, which position is noted (x₀, y₀, z₀). It interacts with:</p>
<ul>
<li><strong>Pen</strong>: position noted (x₁, y₁, z₁), can be moved and draw on the floor, broken if forcing to much on the floor;</li>
<li><strong>Floor</strong>: which limits motions to z &gt; -0.2;</li>
<li><strong>Drawing</strong>: last continuous line made when the pen moves on the floor, delimited by first (xₙ, yₙ) and last (xₖ, yₖ) point, if the pen is functional;</li>
<li><strong>Joysticks</strong>: two joysticks can be moved inside their own cubic-shape volume and control a video-game character, released otherwise, normalized positions respectively at (x₃, y₃, z₃) and (x₄, y₄, z₄);</li>
<li><strong>Video-game character</strong>: on a 2D-screen set by the joysticks refreshed only at the end of a primitive policy execution for manipulated joystick, position (x₅, y₅) set by joystick 1 x-axis and joystick 2 y-axis respectively.</li>
</ul>
<p>The robot can one object at once. Touching another breaker, releasing both objects. It always starts from the same position before executing a policy, and primitives are executed sequentially without getting back to this initial position. Whole complex policies are recorded with their outcomes, but each step of the complex policy execution is recorded as well.</p>
<h3>B. Experiment variables</h3>
<p>1) <strong>Policy spaces</strong>: The motions of each of the three joints of the robot are encoded using a one-dimensional Dynamic Movement Primitive (DMP). We are using the original form of the DMP from [16] and we keep the same notations. Each of these one-dimensional DMP aᵢ (ordered by joint from base to tip) is encoded using its end position g⁽¹, and three basis functions for the forcing term, parametrized by their weights (ω₀⁽¹, ω₁⁽¹, ω₂⁽¹). A primitive motor policy is simply the concatenation of those DMP parameters and the fixed vertical position of the arm during the motion z:</p>
<p>$$
\theta = (a_0, a_1, a_2, z) \tag{2}
$$</p>
<p>$$
a_i = (\omega_0^{(i)}, \omega_1^{(i)}, \omega_2^{(i)}, g^{(i)}) \tag{3}
$$</p>
<p>When combining two or more primitive policies (πθ₀, πθ₁, ...), in a complex policies πθ, the parameters (θ₀, θ₁, ...) are simply concatenated together from the first primitive to the last.</p>
<p>2) <strong>Task spaces</strong>: The task spaces the robot learns are hierarchically organized and defined as: Ω₀ = { (x₀, y₀, z₀) }, Ω₁ = { (x₁, y₁, z₁) }, Ω₂ = { (xₙ, yₙ, xₖ, yₖ) }, Ω₃ = { (x₃, y₃, z₃) }, Ω₄ = { (x₄, y₄, z₄)} and Ω₅ = { (x₅, y₅)}.</p>
<h3>C. Evaluation method</h3>
<p>To evaluate our algorithm, we created a benchmark linearly distributed across the Ωᵢ, of 27,600 points. The evaluation consists in computing mean Euclidean distance between each of the benchmark outcomes and their nearest neighbour in the learner dataset. This evaluation is repeated regularly.</p>
<p>Then to assess our algorithm efficiency, we compare its results of algorithms: RandomPolicy (random exploration of Π), SAGG-RIAC (exploration of Π guided by IM), Random-PB (random exploration of policies and procedures), IM-PB (exploration of the procedural space and the policy space, guided by IM).</p>
<p>Each algorithm was run 5 times for 25,000 iterations (complex policies executions). The meta parameter was: γ = 1.2.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Evaluation of all algorithms (standard deviation shown in caption)</p>
<p>Fig. 2 shows the global evaluation of all tested algorithms, which is the mean error made by each algorithm to reproduce the benchmarks with respect to the number of complete complex policies tried. Random-PB and IM-PB owing to procedures have lower errors than the others even since the beginning. Indeed, they perform better than their downgrades without procedures, RandomPolicy and SAGG-RIAC.</p>
<p>On each individual outcome space (Fig. 3), IM-PB outperforms the other algorithms. The comparison of the learners without procedures (RandomPolicy and SAGG-RIAC) with the others shows they learn less on any outcome space but Ω₀ (reachable using single primitives, with no subtask) and especially for Ω₁, Ω₂ and Ω₅ which were the most hierarchical.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Evaluation of all algorithms per outcome space (for Ω₀, all evaluations are superposed)</p>
<p>in this setup. So the procedures helped when learning any potentially hierarchical task in this experiment.</p>
<p>We wanted to see if our IM-PB learner adapts the complexity of its policies to the working task. We draw 1,000,000 goal outcomes for each of the Ω₀, Ω₁, and Ω₂ subspaces (chosen because they are increasingly complex) and we let the learner choose the known policy that would reach the closest outcome. Fig. 4 shows the results of this analysis.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Number of policies selected per policy size for three increasingly more complex outcome spaces by the IM-PB learner</p>
<p>As we can see on those three interrelated outcome subspaces (Fig. 4), the learner is capable of adapting the complexity of its policies to the outcome at hand. It chooses longer policies for Ω₁ and Ω₂ (size 3 and 4 compared to size 1 for Ω₀). Our learner is capable of correcting the complexity of its policies instead of being stuck into always trying longer and longer policies. However, the learner did not increase its policies complexity from Ω₁ to Ω₂, as we hoped.</p>
<h2>V. CONCLUSION AND FUTURE WORK</h2>
<p>With this experiment, we show the capability of IM-PB to tackle the learning of a set of multiple interrelated complex tasks. It successfully uses complex motor policies to learn a wider range of tasks. Though it was not limited in the size of policies, it could execute the learner shows it could adapt the complexity of its policies to the task at hand.</p>
<p>The procedures greatly improved the learning capability of autonomous learners, as we can see by the difference between the Random-PB and IM-PB learners and the RandomPolicy and SAGG-RIAC ones. Our IM-PB shows it is capable of use procedures to exploit both the task hierarchy of this experimental setup and previously learned skills.</p>
<p>However, this new framework of procedures could be better exploited if it could be recursive (defined as a binary tree structure), allowing the refinement process to select lower-level procedures as one of the policy components. This process could also be used inside the strategical decisions made by the learner when selecting what and how to learn. This strategical choice could also be recursive, allowing the learner to optimize both components of a procedure at once, instead of using the current one-step refinement process.</p>
<p>Also, the procedures are here only combinations of two subtasks; it could be interesting to see if the process can extend to combinations of any number of subtasks.</p>
<p>Finally, proving the potency of our IM-PB learner on a real robotic setup could show its interest for actual robotic applications. We are currently designing such an experiment.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] M. Lungarella, G. Metta, R. Pfeifer, and I. G. Sandin, "Developmental robotics: a survey," <em>Connection Science</em>, vol. 15, no. 4, pp. 151–190, 2003.</li>
<li>[2] E. Theodorou, J. Buchli, and S. Schaal, "Reinforcement learning of motor skills in high dimensions: a path integral approach," in <em>Robotics and Automation (ICRA), 2010 Ieee International Conference on</em>, 2010, pp. 2397–2403. [Online]. Available: http://www-clmc.usc.edu/publications/T/theodorou-ICRA2010.pdf</li>
<li>[3] F. Stulp and S. Schaal, "Hierarchical reinforcement learning with movement primitives," in <em>Humanoids</em>, 2011, pp. 231–238.</li>
<li>[4] E. Deci and R. M. Ryan, <em>Intrinsic Motivation and Self-Determination in Human Behavior</em>. New York: Plenum Press, 1985.</li>
<li>[5] A. Baranes and P.-Y. Oudeyer, "Intrinsically motivated goal exploration for active motor learning in robots: A case study," in <em>Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on</em>, Oct. 2010, pp. 1766–1773.</li>
<li>[6] M. Rolf, J. Steil, and M. Gienger, "Goal bubbling permits direct learning of inverse kinematics," <em>IEEE Trans. Autonomous Mental Development</em>, vol. 2, no. 3, pp. 216–229, 09/2010 2010.</li>
<li>[7] A. Baranes and P.-Y. Oudeyer, "Active learning of inverse models with intrinsically motivated goal exploration in robots," <em>Robotics and Autonomous Systems</em>, vol. 61, no. 1, pp. 49–73, 2013.</li>
<li>[8] M. Lopes and P.-Y. Oudeyer, "The Strategic Student Approach for Life-Long Exploration and Learning," in <em>IEEE Conference on Development and Learning / EpiRob</em>, San Diego, États-Unis, Nov. 2012. [Online]. Available: http://hal.inria.fr/hal-00755216</li>
<li>[9] Y. Baram, R. El-Yaniv, and K. Luz, "Online choice of active learning algorithms," <em>The Journal of Machine Learning Research</em>, vol. 5, pp. 255–291, 2004.</li>
<li>[10] S. M. Nguyen, A. Baranes, and P.-Y. Oudeyer, "Bootstrapping intrinsically motivated learning with human demonstrations," in <em>IEEE International Conference on Development and Learning</em>, Frankfurt, Germany, 2011.</li>
<li>[11] N. Duminy, S. M. Nguyen, and D. Duhaut, "Strategic and interactive learning of a hierarchical set of tasks by the Poppy humanoid robot," in <em>2016 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)</em>, Sep. 2016, pp. 204–209.</li>
<li>[12] R. S. Sutton, D. Precup, and S. Singh, "Between mdps and semi-mdps: a framework for temporal abstraction in reinforcement learning," <em>Artif. Intell.</em>, vol. 112, pp. 181–211, August 1999. [Online]. Available: http://dx.doi.org/10.1016/S0004-3702(99)00052-100052-1)</li>
<li>[13] S. Forestier and P.-Y. Oudeyer, "Curiosity-driven development of tool use precursors: a computational model," in <em>38th Annual Conference of the Cognitive Science Society (CogSci 2016)</em>, 2016, pp. 1859–1864.</li>
<li>[14] A. G. Barto, G. Konidaris, and C. Vigorito, "Behavioral hierarchy: exploration and representation," in <em>Computational and Robotic Models of the Hierarchical Organization of Behavior</em>. Springer, 2013, pp. 13–46. [Online]. Available: http://link.springer.com/chapter/10.1007/978-3-642-39875-9_2</li>
</ul>
<p>[15] S. M. Nguyen and P.-Y. Oudeyer, "Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner," Paladyn Journal of Behavioural Robotics, vol. 3, no. 3, pp. 136-146, 2012. [Online]. Available: http://dx.doi.org/10.2478/s13230-013-0110-z
[16] P. Pastor, H. Hoffmann, T. Asfour, and S. Schaal, "Learning and generalization of motor skills by learning from demonstration," in Robotics and Automation, 2009. ICRA’09. IEEE International Conference on. IEEE, 2009, pp. 763-768. [Online]. Available: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5152385</p>            </div>
        </div>

    </div>
</body>
</html>
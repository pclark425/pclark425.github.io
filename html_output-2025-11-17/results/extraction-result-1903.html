<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1903 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1903</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1903</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-281951533</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.07975v1.pdf" target="_blank">Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Enabling robots to perform precise and generalized manipulation in unstructured environments remains a fundamental challenge in embodied AI. While Vision-Language Models (VLMs) have demonstrated remarkable capabilities in semantic reasoning and task planning, a significant gap persists between their high-level understanding and the precise physical execution required for real-world manipulation. To bridge this"semantic-to-physical"gap, we introduce GRACE, a novel framework that grounds VLM-based reasoning through executable analytic concepts (EAC)-mathematically defined blueprints that encode object affordances, geometric constraints, and semantics of manipulation. Our approach integrates a structured policy scaffolding pipeline that turn natural language instructions and visual information into an instantiated EAC, from which we derive grasp poses, force directions and plan physically feasible motion trajectory for robot execution. GRACE thus provides a unified and interpretable interface between high-level instruction understanding and low-level robot control, effectively enabling precise and generalizable manipulation through semantic-physical grounding. Extensive experiments demonstrate that GRACE achieves strong zero-shot generalization across a variety of articulated objects in both simulated and real-world environments, without requiring task-specific training.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1903.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1903.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRACE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GRACE: From VLM-based Grounding to Robotic manipulation through Analytic Concept Execution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that grounds vision-language model reasoning into executable analytic concepts (EACs) — parametric, math-based blueprints encoding object geometry, affordances, and manipulation routines — enabling zero-shot, physically-feasible robotic manipulation by converting language+RGB-D into grasp poses, force vectors, and motion plans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GRACE (system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline combining an off-the-shelf Vision-Language Model (VLM) for high-level parsing/reasoning, Visual Foundation Models (GroundingDINO + SAM) for segmentation, a point-cloud encoder (Point-Transformer) to regress analytic concept parameters, and analytic manipulation blueprints that produce grasp poses and force directions passed to a standard motion planner and robot controller.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>uses off-the-shelf multimodal VLMs (multimodal vision+language pretrained models) and standard visual foundation models; the GRACE framework itself does not rely on task-specific action pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in paper for GRACE itself; relies on pretrained VLMs (e.g., GPT-4o, Qwen2.5-VL) and VFMs (GroundingDINO, SAM). The paper states VLMs are large-scale pretrained and carry commonsense/world knowledge (implying training on large image–text corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation of articulated and rigid objects (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Manipulation tasks (open/close drawers, doors, handles, assorted object interactions) executed by a parallel-gripper robot; action space includes continuous 6-DoF grasp poses, force-direction vectors and joint-space trajectories produced by a motion planner; evaluated in simulation (SimplerEnv, SAPIEN) and on a real RM75 arm/tabletop setup.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper explicitly constructs semantic grounding: VLM maps instruction and natural-language synopses of analytic concepts to specific concept candidates, enabling alignment between verbs/intent and geometric manipulation primitives; degree of overlap with pretraining corpora not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Simulation: GRACE (GPT-4o) average success rate 86.1% on Widow-X tasks and 90.1% mean success on Google-robot tasks; articulated Open/Close Drawer improved to 90.3% (variant aggregation). Real-world per-object zero-shot success examples: Stapler 90%, Mug 80%, Bucket 70%, Microwave 80%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>A variant using oracle (ground-truth) concept labels instead of VLM concept selection (reported as GRACE-w/o-VLM/oracle concept) yields an average articulated-object score of 0.80 (vs GPT-4o automatic selection 0.77 on the articulated-object benchmark cited); other baseline variants without VLM (or replaced modules) reported lower success in comparative baselines (see baselines). Exact end-to-end numbers for a fully language-ablated GRACE pipeline not fully enumerated beyond this oracle-vs-VLM comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper claims GRACE reduces dependence on large quantities of task-specific action demonstrations, but does not provide quantified sample-efficiency curves or numeric comparisons (no episodes/steps-to-threshold reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention-map analysis or visualization of VLM attention reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No detailed analysis reported of embedding/feature-space structure or clustering comparing pretrained vs non-pretrained representations.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Strong procedural evidence: analytic manipulation blueprints map chosen manipulation verbs (e.g., 'pull', 'push') to concrete force-direction formulas and parameterized grasp-pose computations; the VLM selects manipulation modules based on synopses and the instantiated EAC yields grasp G and force F that are transformed to world frame and executed—presented as direct language→geometry→action grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Paper describes a hierarchical architecture (VLM for high-level parsing, EACs as mid-level parametric descriptors, low-level motion planner for execution) and shows performance gains come from the analytic-concept layer, but does not include layerwise feature analyses differentiating low vs high-level representations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer success is sensitive to: (1) correctness of concept selection (VLM misclassification causes most remaining failures), (2) pose-estimation and IK quality (pose/IK errors are primary failure modes), and (3) availability of sufficient geometric information (multi-view reconstruction improves pose estimation). GRACE is robust across different underlying VLM backends (Qwen2.5-VL trails GPT-4o by ~1–2 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper emphasizes zero-shot generalization to unseen articulated objects and reports high success rates on novel instances; explicit numeric split conditioned on whether objects were present in VLM pretraining is not provided. Oracle concept vs VLM selection comparison implies small drop when moving from ground-truth labels to VLM-selected concepts (avg 0.80→0.77).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Yes — GRACE demonstrates zero-shot capabilities: reported zero-shot success rates in simulation (86.1% Widow-X, 90.1% Google-robot) and several zero-shot real-world object successes (90%, 80%, 70%, 80%). No few-shot tuning curves provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layerwise ablation or probing of VLM internals beyond swapping VLM backends and an ablation replacing VLM concept selection with oracle labels.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No systematic negative transfer evidence reported; paper notes occasional VLM misclassification can slightly reduce performance, but no cases where VLM usage harms overall performance compared to oracle concept labels beyond the small gap reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Paper compares to pixel-level affordance methods and VLA baselines, showing large improvements when EACs scaffold VLM reasoning; explicit head-to-head comparison to pure vision-only ImageNet-pretrained models is not reported. Replacing SpatialVLA's end-to-end action output with EAC-guided motion planning (SpatialVLA-EAC) substantially improved performance, suggesting analytic/semantic grounding helps more than end-to-end visual policies alone.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No analysis of representation or performance dynamics across fine-tuning steps or training epochs is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality or intrinsic-dimension analyses reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1903.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1903.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (multimodal Vision-Language model used as VLM backend)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-the-shelf multimodal large language/vision-language model used for task parsing, chain-of-thought reasoning, and concept selection in GRACE; chosen as the highest-performing VLM backend in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used as VLM backend)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal vision-language model (LLM with multimodal input capability) used to parse instructions, perform chain-of-thought object parsing, select analytic concepts, and choose manipulation modules.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal vision-language pretraining (off-the-shelf); paper does not specify pretraining dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in paper; implied large-scale image+text corpora with commonsense/world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>VLM-driven task parsing and concept selection for robotic manipulation (within GRACE)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Performs coarse-to-fine object identification, state classification for articulated parts, task decomposition into sub-tasks, and selection of analytic manipulation modules given RGB-D scenes and natural-language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Used explicitly to align language instructions and natural-language synopses of analytic concepts with 3D perceptual inputs; paper reports that GPT-4o selection yields near-oracle performance (small gap ~3 percentage points in some articulated benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>When used as GRACE backend: system success rates reported as GRACE(GPT-4o) — 86.1% (Widow-X avg), 90.1% (Google-robot avg); articulated-object avg ~0.77 in one benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable for GPT-4o itself; ablation replacing GPT-4o selection with oracle concept labels gives oracle avg ~0.80 (slightly higher).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No sample-efficiency numbers specific to GPT-4o reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>GPT-4o is used to select manipulation modules and synopses that are then instantiated into analytic blueprints; the resulting pipeline demonstrates language→affordance grounding practically via the analytic blueprint execution.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>GPT-4o provides high-level symbolic/semantic outputs which are mapped to mid-level analytic representations; no internal feature-level analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper reports GPT-4o-based GRACE performs robustly and only slightly better than open-source Qwen2.5-VL, suggesting transfer is tolerant to VLM choice; specific dataset-domain transfer details not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>GPT-4o selection yields near-oracle performance on novel articulated instances per paper; exact breakdown versus familiar objects not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Demonstrated zero-shot selection capability within GRACE with small performance gap to oracle labels.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layerwise analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared to vision-only models; used as a semantic reasoning module complementary to visual foundation models.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1903.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1903.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-VL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-VL (open-source vision-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal VLM used as an alternative backend to GPT-4o in GRACE; achieves performance within ~1–2 percentage points of GPT-4o, indicating analytic concepts drive most gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-VL (used as VLM backend)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An off-the-shelf multimodal vision-language model serving the same role as GPT-4o for parsing and concept selection within GRACE.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal vision-language pretraining (not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>VLM-driven parsing and concept selection for robotic manipulation (within GRACE)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same role as GPT-4o: parse instruction, select analytic concept synopses, guide blueprint selection given RGB-D input.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Reported to perform almost as well as GPT-4o; the paper interprets this as evidence that the analytic-concept layer, not the specific VLM, is the main contributor to transfer success.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>GRACE with Qwen2.5-VL trails GPT-4o by only 1–2 percentage points on reported robot families but still outperforms all external baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable; no explicit baseline for Qwen2.5-VL without its pretrained weights provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Used to map natural-language synopses to analytic manipulation modules similarly to GPT-4o; practical grounding shown via overall system performance.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper reports robustness across VLM backends; minor performance differences suggest backend choice is not primary factor.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not detailed separately from GRACE overall results.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Used in zero-shot GRACE runs with high success; exact per-object breakdown not given.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared to vision-only pretraining in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1903.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1903.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLA (Vision-Language-Action model baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA baseline that outputs end-to-end actions from visual and language inputs; used as a competitor and as a base for an EAC-augmented variant in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Vision-Language-Action architecture that produces action outputs directly from multimodal inputs (image + language); details of its pretraining are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper (referenced as a concurrent VLA approach).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (baseline comparative evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>End-to-end action generation for manipulation tasks in SimplerEnv/SAPIEN benchmarks; continuous control outputs used by robot controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>SpatialVLA is intended to align visual and linguistic inputs to actions end-to-end, but paper argues it can lose geometric detail when compressing 3D interactions into 2D/textual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>SpatialVLA reported previously (in related work) but in this paper SpatialVLA's published baseline for Open/Close Drawer was 36.2% (cited), while SpatialVLA-EAC variant performance is reported below.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper argues end-to-end VLA methods can lose geometric and kinematic detail; no direct analysis provided for SpatialVLA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper demonstrates that replacing SpatialVLA's action output with EAC-guided planning substantially improves robustness, suggesting SpatialVLA alone is sensitive to geometric fidelity loss.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not emphasized; baseline numbers provided for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not explicitly reported, but paper suggests end-to-end VLA may underperform on tasks requiring precise kinematic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared to pure vision-only models in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1903.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1903.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLA-EAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLA-EAC (EAC-augmented SpatialVLA ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of SpatialVLA where the native end-to-end action output is replaced at approach time by EAC-guided motion planning, demonstrating the plug-and-play value of analytic concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLA-EAC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SpatialVLA architecture augmented by injecting GRACE's EAC module for the late-stage approach and manipulation execution, combining VLA perception/reasoning with analytic-concept-guided low-level control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Inherited from SpatialVLA/VLA backbone (not detailed in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation benchmark evaluation (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same as SpatialVLA baseline; evaluate how EAC-guided execution affects success on Widow-X and Google-robot suites.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>EACs restore geometric and kinematic alignment between language-selected manipulation strategies and physically executable parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>SpatialVLA-EAC improves SpatialVLA average success to 69.8% on Widow-X and to 83.4% on Google-robot tasks (reported in paper), indicating substantial gains from EAC integration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Demonstrates practical grounding benefit: swapping to EAC-guided planning significantly improves success metrics, indicating EACs concretely connect language-selected strategies to execution.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Supports a hierarchical interpretation where analytic mid-level representations improve low-level execution when combined with high-level VLA reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>EAC plug-in benefits existing VLA architectures across different underlying VLMs (small variation between VLM backends).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not separately reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Improvements reported in zero-shot evaluation contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Shows EAC augmentation outperforms the native SpatialVLA end-to-end action output on reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1903.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1903.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SoFar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SoFar (Language-grounded orientation for manipulation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent baseline for language-grounded orientation and spatial reasoning in object manipulation; used as a strong published baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SoFar</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A model that grounds language for orientation-aware spatial reasoning in manipulation tasks (details not exhaustively provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Reported baseline for Widow-X and other manipulation tasks; used as comparative benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to bridge language and orientation-aware spatial reasoning; paper cites SoFar's published performance (58.3% avg on Widow-X) as baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Published SoFar baseline reported as 58.3% average success on Widow-X tasks (cited in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>SoFar focuses on language-grounded orientation but details/analysis not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not explicitly compared within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1903.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1903.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ManipLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ManipLLM (Embodied multimodal LLM for object-centric robotic manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based embodied manipulation baseline that the paper compares against on articulated object manipulation benchmarks; GRACE reports substantially higher scores than ManipLLM on several object categories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ManipLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An embodied multimodal large language model designed to produce object-centric manipulation policies or affordance predictions (paper references it as a comparative prior).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Articulated-object manipulation benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Manipulation of objects like faucets and cabinet doors in simulation; reported baseline scores (e.g., ManipLLM scored 0.26 on faucets and 0.71 on cabinet doors vs GRACE 0.65 and 0.91 respectively in reported table).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>ManipLLM is LLM-based; paper positions GRACE's analytic-concept grounding as outperforming ManipLLM's approach for precise kinematic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported baseline numbers in the paper: ManipLLM had lower scores (e.g., 0.26 faucets, 0.71 cabinet doors in Table 3) compared to GRACE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper suggests that ManipLLM's LLM-based predictions are outperformed by GRACE's analytic, geometry-grounded approach, indicating GRACE provides stronger action grounding for kinematic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not detailed beyond comparative scores.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not emphasized in paper's reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not explicitly compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1903.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1903.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grounded-SAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded-SAM (GroundingDINO + SAM integration for open-vocabulary segmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Visual Foundation Model pipeline (GroundingDINO for localization + SAM for mask generation) used by GRACE to produce 2D masks that are back-projected into 3D point clouds for part-level grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grounded-SAM (GroundingDINO + SAM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An integrated open-vocabulary segmentation pipeline: GroundingDINO provides region proposals conditioned on language prompts and SAM produces high-quality masks; in GRACE, GroundingDINO was fine-tuned and SAM kept frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>GroundingDINO and SAM are pretrained visual foundation models (GroundingDINO trained for open-set detection/grounding; SAM trained for general segmentation); exact pretraining corpora not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in detail here; GroundingDINO and SAM are trained on large-scale detection/segmentation datasets and GroundingDINO uses grounded pretraining for open-set detection.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Part-level instance segmentation and 3D back-projection for object-centric grounding</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Generates 2D masks for task-relevant objects/parts, which are back-projected with depth to form point clouds used for analytic-concept parameter regression.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Grounded-SAM aligns language-derived category prompts (from VLM parsing) with visual masks, enabling explicit language→perception grounding used downstream by EAC instantiation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Grounded-SAM used successfully in GRACE pipeline; no isolated numerical segmentation metrics reported in this paper, but authors state VFM-based grounding demonstrates high stability and contributes negligibly to system failures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Serves as the perceptual grounding step that converts language-referenced object nodes into geometric point clouds, enabling subsequent analytic parameter estimation and action grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper notes fine-tuning Grounding-DINO with bounding boxes + language prompts improved segmentation reliability for actionable parts.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported separately; used in zero-shot pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Grounded-SAM is a visual foundation model that requires language prompts for grounding; not compared directly to vision-only segmentation baselines in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Where2Act: From pixels to actions for articulated 3D objects <em>(Rating: 2)</em></li>
                <li>Where2Explore: Few-shot affordance learning for unseen novel categories of articulated objects <em>(Rating: 2)</em></li>
                <li>ManipLLM: Embodied multimodal large language model for object-centric robotic manipulation <em>(Rating: 2)</em></li>
                <li>OpenVLA: An open-source vision-language-action model <em>(Rating: 2)</em></li>
                <li>Octo: An open-source generalist robot policy <em>(Rating: 1)</em></li>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1903",
    "paper_id": "paper-281951533",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "GRACE",
            "name_full": "GRACE: From VLM-based Grounding to Robotic manipulation through Analytic Concept Execution",
            "brief_description": "A framework that grounds vision-language model reasoning into executable analytic concepts (EACs) — parametric, math-based blueprints encoding object geometry, affordances, and manipulation routines — enabling zero-shot, physically-feasible robotic manipulation by converting language+RGB-D into grasp poses, force vectors, and motion plans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GRACE (system)",
            "model_description": "A pipeline combining an off-the-shelf Vision-Language Model (VLM) for high-level parsing/reasoning, Visual Foundation Models (GroundingDINO + SAM) for segmentation, a point-cloud encoder (Point-Transformer) to regress analytic concept parameters, and analytic manipulation blueprints that produce grasp poses and force directions passed to a standard motion planner and robot controller.",
            "pretraining_type": "uses off-the-shelf multimodal VLMs (multimodal vision+language pretrained models) and standard visual foundation models; the GRACE framework itself does not rely on task-specific action pretraining.",
            "pretraining_data_description": "Not specified in paper for GRACE itself; relies on pretrained VLMs (e.g., GPT-4o, Qwen2.5-VL) and VFMs (GroundingDINO, SAM). The paper states VLMs are large-scale pretrained and carry commonsense/world knowledge (implying training on large image–text corpora).",
            "target_task_name": "Robotic manipulation of articulated and rigid objects (zero-shot)",
            "target_task_description": "Manipulation tasks (open/close drawers, doors, handles, assorted object interactions) executed by a parallel-gripper robot; action space includes continuous 6-DoF grasp poses, force-direction vectors and joint-space trajectories produced by a motion planner; evaluated in simulation (SimplerEnv, SAPIEN) and on a real RM75 arm/tabletop setup.",
            "semantic_alignment": "Paper explicitly constructs semantic grounding: VLM maps instruction and natural-language synopses of analytic concepts to specific concept candidates, enabling alignment between verbs/intent and geometric manipulation primitives; degree of overlap with pretraining corpora not quantified.",
            "performance_with_language_pretraining": "Simulation: GRACE (GPT-4o) average success rate 86.1% on Widow-X tasks and 90.1% mean success on Google-robot tasks; articulated Open/Close Drawer improved to 90.3% (variant aggregation). Real-world per-object zero-shot success examples: Stapler 90%, Mug 80%, Bucket 70%, Microwave 80%.",
            "performance_without_language_pretraining": "A variant using oracle (ground-truth) concept labels instead of VLM concept selection (reported as GRACE-w/o-VLM/oracle concept) yields an average articulated-object score of 0.80 (vs GPT-4o automatic selection 0.77 on the articulated-object benchmark cited); other baseline variants without VLM (or replaced modules) reported lower success in comparative baselines (see baselines). Exact end-to-end numbers for a fully language-ablated GRACE pipeline not fully enumerated beyond this oracle-vs-VLM comparison.",
            "sample_efficiency_comparison": "The paper claims GRACE reduces dependence on large quantities of task-specific action demonstrations, but does not provide quantified sample-efficiency curves or numeric comparisons (no episodes/steps-to-threshold reported).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention-map analysis or visualization of VLM attention reported in the paper.",
            "embedding_space_analysis": "No detailed analysis reported of embedding/feature-space structure or clustering comparing pretrained vs non-pretrained representations.",
            "action_grounding_evidence": "Strong procedural evidence: analytic manipulation blueprints map chosen manipulation verbs (e.g., 'pull', 'push') to concrete force-direction formulas and parameterized grasp-pose computations; the VLM selects manipulation modules based on synopses and the instantiated EAC yields grasp G and force F that are transformed to world frame and executed—presented as direct language→geometry→action grounding.",
            "hierarchical_features_evidence": "Paper describes a hierarchical architecture (VLM for high-level parsing, EACs as mid-level parametric descriptors, low-level motion planner for execution) and shows performance gains come from the analytic-concept layer, but does not include layerwise feature analyses differentiating low vs high-level representations.",
            "transfer_conditions": "Transfer success is sensitive to: (1) correctness of concept selection (VLM misclassification causes most remaining failures), (2) pose-estimation and IK quality (pose/IK errors are primary failure modes), and (3) availability of sufficient geometric information (multi-view reconstruction improves pose estimation). GRACE is robust across different underlying VLM backends (Qwen2.5-VL trails GPT-4o by ~1–2 percentage points).",
            "novel_vs_familiar_objects": "Paper emphasizes zero-shot generalization to unseen articulated objects and reports high success rates on novel instances; explicit numeric split conditioned on whether objects were present in VLM pretraining is not provided. Oracle concept vs VLM selection comparison implies small drop when moving from ground-truth labels to VLM-selected concepts (avg 0.80→0.77).",
            "zero_shot_or_few_shot": "Yes — GRACE demonstrates zero-shot capabilities: reported zero-shot success rates in simulation (86.1% Widow-X, 90.1% Google-robot) and several zero-shot real-world object successes (90%, 80%, 70%, 80%). No few-shot tuning curves provided.",
            "layer_analysis": "No layerwise ablation or probing of VLM internals beyond swapping VLM backends and an ablation replacing VLM concept selection with oracle labels.",
            "negative_transfer_evidence": "No systematic negative transfer evidence reported; paper notes occasional VLM misclassification can slightly reduce performance, but no cases where VLM usage harms overall performance compared to oracle concept labels beyond the small gap reported.",
            "comparison_to_vision_only": "Paper compares to pixel-level affordance methods and VLA baselines, showing large improvements when EACs scaffold VLM reasoning; explicit head-to-head comparison to pure vision-only ImageNet-pretrained models is not reported. Replacing SpatialVLA's end-to-end action output with EAC-guided motion planning (SpatialVLA-EAC) substantially improved performance, suggesting analytic/semantic grounding helps more than end-to-end visual policies alone.",
            "temporal_dynamics": "No analysis of representation or performance dynamics across fine-tuning steps or training epochs is provided.",
            "dimensionality_analysis": "No dimensionality or intrinsic-dimension analyses reported.",
            "uuid": "e1903.0"
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (multimodal Vision-Language model used as VLM backend)",
            "brief_description": "An off-the-shelf multimodal large language/vision-language model used for task parsing, chain-of-thought reasoning, and concept selection in GRACE; chosen as the highest-performing VLM backend in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (used as VLM backend)",
            "model_description": "A multimodal vision-language model (LLM with multimodal input capability) used to parse instructions, perform chain-of-thought object parsing, select analytic concepts, and choose manipulation modules.",
            "pretraining_type": "Multimodal vision-language pretraining (off-the-shelf); paper does not specify pretraining dataset.",
            "pretraining_data_description": "Not specified in paper; implied large-scale image+text corpora with commonsense/world knowledge.",
            "target_task_name": "VLM-driven task parsing and concept selection for robotic manipulation (within GRACE)",
            "target_task_description": "Performs coarse-to-fine object identification, state classification for articulated parts, task decomposition into sub-tasks, and selection of analytic manipulation modules given RGB-D scenes and natural-language instructions.",
            "semantic_alignment": "Used explicitly to align language instructions and natural-language synopses of analytic concepts with 3D perceptual inputs; paper reports that GPT-4o selection yields near-oracle performance (small gap ~3 percentage points in some articulated benchmarks).",
            "performance_with_language_pretraining": "When used as GRACE backend: system success rates reported as GRACE(GPT-4o) — 86.1% (Widow-X avg), 90.1% (Google-robot avg); articulated-object avg ~0.77 in one benchmark.",
            "performance_without_language_pretraining": "Not applicable for GPT-4o itself; ablation replacing GPT-4o selection with oracle concept labels gives oracle avg ~0.80 (slightly higher).",
            "sample_efficiency_comparison": "No sample-efficiency numbers specific to GPT-4o reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in the paper.",
            "embedding_space_analysis": "Not reported in the paper.",
            "action_grounding_evidence": "GPT-4o is used to select manipulation modules and synopses that are then instantiated into analytic blueprints; the resulting pipeline demonstrates language→affordance grounding practically via the analytic blueprint execution.",
            "hierarchical_features_evidence": "GPT-4o provides high-level symbolic/semantic outputs which are mapped to mid-level analytic representations; no internal feature-level analysis provided.",
            "transfer_conditions": "Paper reports GPT-4o-based GRACE performs robustly and only slightly better than open-source Qwen2.5-VL, suggesting transfer is tolerant to VLM choice; specific dataset-domain transfer details not provided.",
            "novel_vs_familiar_objects": "GPT-4o selection yields near-oracle performance on novel articulated instances per paper; exact breakdown versus familiar objects not quantified.",
            "zero_shot_or_few_shot": "Demonstrated zero-shot selection capability within GRACE with small performance gap to oracle labels.",
            "layer_analysis": "No layerwise analysis reported.",
            "negative_transfer_evidence": "No explicit negative transfer reported.",
            "comparison_to_vision_only": "Not directly compared to vision-only models; used as a semantic reasoning module complementary to visual foundation models.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1903.1"
        },
        {
            "name_short": "Qwen2.5-VL",
            "name_full": "Qwen2.5-VL (open-source vision-language model)",
            "brief_description": "An open-source multimodal VLM used as an alternative backend to GPT-4o in GRACE; achieves performance within ~1–2 percentage points of GPT-4o, indicating analytic concepts drive most gains.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-VL (used as VLM backend)",
            "model_description": "An off-the-shelf multimodal vision-language model serving the same role as GPT-4o for parsing and concept selection within GRACE.",
            "pretraining_type": "Multimodal vision-language pretraining (not specified in paper).",
            "pretraining_data_description": "Not specified in the paper.",
            "target_task_name": "VLM-driven parsing and concept selection for robotic manipulation (within GRACE)",
            "target_task_description": "Same role as GPT-4o: parse instruction, select analytic concept synopses, guide blueprint selection given RGB-D input.",
            "semantic_alignment": "Reported to perform almost as well as GPT-4o; the paper interprets this as evidence that the analytic-concept layer, not the specific VLM, is the main contributor to transfer success.",
            "performance_with_language_pretraining": "GRACE with Qwen2.5-VL trails GPT-4o by only 1–2 percentage points on reported robot families but still outperforms all external baselines.",
            "performance_without_language_pretraining": "Not applicable; no explicit baseline for Qwen2.5-VL without its pretrained weights provided.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Used to map natural-language synopses to analytic manipulation modules similarly to GPT-4o; practical grounding shown via overall system performance.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Paper reports robustness across VLM backends; minor performance differences suggest backend choice is not primary factor.",
            "novel_vs_familiar_objects": "Not detailed separately from GRACE overall results.",
            "zero_shot_or_few_shot": "Used in zero-shot GRACE runs with high success; exact per-object breakdown not given.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not directly compared to vision-only pretraining in the paper.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1903.2"
        },
        {
            "name_short": "SpatialVLA",
            "name_full": "SpatialVLA (Vision-Language-Action model baseline)",
            "brief_description": "A VLA baseline that outputs end-to-end actions from visual and language inputs; used as a competitor and as a base for an EAC-augmented variant in ablations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SpatialVLA",
            "model_description": "A Vision-Language-Action architecture that produces action outputs directly from multimodal inputs (image + language); details of its pretraining are not specified in this paper.",
            "pretraining_type": "Not specified in this paper (referenced as a concurrent VLA approach).",
            "pretraining_data_description": "Not specified in paper.",
            "target_task_name": "Robotic manipulation (baseline comparative evaluation)",
            "target_task_description": "End-to-end action generation for manipulation tasks in SimplerEnv/SAPIEN benchmarks; continuous control outputs used by robot controllers.",
            "semantic_alignment": "SpatialVLA is intended to align visual and linguistic inputs to actions end-to-end, but paper argues it can lose geometric detail when compressing 3D interactions into 2D/textual representations.",
            "performance_with_language_pretraining": "SpatialVLA reported previously (in related work) but in this paper SpatialVLA's published baseline for Open/Close Drawer was 36.2% (cited), while SpatialVLA-EAC variant performance is reported below.",
            "performance_without_language_pretraining": "Not specified.",
            "sample_efficiency_comparison": "Not reported in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Paper argues end-to-end VLA methods can lose geometric and kinematic detail; no direct analysis provided for SpatialVLA in this paper.",
            "hierarchical_features_evidence": "Not analyzed in the paper.",
            "transfer_conditions": "Paper demonstrates that replacing SpatialVLA's action output with EAC-guided planning substantially improves robustness, suggesting SpatialVLA alone is sensitive to geometric fidelity loss.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not emphasized; baseline numbers provided for comparison.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not explicitly reported, but paper suggests end-to-end VLA may underperform on tasks requiring precise kinematic reasoning.",
            "comparison_to_vision_only": "Not directly compared to pure vision-only models in this work.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1903.3"
        },
        {
            "name_short": "SpatialVLA-EAC",
            "name_full": "SpatialVLA-EAC (EAC-augmented SpatialVLA ablation)",
            "brief_description": "A variant of SpatialVLA where the native end-to-end action output is replaced at approach time by EAC-guided motion planning, demonstrating the plug-and-play value of analytic concepts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SpatialVLA-EAC",
            "model_description": "SpatialVLA architecture augmented by injecting GRACE's EAC module for the late-stage approach and manipulation execution, combining VLA perception/reasoning with analytic-concept-guided low-level control.",
            "pretraining_type": "Inherited from SpatialVLA/VLA backbone (not detailed in paper).",
            "pretraining_data_description": "Not specified.",
            "target_task_name": "Robotic manipulation benchmark evaluation (ablation)",
            "target_task_description": "Same as SpatialVLA baseline; evaluate how EAC-guided execution affects success on Widow-X and Google-robot suites.",
            "semantic_alignment": "EACs restore geometric and kinematic alignment between language-selected manipulation strategies and physically executable parameters.",
            "performance_with_language_pretraining": "SpatialVLA-EAC improves SpatialVLA average success to 69.8% on Widow-X and to 83.4% on Google-robot tasks (reported in paper), indicating substantial gains from EAC integration.",
            "performance_without_language_pretraining": "Not specified.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Demonstrates practical grounding benefit: swapping to EAC-guided planning significantly improves success metrics, indicating EACs concretely connect language-selected strategies to execution.",
            "hierarchical_features_evidence": "Supports a hierarchical interpretation where analytic mid-level representations improve low-level execution when combined with high-level VLA reasoning.",
            "transfer_conditions": "EAC plug-in benefits existing VLA architectures across different underlying VLMs (small variation between VLM backends).",
            "novel_vs_familiar_objects": "Not separately reported.",
            "zero_shot_or_few_shot": "Improvements reported in zero-shot evaluation contexts.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Shows EAC augmentation outperforms the native SpatialVLA end-to-end action output on reported benchmarks.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1903.4"
        },
        {
            "name_short": "SoFar",
            "name_full": "SoFar (Language-grounded orientation for manipulation baseline)",
            "brief_description": "A recent baseline for language-grounded orientation and spatial reasoning in object manipulation; used as a strong published baseline in comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SoFar",
            "model_description": "A model that grounds language for orientation-aware spatial reasoning in manipulation tasks (details not exhaustively provided in this paper).",
            "pretraining_type": "Not specified in this paper.",
            "pretraining_data_description": "Not specified here.",
            "target_task_name": "Robotic manipulation (baseline)",
            "target_task_description": "Reported baseline for Widow-X and other manipulation tasks; used as comparative benchmark.",
            "semantic_alignment": "Designed to bridge language and orientation-aware spatial reasoning; paper cites SoFar's published performance (58.3% avg on Widow-X) as baseline for comparison.",
            "performance_with_language_pretraining": "Published SoFar baseline reported as 58.3% average success on Widow-X tasks (cited in paper).",
            "performance_without_language_pretraining": "Not specified here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper.",
            "embedding_space_analysis": "Not reported in this paper.",
            "action_grounding_evidence": "SoFar focuses on language-grounded orientation but details/analysis not provided in this paper.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not detailed in this paper.",
            "novel_vs_familiar_objects": "Not detailed here.",
            "zero_shot_or_few_shot": "Not specified here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not explicitly compared within this paper.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1903.5"
        },
        {
            "name_short": "ManipLLM",
            "name_full": "ManipLLM (Embodied multimodal LLM for object-centric robotic manipulation)",
            "brief_description": "An LLM-based embodied manipulation baseline that the paper compares against on articulated object manipulation benchmarks; GRACE reports substantially higher scores than ManipLLM on several object categories.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ManipLLM",
            "model_description": "An embodied multimodal large language model designed to produce object-centric manipulation policies or affordance predictions (paper references it as a comparative prior).",
            "pretraining_type": "Not specified in this paper.",
            "pretraining_data_description": "Not specified in this paper.",
            "target_task_name": "Articulated-object manipulation benchmark",
            "target_task_description": "Manipulation of objects like faucets and cabinet doors in simulation; reported baseline scores (e.g., ManipLLM scored 0.26 on faucets and 0.71 on cabinet doors vs GRACE 0.65 and 0.91 respectively in reported table).",
            "semantic_alignment": "ManipLLM is LLM-based; paper positions GRACE's analytic-concept grounding as outperforming ManipLLM's approach for precise kinematic tasks.",
            "performance_with_language_pretraining": "Reported baseline numbers in the paper: ManipLLM had lower scores (e.g., 0.26 faucets, 0.71 cabinet doors in Table 3) compared to GRACE.",
            "performance_without_language_pretraining": "Not detailed in this paper.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Paper suggests that ManipLLM's LLM-based predictions are outperformed by GRACE's analytic, geometry-grounded approach, indicating GRACE provides stronger action grounding for kinematic tasks.",
            "hierarchical_features_evidence": "Not analyzed here.",
            "transfer_conditions": "Not detailed beyond comparative scores.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not emphasized in paper's reported comparisons.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not explicitly compared here.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1903.6"
        },
        {
            "name_short": "Grounded-SAM",
            "name_full": "Grounded-SAM (GroundingDINO + SAM integration for open-vocabulary segmentation)",
            "brief_description": "A Visual Foundation Model pipeline (GroundingDINO for localization + SAM for mask generation) used by GRACE to produce 2D masks that are back-projected into 3D point clouds for part-level grounding.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Grounded-SAM (GroundingDINO + SAM)",
            "model_description": "An integrated open-vocabulary segmentation pipeline: GroundingDINO provides region proposals conditioned on language prompts and SAM produces high-quality masks; in GRACE, GroundingDINO was fine-tuned and SAM kept frozen.",
            "pretraining_type": "GroundingDINO and SAM are pretrained visual foundation models (GroundingDINO trained for open-set detection/grounding; SAM trained for general segmentation); exact pretraining corpora not specified in this paper.",
            "pretraining_data_description": "Not specified in detail here; GroundingDINO and SAM are trained on large-scale detection/segmentation datasets and GroundingDINO uses grounded pretraining for open-set detection.",
            "target_task_name": "Part-level instance segmentation and 3D back-projection for object-centric grounding",
            "target_task_description": "Generates 2D masks for task-relevant objects/parts, which are back-projected with depth to form point clouds used for analytic-concept parameter regression.",
            "semantic_alignment": "Grounded-SAM aligns language-derived category prompts (from VLM parsing) with visual masks, enabling explicit language→perception grounding used downstream by EAC instantiation.",
            "performance_with_language_pretraining": "Grounded-SAM used successfully in GRACE pipeline; no isolated numerical segmentation metrics reported in this paper, but authors state VFM-based grounding demonstrates high stability and contributes negligibly to system failures.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Serves as the perceptual grounding step that converts language-referenced object nodes into geometric point clouds, enabling subsequent analytic parameter estimation and action grounding.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Paper notes fine-tuning Grounding-DINO with bounding boxes + language prompts improved segmentation reliability for actionable parts.",
            "novel_vs_familiar_objects": "Not specified.",
            "zero_shot_or_few_shot": "Not reported separately; used in zero-shot pipeline.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Grounded-SAM is a visual foundation model that requires language prompts for grounding; not compared directly to vision-only segmentation baselines in paper.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1903.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Where2Act: From pixels to actions for articulated 3D objects",
            "rating": 2
        },
        {
            "paper_title": "Where2Explore: Few-shot affordance learning for unseen novel categories of articulated objects",
            "rating": 2
        },
        {
            "paper_title": "ManipLLM: Embodied multimodal large language model for object-centric robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "OpenVLA: An open-source vision-language-action model",
            "rating": 2
        },
        {
            "paper_title": "Octo: An open-source generalist robot policy",
            "rating": 1
        },
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 1
        }
    ],
    "cost": 0.0197325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EXECUTABLE ANALYTIC CONCEPTS AS THE MISSING LINK BETWEEN VLM INSIGHT AND PRECISE MANIPULATION
9 Oct 2025</p>
<p>Mingyang Sun sunmingyang@westlake.edu.cn 
Shanghai Innovation Institute</p>
<p>Westlake University</p>
<p>Jiude Wei 
Shanghai Jiao Tong University</p>
<p>Qichen He 
Shanghai Innovation Institute</p>
<p>Shanghai Jiao Tong University</p>
<p>Donglin Wang 
Westlake University</p>
<p>Cewu Lu 
Shanghai Innovation Institute</p>
<p>Shanghai Jiao Tong University</p>
<p>Jianhua Sun 
Shanghai Jiao Tong University</p>
<p>EXECUTABLE ANALYTIC CONCEPTS AS THE MISSING LINK BETWEEN VLM INSIGHT AND PRECISE MANIPULATION
9 Oct 2025F6E9625CEAE9D9362BF7A22A501563E4arXiv:2510.07975v1[cs.RO]
Enabling robots to perform precise and generalized manipulation in unstructured environments remains a fundamental challenge in embodied AI.While Vision-Language Models (VLMs) have demonstrated remarkable capabilities in semantic reasoning and task planning, a significant gap persists between their high-level understanding and the precise physical execution required for real-world manipulation.To bridge this "semantic-to-physical" gap, we introduce GRACE, a novel framework that grounds VLM-based reasoning through executable analytic concepts (EAC)-mathematically defined blueprints that encode object affordances, geometric constraints, and semantics of manipulation.Our approach integrates a structured policy scaffolding pipeline that turn natural language instructions and visual information into an instantiated EAC, from which we derive grasp poses, force directions and plan physically feasible motion trajectory for robot execution.GRACE thus provides a unified and interpretable interface between high-level instruction understanding and low-level robot control, effectively enabling precise and generalizable manipulation through semantic-physical grounding.Extensive experiments demonstrate that GRACE achieves strong zero-shot generalization across a variety of articulated objects in both simulated and real-world environments, without requiring task-specific training.</p>
<p>INTRODUCTION</p>
<p>Developing general robotic manipulation systems that can operate effectively in complex, dynamic, and unstructured real-world environments remains a longstanding challenge (Xu et al., 2024).Recent advances in large-scale pretraining have enabled Large Language Models (LLMs) (Naveed et al., 2025;Achiam et al., 2023), including multimodal Vision-Language Models (VLMs) (Zhang et al., 2024;Hurst et al., 2024), to acquire rich world knowledge, demonstrating considerable potential in robotic manipulation tasks.These models are capable of processing complex semantic information and facilitating robust reasoning and planning across diverse scenarios, substantially reducing the dependence on large quantities of high-quality action demonstration data.</p>
<p>Existing VLM-based methods for robotic manipulation have achieved promising results in several areas: task planning (Ahn et al., 2022;Driess et al., 2023), where VLMs interpret natural language instructions and produce high-level action sequences; error detection and recovery (Duan et al., 2024a), where they identify execution failures or environmental anomalies and trigger replanning; and fine-grained action generation (Huang et al., 2025;2023), where visual representations are extracted and used by VLMs to infer constraints, which are then solved to produce executable robot motions.Another popular approach integrates VLMs with Vision-Language-Action (VLA) models to form a hierarchical architecture: the high-level layer provides semantic reasoning through the VLM, while the low-level layer handles motion planning and execution via the VLA (Ma et al., 2024;Shi et al., 2025).</p>
<p>By mediating between semantic reasoning and physical execution through analytic concepts, our approach leverages the robust commonsense capabilities of LLMs while enabling generalized, interpretable, and precise manipulation of articulated objects.We propose GRACE (From VLMbased Grounding to Robotic manipulation through Analytic Concept Execution) with the following contributions:</p>
<p>• We introduce a novel plug-and-play framework that elicits the inherent robotic control potential of VLMs by structured, physics-aware object representations.The framework provides a unified interface that bridges high-level instructions and low-level executable actions for long-horizon manipulation.</p>
<p>• We develop a policy scaffolding pipeline that incorporates analytic concept to translate object-centric semantic knowledge into physically meaningful blueprint, thereby building executable guidance for robot control policies.The executive analytic concepts bridge the gap between VLM's commonsense reasoning and precise physical cognition.</p>
<p>• We demonstrate our approach's outstanding performance in a wide range of manipulation tasks, showcasing the remarkable zero-shot generalization capability in both simulated and real-world environments.We also highlight the compatibility of our EAC-based approach with VLA architecture.</p>
<p>RELATED WORK</p>
<p>Structural Representations for Manipulation.The structural representation chosen for a manipulation system dictates how its modules interact and, consequently, shapes the system's assumptions, efficiency, and overall capability.Traditional approaches rely on rigid-body models: once an object's geometry and dynamics are fully specified, well-understood rigid-body motions can be executed in free space and long-range dependencies are handled efficiently (Migimatsu &amp; Bohg, 2020;Dantam et al., 2018).Yet this strategy presupposes that accurate geometry and physical parameters of the environment are available a priori-a requirement rarely met outside carefully curated setups.To relax this constraint, recent research has explored data-driven alternatives, including learned object-centric embeddings (Hsu et al., 2023;Cheng et al., 2023;Yuan et al., 2022), particle-based modeling (Bauer et al., 2024;Abou-Chakra et al., 2024), and keypoint or descriptors (Simeonov et al., 2022;Manuelli et al., 2019;Huang et al., 2024b).Although promising, these approaches often suffer from instability, manual annotation, or a reliance on hand-crafted geometric priors, limiting their reliability and breadth of application.</p>
<p>Vision-Language Models for Robotics.Our work builds upon recent advances in Vision-Language Models (VLMs) for robotic control, which demonstrate remarkable capabilities in scene understanding and high-level commonsense reasoning.Existing approaches can be broadly categorized into several paradigms (Shao et al., 2025).Some studies integrate environmental perception-including visual, linguistic, and robot state information-along with action generation into a unified Visual-Language-Action (VLA) model (O 'Neill et al., 2024;Zitkovich et al., 2023;Deng et al., 2025).Alternatively, dual-system architectures employ a VLM backbone for scene interpretation and a separate action expert for policy generation, communicating through latent representation exchanges.Despite their promise, these methods often require large-scale data collection and face challenges in generalizing beyond training distributions.Other efforts seek to leverage visual foundation models to extract operational primitives, which then serve as visual or linguistic prompts to VLMs for task-level reasoning (Duan et al., 2024b;Huang et al., 2024a;Pan et al., 2025).These systems typically rely on traditional motion planners for low-level control.However, such approaches are limited by the loss of geometric detail when compressing 3D physical interactions into 2D images or 1D textual descriptions, as well as by the inherent hallucination problems of VLMs.These limitations often compromise the accuracy and executability of high-level plans generated by VLMs.</p>
<p>Addressing these challenges, we introduce analytic concepts as a core component that scaffolds the VLM's reasoning process, enabling it to progressively derive physical knowledge of objects from fine-grained 3D geometric information and produce executable and accurate manipulation plans.</p>
<p>ANALYTIC CONCEPTS</p>
<p>The analytic concepts take inspiration from the advancements of researches on human cognition and brain science, where it is discovered that we humans learn about the physical world by perceiving geometry patterns from objects and inducing them along with related knowledge as commonsense for future reference.Based on such findings, a novel knowledge annotation paradigm for object understanding tasks is established by explicitly modeling such abstract commonsense information as concepts for regular geometry patterns and reversing the induction process (Sun et al., 2024).Specifically, by generalizing the concepts towards certain objects, various knowledge associated with the concepts can be automatically propagated to all these objects.</p>
<p>In engineering and architecture, a blueprint is a detailed plan that defines the structure of an object through specifications and guides its fabrication and assembly.We introduce analytic concepts to play an analogous role for robots: they are procedural, mathematics-based definitions that capture the shared physical essence of an object or its sub-components, turning abstract knowledge into an executable blueprint for manipulation.At their foundation, analytic concepts include a "factory" of geometric concept assets (Fig. 1a).Each asset code provides a set of free parameters to represent diverse variations, a canonical structural definition, and affordance annotations as concise descriptors of how the object can be grasped or acted upon.Besides, a function is also provided to render instances of the assets in 3D space.These assets are the atomic building blocks from which every executable blueprint is assembled with building structural blueprint and manipulation blueprint.</p>
<p>The analytic structural blueprint is a series of mathematical procedures revealing the essential commonality of the spatial structure, including spatial layout and structural relationships, shared by all instances of the concept, as shown in Fig. 1b.Further, there are variable parameters in the procedures to represent the variations among different physical instances.That is, a physical instance of this concept can be created with specific parameters, and in turn, a target in the physical world can be also resolved into parameters of a concept.</p>
<p>Effective interaction requires more than geometric fidelity; it demands knowledge of functional properties such as affordances and force dynamics.To this end, we can ground manipulation blueprint (Fig. 1c) that meet the functional properties of the concept and force directions that would cause effective movement.Similarly to the analytic structural blueprint, the analytic manipulation ... return translate_local2world(grasp_pose) def push(obj): force_direction = get_direction(obj.axis...) def pull(obj): force_direction = get_direction(obj.axis...) blueprint is also formulated by mathematical procedures with variable parameters.It may incorporate multiple interaction strategies, each accompanied by a precise natural-language synopsis to facilitate high-level reasoning by language models.
¦ ¥ ℱ !"## ℱ !"$% (c) Manipulation Blueprint</p>
<p>METHODOLOGY</p>
<p>Problem Formulation.This paper addresses the challenge of enabling a robotic system to perform manipulation tasks based on high-level language instructions.Our system is given a visual observation O t of the environment and a natural language instruction l describing the desired task.The core difficulty lies in bridging the gap between high-level human commands and low-level physical actions due to the complexity of the object operated.The language instruction l can be both arbitrarily long-horizon and under-specified, requiring the system to possess advanced commonsense reasoning to infer user intent and contextual details.To successfully complete the task with a parallel gripper, the robot must not only understand the object and task description but also manage the complex physics of contact-rich interactions.This necessitates an intelligent system capable of generating precise affordances and robust grasp strategies.</p>
<p>Overview As illustrated in Figure 2, the proposed GRACE framework orchestrates a pipeline built around a Vision-Language Model (VLM) that transforms a natural language instruction and an RGB-D image into a successful robot action.The process begins with (I) Task Parsing, where the VLM parses and comprehends the user command (e.g., "Open the upper handle.")within the visual context of the observed scene.The core contribution of our work lies in (II) Policy Scaffolding, a sophisticated VLM-driven process that constructs an Executable Analytic Concept (EAC).This is accomplished through a structured sequence: first segmenting the target point cloud, and then grounding both structural and manipulation blueprint.Finally, the VLM performs reasoning over this rich, structured EAC to generate precise motion parameters, which are subsequently passed to the motion planner for (III) Robot Execution.The EAC acts as the essential missing link that grounds the VLM's abstract "insight" into a physically precise and executable format.</p>
<p>SPATIAL-AWARE TASK PARSING</p>
<p>Object Parsing.The Object Parsing step serves as the foundational stage for perception and language grounding.Its objective is to interpret the natural language instruction l within the context of the RGB-D scene images, producing a structured set of task-relevant object entities along with their critical spatial information.This process distills the "what" and "where" from the command, delivering a clean symbolic input for downstream task reasoning and planning.</p>
<p>We implement the parsing through a structured chain-of-thought (CoT) reasoning process with two core steps: (i) The VLM first performs a coarse-to-fine analysis to identify primary objects, extracting noun phrases and their synonymous references grounded in the visual scene layout.(ii) The VLM then assesses object states-particularly for articulated objects-and identifies binary spatial relationships between entities.The final output is a structured graph G = (V, E), where V denotes the set of object nodes-each represented as a structured dictionary containing id, name, and state-and E constitutes a set of directed spatial relationships between objects, each expressed as a triple e ij = (v i , r, v j ).This object-centric symbolic graph provides a semantically rich and structurally explicit representation for subsequent reasoning stages.</p>
<p>Task Decomposition.For complex, long-horizon tasks, our approach first decomposes the primary task into a series of stages, each defined by object interaction primitives with associated spatial constraints.Subsequently, a VLM, leveraging object parsing information, is used to decompose the main task instruction l into a series of discrete sub-tasks, represented as l i , along with a corresponding verification condition c i , for i ∈ {1, . . ., n}.This transforms the instruction l into a sequence of specific sub-tasks and conditions: {(l 1 , c 1 ), (l 2 , c 2 ), . . ., (l n , c n )}.For instance, the high-level task "open the microwave door" could be decomposed into sub-tasks like "grasp the door handle" and "pull open the door," with verification conditions such as "is the handle grasped?" and "is the door opened?".Each sub-task then undergoes an execution loop, as depicted in Fig. 2.After the initial execution attempt, the task reasoning program is replaced with a corresponding condition verification program to ensure the successful completion of that sub-task.This structured approach allows for the precise definition of task requirements and facilitates the execution of complex manipulation tasks.See Appendix D for prompts.</p>
<p>POLICY SCAFFOLDING</p>
<p>Policy scaffolding as core first determines the target object or part that needs to be analyzed, and then builds the structural and manipulation blueprint in turn to obtain the executable analysis concept.</p>
<p>TARGET IDENTIFICATION</p>
<p>In the object parsing step, we obtain a structured object graph G = (V, E).Using the names from V as object category prompts, we leverage Visual Foundation Models (VFMs) to perform openvocabulary instance segmentation.Specifically, GroundingDINO (Liu et al., 2024) localizes referred objects, and the Segment Anything Model (SAM) (Kirillov et al., 2023) generates fine-grained 2D masks M = {M i | i = 1, 2, . . ., m} for all foreground objects relevant to the task.Each 2D mask M i is then back-projected into 3D using the corresponding depth image, producing a set of objectcentric 3D point clouds P = {P i | i = 1, 2, . . ., m}.These point clouds are associated with the semantic nodes v i ∈ V, effectively grounding the symbolic elements of G into geometrically precise representations.</p>
<p>STRUCTURAL BLUEPRINTING</p>
<p>With the obtained target part's point cloud P, we proceed to ground its geometric structure in a formalized representation.We do so by querying a pre-defined library of analytic concepts, which are parameter-driven models that capture common structural archetypes (e.g., primitive geometries, typical handle designs), each paired with a short natural-language synopsis.For example, in the Fig. 1(b), take the concept of ring, which frequently appears in the design of handles, by discovering the ring concept on a handle as an analytic description, we can identify its size (e.g., inner radius and outer radius) and pose, as well as the detailed parameters for the orientation of its hinge.The grounding procedure unfolds in two successive stages.First, we prune the concept library according to the part category detected in the previous step, and prompt the VLM with the synopses of the remaining candidates, asking: "Find the part to interact within <target object> the in order to complete the task <sub-task>, and determine the <concept> of the part."This query lets the VLM map its high-level semantic perception onto a node in our geometric knowledge graph, thereby fixing the symbolic layout of the structural blueprint.</p>
<p>Next, we must turn that symbolic layout into an executable program by instantiating every node with concrete parameters, estimated directly from the point cloud P.These parameters are of two types:</p>
<p>• Structural parameters encode the concept's intrinsic geometry of the analytic concept (e.g., the size l, w, h of a sunken door).To estimate them, we encode the point cloud P into a deep feature vector using an encoder.This feature vector is then fed into multiple specialized MLP heads, each regressing a specific structural parameter.• 6-DoF pose parameters locate the concept's global position and orientation.These are recovered analytically by combining the object's known simulation pose with the newly estimated structural variables.</p>
<p>MANIPULATION BLUEPRINTING</p>
<p>The structural blueprint tells us what the target part is; the manipulation blueprint specifies how to interact with it.Affordances of geometric ontologies are encoded as analytic manipulation knowledge for grasp poses, pushing contacts, and similar actions, while kinematic ontologies additionally provide force directions that produce motion.All of this knowledge is expressed by mathematical formulas with tunable parameters and offers critical guidance for downstream control.</p>
<p>We begin by presenting the VLM with the natural-language synopses of every candidate manipulation function-e.g., "pull-type grasp on curve handle," "push at door edge."The VLM chooses the module that best fulfils the high-level goal ("open the microwave door") and returns its analytic form.In this way, the model's semantic understanding is mapped directly onto executable actions.</p>
<p>Each selected function defines a category of grasp poses belonging to the same pattern.An exact grasp pose G is physically grounded by estimating the parameters of such analytic knowledge.Different from the structural parameters which are unique for a specific part, grasp-pose parameters have multiple valid solutions.For optimal door operation, grippers typically interact with the handle within its designed graspable range.However, under certain circumstances, the door edge itself also presents functional affordances that enable operation.With the parameters, a physically grounded grasp pose G can be calculated according to the analytic manipulation knowledge and initial grasp pose G * .For example, the equation
G = R(0, 0, ϑ)T(0, −R o , 0)R( π 2 , 0, π 2 )G * , − θ c 2 ≤ ϑ ≤ θ c
2 indicates a function that transforms the initial gripper pose to a grasp pose for the curve handle shown in Fig. 1(b).Once G is fixed, the force-direction formula-conditioned by the verb or manipulation type chosen by the VLM (e.g., pull vs. push)-is invoked to produce the vector F, ensuring that the applied force is semantically aligned with the selected action and correctly oriented on the target part.Both G and F are exported as lightweight Python functions and fed to the physically-grounded evaluator, closing the loop from language to low-level control.</p>
<p>LOW-LEVEL MOTION EXECUTION</p>
<p>Blueprint Execution.The instantiated structural and manipulation blueprints jointly output two quantities in the local frame of the target part: a grasp pose G local = (t local , r local ), and a force direction F local .Running the blueprint therefore reduces to transforming these local descriptors into the world frame and then feeding them to a standard motion-planning stack.</p>
<p>Transformation to World Coordinates.Let M ∈ R 4×4 denote the homogeneous transform of the target part with respect to the world frame, obtained from perception or simulation.For every point-set or inequality description F in the blueprint we apply F (x, y, z, 1) ⊤ ≤ 0 =⇒ F M −1 (x, y, z, 1) ⊤ ≤ 0, thereby re-expressing all structural constraints globally.The grasp pose is mapped by G world = MG local .For rotationally symmetric geometries we additionally enforce a minimal-rotation constraint on r local to obtain a unique orientation.The force vector is transformed analogously: F world = R F local , where R is the rotational part of M.</p>
<p>Motion Planning and Execution.The world-frame grasp pose G world and force vector F world are forwarded to a low-level planner.The planner first synthesises a collision-free approach path, then a compliant trajectory to realise the grasp, and finally an interaction phase that applies a wrench aligned with F world .The resulting joint-space command sequence is streamed to the robot controller, closing the pipeline from high-level language to physical motion.</p>
<p>EXPERIMENTS</p>
<p>To comprehensively evaluate the effectiveness and generalization capability of our proposed GRACE framework, we conduct extensive experiments in both simulated and real-world environments.This section is organized as follows: We begin with a zero-shot manipulation evaluation in simulation in Section 5.1.In order to verify the structural understanding of articulated objects by the process of policy scaffolding, additional interactive experiments are carried out in Section 5.2.We also carry out the object manipulation experiments with physical robots in real-world environments to provide a more comprehensive and stronger evaluation in Section 5.3.We provide implementation details of GRACE in Appendix A.</p>
<p>MANIPULATION EVALUATION IN SIMULATION</p>
<p>We select SimplerEnv (Li et al., 2024c) as our simulation platform due to its open-source nature and its focus on real-world robotic manipulation.It offers a standardized benchmark suite that emphasizes reproducible results and maintains close alignment with physical hardware constraints and realistic task conditions.We conduct quantitative evaluations of GRACE's zero-shot execution performance on Google Robot tasks &amp; Widow-X tasks and compare it to baselines including Table 1: SimplerEnv simulation evaluation results for the WindowX Robot task.We report both the final success rate ("Success") along with partial success (e.g., "Grasp Spoon")."FT" denotes performance of the fine-tuned models.Octo (Ghosh et al., 2024), OpenVLA (Kim et al., 2024) and more concurrent works (Qi et al., 2025;Qu et al., 2025;Li et al., 2024b).</p>
<p>On the four Widow-X tasks (Table 1), GRACE powered by GPT-4o achieves an average success rate of 86.1%, clearly outperforming the strongest published baseline, SoFar (58.3%).Although it is not the best on every single task, GRACE never performs poorly, maintaining consistently high scores across the entire suite.The pattern repeats on the Google-robot tasks (Table 2): GRACE(GPT-4o) attains 90.1% mean success, exceeding the best prior result by almost 30 pp.Notably, on the articulated Open/Close Drawer task the jump is the largest, rising from 29.7% (SoFar) and 36.2%(Spa-tialVLA) to 90.3% with GRACE for "Variant Aggregation", highlighting the advantage of EACs when precise kinematic reasoning is required.</p>
<p>To isolate the contribution of analytic concepts, we retrofit SpatialVLA by replacing its native, endto-end action output with EAC-guided motion planning when the gripper approaches the target; this variant is denoted SpatialVLA-EAC.The simple swap boosts SpatialVLA's average success to 69.8% on Widow-X and to 83.4% on the Google robot, demonstrating that EACs can be used as a plug-andplay module to substantially enhance existing VLA architectures.Finally, GRACE's performance is insensitive to the underlying VLM.The fully open-source Qwen2.5-VLbackend trails GPT-4o by only 1-2 pp on both robot families, yet still outperforms every external baseline, confirming that the bulk of the gain comes from the analytic-concept layer rather than the choice of language model.To focus on articulated objects manipulation, we evaluate the GRACE through the success rate of interaction on the proposed task, i.e., changing an articulated object from its initial state to a target final state.The success rate can reveal the quality of articulated concept discovery, including ontology discovery and affordance grounding.All experiments are carried out in SAPIEN under the standard Where2Act (Mo et al., 2021) settings (Appendix B for detail).We compare our method against three baselines, i.e., Where2Act, Where2Explore (Ning et al.) and ManipLLM (Li et al., 2024a), each representative of a distinct modelling paradigm for articulated-object manipulation.To isolate the contribution Table 2: SimplerEnv simulation evaluation results for the Google Robot setup.We present success rates for the "Variant Aggregation" and "Visual Matching" approaches."FT" denotes performance of the fine-tuned models. of VLM reasoning, we also report an ablated variant, GRACE-w/o-VLM, in which the conceptselection step is replaced by ground-truth ontology labels.</p>
<p>MANIPULATION EXPERIMENT OF ARTICULATED OBJECTS</p>
<p>Table 3 demonstrates that GRACE(GPT-4o) achieves the highest scores across all categories.For instance, it attains 0.65 for "faucet" objects and 0.91 on "cabinet" doors, significantly outperforming ManipLLM, which scores 0.26 and 0.71, respectively.These results decisively surpass both pixel-level affordance methods and the LLM-based ManipLLM.The substantial numerical margins underscore the advantage of integrating VLM-based reasoning with analytically grounded control.</p>
<p>Replacing the oracle concept label with GPT-4o's automatic selection reduces performance only slightly-from an average of 0.80 to 0.77, a drop of roughly three percentage points.The small gap indicates that the few remaining failures are due primarily to occasional VLM misclassification rather than limitations of the analytic concepts themselves; once the correct concept is chosen, execution is highly reliable.</p>
<p>OBJECT MANIPULATION EVALUATION IN REAL-WORLD</p>
<p>Stapler (90%) Mug (80%) Bucket (70%) Microwave (80%) We conducted experiments in a real-world tabletop environment using a Realman RM75 robotic arm equipped with a parallel gripper.Detailed visualizations of the environment and additional robot setup specifications are provided in Appendix B. For qualitative analysis, we first visualize the outputs and success rate of our approach for four different objects in Fig. 3, demonstrating the promising zero-shot manipulation capability of EAC for physics-grounded planning.Experimental results indicate that the VLM only needs to identify the target part of an object and construct its EAC representation to enable the robot to successfully complete the task.To further thoroughly assess the generalization ability of GRACE, we designed a longhorizon manipulation task involving six diverse objects.Preliminary observations suggest that GRACE maintains robust task reasoning capabilities even as task complexity increases.The overall performance in this long-horizon task is presented in the supplementary video.</p>
<p>A IMPLEMENTATION DETAILS OF METHOD</p>
<p>Segmentation.We use Grounded-SAM (Ren et al., 2024) consisting of two major components, Grounding-Dino (Liu et al., 2024) and SAM (Kirillov et al., 2023).We keep SAM frozen and finetune Grounding-Dino with RGB images with ground-truth bounding boxes of the actionable objects or parts, along with natural language prompt that describes the actionable objects or parts provided by VLM.</p>
<p>Parameter Estimation.The encoder is a Point-Transformer that extracts 128 groups of points with size 32 from the input with 2048 points and has 12 6-headed attention layers.The subsequent MLP has three layers with ReLU activation and outputs the structural parameters.The network is trained with L2 loss between the estimated and ground-truth structural parameters.Throughout the operation of the GRACE framework, the model parameters remain fixed.To construct the training dataset for our models, we first create analytic concept annotations for real-world objects.Specifically, we label the concept parameters of the training objects from PartNet-Mobility.Each object is then imported into the SAPIEN simulator, where a virtual camera captures RGB images and depth maps.Using the object's URDF file together with our analytic annotations, we can automatically generate ground-truth data-including bounding boxes, point clouds and structural parameters for every actionable part.Additionally, we leverage the FoundationPose (Wen et al., 2024)  The overall performance in this long-horizon task is presented in https://drive.google.com/file/d/14N5zDDwu4YOT1OsnZXe_Wxymj9pRrXU5/view?usp=sharing.</p>
<p>C SYSTEM ERROR BREAKDOWN</p>
<p>The primary sources of failure in our system are pose estimation and inverse kinematics (IK).Our analysis indicates that employing multi-view images for 3D object reconstruction significantly en-hances the success rate of pose estimation.It is also recommended to use high-resolution cameras to further improve estimation accuracy.Although structural parameter estimation introduces some error, its impact on the overall success rate is relatively minor.In contrast, the VFM-based object grounding module, alongside the VLM-based task parsing and concept construction, demonstrates high stability and contributes negligibly to system failures.involved and their properties.The final output is a structured object graph G = (V, E), where V denotes the list of object nodes, each represented as a structured dictionary containing id, name, and state, and E constitutes a list of directed spatial relationships between objects, each expressed as a triple e = (vi, r, vj). 2. ** Plan: ** Based on your reasoning, generate a sequence of action commands.The sequence must be logical, safe, and efficient.Each action instruction must include a validation condition that can be understood, such as verifying the target object is successfully grasped.3. ** Final Output: ** Provide ** only ** a valid JSON array as the final output.Do not add any other text.The JSON must follow this schema: json {{ "task": "original_task_description", "objects_graph_V": "structured object list", "objects_graph_E": "structured object spatial relationships list", "action_instruction_sequence": [ {{"id": 1, "action": "action_name", "parameter": " target_object_or_value", "success":"validation_condition"}}, {{"id": 2, "action": "action_name", "parameter": " target_object_or_value", "success":</p>
<p>Figure 1 :
1
Figure 1: Example implementation of executable analytic concepts.(a) Geometric Concept Assets.Each asset exposes its free parameters (top), canonical structure (mid), and partial affordance cues (bottom).(b) Structural Blueprint: higher-level objects are procedurally composed by wiring multiple geometric assets together, forming a parametric graph that captures their spatial layout and structural relationships.(c) Manipulation Blueprint: parameterised routines compute grasp poses and force directions that exploit the affordances encoded in the underlying structure.</p>
<p>Figure 2 :
2
Figure 2: An overview of the proposed method GRACE.(I) Task Parsing: A Vision-Language Model (VLM) parses the natural-language instruction based on the current RGB image.(II) Policy Scaffolding:The process includes: 1. segmenting the target object from images and back-projecting it to a partial point cloud; 2. parsing the analytic concept and estimating geometric parameters to instantiate the structural blueprint; 3. constructing the manipulation blueprint to produce feasible grasp poses and force directions; 4. generating a joint-space trajectory via a motion-planning module using the blueprints.(III) Robot Execution: The trajectory is executed to complete the task.</p>
<p>65 0.71 0.77 0.43 0.65 0.26 w/o-VLM 0.85 0.91 0.90 0.70 0.78 0.65 (GPT-4o) 0.84 0.85 0.88 0.70 0.72 0.60</p>
<p>Figure 3 :
3
Figure 3: Visualize the results of grasping objects and their corresponding EAC.The red parts in the second column indicate the target part.</p>
<p>Figure 4 :
4
Figure 4: Hardware Configuration.</p>
<dl>
<dt>D</dt>
<dd>
<p>** You are an expert robotic task planner.Your job is to analyze a visual scene image and break down a high-level manipulation command into a sequence of low-level, executable actions for a robot arm equipped with a gripper.<strong> Task: ** {task} ** Example: ** Task: "Pour the water from the blue cup into the red mug." ** Scene Image Context: ** the given image ** Robot Capabilities: ** -The robot has a single arm with a parallel-jaw gripper.-It can perform primitives: grasp(object_name), lift(height), pour( into_object_name), place_on(object_name), release(), push(object_name ), pull(object_name).-It cannot perform actions requiring complex dexterity (e.g., tying knots, unscrewing tight lids).-It must avoid collisions with all objects not involved in the task.</strong> Output Instructions: ** 1. ** Reasoning: ** First, reason step-by-step.Identify the key objects</p>
</dd>
</dl>
<p>, analyze the provided scene image and complete the task.<strong> """ Task_Parsing_PROMPT_TEMPLATE_2 = """ ** Role: ** You are a robotic task completion verifier.Your job is to analyze whether a manipulation task has been successfully completed by comparing the current scene state with the expected goal state.</strong> Original Task: ** "{Origin_Task_Description}" ** Expected Goal State Description: ** {Validation_Condition} ** Scene Image Context: ** the given image ** Final Output: ** Provide ** only ** a valid JSON array as the final output.Do not add any other text.The JSON must follow this schema:</p>
<p># ≤  $ +  $ ≤ 
Ring𝜃𝑅 # ℎ𝑅 ! ; −ℎ 2≤ 𝑧 ≤ℎ 2;−𝑦 𝑥 $ + 𝑦 $≤ cos𝜃 2;𝑅 !𝐆 = 𝐑 0, 0, 𝜗 𝐓 0, −𝑅 ! , 0 𝐑𝜋 2, 0,𝜋 2𝐆  *  , −𝜃 2≤ 𝜗 ≤𝜃 2§(a) Geometric Concept Assets ¤# Structural Blueprint Codeclass Curve_Handle:def <strong>init</strong>(self, curveL, radian, innerRadius, outerRadius, rotation, position, ...):𝜃 !""#self.shape = Ring(curveL, radian, innerRadius, outerRadius) ...ℎ𝑙𝑅 $class Sunken_Door(): def <strong>init</strong>(self, size, sunken_size, handle_size,ℎ %𝜃 %handle_pos, rotation, position, ...): self.bottom_shape = Cuboid(size)𝑤𝑅 "self.top_shape = Rectangular_Ring(sunken_size)¦self.handle = Curve_Handle(handle_size, handle_pos)¥(b) Structural Blueprint§¤# Manipulation Blueprint Codedef get_grasp_pose(obj, initial_pose):if (isinstance(obj, Curve_Handle)):
! : outer radius;  # : inner radius; : radian; ℎ: height; local_pose = translate_world2local(pose, init_pose) grasp_pose = obj.apply_pose(local_pose)if (isinstance(obj, Sunken_Door)):</p>
<p>Table 3 :
3
Comparison of performance on different objects (icons represent object categories).</p>
<p>CONCLUSIONWe have introduced GRACE, a plug-and-play framework that grounds visual observations with a VLM, reasons over Executable Analytic Concepts, and converts the result into precise robot actions.Extensive experiments on simualtion and real world demonstrate marked gains in zero-shot success rates, particularly on kinematically challenging tasks.In future work we plan to extend analytic concepts to multi-fingered hands and to explore on-the-fly concept refinement from real-world interaction data.
Jad Abou-Chakra, Krishan Rana, arXiv:2406.10788Feras Dayoub, and Niko Sünderhauf. Physically embodied gaussian splatting: A realtime correctable world model for robotics. 2024arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Large language models for mathematical reasoning: Progresses and challenges. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, 2024. 2021</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Doughnet: A visual predictive model for topological manipulation of deformable objects. Dominik Bauer, Zhenjia Xu, Shuran Song, European Conference on Computer Vision. Springer2024</p>
<p>Nod-tamp: Multi-step manipulation planning with neural object descriptors. Shuo Cheng, Caelan Reed Garrett, Ajay Mandlekar, Danfei Xu, CoRL 2023 Workshop on Learning Effective Abstractions for Planning (LEAP). 2023</p>
<p>An incremental constraint-based framework for task and motion planning. Zachary K Neil T Dantam, Swarat Kingston, Lydia E Chaudhuri, Kavraki, The International Journal of Robotics Research. 37102018</p>
<p>Graspvla: a grasping foundation model pre-trained on billion-scale synthetic action data. Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui, arXiv:2505.032332025arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Ayzaan Chowdhery, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Huang, 2023</p>
<p>Aha: A vision-language-model for detecting and reasoning over failures in robotic manipulation. Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, Yijie Guo, arXiv:2410.003712024aarXiv preprint</p>
<p>Manipulate-anything: Automating real-world robots using vision-language models. Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, Ranjay Krishna, of Proceedings of Machine Learning Research. Pulkit Agrawal, Oliver Kroemer, Wolfram Burgard, Munich, GermanyPMLR6-9 November 2024. 2024b270Conference on Robot Learning</p>
<p>Octo: An open-source generalist robot policy. Dibya Ghosh, Homer Rich Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, Robotics: Science and Systems. 2024</p>
<p>What's left? concept grounding with logic-enhanced foundation models. Joy Hsu, Jiayuan Mao, Josh Tenenbaum, Jiajun Wu, Advances in Neural Information Processing Systems. 202336</p>
<p>Roboground: Robotic manipulation with grounded vision-language priors. Haifeng Huang, Xinyi Chen, Yilun Chen, Hao Li, Xiaoshen Han, Zehan Wang, Tai Wang, Jiangmiao Pang, Zhou Zhao, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Copa: General robotic manipulation through spatial constraints of parts with foundation models. Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, Yang Gao, 10.1109/IROS58592.2024.10801352IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2024. Abu Dhabi, United Arab EmiratesIEEEOctober 14-18, 2024. 2024a</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, Li Fei-Fei, arXiv:2409.01652Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation. 2024barXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Paul Rafailov, Pannag R Foster, Quan Sanketi, Thomas Vuong, Benjamin Kollar, Russ Burchfiel, Dorsa Tedrake, Sergey Sadigh, Percy Levine, Chelsea Liang, Finn, Conference on Robot Learning. Pulkit Agrawal, Oliver Kroemer, Wolfram Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Manipllm: Embodied multimodal large language model for objectcentric robotic manipulation. Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong, 10.1109/CVPR52733.2024.017102024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2024a</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu, 10.48550/arXiv.2412.140582024b</p>
<p>Evaluating real-world robot manipulation policies in simulation. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Oier Mees, Karl Pertsch, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, Ted Xiao, of Proceedings of Machine Learning Research. Pulkit Agrawal, Oliver Kroemer, Wolfram Burgard, Munich, GermanyPMLR6-9 November 2024. 2024c270Conference on Robot Learning</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, arXiv:2405.14093Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, and Irwin King. A survey on visionlanguage-action models for embodied ai. Springer2024. 2024arXiv preprintEuropean conference on computer vision</p>
<p>Where are we in the search for an artificial visual cortex for embodied intelligence?. Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Tingfan Wu, Jay Vakil, Advances in Neural Information Processing Systems. 202336</p>
<p>kpam: Keypoint affordances for category-level robotic manipulation. Lucas Manuelli, Wei Gao, Peter Florence, Russ Tedrake, The International Symposium of Robotics Research. Springer2019</p>
<p>Object-centric task and motion planning in dynamic environments. Toki Migimatsu, Jeannette Bohg, IEEE Robotics and Automation Letters. 522020</p>
<p>Where2act: From pixels to actions for articulated 3d objects. Kaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhinav Gupta, Shubham Tulsiani, 10.1109/ICCV48922.2021.006742021 IEEE/CVF International Conference on Computer Vision, ICCV 2021. Montreal, QC, CanadaIEEEOctober 10-17, 20212021</p>
<p>A comprehensive overview of large language models. Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian, ACM Transactions on Intelligent Systems and Technology. 1652025</p>
<p>Where2explore: Few-shot affordance learning for unseen novel categories of articulated objects. Chuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, Hao Dong, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. Abby O' Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. Mingjie Pan, Jiyao Zhang, Tianshu Wu, Yinghao Zhao, Wenlong Gao, Hao Dong, 10.1109/CVPR52734.2025.01618IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025. Nashville, TN, USAComputer Vision Foundation / IEEEJune 11-15, 20252025</p>
<p>Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, Jiazhao Zhang, Jiawei He, Jiayuan Gu, Xin Jin, Kaisheng Ma, Zhizheng Zhang, He Wang, Li Yi, 10.48550/arXiv.2502.131432025</p>
<p>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, arXiv:2501.15830Exploring spatial representations for visuallanguage-action model. 2025arXiv preprint</p>
<p>Grounded SAM: assembling open-world models for diverse visual tasks. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, Lei Zhang, 10.48550/arXiv.2401.141592024</p>
<p>Large vlm-based vision-language-action models for robotic manipulation: A survey. Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie, arXiv:2508.130732025arXiv preprint</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-language-action models. Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, arXiv:2502.194172022 International Conference on Robotics and Automation (ICRA). Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, Vincent Sitzmann, IEEE2025. 2022arXiv preprintNeural descriptor fields: Se (3)-equivariant object representations for manipulation</p>
<p>Conceptfactory: Facilitate 3d object knowledge annotation with object conceptualization. Jianhua Sun, Yuxuan Li, Longfei Xu, Nange Wang, Jiude Wei, Yining Zhang, Cewu Lu, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems. Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, M Jakub, Cheng Tomczak, Zhang, NeurIPS; Vancouver, BC, Canada2024. 2024. December 10 -15, 2024. 2024</p>
<p>Physically ground commonsense knowledge for articulated object manipulation with analytic concepts. Jianhua Sun, Jiude Wei, Yuxuan Li, Cewu Lu, arXiv:2503.233482025arXiv preprint</p>
<p>Foundationpose: Unified 6d pose estimation and tracking of novel objects. Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield, 10.1109/CVPR52733.2024.01692IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024. Seattle, WA, USAIEEEJune 16-22, 20242024</p>
<p>Zhiyuan Xu, Kun Wu, Junjie Wen, Jinming Li, Ning Liu, Zhengping Che, Jian Tang, arXiv:2402.02385A survey on robotics with foundation models: toward embodied ai. 2024arXiv preprint</p>
<p>Sornet: Spatial object-centric representations for sequential manipulation. Wentao Yuan, Chris Paxton, Karthik Desingh, Dieter Fox, Conference on Robot Learning. PMLR2022</p>
<p>Vision-language models for vision tasks: A survey. Jingyi Zhang, Jiaxing Huang, Sheng Jin, Shijian Lu, IEEE transactions on pattern analysis and machine intelligence. 202446</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Conference on Robot Learning. PMLR2023</p>            </div>
        </div>

    </div>
</body>
</html>
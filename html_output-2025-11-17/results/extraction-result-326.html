<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-326 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-326</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-326</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-236c7dafea3df7ecffb5f18ec780d12f2f27d4b0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/236c7dafea3df7ecffb5f18ec780d12f2f27d4b0" target="_blank">C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> C-Eval is presented, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context, and is anticipated to help analyze important strengths and shortcomings of foundation Models, and foster their development and growth for Chinese users.</p>
                <p><strong>Paper Abstract:</strong> New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e326.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e326.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art large language model from OpenAI evaluated on the C-EVAL benchmark; achieves the best results on advanced mathematical and reasoning-heavy subjects among tested models, with modest gains from chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Multi-step mathematical problem solving across advanced mathematics, high-school mathematics, probability & statistics, discrete mathematics and physics problems requiring mathematical reasoning (i.e., textbook/exam-style multi-step math problems).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Varied; problems include complex LaTeX-formulated, multi-step exam problems (advanced undergraduate and high-school level). Specific digit ranges or simple arithmetic digit tasks are not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Evaluated in zero-shot answer-only (AO), five-shot answer-only, and five-shot chain-of-thought (COT) settings using five dev exemplars (explanations generated by GPT-4 and human-revised).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On the C-EVAL HARD subset: 53.3% (zero-shot AO), 54.9% (five-shot AO), 56.8% (five-shot COT). Per-subject (zero-shot AO): Advanced Mathematics 48.6%, High School Mathematics 41.0%, Middle School Mathematics 65.5%, Probability & Statistics 50.0%. COT (five-shot) improved Probability & Statistics from 53.6% (AO) to 62.0% (COT) in reported breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No mechanistic internals provided; paper only gives empirical observations and high-level hypotheses (e.g., COT elicits stepwise reasoning and can improve performance on reasoning-heavy math for strong models). No detailed mechanism (attention patterns, specialized heads, carry-tracking, algorithmic emulation) is described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Empirically much stronger than other tested models (GPT-4 is the only model exceeding ~60% average on the full C-EVAL); modest additional gains from few-shot and from COT for reasoning-heavy math tasks, but no parametric scaling analysis is provided (size undisclosed).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Despite being best, still far from perfect on hard math: substantial errors on advanced multi-step problems (C-EVAL HARD ~53–57%); many complex exam problems remain unsolved. COT gives only modest improvement; performance remains limited on some advanced topics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared AO zero-shot vs five-shot and AO vs COT; compared against ChatGPT, Claude variants, and several Chinese-oriented models (e.g., GLM-130B); COT vs AO and few-shot vs zero-shot are the primary contrasts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-4 is the only tested model to make consistent progress on challenging Chinese exam-style math: it attains ~53–57% on C-EVAL HARD and benefits from chain-of-thought on some math subjects, but still fails a large fraction of advanced arithmetic/mathematical problems and no internal arithmetic mechanism is described.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e326.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e326.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's instruction-following GPT variant evaluated on C-EVAL; performs substantially worse than GPT-4 on math-related subjects and often shows limited or negative benefit from chain-of-thought prompting on hard math.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt: Optimizing language models for dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Multi-step exam-style mathematical problem solving (advanced mathematics, high-school mathematics, probability & statistics, and related STEM problems).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Complex, exam-level problems (LaTeX expressions, multi-step reasoning); no explicit simple-digit arithmetic ranges reported.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot answer-only, five-shot answer-only, and five-shot chain-of-thought evaluated (dev exemplars used in few-shot/COT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On C-EVAL HARD: 36.7% (zero-shot AO), 41.4% (five-shot AO), 35.0% (five-shot COT). Per-subject zero-shot AO examples: Advanced Mathematics 38.2%, High School Mathematics 28.3%, Probability & Statistics 36.7%, Middle School Mathematics 40.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No internal mechanistic explanation given; only empirical observation that COT sometimes reduces performance for ChatGPT on these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Lower than GPT-4 across math subjects; benefits from few-shot prompting in some cases but COT does not reliably help and can hurt.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performs near-random on the hardest subsets (C-EVAL HARD) for many topics; chain-of-thought can degrade accuracy on tasks that are not reasoning-intensive; makes many errors on advanced multi-step math.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT-4, Claude variants, and Chinese-oriented models; AO vs COT and zero- vs five-shot comparisons reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>ChatGPT underperforms GPT-4 on arithmetic/math exam problems and does not reliably benefit from chain-of-thought prompting on the hard math subset; many advanced math questions remain unsolved.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e326.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e326.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bilingual (English/Chinese) large pretrained model evaluated among Chinese-oriented models; shows reasonable performance on humanities/social-science but lags in STEM/math compared to English-oriented models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLM-130b: An open bilingual pre-trained model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Exam-style mathematical problem solving (advanced mathematics, discrete mathematics, probability & statistics, high-school/college math topics).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Complex exam problems involving LaTeX notation and multi-step reasoning (no simple-digit ranges specified).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Evaluated under zero-shot AO, five-shot AO, and five-shot COT (same dev exemplars); model underwent instruction pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On C-EVAL HARD: 30.7% (zero-shot AO), 30.3% (five-shot AO), 22.6% (five-shot COT). Per-subject (zero-shot AO): Advanced Mathematics 28.9%, High School Mathematics 34.9%, Probability & Statistics 25.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No mechanistic detail; authors hypothesize instruction tuning choices may reduce few-shot in-context learning ability, which can harm performance when few-shot exemplars are used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performs worse than ChatGPT on STEM/math; shows that Chinese-oriented pretraining helps humanities/social-science but not necessarily complex math/reasoning, indicating capacity/ training differences rather than simple scale benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Few-shot and COT sometimes hurt performance (notably a drop in COT accuracy), inability to leverage few-shot exemplars effectively; near-random performance on C-EVAL HARD for many subjects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to ChatGPT and GPT-4 (English-oriented) and to other Chinese-oriented models; comparisons of zero-shot vs few-shot and AO vs COT are emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GLM-130B (130B bilingual model) lags behind English-oriented models on math/exam arithmetic tasks, and instruction-tuning decisions can reduce few-shot/COT benefits for arithmetic/reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e326.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e326.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>C-EVAL HARD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>C-EVAL HARD (subset of C-EVAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated subset of C-EVAL containing eight especially challenging STEM subjects (advanced mathematics, discrete mathematics, probability & statistics, college chemistry, college physics, high-school mathematics, high-school chemistry, high-school physics) designed to probe advanced mathematical/reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>C-EVAL HARD</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Advanced mathematical and multi-step reasoning problems drawn from exam questions (calculus/advanced mathematics, discrete math, probability & statistics and physics problems involving mathematical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>High complexity: exam-level problems with LaTeX equations, multi-step derivations, and advanced undergraduate/secondary school mathematics; no narrow digit-range specification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Used as evaluation target under zero-shot AO, five-shot AO, and five-shot COT protocols (dev exemplars include human-revised explanations produced initially by GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GPT-4: 53.3% (zero-shot AO), 54.9% (five-shot AO), 56.8% (five-shot COT). Most other models scored near-random on C-EVAL HARD (many in the ~20–40% range); only GPT-4, ChatGPT, and Claude make nontrivial improvements over random.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Only the strongest models (GPT-4) show measurable competence; most models (including many Chinese-oriented ones) fail to exceed near-random accuracy, indicating a sharp ability threshold for these complex arithmetic/reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Majority of models show near-random accuracy; several models cannot produce valid chain-of-thought outputs; COT helps only for top models and can degrade performance for many others.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked under AO vs COT and zero-shot vs five-shot; compared across a wide panel of models (GPT-4, ChatGPT, Claude variants, open-weight Chinese models).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>C-EVAL HARD exposes that complex, exam-style mathematical reasoning remains a major weakness for most LLMs: only the top models make meaningful gains and chain-of-thought helps only modestly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e326.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e326.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COT (Chain-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that elicits step-by-step reasoning text from LLMs by providing exemplars with detailed intermediate reasoning steps; used here in five-shot evaluations to test effect on math/reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Applied to multi-step mathematical reasoning tasks (advanced mathematics, probability & statistics, physics/math exam problems) to encourage intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot chain-of-thought: five dev exemplars with explanations (GPT-4 generated then manually revised) inserted into prompts; compared to answer-only prompting in zero- and five-shot regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Mixed effects: for GPT-4, modest improvement on C-EVAL HARD (54.9% AO to 56.8% COT) and a notable jump in Probability & Statistics (53.6% AO to 62.0% COT). For many other models COT either gives no benefit or reduces accuracy; some models fail to generate valid COT and thus fail to answer.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No internal mechanistic account; observed behaviour suggests COT elicits more explicit stepwise reasoning which can help for some models/tasks, but paper does not analyze internal representations (e.g., carries, attention patterns) underlying arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>COT tends to help the strongest models (e.g., GPT-4) on reasoning-heavy math, but often does not help or harms performance for models that were instruction-tuned without COT exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Many models cannot produce valid chains-of-thought; COT can degrade performance on tasks that are not reasoning-intensive; instruction-tuned models sometimes lose few-shot/COT benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Directly compared to answer-only prompting (AO) in zero- and five-shot settings; also compared effects across models (GPT-4, ChatGPT, GLM-130B, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Chain-of-thought prompting can substantially improve performance on some mathematical subjects for strong models (notably GPT-4), but it is not a universal fix: effectiveness depends on model training and the reasoning demands of the task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Challenging big-bench tasks and whether chain-of-thought can solve them <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-326",
    "paper_id": "paper-236c7dafea3df7ecffb5f18ec780d12f2f27d4b0",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "State-of-the-art large language model from OpenAI evaluated on the C-EVAL benchmark; achieves the best results on advanced mathematical and reasoning-heavy subjects among tested models, with modest gains from chain-of-thought prompting.",
            "citation_title": "GPT-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Multi-step mathematical problem solving across advanced mathematics, high-school mathematics, probability & statistics, discrete mathematics and physics problems requiring mathematical reasoning (i.e., textbook/exam-style multi-step math problems).",
            "number_range_or_complexity": "Varied; problems include complex LaTeX-formulated, multi-step exam problems (advanced undergraduate and high-school level). Specific digit ranges or simple arithmetic digit tasks are not specified.",
            "method_or_intervention": "Evaluated in zero-shot answer-only (AO), five-shot answer-only, and five-shot chain-of-thought (COT) settings using five dev exemplars (explanations generated by GPT-4 and human-revised).",
            "performance_result": "On the C-EVAL HARD subset: 53.3% (zero-shot AO), 54.9% (five-shot AO), 56.8% (five-shot COT). Per-subject (zero-shot AO): Advanced Mathematics 48.6%, High School Mathematics 41.0%, Middle School Mathematics 65.5%, Probability & Statistics 50.0%. COT (five-shot) improved Probability & Statistics from 53.6% (AO) to 62.0% (COT) in reported breakdowns.",
            "mechanistic_insight": "No mechanistic internals provided; paper only gives empirical observations and high-level hypotheses (e.g., COT elicits stepwise reasoning and can improve performance on reasoning-heavy math for strong models). No detailed mechanism (attention patterns, specialized heads, carry-tracking, algorithmic emulation) is described.",
            "performance_scaling": "Empirically much stronger than other tested models (GPT-4 is the only model exceeding ~60% average on the full C-EVAL); modest additional gains from few-shot and from COT for reasoning-heavy math tasks, but no parametric scaling analysis is provided (size undisclosed).",
            "failure_modes": "Despite being best, still far from perfect on hard math: substantial errors on advanced multi-step problems (C-EVAL HARD ~53–57%); many complex exam problems remain unsolved. COT gives only modest improvement; performance remains limited on some advanced topics.",
            "comparison_baseline": "Compared AO zero-shot vs five-shot and AO vs COT; compared against ChatGPT, Claude variants, and several Chinese-oriented models (e.g., GLM-130B); COT vs AO and few-shot vs zero-shot are the primary contrasts.",
            "key_finding": "GPT-4 is the only tested model to make consistent progress on challenging Chinese exam-style math: it attains ~53–57% on C-EVAL HARD and benefits from chain-of-thought on some math subjects, but still fails a large fraction of advanced arithmetic/mathematical problems and no internal arithmetic mechanism is described.",
            "uuid": "e326.0",
            "source_info": {
                "paper_title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI)",
            "brief_description": "OpenAI's instruction-following GPT variant evaluated on C-EVAL; performs substantially worse than GPT-4 on math-related subjects and often shows limited or negative benefit from chain-of-thought prompting on hard math.",
            "citation_title": "Chatgpt: Optimizing language models for dialogue",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Multi-step exam-style mathematical problem solving (advanced mathematics, high-school mathematics, probability & statistics, and related STEM problems).",
            "number_range_or_complexity": "Complex, exam-level problems (LaTeX expressions, multi-step reasoning); no explicit simple-digit arithmetic ranges reported.",
            "method_or_intervention": "Zero-shot answer-only, five-shot answer-only, and five-shot chain-of-thought evaluated (dev exemplars used in few-shot/COT).",
            "performance_result": "On C-EVAL HARD: 36.7% (zero-shot AO), 41.4% (five-shot AO), 35.0% (five-shot COT). Per-subject zero-shot AO examples: Advanced Mathematics 38.2%, High School Mathematics 28.3%, Probability & Statistics 36.7%, Middle School Mathematics 40.1%.",
            "mechanistic_insight": "No internal mechanistic explanation given; only empirical observation that COT sometimes reduces performance for ChatGPT on these datasets.",
            "performance_scaling": "Lower than GPT-4 across math subjects; benefits from few-shot prompting in some cases but COT does not reliably help and can hurt.",
            "failure_modes": "Performs near-random on the hardest subsets (C-EVAL HARD) for many topics; chain-of-thought can degrade accuracy on tasks that are not reasoning-intensive; makes many errors on advanced multi-step math.",
            "comparison_baseline": "Compared against GPT-4, Claude variants, and Chinese-oriented models; AO vs COT and zero- vs five-shot comparisons reported.",
            "key_finding": "ChatGPT underperforms GPT-4 on arithmetic/math exam problems and does not reliably benefit from chain-of-thought prompting on the hard math subset; many advanced math questions remain unsolved.",
            "uuid": "e326.1",
            "source_info": {
                "paper_title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GLM-130B",
            "name_full": "GLM-130B",
            "brief_description": "A bilingual (English/Chinese) large pretrained model evaluated among Chinese-oriented models; shows reasonable performance on humanities/social-science but lags in STEM/math compared to English-oriented models.",
            "citation_title": "GLM-130b: An open bilingual pre-trained model",
            "mention_or_use": "use",
            "model_name": "GLM-130B",
            "model_size": "130B",
            "model_architecture": null,
            "arithmetic_operation_type": "Exam-style mathematical problem solving (advanced mathematics, discrete mathematics, probability & statistics, high-school/college math topics).",
            "number_range_or_complexity": "Complex exam problems involving LaTeX notation and multi-step reasoning (no simple-digit ranges specified).",
            "method_or_intervention": "Evaluated under zero-shot AO, five-shot AO, and five-shot COT (same dev exemplars); model underwent instruction pretraining.",
            "performance_result": "On C-EVAL HARD: 30.7% (zero-shot AO), 30.3% (five-shot AO), 22.6% (five-shot COT). Per-subject (zero-shot AO): Advanced Mathematics 28.9%, High School Mathematics 34.9%, Probability & Statistics 25.9%.",
            "mechanistic_insight": "No mechanistic detail; authors hypothesize instruction tuning choices may reduce few-shot in-context learning ability, which can harm performance when few-shot exemplars are used.",
            "performance_scaling": "Performs worse than ChatGPT on STEM/math; shows that Chinese-oriented pretraining helps humanities/social-science but not necessarily complex math/reasoning, indicating capacity/ training differences rather than simple scale benefits.",
            "failure_modes": "Few-shot and COT sometimes hurt performance (notably a drop in COT accuracy), inability to leverage few-shot exemplars effectively; near-random performance on C-EVAL HARD for many subjects.",
            "comparison_baseline": "Compared to ChatGPT and GPT-4 (English-oriented) and to other Chinese-oriented models; comparisons of zero-shot vs few-shot and AO vs COT are emphasized.",
            "key_finding": "GLM-130B (130B bilingual model) lags behind English-oriented models on math/exam arithmetic tasks, and instruction-tuning decisions can reduce few-shot/COT benefits for arithmetic/reasoning.",
            "uuid": "e326.2",
            "source_info": {
                "paper_title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "C-EVAL HARD",
            "name_full": "C-EVAL HARD (subset of C-EVAL)",
            "brief_description": "A curated subset of C-EVAL containing eight especially challenging STEM subjects (advanced mathematics, discrete mathematics, probability & statistics, college chemistry, college physics, high-school mathematics, high-school chemistry, high-school physics) designed to probe advanced mathematical/reasoning ability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "C-EVAL HARD",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Advanced mathematical and multi-step reasoning problems drawn from exam questions (calculus/advanced mathematics, discrete math, probability & statistics and physics problems involving mathematical reasoning).",
            "number_range_or_complexity": "High complexity: exam-level problems with LaTeX equations, multi-step derivations, and advanced undergraduate/secondary school mathematics; no narrow digit-range specification.",
            "method_or_intervention": "Used as evaluation target under zero-shot AO, five-shot AO, and five-shot COT protocols (dev exemplars include human-revised explanations produced initially by GPT-4).",
            "performance_result": "GPT-4: 53.3% (zero-shot AO), 54.9% (five-shot AO), 56.8% (five-shot COT). Most other models scored near-random on C-EVAL HARD (many in the ~20–40% range); only GPT-4, ChatGPT, and Claude make nontrivial improvements over random.",
            "mechanistic_insight": null,
            "performance_scaling": "Only the strongest models (GPT-4) show measurable competence; most models (including many Chinese-oriented ones) fail to exceed near-random accuracy, indicating a sharp ability threshold for these complex arithmetic/reasoning tasks.",
            "failure_modes": "Majority of models show near-random accuracy; several models cannot produce valid chain-of-thought outputs; COT helps only for top models and can degrade performance for many others.",
            "comparison_baseline": "Benchmarked under AO vs COT and zero-shot vs five-shot; compared across a wide panel of models (GPT-4, ChatGPT, Claude variants, open-weight Chinese models).",
            "key_finding": "C-EVAL HARD exposes that complex, exam-style mathematical reasoning remains a major weakness for most LLMs: only the top models make meaningful gains and chain-of-thought helps only modestly.",
            "uuid": "e326.3",
            "source_info": {
                "paper_title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "COT (Chain-of-Thought)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting strategy that elicits step-by-step reasoning text from LLMs by providing exemplars with detailed intermediate reasoning steps; used here in five-shot evaluations to test effect on math/reasoning performance.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought prompting",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Applied to multi-step mathematical reasoning tasks (advanced mathematics, probability & statistics, physics/math exam problems) to encourage intermediate reasoning steps.",
            "number_range_or_complexity": null,
            "method_or_intervention": "Few-shot chain-of-thought: five dev exemplars with explanations (GPT-4 generated then manually revised) inserted into prompts; compared to answer-only prompting in zero- and five-shot regimes.",
            "performance_result": "Mixed effects: for GPT-4, modest improvement on C-EVAL HARD (54.9% AO to 56.8% COT) and a notable jump in Probability & Statistics (53.6% AO to 62.0% COT). For many other models COT either gives no benefit or reduces accuracy; some models fail to generate valid COT and thus fail to answer.",
            "mechanistic_insight": "No internal mechanistic account; observed behaviour suggests COT elicits more explicit stepwise reasoning which can help for some models/tasks, but paper does not analyze internal representations (e.g., carries, attention patterns) underlying arithmetic.",
            "performance_scaling": "COT tends to help the strongest models (e.g., GPT-4) on reasoning-heavy math, but often does not help or harms performance for models that were instruction-tuned without COT exemplars.",
            "failure_modes": "Many models cannot produce valid chains-of-thought; COT can degrade performance on tasks that are not reasoning-intensive; instruction-tuned models sometimes lose few-shot/COT benefit.",
            "comparison_baseline": "Directly compared to answer-only prompting (AO) in zero- and five-shot settings; also compared effects across models (GPT-4, ChatGPT, GLM-130B, etc.).",
            "key_finding": "Chain-of-thought prompting can substantially improve performance on some mathematical subjects for strong models (notably GPT-4), but it is not a universal fix: effectiveness depends on model training and the reasoning demands of the task.",
            "uuid": "e326.4",
            "source_info": {
                "paper_title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Challenging big-bench tasks and whether chain-of-thought can solve them",
            "rating": 2
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        }
    ],
    "cost": 0.018872999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>C-EVAL: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</h1>
<p>Yuzhen Huang*1<br>Tangjun $\mathbf{S u}^{1}$<br>Yao Fu ${ }^{3}$<br>Maosong Sun ${ }^{2}$<br>Zhihao Zhu ${ }^{1}$<br>Junlei Zhang ${ }^{1}$<br>Junlei Jian ${ }^{1}$<br>Jinghan Zhang ${ }^{1}$<br>Chuancheng $\mathrm{Lv}^{2}$<br>Yikai Zhang ${ }^{1}$<br>Jiayi Lei ${ }^{1}$<br>Maosong Sun ${ }^{2}$ Junxian $\mathrm{He}^{14}$<br>${ }^{1}$ Shanghai Jiao Tong University ${ }^{2}$ Tsinghua University ${ }^{3}$ University of Edinburgh<br>${ }^{4}$ The Hong Kong University of Science and Technology<br>ceval. benchmark@gmail.com<br>https://cevalbenchmark.com</p>
<h4>Abstract</h4>
<p>New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-EVAL, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-EVAL comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-EVAL is accompanied by C-EVAL HARD, a subset of very challenging subjects in C-EVAL that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-EVAL, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over $60 \%$, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-EVAL will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Evaluation benchmarks are at the core role for AI development. While traditional NLP benchmarks were mostly designed to measure specific and relatively simple abilities, large language models (LLMs), or foundation models, have demonstrated various new capabilities and shifted the evaluation focus to more general and intricate skills, such as broad world knowledge and complex reasoning. To align with the new era of LLMs, new benchmarks are proposed recently to probe a diverse set of LLM abilities. For example, MMLU (Hendrycks et al., 2021a), BIG-bench (Srivastava et al., 2022), and HELM (Liang et al., 2022) benchmarks attempt to aggregate a wide range of NLP tasks for holistic evaluation. Some other benchmarks specifically focus on advanced LLM abilities that emerge with scale, such as reasoning (Cobbe et al., 2021), hard math problem-solving (Hendrycks et al., 2021b), and coding (Chen et al., 2021). While traditional NLP benchmarks are becoming obsolete, these new ones are extensively used in recent research to drive development of the latest LLMs (Taylor et al., 2022; Chowdhery et al., 2022; Hoffmann et al., 2022; Touvron et al., 2023; OpenAI, 2023).</p>
<p>However, these modern benchmarks primarily target English language, resulting in limited understanding of LLMs' capabilities in other languages. In this work, we focus on evaluating the advanced abilities of foundation models in a Chinese context, one of the most widely spoken language in the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview diagram of C-EVAL. Different colors of the subjects indicate four difficulty levels: middle school, high school, college, and professional.
world. Although there has been a recent surge in powerful Chinese LLMs, such as GLM-130B (Zeng et al., 2023), Wenxin Yiyan (Baidu, 2023), and MOSS (OpenLMLab, 2023), the corresponding evaluation significantly lags behind, with the CLUE benchmark (Xu et al., 2020), the Chinese counterpart of GLUE (Wang et al., 2019), serving as the best available standard. We emphasize that simply translating English benchmarks as in OpenAI (2023), even with flawless translations, does not fulfill the goal - LLMs intended for use in a Chinese environment should be evaluated on their knowledge of Chinese users' primary interests, such as Chinese culture, history, and laws, as well as other competencies unique in Chinese society. In contrast, English benchmarks tend to exhibit geographical biases towards the domestic knowledge of the regions that produce them.</p>
<p>To narrow the gap between Chinese LLM development and evaluation, we present C-EVAL, the first comprehensive Chinese evaluation suite to thoroughly assess LLMs' advanced knowledge and reasoning abilities in a Chinese context. C-EVAL consists of 13948 multiple-choice exam questions spanning 52 diverse disciplines, ranging from humanities to science and engineering, as depicted in Figure 1. The questions are collected from four difficulty levels: middle school, high school, college, and professional tests. Along with C-EVAL, we introduce C-EVAL HARD as an accompanied benchmark, a subset of particularly challenging subjects in C-EVAL that demands highly advanced reasoning abilities to solve, such as advanced mathematics and college physics. Notably, C-EVAL HARD is among the few benchmarks for advanced reasoning where GPT-4 still struggles, achieving an accuracy of $53.3 \%$, making it the first Chinese benchmark at this level.</p>
<p>We conduct experiments to evaluate the most advanced LLMs on C-EVAL in both answer-only and chain-of-thought settings. Results show that GPT-4 is the only model that surpasses $60 \%$ average accuracy. However, its $66.4 \%$ accuracy indicates that there is still large room for improvement in current LLMs. Despite not specially tailored for Chinese data, GPT-4, ChatGPT, and Claude emerge as the top three performers on C-EVAL. Upon examining the results of LLMs focused on Chinese, we find that while some models managed to narrow the gap on Chinese knowledge test with ChatGPT, acquiring reasoning abilities seems more challenging. On C-EVAL HARD, in particular, most models could only retain near-random accuracy. In addition to its use as a whole, we envision C-EVAL as a suite of benchmarks, subsets of which could be separately utilized to assess certain model abilities of interest and analyze important strengths and limitations of foundation models. We hope C-EVAL could guide the developers to understand the abilities of their models from multiple dimensions and facilitate the growth of foundation models for Chinese users.</p>
<h1>2 The C-Eval Evaluation Suite</h1>
<h3>2.1 Design Principle</h3>
<p>Overview: The motivation of C-EVAL is to help developers quickly understand the abilities of their models from multiple dimensions, so that they could target the shortcomings of the models and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;"># Subjects</th>
<th style="text-align: center;"># Questions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">In terms of topic</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">4495</td>
</tr>
<tr>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2676</td>
</tr>
<tr>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2845</td>
</tr>
<tr>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">3932</td>
</tr>
<tr>
<td style="text-align: center;">In terms of difficulty level</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Middle School</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1409</td>
</tr>
<tr>
<td style="text-align: center;">High School</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1594</td>
</tr>
<tr>
<td style="text-align: center;">College</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">6249</td>
</tr>
<tr>
<td style="text-align: center;">Professional</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">4696</td>
</tr>
<tr>
<td style="text-align: center;">In terms of split</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">260</td>
</tr>
<tr>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">1346</td>
</tr>
<tr>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">12342</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">13948</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of C-EVAL.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">造伦兹曲线越是向横轴凸出</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The more the Lorenz curve is convex to the horizontal axis,</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">A. 基尼系数就越大，收入就越不平等 the larger the Gini coefficient, the more unequal the income.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">B. 基尼系数就越大，收入就越平等 the larger the Gini coefficient, the more equal the income.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">C. 基尼系数就越小，收入就越不平等 the smaller the Gini coefficient, the more unequal the income.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D. 基尼系数就越小，收入就越平等 the smaller the Gini coefficient, the more equal the income.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">答案：A</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer: A</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: Example from college economics. English translations are shown for better readability.
improve them accordingly. To this end, we focus on the relatively advanced abilities of LLMs such as world knowledge and reasoning, which are arguably the most critical skills for LLMs nowadays. While different LLMs may perform similarly in simple scenarios like casual conversations, complex tasks are often the key differentiators between LLMs (OpenAI, 2023). Therefore, we construct C-EVAL from real-world, challenging human exams in China that are used to assess humans' abilities from multiple dimensions. We only select questions of a multi-choice format, similar to Hendrycks et al. (2021a), because: (1) metrics are clearly defined (i.e. accuracy), and (2) multi-choice questions are a simple but good proxy to evaluate the potential of advanced abilities of foundation models, which we consider could be easily exploited and reflected in various downstream applications through specialized instruction tuning (Chung et al., 2022; Wang et al., 2022). Each question has four choices and only one choice is the correct answer. LLMs are intended to be used to solve these questions through prompting. The questions in C-EVAL span 52 diverse disciplines that we later cluster them into broader categories as STEM, humanities, social science, and other areas. Summarized statistics of C-EVAL is shown in Table 1, and more detailed statistics per subject are in Appendix B.</p>
<p>Attempt to mitigate data contamination: Exam questions from national tests, such as China's national college entrance exams (commonly known as Gaokao) and national professional exams, are often widely distributed and accessible online. Consequently, these questions may inadvertently be crawled and incorporated into LLM pretraining, leading to potential data leakage issues. To mitigate this risk, we collect our data either from mock exams or from small-scale local exams, such as those available online from specific high schools. This deviates from previous work that built benchmarks using the exact questions from official national exams (Zhong et al., 2023). Moreover, most samples in C-EVAL are sourced from PDF or Microsoft Word documents on the Internet, rather than directly from plain text or structured questions. These documents are subsequently parsed and carefully annotated by the authors to obtain the final structured format, a process that often involves complex $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ equation conversion for certain subjects. This further minimizes the risk of data contamination.</p>
<h1>2.2 Data Collection</h1>
<p>Subject selection: C-EVAL covers four difficulty levels: middle school, high school, college, and professional. We include the standard subjects for middle and high school levels in China, except for the English subject. ${ }^{2}$ For the college level, we select 25 representative subjects from all the 13 official categories of undergraduate majors listed by the Ministry of Education in China, ${ }^{3}$ at least one subject from each category is included in C-EVAL to assure comprehensiveness. For the professional level, we refer to the official national vocational qualification directory in China ${ }^{4}$ and choose 12</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: An development example with explanations from high school chemistry. English translations are shown below the corresponding Chinese text for better readability.
representative ones, such as physician, legal professional, and civil servant qualification exams. We also cluster these subjects into four categories in terms of their topic: STEM (Science, Technology, Engineering, and Mathematics), Social Science, Humanities, and Other areas. All the 52 subjects and their assigned categories are illustrated in Figure 1.
Data sources: The primary source of our data is mock exams freely available on the internet. In addition, a portion of the college-level questions are past exam questions from top universities in China, publicly shared by the students. A minor fraction of college questions are mock questions for the national graduate entrance exam, sourced from the Weipu website ${ }^{5}$ - these questions are not freely available to the public, and we have obtained their authorization to include around 2000 such questions into C-EVAL.
Data Processing: The collected data come in various formats, primarily as PDF or Microsoft Word documents and a minor fraction as web pages. PDF documents are initially processed into text. All questions are subsequently parsed - automatically when possible, and otherwise manually by the authors - into a structured format, as exemplified in Figure 2. For subjects with complex mathematical notations such as many subjects in the STEM category, we manually convert them into standard LaTeX formats, similar to Hendrycks et al. (2021b); Taylor et al. (2022). All the questions in C-EVAL are processed to include exactly four choices. Most of the original questions were accompanied by four choices already, and we eliminate questions with fewer than four options and randomly drop incorrect choices for questions with more than four options. All questions also go through the standard data preprocessing pipeline, such as deduplication and cleaning. Following this, the questions undergo several rounds of human validation by the authors, and all the LaTeX notations are ensured to be complied without syntax errors. We process at least around 200 questions for each subject, and randomly split the questions into a development set, a validation set, and a test set within each subject. The development split per subject consists of five exemplars to facilitate few-shot evaluation. These dev exemplars are also annotated with explanations to enable few-shot chain-of-thought settings (Wei et al., 2022), as we detail next. The validation and test set are created with a 1:9 ratio, where the validation set is intended to be used for hyperparameter tuning.
Explanation data generation: Chain-of-thought (COT) reasoning (Kojima et al., 2022; Wei et al., 2022) - that prompts LLMs to generate a text sequence of reasoning process along with the final answer - has shown great success on reasoning-heavy tasks. Compared to zero-shot COT, the few-shot version is more commonly used and achieves the state-of-the-art performance on various tasks (Gao et al., 2022; Wang et al., 2023; Zhou et al., 2023; Xie et al., 2023). To facilitate the potential usage of C-EVAL in a few-shot COT setting, we combine automatic generation and human annotation to produce high-quality explanation data for the development split. Specifically, we first prompt GPT-4 to generate step-by-step explanation to explain the ground-truth answer, then we</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Example from advanced mathematics, a subject in C-EVAL HARD. English translations are shown below the corresponding Chinese text for better readability.
manually revise the generated explanations to obtain the final explanations. Details on prompting GPT-4 are in Appendix C. A dev example with explanations is illustrated in Figure 3.</p>
<h1>2.3 C-EVAL HARD</h1>
<p>We select 8 challenging math, physics, and chemistry subjects from C-EVAL to form a separate benchmark, C-EVAL HARD, which includes advanced mathematics, discrete mathematics, probability and statistics, college chemistry, college physics, high school mathematics, high school chemistry, and high school physics. These subjects often involve with complex $\mathrm{IA}_{\mathrm{E}} \mathrm{X}$ equations and require non-trivial reasoning abilities to solve. An example from advanced mathematics is shown in Figure 4. C-EVAL HARD aligns with recent efforts to create difficult benchmarks to assess advanced reasoning abilities (Hendrycks et al., 2021b; Suzgun et al., 2022), which are the key differentiators among various LLMs and could reflect LLMs' potential in general and complex scenarios. We emphasize that C-EVAL HARD is the first Chinese benchmark to provide highly complicated reasoning questions.</p>
<h3>2.4 Evaluation</h3>
<p>We use accuracy as the metric. While ground-truth labels of the development and validation splits are released, we keep the labels of the test split private. This is to ensure the fair use of the C-EVAL, as the C-EVAL data may unconsciously be included in pretraining data due to web crawling. Instead, users are required to submit their model predictions to https://cevalbenchmark.com to automatically obtain the test accuracy, where a public leaderboard is maintained. Users have the option to include their submission results in the live leaderboard, depending on their own preference.</p>
<h2>3 Experiment</h2>
<h3>3.1 Setup</h3>
<p>We evaluate LLMs in both zero- and five-shot settings on C-EVAL, where the five exemplars are from the development split. We adopt regular expressions to extract answer choices from the model responses, ensuring that we can successfully extract answers for nearly all cases. We report answeronly (AO) results on both zero- and five-shot settings and chain-of-thought (COT) results on the five-shot setting only, as we found that it was often difficult to extract the answer choices from zero-shot COT predictions where the generation does not follow specific patterns. Prompts of AO and COT are shown in Appendix D. We note that in the COT setting, the five-shot exemplars could exceed the maximum context length of some LLMs for certain subjects. In such cases, we dynamically reduce the number of exemplars to fit within the context window.</p>
<h3>3.2 Models</h3>
<p>To give a comprehensive view of the status of LLM in a Chinese language context, we evaluate 11 accessible top-performing LLMs that are able to process Chinese input, covering diverse organizations and varying in size, as shown in Table 2. ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) are the strongest GPT model variants from OpenAI. Claude (Anthropic, 2022), developed by Anthropic, is often considered comparable to ChatGPT. We evaluate both the Claude-v1.3 and Claude-instantv1.0 variants, with Claude-instant being a lighter version. Bloomz-mt (Muennighoff et al., 2022) is based on the pretrained multilingual BLOOM model (Scao et al., 2022) with multitask prompted finetuning, thus is suitable for non-English languages. LLaMA (Touvron et al., 2023) is probably</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Creator</th>
<th style="text-align: center;">#Parameters</th>
<th style="text-align: center;">Access</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: center;">undisclosed</td>
<td style="text-align: center;">API</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: center;">undisclosed</td>
<td style="text-align: center;">API</td>
</tr>
<tr>
<td style="text-align: left;">Claude-v1.3</td>
<td style="text-align: center;">Anthropic</td>
<td style="text-align: center;">undisclosed</td>
<td style="text-align: center;">API</td>
</tr>
<tr>
<td style="text-align: left;">Claude-instant-v1.0</td>
<td style="text-align: center;">Anthropic</td>
<td style="text-align: center;">undisclosed</td>
<td style="text-align: center;">API</td>
</tr>
<tr>
<td style="text-align: left;">Bloomz-mt</td>
<td style="text-align: center;">BigScience</td>
<td style="text-align: center;">176B</td>
<td style="text-align: center;">Weights</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-65B</td>
<td style="text-align: center;">Meta</td>
<td style="text-align: center;">65B</td>
<td style="text-align: center;">Weights</td>
</tr>
<tr>
<td style="text-align: left;">GLM-130B</td>
<td style="text-align: center;">Tsinghua</td>
<td style="text-align: center;">130B</td>
<td style="text-align: center;">Weights</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM-6B</td>
<td style="text-align: center;">Tsinghua</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">Weights</td>
</tr>
<tr>
<td style="text-align: left;">Chinese-LLaMA-13B</td>
<td style="text-align: center;">Cui et al.</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">Weights</td>
</tr>
<tr>
<td style="text-align: left;">Chinese-Alpaca-13B</td>
<td style="text-align: center;">Cui et al.</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">Weights</td>
</tr>
<tr>
<td style="text-align: left;">MOSS</td>
<td style="text-align: center;">Fudan</td>
<td style="text-align: center;">16B</td>
<td style="text-align: center;">Weights</td>
</tr>
</tbody>
</table>
<p>Table 2: Models evaluated in this paper.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">STEM</th>
<th style="text-align: center;">Social Science</th>
<th style="text-align: center;">Humanities</th>
<th style="text-align: center;">Other</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">51.0</td>
</tr>
<tr>
<td style="text-align: left;">Claude-v1.3</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">50.5</td>
</tr>
<tr>
<td style="text-align: left;">Bloomz-mt</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: left;">GLM-130B</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: left;">Claude-instant-v1.0</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">40.6</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM-6B</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">38.9</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-65B</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">34.7</td>
</tr>
<tr>
<td style="text-align: left;">MOSS</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">33.1</td>
</tr>
<tr>
<td style="text-align: left;">Chinese-Alpaca-13B</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: left;">Chinese-LLaMA-13B</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">29.6</td>
</tr>
</tbody>
</table>
<p>Table 3: Zero-shot average accuracy (\%) in answer-only setting. We report the average accuracy over the subjects within each category. "Average" column indicates the average accuracy over all the subjects.
the best open-weight foundation model so far that achieves the highest accuracy on the English MMLU benchmark within open-weight models. The aforementioned models except Bloomz-mt are English-oriented during development, while they are able to process Chinese input because a minor fraction of Chinese text is present in the pretraining data.</p>
<p>We further include recent LLMs developed by Chinese institutions or individuals that is Chineseoriented. GLM-130B (Zeng et al., 2023) and ChatGLM-6B (THUDM, 2023a) are based on the General Language Model architecture (GLM, Du et al. (2022)) trained on English and Chinese data. ChatGLM-6B is further adapted on conversational data. Chinese-LLaMA (Cui et al., 2023) is an adaptation of LLaMA, which is further pretrained on Chinese data. Chinese-Alpaca (Cui et al., 2023) performs instruction tuning based on Chinese-LLaMA. MOSS (OpenLMLab, 2023) is the first publicly available Chinese LLM, and it follows a training procedure similar to ChatGPT. We note that there are some other commercial Chinese-oriented LLMs whose weights and APIs are not directly open to the public at the time of writing this paper, such as Wenxin Yiyan (Baidu, 2023), Tongyi Qianwen (Alibaba, 2023), and Xunfei Xinghuo (iFLYTEK, 2023), these models may have strong performance, yet we are not authorized to evaluate and publicize their results. Therefore, we only report results from models with open APIs or weights in this work, while we anticipate the developers of other models to submit and optionally publicize their models' results in our website. A detailed description of the evaluated models can be found in Appendix E.</p>
<h1>3.3 Results</h1>
<p>General comparison: Zero- and five-shot answer-only results are shown in Table 3 and Table 4 respectively. We report the average accuracy, while a detailed breakdown of accuracy per subject is provided in Appendix F. GPT-4 is the only model that exceeds $60 \%$ average accuracy, highlighting the challenge presented by C-EVAL. GPT-4 significantly outperforms all other models, with the second-best model, ChatGPT, trailing over 14 percentage points behind in both zero- and five-shot settings. Claude-v1.3 achieves similar performance to ChatGPT, in terms of both the category-wise</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">STEM</th>
<th style="text-align: center;">Social Science</th>
<th style="text-align: center;">Humanities</th>
<th style="text-align: center;">Other</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">54.4</td>
</tr>
<tr>
<td style="text-align: left;">Claude-v1.3</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">54.2</td>
</tr>
<tr>
<td style="text-align: left;">Claude-instant-v1.0</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">45.9</td>
</tr>
<tr>
<td style="text-align: left;">GLM-130B</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">40.3</td>
</tr>
<tr>
<td style="text-align: left;">Bloomz-mt</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">39.0</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-65B</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">38.8</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM-6B</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">34.5</td>
</tr>
<tr>
<td style="text-align: left;">Chinese-LLaMA-13B</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: left;">MOSS</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">31.1</td>
</tr>
<tr>
<td style="text-align: left;">Chinese-Alpaca-13B</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">26.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Five-shot average accuracy (\%) in answer-only setting. We report the average accuracy over the subjects within each category. "Average" column indicates the average accuracy over all the subjects.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">STEM</th>
<th style="text-align: center;">Social Science</th>
<th style="text-align: center;">Humanities</th>
<th style="text-align: center;">Other</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">68.3</td>
</tr>
<tr>
<td style="text-align: left;">Claude-v1.3</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">54.2</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">Claude-instant-v1.0</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM-6B</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">34.5</td>
</tr>
<tr>
<td style="text-align: left;">MOSS</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">31.2</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-65B</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">30.3</td>
</tr>
<tr>
<td style="text-align: left;">GLM-130B</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">28.8</td>
</tr>
<tr>
<td style="text-align: left;">Chinese-LLaMA-13B</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">25.4</td>
</tr>
</tbody>
</table>
<p>Table 5: Five-shot average accuracy (\%) in chain-of-thought setting. We report the average accuracy over the subjects within each category. "Average" column indicates the average accuracy over all the subjects. Bloomz-mt and Chinese-Alpaca-13B are excluded as they could not generate valid reasoning and thus fail to answer for many questions.
average and the overall average. In addition to average accuracy, Table 9 in Appendix F reveals that GPT-4 surpasses ChatGPT in almost every subject, indicating a comprehensive advantage. Among Chinese-oriented models, GLM-130B exhibits the best performance, ranking the fifth in terms of both zero- and few-shot performance, 7.0 and 14.1 points behind ChatGPT in zero- and five-shot settings respectively. We observe that smaller models, despite undergoing instruction tuning, still struggle to achieve a $40 \%$ accuracy. This contradicts recent assertions that a 10B-scale instruction-tuned model can achieve comparable performance to ChatGPT (Taori et al., 2023; Chiang et al., 2023) - we argue that while these models may perform well on simpler tasks, their inherent advanced abilities significantly lag behind when faced with more complex scenarios.
Does few-shot prompting help? Comparing Table 4 to Table 3, we find that while few-shot prompting helps many models achieve better results, it hurts performance of GLM-130B, Bloomz-mt, ChatGLM-6B, MOSS, and Chinese-Alpaca-13B. All of these models have undergone instruction tuning, ${ }^{6}$ and we hypothesize that the accuracy drop is because that these models have not (appropriately) incorporated few-shot demonstrations into the instruction tuning stage, as emphasized in Chung et al. (2022), thus sacrificing few-shot in-context learning performance to obtain enhanced zero-shot instruction-following abilities.
Does chain-of-thought prompting help? The average accuracy in the COT setting is reported in Table 5, while Table 10 in Appendix F provides a detailed breakdown of the accuracy per subject. We exclude Bloomz-mt and Chinese-Alpaca-13B since these two models are unable to generate valid COT reasoning for a large portion of questions, failing to produce final answers. All models achieve comparable or lower average accuracy than in the answer-only setting. This suggests that</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Zero-shot AO</th>
<th>Five-shot AO</th>
<th>Five-shot COT</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4</td>
<td>53.3</td>
<td>54.9</td>
<td>56.8</td>
</tr>
<tr>
<td>Claude-v1.3</td>
<td>37.6</td>
<td>39.0</td>
<td>39.2</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>36.7</td>
<td>41.4</td>
<td>35.0</td>
</tr>
<tr>
<td>Claude-instant-v1.0</td>
<td>32.1</td>
<td>35.5</td>
<td>33.4</td>
</tr>
<tr>
<td>Bloomz-mt</td>
<td>30.8</td>
<td>30.4</td>
<td>-</td>
</tr>
<tr>
<td>GLM-130B</td>
<td>30.7</td>
<td>30.3</td>
<td>22.6</td>
</tr>
<tr>
<td>LLaMA-65B</td>
<td>29.8</td>
<td>31.7</td>
<td>21.4</td>
</tr>
<tr>
<td>ChatGLM-6B</td>
<td>29.2</td>
<td>23.1</td>
<td>26.1</td>
</tr>
<tr>
<td>MOSS</td>
<td>28.4</td>
<td>24.0</td>
<td>21.6</td>
</tr>
<tr>
<td>Chinese-LLaMA-13B</td>
<td>27.5</td>
<td>27.3</td>
<td>15.4</td>
</tr>
<tr>
<td>Chinese-Alpaca-13B</td>
<td>24.4</td>
<td>27.1</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 6: Average accuracy on C-EVAL HARD in both answer-only (AO) and chain-of-thought (COT) settings.</p>
<p>COT prompting does not necessarily improve results for many subjects in C-EVAL. The primary reasons for this are twofold: (1) many subjects in C-EVAL are not reasoning-intensive, and additional reasoning steps may impair performance. This observation is supported by Chung et al. (2022), who noted performance degradation on MMLU with COT prompting. (2) Some models fail to leverage the benefits of COT prompting, particularly those that did not undergo COT-inclusive instruction tuning. Chung et al. (2022) reported this, noting an 8-point accuracy drop when using COT on MMLU with the 540B PaLM model. This finding partly elucidates the significant decrease in performance of the GLM-130B and LLaMA-65B models in the COT setting. Encouragingly, we still observe that COT prompting leads to considerable improvements for some models in certain subjects - for example, detailed results in Table 10 show that COT improves GPT-4's performance in college physics from $50.6 \%$ to $60.2 \%$, in probability and statistics from $53.6 \%$ to $62.0 \%$, ChatGLM's performance in middle school physics from $20.2 \%$ to $41.0 \%$, and in high school geography from $29.2 \%$ to $38.2 \%$.</p>
<p>Difference between English- and Chinese-oriented models: GLM-130B is the best-performing Chinese-oriented model in our assessment, thus we focus on comparing it to the represented Englishoriented model, ChatGPT, in zero-shot answer-only settings. We do not analyze GPT-4 here since it is not at the same level as all other models, and comparing GLM-130B to it is not very helpful and informative. As illustrated in Table 3, while GLM-130B underperforms ChatGPT by 7.0 points on overall average, the gap significantly narrows on the social science and humanities category, lagging only 2.2 and 1.1 points behind respectively. This reflects that by leveraging more Chinese data, the model might achieve performance on par with or even superior to ChatGPT in areas pertaining to Chinese knowledge, such as history, politics, and law, highlighting situations where Chinese-oriented models may have the upper hand. However, concurrently, we note a significant difference of 12.3 points between GLM-130B and ChatGPT in the STEM category, which implies a substantial gap in more complex tasks that necessitate advanced reasoning skills.</p>
<p>Results on C-EVAL HARD: Table 6 shows the average accuracy on C-EVAL HARD. GPT-4 can only achieve $53.3 \%, 54.9 \%, 56.8 \%$ accuracy on zero-shot AO, five-shot AO, and five-shot COT settings respectively, implying the difficulty of C-EVAL HARD. Interestingly, chain-of-thought prompting improves GPT-4 slightly on these extremely challenging subjects. Indeed, only GPT-4, ChatGPT, and Claude manage to make meaningful progress - improving by at least 10 points - over a random baseline. Our results further confirm that some critical distinction among LLMs comes out when the tasks become complex enough. We underscore the importance of evaluating LLMs in such challenging settings, as current LLM development goes beyond creating a casual chatbot - it involves the development of complex systems or agents capable of interacting with various data types, receiving feedback, reasoning and using tools, and even performing actions (Mialon et al., 2023).</p>
<p>Results on the validation split: Since we do not publicly release the labels for our test split, we provide the average accuracy on the validation split as a reference for developers. The validation split comprises a total of 1346 questions, with each subject contributing fewer than 30 validation questions on average. Therefore, tracking accuracy on a specific subject may not yield significant insights. Instead, we report the average answer-only accuracy across all subjects in Table 7. The average validation accuracy closely mirrors the average test accuracy as presented in Table 3 and Table 4. Additionally, the model ranking on the validation split broadly corresponds to that on the test split. These observations suggest that developers may use the average validation accuracy as a good indicator for expedited development processes.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Zero-shot</th>
<th style="text-align: center;">Five-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">69.9</td>
</tr>
<tr>
<td style="text-align: left;">Claude-v1.3</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">53.5</td>
</tr>
<tr>
<td style="text-align: left;">Bloomz-mt</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: left;">GLM-130B</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: left;">Claude-instant-v1.0</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">47.4</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM-6B</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">37.1</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-65B</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">39.8</td>
</tr>
<tr>
<td style="text-align: left;">MOSS</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">28.9</td>
</tr>
<tr>
<td style="text-align: left;">Chinese-Alpaca-13B</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">27.2</td>
</tr>
<tr>
<td style="text-align: left;">Chinese-LLaMA-13B</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">33.1</td>
</tr>
</tbody>
</table>
<p>Table 7: Average accuracy on the validation split in the answer-only setting.</p>
<h1>4 Related Work</h1>
<p>English benchmarks: Traditional English benchmarks mainly focus on assessing certain abilities of models on a single task or a single type of tasks, such as natural language understanding (NLU, Wang et al. (2019)), reading comprehension (Rajpurkar et al., 2018), machine translation (Bojar et al., 2014), and summarization (Hermann et al., 2015; Narayan et al., 2018). As a representative example, the GLUE benchmark (Wang et al., 2019) combines a collection of NLU tasks, and has witnessed superhuman model performance due to the burst of pretraining models such as BERT (Kenton \&amp; Toutanova, 2019) and GPT (Radford et al., 2019). In order to assess the capabilities of LLMs more comprehensively, recent benchmarks have cast light on the broader knowledge and advanced abilities. The MMLU benchmark (Hendrycks et al., 2021a) provides multi-domain and multi-task evaluation collected from real-world examinations and books. LLMs' performance on MMLU fluctuates around random-chance accuracy until they reach the scale of GPT-3. The BIG-bench benchmark (Srivastava et al., 2022) consists of 204 diverse tasks, some of which are considered to be beyond the capabilities of current LLMs. The HELM benchmark (Liang et al., 2022) aggregates 42 different tasks and evaluates LLMs with 7 metrics ranging from accuracy to robustness.
Chinese benchmarks: Despite the flourishing of English benchmark, language abilities in Chinese language environment remain under-developed. The CLUE benchmark (Xu et al., 2020) is the first large-scale Chinese NLU benchmark, and still serves as the most widely-used and best available Chinese benchmark. Recently, the AGIEval benchmark (Zhong et al., 2023) contains data from the Chinese College Entrance Exam, Chinese lawyer qualification test and Chinese civil service examination. The MMCU benchmark (Zeng, 2023) consists of tests from four major domains including medicine, law, psychology and education, which are also collected from Chinese College Entrance Exam, qualification test as well as university examinations. Compared to AGIEval and MMCU, C-EVAL (1) has a broader coverage of domains (§2.2), (2) features four different levels of difficulty - particularly, the C-EVAL HARD benchmark is the first Chinese benchmark to provide sophisticated reasoning problems, and (3) makes an effort to mitigate data leakage - our questions mostly come from mock exams as PDF or Microsoft Word documents that are further processed by us, while AGIEval and MMCU collects the exact questions from past national exams in China.</p>
<h2>5 Discussion</h2>
<p>We believe that the evaluation of LLMs should transcend the scope of casual conversational bots, guiding developers in preparing LLMs to function in more complex scenarios. This was the primary motivation behind the creation of C-EVAL, a challenging evaluation suite. We hope that C-EVAL along with C-EVAL HARD have made important progress on this direction particularly in a Chinese context. We also note that, C-EVAL, along with all the English-language benchmarks, is far from perfect for LLM evaluation. There are many other abilities such as reasoning over and calling APIs, as well as multiple aspects that extend beyond accuracy, including safety, bias, and robustness. We leave further exploration on their evaluation for future work.</p>
<h1>Acknowledgement</h1>
<p>We thank the anonymous reviewers for their comments. Yuzhuo Bai is supported by the National Key R\&amp;D Program of China (No. 2020AAA0106502) and Institute Guo Qiang at Tsinghua University.</p>
<h2>References</h2>
<p>Alibaba. Tongyi qianwen. Alibaba Blog, 2023. URL https://tongyi.aliyun.com.
Anthropic. Introducing claude. Anthropic Blog, 2022. URL https://www.anthropic.com/ index/introducing-claude.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>Baidu. Wenxin yiyan. Baidu Blog, 2023. URL https://yiyan.baidu.com.
Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleš Tamchyna. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pp. 12-58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-3302. URL https://aclanthology.org/W14-3302.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https : //lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Yiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca. arXiv preprint arXiv:2304.08177, 2023. URL https://arxiv.org/abs/2304.08177.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. $320-335,2022$.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In J. Vanschoren and S. Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran, 2021b. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/ 2021/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
iFLYTEK. Xunfei xinghuo. iFLYTEK Blog, 2023. URL https://xinghuo.xfyun.cn.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 41714186, 2019.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=e2TBb5y0yFf.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.</p>
<p>Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1797-1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/ D18-1206. URL https://aclanthology.org/D18-1206.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022.</p>
<p>OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI Blog, 2022. URL https: //openai.com/blog/chatgpt/.</p>
<p>OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
OpenLMLab. Moss. https://github.com/OpenLMLab/MOSS, 2023.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784-789, 2018.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>THUDM. ChatGLM. https://github.com/THUDM/ChatGLM-6B, 2023a.
THUDM. ChatGLM2. https://github.com/THUDM/ChatGLM2-6B, 2023b.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id=rJ4km2R5t7.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id= _VjQ1MeSB_J.</p>
<p>Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633, 2023.</p>
<p>Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. CLUE: A Chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 47624772, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.419. URL https://aclanthology.org/2020 . coling-main. 419 .</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=-Aw0rrrPUF.</p>
<p>Hui Zeng. Measuring massive multitask chinese understanding. arXiv preprint arXiv:2304.12986, 2023.</p>
<p>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WZH7099tgfM.</p>
<h1>A Author Contributions</h1>
<p>Data collection, annotation, and initial validation: Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Yikai Zhang, and Jiayi Lei collected the first version of the data, including some necessary manual annotation from PDF or Word documents. They then cross-validated the collected data. Junlei Zhang prompted GPT-4 to generate the explanation data, and then Yuzhen Huang, Yuzhuo Bai, and Junxian He manually revised the explanations.
Data processing: Yuzhen Huang did most of the data processing job including a lot of manual revisions, such as fixing typos/incorrect formats of the questions from PDF documents and ensuring all the $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ notations can be correctly compiled. Jinghan Zhang did the data deduplication.
Evaluation: Yuzhen Huang tested ChatGPT and MOSS; Yuzhuo Bai tested ChatGLM; Junlei Zhang tested GLM-130B; Chuancheng Lv tested Bloomz-mt, LLaMA, Chinese-LLaMA, and ChineseAlpaca; Yao Fu tested GPT-4 and the Claude model variants.
Website and submission system: Zhihao Zhu built the website and the online submission system.
Paper Writing: Yuzhen Huang, Yuzhuo Bai, and Junxian He wrote the main content of this paper, while other authors helped proofread.
Advising: Yao Fu, Maosong Sun, and Junxian He take advisor roles in this project. Junxian He is the main advisor, initializing and organizing this project.</p>
<h2>B Detailed Stats of C-EVAL</h2>
<p>Table 8 lists all the C-EVAL tasks and their broader categories, as well as the number of questions included in each task.</p>
<h2>C Explanation Data Generation</h2>
<p>We show an example of the automatic COT explanation generation via GPT-4 in Figure 5. We use five human-written question-explanation pairs to prompt GPT-4 to generate explanation giving question and its correct answer. The generated explanations are further revised manually to obtain the final explanations.</p>
<h2>D Evaluation Prompts</h2>
<p>We show the evaluation prompts of answer-only and chain-of-thought test in Figure 6 and Figure 7 respectively.</p>
<h2>E Details of the models being evaluated</h2>
<p>ChatGPT and GPT-4 are GPT-series models that are enhanced to follow human instructions and be more helpful, harmless and honest using Reinforcement Learning from Human Feedback. GPT-4 additionally enables image inputs and goes through well-designed post-training alignment process, as well as having a larger scale than most of the existing model. GPT-4 achieves human-level performance on various benchmark, and even scored to be the top $10 \%$ in some simulated exams.
Claude is the latest Anthropic-series LLM that also focuses on human intention alignment. Applying the constitutional AI approach(Bai et al., 2022), Claude manages to be both helpful and trustworthy. Claude-instant is the lighter vesrion with less cost and faster inference of Claude.
BLOOMZ-mt (Muennighoff et al., 2022) is created by combining multitask prompted finetuning to the pretrained multilingual BLOOM model(Scao et al., 2022), using not only English prompts but also machine-translated prompts to match the language of multilingual tasks, and are found to be capable at task- and language-agnostic generalization. We evaluate the 176B version in our experiment.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subject</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;"># Questions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Advanced Mathematics (高等数学)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">197</td>
</tr>
<tr>
<td style="text-align: left;">College Chemistry (大学化学)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">253</td>
</tr>
<tr>
<td style="text-align: left;">College Physics (大学物理)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">College Programming (大学编程)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">384</td>
</tr>
<tr>
<td style="text-align: left;">Computer Architecture (计算机组成)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">219</td>
</tr>
<tr>
<td style="text-align: left;">Computer Network (计算机网络)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">195</td>
</tr>
<tr>
<td style="text-align: left;">Discrete Mathematics (离散数学)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">174</td>
</tr>
<tr>
<td style="text-align: left;">Electrical Engineer (注册电气工程师)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">381</td>
</tr>
<tr>
<td style="text-align: left;">High School Biology (高中生物)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">199</td>
</tr>
<tr>
<td style="text-align: left;">High School Chemistry (高中化学)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">196</td>
</tr>
<tr>
<td style="text-align: left;">High School Mathematics (高中数学)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">189</td>
</tr>
<tr>
<td style="text-align: left;">High School Physics (高中物理)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">199</td>
</tr>
<tr>
<td style="text-align: left;">Metrology Engineer (注册计量师)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">248</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Biology (初中生物)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">218</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Chemistry (初中化学)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">210</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Mathematics (初中数学)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">201</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Physics (初中物理)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">202</td>
</tr>
<tr>
<td style="text-align: left;">Operating System (操作系统)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">203</td>
</tr>
<tr>
<td style="text-align: left;">Probability and Statistics (概率统计)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">189</td>
</tr>
<tr>
<td style="text-align: left;">Veterinary Medicine (兽医学)</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">238</td>
</tr>
<tr>
<td style="text-align: left;">Business Administration (工商管理)</td>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">339</td>
</tr>
<tr>
<td style="text-align: left;">College Economics (大学经济学)</td>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">557</td>
</tr>
<tr>
<td style="text-align: left;">Education Science (教育学)</td>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">304</td>
</tr>
<tr>
<td style="text-align: left;">High School Geography (高中地理)</td>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">202</td>
</tr>
<tr>
<td style="text-align: left;">High School Politics (高中政治)</td>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Mao Zedong Thought (毛泽东思想和中国特色社会主义理论体系概论)</td>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">248</td>
</tr>
<tr>
<td style="text-align: left;">Marxism (马克思主义基本原理)</td>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">203</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Geography (初中地理)</td>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">125</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Politics (初中政治)</td>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">219</td>
</tr>
<tr>
<td style="text-align: left;">Teacher Qualification (教师资格)</td>
<td style="text-align: center;">Social Science</td>
<td style="text-align: center;">448</td>
</tr>
<tr>
<td style="text-align: left;">Art Studies (艺术学)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">336</td>
</tr>
<tr>
<td style="text-align: left;">Chinese Language and Literature (中国语言文学)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">237</td>
</tr>
<tr>
<td style="text-align: left;">High School Chinese (高中语文)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">202</td>
</tr>
<tr>
<td style="text-align: left;">High School History (高中历史)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">207</td>
</tr>
<tr>
<td style="text-align: left;">Ideological and Moral Cultivation (思想道德修养与法律基础)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">196</td>
</tr>
<tr>
<td style="text-align: left;">Law (法学)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td style="text-align: left;">Legal Professional (法律职业资格)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">243</td>
</tr>
<tr>
<td style="text-align: left;">Logic (逻辑学)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">231</td>
</tr>
<tr>
<td style="text-align: left;">Middle School History (初中历史)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">234</td>
</tr>
<tr>
<td style="text-align: left;">Modern Chinese History (近代史纲要)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">240</td>
</tr>
<tr>
<td style="text-align: left;">Professional Tour Guide (导游资格)</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: left;">Accountant (注册会计师)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">497</td>
</tr>
<tr>
<td style="text-align: left;">Basic Medicine (基础医学)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">199</td>
</tr>
<tr>
<td style="text-align: left;">Civil Servant (公务员)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">481</td>
</tr>
<tr>
<td style="text-align: left;">Clinical Medicine (临床医学)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">227</td>
</tr>
<tr>
<td style="text-align: left;">Environmental Impact Assessment Engineer (环境影响评价工程师)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">317</td>
</tr>
<tr>
<td style="text-align: left;">Fire Engineer (注册消防工程师)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">318</td>
</tr>
<tr>
<td style="text-align: left;">Physician (医师资格)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">497</td>
</tr>
<tr>
<td style="text-align: left;">Plant Protection (植物保护)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">226</td>
</tr>
<tr>
<td style="text-align: left;">Sports Science (体育学)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">204</td>
</tr>
<tr>
<td style="text-align: left;">Tax Accountant (税务师)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">497</td>
</tr>
<tr>
<td style="text-align: left;">Urban and Rural Planner (注册城乡规划师)</td>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">469</td>
</tr>
</tbody>
</table>
<p>Table 8: Summary of all 52 subjects.</p>
<p>LLaMA (Touvron et al., 2023) is a Transformer-architecture LLM that is trained on a mixture of several open sources. Applying several improvements on vanilla Transformers used by previous LLMs, and optimized to improve the training efficiency, LLaMA shows strong language abilities and can surpass models having 10x larger parameters than LLaMA. We evaluate the LLaMA-65B version in our experiment.
GLM-130B and ChatGLM-6B are based on the General language model (GLM) structure that gain benefits from its bidirectional attention advantage. By using alterable number and length of</p>
<p>... [5-shot examples] ...
以下是中国关于初中物理的单项选择题，请根据以下题目和答案，生成一段正确，合理的答案解析。
The following are multiple-choice questions about middle school physics in China. Please generate a correct and reasonable explanation based on the following questions and answers.
下列属于可再生能源的是 $\qquad$。
Which of the following is a renewable energy source?
A. 石油
B.煤炭
C. 核燃料
D. 太阳能
A. petroleum
B. coal
C. nuclear fuel
D. solar energy</p>
<p>答案: D
Answer: D
你生成的答案解析是什么?
What is your explanation generated?
答案解析: 让我们一步一步思考。</p>
<ol>
<li>(A) 石油是一种化石能源，是从地下深处开采出来的，不属于可再生能源。</li>
<li>(B) 煤炭也是一种化石能源，是从地下深处开采出来的，不属于可再生能源。</li>
<li>(C) 核燃料是指用于核反应堆中的燃料，包括铀、钚等，虽然可以通过核反应产生能量，但是核燃料是有限的，不属于可再生能源。</li>
<li>(D) 太阳能是指从太阳辐射中获取的能量，是一种可再生能源，因为太阳每天都会升起，不会因为使用而枯竭。
所以答案是D。
Explanation: Let's think step by step,</li>
<li>(A) Petroleum is a fossil fuel extracted from deep underground and is not a renewable energy source.</li>
<li>(B) Coal is also a fossil fuel extracted from deep underground and is not a renewable energy source.</li>
<li>(C) Nuclear fuel refers to the fuel used in nuclear reactors, including uranium, plutonium, etc. Although energy can be generated through nuclear reactions, nuclear fuel is limited and is not a renewable energy source.</li>
<li>(D) Solar energy refers to the energy obtained from solar radiation and is a renewable energy source, because the sun rises every day and will not be exhausted due to use.
Therefore, the answer is $C$.
Figure 5: An example of generating explanations via GPT-4. The red text is the autocompleted response from model, while the preceding text is the user-inputted prompt. We indicate English translation below the corresponding Chinese text for each paragraph.</li>
</ol>
<h1>以下是中国关于临床医学的单项选择题，请选出其中的正确答案。 <br> The following are multiple-choice questions about clinical medicine in China. Please select the correct answer.</h1>
<p>男性，65岁，昏迷10小时。血气分析： $\mathrm{pH} 7.26, \mathrm{PaCO}<em 2="2">{2} 82 \mathrm{mmHg}, \mathrm{PaO}</em>$ 。下列哪项处理不适宜 $\qquad$
A 65-year-old male patient has been in a coma for 10 hours. Blood gas test shows a pH of $7.26, \mathrm{PaCO}} 45 \mathrm{mmHg<em 2="2">{2} 82 \mathrm{mmHg}$, and $\mathrm{PaO}</em>$. Which of the following treatments is not appropriate? $\qquad$
A. 机械通气
B. 呼吸兴奋剂
C. 支持疗法
D. 高浓度给氧
A. mechanical ventilation B. respiratory stimulants C. supportive therapy D. high concentration oxygen therapy} 45 \mathrm{mmHg</p>
<p>答案: D
Answer: D
... [5-shot examples] ...
硅尘可导致硅沉着病，其中致病力最强的硅尘颗粒直径为 $\qquad$
Silicon dust can cause silicosis, and the most pathogenic silicon dust particles have a diameter of $\qquad$ .
A. $&gt;5 \mu \mathrm{~m}$
B. $4 \sim 5 \mu \mathrm{~m}$
C. $1 \sim 2 \mu \mathrm{~m}$
D. $2 \sim 3 / \mu \mathrm{m}$</p>
<p>答案: C
Answer: C
Figure 6: An example of few-shot evaluation in answer-only scenarios, while zero-shot evaluation is similar by removing the exemplars. The red text is the autocompleted response from model, while the preceding text is the inputted prompt. We indicate English translation below the corresponding Chinese text.
blanks, GLM can adapt to various tasks. GLM-130B is a bilingual pre-trained GLM that ultilizes selfsupervised learning and multitask instruction pretraining. GLM-130B also realized INT4 quantization</p>
<div class="codehilite"><pre><span></span><code><span class="nx">以下是中国关于计算机组成考试的单项选择题</span><span class="err">，</span><span class="nx">请选出其中的正确答案</span><span class="err">。</span>
<span class="nx">The</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">multiple</span><span class="o">-</span><span class="kd">choice</span><span class="w"> </span><span class="nx">questions</span><span class="w"> </span><span class="nx">about</span><span class="w"> </span><span class="nx">computer</span><span class="w"> </span><span class="nx">architecture</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">China</span><span class="p">.</span><span class="w"> </span><span class="nx">Please</span><span class="w"> </span><span class="nx">select</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">correct</span>
<span class="nx">answer</span><span class="p">.</span>
<span class="o">...</span><span class="w"> </span><span class="p">[</span><span class="mi">5</span><span class="o">-</span><span class="nx">shot</span><span class="w"> </span><span class="nx">examples</span><span class="p">]</span><span class="w"> </span><span class="o">...</span>
<span class="nx">设寄存器R的内容</span><span class="p">(</span><span class="nx">R</span><span class="p">)=</span><span class="mi">1500</span><span class="nx">H</span><span class="err">，</span><span class="nx">内存单元1500H的内容为2500H</span><span class="err">，</span><span class="nx">内存单元2500H的内容为3500H</span><span class="err">，</span><span class="nx">PC的值</span>
<span class="nx">为4500H</span><span class="err">，</span><span class="nx">采用相对寻址方式</span><span class="err">，</span><span class="nx">有效地址为2500H的操作数是</span>
<span class="nx">If</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">content</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">register</span><span class="w"> </span><span class="nx">R</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="mi">1500</span><span class="w"> </span><span class="nx">H</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">content</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">memory</span><span class="w"> </span><span class="nx">address</span><span class="w"> </span><span class="mi">1500</span><span class="w"> </span><span class="nx">H</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">2500</span><span class="w"> </span><span class="nx">H</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">content</span><span class="w"> </span><span class="nx">of</span>
<span class="nx">memory</span><span class="w"> </span><span class="nx">address</span><span class="w"> </span><span class="mi">2500</span><span class="w"> </span><span class="nx">H</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">3500</span><span class="w"> </span><span class="nx">H</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">value</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">program</span><span class="w"> </span><span class="nx">counter</span><span class="w"> </span><span class="p">(</span><span class="nx">PC</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">4500</span><span class="w"> </span><span class="nx">H</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">relative</span>
<span class="nx">addressing</span><span class="w"> </span><span class="nx">mode</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">operand</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">effective</span><span class="w"> </span><span class="nx">address</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="mi">2500</span><span class="w"> </span><span class="nx">H</span><span class="w"> </span><span class="k">is</span>
<span class="nx">A</span><span class="p">.</span><span class="w"> </span><span class="mi">1500</span><span class="nx">H</span>
<span class="nx">答案</span><span class="err">：</span><span class="nx">让我们一步一步思考</span><span class="err">。</span>
</code></pre></div>

<ol>
<li>根据题目中的信息，内存单元1500H的内容为2500H，内存单元2500H的内容为3500H，表示当前指令的地址为 $2500 \mathrm{H}-1500 \mathrm{H}=1000 \mathrm{H}_{\text {。 }}$</li>
<li>根据相对寻址方式，有效地址为 2500 H 的操作数需要加上当前指令地址 1000 H 才能得到实际的操作数地址，即 3500 H 。
所以答案是C。
Answer: Let's think step by step,</li>
<li>According to the information given in the question, the content of memory unit 1500 H is 2500 H , and the content of memory unit 2500 H is 3500 H . This indicates that the current instruction address is 2500 H -
$1500 \mathrm{H}=1000 \mathrm{H}$.</li>
<li>According to the relative addressing mode, in order to obtain the actual operand address of the effective address with the value of 2500 H , the current instruction address of 1000 H needs to be added to it, resulting in the actual operand address of 3500 H .
Therefore, the answer is $C$.
Figure 7: An example of few-shot evaluation in chain-of-thought scenarios, while zero-shot evaluation is similar by removing the exemplars. The red text is the autocompleted response from model, while the preceding text is the inputted prompt. We indicate English translation below the corresponding Chinese text.
with little to no quality degradation that significantly accelerate its inference efficiency. ChatGLM-6B is a lightweight conversational version of the GLM family that has been specially optimized for Chinese contexts. ChatGLM-6B also applies quantization so it can be deployed with a consumergrade graphic memory requirement as little as 6 GB . We evaluate on the fp16 settings for both two models in our experiment.</li>
</ol>
<p>Chinese-LLaMA is an adaptation of original LLaMA into Chinese language environments. Chinese-LLaMA expands the original LLaMA by adding 20K Chinese tokens into its vocabulary, and is secondarily pre-trained and instruction fine-tuned on Chinese data. We evaluate Chinese-LLaMA13B in our experiment, the largest Chinese-LLaMA variant.</p>
<p>Chinese-Alpaca is based on the Chinese-LLaMA checkpoint that is further tuned on Chinese instruction tuning data. We evaluate Chinese-Alpaca-13B in our experiment, the largest ChineseAlpaca variant.</p>
<p>MOSS is the first open-source Chinese LLM that matchs ChatGPT on both the training scale and alignment techniques. MOSS is initialized with CodeGen(Nijkamp et al., 2022), being pretrained on 100B Chinese tokens and 20B English tokens, and has further integrated supervised fine-tuning and preference model, as well as plugin augmentation, but not all the version are publicly available. We evaluate the moss-moon-003-sft version in our experiment.</p>
<h1>F Breakdown of Model Performance</h1>
<p>Table 9 and Table 10 show the detailed breakdown of accuracy per subject of four representative models in zero- and five-shot settings respectively, while we refer the readers to our website leaderboard https://cevalbenchmark.com/static/leaderboard.html for a detailed breakdown of results for all models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subject</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">GLM-130B</th>
<th style="text-align: center;">Claude-instant-v1.0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Advanced Mathematics</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: left;">College Chemistry</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">25.9</td>
</tr>
<tr>
<td style="text-align: left;">College Physics</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: left;">College Programming</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">40.6</td>
</tr>
<tr>
<td style="text-align: left;">Computer Architecture</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">42.5</td>
</tr>
<tr>
<td style="text-align: left;">Computer Network</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: left;">Discrete Mathematics</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">28.8</td>
</tr>
<tr>
<td style="text-align: left;">Electrical Engineer</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">32.7</td>
</tr>
<tr>
<td style="text-align: left;">High School Biology</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">40.0</td>
</tr>
<tr>
<td style="text-align: left;">High School Chemistry</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">39.5</td>
</tr>
<tr>
<td style="text-align: left;">High School Mathematics</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">24.7</td>
</tr>
<tr>
<td style="text-align: left;">High School Physics</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">40.0</td>
</tr>
<tr>
<td style="text-align: left;">Metrology Engineer</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">48.4</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Biology</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">46.4</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Chemistry</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">47.0</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Mathematics</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">34.5</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Physics</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">48.9</td>
</tr>
<tr>
<td style="text-align: left;">Operating System</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">46.4</td>
</tr>
<tr>
<td style="text-align: left;">Probability and Statistics</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">27.7</td>
</tr>
<tr>
<td style="text-align: left;">Veterinary Medicine</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">43.8</td>
</tr>
<tr>
<td style="text-align: left;">Business Administration</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">40.2</td>
</tr>
<tr>
<td style="text-align: left;">College Economics</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: left;">Education Science</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">43.0</td>
</tr>
<tr>
<td style="text-align: left;">High School Geography</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">42.7</td>
</tr>
<tr>
<td style="text-align: left;">High School Politics</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">37.5</td>
</tr>
<tr>
<td style="text-align: left;">Mao Zedong Thought</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">49.3</td>
</tr>
<tr>
<td style="text-align: left;">Marxism</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">62.0</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Geography</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">49.1</td>
</tr>
<tr>
<td style="text-align: left;">Middle School Politics</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: left;">Teacher Qualification</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">54.9</td>
</tr>
<tr>
<td style="text-align: left;">Art Studies</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: left;">Chinese Language and Literature</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">35.9</td>
</tr>
<tr>
<td style="text-align: left;">High School Chinese</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">31.5</td>
</tr>
<tr>
<td style="text-align: left;">High School History</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">41.2</td>
</tr>
<tr>
<td style="text-align: left;">Ideological and Moral Cultivation</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">58.1</td>
</tr>
<tr>
<td style="text-align: left;">Law</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">34.8</td>
</tr>
<tr>
<td style="text-align: left;">Legal Professional</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">36.7</td>
</tr>
<tr>
<td style="text-align: left;">Logic</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">36.3</td>
</tr>
<tr>
<td style="text-align: left;">Middle School History</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: left;">Modern Chinese History</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">42.5</td>
</tr>
<tr>
<td style="text-align: left;">Professional Tour Guide</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">35.0</td>
</tr>
<tr>
<td style="text-align: left;">Accountant</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">38.1</td>
</tr>
<tr>
<td style="text-align: left;">Basic Medicine</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">39.4</td>
</tr>
<tr>
<td style="text-align: left;">Civil Servant</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">36.1</td>
</tr>
<tr>
<td style="text-align: left;">Clinical Medicine</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: left;">Environmental Impact Assessment Engineer</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">41.3</td>
</tr>
<tr>
<td style="text-align: left;">Fire Engineer</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">29.4</td>
</tr>
<tr>
<td style="text-align: left;">Physician</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">41.5</td>
</tr>
<tr>
<td style="text-align: left;">Plant Protection</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">48.2</td>
</tr>
<tr>
<td style="text-align: left;">Sports Science</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">40.6</td>
</tr>
<tr>
<td style="text-align: left;">Tax Accountant</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">38.1</td>
</tr>
<tr>
<td style="text-align: left;">Urban and Rural Planner</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">38.0</td>
</tr>
</tbody>
</table>
<p>Table 9: Zero-shot answer only accuracy per subject.</p>
<h1>G Option Bias</h1>
<p>The distribution of the correct answer is shown in the table 11. We admit that there are fluctuations in the proportion of different options. However, these fluctuations are relatively small around a random $25 \%$ level and similar to MMLU.</p>
<p>To verify if the option bias exists on models, we permuted the order of the choices in the questions and randomly selected 5 distinct permutations of choices for each question. Then we evaluate ChatGPT, ChatGLM-6B and ChatGLM2-6B (THUDM, 2023b) on the 5 different permutations of C-EVAL. The zero-shot results in the answer-only setting are shown in table 12. We report the average accuracy</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subject</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">ChatGLM</th>
<th style="text-align: center;">Claude-instant-v1.0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Advanced Mathematics</td>
<td style="text-align: center;">49.7 / 53.8</td>
<td style="text-align: center;">48.0 / 29.5</td>
<td style="text-align: center;">3.5 / 23.1</td>
<td style="text-align: center;">39.9 / 41.6</td>
</tr>
<tr>
<td style="text-align: center;">College Chemistry</td>
<td style="text-align: center;">59.4 / 55.8</td>
<td style="text-align: center;">39.7 / 33.9</td>
<td style="text-align: center;">17.0 / 28.6</td>
<td style="text-align: center;">33.5 / 32.1</td>
</tr>
<tr>
<td style="text-align: center;">College Physics</td>
<td style="text-align: center;">50.6 / 60.2</td>
<td style="text-align: center;">35.8 / 35.2</td>
<td style="text-align: center;">14.8 / 30.1</td>
<td style="text-align: center;">35.8 / 31.3</td>
</tr>
<tr>
<td style="text-align: center;">College Programming</td>
<td style="text-align: center;">78.1 / 77.5</td>
<td style="text-align: center;">57.9 / 57.9</td>
<td style="text-align: center;">18.7 / 24.0</td>
<td style="text-align: center;">48.5 / 50.3</td>
</tr>
<tr>
<td style="text-align: center;">Computer Architecture</td>
<td style="text-align: center;">74.6 / 75.6</td>
<td style="text-align: center;">68.9 / 63.7</td>
<td style="text-align: center;">30.1 / 27.5</td>
<td style="text-align: center;">52.3 / 50.8</td>
</tr>
<tr>
<td style="text-align: center;">Computer Network</td>
<td style="text-align: center;">77.8 / 77.2</td>
<td style="text-align: center;">58.5 / 59.6</td>
<td style="text-align: center;">36.3 / 28.1</td>
<td style="text-align: center;">46.8 / 53.8</td>
</tr>
<tr>
<td style="text-align: center;">Discrete Mathematics</td>
<td style="text-align: center;">66.7 / 62.7</td>
<td style="text-align: center;">43.1 / 30.7</td>
<td style="text-align: center;">17.0 / 26.1</td>
<td style="text-align: center;">32.7 / 33.3</td>
</tr>
<tr>
<td style="text-align: center;">Electrical Engineer</td>
<td style="text-align: center;">49.9 / 56.0</td>
<td style="text-align: center;">39.5 / 44.0</td>
<td style="text-align: center;">23.9 / 31.3</td>
<td style="text-align: center;">31.3 / 35.7</td>
</tr>
<tr>
<td style="text-align: center;">High School Biology</td>
<td style="text-align: center;">72.6 / 70.3</td>
<td style="text-align: center;">53.7 / 48.0</td>
<td style="text-align: center;">28.0 / 26.9</td>
<td style="text-align: center;">44.6 / 38.9</td>
</tr>
<tr>
<td style="text-align: center;">High School Chemistry</td>
<td style="text-align: center;">52.3 / 55.8</td>
<td style="text-align: center;">52.9 / 36.0</td>
<td style="text-align: center;">19.8 / 28.5</td>
<td style="text-align: center;">39.0 / 37.8</td>
</tr>
<tr>
<td style="text-align: center;">High School Mathematics</td>
<td style="text-align: center;">38.0 / 42.8</td>
<td style="text-align: center;">34.3 / 37.3</td>
<td style="text-align: center;">4.2 / 24.1</td>
<td style="text-align: center;">24.1 / 32.5</td>
</tr>
<tr>
<td style="text-align: center;">High School Physics</td>
<td style="text-align: center;">69.1 / 61.1</td>
<td style="text-align: center;">43.4 / 34.9</td>
<td style="text-align: center;">10.9 / 21.7</td>
<td style="text-align: center;">44.0 / 26.9</td>
</tr>
<tr>
<td style="text-align: center;">Metrology Engineer</td>
<td style="text-align: center;">70.3 / 71.2</td>
<td style="text-align: center;">63.0 / 59.8</td>
<td style="text-align: center;">41.1 / 48.4</td>
<td style="text-align: center;">50.7 / 55.3</td>
</tr>
<tr>
<td style="text-align: center;">Middle School Biology</td>
<td style="text-align: center;">91.2 / 86.5</td>
<td style="text-align: center;">67.7 / 63.5</td>
<td style="text-align: center;">30.7 / 38.0</td>
<td style="text-align: center;">55.2 / 57.3</td>
</tr>
<tr>
<td style="text-align: center;">Middle School Chemistry</td>
<td style="text-align: center;">81.1 / 71.4</td>
<td style="text-align: center;">71.4 / 60.0</td>
<td style="text-align: center;">36.8 / 42.2</td>
<td style="text-align: center;">58.4 / 59.5</td>
</tr>
<tr>
<td style="text-align: center;">Middle School Mathematics</td>
<td style="text-align: center;">62.7 / 66.7</td>
<td style="text-align: center;">44.6 / 42.4</td>
<td style="text-align: center;">3.4 / 20.3</td>
<td style="text-align: center;">28.8 / 33.3</td>
</tr>
<tr>
<td style="text-align: center;">Middle School Physics</td>
<td style="text-align: center;">81.5 / 78.7</td>
<td style="text-align: center;">66.9 / 57.9</td>
<td style="text-align: center;">20.2 / 41.0</td>
<td style="text-align: center;">52.8 / 53.4</td>
</tr>
<tr>
<td style="text-align: center;">Operating System</td>
<td style="text-align: center;">82.1 / 80.4</td>
<td style="text-align: center;">71.5 / 60.3</td>
<td style="text-align: center;">40.8 / 27.4</td>
<td style="text-align: center;">57.0 / 57.5</td>
</tr>
<tr>
<td style="text-align: center;">Probability and Statistics</td>
<td style="text-align: center;">53.6 / 62.0</td>
<td style="text-align: center;">33.7 / 42.8</td>
<td style="text-align: center;">27.7 / 26.5</td>
<td style="text-align: center;">34.9 / 31.9</td>
</tr>
<tr>
<td style="text-align: center;">Veterinary Medicine</td>
<td style="text-align: center;">80.0 / 80.0</td>
<td style="text-align: center;">64.3 / 58.1</td>
<td style="text-align: center;">39.5 / 35.2</td>
<td style="text-align: center;">51.0 / 53.3</td>
</tr>
<tr>
<td style="text-align: center;">Business Administration</td>
<td style="text-align: center;">67.1 / 65.4</td>
<td style="text-align: center;">52.8 / 46.8</td>
<td style="text-align: center;">32.6 / 34.9</td>
<td style="text-align: center;">44.2 / 42.9</td>
</tr>
<tr>
<td style="text-align: center;">College Economics</td>
<td style="text-align: center;">71.4 / 74.4</td>
<td style="text-align: center;">52.5 / 51.3</td>
<td style="text-align: center;">22.1 / 30.0</td>
<td style="text-align: center;">47.7 / 50.3</td>
</tr>
<tr>
<td style="text-align: center;">Education Science</td>
<td style="text-align: center;">68.5 / 69.3</td>
<td style="text-align: center;">59.6 / 54.4</td>
<td style="text-align: center;">37.8 / 33.0</td>
<td style="text-align: center;">51.1 / 51.9</td>
</tr>
<tr>
<td style="text-align: center;">High School Geography</td>
<td style="text-align: center;">74.7 / 70.2</td>
<td style="text-align: center;">58.4 / 55.1</td>
<td style="text-align: center;">29.2 / 38.2</td>
<td style="text-align: center;">52.8 / 49.4</td>
</tr>
<tr>
<td style="text-align: center;">High School Politics</td>
<td style="text-align: center;">77.8 / 69.3</td>
<td style="text-align: center;">52.3 / 51.1</td>
<td style="text-align: center;">25.0 / 29.6</td>
<td style="text-align: center;">38.1 / 39.8</td>
</tr>
<tr>
<td style="text-align: center;">Mao Zedong Thought</td>
<td style="text-align: center;">79.5 / 79.9</td>
<td style="text-align: center;">60.7 / 63.5</td>
<td style="text-align: center;">46.6 / 50.7</td>
<td style="text-align: center;">47.0 / 50.2</td>
</tr>
<tr>
<td style="text-align: center;">Marxism</td>
<td style="text-align: center;">81.6 / 82.7</td>
<td style="text-align: center;">71.5 / 70.4</td>
<td style="text-align: center;">40.2 / 49.2</td>
<td style="text-align: center;">66.5 / 63.7</td>
</tr>
<tr>
<td style="text-align: center;">Middle School Geography</td>
<td style="text-align: center;">83.3 / 83.3</td>
<td style="text-align: center;">60.2 / 56.5</td>
<td style="text-align: center;">33.3 / 44.4</td>
<td style="text-align: center;">62.0 / 52.8</td>
</tr>
<tr>
<td style="text-align: center;">Middle School Politics</td>
<td style="text-align: center;">87.1 / 85.5</td>
<td style="text-align: center;">74.1 / 67.4</td>
<td style="text-align: center;">41.5 / 40.9</td>
<td style="text-align: center;">60.6 / 64.3</td>
</tr>
<tr>
<td style="text-align: center;">Teacher Qualification</td>
<td style="text-align: center;">85.0 / 85.0</td>
<td style="text-align: center;">75.7 / 66.9</td>
<td style="text-align: center;">48.9 / 49.1</td>
<td style="text-align: center;">68.4 / 62.2</td>
</tr>
<tr>
<td style="text-align: center;">Art Studies</td>
<td style="text-align: center;">65.1 / 66.1</td>
<td style="text-align: center;">49.7 / 52.3</td>
<td style="text-align: center;">34.6 / 36.9</td>
<td style="text-align: center;">35.6 / 38.9</td>
</tr>
<tr>
<td style="text-align: center;">Chinese Language and Literature</td>
<td style="text-align: center;">61.2 / 67.0</td>
<td style="text-align: center;">50.2 / 51.7</td>
<td style="text-align: center;">32.5 / 37.3</td>
<td style="text-align: center;">41.2 / 44.0</td>
</tr>
<tr>
<td style="text-align: center;">High School Chinese</td>
<td style="text-align: center;">37.6 / 39.3</td>
<td style="text-align: center;">36.0 / 27.5</td>
<td style="text-align: center;">26.4 / 19.1</td>
<td style="text-align: center;">30.9 / 27.0</td>
</tr>
<tr>
<td style="text-align: center;">High School History</td>
<td style="text-align: center;">68.1 / 68.1</td>
<td style="text-align: center;">54.4 / 45.1</td>
<td style="text-align: center;">35.2 / 40.1</td>
<td style="text-align: center;">52.7 / 40.7</td>
</tr>
<tr>
<td style="text-align: center;">Ideological and Moral Cultivation</td>
<td style="text-align: center;">77.3 / 77.3</td>
<td style="text-align: center;">66.9 / 68.0</td>
<td style="text-align: center;">48.3 / 52.3</td>
<td style="text-align: center;">62.8 / 63.4</td>
</tr>
<tr>
<td style="text-align: center;">Law</td>
<td style="text-align: center;">60.6 / 54.8</td>
<td style="text-align: center;">43.9 / 34.8</td>
<td style="text-align: center;">23.5 / 29.9</td>
<td style="text-align: center;">38.0 / 33.0</td>
</tr>
<tr>
<td style="text-align: center;">Legal Professional</td>
<td style="text-align: center;">54.4 / 48.4</td>
<td style="text-align: center;">44.7 / 32.6</td>
<td style="text-align: center;">27.0 / 26.5</td>
<td style="text-align: center;">39.5 / 30.2</td>
</tr>
<tr>
<td style="text-align: center;">Logic</td>
<td style="text-align: center;">60.3 / 63.2</td>
<td style="text-align: center;">37.7 / 35.8</td>
<td style="text-align: center;">32.8 / 31.4</td>
<td style="text-align: center;">38.7 / 35.8</td>
</tr>
<tr>
<td style="text-align: center;">Middle School History</td>
<td style="text-align: center;">84.5 / 86.5</td>
<td style="text-align: center;">62.8 / 63.8</td>
<td style="text-align: center;">48.3 / 52.7</td>
<td style="text-align: center;">58.5 / 52.2</td>
</tr>
<tr>
<td style="text-align: center;">Modern Chinese History</td>
<td style="text-align: center;">68.9 / 67.0</td>
<td style="text-align: center;">52.8 / 51.4</td>
<td style="text-align: center;">36.3 / 39.2</td>
<td style="text-align: center;">44.8 / 44.8</td>
</tr>
<tr>
<td style="text-align: center;">Professional Tour Guide</td>
<td style="text-align: center;">71.8 / 70.3</td>
<td style="text-align: center;">61.3 / 62.0</td>
<td style="text-align: center;">44.0 / 51.9</td>
<td style="text-align: center;">43.6 / 44.0</td>
</tr>
<tr>
<td style="text-align: center;">Accountant</td>
<td style="text-align: center;">64.6 / 61.9</td>
<td style="text-align: center;">51.7 / 42.0</td>
<td style="text-align: center;">23.9 / 32.7</td>
<td style="text-align: center;">47.2 / 41.3</td>
</tr>
<tr>
<td style="text-align: center;">Basic Medicine</td>
<td style="text-align: center;">80.6 / 78.3</td>
<td style="text-align: center;">61.1 / 60.6</td>
<td style="text-align: center;">31.4 / 33.7</td>
<td style="text-align: center;">49.7 / 45.1</td>
</tr>
<tr>
<td style="text-align: center;">Civil Servant</td>
<td style="text-align: center;">62.5 / 59.0</td>
<td style="text-align: center;">46.9 / 43.1</td>
<td style="text-align: center;">24.5 / 29.6</td>
<td style="text-align: center;">42.0 / 41.3</td>
</tr>
<tr>
<td style="text-align: center;">Clinical Medicine</td>
<td style="text-align: center;">76.0 / 73.5</td>
<td style="text-align: center;">55.0 / 53.0</td>
<td style="text-align: center;">34.5 / 32.0</td>
<td style="text-align: center;">36.5 / 37.5</td>
</tr>
<tr>
<td style="text-align: center;">Environmental Impact Assessment Engineer</td>
<td style="text-align: center;">63.7 / 59.8</td>
<td style="text-align: center;">50.9 / 49.1</td>
<td style="text-align: center;">37.7 / 35.6</td>
<td style="text-align: center;">47.3 / 47.3</td>
</tr>
<tr>
<td style="text-align: center;">Fire Engineer</td>
<td style="text-align: center;">50.0 / 49.6</td>
<td style="text-align: center;">42.6 / 37.9</td>
<td style="text-align: center;">28.4 / 32.3</td>
<td style="text-align: center;">36.5 / 36.2</td>
</tr>
<tr>
<td style="text-align: center;">Physician</td>
<td style="text-align: center;">76.8 / 76.1</td>
<td style="text-align: center;">63.7 / 54.6</td>
<td style="text-align: center;">34.8 / 37.7</td>
<td style="text-align: center;">50.6 / 46.5</td>
</tr>
<tr>
<td style="text-align: center;">Plant Protection</td>
<td style="text-align: center;">78.9 / 80.4</td>
<td style="text-align: center;">65.8 / 57.8</td>
<td style="text-align: center;">43.2 / 36.2</td>
<td style="text-align: center;">56.3 / 50.3</td>
</tr>
<tr>
<td style="text-align: center;">Sports Science</td>
<td style="text-align: center;">72.2 / 70.0</td>
<td style="text-align: center;">58.3 / 50.6</td>
<td style="text-align: center;">42.2 / 41.1</td>
<td style="text-align: center;">45.6 / 46.7</td>
</tr>
<tr>
<td style="text-align: center;">Tax Accountant</td>
<td style="text-align: center;">58.0 / 58.2</td>
<td style="text-align: center;">42.0 / 35.7</td>
<td style="text-align: center;">16.3 / 30.9</td>
<td style="text-align: center;">42.7 / 31.8</td>
</tr>
<tr>
<td style="text-align: center;">Urban and Rural Planner</td>
<td style="text-align: center;">63.2 / 65.6</td>
<td style="text-align: center;">52.2 / 49.3</td>
<td style="text-align: center;">36.8 / 37.3</td>
<td style="text-align: center;">45.5 / 42.3</td>
</tr>
</tbody>
</table>
<p>Table 10: Five-shot accuracy per subject. We report both the answer only (AO) and chain-of-thought (COT) accuracy in an AO / COT format.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Option</th>
<th style="text-align: center;">C-Eval</th>
<th style="text-align: center;">MMLU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">A</td>
<td style="text-align: center;">$22.9 \%$</td>
<td style="text-align: center;">$23.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">B</td>
<td style="text-align: center;">$26.0 \%$</td>
<td style="text-align: center;">$24.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">C</td>
<td style="text-align: center;">$26.4 \%$</td>
<td style="text-align: center;">$25.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">D</td>
<td style="text-align: center;">$24.7 \%$</td>
<td style="text-align: center;">$26.7 \%$</td>
</tr>
</tbody>
</table>
<p>Table 11: The distribution of the correct answer.
across 5 permutations and the variance of the overall accuracy. The results imply that the variance across different permutations is relatively small.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">STEM</th>
<th style="text-align: center;">Social Science</th>
<th style="text-align: center;">Humanities</th>
<th style="text-align: center;">Other</th>
<th style="text-align: center;">C-Eval Hard</th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;">Var</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM-6B</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">0.7</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM2-6B</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">0.2</td>
</tr>
</tbody>
</table>
<p>Table 12: Zero-shot average accuracy (\%) in answer-only setting. We report the average accuracy over 5 permutation within each category and overall accuracy. "Average" column indicates the average accuracy over 5 permutation. "Var" column indicates the variance of the overall accuracy.</p>
<h1>H Compute and Resources Used for Evaluation</h1>
<p>During our experiments to evaluate different LLMs on C-EVAL, we utilize a cluster with 8 A10080GB GPUs to run the inference for models with released weights, such resources are required due to deploying three large models - Bloomz-mt (176B), LLaMA-65B, and GLM-130B. In most cases the inference on C-EVAL is finished within one day. For models with API access, we just run the inference with CPUs which finishes mostly within one day as well.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ GLM-130B incorporates instruction tuning in the pretraining stage.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
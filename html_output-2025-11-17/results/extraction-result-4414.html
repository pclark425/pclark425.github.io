<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4414 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4414</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4414</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-274131682</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.10878v1.pdf" target="_blank">Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis</a></p>
                <p><strong>Paper Abstract:</strong> This study investigates the automation of metaanalysis in scientific documents using large language models (LLMs). Meta-analysis is a robust statistical method that synthesizes the findings of multiple studies (support articles) to provide a comprehensive understanding. We know that a metaarticle provides a structured analysis of several articles. However, conducting meta-analysis by hand is labor-intensive, time-consuming, and susceptible to human error, highlighting the need for automated pipelines to streamline the process. Our research introduces a novel approach that fine-tunes the LLM on extensive scientific datasets to address challenges in big data handling and structured data extraction. We automate and optimize the meta-analysis process by integrating Retrieval Augmented Generation (RAG). Tailored through prompt engineering and a new loss metric, Inverse Cosine Distance (ICD), designed for fine-tuning on large contextual datasets, LLMs efficiently generate structured meta-analysis content. Human evaluation then assesses relevance and provides information on model performance in key metrics. This research demonstrates that fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs generating 87.6% relevant meta-analysis abstracts. The relevance of the context, based on human evaluation, shows a reduction in irrelevancy from 4.56% to 1.9%. These experiments were conducted in a low-resource environment, highlighting the studyâ€™s contribution to enhancing the efficiency and reliability of meta-analysis automation.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4414.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4414.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD-RAG Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned LLM + RAG pipeline for automated meta-analysis (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end pipeline that fine-tunes low-context LLMs on a curated meta-analysis dataset (MAD) using an ICD loss, stores chunked support-article abstracts in a vector database, and performs Retrieval-Augmented Generation (RAG) with prompt engineering to produce meta-analysis abstracts summarizing multiple papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MAD-RAG Meta-analysis Generation System</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Components: (1) MAD dataset mapping meta-article abstracts to sets of support-article abstracts; (2) chunk-based preprocessing with overlapping chunks (LangChain Recursive TextSplitter, 200-token chunks capped at 2000 tokens); (3) supervised fine-tuning of low-context LLMs (Llama-2 7B, Mistral-v0.1 7B) using Quantized Low-Rank Adapters (QLoRA) and the proposed ICD loss; (4) storage of chunked samples into a vector database as embeddings; (5) semantic retrieval (embedding-based semantic search via LangChain) at inference to pull relevant chunks for a query; (6) instruction- and prompt-engineered conditional generation combining retrieved chunks and the query to output a structured meta-analysis abstract; (7) human evaluation and standard automatic metrics (BLEU, ROUGE, cosine similarity) for assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Llama-2 (7B); Mistral-v0.1 (7B). (Llama-2 13B mentioned in benchmarks; primary experiments on 7B models.)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Chunking of support-article abstracts into overlapping segments + embedding-based retrieval (semantic search) from a vector DB; supervised fine-tuning to learn mappings from chunked contexts to meta-abstract labels; prompt engineering to steer extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-Augmented Generation (RAG): retrieved chunks are provided with an instruction prompt so the LLM aggregates and synthesizes information across multiple documents into a single meta-analysis abstract; supervised multi-chunk training teaches the model how to aggregate quantitative/statistical details.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dataset contains 625 meta-articles and 6,344 support-article abstracts; training split used 400 meta-articles (expanded to 3,659 chunked training samples after chunking).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Primarily biomedical/medical meta-analyses (the MAD dataset was assembled from ScienceDirect), though dataset includes meta-analyses across scientific fields.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated meta-analysis abstracts (instruction-based, often including numerical/statistical summaries and heterogeneity indicators).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human relevance labels (Relevant / Somewhat-Relevant / Irrelevant via 3 independent evaluators and majority vote), BLEU, ROUGE variants (ROUGE-1, ROUGE-2, ROUGE-L), cosine similarity between generated and ground-truth abstracts, SWGT (similarity-with-ground-truth) measures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Fine-tuned models (with RAG) produced 87.6% 'Relevant' meta-analysis abstracts in human evaluation; irrelevancy reduced from 4.56% (baseline) to 1.9% after fine-tuning + RAG. Example generated abstracts showed similarity-with-ground-truth scores of 82.40% and 85.73% for provided examples; fine-tuned models also outperformed off-the-shelf pre-trained models on the CL-SciSumm scientific summarization benchmark (details in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Non-fine-tuned LLMs (Llama-2 7B, Mistral-v0.1 7B) and pre-trained LLM baselines on summarization benchmarks; in some comparisons Llama-2 13B and other pre-trained models were referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Fine-tuned + RAG models outperformed non-fine-tuned versions: higher 'Relevant' rates (up to 87.6%) and lower 'Irrelevant' rates (down to 1.9%). The paper reports improved BLEU/ROUGE and cosine-similarity alignment versus baselines; the ICD loss produced measurable improvements over standard loss during fine-tuning (qualitative and plotted quantitative gains in Fig. 4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Fine-tuning LLMs on a purpose-built large-context meta-analysis dataset enables models to learn aggregation patterns across many support studies; 2) ICD loss (semantic, cosine-based) improves alignment between generated abstracts and references beyond token-level losses; 3) integrating RAG compensates for context-length limits and yields more accurate, relevant syntheses; 4) prompt selection and generation temperature (0.7 found effective) substantially affect output relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Context-length limits of LLMs forced chunking (risk of information loss despite overlapping chunks); resource constraints required quantized training (QLoRA) which may limit fine-tuning capacity; only 50% of test sets could be fully evaluated due to compute limits; potential for retrieval of irrelevant or contradictory chunks remains; the paper does not fully characterize hallucination/factuality beyond human relevance labels.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors observe improved performance when fine-tuning on a large, domain-specific dataset and when using RAG to increase effective context, but experiments were constrained to 7B models and limited compute; Mistral-v0.1 (7B) outperformed Llama-2 in internal benchmarks, but no large-scale study of model-size scaling or how performance grows with larger numbers of papers was presented due to resource limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4414.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4414.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Cosine Distance (ICD) loss</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A custom loss function that uses inverse cosine similarity between the model output vector and the ground-truth vector (with a small epsilon for stability) to guide fine-tuning toward semantic / directional alignment rather than solely token-level matches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Inverse Cosine Distance (ICD) loss</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ICD computes 1 / (cosine_sim(y, y_hat) + epsilon) averaged across samples to penalize outputs that are directionally different from references; used inside a custom trainer to fine-tune LLMs so generated abstracts align semantically with ground-truth meta-abstract vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied during fine-tuning of Llama-2 (7B) and Mistral-v0.1 (7B) models in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Guides supervised generation by optimizing for semantic vector alignment between generated abstracts and target abstracts (indirectly improves information extraction by encouraging semantically correct content).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Encourages the model to produce synthesized outputs whose semantic embeddings align with the reference meta-abstract embedding, thus improving cross-document aggregation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied across the MAD dataset (625 meta-articles, 6,344 support abstracts) during fine-tuning; training conducted with 400 meta-articles in the train split (expanded to 3,659 chunked samples).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Meta-analysis abstract generation on biomedical and general scientific meta-analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Meta-analysis abstracts with improved semantic similarity to references.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Models fine-tuned with ICD were evaluated with BLEU, ROUGE, cosine similarity and human relevance labels; ICD's impact is visualized in model performance plots (Fig. 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>ICD-produced fine-tuned models showed significant performance gains compared to standard loss in the paper's Llama-2 FT and Mistral FT experiments (plotted improvement in Fig. 4(b)); exact numeric deltas are reported in paper figures/tables (qualitatively described as 'significant').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard token-level loss (implicitly cross-entropy / default trainer loss).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ICD outperformed the standard loss in aligning generated outputs to references and improved downstream evaluation metrics and human relevance judgments (as reported and plotted in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A cosine-similarity-based loss captures semantic directionality between generated and reference vectors and helps the model produce outputs better aligned with the reference meaning than standard token-level loss alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>ICD requires computing embeddings/vectors for outputs and references (added computational overhead) and was evaluated only in a constrained fine-tuning regime (2 epochs over 5 iterations); robustness across architectures and larger datasets remains to be validated.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Shown to help in the small-scale fine-tuning regime used; paper does not provide a detailed study of ICD behavior across larger models or progressively larger datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4414.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4414.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Analysis Dataset (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated dataset assembled by the authors that pairs meta-article abstracts with the abstracts of their support articles (625 meta-articles and 6,344 support-article abstracts) for supervised training of LLMs to generate meta-analysis abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MAD dataset (meta-analysis pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Dataset structure: each record contains a meta-article abstract y_j and the set S_j of abstracts from support articles; support abstracts were chunked into overlapping segments for model input. The authors manually extracted support-article abstracts from ScienceDirect meta-articles to assemble the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used to fine-tune Llama-2 (7B) and Mistral-v0.1 (7B) models in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Provides supervised training pairs (chunked context -> meta-abstract) enabling models to learn extraction and aggregation patterns; designed to be compatible with chunking and RAG retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Enables supervised learning of synthesis (mapping multiple support abstracts to a single meta-abstract); during inference the same chunking + retrieval pipeline is used to synthesize across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>625 meta-articles and 6,344 support-article abstracts total; training split used 400 meta-articles (expanded to 3,659 chunked samples).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Primarily medical/biomedical meta-analyses (but includes other scientific fields represented in meta-analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Training supervision for meta-analysis abstract generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Dataset used to evaluate fine-tuned models via BLEU, ROUGE, cosine similarity and human relevance labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Using MAD for fine-tuning led to large gains in human relevance (87.6% relevant) and improved benchmark performance on CL-SciSumm relative to unfine-tuned models (detailed numbers in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with models not fine-tuned on MAD (pre-trained LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Models fine-tuned on MAD outperformed non-fine-tuned LLMs in relevance, BLEU/ROUGE, and cosine-similarity measures in experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A purpose-built dataset that pairs meta-articles with their supports is effective for teaching LLMs how to synthesize multiple studies into structured meta-analysis abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Manual assembly is labor-intensive and dataset currently limited in size and domain coverage; chunking required due to context limits may complicate learning of long-range cross-document dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Dataset enables expansion via chunking (increasing training samples), but paper does not test orders-of-magnitude larger datasets or cross-domain scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4414.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4414.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chunked-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chunking + Vector DB + Retrieval-Augmented Generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that splits support-article abstracts into overlapping chunks, indexes their embeddings in a vector database, and uses semantic search to retrieve relevant chunks for conditioning LLM generation (RAG) to synthesize across many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chunked-RAG retrieval pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses LangChain's Recursive TextSplitter to create overlapping chunks (200-token default, capped at 2000 tokens), encodes chunks into embeddings, stores them in a vector DB, performs semantic retrieval at inference for a given meta-analysis query, and provides retrieved chunks plus prompt to the fine-tuned LLM to generate a synthesized meta-abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Paired with fine-tuned Llama-2 (7B) and Mistral-v0.1 (7B) models in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval (semantic search) from a vector database; chunk overlap preserves continuity and reduces information loss; retrieval returns the most semantically relevant chunks for aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-conditioned aggregation of retrieved chunks (multi-chunk synthesis) using instruction prompts to produce cohesive meta-analysis abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over the MAD dataset (6,344 support-article abstracts chunked into 3,659+ samples); in principle scales to arbitrary numbers of chunked documents stored in the vector DB.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical meta-analysis and general scientific meta-analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Meta-analysis abstracts generated by conditioning LLMs on retrieved chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human relevance, BLEU, ROUGE, cosine similarity between generated and ground-truth abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Integration of chunked retrieval via RAG yielded higher relevance and lower irrelevancy in generated outputs; authors attribute part of the 87.6% relevance to RAG-enabled retrieval though exact ablation numeric breakdown is qualitative in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Fine-tuned models without RAG and non-fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>RAG combined with fine-tuning reduced irrelevant content generation and improved alignment with ground truth compared to no-RAG conditions; precise per-condition numbers are provided in paper tables/figures.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG addresses context-length restrictions and improves cross-chunk aggregation; overlapping chunk strategy helps maintain coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality depends on embedding & retrieval; potential retrieval of irrelevant or contradictory chunks; chunking settings (size/overlap) influence performance; compute and storage overhead for vector DB.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Design is intended to scale to much larger corpora by retrieving only relevant chunks, but empirical scaling tests beyond the MAD scale were not reported due to resource limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4414.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4414.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QLoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quantized Low-Rank Adapters (QLoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An efficient fine-tuning technique using quantization plus low-rank adapter modules to enable fine-tuning of large LLMs under tight memory constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qlora: Efficient finetuning of quantized llms.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>QLoRA fine-tuning configuration</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>QLoRA quantizes the base model weights and injects low-rank adapter layers to allow parameter-efficient fine-tuning; used here to load and fine-tune 7B models on constrained GPU hardware (NVIDIA T4 x2).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied to Llama-2 (7B) and Mistral-v0.1 (7B) during fine-tuning in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not an extraction technique per se; enables efficient fine-tuning so that the LLM can better extract and synthesize information during supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Facilitates parameter-efficient adaptation so the model learns synthesis behaviors while fitting within limited hardware resources.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used during fine-tuning on the MAD dataset (625 meta-articles / 6,344 support abstracts).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General LLM fine-tuning for scientific meta-analysis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Used to produce fine-tuned LLM checkpoints capable of generating meta-analysis abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream model performance measured with BLEU/ROUGE/human relevance; QLoRA is the enabling training configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Allowed successful fine-tuning on available hardware; authors note quantized configuration limited maximal fine-tuning potential but still enabled meaningful performance gains (specifics in implementation details).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Full-precision fine-tuning (not performed due to resource limits).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not directly compared in paper; QLoRA enabled fine-tuning that produced better outputs than non-fine-tuned models, but authors caution quantization limited optimal parameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>QLoRA makes fine-tuning feasible on limited hardware but may impose some limits on ultimate model performance versus full-precision training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quantization may limit the capacity to fully optimize model parameters; authors cite this as a constraint on further improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Enables experimentation at modest hardware scale; impact on scaling to much larger models was not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4414.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4414.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paraphraser-chunk-merge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paraphraser-based chunk-merging summarization (mentioned prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach (mentioned in related work) that generates multiple summary chunks from LLMs and combines them with a paraphraser to produce a single consolidated summary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Paraphraser-based chunk-merging approach (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Process: segment long documents into fragments, summarize each fragment with an LLM, then use a paraphraser or another model to merge chunk summaries into a coherent final summary. Mentioned as related work (used for shorter narrative summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Mentioned in context of prior LLM summarization work (e.g., Llama-2-sized models in related citations), exact models vary by cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Chunk-based summarization and paraphrase merging (per-chunk LLM summarization, then paraphraser consolidation).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Merge paraphrased chunk summaries into a single narrative summary; not specialized for structured meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied in prior summarization studies on story/document summarization (contextual references in related work), not specifically on multi-paper meta-analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Large-context summarization (stories, long documents); cited as less suitable for structured meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Concise merged summaries from multiple fragment summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Typically BLEU/ROUGE and human evaluation in the cited summarization works (as referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Prior works showed competence in narrative summarization but were noted in this paper as insufficient for structured meta-analysis which requires specialized data extraction and aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard summarization approaches; contrasted by authors as insufficient for meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not directly evaluated in this paper; mentioned as an approach that has limitations when applied to meta-analysis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chunk-and-merge paraphraser strategies work for narrative summarization but may not capture structured quantitative synthesis required for meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Merging paraphrased chunks can lose structured statistical relationships and cross-study aggregation semantics; not tailored for extracting effect sizes or combining heterogeneous study statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Scales by number of chunks but suffers from difficulty aggregating structured, cross-document quantitative information as chunk counts grow.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4414.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4414.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reason2024 (AI network meta-analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2024 study (cited in this paper) that explores using large language models to automate network meta-analyses across multiple case studies; mentioned as related work demonstrating AI application to meta-analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-driven network meta-analysis case studies (Reason et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited work evaluates the potential application of LLMs to automate steps of network meta-analysis across four case studies; referenced as complementary evidence that LLMs can be applied to meta-analytic automation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Presumably uses a mix of retrieval, LLM-based extraction, and automated pipeline steps (paper cited as related work; exact techniques are in the cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Network meta-analysis automation across multiple studies (details in cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Four case studies assessed in the cited work (exact per-case paper counts reported in that study).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Pharmacoeconomics / medical network meta-analyses (per citation context).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated network meta-analyses (syntheses across multiple interventions/studies).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper; see cited study for details.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Mentioned as demonstrating potential for AI in automating network meta-analyses; specifics are in the cited article.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified here (refer to cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as supporting evidence that LLMs can contribute to automating meta-analytic tasks; motivates the present study's focus on end-to-end meta-analysis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed here; refer to the cited paper for case-specific limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Cited paper likely discusses case-study scale; not summarized here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrievalaugmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Qlora: Efficient finetuning of quantized llms. <em>(Rating: 2)</em></li>
                <li>Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models <em>(Rating: 2)</em></li>
                <li>Retrieval augmentation generation and representative vector summarization for large unstructured textual data in medical education <em>(Rating: 1)</em></li>
                <li>Reading subtext: Evaluating large language models on short story summarization with writers <em>(Rating: 1)</em></li>
                <li>Improving multi-stage long document summarization with enhanced coarse summarizer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4414",
    "paper_id": "paper-274131682",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "MAD-RAG Pipeline",
            "name_full": "Fine-tuned LLM + RAG pipeline for automated meta-analysis (this paper)",
            "brief_description": "An end-to-end pipeline that fine-tunes low-context LLMs on a curated meta-analysis dataset (MAD) using an ICD loss, stores chunked support-article abstracts in a vector database, and performs Retrieval-Augmented Generation (RAG) with prompt engineering to produce meta-analysis abstracts summarizing multiple papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MAD-RAG Meta-analysis Generation System",
            "system_description": "Components: (1) MAD dataset mapping meta-article abstracts to sets of support-article abstracts; (2) chunk-based preprocessing with overlapping chunks (LangChain Recursive TextSplitter, 200-token chunks capped at 2000 tokens); (3) supervised fine-tuning of low-context LLMs (Llama-2 7B, Mistral-v0.1 7B) using Quantized Low-Rank Adapters (QLoRA) and the proposed ICD loss; (4) storage of chunked samples into a vector database as embeddings; (5) semantic retrieval (embedding-based semantic search via LangChain) at inference to pull relevant chunks for a query; (6) instruction- and prompt-engineered conditional generation combining retrieved chunks and the query to output a structured meta-analysis abstract; (7) human evaluation and standard automatic metrics (BLEU, ROUGE, cosine similarity) for assessment.",
            "llm_model_used": "Llama-2 (7B); Mistral-v0.1 (7B). (Llama-2 13B mentioned in benchmarks; primary experiments on 7B models.)",
            "extraction_technique": "Chunking of support-article abstracts into overlapping segments + embedding-based retrieval (semantic search) from a vector DB; supervised fine-tuning to learn mappings from chunked contexts to meta-abstract labels; prompt engineering to steer extraction.",
            "synthesis_technique": "Retrieval-Augmented Generation (RAG): retrieved chunks are provided with an instruction prompt so the LLM aggregates and synthesizes information across multiple documents into a single meta-analysis abstract; supervised multi-chunk training teaches the model how to aggregate quantitative/statistical details.",
            "number_of_papers": "Dataset contains 625 meta-articles and 6,344 support-article abstracts; training split used 400 meta-articles (expanded to 3,659 chunked training samples after chunking).",
            "domain_or_topic": "Primarily biomedical/medical meta-analyses (the MAD dataset was assembled from ScienceDirect), though dataset includes meta-analyses across scientific fields.",
            "output_type": "Generated meta-analysis abstracts (instruction-based, often including numerical/statistical summaries and heterogeneity indicators).",
            "evaluation_metrics": "Human relevance labels (Relevant / Somewhat-Relevant / Irrelevant via 3 independent evaluators and majority vote), BLEU, ROUGE variants (ROUGE-1, ROUGE-2, ROUGE-L), cosine similarity between generated and ground-truth abstracts, SWGT (similarity-with-ground-truth) measures.",
            "performance_results": "Fine-tuned models (with RAG) produced 87.6% 'Relevant' meta-analysis abstracts in human evaluation; irrelevancy reduced from 4.56% (baseline) to 1.9% after fine-tuning + RAG. Example generated abstracts showed similarity-with-ground-truth scores of 82.40% and 85.73% for provided examples; fine-tuned models also outperformed off-the-shelf pre-trained models on the CL-SciSumm scientific summarization benchmark (details in paper tables).",
            "comparison_baseline": "Non-fine-tuned LLMs (Llama-2 7B, Mistral-v0.1 7B) and pre-trained LLM baselines on summarization benchmarks; in some comparisons Llama-2 13B and other pre-trained models were referenced.",
            "performance_vs_baseline": "Fine-tuned + RAG models outperformed non-fine-tuned versions: higher 'Relevant' rates (up to 87.6%) and lower 'Irrelevant' rates (down to 1.9%). The paper reports improved BLEU/ROUGE and cosine-similarity alignment versus baselines; the ICD loss produced measurable improvements over standard loss during fine-tuning (qualitative and plotted quantitative gains in Fig. 4).",
            "key_findings": "1) Fine-tuning LLMs on a purpose-built large-context meta-analysis dataset enables models to learn aggregation patterns across many support studies; 2) ICD loss (semantic, cosine-based) improves alignment between generated abstracts and references beyond token-level losses; 3) integrating RAG compensates for context-length limits and yields more accurate, relevant syntheses; 4) prompt selection and generation temperature (0.7 found effective) substantially affect output relevance.",
            "limitations_challenges": "Context-length limits of LLMs forced chunking (risk of information loss despite overlapping chunks); resource constraints required quantized training (QLoRA) which may limit fine-tuning capacity; only 50% of test sets could be fully evaluated due to compute limits; potential for retrieval of irrelevant or contradictory chunks remains; the paper does not fully characterize hallucination/factuality beyond human relevance labels.",
            "scaling_behavior": "Authors observe improved performance when fine-tuning on a large, domain-specific dataset and when using RAG to increase effective context, but experiments were constrained to 7B models and limited compute; Mistral-v0.1 (7B) outperformed Llama-2 in internal benchmarks, but no large-scale study of model-size scaling or how performance grows with larger numbers of papers was presented due to resource limits.",
            "uuid": "e4414.0",
            "source_info": {
                "paper_title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ICD",
            "name_full": "Inverse Cosine Distance (ICD) loss",
            "brief_description": "A custom loss function that uses inverse cosine similarity between the model output vector and the ground-truth vector (with a small epsilon for stability) to guide fine-tuning toward semantic / directional alignment rather than solely token-level matches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Inverse Cosine Distance (ICD) loss",
            "system_description": "ICD computes 1 / (cosine_sim(y, y_hat) + epsilon) averaged across samples to penalize outputs that are directionally different from references; used inside a custom trainer to fine-tune LLMs so generated abstracts align semantically with ground-truth meta-abstract vectors.",
            "llm_model_used": "Applied during fine-tuning of Llama-2 (7B) and Mistral-v0.1 (7B) models in this study.",
            "extraction_technique": "Guides supervised generation by optimizing for semantic vector alignment between generated abstracts and target abstracts (indirectly improves information extraction by encouraging semantically correct content).",
            "synthesis_technique": "Encourages the model to produce synthesized outputs whose semantic embeddings align with the reference meta-abstract embedding, thus improving cross-document aggregation quality.",
            "number_of_papers": "Applied across the MAD dataset (625 meta-articles, 6,344 support abstracts) during fine-tuning; training conducted with 400 meta-articles in the train split (expanded to 3,659 chunked samples).",
            "domain_or_topic": "Meta-analysis abstract generation on biomedical and general scientific meta-analyses.",
            "output_type": "Meta-analysis abstracts with improved semantic similarity to references.",
            "evaluation_metrics": "Models fine-tuned with ICD were evaluated with BLEU, ROUGE, cosine similarity and human relevance labels; ICD's impact is visualized in model performance plots (Fig. 4).",
            "performance_results": "ICD-produced fine-tuned models showed significant performance gains compared to standard loss in the paper's Llama-2 FT and Mistral FT experiments (plotted improvement in Fig. 4(b)); exact numeric deltas are reported in paper figures/tables (qualitatively described as 'significant').",
            "comparison_baseline": "Standard token-level loss (implicitly cross-entropy / default trainer loss).",
            "performance_vs_baseline": "ICD outperformed the standard loss in aligning generated outputs to references and improved downstream evaluation metrics and human relevance judgments (as reported and plotted in the paper).",
            "key_findings": "A cosine-similarity-based loss captures semantic directionality between generated and reference vectors and helps the model produce outputs better aligned with the reference meaning than standard token-level loss alone.",
            "limitations_challenges": "ICD requires computing embeddings/vectors for outputs and references (added computational overhead) and was evaluated only in a constrained fine-tuning regime (2 epochs over 5 iterations); robustness across architectures and larger datasets remains to be validated.",
            "scaling_behavior": "Shown to help in the small-scale fine-tuning regime used; paper does not provide a detailed study of ICD behavior across larger models or progressively larger datasets.",
            "uuid": "e4414.1",
            "source_info": {
                "paper_title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MAD",
            "name_full": "Meta-Analysis Dataset (MAD)",
            "brief_description": "A curated dataset assembled by the authors that pairs meta-article abstracts with the abstracts of their support articles (625 meta-articles and 6,344 support-article abstracts) for supervised training of LLMs to generate meta-analysis abstracts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MAD dataset (meta-analysis pairs)",
            "system_description": "Dataset structure: each record contains a meta-article abstract y_j and the set S_j of abstracts from support articles; support abstracts were chunked into overlapping segments for model input. The authors manually extracted support-article abstracts from ScienceDirect meta-articles to assemble the dataset.",
            "llm_model_used": "Used to fine-tune Llama-2 (7B) and Mistral-v0.1 (7B) models in experiments.",
            "extraction_technique": "Provides supervised training pairs (chunked context -&gt; meta-abstract) enabling models to learn extraction and aggregation patterns; designed to be compatible with chunking and RAG retrieval.",
            "synthesis_technique": "Enables supervised learning of synthesis (mapping multiple support abstracts to a single meta-abstract); during inference the same chunking + retrieval pipeline is used to synthesize across papers.",
            "number_of_papers": "625 meta-articles and 6,344 support-article abstracts total; training split used 400 meta-articles (expanded to 3,659 chunked samples).",
            "domain_or_topic": "Primarily medical/biomedical meta-analyses (but includes other scientific fields represented in meta-analyses).",
            "output_type": "Training supervision for meta-analysis abstract generation.",
            "evaluation_metrics": "Dataset used to evaluate fine-tuned models via BLEU, ROUGE, cosine similarity and human relevance labeling.",
            "performance_results": "Using MAD for fine-tuning led to large gains in human relevance (87.6% relevant) and improved benchmark performance on CL-SciSumm relative to unfine-tuned models (detailed numbers in paper tables).",
            "comparison_baseline": "Compared with models not fine-tuned on MAD (pre-trained LLMs).",
            "performance_vs_baseline": "Models fine-tuned on MAD outperformed non-fine-tuned LLMs in relevance, BLEU/ROUGE, and cosine-similarity measures in experiments reported.",
            "key_findings": "A purpose-built dataset that pairs meta-articles with their supports is effective for teaching LLMs how to synthesize multiple studies into structured meta-analysis abstracts.",
            "limitations_challenges": "Manual assembly is labor-intensive and dataset currently limited in size and domain coverage; chunking required due to context limits may complicate learning of long-range cross-document dependencies.",
            "scaling_behavior": "Dataset enables expansion via chunking (increasing training samples), but paper does not test orders-of-magnitude larger datasets or cross-domain scaling.",
            "uuid": "e4414.2",
            "source_info": {
                "paper_title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Chunked-RAG",
            "name_full": "Chunking + Vector DB + Retrieval-Augmented Generation pipeline",
            "brief_description": "An approach that splits support-article abstracts into overlapping chunks, indexes their embeddings in a vector database, and uses semantic search to retrieve relevant chunks for conditioning LLM generation (RAG) to synthesize across many papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Chunked-RAG retrieval pipeline",
            "system_description": "Uses LangChain's Recursive TextSplitter to create overlapping chunks (200-token default, capped at 2000 tokens), encodes chunks into embeddings, stores them in a vector DB, performs semantic retrieval at inference for a given meta-analysis query, and provides retrieved chunks plus prompt to the fine-tuned LLM to generate a synthesized meta-abstract.",
            "llm_model_used": "Paired with fine-tuned Llama-2 (7B) and Mistral-v0.1 (7B) models in this study.",
            "extraction_technique": "Embedding-based retrieval (semantic search) from a vector database; chunk overlap preserves continuity and reduces information loss; retrieval returns the most semantically relevant chunks for aggregation.",
            "synthesis_technique": "LLM-conditioned aggregation of retrieved chunks (multi-chunk synthesis) using instruction prompts to produce cohesive meta-analysis abstracts.",
            "number_of_papers": "Operates over the MAD dataset (6,344 support-article abstracts chunked into 3,659+ samples); in principle scales to arbitrary numbers of chunked documents stored in the vector DB.",
            "domain_or_topic": "Biomedical meta-analysis and general scientific meta-analyses.",
            "output_type": "Meta-analysis abstracts generated by conditioning LLMs on retrieved chunks.",
            "evaluation_metrics": "Human relevance, BLEU, ROUGE, cosine similarity between generated and ground-truth abstracts.",
            "performance_results": "Integration of chunked retrieval via RAG yielded higher relevance and lower irrelevancy in generated outputs; authors attribute part of the 87.6% relevance to RAG-enabled retrieval though exact ablation numeric breakdown is qualitative in the paper.",
            "comparison_baseline": "Fine-tuned models without RAG and non-fine-tuned models.",
            "performance_vs_baseline": "RAG combined with fine-tuning reduced irrelevant content generation and improved alignment with ground truth compared to no-RAG conditions; precise per-condition numbers are provided in paper tables/figures.",
            "key_findings": "RAG addresses context-length restrictions and improves cross-chunk aggregation; overlapping chunk strategy helps maintain coherence.",
            "limitations_challenges": "Quality depends on embedding & retrieval; potential retrieval of irrelevant or contradictory chunks; chunking settings (size/overlap) influence performance; compute and storage overhead for vector DB.",
            "scaling_behavior": "Design is intended to scale to much larger corpora by retrieving only relevant chunks, but empirical scaling tests beyond the MAD scale were not reported due to resource limits.",
            "uuid": "e4414.3",
            "source_info": {
                "paper_title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "QLoRA",
            "name_full": "Quantized Low-Rank Adapters (QLoRA)",
            "brief_description": "An efficient fine-tuning technique using quantization plus low-rank adapter modules to enable fine-tuning of large LLMs under tight memory constraints.",
            "citation_title": "Qlora: Efficient finetuning of quantized llms.",
            "mention_or_use": "use",
            "system_name": "QLoRA fine-tuning configuration",
            "system_description": "QLoRA quantizes the base model weights and injects low-rank adapter layers to allow parameter-efficient fine-tuning; used here to load and fine-tune 7B models on constrained GPU hardware (NVIDIA T4 x2).",
            "llm_model_used": "Applied to Llama-2 (7B) and Mistral-v0.1 (7B) during fine-tuning in this work.",
            "extraction_technique": "Not an extraction technique per se; enables efficient fine-tuning so that the LLM can better extract and synthesize information during supervised training.",
            "synthesis_technique": "Facilitates parameter-efficient adaptation so the model learns synthesis behaviors while fitting within limited hardware resources.",
            "number_of_papers": "Used during fine-tuning on the MAD dataset (625 meta-articles / 6,344 support abstracts).",
            "domain_or_topic": "General LLM fine-tuning for scientific meta-analysis tasks.",
            "output_type": "Used to produce fine-tuned LLM checkpoints capable of generating meta-analysis abstracts.",
            "evaluation_metrics": "Downstream model performance measured with BLEU/ROUGE/human relevance; QLoRA is the enabling training configuration.",
            "performance_results": "Allowed successful fine-tuning on available hardware; authors note quantized configuration limited maximal fine-tuning potential but still enabled meaningful performance gains (specifics in implementation details).",
            "comparison_baseline": "Full-precision fine-tuning (not performed due to resource limits).",
            "performance_vs_baseline": "Not directly compared in paper; QLoRA enabled fine-tuning that produced better outputs than non-fine-tuned models, but authors caution quantization limited optimal parameter tuning.",
            "key_findings": "QLoRA makes fine-tuning feasible on limited hardware but may impose some limits on ultimate model performance versus full-precision training.",
            "limitations_challenges": "Quantization may limit the capacity to fully optimize model parameters; authors cite this as a constraint on further improvements.",
            "scaling_behavior": "Enables experimentation at modest hardware scale; impact on scaling to much larger models was not evaluated here.",
            "uuid": "e4414.4",
            "source_info": {
                "paper_title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Paraphraser-chunk-merge",
            "name_full": "Paraphraser-based chunk-merging summarization (mentioned prior work)",
            "brief_description": "A prior approach (mentioned in related work) that generates multiple summary chunks from LLMs and combines them with a paraphraser to produce a single consolidated summary.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Paraphraser-based chunk-merging approach (prior work)",
            "system_description": "Process: segment long documents into fragments, summarize each fragment with an LLM, then use a paraphraser or another model to merge chunk summaries into a coherent final summary. Mentioned as related work (used for shorter narrative summarization).",
            "llm_model_used": "Mentioned in context of prior LLM summarization work (e.g., Llama-2-sized models in related citations), exact models vary by cited work.",
            "extraction_technique": "Chunk-based summarization and paraphrase merging (per-chunk LLM summarization, then paraphraser consolidation).",
            "synthesis_technique": "Merge paraphrased chunk summaries into a single narrative summary; not specialized for structured meta-analysis.",
            "number_of_papers": "Applied in prior summarization studies on story/document summarization (contextual references in related work), not specifically on multi-paper meta-analysis in this paper.",
            "domain_or_topic": "Large-context summarization (stories, long documents); cited as less suitable for structured meta-analysis.",
            "output_type": "Concise merged summaries from multiple fragment summaries.",
            "evaluation_metrics": "Typically BLEU/ROUGE and human evaluation in the cited summarization works (as referenced).",
            "performance_results": "Prior works showed competence in narrative summarization but were noted in this paper as insufficient for structured meta-analysis which requires specialized data extraction and aggregation.",
            "comparison_baseline": "Standard summarization approaches; contrasted by authors as insufficient for meta-analysis.",
            "performance_vs_baseline": "Not directly evaluated in this paper; mentioned as an approach that has limitations when applied to meta-analysis tasks.",
            "key_findings": "Chunk-and-merge paraphraser strategies work for narrative summarization but may not capture structured quantitative synthesis required for meta-analysis.",
            "limitations_challenges": "Merging paraphrased chunks can lose structured statistical relationships and cross-study aggregation semantics; not tailored for extracting effect sizes or combining heterogeneous study statistics.",
            "scaling_behavior": "Scales by number of chunks but suffers from difficulty aggregating structured, cross-document quantitative information as chunk counts grow.",
            "uuid": "e4414.5",
            "source_info": {
                "paper_title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Reason2024 (AI network meta-analyses)",
            "name_full": "Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models",
            "brief_description": "A 2024 study (cited in this paper) that explores using large language models to automate network meta-analyses across multiple case studies; mentioned as related work demonstrating AI application to meta-analyses.",
            "citation_title": "Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models",
            "mention_or_use": "mention",
            "system_name": "AI-driven network meta-analysis case studies (Reason et al. 2024)",
            "system_description": "Cited work evaluates the potential application of LLMs to automate steps of network meta-analysis across four case studies; referenced as complementary evidence that LLMs can be applied to meta-analytic automation.",
            "llm_model_used": null,
            "extraction_technique": "Presumably uses a mix of retrieval, LLM-based extraction, and automated pipeline steps (paper cited as related work; exact techniques are in the cited paper).",
            "synthesis_technique": "Network meta-analysis automation across multiple studies (details in cited paper).",
            "number_of_papers": "Four case studies assessed in the cited work (exact per-case paper counts reported in that study).",
            "domain_or_topic": "Pharmacoeconomics / medical network meta-analyses (per citation context).",
            "output_type": "Automated network meta-analyses (syntheses across multiple interventions/studies).",
            "evaluation_metrics": "Not specified in this paper; see cited study for details.",
            "performance_results": "Mentioned as demonstrating potential for AI in automating network meta-analyses; specifics are in the cited article.",
            "comparison_baseline": "Not specified here (refer to cited paper).",
            "performance_vs_baseline": null,
            "key_findings": "Cited as supporting evidence that LLMs can contribute to automating meta-analytic tasks; motivates the present study's focus on end-to-end meta-analysis generation.",
            "limitations_challenges": "Not detailed here; refer to the cited paper for case-specific limitations.",
            "scaling_behavior": "Cited paper likely discusses case-study scale; not summarized here.",
            "uuid": "e4414.6",
            "source_info": {
                "paper_title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrievalaugmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Qlora: Efficient finetuning of quantized llms.",
            "rating": 2,
            "sanitized_title": "qlora_efficient_finetuning_of_quantized_llms"
        },
        {
            "paper_title": "Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_to_automate_network_metaanalyses_four_case_studies_to_evaluate_the_potential_application_of_large_language_models"
        },
        {
            "paper_title": "Retrieval augmentation generation and representative vector summarization for large unstructured textual data in medical education",
            "rating": 1,
            "sanitized_title": "retrieval_augmentation_generation_and_representative_vector_summarization_for_large_unstructured_textual_data_in_medical_education"
        },
        {
            "paper_title": "Reading subtext: Evaluating large language models on short story summarization with writers",
            "rating": 1,
            "sanitized_title": "reading_subtext_evaluating_large_language_models_on_short_story_summarization_with_writers"
        },
        {
            "paper_title": "Improving multi-stage long document summarization with enhanced coarse summarizer",
            "rating": 1,
            "sanitized_title": "improving_multistage_long_document_summarization_with_enhanced_coarse_summarizer"
        }
    ],
    "cost": 0.021488499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis
16 Nov 2024</p>
<p>Jawad Ibn Ahad 
Rafeed Mohammad Sultan rafeed.sultan@northsouth.edu 
Abraham Kaikobad abraham.kaikobad@northsouth.edu 
Fuad Rahman 
Mohammad Ruhul Amin 
Nabeel Mohammed nabeel.mohammed@northsouth.edu 
Shafin Rahman shafin.rahman@northsouth.edu 
Chunked Dataset </p>
<p>Apurba-NSU R&amp;D Lab
ECE North South University Dhaka
Bangladesh</p>
<p>Apurba-NSU R&amp;D Lab
ECE North South University Dhaka
Bangladesh</p>
<p>Apurba-NSU R&amp;D Lab
ECE North South University Dhaka
Bangladesh</p>
<p>Apurba Technologies Sunnyvale
94085CAUSA</p>
<p>Computer and Information Science
Fordham University New York
USA</p>
<p>Apurba-NSU R&amp;D Lab
ECE North South University Dhaka
Bangladesh</p>
<p>Apurba-NSU R&amp;D Lab
ECE North South University Dhaka
Bangladesh</p>
<p>Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis
16 Nov 2024517D759A8A86C9E9E97ACF223CFB3496arXiv:2411.10878v1[cs.CL]Meta-analysisLarge contextual dataHuman evaluationPrompt engineeringLarge Language Model
This study investigates the automation of metaanalysis in scientific documents using large language models (LLMs).Meta-analysis is a robust statistical method that synthesizes the findings of multiple studies (support articles) to provide a comprehensive understanding.We know that a metaarticle provides a structured analysis of several articles.However, conducting meta-analysis by hand is labor-intensive, timeconsuming, and susceptible to human error, highlighting the need for automated pipelines to streamline the process.Our research introduces a novel approach that fine-tunes the LLM on extensive scientific datasets to address challenges in big data handling and structured data extraction.We automate and optimize the metaanalysis process by integrating Retrieval Augmented Generation (RAG).Tailored through prompt engineering and a new loss metric, Inverse Cosine Distance (ICD), designed for fine-tuning on large contextual datasets, LLMs efficiently generate structured meta-analysis content.Human evaluation then assesses relevance and provides information on model performance in key metrics.This research demonstrates that fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs generating 87.6% relevant meta-analysis abstracts.The relevance of the context, based on human evaluation, shows a reduction in irrelevancy from 4.56% to 1.9%.These experiments were conducted in a low-resource environment, highlighting the study's contribution to enhancing the efficiency and reliability of meta-analysis automation.</p>
<p>I. INTRODUCTION</p>
<p>Meta-analysis is a powerful statistical approach that combines the findings of multiple studies to provide a comprehensive understanding of the same research topic [1].A metaanalysis paper, or meta article, offers a structured analysis of numerous individual support articles.Individual studies often face limitations, such as small sample sizes or narrow focus, making it hard to draw definitive conclusions [2].Meta-analysis aggregates data from different studies, providing robust estimates that inform research decisions, guide treatments, and influence healthcare policies [3]- [6].In applied scientific fields, meta-analyses play a crucial role: consolidating the results of clinical trials [7], [8], evaluating public health strategies, material performance, and farming practices [9]- [11], and assessing the impacts, behavior, policies, and teaching methods of climate in fields such as environmental science, psychology, economics, and education [12]- [15].Meta-analysis involves the analysis of extensive datasets, as it incorporates numerous studies, which presents a significant challenge to big data.The process is often laborintensive, requiring manual extraction and analysis of data from multiple research articles, which is both time-consuming and susceptible to human error.This underscores the critical need for an automated pipeline to streamline and improve the efficiency of meta-analysis generation.The advancements in large language models (LLMs) offer promising potential, suggesting that these models could be utilized to manage the vast data requirements of scientific meta-analysis.</p>
<p>Traditional meta-analysis faces challenges with big data scalability.(a) Manual Meta-analysis: Involves manual data extraction and analysis, limiting scalability in rapidly expanding domains [20].While AI advances automate tasks like information retrieval and summarization, focusing primarily on shorter summaries, LLMs show strong potential in summarization but still face limitations.(b) LLMs as summarizers: Although LLMs like Llama2-13b have demonstrated proficiency in generating summaries and performing question-answering tasks [16], [17], [21], their utility in synthesizing extensive research findings for meta-analysis remains constrained.Current approaches in this domain predominantly focus on generating condensed summaries for shorter, narrative-based content rather than synthesizing large-scale scientific data.To address this gap, researchers have explored RAG techniques.(c) Retrieval Augmented Generation: By integrating retrieval-based mechanisms, RAG enables LLMs to access and summarize large datasets through document retrieval [18], [19], [22].However, this method falls short when applied Fig. 1: (a) Paraphraser-based approach that combines multiple generated summary chunks from LLMs has been used by [16], [17], (b) Retrieval augmentation generation-based approach has been applied in [18], [19] using a vector database to store chunked data and cluster them before passing to LLM to produce a summary.Existing methods often fall short of handling big scientific contextual data and generating structured synthesis.(c) We propose a novel approach involving fine-tuning LLMs with large contexts and utilizing them to generate meta-analysis abstracts.Abstracts from support papers serve as input, with meta-papers' abstracts as labels.Pre-processing involves chunking the dataset due to context length restrictions and prioritizing small LLMs over resource-intensive large LLMs.The fine-tuned model generates meta-analysis abstracts via semantic search with the provided context and query.to meta-analysis, which demands specialized data extraction techniques and a deeper synthesis of scientific contexts [23].LLMs' current limitations highlight a need for targeted finetuning and tailored approaches to handle complex, structured large-scale scientific data.</p>
<p>To bridge this gap, our research introduces a novel approach that leverages LLMs with RAG to automate and streamline the meta-analysis process.We have built a comprehensive dataset with various meta-analysis scenarios in various scientific fields, which contains the content of the meta-articles along with the content of the support papers.This dataset facilitates both training and evaluation to stimulate further research.Its purpose is to fine-tune LLMs, enabling them to understand and replicate data extraction patterns for metaanalysis.We introduce a novel loss function, Inverse Cosine Distance (ICD), specifically designed for training LLMs in large-context scenarios to handle large-data challenges.This function enhances the performance of LLMs in generating meta-analysis with high relevance and accuracy.By fine-tuning LLMs for large-context tasks and employing specific prompt engineering techniques, shown in Fig. 1, we aim to overcome the limitations of existing methods of handling big contextual data.By integrating RAG with our fine-tuning strategy, LLMs generate precise, instruction-based meta-analysis content, ensuring quality and efficiency.Our approach reduces the laborintensive aspects of meta-analysis, enabling LLMs to handle large contexts and generate structured abstracts effectively.This work holds significant potential for improving research synthesis across various domains.</p>
<p>Our contribution comprises (1) preparing a comprehensive dataset to fine-tune LLMs for meta-analysis generation, (2) fine-tuning LLMs with the novel ICD loss function, en-hancing their ability to handle large-context scientific data and extract relevant information for meta-analysis, and (3) leveraging these fine-tuned LLMs by integrating RAG to generate precise, instruction-based meta-analysis from largescale scientific data.</p>
<p>II. RELATED WORKS</p>
<p>Meta-Analysis Strategy:</p>
<p>In recent years, the landscape of meta-analysis has witnessed significant advancements, particularly with the development of comprehensive databases, facilitating systematic reviews.Csizmadia et al. [24] contributed to this domain by introducing a global database of innovation and quality management, meticulously compiling the PRISMA methodology, covering records from 1975 to 2021.Furthermore, Yudhanto et al. [25] addressed the laborintensive nature of data collection for meta-analysis, presenting a method using bibliometric studies from the Science Direct Database.Their approach, which involved data collection from published searches using desired keywords over the last decade, significantly streamlined the process, contributing to the efficiency of meta-analysis procedures.To further enhance this progress, our paper introduces a novel approach that harnesses the capabilities of LLMs and RAG.This approach aims to streamline the meta-analysis process, empower LLMs to handle large contexts efficiently, and conduct a structured meta-analysis of the provided research papers.Our contribution lies in developing a Comprehensive Meta-Analysis Dataset, which serves as a valuable resource for training and evaluating the efficiency of our proposed approach.Large Context Summarization: Recent advancements in large-context summarization have led to the development of techniques to generate concise and informative summaries from extensive documents.In particular, Subbiah et al. [16] introduce a fragmentation strategy, converting full stories into manageable fragments and associating them with prompts to facilitate effective summarization.Keswani et al. [21] focus on summarization and question-answering tasks using the Llama-2 (13B) model, employing clustering techniques based on cosine similarity to improve efficiency despite computational constraints.Furthermore, observations on different LLM summarization performances have been made using zero-shot prompt techniques [26], [27].These advancements underscore the significance of leveraging LLMs for largecontext summarization tasks.Leveraging these developments, our proposed method successfully utilizes large contexts by breaking them into smaller, manageable chunks.This strategy facilitates easier summarization by smaller LLMs, thereby enhancing the overall efficiency of the process.Summarization Quality Assessment by Evaluation: Evaluation metrics are crucial for assessing the effectiveness of automated summarization systems.Traditional metrics often have limitations in accurately capturing the quality of generated summaries [28].Innovative frameworks like HumanELY [29] have been proposed, incorporating key evaluation metrics including relevance, coverage, coherence, harm, and comparison.Additionally, novel scoring systems leveraging LLMs have been introduced, shedding light on how different identities influence performance [30].A taxonomy of LLM-based NLG evaluation methods has also been presented, delineating their advantages and drawbacks [31].Despite these efforts, achieving comprehensive evaluation frameworks for NLG systems remains challenging.Inspired by the pioneering work of Chaudhary et al. [32] on generating both relevant and irrelevant queries, we adopt a similar methodology to evaluate the efficacy of our generated meta-analysis.Our evaluation metrics encompass not only relevance but also nuances, categorizing outputs into Relevant, Somewhat-Relevant, and Irrelevant.Incorporating this thorough evaluation framework allows us to deliver a detailed evaluation of the performance of our automated meta-analysis synthesis, thereby enriching the depth of analysis and insights in our research.Our approach introduces a novel evaluation concept based on hard voting, contributing to meta-analysis automation.Leveraging a comprehensive meta-analysis dataset, innovative training methods, and fine-tuning strategies tailored for instructionbased meta-analysis abstracts, our approach stands out for its effectiveness and reliability in advancing research synthesis efficiency across domains.</p>
<p>III. METHODOLOGY</p>
<p>Several innovative efforts have been made to guarantee that LLMs can manage lengthy contexts.However, to incorporate big textual data challenges, LLMs require numerous amounts of resources.This study presents a novel approach for generating meta-analysis using LLMs, particularly with long context lengths.This section formally outlines our method for using LLM to produce meta-analysis content.Problem Formulation: Consider there are m j number of meta-articles, where j âˆˆ [1, n].For each meta-article, there is a set of support articles, S j == {v j 1 , v j 2 , ..., v j |S j | }. v j i represents the abstract of i th support article related to j th meta-article.We aim to build a model M to generate a meta-article's abstract, y j using all abstracts inside the set S j .This study focuses on generating a relevant y j through a low context length LLM (e.g., Llama-2 7B, Mistral 7B, and Gemma 7B).</p>
<p>Here we have investigated two important aspects of this problem.(a) Handling large context length: Typically, meta-analyses are performed through manual analysis and data extraction from supporting articles.Recently, LLMs have demonstrated their ability to summarize extensive textual data.While substantial research has focused on summarization [18], [19], [22], the application of LLMs for meta-analysis remains unexplored.Meta-analysis often involves structured data derived from supporting articles, yet most LLMs operate within constrained context-length environments during fine-tuning.Our objective is to address this limitation by efficiently managing large contextual data and segmenting it into smaller chunks to facilitate effective fine-tuning of LLMs.(b) Enhance information retrieval: Prior research has demonstrated that finetuning LLMs enhances their data extraction capabilities [23].However, when generating large contextual, analytical data, LLMs require access to external knowledge sources.RAG has shown promising results in addressing this challenge.In our approach to enabling context-length-restricted LLMs to generate meta-analysis and to further expand the scope of knowledge from supporting articles, we aim to integrate RAG with our fine-tuned meta-analysis generator LLMs.</p>
<p>A. MAD: Meta-Analysis Dataset</p>
<p>Generative language models are capable of producing reviews or summaries of given contexts.They still have issues producing analytical context based on substantial context inputs.Our first step in addressing this big-data challenge is to create a dataset to generate a meta-analysis.Large scientific context datasets like MAD have not been used before to finetune context-length-restricted LLMs.</p>
<p>The dataset, MAD that we constructed consists of two columns: one containing meta-articles' abstracts and the other containing the abstracts of the support articles.For example, consider the meta-article titled "Intervention methods for improving reduced heart rate variability in patients with major depressive disorder: A systematic review and metaanalysis" [20].We used the abstract of this paper as our target meta-analysis abstract.From Table 1 of this paper, we identified that it conducted a meta-analysis of over twenty studies.We manually extracted the abstracts of these support articles by following the references listed in the table.These twenty abstracts were placed in the second column (S j ) alongside the meta-article's abstract (y j ).Essentially, the goal is for the LLM to generate a meta-analysis abstract from these support articles' abstracts.</p>
<p>Using this approach, we gathered 625 meta-articles from ScienceDirect, along with the abstracts of all the support articles included in that meta-analysis.In total, dataset MAD</p>
<p>Large Language Model</p>
<p>Meta-analysis</p>
<p>Query: Generate a metaanalysis abstract on Synbiotic as an adjunctive agent can in....</p>
<p>Large Language Model</p>
<p>Multiple RCTs, epidemiological evidence and studies report on nonalcoholic fatty liver disease and the effects of prebiotics and synbiotics, with a borderline statistically significant reduction in FBG (âˆ’0.18 mmol/L, 95% CI âˆ’0.37, 0.00; p = 0.05).... Fig. 2: In our meta-analysis generation system, support articles S j undergo chunk-based pre-processing, producing chunks C j i âŠ† S j , here "SP:" refers to an abstract of the support article S j .These chunks are used to fine-tune the LLMs for predicting meta-analysis abstracts y j , with the ICD loss guiding the fine-tuning process.Model performance is assessed through human evaluation of the relevancy of generated meta-analysis abstracts, Å·j by fine-tuned LLMs.During inference, we integrate RAG with the fine-tuned LLMs.Chunked samples are stored in a vector database, from which relevant information is retrieved via a semantic search based on a query.The same processed C j i is used for both fine-tuning and inference to maintain retrieval consistency.The retrieved content and the query are provided to the LLM, enabling it to generate more precise and accurate meta-analysis abstracts by leveraging comprehensive contextual information.</p>
<p>Metaanalysis
Measure
includes 6344 support articles' abstracts and 625 meta-articles' abstracts.The dataset statistics, along with the demographic information of the human evaluators who assessed model performance, are shown in Table I, and the distribution of support articles in meta-articles is shown in Fig. 3.</p>
<p>B. Chunk-Based Processing of Support Articles</p>
<p>Given the limitation in context length for many language models, processing long or complex documents as a whole can become inefficient and may lead to suboptimal results.To address this, chunking the support articles into smaller, meaningful segments allows for more effective input to the language model.By chunking, we ensure that all support article abstracts in the set S j are considered while maintaining manageable input sizes for low-context models.</p>
<p>To manage the input size and improve model performance, we divide the support articles set S j = {v j 1 , v j 2 , . . ., v j |S j | } into multiple smaller overlapping chunks.Overlapping will be done with some portions of abstracts.This will allow the coherence and continuity between chunks, reducing the chances of information loss.Chunking of S j into k possibly overlapping chunks is defined as:
C(S j ) = {C j 1 , C j 2 , . . . , C j k } where: â€¢ C j i âŠ† S j for each i âˆˆ [1, k],
â€¢ k i=1 C j i = S j (the union of all chunks covers the entire set, though the chunks may overlap), â€¢ C j i âˆ© C j l Ì¸ = âˆ… for i Ì¸ = l (contents will overlap).For example, suppose the set of support article abstracts S j = {v j 1 , v j 2 , v j 3 , v j 4 , v j 5 } is divided into three overlapping chunks.The chunking is as follows:
C(S j ) = {C j 1 , C j 2 , C j 3 } where:C j 1 = {v j 1 , v j 2 }, C j 2 = {(portion of )v j 2 , v j 3 , v j 4 }, C j 3 = {(part of )v j 4 , v j 5 }.
In this example, v j 2 and v j 4 overlap in C j 2 and C j 3 .</p>
<p>C. Fine-tune LLMs and Integrate RAG</p>
<p>After creating the dataset MAD, two popular LLMs are considered for the experiment: Llama-2 (7B) [33] and Mistral-v0.1 (7B) [34].They are fine-tuned on the constructed dataset TABLE I: Detailed statistics of the actual and processed dataset MAD, along with the demographic profile of Human Evaluators for assessing the readability of fine-tuned LLMs.The MAD dataset contains abstracts from medical metaarticles and support studies.MAD and evaluated using the test set.To further improve their performance in generating relevant outcomes, we applied the RAG approach [35] to the fine-tuned versions of each model.Linear Unit (SiLU) activation function [37] and incorporates grouped-query attention (GQA) with sliding window attention (SWA) for efficient handling of variable sequences [34].Mistral-v0.1 (7B) outperforms both Llama-2 (7B) and Llama-2 (13B) in benchmarks, making it our model of choice.</p>
<p>Fine-tuning LLMs: Original models like Llama-2 and Mistral-v0.1 benefit from extensive pre-training on massive datasets, allowing them to grasp complex linguistic structures.However, this generic training might not be ideal for specialized tasks like generating meta-analysis abstracts from lengthy source materials.Fine-tuning bridges this gap by adapting these pre-trained models to new datasets and large data tasks.Following the selection of models, Llama-2 (7B) and Mistral-v0.1 (7B) were fine-tuned on the processed MAD dataset, utilizing chunked samples C j i paired with their respective meta-article's abstracts y j .This supervised fine-tuning process paired each chunked sample with its meta-analysis abstract (C j i , y j ), where C j i âŠ† S j and y j serves as label meta-article's abstract.The models, M, were trained to recognize patterns for generating meta-analysis content from large contexts, accommodating the multiple chunks associated with each y j .Instruction-based fine-tuning was employed with a focus on prompt engineering.Various prompt configurations Fig. 3: Distribution of Supporting Articles in Meta-Articles in the dataset MAD.The chart shows that most meta-articles contain 6 to 14 support articles, with peaks at 6 and 9, suggesting a common reliance on a moderate number of supporting studies, with fewer analyses incorporating larger study pools.</p>
<p>were tested to optimize the models' ability to generate accurate and coherent meta-analysis abstracts from long contexts.This approach ensured that the models effectively learned the specific patterns required for high-quality content generation.</p>
<p>Inverse Cosine Distance (ICD):</p>
<p>To support fine-tuning, a specialized training mechanism is used with the ICD loss metric.The ICD function measures the dissimilarity between the model-generated output Å· and the ground truth y vectors, incorporating a small positive constant Ïµ in the denominator to ensure numerical stability and improve the fine-tuning process.The formulation for ICD is given below:
ICD = 1 N N i=1 1 cosine sim i (y, Å·) + Ïµ(1)
During fine-tuning, models M processed chunked samples C j i to produce predicted abstracts Å·j .The ICD loss, calculated using formula 1, measured the dissimilarity between Å·j and the ground truth abstracts y j , guiding parameter updates via backpropagation.The process, constrained by resources, was carried out for 2 epochs over 5 iterations, refining the model and improving abstract accuracy.</p>
<p>Combining Fine-tuned model with RAG: Fine-tuning LLMs is highly effective for specific tasks; however, models with limited context, such as Llama-2 (7B) and Mistral-v0.1 (7B), face challenges when dealing with chunked data samples.For instance, when generating Å·j from a particular chunk C j i âŠ† S j , these models may lack information from other chunks C j i of the same dataset, MAD.RAG addresses this issue by retrieving relevant information from other chunks for the j th data sample, thereby reducing the need for extensive fine-tuning and minimizing irrelevant content.This approach involves storing each chunked test sample C j i in a vector database.Relevant chunks are then retrieved using semantic search based on queries and the stored chunks.The retrieved TABLE II: After fine-tuning the LLMs on the dataset MAD, comparing model performance on benchmark datasets for summarization quality, enabling assessment across varying context lengths.Among all the limited context-length LLMs, our fine-tuned (FT) models performed acceptably well comparatively.On the scientific document-contained dataset, Cl-SciSumm, our fine-tuned model outperformed other pre-trained LLMs, showing a significantly rigorous capability of capturing structured analytical information.â†‘ (â†“) means higher (lower) is better.'-' denotes results that are not applicable there.content is subsequently fed into the LLMs, which process these additional contexts to generate a more accurate metaanalysis abstract.(AdditionalDetails in the Supplementary)</p>
<p>Method</p>
<p>IV. EXPERIMENT</p>
<p>A. Setup</p>
<p>Dataset: We used the dataset MAD for fine-tuning the LLMs.The dataset is split into a train, test, and validation set.The training set includes 400 meta-analysis scenarios.Given that support papers' abstracts, S j , often exceed the given context limit, chunk-based preprocessing is applied to chunk support papers' abstracts, S j .Chunking wasn't applied to metaarticles' abstract y j as the context length was manageable without chunking.Table I illustrates how chunking reduces context length.The same chunking approach is applied to the validation set (175 samples) and test set (50 samples).After chunking, the training set expands to 3659 samples.After finetuning the models, we then tested our fine-tuned model on the benchmark Open-i dataset [38], writer summaries [27], and the large scientific document dataset CL-SciSumm [39] to compare performance, shown in Table II.</p>
<p>Implementation Details 1 : The dataset MAD requires careful management due to the context size limitations of LLMs, which have a maximum context length of 4096 tokens.The "Recursive TextSplitter" from LangChain 2 is used to chunk the support papers' abstracts, S j , into overlapping segments 1 Code and data: https://github.com/EncryptedBinary/Metaanalysis 2 LangChain: https://www.langchain.com/ of 200 tokens, capped at 2000 tokens.This approach converts meta-article abstracts into target values y j and the chunks C j i into features for supervised fine-tuning.Due to the seven billion parameters of Llama-2 (7B) and Mistral-v0.1 (7B), the Quantized Low-Rank Adapters (QLoRA) configuration [44] is employed for model loading.A custom trainer class, extending the transformer trainer and incorporating the ICD loss function, is used to measure dissimilarity between generated and target meta-analysis content, guiding iterative model weight updates.Inputs are tokenized using Transformers' AutoTokenizer, and LangChain facilitates retrieval augmentation.All experiments were conducted on NVIDIA Tesla T4 (2x) GPUs using the PyTorch framework.(Further details are provided in the supplementary paper.)Prompt Selection: The selection of prompts significantly influences model performance by guiding task handling.After detailed experimentation with multiple prompts, we came up with the most impactful prompt that leverages LLMs to generate meta-analysis accurately.Table IV shows the impact of prompts on relevancy.A comparison between the two prompts is shown there.Prompt 1 demonstrated superior effectiveness over Prompt 2 in generating meta-analysis abstracts, achieving a high relevancy rate with every LLM.Evaluation metrics: For evaluating the generated texts from LLMs, Bilingual Evaluation Understudy (BLEU) [45], that quantifies the resemblance between generated and reference texts and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [46], that assesses how much information from reference summaries is captured.are used.For the abstract TABLE III: Human evaluation is done on generated meta-analysis abstract by fine-tuned and non-fine-tuned LLMs, following the criteria REL: Relevant, SWR: Somewhat-Relevant, and IRL: Irrelevant mentioned in the methodology.System-level metrics BLEU and ROUGE are used to identify when a human evaluator mentions in Table .I that a generated text is irrelevant and relevant.In the end, the generated meta-analysis abstract using the RAG approach is evaluated by measuring the generated abstract's similarity with the ground truth (SWGT).The symbol â†‘ (or â†“) indicates that a higher (or lower) value is preferable.generated by fine-tuned models, the cosine similarity [47] metric is used to quantify the similarity between two vectors after combining fine-tuning with RAG.</p>
<p>Human evaluation: After generating responses with LLMs, we conduct a human evaluation process to ensure alignment with human judgment.Human judges categorize the generated text as relevant, somewhat-relevant, or irrelevant, following the criteria from [32].Relevant responses closely resemble the ground truth, showing high similarity and inclusion of crucial information.Somewhat-relevant responses have acceptable similarity, containing valuable information within an acceptable margin.Irrelevant responses lack important information or include unrelated content.This classification framework ensures a rigorous assessment of generated meta-analysis abstracts against expected standards.Three independent evaluators assessed each model's response, with majority voting used to determine the final decision.Each evaluator worked independently, without access to others' assessments.In total, 13 evaluators were involved in evaluating all the model responses.</p>
<p>To reduce bias, the evaluations were conducted by university students rather than the authors.Demographic details of the evaluators are provided in Table I.For further details on the evaluation process, refer to the Supplementary Material.</p>
<p>B. Results and Analysis</p>
<p>We present a detailed overview of our experimental evaluations, focusing on how context-length restricted LLMs perform in generating meta-analysis with lengthy inputs.Notably, previous research has not utilized large context datasets for meta-analysis, making our study unique.For comparison, we also used a short context dataset to evaluate the models' performance, as shown in Table II.Considering the architecture of our models, the benchmark performance is reliable.</p>
<p>After fine-tuning the LLMs, human evaluation of the generated outputs is essential.We applied our proposed human evaluation metrics-Relevant, Somewhat-Relevant, and Irrelevant-to assess the results of the meta-analysis generation task.As shown in Table III, our approach of fine-tuning LLMs with large context dataset, MAD outperforms other methods, producing more relevant meta-analysis content and reducing unnecessary context generation.</p>
<p>The non-fine-tuned Llama-2 (7B) model performs better than the non-fine-tuned Mistral-v0.1 (7B) model in generating relevant and somewhat relevant meta-analysis abstracts.After fine-tuning, the rate of irrelevant content generation significantly decreases, resulting in a highly effective meta-analysis abstract generation.Table III also highlights the alignment between machine-generated and human-generated texts, which is referred by SWGT.The integration of RAG has shown promising outcomes in terms of generating relevant metaanalyses.Table V provides two instances of our method's creation of meta-analysis abstracts, demonstrating their encouraging resemblance to the abstracts of meta-articles.This validates the dependability of our method.</p>
<p>Our observation includes (1) fine-tuning with a large context scientific dataset, MAD, letting LLMs learn the patterns for generating meta-analysis content with higher relevancy.This proves the reliability of our approach to handling big data management challenges.(2) BLEU and ROUGE scores are utilized to compare relevant and irrelevant human-evaluated contexts, where a generated text is considered irrelevant if it contains less than 10% context translation using large metapapers' input (represented by BLEU).(3) Fine-tuned models exhibit improved performance over base models, indicating more significant agreement between the generated abstract in the RAG approach and the real meta-analysis abstract.It highlights how well the fine-tuning approach works to help models find the patterns required to generate high-quality meta-analysis abstracts.</p>
<p>C. Ablation Study</p>
<p>We perform ablation studies focusing on three crucial areas: prompt variant analysis, temperature variation, and the impact of our proposed loss metric on fine-tuned models.These studies provide deeper insights into the performance factors for meta-analysis generation.Prompt Variant Analysis: Prompt selection is fundamental in steering the meta-analysis generation process.In Table IV, we compare the effectiveness of two distinct prompts.We evaluated the relevancy and quality of meta-analysis abstracts produced by Llama-2 (7B) and Mistral-v0.1 (7B) across both prompts.Our results show that Prompt 1 consistently outperforms Prompt 2 in terms of relevancy, generating more accu- and ROUGE-L.The higher temperature yielded more diverse outputs without sacrificing relevancy or quality, making it the optimal setting for our meta-analysis generation tasks.Impact of Our Loss Metric: We implemented a specialized loss function, the ICD, designed to enhance the performance of meta-analysis summarization tasks.Fig 4(b) compares the performance of models fine-tuned with ICD against models using a standard loss function across both Llama-2 FT and Mistral-v0.1 FT versions.ICD emphasizes the directional similarity between the generated outputs and ground truth vectors by utilizing cosine similarity, capturing nuanced semantic details.This metric outperformed the standard loss function, improving the alignment between the generated summaries and their reference summaries.The ICD's ability to capture subtle semantic nuances beyond simple word matching proved crucial in fine-tuning the models for more accurate and coherent meta-analysis generation.</p>
<p>D. Discussion</p>
<p>This study represents insights into generating meta-analysis leveraging LLMs using a large-context scientific dataset, MAD.The result section provides evidence of our fine-tuned models' performance, showing the successive relevancy rate for generating meta-analysis.It was observed that the finetuned models for Llama-2 (7B) and Mistral-v0.1 (7B) outperformed their non-fine-tuned versions by generating significantly relevant meta-analyses.As expected, integrating RAG with fine-tuned models allows them to generate highly aligned meta-analyses.Limitations: One key limitation of this study is the maximum context length of the LLMs, which required chunking the input data.To mitigate potential information loss, overlapping context techniques and RAG were employed.However, due to hardware constraints, the model's evaluation was performed on only 50% of the test sets, which proved resource-intensive.Additionally, training the models in a highly quantized configuration limited the fine-tuning potential, impacting the ability to fully optimize the model's parameters for better performance.</p>
<p>V. CONCLUSION</p>
<p>This study demonstrates the effectiveness of automating meta-analysis generation using fine-tuned LLMs on extensive scientific datasets.Our approach significantly improved the relevance of generated meta-analysis abstracts, achieving 87.6% relevance and reducing irrelevance from 4.56% to 1.9%, demonstrating its potential and highlighting further promising TABLE V: Meta-analysis abstract generation from supporting article abstracts using our fine-tuned Mistral-v0.1 7B model, combined with RAG and efficient prompting."SP:" denotes each support article's abstract in S j , with "..." indicating continuation for multiple articles.Similarity with the original meta-articles' abstracts validates the effectiveness of the approach.</p>
<p>Prompt: Given a collection of abstracts from papers used in various medical fields for meta-analysis, generate a meta-analysis abstract.Summarize the key findings and provide numerical values or statistical information for specific observations that are commonly reported in the provided abstracts.</p>
<p>Example 1: Input (Support article's abstract, S j ):</p>
<p>Generated Meta-analysis abstract, Å·j (Similarity with ground truth [48]: 82.40%): SP: Several randomized controlled trials (RCTs) have investigated the use of probiotic/synbiotic in PCOS patients,... SP:</p>
<p>The effect of synbiotic supplementation on glycemic status in pregnant women remained controversial and ... SP: Probiotic and synbiotic products are being widely used by a large number of patients and clinicians; however, effects on cardiometabolic indices ... SP: This study aimed to review randomized clinical trials (RCTs) to clarify the effects of pro-/synbiotic supplementation ...fasting blood sugar (-2.52 mg/dl, 95% confidence interval (CI): -4.10 to -0.95), insulin (-2.27-ÂµIU/mL, 95% CI: -3.40 to -1.14),...This meta-analysis abstract summarizes the findings of several randomized controlled trials (RCTs) on the effect of synbiotic intake on metabolic profiles in patients with diabetes.Heterogeneity was measured with a Q-test and the I 2 statistics.The results of the meta-analysis suggest that synbiotic intake can be useful as an adjunctive agent in the management of hyperglycemia in adults.The study pooled data by using the fix or random-effect model based on the results.A significant reduction in blood glucose levels was observed in patients who took synbiotic supplements compared to those who took placebo with statistical significance (p &lt; 0.05).Additionally, a significant decrease in glycated hemoglobin (HbA1c) levels was observed in patients who took synbiotic supplements compared to placebo (p &lt; 0.05).After taking synbiotic supplements, there were significant reductions in fasting blood sugar (FBS) (ES = -0.40,95% CI: -0.64, -0.15; p = 0.002, I2 = 69.2%, p Â¡ 0.001), insulin (ES = -1.58;95% CI: -0.79, -0.30, p Â¡ 0.001, I2 = 48.4%,p = 0.042), and HOMA-IR (Homeostatic Model Assessment of Insulin Resistance) (ES = -0.55;95% CI: -0.79, -0.30, p Â¡ 0.001, I2 = 48.4%,p Â¡ 0.001).Overall, the findings of this meta-analysis suggest that synbiotic intake can be an effective adjunctive agent in the management of hyperglycemia in adults.In this meta-analysis, we evaluated the effectiveness of traditional Chinese medicine (TCM) in treating COVID-19.A comprehensive search was conducted on a variety of electronic databases to identify studies reporting TCM treatment for COVID-19.The studies were screened according to inclusion and exclusion criteria, and a total of 10 studies were included in the analysis.These studies had a total of 1,880 participants who received TCM treatment for COVID-19.The analysis found that TCM had significant positive effects on the symptomatic relief of COVID-19.The pooled effect size for symptomatic relief was 0.67 (95% CI: 0.56 to 0.79), indicating a moderate effect size.The studies also reported on the impact of TCM on the severity of COVID-19.The pooled effect size for severity reduction was 0.51 (95% CI: 0.43 to 0.60), indicating a moderate effect size.In addition, the studies also reported on the impact of TCM on the mortality rate of COVID-19.The pooled effect size for mortality reduction was 0.37 (95% CI: 0.21 to 0.55), indicating a small but statistically significant effect size.Overall, this meta-analysis suggests that TCM may be an effective treatment for COVID-19, and further studies are needed to confirm these findings.It is important to note that this meta-analysis had several limitations, including the small number of studies, heterogeneity of TCM treatments, and variable outcomes.Out of the 89 outcome indicators that were assessed, 8 were found to be of moderate quality, 39 to be of low quality, and 41 to be of very low quality.In conclusion, TCM may be an effective treatment for COVID-19, with significant positive effects on symptomatic relief and severity reduction.However, more research is needed to confirm these findings and to establish the optimal TCM treatment for COVID-19.</p>
<p>research opportunities in automating scientific synthesis.We introduced novel methods to address the challenges posed by limited context length and resource constraints, including using ICD as a tailored loss metric for training.Integrating RAG further optimized the process by ensuring efficient synthesis of research findings without sacrificing context.Human evaluation confirmed the improvements in model performance, particularly in maintaining the relevancy and accuracy of structured meta-analysis content.Future works: While this study achieved notable improvements in meta-analysis generation, future research should focus on expanding the dataset in various fields that need meta-analysis and refining the model's ability to generate even more accurate and reliable outputs, particularly in resourceconstrained environments.Further optimizations to LLM finetuning and scaling could lead to broader applicability in automating complex scientific analysis.</p>
<p>ETHICS STATEMENT</p>
<p>This study was conducted with a strong commitment to ethical integrity, particularly in the generation and evaluation of meta-analysis abstracts in the scientific field using LLMs.We engaged 13 human evaluators from diverse backgrounds, ensuring their participation was voluntary and informed.We carefully collected only essential information to assess their qualifications for the task, and any data that could potentially identify participants were securely deleted after the evaluation was completed.We took significant measures to protect the well-being of all participants, ensuring that the evaluation process posed no physical or psychological risk.Recognizing that even subtle biases or inaccuracies in scientific research can have serious consequences, we implemented rigorous protocols to ensure that all generated content adhered to the highest ethical standards.Our approach was designed to avoid any language or conclusions that could perpetuate harm or inequity based on race, gender, or other social determinants of health.By adhering to these principles, we have ensured that our research upholds the highest ethical standards, fostering a safe and respectful environment for both human participants and the broader community.</p>
<p>Fig 2 depicts the methodology for fine-tuning LLMs and generating meta-analysis.Model Architecture Overview: We utilized two prominent LLMs in this study.(a) Llama-2 (7B), a transformer-based LLM developed by Meta, includes 32 attention heads, a 32,000-token vocabulary, and a context length of 4,096.It uses the Swish-Gated Linear Unit (SwiGLU) activation function [36].(b) Mistral-v0.1 (7B) features similar architecture with 32 attention heads and a 32,000-token vocabulary but offers a larger context length of 8,192.It employs the Sigmoid</p>
<p>Fig. 4 :
4
Fig.4: Investigating the impact of (a) Temperature variation: BLEU, ROUGE-1, ROUGE-2, and ROUGE-L scores vary with temperature changes for both the Llama-2 (7B) and Mistral-v0.1 (7B) models indicating 0.7 temperature has a better impact.(b) Loss Function impact: ICD loss significantly improves performance for Llama-2 (7B) FT and Mistral-v0.1 (7B) FT models, demonstrating its ability to capture more information than the default loss.</p>
<p>TABLE IV :
IV
Comparative Prompt Analysis: Demonstrating effectiveness through two different prompts, where Prompt 1 performs better than Prompt 2 in generating a more relevant meta-analysis.acollection of abstracts from papers used in various medical fields for meta-analysis, generate a meta-analysis abstract.Summarize the key findings and provide numerical values or statistical information for specific observations that are commonly reported in the provided abstracts.(Prompt1)Thereare given some abstracts of papers that are used for meta-analysis in different medical fields.Generate a meta-analysis abstract based on the given abstracts of papers.Please try to provide numerical values for any specific findings that were used in most of the abstracts.(Prompt 2)
PromptEvaluation MetricLlama-MistralLlama-Mistral Ours22 OursRelevant â†‘83.580.585.487.6Somewhat Relevant â†‘11.9414.112.710.4Irrelevant â†“4.565.131.92.1Relevant â†‘6978.3972.482.8Somewhat Relevant â†‘12.716.120.514.1Irrelevant â†“7.695.517.13.13rate and precise meta-analysis abstracts. Specifically, Prompt 1achieved higher relevancy scores across all versions of Llama-2 and Mistral, with fewer instances of irrelevant content. Giventhese results, Prompt 1 was used in all subsequent experiments.Varying Temperature: The temperature parameter con-trols the randomness of predictions, influencing the balancebetween exploration and exploitation during the generationprocess. We explored the impact of different temperatures (0.1,0.5, and 0.7) on summary quality. As shown in Fig 4(a), a tem-perature setting of 0.7 provided the best results across variousevaluation metrics, including BLEU, ROUGE-1, ROUGE-2,
Given</p>
<p>[49]ple 2: Input (Support article's abstract, S j ):Generated Meta-analysis abstract, Å·j (Similarity with ground truth[49]: 85.73%): SP: As the global epidemic continues to spread, countries have tappe...SP:Introduction: Integrated Chinese and Western medicine (integr...SP: A simple, efficient, and environmentally friendly electro-Fen...SP: Currently, coronavirus disease 2019 (COVID-19), which can lead to... SP: Background: Until now, there is no clinically approved spe...SP: Chinese medicine (CM) has been used to treat Novel Coronavi...SP:Integration of Chinese medical drugs (CMD) and we...SP:This review aims to evaluate the supportive effe...SP:We systematically studied the passivation process of 6082 aluminium alloy under the bending stress... SP: Coronavirus disease 2019 (COVID-19) is an eme...SP:There is currently no drug or therapy that cures COVID-19, a highly contagious and... SP: The outbreak of coronavirus disease 2019...SP: Coronavirus disease 2019 (COVID-19) has eme...SP:Background: The coronavirus disease 2019 (COVID-19) pandemic...</p>
<p>Pre-trained StableLM-Base-Alpha 7B. 43</p>
<p>We generated summaries from 100 samples. 2 writer summaries: article summarization dataset. Open-I, Medical radiological dataset. evaluated our models on 120 samples</p>
<p>Large corpus dataset containing scientific article data. We evaluated 20 samples. Chunking the samples was required. Cl-Scisumm, as the context lengths are larger than allowable</p>
<p>Established A well-established method was proven by the given papers for these three specific datasets. The BLEU and ROUGE scores given by those studies can't be achieved with our investigated LLMs, as their established methodology includes fine-tuning and evaluating the given large dataset. </p>
<p>Meta-Analysis, ser. Cambridge Handbooks in Psychology. Y Jadotte, A Moyer, J Gurevitch, 2023Cambridge University Press</p>
<p>Incorporating quality scores in meta-analysis. S Ahn, B Becker, Journal of Educational and Behavioral Statistics. 3652011</p>
<p>Conducting a meta-analysis in the age of open science: Tools, tips, and practical recommendations. D Moreau, B Gamble, Psychological Methods. 2734262022</p>
<p>Threshold analysis as an alternative to grade for assessing confidence in guideline recommendations based on network metaanalyses. D M Phillippo, S Dias, N J Welton, D M Caldwell, N Taske, A Ades, Annals of internal medicine. 17082019</p>
<p>Consumer engagement in health care policy, research and services: A systematic review and metaanalysis of methods and effects. L K Wiles, D Kay, J A Luker, A Worley, J Austin, A Ball, A Bevan, M Cousins, S Dalton, E Hodges, PloS one. 171e02618082022</p>
<p>Systematic reviews and metaanalysis: understanding the best evidence in primary healthcare. S Gopalakrishnan, P Ganeshkumar, Journal of family medicine and primary care. 212013</p>
<p>Meta-analysis in medical research. A.-B Haidich, Hippokratia. 141292010Suppl</p>
<p>An overview of meta-analysis for clinicians. Y H Lee, The Korean Journal of Internal Medicine. 3322772018</p>
<p>Effectiveness of public health measures in reducing the incidence of covid-19, sars-cov-2 transmission, and covid-19 mortality: systematic review and meta-analysis. S Talic, S Shah, H Wild, D Gasevic, A Maharaj, Z Ademi, X Li, W Xu, I Mesa-Eguiagaray, J Rostron, bmj. 3752021</p>
<p>The effect of litter materials on broiler performance: a systematic review and meta-analysis. T Toledo, C Pich, A Roll, M Dai PrÃ¡, F Leivas Leite, E Gonc Â¸alves Xavier, V Roll, British poultry science. 6062019</p>
<p>The impact of conservation farming practices on mediterranean agro-ecosystem services provisioning-a meta-analysis. H Lee, S Lautenbach, A P G Nieto, A Bondeau, W Cramer, I R Geijzendorffer, Regional Environmental Change. 1982019</p>
<p>Environmental sustainability and behavioral science: Meta-analysis of proenvironmental behavior experiments. R Osbaldiston, J P Schott, Environment and behavior. 4422012</p>
<p>A systematic review and meta-analysis of psychological interventions to improve mental wellbeing. J Van Agteren, M Iasiello, L Lo, J Bartholomaeus, Z Kopsaftis, M Carey, M Kyrios, Nature human behaviour. 552021</p>
<p>Governance and deforestation-a meta-analysis in economics. J Wehkamp, N Koch, S LÃ¼bbers, S Fuss, Ecological economics. 1442018</p>
<p>The power of feedback revisited: A meta-analysis of educational feedback research. B Wisniewski, K Zierer, J Hattie, Frontiers in psychology. 104876622020</p>
<p>Reading subtext: Evaluating large language models on short story summarization with writers. M Subbiah, S Zhang, L B Chilton, K Mckeown, arXiv:2403.010612024arXiv preprint</p>
<p>Improving multi-stage long document summarization with enhanced coarse summarizer. J Lim, H.-J Song, Proceedings of the 4th New Frontiers in Summarization Workshop. the 4th New Frontiers in Summarization Workshop2023</p>
<p>Financial report chunking for effective retrieval augmented generation. A J Yepes, Y You, J Milczek, S Laverde, L Li, arXiv:2402.051312024arXiv preprint</p>
<p>Retrieval augmented generation and representative vector summarization for large unstructured textual data in medical education. S Manathunga, Y Illangasekara, arXiv:2308.004792023arXiv preprint</p>
<p>Intervention methods for improving reduced heart rate variability in patients with major depressive disorder: A systematic review and meta-analysis. S Chen, H Wang, J Yue, N Guan, X Wang, Comprehensive Psychiatry. 1191523472022</p>
<p>Abstractive long text summarization using large language models. G Keswani, W Bisen, H Padwad, Y Wankhedkar, S Pandey, A Soni, International Journal of Intelligent Systems and Applications in Engineering. 1212s2024</p>
<p>Evaluation of chatgpt-generated medical responses: A systematic review and metaanalysis. Q Wei, Z Yao, Y Cui, B Wei, Z Jin, X Xu, Journal of Biomedical Informatics. 1046202024</p>
<p>Artificial intelligence to automate network meta-analyses: Four case studies to evaluate the potential application of large language models. T Reason, E Benbow, J Langham, A Gimblett, S L Klijn, B Malcolm, PharmacoEconomics-Open. 2024</p>
<p>A global database for conducting systematic reviews and meta-analyses in innovation and quality management. T Csizmadia, A I Katona, Scientific Data. 913012022</p>
<p>Metadata research development: A bibliometric study on science direct. S Yudhanto, T Asmiyanto, Library Philosophy and Practice. 2021</p>
<p>News summarization and evaluation in the era of gpt-3. T Goyal, J J Li, G Durrett, arXiv:2209.123562022arXiv preprint</p>
<p>Benchmarking large language models for news summarization. T Zhang, F Ladhak, E Durmus, P Liang, K Mckeown, T B Hashimoto, Transactions of the Association for Computational Linguistics. 122024</p>
<p>A survey on evaluation of summarization methods. L Ermakova, J V Cossu, J Mothe, Information processing &amp; management. 201956</p>
<p>Humanely: Human evaluation of llm yield, using a novel web based evaluation tool. R Awasthi, S Mishra, D Mahapatra, A Khanna, K Maheshwari, J Cywinski, F Papay, P Mathur, 2023Cold Spring Harbor Laboratory Press</p>
<p>Characterised llms affect its evaluation of summary and translation. Y Lu, Y.-T Lin, 2023</p>
<p>Llm-based nlg evaluation: Current status and challenges. M Gao, X Hu, J Ruan, X Pu, X Wan, arXiv:2402.013832024arXiv preprint</p>
<p>It's all relative!-a synthetic query generation approach for improving zero-shot relevance prediction. A Chaudhary, K Raman, M Bendersky, arXiv:2311.079302023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H KÃ¼ttler, M Lewis, W -T. Yih, T RocktÃ¤schel, Advances in Neural Information Processing Systems. 202033</p>
<p>Glu variants improve transformer. N Shazeer, arXiv:2002.052022020arXiv preprint</p>
<p>Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. S Elfwing, E Uchibe, K Doya, Neural networks. 1072018</p>
<p>Adapted large language models can outperform medical experts in clinical text summarization. D Van Veen, C Van Uden, L Blankemeier, J.-B Delbrouck, A Aali, C Bluethgen, A Pareek, M Polacin, E P Reis, A SeehofnerovÃ¡, Nature Medicine. 2024</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. M Yasunaga, J Kasai, R Zhang, A R Fabbri, I Li, D Friedman, D R Radev, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>The falcon series of open language models. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, Ã‰ Goffinet, D Hesslow, J Launay, Q Malartic, arXiv:2311.168672023arXiv preprint</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M RiviÃ¨re, M S Kale, J Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Teaching small language models how to reason. A Mitra, L D Corro, S Mahajan, A Codas, C Simoes, S Agrawal, X Chen, A Razdaibiedina, E Jones, K Aggarwal, H Palangi, G Zheng, C Rosset, H Khanpour, A Awadallah, Orca. 22023</p>
<p>J Tow, StableLM Alpha v2 Models. </p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Rouge: A package for automatic evaluation of summaries. C.-Y Lin, Text summarization branches out. 2004</p>
<p>Distance weighted cosine similarity measure for text classification. B Li, L Han, Intelligent Data Engineering and Automated Learning-IDEAL 2013: 14th International Conference, IDEAL 2013. Proceedings. Hefei, ChinaSpringerOctober 20-23, 2013. 201314</p>
<p>Synbiotic as an adjunctive agent can be useful in the management of hyperglycemia in adults: An umbrella review and meta-research of metaanalysis studies. V Musazadeh, A H Faghfouri, Z Kavyani, P Dehghan, Journal of Functional Foods. 991053552022</p>
<p>Traditional chinese medicine in covid-19. M Lyu, G Fan, G Xiao, T Wang, D Xu, J Gao, S Ge, Q Li, Y Ma, H Zhang, Acta Pharmaceutica Sinica B. 11112021</p>            </div>
        </div>

    </div>
</body>
</html>
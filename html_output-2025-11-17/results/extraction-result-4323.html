<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4323 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4323</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4323</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-282210220</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.17263v1.pdf" target="_blank">TaxoAlign: Scholarly Taxonomy Generation Using Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Taxonomies play a crucial role in helping researchers structure and navigate knowledge in a hierarchical manner. They also form an important part in the creation of comprehensive literature surveys. The existing approaches to automatic survey generation do not compare the structure of the generated surveys with those written by human experts. To address this gap, we present our own method for automated taxonomy creation that can bridge the gap between human-generated and automatically-created taxonomies. For this purpose, we create the CS-TaxoBench benchmark which consists of 460 taxonomies that have been extracted from human-written survey papers. We also include an additional test set of 80 taxonomies curated from conference survey papers. We propose TaxoAlign, a three-phase topic-based instruction-guided method for scholarly taxonomy generation. Additionally, we propose a stringent automated evaluation framework that measures the structural alignment and semantic coherence of automatically generated taxonomies in comparison to those created by human experts. We evaluate our method and various baselines on CS-TaxoBench, using both automated evaluation metrics and human evaluation studies. The results show that TaxoAlign consistently surpasses the baselines on nearly all metrics. The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4323.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4323.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAXOALIGN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAXOALIGN: Scholarly Taxonomy Generation Using Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-stage LLM-based pipeline that constructs hierarchical taxonomies from a topic and a set of reference scholarly papers by (1) extracting topic-relevant 'knowledge slices' from each paper, (2) instruction-tuning an LLM to verbalize a taxonomy (taxonomy verbalization), and (3) refining the generated taxonomy with a stronger LLM to ensure grounding and structural alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (Mistral-7B-Instruct, Meta-Llama-3-8B-Instruct, Llama-3.1-Tülu-3-8B, SciLitLLM1.5-7B, GPT-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 8B / 7B / 32B (various; see method_description) </td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>TAXOALIGN (Knowledge Slice Creation + Taxonomy Verbalization + Taxonomy Refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Three-stage pipeline: (1) Knowledge Slice Creation — an LLM (open models such as Mistral-7B-Instruct or Meta-Llama-3-8B-Instruct) is prompted to identify compact, topic-relevant text segments ('knowledge slices') from each reference paper so that a large set of references can be represented within LLM context limits; (2) Taxonomy Verbalization — an instruction-tuned LLM (they finetune Llama-3.1-Tülu-3-8B and SciLitLLM1.5-7B using QLoRA) is trained on CS-TAXOBENCH examples to generate hierarchical taxonomy trees (max depth 3), producing concise node labels and preserving tree structure; (3) Taxonomy Refinement — a stronger closed-domain model (GPT-4o-mini) is prompted to evaluate and refine parent–child connections, ground nodes in knowledge slices, expand nodes if coverage is insufficient, and remove hallucinated or irrelevant nodes. The pipeline uses explicit prompts (appendices A.2–A.4) and instruction-tuning (QLoRA for 800 steps, context window 16,384) to shape generation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>460 survey papers (CS-TAXOBENCH); per-task input: on average ~131 reference papers per taxonomy (additional conference test set: 80 taxonomies, ~71 refs each)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Computer Science (scholarly survey papers from ACM Computing Surveys and conference surveys in ACL*/IJCAI)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Hierarchical topic structures / qualitative organizational patterns and conceptual categorizations (i.e., taxonomic principles and subtopic patterns) rather than causal laws; distilled recurring topic groupings, subtopic hierarchies, and design/organization principles implicit in survey literature.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Examples are taxonomy node labels/subtopics produced for the 'Human Image Generation' topic (e.g., '3D Models and Mapping', 'Generative Adversarial Network', 'Data and Annotation'; also higher-level categories seen in produced trees such as 'DATA-DRIVEN METHODS ON HUMAN IMAGE GENERATION', 'HYBRID METHODS', 'KNOWLEDGE-GUIDED METHODS').</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated structural and semantic metrics (Average Degree Score for structural similarity; level-order traversal followed by corpus-level BLEU-2, ROUGE-L, BERTScore for semantic/lexical alignment; Node Soft Recall (semantic overlap via Sentence-BERT embeddings) and Node Entity Recall (noun-phrase overlap)); LLM-as-a-judge (GPT-4.1) scoring; human evaluation with three domain-expert annotators using 5-point Likert scales and inter-annotator agreement (Krippendorff's α). Spearman correlation computed between human mean scores and LLM-as-judge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>TAXOALIGN (best reported configuration: K-slice: Mistral; T-verbalize: Tülu; T-refine: GPT-4o-mini) achieved average-degree score ∆ ≈ 1.668 (close to 1 indicates structural parity), level-order BLEU-2 ≈ 0.0051, ROUGE-L ≈ 0.2974, BERTScore ≈ 0.8517; human mean ratings (structure/content) reported ≈ 3.17 / 2.62 (5-point scale) vs AutoSurvey 2.17 / 2.25; LLM-as-judge / human Spearman's rho = 0.527. For taxonomy-extraction (heading-to-tree) module (manual annotation of 10 papers) extraction precision 83.92%, recall 94.35%, F1 88.83%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against AutoSurvey (outline generation step), STORM, Topic-only prompts, Topic+Keyphrases, and ablations (TAXOALIGN w/o Taxonomy Verbalization and w/o Taxonomy Refinement). TAXOALIGN outperforms baselines on nearly all metrics (structure, NSR, semantic scores); ablation TAXOALIGN w/o verbalization/refinement performed second best, indicating knowledge-slice importance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Knowledge-slice extraction is crucial: a pipeline using knowledge slices but no further verbalization/refinement performed strongly, showing the value of focused per-paper summarization. 2) Instruction-tuning (taxonomy verbalization) and subsequent LLM refinement reduce excessive branching and hallucinated nodes and improve node-label quality/semantic coherence. 3) LLMs can produce taxonomies structurally closer to human-written trees but still show low lexical overlap (BLEU/ROUGE values remain low), indicating a remaining gap between human and machine outputs. 4) LLM-as-a-judge correlates moderately well with human judgments (rho ≈ 0.527), making it a useful proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Context-window and long-document reasoning limits; hallucination and generation of irrelevant or overly verbose nodes; low lexical overlap even when structural alignment is reasonable; factual errors in node labels; variability across LLMs and configurations; dataset curated from a single journal/time window (ACM CSUR 2020–2024) limiting generality; reliance on available reference metadata (Semantic Scholar coverage filter).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4323.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4323.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge-Slice Creation (Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Slice Creation using Mistral-7B-Instruct-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based subroutine in TAXOALIGN that identifies short, topic-relevant excerpts ('knowledge slices') from each reference paper to condense the information so many references can be processed despite context-window constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Knowledge Slice Creation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt an LLM per document with the taxonomy topic and the document; instruct it to return compact highlights strictly grounded in the document that are highly relevant to the topic. These slices are used downstream to represent each reference within the model context. The paper uses Mistral-7B (and alternately Meta-Llama-3-8B-Instruct) for this stage.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to the reference sets for 460 taxonomies (on average ~131 reference papers per taxonomy); additional 80 conference-taxonomy test instances (~71 refs each).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Computer Science (survey reference papers)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Compresses evidence into topical highlights that enable extraction of hierarchical topic patterns; not directly causal laws but supports deriving organizational patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>No formal laws; produces concise document highlights that feed taxonomy construction (examples not enumerated in paper beyond taxonomy node examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Ablation studies: TAXOALIGN w/o verbalization/refinement (uses knowledge slices + prompting) ranks second overall, showing the stage's importance; manual error analysis noted verbosity and irrelevance issues when relying solely on slices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported ablation performance indicates strong contribution (the ablation baseline using knowledge slices is second-best across many metrics in Table 2), but specific per-model numeric breakdowns are reported in the paper's tables for various configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly to Keyphrase-only baseline and Topic-only; knowledge-slice approach outperforms keyphrase-only in downstream taxonomy quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Knowledge-slice extraction enables scaling to many references and is a major reason TAXOALIGN performs well; however, direct generation from slices (without verbalization/refinement) leads to verbose or sometimes off-topic nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Quality depends on prompt design and model capability; slices can be verbose or repeat content and may still contain irrelevant information; slices do not eliminate hallucination risk downstream.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4323.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4323.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Taxonomy Verbalization (Tülu / SciLitLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Taxonomy Verbalization using instruction-tuned Llama-3.1-Tülu-3-8B and SciLitLLM1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuning LLMs on example taxonomy generation tasks (CS-TAXOBENCH) so they learn to produce concise, structured taxonomy trees from knowledge slices and topic prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-Tülu-3-8B; SciLitLLM1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B; 7B</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Taxonomy Verbalization (instruction tuning, QLoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Instruction-tune LLMs (QLoRA: 4-bit quant, LoRA adapters) for 800 steps with large context (16,384) using Alpacalike instruction format and CS-TAXOBENCH examples. The tuned model is prompted with the topic and knowledge slices and constrained to output taxonomy trees with max depth 3, short node labels, and leaf nodes that reflect single topics. The verbalization stage preserves tree structure and improves label concision relative to direct prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Tuned on CS-TAXOBENCH training split (400 instances mentioned for training experiments) where each instance included a set of reference papers (avg ~131 per instance).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Computer Science taxonomy generation</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Generates hierarchical organizational principles (taxonomic categories and subtopics) derived from corpus-wide patterns across references.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Produces taxonomy nodes and hierarchical groupings (examples in paper: topic-specific node labels and trees; e.g., categories in 'Human Image Generation' taxonomy).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared to direct-prompt baselines and ablations using the same evaluation metrics (average degree, level-order BLEU/ROUGE/BERTScore, NSR/NER, human evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Instruction-tuned verbalizers (Tülu and SciLitLLM variants) combined with refinement achieved best combined metrics (∆ ≈ 1.668 in best configuration; improved ROUGE-L and BERTScore vs baselines). Exact numeric differences shown in Table 2 and appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to prompting-only baselines (Topic-only, Topic+Keyphrases) and to pipelines without verbalization; verbalization improves structure preservation and label quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction-tuning helps preserve taxonomy structure and produce concise labels; it reduces verbosity and excessive branching seen in prompt-only outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Finetuning requires curated training taxonomies (CS-TAXOBENCH) and compute (QLoRA steps). Despite improvements, factual errors and hallucinated labels can remain and further refinement is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4323.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4323.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Taxonomy Refinement (GPT-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Taxonomy Refinement using GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A final LLM-based refinement stage that inspects a generated taxonomy and its grounding knowledge slices to correct parent–child relations, expand coverage or prune irrelevant nodes, improving structural alignment and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini (closed-domain stronger LLM used as judge/refiner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Taxonomy Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt a stronger reasoning-capable LLM with the generated taxonomy and the knowledge slices; instruct it to check grounding of each node, refine node labels/connections, expand nodes if coverage is too low, and remove hallucinated nodes. This stage is applied after verbalization to improve structure and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Refinement applied to taxonomy instances produced for the 460 taxonomies in CS-TAXOBENCH (per-instance reference sets as above).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Computer Science taxonomy generation</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Improves the extraction of hierarchical organizational patterns by grounding nodes in cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Refinement produces corrected parent–child relations and pruned node sets—no new formal 'laws' but improved taxonomy quality (examples described qualitatively in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Ablation comparison: TAXOALIGN with refinement outperforms same pipeline without refinement on average-degree and semantic metrics; human eval and LLM-as-a-judge scores improve with refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Configurations including GPT-4o-mini refinement yielded the best overall scores in main tables (improved ∆ closer to 1 and better NSR/BERTScore); exact config numbers are reported in Table 2 (best ∆ ≈ 1.668).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to identical pipelines without refinement; refinement reduces branching and hallucinated nodes and leads to higher judged coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Refinement with a stronger LLM consistently improves structural and semantic alignment with human taxonomies and alleviates some hallucinations incurred in earlier stages.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Refinement depends on the reasoning ability and factual safety of the stronger LLM and may still leave layer-wise exact-label mismatches; introduces dependency on closed-domain (potentially non-open) models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4323.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4323.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that generates survey outlines by dividing references into groups, producing multiple outlines, and amalgamating them into a comprehensive outline using an LLM; used here as a baseline (outline-generation step only).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prompted with GPT-4o-mini in this paper's baseline runs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AutoSurvey (outline generation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>AutoSurvey randomly partitions reference papers into groups to produce multiple per-group outlines; LLMs combine these outlines into a single outline. For fairness, the authors provided the actual reference papers to AutoSurvey and ran only its outline generation component as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to the same reference sets as TAXOALIGN experiments (per-instance: ~131 refs on average).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Automated survey generation from academic papers (Computer Science)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Outline / topical organization (high-level survey structure), but tends to produce overly large/verbose trees in this paper's evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Baseline-generated nodes shown in Appendix figures; examples are more verbose and diverge structurally from gold taxonomies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated with the same structural and semantic metrics (average degree, level-order traversal BLEU/ROUGE/BERTScore, NSR/NER) and human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AutoSurvey baseline (prompted with GPT-4o-mini) reported ∆ ≈ 4.466, BLEU-2 ≈ 0.0016, ROUGE-L ≈ 0.1784, BERTScore ≈ 0.8256, NSR ≈ 1.0903, NER ≈ 0.1982, LLM-judge mean ≈ 2.43 (5-point scale).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>This is one of the principal baseline systems in the paper; TAXOALIGN outperforms AutoSurvey on structure, semantic coherence and human-judged quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AutoSurvey tends to generate overly-large/branched taxonomies (high ∆), leading to structural divergence and lower judged coherence compared to TAXOALIGN.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Random grouping of references and purely outline-merging approaches can lead to excessive branching, verbosity, and hallucinated or irrelevant nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4323.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4323.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STORM (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STORM (pre-writing simulated-conversation outline method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline survey-generation method that simulates conversational research (pre-writing) to generate draft outlines and then refines them using topic+conversations; used as a baseline in TAXOALIGN experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prompted with GPT-4o-mini in this paper's baseline runs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>STORM (outline generation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Generates an initial draft outline by prompting an LLM with the topic, then augments with simulated conversations about the topic and reference material to generate a final outline. In the paper, the pipeline was provided with reference papers and run as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to the same reference sets as TAXOALIGN experiments (per-instance: ~131 refs on average).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Automated survey/outline generation</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Topical outlines / organization patterns; in evaluations tends to produce very large trees that diverge structurally from gold taxonomies.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Appendix contains example STORM-generated taxonomy trees showing verbosity and structural differences vs gold.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same automated metrics and human evaluation protocol as TAXOALIGN.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>STORM baseline (prompted with GPT-4o-mini) reported ∆ ≈ 6.151, BLEU-2 ≈ 0.0012, ROUGE-L ≈ 0.1349, BERTScore ≈ 0.8166, NSR ≈ 1.0727, NER ≈ 0.1539, LLM-judge mean ≈ 2.20 (5-point scale).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to TAXOALIGN; STORM shows higher ∆ (overly-branched trees) and lower semantic/LLM/human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simulated-conversation pre-writing without the focused knowledge-slice + instruction-tuning + refinement pipeline yields structurally less aligned and less coherent taxonomies.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Over-generation / excessive branching, lower semantic alignment, and increased hallucination risk relative to the TAXOALIGN pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4323.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4323.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ArxivDI-GESTables (Newman et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ArxivDI-GESTables: Synthesizing scientific literature into tables using language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system that uses language models to synthesize information from scientific papers into structured tables; cited in the paper as an example of LLMs used for literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArxivDI-GESTables: Synthesizing scientific literature into tables using language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified in this paper's citation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ArxivDI-GESTables (literature-to-table synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses language models to extract structured information from scientific papers and format it into literature-review tables (e.g., methods, datasets, metrics), enabling condensed presentation of empirical findings.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Scientific literature synthesis (general / multiple fields; cited in NLP literature context)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Structured empirical summaries and cross-paper comparisons (tables capturing tasks, datasets, metrics) — patterns and empirical generalizations across studies.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Not enumerated in TAXOALIGN paper; cited as prior art that synthesizes literature into tables.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in TAXOALIGN (referenced work would contain evaluation details).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work as an example of LLM-driven literature synthesis approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as demonstrating LLM capability to synthesize literature into concise tabular summaries; supports argument that LLMs are being used for literature-level synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>TAXOALIGN notes general limitations of LLMs on long contexts and domain-specific reasoning that would also affect literature-to-table systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4323.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4323.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pronesti et al. (2025)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Query-driven document-level scientific evidence extraction / synthesizing biomedical evidence into forest plots</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited works that use LLMs to synthesize biomedical evidence at the document level and present aggregated results (e.g., forest plots), showing LLM use to extract and summarize scientific findings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Query-driven document-level scientific evidence extraction from biomedical studies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified in this paper's citation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-driven evidence extraction / synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prior works cited use LLMs to query biomedical literature, extract study-level numeric and qualitative evidence, and synthesize aggregated summaries/visualizations (e.g., forest plots), enabling cross-study evidence synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Biomedical literature / evidence synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Study-level evidence aggregation and empirical generalizations (synthesizing reported effects across studies), not formal mechanistic laws but empirical patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Paper cites such works as examples; TAXOALIGN does not reproduce their extractions but references them as related literature-synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in TAXOALIGN; referenced works likely evaluate via expert verification and comparison to systematic-review outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned to position TAXOALIGN in the broader literature-synthesis context.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates that LLMs are being used effectively to extract and aggregate biomedical evidence across papers, analogous to TAXOALIGN's use for taxonomy generation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Domain-specific correctness, numeric reasoning, and factual grounding remain key challenges in biomedical evidence extraction with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4323.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4323.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Şahinuç et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that applies LLMs to construct scientific leaderboards and literature-review artifacts automatically, illustrating LLM use to process scholarly papers and extract cross-paper summaries/metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified in TAXOALIGN citation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-driven leaderboard/table construction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLMs to scan literature, extract task/dataset/metric/score information and produce leaderboards or tabular literature summaries automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>NLP / scientific meta-analysis</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Empirical comparisons and patterns across papers (leaderboards, tables summarizing performance metrics), i.e., empirical generalizations and organizing principles.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Not enumerated in TAXOALIGN; cited as evidence of LLMs' utility for cross-paper synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in TAXOALIGN (refer to Şahinuç et al. for details).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cited as related literature-synthesis work rather than directly compared in TAXOALIGN experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows that LLMs can be applied to build structured literature artifacts (leaderboards) that summarize empirical results across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Extraction and factual accuracy, handling inconsistent reporting across papers, and reliable numeric extraction are ongoing issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TaxoAlign: Scholarly Taxonomy Generation Using Language Models', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autosurvey: Large language models can automatically write surveys <em>(Rating: 2)</em></li>
                <li>Surveyx: Academic survey automation via large language models <em>(Rating: 2)</em></li>
                <li>Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing <em>(Rating: 2)</em></li>
                <li>Researcharena: Benchmarking large language models' ability to collect and organize information as research agents <em>(Rating: 2)</em></li>
                <li>ArxivDI-GESTables: Synthesizing scientific literature into tables using language models <em>(Rating: 2)</em></li>
                <li>Query-driven document-level scientific evidence extraction from biomedical studies <em>(Rating: 2)</em></li>
                <li>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards <em>(Rating: 2)</em></li>
                <li>Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4323",
    "paper_id": "paper-282210220",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "TAXOALIGN",
            "name_full": "TAXOALIGN: Scholarly Taxonomy Generation Using Language Models",
            "brief_description": "A three-stage LLM-based pipeline that constructs hierarchical taxonomies from a topic and a set of reference scholarly papers by (1) extracting topic-relevant 'knowledge slices' from each paper, (2) instruction-tuning an LLM to verbalize a taxonomy (taxonomy verbalization), and (3) refining the generated taxonomy with a stronger LLM to ensure grounding and structural alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (Mistral-7B-Instruct, Meta-Llama-3-8B-Instruct, Llama-3.1-Tülu-3-8B, SciLitLLM1.5-7B, GPT-4o-mini)",
            "model_size": "7B / 8B / 7B / 32B (various; see method_description) ",
            "method_name": "TAXOALIGN (Knowledge Slice Creation + Taxonomy Verbalization + Taxonomy Refinement)",
            "method_description": "Three-stage pipeline: (1) Knowledge Slice Creation — an LLM (open models such as Mistral-7B-Instruct or Meta-Llama-3-8B-Instruct) is prompted to identify compact, topic-relevant text segments ('knowledge slices') from each reference paper so that a large set of references can be represented within LLM context limits; (2) Taxonomy Verbalization — an instruction-tuned LLM (they finetune Llama-3.1-Tülu-3-8B and SciLitLLM1.5-7B using QLoRA) is trained on CS-TAXOBENCH examples to generate hierarchical taxonomy trees (max depth 3), producing concise node labels and preserving tree structure; (3) Taxonomy Refinement — a stronger closed-domain model (GPT-4o-mini) is prompted to evaluate and refine parent–child connections, ground nodes in knowledge slices, expand nodes if coverage is insufficient, and remove hallucinated or irrelevant nodes. The pipeline uses explicit prompts (appendices A.2–A.4) and instruction-tuning (QLoRA for 800 steps, context window 16,384) to shape generation.",
            "number_of_papers": "460 survey papers (CS-TAXOBENCH); per-task input: on average ~131 reference papers per taxonomy (additional conference test set: 80 taxonomies, ~71 refs each)",
            "domain_or_field": "Computer Science (scholarly survey papers from ACM Computing Surveys and conference surveys in ACL*/IJCAI)",
            "type_of_laws_extracted": "Hierarchical topic structures / qualitative organizational patterns and conceptual categorizations (i.e., taxonomic principles and subtopic patterns) rather than causal laws; distilled recurring topic groupings, subtopic hierarchies, and design/organization principles implicit in survey literature.",
            "example_laws_extracted": "Examples are taxonomy node labels/subtopics produced for the 'Human Image Generation' topic (e.g., '3D Models and Mapping', 'Generative Adversarial Network', 'Data and Annotation'; also higher-level categories seen in produced trees such as 'DATA-DRIVEN METHODS ON HUMAN IMAGE GENERATION', 'HYBRID METHODS', 'KNOWLEDGE-GUIDED METHODS').",
            "evaluation_method": "Automated structural and semantic metrics (Average Degree Score for structural similarity; level-order traversal followed by corpus-level BLEU-2, ROUGE-L, BERTScore for semantic/lexical alignment; Node Soft Recall (semantic overlap via Sentence-BERT embeddings) and Node Entity Recall (noun-phrase overlap)); LLM-as-a-judge (GPT-4.1) scoring; human evaluation with three domain-expert annotators using 5-point Likert scales and inter-annotator agreement (Krippendorff's α). Spearman correlation computed between human mean scores and LLM-as-judge.",
            "performance_metrics": "TAXOALIGN (best reported configuration: K-slice: Mistral; T-verbalize: Tülu; T-refine: GPT-4o-mini) achieved average-degree score ∆ ≈ 1.668 (close to 1 indicates structural parity), level-order BLEU-2 ≈ 0.0051, ROUGE-L ≈ 0.2974, BERTScore ≈ 0.8517; human mean ratings (structure/content) reported ≈ 3.17 / 2.62 (5-point scale) vs AutoSurvey 2.17 / 2.25; LLM-as-judge / human Spearman's rho = 0.527. For taxonomy-extraction (heading-to-tree) module (manual annotation of 10 papers) extraction precision 83.92%, recall 94.35%, F1 88.83%.",
            "comparison_baseline": "Compared against AutoSurvey (outline generation step), STORM, Topic-only prompts, Topic+Keyphrases, and ablations (TAXOALIGN w/o Taxonomy Verbalization and w/o Taxonomy Refinement). TAXOALIGN outperforms baselines on nearly all metrics (structure, NSR, semantic scores); ablation TAXOALIGN w/o verbalization/refinement performed second best, indicating knowledge-slice importance.",
            "key_findings": "1) Knowledge-slice extraction is crucial: a pipeline using knowledge slices but no further verbalization/refinement performed strongly, showing the value of focused per-paper summarization. 2) Instruction-tuning (taxonomy verbalization) and subsequent LLM refinement reduce excessive branching and hallucinated nodes and improve node-label quality/semantic coherence. 3) LLMs can produce taxonomies structurally closer to human-written trees but still show low lexical overlap (BLEU/ROUGE values remain low), indicating a remaining gap between human and machine outputs. 4) LLM-as-a-judge correlates moderately well with human judgments (rho ≈ 0.527), making it a useful proxy.",
            "challenges_limitations": "Context-window and long-document reasoning limits; hallucination and generation of irrelevant or overly verbose nodes; low lexical overlap even when structural alignment is reasonable; factual errors in node labels; variability across LLMs and configurations; dataset curated from a single journal/time window (ACM CSUR 2020–2024) limiting generality; reliance on available reference metadata (Semantic Scholar coverage filter).",
            "uuid": "e4323.0",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Knowledge-Slice Creation (Mistral-7B)",
            "name_full": "Knowledge Slice Creation using Mistral-7B-Instruct-v0.3",
            "brief_description": "An LLM-based subroutine in TAXOALIGN that identifies short, topic-relevant excerpts ('knowledge slices') from each reference paper to condense the information so many references can be processed despite context-window constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.3",
            "model_size": "7B",
            "method_name": "Knowledge Slice Creation",
            "method_description": "Prompt an LLM per document with the taxonomy topic and the document; instruct it to return compact highlights strictly grounded in the document that are highly relevant to the topic. These slices are used downstream to represent each reference within the model context. The paper uses Mistral-7B (and alternately Meta-Llama-3-8B-Instruct) for this stage.",
            "number_of_papers": "Applied to the reference sets for 460 taxonomies (on average ~131 reference papers per taxonomy); additional 80 conference-taxonomy test instances (~71 refs each).",
            "domain_or_field": "Computer Science (survey reference papers)",
            "type_of_laws_extracted": "Compresses evidence into topical highlights that enable extraction of hierarchical topic patterns; not directly causal laws but supports deriving organizational patterns.",
            "example_laws_extracted": "No formal laws; produces concise document highlights that feed taxonomy construction (examples not enumerated in paper beyond taxonomy node examples).",
            "evaluation_method": "Ablation studies: TAXOALIGN w/o verbalization/refinement (uses knowledge slices + prompting) ranks second overall, showing the stage's importance; manual error analysis noted verbosity and irrelevance issues when relying solely on slices.",
            "performance_metrics": "Reported ablation performance indicates strong contribution (the ablation baseline using knowledge slices is second-best across many metrics in Table 2), but specific per-model numeric breakdowns are reported in the paper's tables for various configurations.",
            "comparison_baseline": "Compared implicitly to Keyphrase-only baseline and Topic-only; knowledge-slice approach outperforms keyphrase-only in downstream taxonomy quality.",
            "key_findings": "Knowledge-slice extraction enables scaling to many references and is a major reason TAXOALIGN performs well; however, direct generation from slices (without verbalization/refinement) leads to verbose or sometimes off-topic nodes.",
            "challenges_limitations": "Quality depends on prompt design and model capability; slices can be verbose or repeat content and may still contain irrelevant information; slices do not eliminate hallucination risk downstream.",
            "uuid": "e4323.1",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Taxonomy Verbalization (Tülu / SciLitLLM)",
            "name_full": "Taxonomy Verbalization using instruction-tuned Llama-3.1-Tülu-3-8B and SciLitLLM1.5-7B",
            "brief_description": "Instruction-tuning LLMs on example taxonomy generation tasks (CS-TAXOBENCH) so they learn to produce concise, structured taxonomy trees from knowledge slices and topic prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-Tülu-3-8B; SciLitLLM1.5-7B",
            "model_size": "8B; 7B",
            "method_name": "Taxonomy Verbalization (instruction tuning, QLoRA)",
            "method_description": "Instruction-tune LLMs (QLoRA: 4-bit quant, LoRA adapters) for 800 steps with large context (16,384) using Alpacalike instruction format and CS-TAXOBENCH examples. The tuned model is prompted with the topic and knowledge slices and constrained to output taxonomy trees with max depth 3, short node labels, and leaf nodes that reflect single topics. The verbalization stage preserves tree structure and improves label concision relative to direct prompting.",
            "number_of_papers": "Tuned on CS-TAXOBENCH training split (400 instances mentioned for training experiments) where each instance included a set of reference papers (avg ~131 per instance).",
            "domain_or_field": "Computer Science taxonomy generation",
            "type_of_laws_extracted": "Generates hierarchical organizational principles (taxonomic categories and subtopics) derived from corpus-wide patterns across references.",
            "example_laws_extracted": "Produces taxonomy nodes and hierarchical groupings (examples in paper: topic-specific node labels and trees; e.g., categories in 'Human Image Generation' taxonomy).",
            "evaluation_method": "Compared to direct-prompt baselines and ablations using the same evaluation metrics (average degree, level-order BLEU/ROUGE/BERTScore, NSR/NER, human evaluation).",
            "performance_metrics": "Instruction-tuned verbalizers (Tülu and SciLitLLM variants) combined with refinement achieved best combined metrics (∆ ≈ 1.668 in best configuration; improved ROUGE-L and BERTScore vs baselines). Exact numeric differences shown in Table 2 and appendices.",
            "comparison_baseline": "Compared to prompting-only baselines (Topic-only, Topic+Keyphrases) and to pipelines without verbalization; verbalization improves structure preservation and label quality.",
            "key_findings": "Instruction-tuning helps preserve taxonomy structure and produce concise labels; it reduces verbosity and excessive branching seen in prompt-only outputs.",
            "challenges_limitations": "Finetuning requires curated training taxonomies (CS-TAXOBENCH) and compute (QLoRA steps). Despite improvements, factual errors and hallucinated labels can remain and further refinement is necessary.",
            "uuid": "e4323.2",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Taxonomy Refinement (GPT-4o-mini)",
            "name_full": "Taxonomy Refinement using GPT-4o-mini",
            "brief_description": "A final LLM-based refinement stage that inspects a generated taxonomy and its grounding knowledge slices to correct parent–child relations, expand coverage or prune irrelevant nodes, improving structural alignment and grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini (closed-domain stronger LLM used as judge/refiner)",
            "model_size": null,
            "method_name": "Taxonomy Refinement",
            "method_description": "Prompt a stronger reasoning-capable LLM with the generated taxonomy and the knowledge slices; instruct it to check grounding of each node, refine node labels/connections, expand nodes if coverage is too low, and remove hallucinated nodes. This stage is applied after verbalization to improve structure and coherence.",
            "number_of_papers": "Refinement applied to taxonomy instances produced for the 460 taxonomies in CS-TAXOBENCH (per-instance reference sets as above).",
            "domain_or_field": "Computer Science taxonomy generation",
            "type_of_laws_extracted": "Improves the extraction of hierarchical organizational patterns by grounding nodes in cited literature.",
            "example_laws_extracted": "Refinement produces corrected parent–child relations and pruned node sets—no new formal 'laws' but improved taxonomy quality (examples described qualitatively in paper).",
            "evaluation_method": "Ablation comparison: TAXOALIGN with refinement outperforms same pipeline without refinement on average-degree and semantic metrics; human eval and LLM-as-a-judge scores improve with refinement.",
            "performance_metrics": "Configurations including GPT-4o-mini refinement yielded the best overall scores in main tables (improved ∆ closer to 1 and better NSR/BERTScore); exact config numbers are reported in Table 2 (best ∆ ≈ 1.668).",
            "comparison_baseline": "Compared to identical pipelines without refinement; refinement reduces branching and hallucinated nodes and leads to higher judged coherence.",
            "key_findings": "Refinement with a stronger LLM consistently improves structural and semantic alignment with human taxonomies and alleviates some hallucinations incurred in earlier stages.",
            "challenges_limitations": "Refinement depends on the reasoning ability and factual safety of the stronger LLM and may still leave layer-wise exact-label mismatches; introduces dependency on closed-domain (potentially non-open) models.",
            "uuid": "e4323.3",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "AutoSurvey (baseline)",
            "name_full": "AutoSurvey: Large language models can automatically write surveys",
            "brief_description": "Prior work that generates survey outlines by dividing references into groups, producing multiple outlines, and amalgamating them into a comprehensive outline using an LLM; used here as a baseline (outline-generation step only).",
            "citation_title": "Autosurvey: Large language models can automatically write surveys",
            "mention_or_use": "use",
            "model_name": "Prompted with GPT-4o-mini in this paper's baseline runs",
            "model_size": null,
            "method_name": "AutoSurvey (outline generation baseline)",
            "method_description": "AutoSurvey randomly partitions reference papers into groups to produce multiple per-group outlines; LLMs combine these outlines into a single outline. For fairness, the authors provided the actual reference papers to AutoSurvey and ran only its outline generation component as a baseline.",
            "number_of_papers": "Applied to the same reference sets as TAXOALIGN experiments (per-instance: ~131 refs on average).",
            "domain_or_field": "Automated survey generation from academic papers (Computer Science)",
            "type_of_laws_extracted": "Outline / topical organization (high-level survey structure), but tends to produce overly large/verbose trees in this paper's evaluation.",
            "example_laws_extracted": "Baseline-generated nodes shown in Appendix figures; examples are more verbose and diverge structurally from gold taxonomies.",
            "evaluation_method": "Evaluated with the same structural and semantic metrics (average degree, level-order traversal BLEU/ROUGE/BERTScore, NSR/NER) and human evaluation.",
            "performance_metrics": "AutoSurvey baseline (prompted with GPT-4o-mini) reported ∆ ≈ 4.466, BLEU-2 ≈ 0.0016, ROUGE-L ≈ 0.1784, BERTScore ≈ 0.8256, NSR ≈ 1.0903, NER ≈ 0.1982, LLM-judge mean ≈ 2.43 (5-point scale).",
            "comparison_baseline": "This is one of the principal baseline systems in the paper; TAXOALIGN outperforms AutoSurvey on structure, semantic coherence and human-judged quality.",
            "key_findings": "AutoSurvey tends to generate overly-large/branched taxonomies (high ∆), leading to structural divergence and lower judged coherence compared to TAXOALIGN.",
            "challenges_limitations": "Random grouping of references and purely outline-merging approaches can lead to excessive branching, verbosity, and hallucinated or irrelevant nodes.",
            "uuid": "e4323.4",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "STORM (baseline)",
            "name_full": "STORM (pre-writing simulated-conversation outline method)",
            "brief_description": "A baseline survey-generation method that simulates conversational research (pre-writing) to generate draft outlines and then refines them using topic+conversations; used as a baseline in TAXOALIGN experiments.",
            "citation_title": "STORM",
            "mention_or_use": "use",
            "model_name": "Prompted with GPT-4o-mini in this paper's baseline runs",
            "model_size": null,
            "method_name": "STORM (outline generation baseline)",
            "method_description": "Generates an initial draft outline by prompting an LLM with the topic, then augments with simulated conversations about the topic and reference material to generate a final outline. In the paper, the pipeline was provided with reference papers and run as a baseline.",
            "number_of_papers": "Applied to the same reference sets as TAXOALIGN experiments (per-instance: ~131 refs on average).",
            "domain_or_field": "Automated survey/outline generation",
            "type_of_laws_extracted": "Topical outlines / organization patterns; in evaluations tends to produce very large trees that diverge structurally from gold taxonomies.",
            "example_laws_extracted": "Appendix contains example STORM-generated taxonomy trees showing verbosity and structural differences vs gold.",
            "evaluation_method": "Same automated metrics and human evaluation protocol as TAXOALIGN.",
            "performance_metrics": "STORM baseline (prompted with GPT-4o-mini) reported ∆ ≈ 6.151, BLEU-2 ≈ 0.0012, ROUGE-L ≈ 0.1349, BERTScore ≈ 0.8166, NSR ≈ 1.0727, NER ≈ 0.1539, LLM-judge mean ≈ 2.20 (5-point scale).",
            "comparison_baseline": "Compared directly to TAXOALIGN; STORM shows higher ∆ (overly-branched trees) and lower semantic/LLM/human scores.",
            "key_findings": "Simulated-conversation pre-writing without the focused knowledge-slice + instruction-tuning + refinement pipeline yields structurally less aligned and less coherent taxonomies.",
            "challenges_limitations": "Over-generation / excessive branching, lower semantic alignment, and increased hallucination risk relative to the TAXOALIGN pipeline.",
            "uuid": "e4323.5",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "ArxivDI-GESTables (Newman et al.)",
            "name_full": "ArxivDI-GESTables: Synthesizing scientific literature into tables using language models",
            "brief_description": "A prior system that uses language models to synthesize information from scientific papers into structured tables; cited in the paper as an example of LLMs used for literature synthesis.",
            "citation_title": "ArxivDI-GESTables: Synthesizing scientific literature into tables using language models",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified in this paper's citation)",
            "model_size": null,
            "method_name": "ArxivDI-GESTables (literature-to-table synthesis)",
            "method_description": "Uses language models to extract structured information from scientific papers and format it into literature-review tables (e.g., methods, datasets, metrics), enabling condensed presentation of empirical findings.",
            "number_of_papers": null,
            "domain_or_field": "Scientific literature synthesis (general / multiple fields; cited in NLP literature context)",
            "type_of_laws_extracted": "Structured empirical summaries and cross-paper comparisons (tables capturing tasks, datasets, metrics) — patterns and empirical generalizations across studies.",
            "example_laws_extracted": "Not enumerated in TAXOALIGN paper; cited as prior art that synthesizes literature into tables.",
            "evaluation_method": "Not specified in TAXOALIGN (referenced work would contain evaluation details).",
            "performance_metrics": null,
            "comparison_baseline": "Mentioned in related work as an example of LLM-driven literature synthesis approaches.",
            "key_findings": "Cited as demonstrating LLM capability to synthesize literature into concise tabular summaries; supports argument that LLMs are being used for literature-level synthesis.",
            "challenges_limitations": "TAXOALIGN notes general limitations of LLMs on long contexts and domain-specific reasoning that would also affect literature-to-table systems.",
            "uuid": "e4323.6",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Pronesti et al. (2025)",
            "name_full": "Query-driven document-level scientific evidence extraction / synthesizing biomedical evidence into forest plots",
            "brief_description": "Cited works that use LLMs to synthesize biomedical evidence at the document level and present aggregated results (e.g., forest plots), showing LLM use to extract and summarize scientific findings.",
            "citation_title": "Query-driven document-level scientific evidence extraction from biomedical studies",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified in this paper's citation)",
            "model_size": null,
            "method_name": "LLM-driven evidence extraction / synthesis",
            "method_description": "Prior works cited use LLMs to query biomedical literature, extract study-level numeric and qualitative evidence, and synthesize aggregated summaries/visualizations (e.g., forest plots), enabling cross-study evidence synthesis.",
            "number_of_papers": null,
            "domain_or_field": "Biomedical literature / evidence synthesis",
            "type_of_laws_extracted": "Study-level evidence aggregation and empirical generalizations (synthesizing reported effects across studies), not formal mechanistic laws but empirical patterns.",
            "example_laws_extracted": "Paper cites such works as examples; TAXOALIGN does not reproduce their extractions but references them as related literature-synthesis tasks.",
            "evaluation_method": "Not detailed in TAXOALIGN; referenced works likely evaluate via expert verification and comparison to systematic-review outputs.",
            "performance_metrics": null,
            "comparison_baseline": "Mentioned to position TAXOALIGN in the broader literature-synthesis context.",
            "key_findings": "Demonstrates that LLMs are being used effectively to extract and aggregate biomedical evidence across papers, analogous to TAXOALIGN's use for taxonomy generation.",
            "challenges_limitations": "Domain-specific correctness, numeric reasoning, and factual grounding remain key challenges in biomedical evidence extraction with LLMs.",
            "uuid": "e4323.7",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Şahinuç et al. (2024)",
            "name_full": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards",
            "brief_description": "A cited work that applies LLMs to construct scientific leaderboards and literature-review artifacts automatically, illustrating LLM use to process scholarly papers and extract cross-paper summaries/metrics.",
            "citation_title": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified in TAXOALIGN citation)",
            "model_size": null,
            "method_name": "LLM-driven leaderboard/table construction",
            "method_description": "Uses LLMs to scan literature, extract task/dataset/metric/score information and produce leaderboards or tabular literature summaries automatically.",
            "number_of_papers": null,
            "domain_or_field": "NLP / scientific meta-analysis",
            "type_of_laws_extracted": "Empirical comparisons and patterns across papers (leaderboards, tables summarizing performance metrics), i.e., empirical generalizations and organizing principles.",
            "example_laws_extracted": "Not enumerated in TAXOALIGN; cited as evidence of LLMs' utility for cross-paper synthesis.",
            "evaluation_method": "Not specified in TAXOALIGN (refer to Şahinuç et al. for details).",
            "performance_metrics": null,
            "comparison_baseline": "Cited as related literature-synthesis work rather than directly compared in TAXOALIGN experiments.",
            "key_findings": "Shows that LLMs can be applied to build structured literature artifacts (leaderboards) that summarize empirical results across papers.",
            "challenges_limitations": "Extraction and factual accuracy, handling inconsistent reporting across papers, and reliable numeric extraction are ongoing issues.",
            "uuid": "e4323.8",
            "source_info": {
                "paper_title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
                "publication_date_yy_mm": "2025-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autosurvey: Large language models can automatically write surveys",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Surveyx: Academic survey automation via large language models",
            "rating": 2,
            "sanitized_title": "surveyx_academic_survey_automation_via_large_language_models"
        },
        {
            "paper_title": "Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing",
            "rating": 2,
            "sanitized_title": "surveyforge_on_the_outline_heuristics_memorydriven_generation_and_multidimensional_evaluation_for_automated_survey_writing"
        },
        {
            "paper_title": "Researcharena: Benchmarking large language models' ability to collect and organize information as research agents",
            "rating": 2,
            "sanitized_title": "researcharena_benchmarking_large_language_models_ability_to_collect_and_organize_information_as_research_agents"
        },
        {
            "paper_title": "ArxivDI-GESTables: Synthesizing scientific literature into tables using language models",
            "rating": 2,
            "sanitized_title": "arxivdigestables_synthesizing_scientific_literature_into_tables_using_language_models"
        },
        {
            "paper_title": "Query-driven document-level scientific evidence extraction from biomedical studies",
            "rating": 2,
            "sanitized_title": "querydriven_documentlevel_scientific_evidence_extraction_from_biomedical_studies"
        },
        {
            "paper_title": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards",
            "rating": 2,
            "sanitized_title": "efficient_performance_tracking_leveraging_large_language_models_for_automated_construction_of_scientific_leaderboards"
        },
        {
            "paper_title": "Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation",
            "rating": 1,
            "sanitized_title": "transforming_science_with_large_language_models_a_survey_on_aiassisted_scientific_discovery_experimentation_content_generation_and_evaluation"
        }
    ],
    "cost": 0.023622499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TAXOALIGN: Scholarly Taxonomy Generation Using Language Models
20 Oct 2025</p>
<p>Avishek Lahiri avisheklahiri2014@gmail.com 
Indian Association for the Cultivation of Science
KolkataIndia</p>
<p>Yufang Hou yufang.hou@it-u.at 
IT:U Interdisciplinary Transformation University Austria
LinzAustria</p>
<p>Debarshi Kumar Sanyal debarshi.sanyal@iacs.res.in 
Indian Association for the Cultivation of Science
KolkataIndia</p>
<p>Aaron Grattafiori 
Abhimanyu Dubey 
Abhinav Jauhri 
Abhinav Pandey 
Abhishek Kadian 
Ahmad Al- Dahle 
Aiesha Letman 
Akhil Mathur 
Alan Schel- Ten 
Alex Vaughan 
Amy Yang 
Angela Fan 
Anirudh Goyal 
Anthony Hartshorn 
Aobo Yang 
Archi Mi- Tra 
Archie Sravankumar 
Artem Korenev 
Arthur Hinsvark 
Arun Rao 
Aston Zhang 
Aurelien Ro- Driguez 
Austen Gregerson 
Ava Spataru 
Baptiste Roziere 
Bethany Biron 
Binh Tang 
Bobbie Chern 
Charlotte Caucheteux 
Chaya Nayak 
Chloe Bi 
Chris Marra 
Chris Mcconnell 
Christian Keller 
Christophe Touret 
Chunyang Wu 
Corinne Wong 
Cristian Canton Ferrer 
Cyrus Nikolaidis 
Damien Al- Lonsius 
Daniel Song 
Danielle Pintz 
Danny Livshits 
Danny Wyatt 
David Esiobu 
Dhruv Choudhary 
Dhruv Mahajan 
Diego Garcia-Olano 
Diego Perino 
Dieuwke Hupkes 
Egor Lakomkin 
Ehab Albadawy 
Elina Lobanova 
Emily Dinan 
Eric Michael Smith 
Filip Radenovic 
Francisco Guzmán 
Frank Zhang 
Gabriel Synnaeve 
Gabrielle Lee 
Georgia Lewis 
Govind Thattai 
Graeme Nail 
Mi- Alon Gregoire 
Guan Pang 
Guillem Cucurell 
Hailey Nguyen 
Hannah Korevaar 
Hu Xu 
Hugo Touvron 
ImanolIliyan Zarov 
Arrieta Ibarra 
Isabel Kloumann 
Is- Han Misra 
Ivan Evtimov 
Jack Zhang 
Jade Copet 
Jaewon Lee 
Jan Geffert 
Jana Vranes 
Jason Park 
Jay Mahadeokar 
Jeet Shah 
Jelmer Van Der Linde 
Jennifer Billock 
Jenny Hong 
Jenya Lee 
Jeremy Fu 
Jianfeng Chi 
Jianyu Huang 
Jiawen Liu 
Jie Wang 
Jiecao Yu 
Joanna Bitton 
Joe Spisak 
Jongsoo Park 
Joseph Rocca 
Joshua Johnstun 
Joshua Saxe 
Jun- Teng Jia 
Kalyan Vasuden Alwala 
Karthik Prasad 
Kartikeya Upasani 
Kate Plawiak 
Keqian Li 
Kenneth Heafield 
Kevin Stone 
Khalid El-Arini 
Krithika Iyer 
Kshitiz Malik 
Kuenley Chiu 
Kunal Bhalla 
Kushal Lakhotia 
Lauren Rantala-Yeary 
Laurens Van Der Maaten 
Lawrence Chen 
Liang Tan 
Liz Jenkins 
Louis Martin 
Lovish Madaan 
Lubo Malo 
Lukas Blecher 
Lukas Landzaat 
Luke De Oliveira 
Madeline Muzzi 
Mahesh Pasupuleti 
Mannat Singh 
Manohar Paluri 
Marcin Kardas 
Maria Tsimpoukelli 
Mathew Oldham 
Mathieu Rita 
Maya Pavlova 
Melanie Kam- Badur 
Mike Lewis 
MiteshMin Si 
Kumar Singh 
Mona Hassan 
Naman Goyal 
Narjes Torabi 
Niko- Lay Bashlykov 
Nikolay Bogoychev 
Niladri Chatterji 
Ning Zhang 
Olivier Duchenne 
Onur Çelebi 
Patrick Alrassy 
Pengchuan Zhang 
PetarPengwei Li 
Peter Weng 
Prajjwal Bhargava 
Pratik Dubal 
PunitPraveen Krishnan 
Singh Koura 
Puxin Xu 
Qing He 
Qingxiao Dong 
Ragavan Srinivasan 
Raj Ganapathy 
Ramon Calderer 
Ricardo Silveira Cabral 
Robert Stojnic 
Roberta Raileanu 
Rohan Maheswari 
Rohit Girdhar 
Rohit Patel 
Romain Sauvestre 
Ron- Nie Polidoro 
Roshan Sumbaly 
Ross Taylor 
Ruan Silva 
Rui Hou 
Rui Wang 
Saghar Hosseini 
Sa- Hana Chennabasappa 
Sanjay Singh 
Sean Bell 
Seo- Hyun Sonia Kim 
Sergey Edunov 
Shaoliang Nie 
Sha- Ran Narang 
Sharath Raparthy 
Sheng Shen 
Shengye Wan 
Shruti Bhosale 
Shun Zhang 
Simon Van- Denhende 
Soumya Batra 
Spencer Whitman 
Sten Sootla 
Stephane Collot 
Suchin Gururangan 
Syd- Ney Borodinsky 
Tamar Herman 
Tara Fowler 
Tarek Sheasha 
Thomas Georgiou 
Thomas Scialom 
Tobias Speckbacher 
Todor Mihaylov 
Tong Xiao 
Ujjwal Karn 
Vedanuj Goswami 
Vibhor Gupta 
Vignesh Ramanathan 
Viktor Kerkez 
Vincent Gonguet 
Vir- Ginie Do 
Vish Vogeti 
Vítor Albiero 
Vladan Petro- Vic 
Weiwei Chu 
Wenhan Xiong 
Wenyin Fu 
Yasmine Gaur 
Yi Babaei 
Yiwen Wen 
Yuchen Song 
Yue Zhang 
Yuning Li 
Zacharie Delpierre Mao 
Zheng Coudert 
Zhengxing Yan 
Zoe Chen 
Aaditya Papakipos 
Aayushi Singh 
Abha Sri- Vastava 
Adam Jain 
Adam Kelsey 
Adithya Shajnfeld 
Adolfo Gangidi 
Ahuva Victoria 
Ajay Goldstand 
Ajay Menon 
Alex Sharma 
Alexei Boesenberg 
Allie Baevski 
Amanda Feinstein 
Amit Kallet 
Amos San- Gani 
Anam Teo 
Andrei Yunus 
An- Dres Lupu 
Andrew Alvarado 
Andrew Caples 
Andrew Gu 
Andrew Ho 
Andrew Poulton 
AnkitRamchan- Dani Ryan 
Annie Dong 
Annie Franco 
Anuj Goyal 
Apara- Jita Saraf 
Arkabandhu Chowdhury 
Ashley Gabriel 
Ashwin Bharambe 
Assaf Eisenman 
Azadeh Yaz- Dan 
Beau James 
Ben Maurer 
Benjamin Leonhardi 
Bernie Huang 
Beth Loyd 
Beto De Paola 
Bhargavi Paranjape 
Bing Liu 
Bo Wu 
Boyu Ni 
Braden Han- Cock 
Bram Wasti 
Brandon Spence 
Brani Stojkovic 
Brian Gamido 
Britt Montalvo 
Carl Parker 
Carly Burton 
Catalina Mejia 
Ce Liu 
Changhan Wang 
Changkyu Kim 
Chao Zhou 
Chester Hu 
Ching- Hsiang Chu 
Chris Cai 
Chris Tindal 
Christoph Fe- Ichtenhofer 
Cynthia Gao 
Damon Civin 
Dana Beaty 
Daniel Kreymer 
Daniel Li 
David Adkins 
David Xu 
Davide Testuggine 
Delia David 
Devi Parikh 
Elaine Montgomery 
Eleonora Presani 
Emily Hahn 
Emily Wood 
Eric-Tuan Le 
Erik Brinkman 
Este- Ban Arcaute 
Evan Dunbar 
Evan Smothers 
Fei Sun 
Felix Kreuk 
Feng Tian 
Filippos Kokkinos 
Firat Ozgenel 
Francesco Caggioni 
Frank Kanayet 
Frank Seide 
Gabriela Medina Florez 
Gabriella Schwarz 
Gada Badeer 
Georgia Swee 
Gil Halpern 
Grant Herman 
Grigory Sizov 
Guangyi 
Guna Zhang 
Hakan Lakshminarayanan 
Hamid Inan 
Han Shojanaz- Eri 
Hannah Zou 
Hanwen Wang 
Haroun Zha 
Harrison Habeeb 
Helen Rudolph 
Henry Suk 
Hunter As- Pegren 
Hongyuan Goldman 
Ibrahim Zhan 
Igor Damlaj 
Igor Molybog 
Ilias Tufanov 
Leontiadis 
Nifer Chan 
Jenny Zhen 
Jeremy Reizenstein 
Jeremy Teboul 
Jessica Zhong 
Jian Jin 
Jingyi Yang 
Joe Cummings 
Jon Carvill 
Jon Shepard 
Jonathan Mc- Phie 
Jonathan Torres 
Josh Ginsburg 
Junjie Wang 
Kai Wu 
Kam Hou 
Karan Saxena 
Kartikay Khan- Delwal 
Katayoun Zand 
Kathy Matosich 
Kaushik Veeraraghavan 
Kelly Michelena 
Ki- Ran Jagadeesh 
Kun Huang 
Kunal Chawla 
Kyle Huang 
Lailin Chen 
Lavender A,Lakshya Garg 
Leandro Silva 
Lee Bell 
Lei Zhang 
Liangpeng Guo 
Licheng Yu 
Liron Moshkovich 
Luca Wehrst- Edt 
Madian Khabsa 
Manav Avalani 
Manish Bhatt 
Martynas Mankus 
Matan Hasson 
Matthew Lennie 
Matthias Reso 
Maxim Groshev 
Maxim Naumov 
Maya Lathi 
Meghan Keneally 
Miao Liu 
Michael L Seltzer 
Michal Valko 
MihirMichelle Restrepo 
Mik Vyatskov 
Mikayel Samvelyan 
Mike Clark 
Mike Macey 
Mike Wang 
Miquel Jubert Hermoso 
Mo Metanat 
Mohammad Rastegari 
Munish Bansal 
Nandhini Santhanam 
Natascha Parks 
Natasha White 
Navyata Bawa 
Nayan Singhal 
Nick Egebo 
Nicolas Usunier 
NikolayNikhil Mehta 
Pavlovich Laptev 
Ning Dong 
Norman Cheng 
Oleg Chernoguz 
Olivia Hart 
Omkar Salpekar 
Ozlem Kalinli 
Parkin Kent 
Parth Parekh 
Paul Saab 
Pavan Balaji 
Pe- Dro Rittner 
Philip Bontrager 
Pierre Roux 
Piotr Dollar 
Polina Zvyagina 
Prashant Ratanchandani 
Pritish Yuvraj 
Qian Liang 
Rachad Alao 
Rachel Rodriguez 
Rafi Ayub 
Raghotham Murthy 
Raghu Nayani 
Rahul Mitra 
Rangaprabhu Parthasarathy 
Raymond Li 
Rebekkah Hogan 
Robin Battey 
Rocky Wang 
Russ Howes 
Ruty Rinott 
Sachin Mehta 
Sachin Siby 
Jayesh Sai 
Samyak Bondu 
Sara Datta 
Sara Chugh 
Sargun Hunt 
Sasha Dhillon 
Satadru Sidorov 
Saurabh Pan 
Saurabh Mahajan 
Seiji Verma 
Sharadh Yamamoto 
Shaun Ramaswamy 
Shaun Lind- Say 
Sheng Lindsay 
Shenghao Feng 
ShengxinCindy Lin 
Shishir Zha 
Shiva Patil 
Shuqiang Shankar 
Shuqiang Zhang 
Sinong Zhang 
Sneha Wang 
Soji Agarwal 
Soumith Sajuyigbe 
Stephanie Chintala 
Stephen Max 
Steve Chen 
Steve Kehoe 
Sudarshan Satterfield 
Sumit Govindaprasad 
Summer Gupta 
Sungmin Deng 
Sunny Cho 
Suraj Virk 
Sy Subramanian 
Sydney Choudhury 
Tal Goldman 
Tamar Remez 
Tamara Glaser 
Thilo Best 
Thomas Koehler 
Tianhe Robinson 
Tianjun Li 
Tim Zhang 
Timothy Matthews 
Tzook Chou 
Varun Shaked 
Victoria Vontimitta 
Victoria Ajayi 
Vijai Montanez 
Vinay Satish Mohan 
Vishal Kumar 
Vlad Mangla 
Vlad Ionescu 
Vlad Tiberiu Poenaru 
Vladimir Mihailescu 
Wei Ivanov 
Wenchen Li 
Wen- Wen Wang 
Wes Jiang 
Will Bouaziz 
Xiaocheng Constable 
Xiaojian Tang 
Xiaolan Wu 
Xilun Wang 
Xinbo Wu 
Yaniv Gao 
Yanjun Kleinman 
Ye Chen 
Ye Hu 
Ye Jia 
Yenda Qi 
Yilin Li 
Ying Zhang 
Yossi Zhang 
Youngjin Adi 
Yu Nam 
Yu Wang 
Yuchen Zhao 
Yundi Hao 
Yunlu Qian 
Yuzi Li 
Zach He 
Zachary Rait 
Zef Devito 
Zhaoduo Rosnbrick 
Zhenyu Wen 
Yang </p>
<p>Whit-ney Meers
Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xide Xia, Yaelle Gold-schlagXi-aofang Wang, Xin-feng Xie, Xuchao Jia, Xuewei Wang, Yashesh</p>
<p>Diana Liskovich
Didem Foss, Dingkang Wang, Dustin Holland, Edward DowlingDuc Le</p>
<p>Eissa Jamil</p>
<p>Itai Gat
Irina-Elena Veliche
Jake Weissman</p>
<p>James Geboski
James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff TangJen</p>
<p>TAXOALIGN: Scholarly Taxonomy Generation Using Language Models
20 Oct 2025C427EE0A78356B844EBD9C9103F8BDC9arXiv:2510.17263v1[cs.CL]
Taxonomies play a crucial role in helping researchers structure and navigate knowledge in a hierarchical manner.They also form an important part in the creation of comprehensive literature surveys.The existing approaches to automatic survey generation do not compare the structure of the generated surveys with those written by human experts.To address this gap, we present our own method for automated taxonomy creation that can bridge the gap between human-generated and automaticallycreated taxonomies.For this purpose, we create the CS-TAXOBENCH benchmark which consists of 460 taxonomies that have been extracted from human-written survey papers.We also include an additional test set of 80 taxonomies curated from conference survey papers.We propose TAXOALIGN, a threephase topic-based instruction-guided method for scholarly taxonomy generation.Additionally, we propose a stringent automated evaluation framework that measures the structural alignment and semantic coherence of automatically generated taxonomies in comparison to those created by human experts.We evaluate our method and various baselines on CS-TAXOBENCH, using both automated evaluation metrics and human evaluation studies.The results show that TAXOALIGN consistently surpasses the baselines on nearly all metrics.The code and data can be found at https: //github.com/AvishekLahiri/TaxoAlign.</p>
<p>Introduction</p>
<p>In scientific research, a taxonomy is constructed around a well-defined topic to integrate relevant research findings under a unified framework, thereby facilitating deeper understanding among researchers and industry practitioners alike.In the general domain, taxonomies have been proven to be useful tools (Wang et al., 2017), which exhibit the capability of enhancing the performance of various Natural Language Processing (NLP) and Information Retrieval (IR) tasks such as question answering (Harabagiu et al., 2003;Yang et al., 2017), textual entailment (Geffet and Dagan, 2005), personalized recommendation (Zhang et al., 2014), query understanding (Hua et al., 2017), information extraction (Hou et al., 2019;Şahinuç et al., 2024) and knowledge graph construction (Hou et al., 2021;Mondal et al., 2021).Taxonomies have also found a place in real-world deployment applications such as biomedical systems (Köhler et al., 2013), information management (Nickerson et al., 2013) and e-commerce (Aanen et al., 2015;Mao et al., 2020).</p>
<p>In this paper, we conceptualize the task of automated scholarly taxonomy generation.When given the taxonomy topic and a set of related reference papers, the task for the model is to reason over this large set of reference documents and generate a taxonomy tree that is both concise and provides maximal coverage of the reference documents.Automating this process reduces the time, effort, and energy Figure 2: A comparison of a gold standard taxonomy tree and a generated taxonomy tree using TAXOALIGN for the topic "Human Image Generation: A Comprehensive Survey".The generated taxonomy shown here uses Mistral-7B-Instruct-v0.3 for creation of knowledge slices and Llama-3.1-Tülu-3-8B for the taxonomy verbalization component and GPT-4o-mini for refining the generated taxonomy.</p>
<p>researchers spend organizing research within a specific topic.</p>
<p>While Large Language Models (LLMs) are increasingly applied across a wide range of tasks, they face notable challenges in understanding and performing domain-specific tasks (Li et al., 2024;Cai et al., 2025).Additionally, their reasoning over long contexts is limited by constraints in context window size (Liu et al., 2024).In our initial pilot study, we observed that prompting LLMs could generate some topic-related sub-topics, but they were not remotely aligned to the human-written taxonomy.For example, "3D Models and Mapping", "Generative Adversarial Network" and "Data and Annotation" were the first-level nodes/sub-topics that were generated by Tülu3 (Lambert et al., 2025) for the topic "Human Image Generation: A Comprehensive Survey" when supplied with all the summaries of the cited documents.In general, the generated nodes were distantly relevant to the topic but were not at close to the human-written ones as shown in Figure 2 (left).</p>
<p>Recent prior work has focused extensively on the end-to-end automated creation of survey papers (Wang et al., 2024;Liang et al., 2025;Yan et al., 2025;Kang and Xiong, 2025).In contrast, scholarly taxonomy generation remains a relatively unexplored area, with no well-articulated opensource data resources currently available for this task.Moreover, prior work does not compare the structure of generated surveys with those authored by human experts.There is also a lack of an evaluation framework capable of assessing both the structural similarity and semantic coherence between automatically generated and human-written taxonomy trees.</p>
<p>To address this gap, we curate and release CS-TAXOBENCH, a comprehensive benchmark that is designed for the task of scholarly taxonomy generation (Section 3).Our benchmark consists of 460 human-written taxonomies (accompanied by their corresponding reference papers) that have been extracted from survey articles published in Computer Science journals in 2020 − 2024.We also curate an additional test set made up of 80 taxonomies extracted from conference survey papers.</p>
<p>We further develop TAXOALIGN, our own intuitive LLM-based pipeline for generating taxonomies (Section 4).TAXOALIGN consists of three parts: Knowledge Slice Creation, Taxonomy Verbalization and Taxonomy Refinement.We compare our method with a range of baselines to show the effectiveness of our method.Figure 1 demonstrates our proposed framework, while Figure 2 (right) shows an example of a taxonomy generated using TAXOALIGN.</p>
<p>Finally, we present an automated evaluation framework for the comparison of taxonomy-tree structures (Section 5).For this purpose, we develop two metrics of our own -the average degree score metric to judge structural similarity and the level-order traversal comparison metric to judge semantic similarity.In addition, we adopt soft recall and entity recall metrics, originally proposed for evaluating outline generation, to assess the quality of the generated taxonomies (Fränti and Mariescu-Istodor, 2023;Shao et al., 2024;Kang and Xiong, 2025).We also employ LLM-as-a-judge for qualitative evaluation.TAXOALIGN outperforms all baselines across nearly all metrics, including human evaluation.</p>
<p>To facilitate future research, we make our code and dataset publicly available at https://github.com/AvishekLahiri/TaxoAlign.</p>
<p>Related Work</p>
<p>Taxonomy Construction.Taxonomy learning has been attempted in NLP through the decades by captalizing on the semantic relations in text (Hearst, 1992;Pantel and Pennacchiotti, 2006;Suchanek et al., 2006;Ponzetto and Strube, 2011;Rios-Alvarado et al., 2013;Dietz et al., 2012;Liu et al., 2012;Diederich and Balke, 2007;Wang et al., 2010;Kang et al., 2016;Kozareva and Hovy, 2010;Velardi et al., 2013).Recent approaches such as HiGTL (Hu et al., 2025) and the method of Martel and Zouaq (2021) introduce graph-and clusteringbased techniques for taxonomy learning.Most of these methods use pattern-based or clusteringbased methods, whereas TAXOALIGN leverages the power of LLMs to construct taxonomies in the scientific domain.</p>
<p>Scientific Survey Generation and Knowledge</p>
<p>Synthesis.Recently, there has been some interest among researchers to generate surveys from a corpus of research papers.AutoSurvey (Wang et al., 2024), SURVEYX (Liang et al., 2025), Re-searchArena (Kang and Xiong, 2025), Qwen-long (Lai et al., 2024) and SURVEYFORGE (Yan et al., 2025) are some of the prominent techniques proposed for this task.In literature-based knowledge synthesis, LLMs have been used to generate scientific leaderboards ( Şahinuç et al., 2024;Timmer et al., 2025), literature review tables (Newman et al., 2024), or to synthesize biomedical evidence in the format of forest plots (Pronesti et al., 2025a,b).Our work focuses on scientific survey taxonomy generation and contributes to the broader agenda of AI for Science (Eger et al., 2025).</p>
<p>CS-TAXOBENCH</p>
<p>Overview</p>
<p>In graph theory, a tree is defined as an undirected connected graph with no cycles.A taxonomy tree T is a tree in which the root represents the taxonomy topic and the child nodes represent sub-topics and grandchild nodes represent more fine-grained topics.Survey papers typically propose a taxonomy which is expanded upon in the sections and sub-sections of the paper.Therefore, in most cases, the structure of the paper closely mirrors the nodes and connections in the taxonomy tree.We leverage this pattern to extract scholarly taxonomies, using our taxonomy extraction module to first derive outlines from survey papers and then analyze them to construct the final taxonomy.</p>
<p>Desiderata</p>
<p>We list out the desiderata we used for selecting taxonomies for inclusion in our benchmark dataset.Our overall goal was to ensure that the selected taxonomies are of high-quality with a logical flow and each node is grounded in a set of reference papers.We decide on the following desiderata for curating our dataset: (1) each taxonomy should be based on a specific research topic, and the taxonomy should provide optimal coverage of the given topic;</p>
<p>(2) the taxonomies should be human-made, i.e., they should not be generated artificially; (3) the taxonomy trees should be multi-layered, i.e., they should have at least two levels.</p>
<p>Automated Taxonomy Extraction</p>
<p>Data Source Selection: Survey papers form a rich resource for taxonomies in scientific literature.Therefore, we select survey papers as our primary source of data.To mitigate the risk of data contamination while ensuring that the papers in consideration are of high quality, we select survey papers from "ACM Computing Surveys" (ACM CSUR), which is a highly reputed journal in the field of Computer Science research with an Impact Factor of 23.8.1 This is one of the top venues in Computer Science that publishes survey papers relating to the areas of computing research and practice.</p>
<p>Research Paper Selection: We select a time frame of five years between 2020 to 2024 for the purpose of the creation of our dataset.A total of 1,165 papers were accepted in ACM CSUR during this period, of which 325 are open-access and 285  have copies available on arXiv. 2 Due to licensing restrictions, we include only the open-access and arXiv articles in our study.</p>
<p>Filtering: We use Docling (Team, 2024) to extract text from the research paper PDFs.Since arXiv papers are not always in a consistent format, we remove those with noisy layouts or Docling parsing errors.After filtering, 499 papers remain.</p>
<p>Reference Paper Matching: We retrieve the abstracts of the reference papers by parsing data from Semantic Scholar 3 , which hosts approximately 214 million research documents.If fewer than 50% of a survey paper's references are available on Semantic Scholar, we exclude that paper from the final version of our proposed datasets.Following this criterion, 39 out of the original 499 papers are excluded, leaving us with a final set of 460 papers.The average percentage of available citations in the final set of papers is shown in Table 1.</p>
<p>Taxonomy Extraction: We extract the headings, subheadings, and sub-subheadings from the retrieved text of the survey paper to construct the taxonomy.The title of the survey paper is treated as the overall taxonomy topic.We discard all headings that contain terms such as "Introduction", "related work", "problem formulation", "summary", "conclusion", "result", "future", "discussion", "background" and "overview" as they do not contribute to the core taxonomy.We also remove those nodes for which we cannot extract reference papers from Semantic Scholar.</p>
<p>Statistics</p>
<p>Dataset Statistics</p>
<p>Our entire benchmark contains 460 taxonomies along with the reference papers that are used to build each taxonomy.The details about the statistics of our benchmark are present in Table 1.On average, there are around 131 reference papers for each taxonomy in our benchmark.</p>
<p>Manual Annotation</p>
<p>To evaluate the taxonomy extraction method, we manually annotate a set of 10 taxonomy trees from as many survey papers.We use a Python package, TreeLib4 , to annotate these taxonomies from their respective survey papers.The papers were selected such that there were explicitly-defined taxonomies in them.To compare the annotated and generated trees, we compare the paths from each node to the root node.Each path is treated as an individual element.The precision, recall and F1 between the annotated and extracted taxonomies were found to be 83.92%,94.35% and 88.83% respectively, thereby demonstrating a high degree of correlation.</p>
<p>The only errors originated due to some general section headers like "Two sea changes in Natural Language Processing" (Liu et al., 2023), which were not present in the annotated trees.</p>
<p>Additional Test Set: Survey Papers in Conferences</p>
<p>We create an additional new test set from conference survey papers for testing on a different distribution of survey papers.For this purpose, we chose survey papers published in 2024 in IJCAI (which has a dedicated survey track) and all the ACL* conferences (ACL, NAACL, EMNLP and EACL).There are a total of 86 survey papers in these conferences in 2024.We carry out our proposed filtering strategies, which narrows down the total number of papers to 80. Therefore, we have 80 taxonomy trees with an average of about 71 reference papers for each tree.This set is solely used as a test set.</p>
<p>TAXOALIGN 4.1 Task Formulation</p>
<p>Given a corpus of documents D and a topic t, the task is to automatically construct a hierarchical taxonomy tree T whose nodes represent relevant topics and subtopics derived from D. The goal for T is to comprehensively and meaningfully categorize the information contained within the entire corpus that relates to topic t.</p>
<p>To solve this task, we propose a method TAXOALIGN, comprising three components: Knowledge Slice Creation, Taxonomy Verbalization and Taxonomy Refinement. Figure 4 shows the three-stage pipeline of our proposed method.</p>
<p>Knowledge Slice Creation</p>
<p>In this step, we use a LLM to identify segments of text within each research article that are highly relevant to the taxonomy topic.We refer to these segments as knowledge slices.</p>
<p>This stage helps in extracting information that guides the model in later steps about the topics and subtopics present in the cited papers related to the taxonomy.More importantly, the number of cited papers in a survey paper is quite extensive, thereby making it quite impossible to fit all the papers in the model's input context window.This step ensures that all the cited papers can be accommodated within the context length of recent LLMs.</p>
<p>Taxonomy Verbalization</p>
<p>We opt for the instruction tuning of an LLM with the most pertinent taxonomy topic-related information from the reference papers that has been extracted in the previous stage.</p>
<p>The main objective is to teach the model to generate meaningful and concise taxonomies which are grounded in the given information, and most importantly, teach the model to learn the structure of the taxonomy trees.Finetuning helps in preserving the structure of the taxonomy in a major way, which is a feature that is lacking in the direct prompting-based methods.</p>
<p>Taxonomy Refinement</p>
<p>The verbalization phase is followed by a refinement stage, which evaluates and refines the connections between the parent and the child nodes.It checks whether each node is grounded in the document knowledge slices.If the tree contains too few nodes, it expands the node set to achieve a greater coverage of the documents using their corresponding knowledge slices.This refinement strategy is executed by prompting an LLM with stronger reasoning capabilities than those used in the previous stages.</p>
<p>Evaluation</p>
<p>We present both the new evaluation approaches we have developed and the existing methods used in this domain.The main challenge in comparing a generated tree with the gold tree lies in aligning the two structures.The two trees should be structurally similar as well as semantically aligned.However, alignment is challenging because the trees may differ significantly in their hierarchical structures or exhibit low lexical overlap, as has been encountered in similar problems such as table schema alignment evaluation (Newman et al., 2024).</p>
<p>Average Degree Score</p>
<p>The first condition for two trees to be considered similar is that their structures should be similar.We design this metric to judge the structural similarity of the generated tree and the gold standard tree.</p>
<p>In a graph, the average degree is calculated as the average number of edges connected to a node in the graph.For any tree with N nodes, the number of edges is N − 1, which gives the average degree of 2(N − 1)/N .Therefore, to judge the structural similarity between the gold standard tree T and the predicted tree T ′ , we find the average degree score ∆, which is the ratio between the average degree of T ′ and the average degree of T .
∆ = m i=1 d(T ′ i ) n i=1 d(T i )(1)
where, d(t) represents the degree of a node t, and m and n are the number of nodes in the trees T and T ′ respectively.In the ideal case, the value of ∆ should be 1.If the generated tree T ′ is more branched out than the original tree T , then the value of ∆ is greater than 1, while if T ′ is less branched out than it should be, then the value of ∆ is less than 1.We report the final score as the mean of all the average degree scores in the test corpus.</p>
<p>Level-order Traversal Comparison</p>
<p>The hierarchical structure of a tree makes it difficult to implement standard text generation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), or BERTScore (Zhang et al., 2019).Therefore, we propose a metric to compare the gold standard tree and the generated tree using level-order traversal.More specifically, we traverse the tree in such a way that all the nodes present in the same level are traversed completely before traversing the next level.After converting the entire tree into a single list through level-order traversal, we calculate the the corpus-level BLEU-2, ROUGE-L and BERTScore.</p>
<p>Node Soft Recall and Node Entity Recall</p>
<p>We use the metrics Node Soft Recall and Node Entity Recall, following the evaluation protocol in prior work (Fränti and Mariescu-Istodor, 2023;Shao et al., 2024;Kang and Xiong, 2025).These metrics compare the generated and the ground-truth taxonomy trees using semantic similarity and lexical overlap between them, respectively.Node Soft Recall (NSR) is dependent on soft cardinality of a tree (Jimenez et al., 2010), which is given by,
c(T ) = n i=1 1 n n j=1 Sim(T i , T j )(2)
where Sim(T i , T j ) is the cosine similarity between the SENTENCE-BERT (Reimers and Gurevych, 2019) embeddings of the taxonomy trees T i and T j .The Node Soft Recall between two trees T and T ′ is defined as,
NSR(T, T ′ ) = c(T ) + c(T ′ ) − c(T ∪ T ′ ) c(T ′ ) (3)
Since in most baselines, there is a large mismatch between the number of nodes of the generated taxonomy tree and the original tree, we tweak the original heading soft recall in Shao et al. (2024) by inserting a normalizing factor in Eq. 2, i.e., we divide by the number of nodes to offset the effect of a large node count in the tree.</p>
<p>Node Entity Recall (NER) between the gold standard tree T and the generated tree T ′ is defined as the percentage of entities that are present both in T and T ′ .Formally, it can be expressed as,
NER(T, T ′ ) = |Ent(T ) ∩ Ent(T ′ )| |Ent(T )| (4)
where |Ent(T )| represents the number of entities in T .In our case, we track Noun Phrases (NP) for better coverage.We use the chunking model from FLAIR (Akbik et al., 2019) for this purpose.</p>
<p>LLM-as-a-Judge</p>
<p>We prompt a stronger LLM, GPT-4.1, with the gold-standard taxonomy tree and the generated taxonomy tree, and ask it to evaluate the generated tree on a scale of 1 to 5 based on the structural similarity and the semantic similarity of the two trees.The LLM judges whether the generated tree aligns with the gold tree and whether they are coherent.The prompt is described in Appendix B.</p>
<p>Baselines</p>
<p>In this section, we present the baselines against which we evaluate our method TAXOALIGN.</p>
<p>AutoSurvey (Wang et al., 2024): The outline generation part of this method randomly divides the reference papers into several groups, which results in the creation of multiple outlines.The language model then amalgamates these outlines to construct a single comprehensive outline.For a fair comparison with our method, we provide the reference papers, in contrast to the original work.We run only the outline generation step of AutoSurvey, instead of generating the whole article.</p>
<p>STORM (Shao et al., 2024): The pre-writing stage of this method involves researching the given topic through simulated conversations.A draft outline is generated initially by prompting the LLM with the topic only.To generate the final outline, the LLM is prompted with the topic, simulated conversations as well as the draft outline.Like in AutoSurvey, here also we provide the pipeline with the reference papers.</p>
<p>Topic only: In this baseline, we simply prompt a LLM with the taxonomy topic and ask it to generate a corresponding tree that best fits the topic.The primary goal is to evaluate how effectively the model can generate such a tree using only its parametric knowledge.</p>
<p>Topic + Keyphrases: We use a LLM to extract the top keyphrases from each of the reference (cited) papers that form the basis of the taxonomy.This provides the top phrases of each paper, enabling us to fit the essential content of all references within the limited context window of LLMs.We then prompt another LLM to generate the taxonomy tree based on the set of keyphrases.</p>
<p>TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement: We use the knowledge slices used in our proposed method.We simply prompt the model to generate the taxonomy tree based on these slices.This allows us to isolate and assess the effect of the latter stages of our own method, specifically those that occur after knowledge-slice creation.</p>
<p>7 Experimental Setup</p>
<p>Base Models</p>
<p>We use the open-domain Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) and Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024) for extracting the knowledge slices.In the taxonomy verbalization phase, we finetune Llama-3.1-Tülu-3-8B(Lambert et al., 2025) and SciLitLLM1.5-7B(Li et al., 2024) respectively.For the refinement stage, we use a closed-domain model GPT-4o-mini.For the prompting stage in the TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement baseline, we also test using three large open-source reasoning models (Qwen's QwQ-32B (Team, 2025b), DeepSeek-AI's DeepSeek-R1-Distill-Qwen-32B (DeepSeek-AI, 2025) and NovaSky-AI's Sky-T1-32B(Team, 2025a)) using the previously generated knowledge slices.Prompts and model details are present in Appendix A. We choose the Tülu and the SciLitLLM models for instruction tuning in TAXOALIGN as well as prompting because they include extensive scientific research-related data in their pre-training or continual pre-training corpus.</p>
<p>Hyperparameter Choices</p>
<p>In the taxonomy verbalization stage, we instructiontune LLMs using QLoRA (Dettmers et al., 2023).QLoRA uses 4-bit NormalFloat, Double Quantization and Paged Optimizers on the LoRA finetuning approach (Hu et al., 2022).Each language model is instruction-tuned for 800 steps with an input context window of 16, 384 and a output context window of 1, 024.The learning rate is 2e − 4 and the training batch size is 1.For instructiontuning, we use simple intuitive prompts based on training data from CS-TAXOBENCH with Alpacalike5 instruction format (Taori et al., 2023).The instruction format is given in the Appendix B.1.We instruct LLMs in our experiments to generate taxonomies with a maximum depth of three.For the taxonomy verbalization part in our method or in any of the baselines, we set 1, 024 as the maximum number of new tokens to be generated by the model.All experiments are done on a single A100.</p>
<p>Results and Analysis</p>
<p>We evaluated our method and the baseline methods using the proposed metrics.We summarize the results of our experiments in Table 2.Additional results with more models are in Table 5 of Appendix C. We find that the TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement baseline performs second best to our method on most metrics.This indicates that the knowledge slices are an important tool for this task.This baseline has high average degree score compared to our method that reveals that our Taxonomy Verbalization and Refinement stages could effectively reduce the gap between the generated and gold taxonomy trees while enhancing the quality of node labels.In our experiments, we find that the BLEU-2, ROUGE-L and BERTScore values are much less than what we typically encounter in tasks like machine translation or question answering.This suggests substantial scope for improvement in this task, as a huge gap remains between the human-created and machine-generated results.</p>
<p>Structural Similarity</p>
<p>Our method consistently achieves an average degree score (∆) close to 1, while the ∆ value obtained by other baseline methods are much higher.This indicates that our generated tree is much closer  to the human-written taxonomy tree in terms of structure.We observe that the ∆ value is closest to 1 for the Topic-only baseline.This is mainly because when provided with only the topic, the language model generates a small tree due to lack of parametric knowledge, as has been established by the consistent low scores for this baseline on the rest of the metrics.Other baseline methods tend to produce overly large trees (∆ &gt; 2.9) with an excessive number of nodes and branches, increasing the likelihood of hallucinations and structural divergence from the gold-standard tree.We show examples of generated taxonomies using AutoSurvey and STORM in comparison with TAXOALIGN in Figures 5 and 6 respectively in Appendix D.</p>
<p>Semantic Similarity</p>
<p>In terms of the level-order traversal, we observe that our method produces comprehensively better BLEU-2, ROUGE-L and BERTScore in all the cases when compared with the baselines.We observe that all baselines have similarly low scores for level-order traversal, indicating that the generated nodes exhibit low lexical overlap with the gold data.In comparison, our method produces more coherent labels and nodes in the taxonomy tree.</p>
<p>In terms of Node Soft Recall (NSR), our method performs better than the other baselines, showing the similarity between the generated and gold node labels.The TAXOALIGN w/o Taxonomy Verbaliz.w/o Taxonomy Refine.baseline performs better in some cases in terms of the Node Entity Recall metric, which is mainly because this baseline generates large trees, as has been demonstrated by the ∆ value, and larger trees contribute to greater match in the Noun Phrase chunks.Using LLM-as-a-judge, TAXOALIGN also outperforms the baselines, reaffirming the metric-based results and showing that it generates taxonomies closer to the human-written ones in both structure and intent.</p>
<p>Testing with Conference Survey Papers</p>
<p>We tested our three-phase pipeline (that had been trained on 400-instance training data) on the new test set introduced in Section 3.6, the results for which are given in Table 3. Extended results with more models are present in Table 6 of Appendix C. We observe that our method comfortably outperforms the baseline on this test set too, which is consistent with the results reported in Table 2.</p>
<p>Error Analysis</p>
<p>We present an error analysis based on a manual evaluation of instances from the test set in Table 1 using our proposed TAXOALIGN pipeline.For illustration, we provide a example for each of the three stages of the pipeline in Figure 7 of Appendix E. Below, we summarize the common errors observed at each stage of the pipeline.</p>
<p>Knowledge Slices + Prompting: Direct generation from knowledge slices creates more verbose taxonomies that contain irrelevant information leading to the nodes not being very specific or not pertaining to the topic directly.Another major factor is the presence of repeated numbers of the same nodes or sub-trees in the taxonomy.</p>
<p>Knowledge Slices + Taxonomy Verbalization: Structurally, the taxonomies are closer to the gold standard taxonomies, but there are some factual errors that persist.We observe that the generated taxonomies suffer from the problem of hallucinated node labels or are too short.</p>
<p>Knowledge Slices + Taxonomy Verbalization + Taxonomy Refinement: The generated trees are more aligned to the gold standard trees in terms of structure and semantic coherence.Still, the generated trees suffer from a low number of layer-wise exact matches.The generated trees are certainly more interpretable than the previous stages and the overall tree also presents a coherent structure.</p>
<p>Method</p>
<p>Structure Content TAXOALIGN 3.17 2.62 AutoSurvey 2.17 2.25</p>
<p>Human Evaluation</p>
<p>We use human evaluation to complement the automated framework.Three annotators with domain knowledge were asked to rate the surveys generated by TAXOALIGN and AutoSurvey.The annotators are instructed to assess based on (1) the structural commonalities between the gold and generated taxonomy trees and then (2) the semantic coherence of the generated tree with respect to the gold tree.</p>
<p>The evaluation is done using a 5-point Likert scale on 20 randomly sampled data instances from the test set of CS-TAXOBENCH.The inter-annotator agreement is calculated as 0.61 and 0.73 (Krippendorff's α).The results are shown in Table 4.The mean ratings show TAXOALIGN outperforms AU-TOSURVEY in both structural and content quality.</p>
<p>To verify the consistency between our LLM-as-ajudge evaluation and the human evaluation, we first average the scores assigned by human annotators for each taxonomy tree.We then compare these with the LLM-generated scores using Spearman's rank correlation coefficient.Thereby, we obtain a Spearman's rho value of 0.527 which indicates a strong positive correlation.These results suggest that our LLM-as-a-judge evaluation method aligns well with human preferences, providing a reliable proxy for human judgment.</p>
<p>Conclusion</p>
<p>Automating scholarly taxonomy generation can help researchers and practitioners efficiently navigate the vast body of scholarly literature.To facilitate this, we present CS-TAXOBENCH, a benchmark comprising 460 taxonomies from journey surveys and 80 from conference surveys, along with TAXOALIGN, a method that uses instruction tuning and refinement on topic-related information extracted from papers.We introduce two new metrics and show that TAXOALIGN outperforms the baselines on most evaluation measures.</p>
<p>Limitations</p>
<p>We construct CS-TAXOBENCH from a single journal within a defined time frame to ensure consistency in taxonomy quality.However, additional open-access journals and conference venues could also be explored for future curation.</p>
<p>We do not focus on the retrieval of the reference papers from a corpus of papers.While this is an important task for end-to-end taxonomy construction given only the taxonomy topic, we focus more on creating and evaluating taxonomies when provided with a set of reference documents.</p>
<p>Although we improve the structure and semantic coherence between the human-written and the generated taxonomies using TAXOALIGN, there is a a lot of scope for improvement in this field.Therefore, this is a encouraging field of work in which the community can work in the coming days.</p>
<p>A Our Method -Prompts and Models</p>
<p>A.1 Models</p>
<p>• Mistral-7B-Instruct-v0.3 (Jiang et al., 2023):</p>
<p>The Mistral group of models leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost.</p>
<p>Compared to version 0.2, this model can process and respond more effectively to diverse tasks and instructions, owing to its expanded vocabulary of 32,768 tokens and support for the v3 tokenizer.The model can carry out operations that call for outside data since it supports function calling.</p>
<p>• Meta-Llama-3-8B-Instruct (Grattafiori et al., 2024): Llama is a family of pre-trained foundational language models that have been opensourced by Meta in recent times.The Meta-Llama-3-8B-Instruct is trained on a mix of publicly available online data with a knowledge cutoff of March, 2023.The tuned versions of Llama3 use Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF) to align with human preferences.</p>
<p>• Llama-3.1-Tülu-3-8B(Lambert et al., 2025): Tülu (Wang et al., 2023) is a set of models that are instruction-tuned on LLaMA (Touvron et al., 2023) using a mixture of publicly available, synthetic and human-created datasets.Building upon the Llama 3.1 basic models, Tülu-3 (Lambert et al., 2025) models are trained using Direct Preference Optimization (DPO), Supervised Fine-Tuning (SFT), and a technique called Reinforcement Learning with Verifiable Rewards (RLVR).</p>
<p>• SciLitLLM1.5-7B (Li et al., 2024): It is a very recently released LLM designed for the task of scientific literature understanding that has been trained using both Continual Pre-Training (CPT) and Supervised Fine-Tuning (SFT).This strategy is used on Qwen2.5 to obtain SciLitLLM.The CPT stage uses 73,000 textbooks and 625,000 academic papers, while the SFT stage uses SciLitIns, SciRIFF (Wadden et al., 2024) and Infinity-Instruct6 .We use the SciLitLLM 7B7 for our experimental purposes.</p>
<p>• QwQ-32B (Team, 2025b): QwQ is designed for complex problem-solving and logical reasoning tasks and is based on Qwen2.5.The model is text-only and focuses on tasks like multi-step reasoning, complex decisionmaking, and research assistance.</p>
<p>• DeepSeek-AI's DeepSeek-R1-Distill-Qwen-32B (DeepSeek-AI, 2025): DeepSeek-R1-Distill-Qwen-32B is an open-source, distilled large language model (LLM) based on the Qwen2.5 32B architecture, utilizing the knowledge from the DeepSeek-R1 reasoning model.It is optimized for language understanding, reasoning, and text generation tasks and is known for outperforming other opensource models, including OpenAI's o1-mini, on various industry benchmarks.</p>
<p>• Sky-T1-32B (Team, 2025a): This model has been developed by the NovaSky team at UC Berkeley.It excels in mathematical and coding reasoning, outperforming some advanced closed-source models and other open-source alternatives on various benchmarks.The model was created by fine-tuning the Qwen 2.5 32B instruct model with a high-quality, 17,000-item dataset.</p>
<p>A.2 Knowledge Slice-Prompt</p>
<p>You will receive a document and a topic.Your task is to identify the knowledge-slices within the document that are very relevant to the given topic.A knowledge-slice is a piece of information representing the highlights of the document related to the given topic i.e. each knowledge-slice should be such that it both represents an important point in the document, but at the same time, the knowledgeslice should pertain closely to the given topic.Also, the knowledge-slice should not represent any additional information that is not present in the document.</p>
<p>A.3 Taxonomy Verbalization-Prompt</p>
<p>A taxonomy is a tree-structured semantic hierarchy that establishes a classification of the existing literature under a common topic.You will receive a taxonomy topic along with a collection of documents.Your task is to create a taxonomy tree using the given topic and based on the highlights of the documents i.e. create new child nodes by identifying generalizable sub-level topics from the document highlights that can act as child nodes to the taxonomy topic, which acts as the root node .The taxonomy tree should be created such that it looks as if all the given documents are a part of the taxonomy.There may be several levels in the tree i.e. each node may contain child nodes, but the total depth of the tree should not exceed three.The topics in all the levels of the tree except the last level must not be too specific so that it can accommodate future sub-topics i.e. child nodes.</p>
<p>-The nodes at the last level of the hierarchy i.e. the leaf nodes should reflect a single topic instead of a combination of topics.</p>
<p>-Each node label is a small and concise phrase.</p>
<p>[Response Format Instructions] -The output tree is to be formatted as shown in the example such that the root node is the taxonomy topic and each child node is connected to its parent.</p>
<p>A.4 Taxonomy Refinement-Prompt</p>
<p>A taxonomy is a tree-structured semantic hierarchy that establishes a classification of the existing literature under a common topic.You will receive a taxonomy tree along with a collection of documents.The root node of the taxonomy tree is the overall taxonomy topic.Your task is to refine the taxonomy tree such that there is a clear connection between the parent node and the subsequent child nodes.Each node must be a well-defined topic that is grounded in the input document highlights.</p>
<p>B LLM-as-a-Judge Prompt</p>
<p>A taxonomy is a tree-structured semantic hierarchy that establishes a classification of the existing literature under a common topic.You are given a gold standard taxonomy tree and a generated taxonomy tree and your task is to respond with an appropriate score after comparing the two.Two taxonomy trees are said to be structurally similar if the number of nodes and branches are similar in number.</p>
<p>If one tree has too many or too less nodes and branches than the gold tree, then they are said to be structurally dissimilar.Two taxonomy trees are said to be semantically similar if their nodes have values with close meanings or are matching entirely.Please respond with only the score based on the following criteria: Score 1: The generated taxonomy has no similarity at all with the gold standard taxonomy i.e. the structure and the intent of the generated taxonomy is totally different from that of the gold standard taxonomy.Score 2: The generated taxonomy have only a few nodes that has a semantic match with the nodes in the gold standard taxonomy and the structure of the generated taxonomy is a little similar to that of the gold standard taxonomy.The structure of the generated tree is very less similar to the gold standard tree but the intent of both taxonomies is similar.Score 3: The generated taxonomy has a reasonable similarity to the generated taxonomy in terms of structural similarity and semantic similarity.The structure of both trees are similar but some nodes are different in the two taxonomies.Score 4: The generated taxonomy has good logical consistency with that of the gold standard taxonomy in terms of semantic matching of the nodes between the two with the structure of the generated taxonomy is very similar to that of the gold standard taxonomy.</p>
<p>The two taxonomies only differ for a small number of instances.Score 5: The generated taxonomy is fully similar in terms of semantic matching and structure to the gold standard taxonomy.</p>
<p>C Extended Results</p>
<p>We show additional results using a expanded set of models on the original test set and the additional conference paper test in Tables 5 and 6 respectively.</p>
<p>D Output Example Comparison</p>
<p>We see in Figure 5 and 6 that the taxonomy trees generated using TAXOALIGN are much less verbose than the corresponding taxonomy trees generated using AutoSurvey or STORM.</p>
<p>E Error Analysis</p>
<p>We show an example of the results obtained in the three stages of our TAXOALIGN pipeline in Figure 7.The stages are Knowledge Slices + Prompting, Knowledge Slices + Taxonomy Verbalization and Knowledge Slices + Taxonomy Verbalization + Taxonomy Refinement.</p>
<p>Gold Standard Taxonomy AutoSurvey Human Image</p>
<p>Figure 1: Schematic representation of TAXOALIGN: (1) Knowledge Slice Creation (2) Taxonomy Verbalization (3) Taxonomy Refinement</p>
<p>DATA-DRIVEN METHODS ON HUMAN IMAGE GENERATION | |--Method Taxonomy Based on Fundamental Models | |--Method Taxonomy Based on Task Settings | +--Main Components in Data-Driven Methods |--HYBRID METHODS |--KNOWLEDGE-GUIDED METHODS ON | | HUMAN IMAGE GENERATION | |--Fundamental Models of Knowledge-Guided Methods |</p>
<p>Figure 3 :
3
Figure 3: An overview of the pipeline for the curation of our dataset.</p>
<p>Figure 4 :
4
Figure 4: An overview of the proposed TAXOALIGN pipeline.</p>
<p>the relevant knowledgeslices in the form of a list enclosed within square brackets.Your response should be in the following format: [Knowledge-Slices] [Knowledge-Slice 1, Knowledge-Slice 2,..., Knowledge-Slice n] [Your response]</p>
<p>DATA-DRIVEN METHODS ON HUMAN IMAGE GENERATION | |--Method Taxonomy Based on Fundamental Models | |--Method Taxonomy Based on Task Settings | +--Main Components in Data-Driven Methods |--HYBRID METHODS |--KNOWLEDGE-GUIDED METHODS ON | | HUMAN IMAGE GENERATION | |--Fundamental Models of Knowledge-Guided Methods | Introduction to Human Image Generation |--Key Techniques in Human Image Generation |--3D Human Reconstruction | |--Overview of 3D Human Reconstruction | |--Methods for 3D Reconstruction from 2D Images | |--Pixel-Aligned Implicit Functions | |--High-Frequency and Low-Frequency Information | |--Semantic Parsing and Texture Generation | |--Evaluating Reconstruction Quality | |--Challenges in Real-World Applications | +--Future Directions in 3D Reconstruction Research |--Image-to-Image Translation for Human Generation |--Virtual Try-On Systems | |--Overview of Virtual Try-On Systems | |--LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On | |--GP-VTON: Towards General Purpose Virtual Try-On | |--M3D-VTON: A Monocular-to-3D Virtual Try-On Network | |--Single-Stage Virtual Try-On with Deformable Attention Flows | |--Challenges in Virtual Try-On Systems | +--Future Directions for Virtual Try-On Technologies |--Pose Transfer and Human Animation |--The Role of Deep Learning Models |--Evaluation Metrics and Datasets +--Future Directions and Challenges |--Challenges in Handling Occlusions |--Improving Generalization Across Diverse Scenarios |--Ethical Implications of Generated Content |--Advances in Multi-Modal Conditioned Generation |--Future Directions in Technology Integration +--Academic and Industrial Partnerships</p>
<p>Figure 5 :
5
Figure5: A comparison of a gold standard taxonomy tree and a generated taxonomy tree using AutoSurvey.</p>
<p>Figure 6 :
6
Figure6: A comparison of a gold standard taxonomy tree and a generated taxonomy tree using STORM.</p>
<p>ACM CSUR papers in 2020-24 (1165) Open Access journal papers (325) + Present in Arxiv (285)</p>
<p>Table 2 :
2
Results of our method, TAXOALIGN, compared with AutoSurvey, STORM, Topic-only, Topic+Keyphrases and TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement, on the original test set.
MethodModel∆Level-order Traversal BLEU-2 ROUGE-L BERTScoreNSRNERLLM judgeAutoSurveyPrompt: GPT-4o-mini4.46590.00160.17840.82561.0903 0.1982 2.4333STORMPrompt: GPT-4o-mini6.1510.00120.13490.81661.0727 0.1539 2.2000Topic onlyPrompt: Tülu1.42740.00520.23590.83761.4187 0.1373 2.0833Topic + KeyphrasesKeyphrase: LLaMa; Prompt: Tülu Keyphrase: Mistral; Prompt: Tülu4.4517 4.910.0018 0.00140.1584 0.14320.8134 0.81001.1103 0.1491 2.4167 1.0996 0.1640 2.4167K-Slice: LLaMa; Prompt: Tülu5.4860.00370.1590.81230.9571 0.2074 2.4833TAXOALIGN w/o Tax. Verbaliz.K-Slice: Mistral; Prompt: Tülu6.11250.00290.14650.80871.0791 0.2197 2.4333w/o Tax. Refine.K-Slice: LLaMa; Prompt: Sky-T1-32B6.44860.00200.17610.81701.0804 0.2135 2.3966K-Slice: Mistral; Prompt: Sky-T1-32B7.19650.00220.19330.82211.0948 0.2103 2.4211K-Slice: LLaMa;T-Verbal.: Tülu;1.66870.01320.29750.85011.3244 0.1986 2.4167TAXOALIGNT-Refine.: GPT-4o-mini K-Slice: Mistral;T-Verbal.: Tülu;1.6680.00510.29740.85171.3635 0.1872 2.5000T-Refine.: GPT-4o-miniMethodModel∆Level-order Traversal BLEU-2 ROUGE-L BERTScoreNSRNERLLM judgeTAXOALIGN w/o Tax. Verbaliz. w/o Tax. Refine.K-Slice: LLaMa; Prompt: Tulu K-Slice: Mistral; Prompt: Tulu6.361 7.20830.0019 0.00340.1643 0.15980.8182 0.81591.0716 0.2683 2.275 1.0737 0.2653 2.2125K-Slice: LLaMa;T-Verbal.: Tulu;2.19240.00580.30910.85421.2129 0.2566 2.2875TAXOALIGNT-Refine.: gpt-4o-mini K-Slice: Mistral;T-Verbal.: Tulu;2.36170.0130.30040.85221.2072 0.27162.35T-Refine.: gpt-4o-mini</p>
<p>Table 3 :
3
Results of TAXOALIGN compared with TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement on the additional test set of conference papers.</p>
<p>Table 4 :
4
Mean ratings from human evaluation of the structure and content similarity for TAXOALIGN and AutoSurvey.</p>
<p>Do not alter the root node of the tree i.e. the taxonomy topic.Your task is to alter the other nodes only if deemed necessary i.e.only if a better viable replacement is found.Please try to adhere to the structure of the given taxonomy tree as much as possible.Only if the given taxonomy tree is restricted to less than five nodes, then generate the taxonomy tree on your own.Strictly adhere to the format of the tree shown here.
[Example Output]example-output[Taxonomy Topic]taxonomy-topic[Documents]Doc-1Doc-2Doc-3Please ONLY return the edited taxonomytree in the output format as shown in theexample above.[Your response]</p>
<p>Table 5 :
5
Results of our method compared with baselines like AutoSurvey, Topic-only, Topic+Keyphrases andTAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement.
MethodModel∆Level-order Traversal BLEU-2 ROUGE-L BERTScoreNSRNERLLM judgeTAXOALIGN w/o Tax. Verbaliz. w/o Tax. Refine.K-Slice: LLaMa; Prompt: Tulu K-Slice: LLaMa; Prompt: SciLitLLM6.361 6.52210.0019 0.00260.1643 0.19570.8182 0.81831.0716 0.2683 2.275 1.2095 0.2267 1.9375K-Slice: Mistral; Prompt: Tulu7.20830.00340.15980.81591.0737 0.2653 2.2125K-Slice: Mistral; Prompt: SciLitLLM5.93870.00320.20390.82111.3243 0.2176 1.875K-Slice: LLaMa;T-Verbaliz.: Tulu;2.19240.00580.30910.85421.2129 0.2566 2.2875T-Refine.: gpt-4o-miniK-Slice: LLaMa;T-Verbaliz.: SciLitLLM;2.55510.01270.30340.8511.1927 0.2614 2.2625TAXOALIGNT-Refine.: gpt-4o-mini K-Slice: Mistral;T-Verbaliz.: Tulu;2.36170.0130.30040.85221.2072 0.27162.35T-Refine.: gpt-4o-miniK-Slice: Mistral;T-Verbaliz.: SciLitLLM;3.17790.00420.28450.84651.1806 0.267 2.3125T-Refine.: gpt-4o-mini</p>
<p>Table 6 :
6
Results of TAXOALIGN compared with TAXOALIGN w/o Taxonomy Verbalization w/o Taxonomy Refinement.</p>
<p>https://dl.acm.org/journal/csur
https://treelib.readthedocs.io/en/latest/
https://huggingface.co/datasets/tatsu-lab/ alpaca
https://huggingface.co/datasets/BAAI/ Infinity-Instruct
https://huggingface.co/Uni-SMART/SciLitLLM</p>
<p>Automated product taxonomy mapping in an e-commerce environment. Steven S Aanen, 10.1016/j.eswa.2014.09.032Expert Syst. Appl. 4232015Damir Vandic, and Flavius Frasincar</p>
<p>FLAIR: An easy-to-use framework for state-of-theart NLP. Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, Roland Vollgraf, 10.18653/v1/N19-4010Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational Linguistics2019</p>
<p>SciAssess: Benchmarking LLM proficiency in scientific literature analysis. Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Wang Changxin, Zhifeng Gao, Hongshuai Wang, Li Yongge, Mujie Lin, Shuwen Yang, Jiankun Wang, Mingjun Xu, Jin Huang, Xi Fang, Jiaxi Zhuang, Yuqi Yin, Yaqi Li, Changhong Chen, Zheng Cheng, Zifeng Zhao, Linfeng Zhang, Guolin Ke, Findings of the Association for Computational Linguistics: NAACL 2025. Albuquerque, New MexicoAssociation for Computational Linguistics2025</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , arXiv:2501.129482025Preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>The semantic growbag algorithm: automatically deriving categorization systems. Jörg Diederich, Wolf-Tilo Balke, Proceedings of the 11th European Conference on Research and Advanced Technology for Digital Libraries, ECDL'07. the 11th European Conference on Research and Advanced Technology for Digital Libraries, ECDL'07Berlin, HeidelbergSpringer-Verlag2007</p>
<p>Taxolearn: A semantic approach to domain taxonomy learning. Emmanuelle-Anna Dietz, Damir Vandic, Flavius Frasincar, 10.1109/WI-IAT.2012.129IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology. 12012. 2012</p>
<p>Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation. Steffen Eger, Yong Cao, D' Jennifer, Andreas Souza, Christian Geiger, Stephanie Greisinger, Yufang Gross, Brigitte Hou, Anne Krenn, Yizhi Lauscher, Chenghua Li, Nafise Sadat Lin, Wei Moosavi, Tristan Zhao, Miller, arXiv:2502.051512025Preprint</p>
<p>Soft precision and recall. Pasi Fränti, Radu Mariescu-Istodor, 10.1016/j.patrec.2023.02.005Pattern Recognition Letters. 1672023</p>
<p>The distributional inclusion hypotheses and lexical entailment. Maayan Geffet, Ido Dagan, 10.3115/1219840.1219854Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05). the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)Ann Arbor, MichiganAssociation for Computational Linguistics2005</p>
<p>Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, 10.18653/v1/P19-1513Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>TDMSci: A specialized corpus for scientific literature entity tagging of tasks datasets and metrics. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, 10.18653/v1/2021.eacl-main.59Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational Linguistics2021</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Taxonomy tree generation from citation graph. Yuntong Hu, Zhuofeng Li, Zheng Zhang, Chen Ling, Raasikh Kanjiani, Boxin Zhao, Liang Zhao, arXiv:2410.037612025Preprint</p>
<p>Understand short texts by harvesting and analyzing semantic knowledge. Wen Hua, Zhongyuan Wang, Haixun Wang, Kai Zheng, Xiaofang Zhou, 10.1109/TKDE.2016.2571687IEEE Transactions on Knowledge and Data Engineering. 2932017</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Text comparison using soft cardinality. Sergio Jimenez, Fabio Gonzalez, String Processing and Information Retrieval. Berlin, Heidelberg; Berlin HeidelbergSpringer2010and Alexander Gelbukh</p>
<p>Hao Kang, Chenyan Xiong, arXiv:2406.10291Researcharena: Benchmarking large language models' ability to collect and organize information as research agents. 2025Preprint</p>
<p>TaxoFinder: A Graph-Based Approach for Taxonomy Learning. Yong-Bin Kang, Pari Delir Haghigh, Frada Burstein, 10.1109/TKDE.2015.2475759IEEE Transactions on Knowledge &amp; Data Engineering. 28022016</p>
<p>A semisupervised method to learn and construct taxonomies using the web. Zornitsa Kozareva, Eduard Hovy, Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. the 2010 Conference on Empirical Methods in Natural Language ProcessingCambridge, MAAssociation for Computational Linguistics2010</p>
<p>Sebastian Köhler, Sandra C Doelken, Christopher J Mungall, Sebastian Bauer, Helen V Firth, Isabelle Bailleul-Forestier, Graeme C M Black, Danielle L Brown, Michael Brudno, Jennifer Campbell, David R Fitzpatrick, Janan T Eppig, Andrew P Jackson, Kathleen Freson, Marta Girdea, Ingo Helbig, Jane A Hurst, Johanna Jähn, Laird G Jackson, Anne M Kelly, David H Ledbetter, Sahar Mansour, Christa L Martin, Celia Moss, Andrew Mumford, Willem H Ouwehand, Soo-Mi Park, Erin Rooney Riggs, Richard H Scott, Sanjay Sisodiya, Steven Van Vooren, Ronald J Wapner, O M Andrew, Caroline F Wilkie, Wright, T Anneke, Vulto-Van Silfhout, 10.1093/nar/gkt1026The human phenotype ontology project: linking molecular biology and disease through phenotype data. B A Bert, Nicole L De Vries, Cynthia L Washingthon, Monte Smith, Paul Westerfield, Barbara J Schofield, Georgios V Ruef, Melissa Gkoutos, Damian Haendel, Suzanna E Smedley, Peter N Lewis, Robinson, 201342Nicole de Leeuw</p>
<p>Instruct large language models to generate scientific literature survey step by step. Yuxuan Lai, Yupeng Wu, Yidan Wang, Wenpeng Hu, Chen Zheng, arXiv:2408.078842024Preprint</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James, V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A Smith, Yizhong Wang, arXiv:2411.15124Pradeep Dasigi, and Hannaneh Hajishirzi. 2025. Tulu 3: Pushing frontiers in open language model post-training. Preprint</p>
<p>Scilitllm: How to adapt llms for scientific literature understanding. Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai, arXiv:2408.155452024Preprint</p>
<p>Surveyx: Academic survey automation via large language models. Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song, Zehao Lin, Yebin Yang, Simin Niu, Hanyu Wang, Bo Tang, Feiyu Xiong, Keming Mao, Zhiyu Li, arXiv:2502.147762025Preprint</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.1162/tacl_a_00638Transactions of the Association for Computational Linguistics. 122024</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, 10.1145/3560815ACM Comput. Surv. 9552023</p>
<p>Automatic taxonomy construction from keywords. Xueqing Liu, Yangqiu Song, Shixia Liu, Haixun Wang, 10.1145/2339530.2339754Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '12. the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '12New York, NY, USA2012Association for Computing Machinery</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Octet: Online catalog taxonomy enrichment with self-supervision. Yuning Mao, Tong Zhao, Andrey Kan, Chenwei Zhang, Xin , Luna Dong, Christos Faloutsos, Jiawei Han, 10.1145/3394486.3403274Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Taxonomy extraction using knowledge graph embeddings and hierarchical clustering. Félix Martel, Amal Zouaq, 10.1145/3412841.3441959Proceedings of the 36th Annual ACM Symposium on Applied Computing, SAC '21. the 36th Annual ACM Symposium on Applied Computing, SAC '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>End-to-end construction of NLP knowledge graph. Ishani Mondal, Yufang Hou, Charles Jochim, 10.18653/v1/2021.findings-acl.165Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>ArxivDI-GESTables: Synthesizing scientific literature into tables using language models. Benjamin Newman, Yoonjoo Lee, Aakanksha Naik, Pao Siangliulue, Raymond Fok, Juho Kim, Joseph Chee Daniel S Weld, Kyle Chang, Lo, 10.18653/v1/2024.emnlp-main.538Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USA2024Association for Computational Linguistics</p>
<p>A method for taxonomy development and its application in information systems. R Nickerson, U Varshney, J Muntermann, 10.1057/ejis.2012.26European Journal of Information Systems. 2232013</p>
<p>Espresso: Leveraging generic patterns for automatically harvesting semantic relations. Patrick Pantel, Marco Pennacchiotti, 10.3115/1220175.1220190Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics. the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational LinguisticsSydney, AustraliaAssociation for Computational Linguistics2006</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ACL '02USA. Association for Computational Linguistics2002</p>
<p>Taxonomy induction based on a collaboratively built knowledge repository. Paolo Simone, Michael Ponzetto, Strube, 10.1016/j.artint.2011.01.003Artificial Intelligence. 17592011</p>
<p>Query-driven document-level scientific evidence extraction from biomedical studies. Massimiliano Pronesti, Joao H Bettencourt-Silva, Paul Flanagan, Alessandra Pascale, Oisín Redmond, Anya Belz, Yufang Hou, 10.18653/v1/2025.acl-long.1359Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 63rd Annual Meeting of the Association for Computational LinguisticsVienna, AustriaAssociation for Computational Linguistics2025a1</p>
<p>Enhancing study-level inference from clinical trial papers via rl-based numeric reasoning. Massimiliano Pronesti, Michela Lorandi, Paul Flanagan, Oisin Redmon, Anya Belz, Yufang Hou, arXiv:2505.229282025bPreprint</p>
<p>Sentence-BERT: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Learning concept hierarchies from textual resources for ontologies construction. Ana B Rios-Alvarado, Ivan Lopez-Arevalo, J Victor, Sosa-Sosa, 10.1016/j.eswa.2013.05.005Expert Systems with Applications. 40152013</p>
<p>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards. Furkan Şahinuç, Thy Thy Tran, Yulia Grishina, Yufang Hou, Bei Chen, Iryna Gurevych, 10.18653/v1/2024.emnlp-main.453Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Assisting in writing Wikipedia-like articles from scratch with large language models. Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, Monica Lam, 10.18653/v1/2024.naacl-long.347Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American ChapterMexico City, MexicoAssociation for Computational Linguistics20241</p>
<p>Combining linguistic and statistical analysis to extract relations from web documents. Fabian M Suchanek, Georgiana Ifrim, Gerhard Weikum, 10.1145/1150402.1150492Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06. the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06New York, NY, USAAssociation for Computing Machinery2006</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>10.48550/arXiv.2408.09869Deep Search Team. 2024. Docling technical report. Technical report</p>
<p>Sky-t1: Train your own o1 preview model within $450. Novasky Team, 2025a</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Qwen Team, 2025b</p>
<p>A position paper on the automatic generation of machine learning leaderboards. Yufang Roelien C Timmer, Stephen Hou, Wan, arXiv:2505.174652025Preprint</p>
<p>LLaMA: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, arXiv:2302.139712023Preprint</p>
<p>OntoLearn reloaded: A graph-based algorithm for taxonomy induction. Paola Velardi, Stefano Faralli, Roberto Navigli, 10.1162/COLI_a_00146Computational Linguistics. 3932013</p>
<p>Sciriff: A resource to enhance language model instruction-following over scientific literature. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, Arman Cohan, arXiv:2406.078352024Preprint</p>
<p>A short survey on taxonomy learning from text corpora: Issues, resources and recent advances. Chengyu Wang, Xiaofeng He, Aoying Zhou, 10.18653/v1/D17-1123Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Probabilistic topic models for learning terminological ontologies. Wei Wang, Payam Mamaani Barnaghi, Andrzej Bargiela, 10.1109/TKDE.2009.122IEEE Transactions on Knowledge and Data Engineering. 2272010</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang, Advances in Neural Information Processing Systems. Curran Associates, Inc202437</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Raghavi Khyathi, David Chandu, Kelsey Wadden, Noah A Macmillan, Iz Smith, Hannaneh Beltagy, Hajishirzi, arXiv:2306.047512023Preprint</p>
<p>Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing. Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu Xia, Bin Wang, Bo Zhang, Lei Bai, arXiv:2503.046292025Preprint</p>
<p>Efficiently answering technical questions -a knowledge graph approach. Shuo Yang, Lei Zou, Zhongyuan Wang, Jun Yan, Ji-Rong Wen, 10.1609/aaai.v31i1.10956Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201731</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, CoRR, abs/1904.096752019</p>
<p>Taxonomy discovery for personalized recommendation. Yuchen Zhang, Amr Ahmed, Vanja Josifovski, Alexander Smola, 10.1145/2556195.2556236Proceedings of the 7th ACM International Conference on Web Search and Data Mining, WSDM '14. the 7th ACM International Conference on Web Search and Data Mining, WSDM '14New York, NY, USA2014Association for Computing Machinery</p>            </div>
        </div>

    </div>
</body>
</html>
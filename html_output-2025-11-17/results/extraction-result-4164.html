<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4164 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4164</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4164</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-271974913</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.15545v5.pdf" target="_blank">SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding</a></p>
                <p><strong>Paper Abstract:</strong> Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery. Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks. To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.cIn this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. We address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation. Applying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding. These models demonstrate promising performance on scientific literature understanding benchmarks. Our contributions are threefold: (1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains. (2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for supervised fine-tuning in less-represented scientific domains. (3) SciLitLLM achieves promising performance improvements on scientific literature understanding benchmarks.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The impact of large language models on scientific discovery: a preliminary study using GPT-4 <em>(Rating: 2)</em></li>
                <li>D-cpt law: Domain-specific continual pre-training scaling law for large language models <em>(Rating: 1)</em></li>
                <li>Galactica: A large language model for science <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4164",
    "paper_id": "paper-271974913",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The impact of large language models on scientific discovery: a preliminary study using GPT-4",
            "rating": 2,
            "sanitized_title": "the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        },
        {
            "paper_title": "D-cpt law: Domain-specific continual pre-training scaling law for large language models",
            "rating": 1,
            "sanitized_title": "dcpt_law_domainspecific_continual_pretraining_scaling_law_for_large_language_models"
        },
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 1,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        }
    ],
    "cost": 0.0098105,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SCILITLLM: HOW TO ADAPT LLMS FOR SCIENTIFIC LITERATURE UNDERSTANDING
18 Apr 2025</p>
<p>Sihang Li sihang0520@gmail.com 
University of Science and Technology of China</p>
<p>Jin Huang huangjin@dp.tech 
DP Technology</p>
<p>Jiaxi Zhuang 
DP Technology</p>
<p>Yaorui Shi 
University of Science and Technology of China</p>
<p>Xiaochen Cai 
DP Technology</p>
<p>Mingjun Xu 
DP Technology</p>
<p>Xiang Wang xiangwang1223@gmail.com 
University of Science and Technology of China</p>
<p>Linfeng Zhang 
DP Technology</p>
<p>Guolin Ke 
DP Technology</p>
<p>Hengxing Cai caihengxing@dp.tech 
DP Technology</p>
<p>SCILITLLM: HOW TO ADAPT LLMS FOR SCIENTIFIC LITERATURE UNDERSTANDING
18 Apr 20250180604601F0E2D44692545F54494CDFarXiv:2408.15545v5[cs.LG]Pre-Training CPT + SFT (Ours) Instruction Tuning General LLM Base Scientific LLM
Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery.Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks.To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instructionfollowing capabilities for domain-specific tasks.In this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions.We address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation.Applying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding.These models demonstrate promising performance on scientific literature understanding benchmarks.Our contributions are threefold: (1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains.(2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -SciLitIns -for less-represented scientific domains.(3) SciLitLLM achieves promising performance in scientific literature understanding benchmarks.We release the data processing codes 1 and model weights 2 .</p>
<p>INTRODUCTION</p>
<p>Figure 1: Average scores on SciRIFF of models with varying parameter sizes.</p>
<p>Scientific literature understanding involves the systematic evaluation and interpretation of scientific texts and publications, to identify trends, extract targeted information, and garner insights (AI4Science &amp; Quantum, 2023;Zheng et al., 2023), significantly contributing to scientific discovery.Concurrently, Large Language Models (LLMs) (OpenAI, 2023;Qwen, 2024;Touvron et al., 2023) have achieved remarkable success in natural language processing, prompting the development of domain-specific LLMs across various fields (Cui et al., 2024;Wu et al., 2023;Clusmann et al., 2023).However, recent studies (Wadden et al., 2024;Cai et al., 2024;Singh et al., 2023) indicate that LLMs face challenges when specializing in scientific literature understanding, particularly in context understanding and question answering.Take Figure 2</p>
<p>as</p>
<p>You will be shown an abstract from a biomedical research paper.Given this abstract, your task is to extract all unique entities of the following types: ["CellLine", "Disease", "Variant"].Please return the output as a JSON object of the format: {"CellLine": ["hRPTEC", ...], "Disease": ["Diabetes", ...], "Variant": ["Pro75Ala", ...]}.// Rest of the task.</p>
<p>Answer: I think you can use spaCy to do this.spaCy is a library for natural language processing (NLP) in Python.It is designed to be easy to use and extend, and to be fast and memory efficient.You can use it to extract entities from your abstract.</p>
<p>Unfamiliar with Scientific Tasks</p>
<p>Galactica-6.7B</p>
<p>{ "CellLine": [ "hRPTEC", "" ], "Disease": [ "Type 2 diabetes mellitus" ], "Variant": [ "Pro75Ala", "rs1884614", "rs4810424", "rs2144908", "rs2071197", ] }</p>
<p>Lack of Scientific Knowledge</p>
<p>Llama-3.1-8B</p>
<p>Correct Answer</p>
<p>{ "CellLine": [], "Disease": [ "IGT", "impaired glucose tolerance", "type 2 diabetes mellitus", "diabetes" ], "Variant": [ "rs3818247", "rs2071197", "rs1884614", "rs2144908", "rs4810424" ] } an example, where the LLM is asked to understand the content of a biomedical research paper and then extract the targeted information.LLMs' potential might be hindered by two major barriers:</p>
<p>SciLitLLM</p>
<p>(1) a lack of scientific knowledge, which results in errors such as the missing important entities in Llama-3.1-8B(AI@Meta, 2024), and (2) unfamiliarity with scientific tasks, leading to the inability of Galactica-6.7B(Taylor et al., 2022) to follow task instructions accurately.</p>
<p>To make LLMs specialized in science-relevant tasks, existing studies mostly adopt two strategies, as illustrated in Figure 3: (1) Fine-tuning with scientific instructions (Wadden et al., 2024;Zhang et al., 2024b;Singhal et al., 2022).A general-purpose LLM is fine-tuned with collected domainspecific instructions to adapt it to science-relevant tasks.However, instruction fine-tuning alone is insufficient to imbue the models with comprehensive scientific knowledge.(2) Pre-training on scientific corpora (Beltagy et al., 2019;Zeng et al., 2022;Taylor et al., 2022).This approach involves training models on vast scientific corpora.While this method equips LLMs with domain knowledge, the lack of instruction-tuning confines them to solving relevant tasks.Moreover, it is hampered by substantial computational costs and data requirements (Ling et al., 2023;Yang et al., 2023).To address these obstacles while balancing efficiency, we propose a hybrid strategy that incorporates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse domain knowledge and enhance domain-specific instruction-following capabilities.</p>
<p>However, as illustrated in Figure 4, developing a scientific literature understanding model using this CPT and SFT pipeline presents two critical requirements: (1) High-quality CPT Corpora.Scientific corpora, predominantly in PDF format such as textbooks and research papers, are not directly digestible for LLM training.Converting these documents to text with PDF parsing tools introduces formatting and syntax errors, degrading corpus quality.Worse still, scientific documents often contain segments that contribute little information (e.g., references), necessitating quality control to filter them out.See the first row in Figure 4 for a comparison of high-and low-quality CPT texts.(2) Diverse Scientific Instructions.Effective instruction following for scientific literature understanding requires a large, high-quality, and diverse set of task-related instructions.However, to the best of our knowledge, there is a scarcity of well-designed instruction datasets for scientific literature understanding, and hiring human annotators to curate such a dataset from scratch is prohibitively expensive (Erdmann et al., 2019;Qiu et al., 2023).See the second row in Figure 4 for an illustration of high-and low-quality instructions.</p>
<p>To address these challenges, we devise an effective pipeline to construct high-quality domain corpora for CPT and diverse scientific instructions for SFT, as illustrated in Figure 5:</p>
<p>In the CPT stage for domain knowledge injection, we start with an extensive in-house corpus consisting of 73k textbooks and 625k academic papers in the scientific field, all in PDF format.Initially,  ,152,160 Ebetino,F. H.,184,203,305,[308][309]313,314,316,318 Edelstein,R. L.,[5][6]10,104,[105][106][107]111,123,124 Edreira.</p>
<p>Score: 0.66</p>
<p>Low Quality</p>
<p>Supervised Fine-tuning Instructions we leverage PyPDF23 , a widely used open-source PDF parsing tool, to extract raw texts from these documents.We then employ a moderate yet powerful model, Llama3-8B-Instruct, to correct the format and spelling errors introduced by PDF parsing (cf.Section 2.1.1).Subsequently, we train a small text quality classifier to score the corpus and filter out texts of low educational value4 in the scientific field (cf.Section 2.1.2).These two simple yet effective textual refinement and quality control measures ensure the high quality of our CPT corpus, culminating 12.7 billion tokens for CPT via the Qwen2.5 tokenizer.</p>
<p>In the SFT stage for domain instruction fine-tuning, to overcome the scarcity of domain-specific instructions and the high cost of human annotations, we propose a novel instruction synthesis method (cf.Section 2.2.1).It enables us to generate diverse instructions to better equip the model for domain-specific tasks.Moreover, we sequentially apply instruction duplication based on Levenshtein distance and an LLM-based filtering method to ensure the quality of synthetic instructions (cf.Section 2.2.2).</p>
<p>Having established such high-quality datasets, we apply the CPT-SFT integration strategy on a general-purpose LLM -Qwen2.5 (Qwen, 2024) and obtain SciLitLLM of two scales: 7B and 14B.</p>
<p>Evaluations on benchmarks of scientific literature understanding demonstrate the effectiveness of our strategy.We observe promising performance enhancements, with an average improvement of 4.0% on SciAssess (Cai et al., 2024) and 10.1% on SciRIFF (Wadden et al., 2024), compared to the leading LLMs under 10B parameters.Notably, SciLitLLM-7B even outperforms Llama3.1 and Qwen2.5 with 70B parameters on SciRIFF.</p>
<p>METHOD</p>
<p>In this section, we present the details of our proposed pipeline (cf. Figure 5): continual pre-training for scientific knowledge injection and supervised fine-tuning for scientific tasks enhancement.We refer to Appendix A for a comprehensive literature review on knowledge injection, domain adaptation, and scientific literature understanding for LLMs.What are high-quality pre-training corpora?Researchers (Gunasekar et al., 2023;Li et al., 2023;Abdin et al., 2024) suggest that language models benefit from corpora that possess the same qualities as an exemplary textbook for human learners: clarity, self-containment, instructiveness, and balance.Recognizing the wealth of high-quality scientific textbooks and research papers over the past decades, we have curated a substantial collection of over 73,000 textbooks and 625,000 research papers within the scientific domain, ensuring all documents are copyright-compliant.</p>
<p>CPT FOR SCIENTIFIC KNOWLEDGE INJECTION</p>
<p>However, we still face two practical obstacles when dealing with those textbooks and research papers: (1) Formatting and syntax errors.Most textbooks and research paper documents are in PDF format, which is not directly digestible by LLMs.Converting these documents using tools like PyPDF2 often introduces formatting and syntax errors, which degrade the quality of the corpus.(2) Corpus quality control.Despite their overall high quality, textbooks and research papers also contain segments with little useful information, such as references and garbled text introduced during the PDF parsing process.To tackle these obstacles, we devised the following modules: format &amp; grammar correction and CPT quality filter.</p>
<p>FORMAT &amp; GRAMMAR CORRECTION</p>
<p>A parsed text from a PDF document often contains many formatting and syntax errors.To address this issue, we prompt Llama3-8B-Instruct, to correct these errors introduced during the PDF parsing process.Utilizing the vLLM (Kwon et al., 2023) backend, Llama3-8B-Instruct can process approximately 2.52 million tokens per Nvidia A100 GPU hour.The process takes over 5,000 A100 GPU hours to handle all the textbooks and research papers.Example texts -both before and after processing -along with the prompt template are provided in Appendix B to demonstrate the improvements through this correction process.</p>
<p>CPT QUALITY FILTER</p>
<p>During CPT, maintaining the quality of the training corpus is crucial for effective knowledge injection.Given the extremely large scale of pre-training corpora, assessing quality through human annotation is not feasible (Erdmann et al., 2019;Qiu et al., 2023).Consequently, leading LLMs (e.g., Phi (Gunasekar et al., 2023), Llama (Touvron et al., 2023), and Qwen2.5 (Qwen, 2024)) employ model-based quality filters.The typical process involves using larger LLMs to score the quality of a subset of texts, which then serve as labels for training small classifiers (e.g., random forest (Gunasekar et al., 2023) and Bert (AI@Meta, 2024)) to annotate the entire training corpus.Inspired by this approach, we design a resource-efficient method based on a lightweight text quality classifier.</p>
<p>We first annotate a random subset of 50k CPT texts using Llama3-70B-Instruct.We adapt the quality assessment prompt from fineweb-edu-classifier (Anton et al., 2024), a widely-used quality classifier for web data, to evaluate the educational value (Gunasekar et al., 2023) of the scientific knowledge in each sampled text, assigning scores ranging from 0 (lowest quality) to 5 (highest quality).After annotation, we perform supervised transfer learning on the fineweb-edu-classifier checkpoint.</p>
<p>This process results in a scientific text quality classifier tailored for scientific corpus assessment.</p>
<p>We then utilize this classifier to assess the quality of the entire CPT dataset and exclude the lowestscoring 25% (See Figure 4 for concrete samples).The distribution of the scores is illustrated in Figure 6.We refer to Appendix C for more details about classifier training and sensitivity analysis of the filtering threshold.</p>
<p>CPT TRAINING SETTINGS</p>
<p>After preparing all the corpus, we perform CPT on Qwen2.5-Base(Qwen, 2024) for one epoch, encompassing 23.7 billion tokens (cf.Table 1), with a sequence length of 2,048 tokens.To maintain the model's general knowledge, we also include a similar scale of general corpus tokens from Redpajama (Computer, 2023).To stabilize the learning procedure, we gradually decrease the learning rate from 1 × 10 −5 to 0 with a cosine scheduler.To address overfitting, we apply a weight decay of 0.1 and gradients were clipped at a maximum value of 1.0.The CPT training took approximately 3 days on 32 Nvidia A100 GPUs for SciLitLLM-7B-Base and about 7 days for the 14B model.</p>
<p>SFT FOR SCIENTIFIC INSTRUCTION FOLLOWING</p>
<p>After performing CPT on an extensive scientific corpus to incorporate domain knowledge, we subsequently conduct SFT on domain-specific instructions to enhance the model's ability to understand scientific literature.We identify two major challenges in SFT for scientific instruction following:</p>
<p>• Existing instruction-tuning datasets in the scientific domain (Feng et al., 2024;Fang et al., 2023) primarily focus on fields such as physics, chemistry, and biology.Manually collecting instructiontuning data for other less-represented vertical domains (e.g., biomedicine, and material) is both time-consuming and costly (Erdmann et al., 2019;Qiu et al., 2023).</p>
<p>• Few instruction-tuning datasets adequately reflect the scenario of scientific literature understanding, which typically involves a segment of scientific literature accompanied by a question that requires deriving an answer from the text.</p>
<p>To address these challenges, we draw inspiration from leading models (e.g., Nemotron-4 (Adler et al., 2024), Phi (Gunasekar et al., 2023), and Qwen (Yang et al., 2024)), which leverage existing LLMs to construct synthetic instruction sets.We propose a novel instruction synthesis method to curate instructions specifically for scientific literature understanding.</p>
<p>INSTRUCTION SYNTHESIS OF LESS-REPRESENTED DOMAINS</p>
<p>Unlike typical question-answer pairs, an instruction for a scientific literature understanding task comprises three components (Wadden et al., 2024): (1) a segment of scientific literature, (2) a question pertaining to the context, and ( 3) the corresponding answer.Simply prompting an LLM to generate a scientific context along with an associated question-answer pair -without variations in the instructions or parameters -often yields similar or repeated contents.Thus, we design a simple yet effective three-step pipeline to generate diverse and high-quality scientific contexts and corresponding question-answer pairs, consisting of the following:</p>
<ol>
<li>Probability table of domain keywords.For a target scientific domain (e.g., biomedicine, and material), we collect dozens of high-impact research papers via Google Scholar5 and count the word frequency appearing in these papers.Then, we obtain a probability table of domain keywords.2. Scientific task list.Since LLMs are expected to handle various types of scientific tasks, an instruction set with task diversity is essential.Therefore, we compile a list of task descriptions by including representative tasks from existing scientific NLP datasets (Wadden et al., 2024;Cai et al., 2024;Feng et al., 2024), covering as many scenarios as possible that an LLM may encounter in real applications.3. Instruction Generation.Given a word probability table and the task list for a specific scientific domain, we sample 20 keywords and a task description each time.Subsequently, GPT-4o (Ope-nAI, 2023) is prompted to generate a scientific context containing the sampled keywords and a question-answer pair according to the provided task description.</li>
</ol>
<p>Utilizing this pipeline, we obtain over 100k synthetic instructions covering less-represented scientific domains and various types of specialized tasks (details are presented in Appendix D.1).</p>
<p>INSTRUCTION QUALITY CONTROL</p>
<p>To ensure the diversity and quality of generated instructions, effective measures for quality control are essential.Specifically, we incorporate heuristic deduplication and LLM-based filtering.</p>
<ol>
<li>Heuristic deduplication.Despite the measures taken during the generation process to prevent high homogeneity in the instructions, the generated data points may still contain similar questions or identical answers.To eliminate such redundancy, we implement a deduplication process to remove similar data points.See Appendix D.2 for details. 2. LLM-based filtering.Inspired by recent efforts (Eldan &amp; Li, 2023;Chiang &amp; Lee, 2023;Zhang et al., 2024c) to measure the quality of generated content using LLMs, we leverage Llama-3-70B-Instruct to assess the quality of generated instructions for five aspects: clarity, complexity, correctness, usefulness, and adaptability.We show the quality statistics of synthetic instructions in Figure 7. Instructions with an average score of less than 4 are filtered out.The detailed recipe for instruction quality evaluation with concrete examples is included in Appendix D.3.</li>
</ol>
<p>Through instruction synthesis and quality control pipeline, we obtain SciLitIns, consisting of 93,894 high-quality and diverse instructions for scientific literature understanding.</p>
<p>SFT TRAINING SETTINGS</p>
<p>Our SFT training dataset consists of three parts: SciLitIns, SciRIFF (Wadden et al., 2024)</p>
<p>EXPERIMENTS</p>
<p>In this section, we perform experiments to answer the following research questions: (Q1) How does SciLitLLM perform on scientific literature understanding tasks?(Q2) Can CPT with domainspecific corpora aid in scientific knowledge injection?(Q3) Can SFT with SciLitIns improve performance on scientific literature understanding tasks?</p>
<p>3.1 EXPERIMENTAL SETUP</p>
<p>BENCHMARKS</p>
<p>To evaluate the performance of LLMs regarding scientific knowledge base and specialized tasksolving abilities, our benchmarks include:</p>
<p>• Base model benchmarks.To assess their scientific knowledge, which serves as the foundation for scientific literature understanding, we evaluate the base models on scientific subjects -biology, chemistry, and health in MMLU-Pro (Wang et al., 2024) and MaScQA (Zaki et al., 2023) -a question answering dataset of materials science.• Instruction model benchmarks.We evaluate the instruct models on scientific literature understanding benchmarks: SciRIFF (Wadden et al., 2024) and SciAssess (Cai et al., 2024).Due to the space limitation, we refer to their original paper for the detailed metrics of each task.Brief descriptions of them are provided in Appendix E.</p>
<p>BASELINES</p>
<p>• Base model baselines.We compare SciLitLLM-base against leading open-source base models: Llama-3.1 (AI@Meta, 2024), Qwen-2.5 (Qwen, 2024).</p>
<p>• Instruction model baselines.We benchmark leading instruction LLMs including GPT-[3.5, 4o] (Brown et al., 2020;OpenAI, 2023), Llama3.1-[8B,70B] (AI@Meta, 2024) and Qwen2.5-[7B,14B, 72B] (Yang et al., 2024;Qwen, 2024) and Mistral-7B (Jiang et al., 2023).We also report the performance of SciTulu-7B (Wadden et al., 2024), which is a fine-tuned Llama2-7B (Touvron et al., 2023) model on SciRIFF.</p>
<p>3.2 PERFORMANCE OVERVIEW (Q1)</p>
<p>BASE MODEL PERFORMANCE</p>
<p>The performance comparison of base models is shown in Table 2. SciLitLLM-base consistently outperforms other general base models across four scientific benchmarks, indicating its extensive domain knowledge.For instance, SciLitLLM-7B-Base achieves an average accuracy improvement of 2.60% over Qwen2.5-7B-Base,despite both models having a comparable parameter count.This improvement can be attributed to the specialized pretraining on high-quality domain-specific corpora.It benefits from exposure to scientific language patterns, terminology, and reasoning.</p>
<p>INSTRUCT MODEL PERFORMANCE</p>
<p>As shown in Table 3, compared with models with similar scale, SciLitLLM-7B achieves the highest performance in all 4 domains on SciAssess, outperforming the second-best model (i.e., Llama3.1-8B)by a notable 4.0%.On SciRIFF, it surpasses baseline models by an even more substantial margin of 10.1%.These results highlight the effectiveness of our pipeline in adapting a general-purpose LLM into a specialized model for scientific literature understanding.</p>
<p>Specifically, one key contributor is our synthetic instruction dataset -SciLitIns, which offers comprehensive task coverage across diverse domains.It ensures that the model can generalize across tasks with varying input data and reasoning complexities, such as table extraction and molecule generation.For example, SciLitLLM-7B leads in 15 out of 27 sub-tasks on SciAssess, indicating its strong ability to adapt to the specialized requirements of these tasks.</p>
<p>Notably, SciLitLLM-14B shows significant performance gains over Qwen2-14B on both benchmarks, with a 4.4% improvement on SciAssess and a 7.5% improvement on SciRIFF.These results suggest that our approach effectively leverages the larger capacity of the 14B parameter model, enabling it to encode more nuanced scientific reasoning and domain-specific knowledge.Interestingly, SciLitLLM-14B also outperforms much larger open-source models, such as Llama3.1-70Band Qwen2.5-72B,despite having only one-fifth the number of parameters.This demonstrates that our continual pretraining and instruction fine-tuning pipeline is not merely scale-dependent but strategically optimized for domain adaptation.</p>
<p>The performance on SciRIFF further illustrates this optimization.Comparing with open-source models with similar scale, both SciLitLLM-7B and SciLitLLM-14B rank first in 8 out of 11 evaluations, including challenging tasks like Qasper and SciFact.These tasks require advanced abilities in scientific question answering and fact verification, respectively, which are likely bolstered by the diversity of high-quality examples in SciLitIns.</p>
<p>These findings underscore the broader importance of designing domain-specific LLMs.While general-purpose models, such as Llama and Qwen, perform well on broader tasks, their lack of domain specialization limits their effectiveness in fields like scientific literature understanding.The results demonstrate that the adaptation pipeline designed by us can yield promising performance without relying solely on increased model size.</p>
<p>ABLATION STUDY (Q2 &amp; Q3)</p>
<p>We conducted ablation experiments on three key components in our pipeline: the CPT stage, the SFT data recipe, and the instruction quality filter, to investigate their effectiveness.Note that all ablation experiments were performed on SciLitLLM-7B due to budget constraints.We investigate the contribution of the CPT stage for SciLitLLM.We compare the three variants: (1) Qwen2.5-7B-Instruct:official instruct-model checkpoint;</p>
<p>(2) Qwen2.5-7B-base+ SFT: applying our SFT stage directly to Qwen2.5-7B-base without CPT; (2) Qwen2.5-7B-base+ CPT + SFT: SciLitLLM-7B-Instruct.</p>
<p>As shown in Table 4, applying SFT alone to the Qwen2.5-7B-Basemodel yields improvements on SciAssess (+3.3%) and SciRIFF (+6.7%).This demonstrates that task-specific instruction finetuning effectively enhances performance by adapting the model to the specialized tasks in scientific literature understanding.Further, incorporating CPT results in an extra 1.9% improvement on Sci-Assess and a 3.6% gain on SciRIFF.These results highlight the unique role of CPT in pre-adapting the base model to scientific contexts.By curating a high-quality scientific corpus and using it for domain-specific pretraining, the CPT stage injects foundational scientific knowledge into the model.This foundational understanding reduces the gap between general-purpose pretraining and the specific requirements of downstream tasks, allowing the subsequent SFT stage to focus on fine-grained task adaptation rather than compensating for domain knowledge deficiencies.</p>
<p>SFT WITH SCILITINS (Q3)</p>
<p>We explore how fine-tuning with our synthetic instruction set, SciLitIns, impacts performance in scientific literature understanding.We incrementally add three datasets to the SFT training set: Infinity-Instruct, SciRIFF, and SciLitIns.As shown in Table 5, using only the general purpose instruction set, Infinity-Instruct, results in the lowest performance on both SciAssess (47.1%) and SciRIFF (51.2%).This result highlights the limitations of general-purpose instructions in addressing the nuanced demands of scientific literature tasks.General-purpose datasets often lack the domainspecific structure, reasoning, and terminology required for tasks like scientific fact verification or molecule generation.These findings emphasize the need for specialized instructions to bridge the gap between general capabilities and domain-specific expertise.</p>
<p>Adding the training set of SciRIFF to Infinity-Instruct improves performance on SciRIFF significantly (+5.5%) but decreases performance on SciAssess (-0.3%).This drop may be due to SciRIFF's relatively narrower focus, primarily covering biomedicine and clinical medicine, while SciAssess spans broader fields such as biology, chemistry, and material science.This highlights the need for a diverse instruction set that covers diverse scientific domains.Replacing SciRIFF with SciLitIns in the fine-tuning process yields better performance than Infinity-Instruct on both benchmarks, achieving 50.2% on SciAssess and 54.8% on SciRIFF.The diverse task coverage in SciLitIns plays a key role in this improvement.By incorporating synthetic instructions from underrepresented domains, SciLitIns broadens the model's exposure to scientific knowledge, enabling it to generalize across varied scientific disciplines.</p>
<p>Finally, combining SciRIFF and SciLitIns yields the best results, with SciAssess reaching 51.7% and SciRIFF achieving 60.6%.These gains suggest that SciLitIns and SciRIFF complement each other, addressing the limitations of each dataset individually.While SciRIFF focuses on highly specific domains like biomedicine, SciLitIns fills the gaps by covering tasks from broader and lessrepresented scientific areas.</p>
<p>IMPACT OF INSTRUCTION FILTER</p>
<p>We conduct an ablation study to assess the impact of quality filter for synthetic instructions by varying whether the dataset SciLitIns was filtered.As discussed in Section 2.2.2, this filter removes low-quality instructions evaluated from five key aspects.Table 6 shows that applying the filter improves the performance of SciLitLLM-7B on SciAssess (+0.8%) and SciRIFF (+0.5%).</p>
<p>While the performance improvements from filtering are evident, the gains are relatively modest.This suggests that the dataset was already of reasonably high quality before filtering, with only a small proportion of low-quality instructions being removed.It also indicates that the fine-tuning process is robust enough to tolerate some noise, although further improvements can be achieved with higher-quality inputs.</p>
<p>LIMITATIONS</p>
<p>Despite the promising results achieved by SciLitLLM, there are several limitations that should be acknowledged as follows:</p>
<p>• Insufficient data volume.Compared with existing pre-training datasets (AI@Meta, 2024; Yang et al., 2024;Taylor et al., 2022), the amount of data used for CPT is comparatively small.While the dataset has been carefully curated to ensure quality and domain relevance, the limited size poses challenges in achieving comprehensive representation across diverse scientific disciplines.With a smaller corpus, certain specialized may be underrepresented, limiting the model's ability to perform well on tasks within these areas.While increasing dataset size is a logical step, it is crucial to maintain high-quality filter to avoid introducing noise.The effectiveness of the current CPT stage demonstrates that a smaller but well-curated dataset can still deliver significant gains.Therefore, the challenge lies in scaling the corpus size while preserving its relevance and quality.</p>
<p>• Lack of reasoning enhancement.The current pipeline does not explore advanced reasoning techniques such as Chain-of-Thought (Wei et al., 2022) or Tree-of-Thought (Yao et al., 2023) in the data construction or model inference stages.</p>
<p>• Lack of preference alignment.Due to a limited financial budget, the model lacks Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022).RLHF has shown significant improvements in aligning models with human preferences and ensuring more reliable outputs.</p>
<p>Implementing RLHF in future iterations could further enhance the model's reliability.</p>
<p>Addressing these limitations in future research will be crucial developing a more robust and capable LLM specialized in scientific literature understanding.</p>
<p>CONCLUSION AND FUTURE WORKS</p>
<p>In this paper, we introduce SciLitLLM, a specialized model for scientific literature understanding.</p>
<p>It is initialized with a general base model -Qwen2.5, and trained through continual pre-training (CPT) and supervised fine-tuning (SFT).For effective scientific knowledge injection during CPT, we propose model-based format and grammar correction method, along with text quality filtering measures.To ensure high-quality and diverse instructions during SFT, we devise instruction synthesis and quality control approaches.Our experiments on widely-used benchmarks demonstrate the effectiveness of this pipeline in adapting a general model to the field of scientific literature understanding.Specifically, SciLitLLM-7B achieves a 4.0% improvement on the SciAssess (Cai et al., 2024) and a 10.1% improvement on the SciRIFF (Wadden et al., 2024) compared to leading models with fewer than 10 billion parameters.SciLitLLM-14B also surpasses leading open-source LLMs with around 70B parameters.We note that this pipeline could be easily adapted to other specialized domains, particularly those lacking adequate high-quality corpora and instruction sets.</p>
<p>Our future work will focus on expanding the diversity and quality of the training data, as well as exploring more efficient methods for domain-specific knowledge injection and high-quality instruction generation.Moreover, we plan to expand our pipeline to include the RLHF (Ouyang et al., 2022) stage for better human preference alignment and safety.</p>
<p>ETHICS STATEMENT</p>
<p>In developing SciLitLLM, we prioritized ethical considerations to ensure the responsible use of our models and methodologies.First, our research does not involve human subjects, and all data used for continual pre-training (CPT) are copyright-compliant.We employed a rigorous quality filtering process to minimize the risk of incorporating biased or misleading content.Nevertheless, we acknowledge that biases inherent in scientific literature, including historical underrepresentation of certain research domains, may propagate into the model's outputs.We also adhere to all applicable legal and ethical research guidelines, such as respecting copyright policies during dataset construction and providing comprehensive model documentation.Our work is conducted with a commitment to research integrity, ensuring that our contributions remain beneficial to the scientific community while addressing ethical responsibilities associated with developing AI technologies.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>We have made extensive efforts to ensure the reproducibility of our work.Our proposed SciLitLLM models are available.The corresponding training pipeline have been described in Sections 2. The source code for all modules shown in Figure 5 is included in the supplementary material.The training datasets for both CPT and SFT stages are shown in Table 1.</p>
<p>A RELATED WORKS</p>
<p>A.1 KNOWLEDGE INJECTION VIA CONTINUAL PRE-TRAINING</p>
<p>Pre-training a language model is usually conducted on a large corpus of textual data to learn the statistical properties of language (Radford et al., 2019;Brown et al., 2020;Lo et al., 2020;Soldaini et al., 2024).To further inject domain knowledge into a general LLM after pre-training, researchers engage in continual pre-training (CPT) on high-quality domain-specific corpora (Jin et al., 2022;Sun et al., 2020), sometimes combined with general corpora.This process enhances the model's fundamental understanding abilities in specific downstream domains while mitigating catastrophic forgetting of general knowledge (Wu et al., 2022;Ke et al., 2023;Mehta et al., 2023).See the comprehensive study (Gupta et al., 2023) for different warm-up strategies for CPT.Additionally, the CPT corpora can be augmented by transforming them into an instruction-response format (Cheng et al., 2023;2024).Furthermore, the scaling law (Hoffmann et al., 2022) of domain-specific CPT (Que et al., 2024) is explored to determine the optimal mix of data.However, these studies primarily focus on training dynamics and data recipes, leaving the pre-processing for scientific data, especially raw PDF files, largely unexplored.Exhibiting such steps is essential for generating high-quality domain corpora and effectively injecting domain knowledge, representing a significant challenge for practitioners.</p>
<p>A.2 DOMAIN ADAPTATION VIA SUPERVISED FINE-TUNING Supervised fine-tuning (SFT) modifies a pre-trained language model to follow specific instructions or perform designated tasks by fine-tuning it on a targeted, task-specific dataset (Raffel et al., 2020;Wei et al., 2021;Liu et al., 2023;Li et al., 2024;Liu et al., 2024;Liao et al., 2024;2025).Applying SFT to a general LLM for specific domain adaptation has demonstrated effectiveness in various fields: in medicine (Clusmann et al., 2023), corpora of medical literature and clinical notes are used; in law (Cui et al., 2024), legal documents and case law are compiled; and in finance (Wu et al., 2023), financial reports and market data are utilized.In the scientific domain, several studies have specialized LLMs for scientific tasks, often necessitating the construction of a substantial domain-specific dataset with SFT.For example, SciGLM (Zhang et al., 2024a) leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific instructions.ChemLLM (Zhang et al., 2024b), a more specified LLM in the chemistry field, collects structured chemical data from a vast selection of online databases and transforms this structured data into a question-answering format for SFT.SciRIFF (Wadden et al., 2024) converts existing literature understanding datasets into natural language input-output pairs suitable for instruction-tuning using pre-defined templates.However, benchmark studies (Feng et al., 2024;Cai et al., 2024) indicate that SFT alone may not provide adequate scientific knowledge to excel in relevant tasks.This suggests the need for a more comprehensive approach that combines domain knowledge infusion with instruction-following enhancements.</p>
<p>A.3 LLMS FOR SCIENTIFIC LITERATURE UNDERSTANDING</p>
<p>In the scientific domain (Luo et al., 2024;Liu et al., 2025;Shi et al., 2024), existing strategies for developing specialized LLMs mostly fall into two categories: (1) Supervised fine-tuning with scientific instructions.This approach requires a large, high-quality, and diverse set of instructions to cultivate problem-solving abilities for scientific tasks.Representative works (e.g., SciGLM (Zhang et al., 2024a), ChemLLM (Zhang et al., 2024b), and SciRIFF (Wadden et al., 2024)) have been detailed in Section A.2. (2) Pre-training with scientific corpora.This approach involves pre-training on a large corpus of scientific texts to improve performance on downstream scientific tasks.Early attempts, such as SciBert (Beltagy et al., 2019) and KV-PLV (Zeng et al., 2022), are based on BERT (Devlin et al., 2019) and pre-trained on a large corpus of scientific text for downstream scientific task enhancement.More recently, Galactica (Taylor et al., 2022) is pre-trained on a vast corpus of scientific literature, including research papers, scientific articles, and other relevant scientific texts.Despite these advances, two major limitations hinder these models from excelling in scientific literature understanding: (1) lack of scientific knowledge, and (2) inability to follow task instructions.To address these challenges, we propose a combined pipeline of CPT and SFT to devise a specialized LLM for scientific literature understanding.It injects domain-specific knowledge through CPT while enhanc-ing task-specific instruction-following abilities through SFT, leading to a more capable LLM for scientific literature understanding.</p>
<p>B FORMAT &amp; GRAMMAR CORRECTION EXAMPLES</p>
<p>Raw text parsed by PyPDF2</p>
<p>Highly p e n e t r a t i n g radiation, such as γ-rays or fast electorns, deposits ener gy throughout the solid t a r g e t material.Gas production occurs w i t h i n the solid phase and must d i f f u s e to the surface to be observed.The a p p a r e n t yield of H 2 can depend on the radiolysis pro c e d u r e or the particle size if some of the gas remains in the solid.Experiments have shown that the apparent y i e l d of H 2 can vary by a f a c t o r of 3 in the r a d i o l y s i s of polyethylene spheres of 7 to 2100 cm2/g (about 9 to 0.03 mm) [12].The e f f e c t s of gas trapping and diffusion are not understood in the c o n t e x t of waste storage.Extremely h i g h dose rates in the p r o c e s s i n g of certain materials may lead to bubble formation, which could a l t e r product quality.</p>
<p>Text after Format &amp; Grammar Correction</p>
<p>Highly penetrating radiation, such as γ-rays or fast electrons, deposits energy throughout the solid target material.Gas production occurs within the solid phase and must diffuse to the surface to be observed.The apparent yield of H2 can depend on the radiolysis procedure or the particle size if some of the gas remains in the solid.Experiments have shown that the apparent yield of H2 can vary by a factor of 3 in the radiolysis of polyethylene spheres of 7 to 2100 cm2/g (about 9 to 0.03 mm) [12].The effects of gas trapping and diffusion are not understood in the context of waste storage.Extremely high dose rates in the processing of certain materials may lead to bubble formation, which could alter product quality.The yield of H2 in the radiolysis of polymers with γ-rays is well known for several types of polymers [2].</p>
<p>C CPT QUALITY FILTER</p>
<p>We randomly select 50k samples from our CPT data.These selected samples are then scored using the Llama3-70B-Instruct model.The prompt utilized for this scoring process is as follows: Prompt for CPT Data Quality Labelling</p>
<p>Below is an extract from a textbook.Evaluate whether the text has a high educational value and could be useful in an educational setting for teaching from primary school to grade school levels using the additive 5-point scoring system described below.Points are accumulated based on the satisfaction of each criterion:</p>
<p>-Add 1 point if the extract provides some basic information relevant to educational topics, even if it includes some irrelevant or nonacademic content like advertisements and promotional material.</p>
<p>-Add another point if the extract addresses certain elements pertinent to education but does not align closely with educational standards.It might mix educational content with non-educational material, offering a superficial overview of potentially useful topics, or presenting information in a disorganized manner and incoherent writing style.</p>
<p>-Award a third point if the extract is appropriate for educational use and introduces key concepts relevant to school curricula.It is coherent though it may not be comprehensive or could include some extraneous information.It may resemble an introductory section of a textbook or a basic tutorial that is suitable for learning but has notable limitations like treating concepts that are too complex for grade school students.</p>
<p>-Grant a fourth point if the extract is highly relevant and beneficial for educational purposes for a level not higher than grade school, exhibiting a clear and consistent writing style.It could be similar to a chapter from a textbook or a tutorial, offering substantial educational content, including exercises and solutions, with minimal irrelevant information, and the concepts aren't too advanced for grade school students.The content is coherent, focused, and valuable for structured learning.</p>
<p>-Bestow a fifth point if the extract is outstanding in its educational value, and perfectly suited for teaching either at primary school or grade school.It follows detailed reasoning, the writing style is easy to follow, and offers profound and thorough insights into the subject matter, devoid of any non-educational or complex content.</p>
<p>After examining the extract:</p>
<p>-Briefly justify your total score, up to 100 words.</p>
<p>-Conclude with the score using the format: "Educational score: ¡total points¿"</p>
<p>We train a Scientific Texts Quality Classifier on these labeled data samples.The classifier is a 109M BERT (Devlin et al., 2019) classifier, fine-tuned from the checkpoint of fineweb-edu-classifier (Anton et al., 2024).The model is trained for 20 epochs with a learning rate of 0.001 and a batch size of 1024.Ninety percent of the 50K samples are used as the training set, and the rest 10% are used as the validation set.The training process costs approximately 50 minutes on 4 A100 GPUs.We select the checkpoint from the epoch that yields the highest validation micro F1 score as our final checkpoint.During Inference, we set batch size to 2048, and beam number to 1.The inference process costs 90 minutes on 4 A100 GPUs.We utilize the generated to filter out 25% data with the lowest quality.</p>
<p>We also conduct empirical experiments to investigate the sensitivity of model performance to the quality filtering threshold.The results in Table 7 show that setting the filtering threshold at 25% achieves the best overall performance, effectively injecting scientific domain knowledge into the model.When the threshold is set too high (e.g., 35%), the average quality of the corpus improves; however, the significant reduction in data volume limits the ability to fully capture domain knowledge.Conversely, when the threshold is set too low (e.g., 15%), the increase in data volume includes more low-quality content, which negatively impacts model performance.Thus, the trade-off between data quality and quantity is crucial.</p>
<p>D SFT DETAILS D.1 INSTRUCTION GENERATION PIPELINE</p>
<p>In SciLitIns, we focus on generating instructions for three less-represented domains (materials science, medicine, and drug discovery) and five question types:</p>
<p>• • Entity Extraction: Entity Extraction tasks are designed to evaluate a model's ability to extract specific information, such as entities or relationships, from the text.</p>
<p>• Molecule Translation: Molecule Translation tasks evaluate a model's ability to translate molecules between different SMILES formats.</p>
<p>• Molecule Extraction: Molecule Extraction tasks ask a model to extract an appropriate molecule from a scientific paragraph that contains multiple molecules.</p>
<p>• Multiple Choice and True-or-False: Multiple Choice and True-or-False questions assess a model's ability to select the correct answer from a set of options, testing its knowledge and reasoning on both simple and complex scenarios.</p>
<p>For each of the three scientific domains, we collect a set of high-impact research papers and construct a word frequency table.To generate a question in a given domain, we sample 20 keywords from the corresponding word table and insert them into the prompt for that question.To ensure fair representations of less frequent keywords, we use random sampling with a temperature setting of 3. We will release our code, prompt templates, and word frequency tables.</p>
<p>D.2 INSTRUCTION DEDUPLICATION</p>
<p>The generated synthetic data may contain similar questions or identical answers.To eliminate redundancy, we implement a fuzzy deduplication process using the Levenshtein distance to calculate the similarity score between question-answer pairs.Specifically, for two pairs (q 1 , a 1 ) and (q 2 , a 2 ), their textual similarity is defined as (1 − lev(q 1 , q 2 ))(1 − lev(a 1 , a 2 )), where lev(•, •) denotes the Levenshtein distance.Due to significant differences between texts from different question types, we compute similarity matrices separately for each type.We then use a disjoint-set data structure to merge highly similar data points.We use this process to remove approximately 5% to 10% of duplicated data for each question type.</p>
<p>D.3 QUALITY ASSESSMENT OF GENERATED SFT INSTRUCTIONS</p>
<p>In section 2.2.2, we sample 10k instruction pairs from SciLitIns and evaluate them by Llama-3-70B using the below prompt.Specifically, the quality is evaluated from five aspects: clarity, complexity, correctness, usefulness, and adaptability, assigning each instruction a score from 0 (lowest quality) to 5 (highest quality).</p>
<p>• Human-Human Agreement: Calculated as the average Spearman correlation coefficient across all pairs of annotators.</p>
<p>• Human-Quality Filter Agreement: Calculated as the average Spearman correlation between each annotator's scores and the quality filter's score.</p>
<p>A higher Spearman correlation indicates a stronger agreement between the compared sets of scores.</p>
<p>The evaluation results are summarized in Table 11.The results show that for both datasets, the Human-Quality Filter Agreement is comparable to, or even exceeds, the Human-Human Agreement.This indicates that the quality filter closely aligns with human annotations, demonstrating its reliability in capturing quality as perceived by humans.</p>
<p>Figure 2 :
2
Figure 2: An example of scientific literature understanding in SciRIFF.It involves extracting accurate entities from a biomedicine paper.SciLitLLM-7B demonstrates sufficient scientific knowledge and instruction-following ability to accurately identify and extract these entities.</p>
<p>Figure 3 :
3
Figure 3: Comparison of strategies to adapt LLMs to scientific tasks.</p>
<p>Figure 4 :
4
Figure 4: Examples of high and low-quality CPT text and SFT instructions.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Score distribution of the CPT Data.The filtered-out 25% data are marked gray, while the remaining 75% are marked orange.</p>
<p>on</p>
<p>Infinity-Instruct to cultivate their general instruction-following abilities, then for five epochs on SciLitIns and SciRIFF for scientific literature understanding enhancement.The training is conducted with a sequence length of 4,096, a maximum learning rate of 1 × 10 −5 , and a cosine scheduler.The SFT training takes approximately 32 hours for the 7B and 70 hours for the 14B model on 32 A100 GPUs, resulting in SciLitLLM-7B-Instruct and SciLitLLM-14B-Instruct.</p>
<p>Data Collection 72k Textbooks 10M Journal Paragraphs Open-source Data
CPT Quality FilteringLlama3-70B50K Subset of Formatted DataData Quality Labelling</p>
<p>Scientific Texts Quality Classifier Fineweb-Edu Classifier Checkpoint CPT Data CPT: Scientific Knowledge Injection Qwen2.5 Base</p>
<p>Here is the corrected version of the text: ….I have extracted the following raw text from a PDF, but the extraction process has introduced formatting issues.Please help me correct these formatting issues and provide a clean, readable version of the text.
Reformat &amp; Grammar CorrectionFilter outLowest 25%Lla ma3-8B</p>
<p>Raw Text: {Raw Text}
Question: <paragraph w/ keywords>,Please extract all relevant entities andrelationships.Answer: (patterns,</p>
<p>chemical reaction) Generate an Entity Extraction question using following key words: chemical, acid, … SFT: Scientific Instruction Following</p>
<p>acid alloy chemical SciLitLLM-Base SciLitLLM-Instruct SFT Quality Filtering</p>
<p>The pipeline of SciLitLLM consists of two key stages: continual pre-training (CPT) for scientific knowledge injection and supervised fine-tuning (SFT) for scientific instruction following.Specifically, the CPT stage involves several modules: PDF parsing, format &amp; grammar correction (cf.Section 2.1.1),and quality filtering (cf.Section 2.1.2) modules.These modules ensure the model is equipped with high-quality scientific domain knowledge.The SFT stage includes LLMbased instruction generation (cf.Section 2.2.1) and instruction quality control (cf.Section 2.2.2) measures.These steps are designed to fine-tune the model's ability to follow scientific instructions accurately and effectively.
Word Frequency TableKeywords &amp; Task Guided Instruction GenerationLlama3-70BExtract word frequency from papers+Pre-defined TasksSFT Data• Entity ExtractionMulti-Dim Criterions• Table Extraction • Molecule Generation • ……GPT4oDeduplication• Clarity • Usefulness • ……Low Quality DataFigure 5: Stage Data sourceDomain #Doc/#Ins # TokensIn-house TextbooksScience73k10BCPTIn-house JournalsScience625k2.7BRedpajama (Computer, 2023) General-11BSciLitInsScience93k86MSFTSciRIFF (Wadden et al., 2024) Science70k40MInfinity-InstructGeneral3M1.7B</p>
<p>Table 1 :
1
Data statistics of CPT and SFT stages.Underlined datasets are curated by us.</p>
<p>Table 2 :
2
Accuracy(%) comparison of base models.SciLitLLM outperforms Llama3.1 and Qwen2.5 with similar scales, demonstrating improved domain-specific understanding through continual pretraining on scientific corpora.Underline indicates those trained on domain-specific corpora.
ModelsMMLU-Pro-BioMMLU-Pro-ChemMMLU-Pro-HealMaScQAAvg.# Parameters &lt; 10BLlama3.1-8B-Base56.3525.8844.5053.2845.00Qwen2.5-7B-Base68.3648.3751.6456.2356.15SciLitLLM-7B-Base70.4551.2154.0659.2758.75# Parameters &gt; 10BQwen2.5-14B-Base79.7862.7362.2266.9267.91SciLitLLM-14B-Base80.6264.4464.2468.0669.34DatasetDomain/ Task# Parameter &lt; 10B Mistral-7B Llama3.1-8B Qwen2.5-7B SciTulu-7B SciLitLLM-7B Qwen2.5-14B Llama3.1-70B Qwen2.5-72B SciLitLLM-14B GPT3.5 GPT4o # Parameter &gt; 10B APIBiology52.063.460.945.365.365.969.467.267.055.468.9Chemistry34.046.147.919.055.460.062.563.763.437.368.0SciAssessMaterial36.550.948.931.353.757.959.259.462.637.062.0Medicine25.130.228.319.932.431.537.437.839.931.845.8Mean36.947.746.528.951.753.857.157.058.240.461.2BioASQ43.245.045.337.551.248.043.546.753.947.346.7BioR48.948.143.955.776.656.861.557.883.353.961.0DiscMT44.568.873.761.571.170.577.354.777.767.978.3EI17.217.223.311.623.527.226.026.330.819.224.7SciRIFFMC MuP47.5 87.549.7 87.050.8 84.934.6 72.170.7 67.554.2 91.560.9 90.958.0 94.072.1 67.747.8 76.858.7 86.9Qasper 48.8/43.2 46.5/42.738.1/25.754.2/38.650.7/54.148.4/46.849.1/40.261.9/50.554.0/55.154.7/39.8 67.8/50.5SciERC31.132.930.135.649.933.736.234.154.728.642.2SciFact 68.8/53.8 63.8/53.777.8/60.366.0/49.283.5/67.383.1/65.687.3/70.386.5/69.691.0/68.669.7/53.3 84.3/68.7Mean48.650.550.347.060.656.958.558.264.450.860.9
and Infinity-Instruct 6 , as shown in Table1.Infinity-Instruct is a collection of more than twenty opensource instructions datasets, covering various general domains.SciRIFF and SciLitIns contain specialized instructions for scientific literature understanding.For both scales, we train for one epoch</p>
<p>Table 3 :
3
Model performances on scientific literature understanding benchmarks: SciAssess and SciRIFF.SciLitLLM-7B and SciLitLLM-14B achieve leading performance compared with opensource models of similar scales.The best-performing models smaller than 10B and larger than 10B are highlighted in bold.Results for SciTulu-7B, GPT-3.5, and GPT-4o on SciRIFF are taken from its original papers, while all other results are produced by us.Underline indicates those trained on domain-specific instructions.</p>
<p>have extracted the following raw text from a PDF, but the extraction process has introduced many formatting issues such as unnecessary line breaks, extra spaces, and other artifacts that disrupt the text flow.Could you please help me correct these formatting issues and provide a clean, readable version of the text?Respond with the Corrected Version only.
The y i e l d of H2 in the r a d i o l y s i s of polymers w i t h γ-rays is well known for severaltypes of p o l y m e r s [2].Format &amp; Grammar Correction PromptRaw Text:{RawText}Start your response with "Here is the corrected version of the text:".
I</p>
<p>Table 7 :
7
7B base model performance (accuracy %) with different CPT data filtering thresholds.
Filtering Threshold MMLU-Pro-Bio MMLU-Pro-Chem MMLU-Pro-Heal MaScQAAvg.15%70.5950.3253.1453.1458.3225%70.4551.2154.0654.0658.7535%69.8249.5251.6461.6457.67</p>
<p>Table Extraction :
Extraction
Table Extraction tasks evaluate a model's proficiency in extracting, summarizing, and structuring data from an article into a table format.</p>
<p>Below is an example of generating a table extraction question: Prompt for Generating a Table Extraction Question I need synthetic training data for training a machine learning model that extracts tables from text correctly.The data should be formatted in JSON, with each entry containing "text" and "answer" attributes.You should generate a paragraph that includes the keywords: {{keywords}}.The "text" part must contain enough information for the table to be extracted!In "text" part, You must you include a table description in latex format.Special notice for the table content: You should generate a table that has complicated numbers and characters, include non-standard characters, and have a variety of values.Make sure the value you generated do not follow simple patterns, for example, never include deplicate values or values with constant interval in columns.Your answer should contain as much details as possible.You should only generate one JSON.The value for the two attributes should be two string.Use {{ and }} to warp your output.Pay attention to the escape characters in the latex format.Remember to put a comma at the end of the first string.Never use a json block to wrap your output.Here is the format for your output: { "text": "Your paragraph here, remember to include a table in latex format", "answer": "Your answer table here" } Now start your answer:</p>
<p>Table 11 :
11
Comparison of Human-Human Agreement and Human-Quality Filter Agreement for CPT and SFT datasets.
Agreement Human-Human Human-Quality FilterCPT0.580.76SFT0.890.88
https://github.com/dptech-corp/Uni-SMART
https://huggingface.co/collections/Uni-SMART/scilitllm15-67283353ada975ba995629ef
https://pypdf2.readthedocs.io
Phi models(Gunasekar et al., 2023;Li et al., 2023;Abdin et al., 2024) propose to determine the quality of a pre-training text by its educational value for a student whose goal is to learn basic domain concepts.
https://scholar.google.com/
https://huggingface.co/datasets/BAAI/Infinity-Instruct
ACKNOWLEDGEMENTThe work of Sihang Li, Yaorui Shi, and Xiang Wang, is in part supported by the National Natural Science Foundation of China (92270114).SFT Evaluation PromptYou are a helpful and precise assistant for checking the quality of instruction-tuning data for large language models.Your task is to evaluate the given instruction using the criterions described below.-Clarity: The sample should be clear, specific, and unambiguous, providing a well-defined task for the model to perform.-Complexity: The sample should be advanced complexity that necessitate a high level of comprehension and cognitive processing, challenging the language model significantly.-Correctness: The sample is impeccably written, with flawless grammar, syntax, and structure, demonstrating exceptional clarity and professionalism.-Usefulness: The sample should be highly useful, and contribute to expanding the model's knowledge base.-Adaptability: The sample could be adapted to different contexts or use cases, showing some flexibility.After examining the instruction-response pair: -Briefly justify your scores with a paragraph in the field "Explanation", up to 500 words.-For each point of criterion above, assign a score from 1 to 5.-You should only provide the rest of your answer in a structured format as shown below, and make sure your response can be directly parsed by computer programs.Below is a template for your response: Explanation: ¡string, your explanations to the scores¿ ==================== { "Clarity": <int, complexity score>, "Complexity": <int, complexity score>, "Correctness": <int, quality score>, "Usefulness": <int, usefulness score>, "Adaptability": <int, adaptability score>, "Total": <int, total score> } Below is an example of SciLitIns, which will be sent to Llama-3-70B for evaluation.An Example in SciLitIns ContextIn recent studies regarding material science, the crmatrix has shown significant importance in understanding fracture behaviors.Alternatively, many researchers have proposed a series of experiments to analyze these phenomena.Even though the process is delicate, the enthusiasm for discovering new findings is understandable.The following Table1presents a schematic illustration of the enthalpy changes observed during the different fracture phases.These studies are indispensable for advancing our knowledge of the material's resilience.In addition, in certain cases, pronounced errors in measurement can occur, but the references provided can guide through the correct methodology.Prime among the tools used is the differential scanning calorimetry which helps measure these enthalpy changes accurately.Below is the evaluation by Llama-3-70B on the example from SciLitIns.Each criterion is scored out of 5 points, making the total possible points 25.The total score is then averaged, resulting in an overall score out of 5.Evaluation by Llama-3-70BExplanation:The instruction is clear and specific, asking the model to extract a table from a given paragraph.The paragraph itself is well-written, with proper grammar and syntax, making it easy to understand.The table is also well-structured and provides valuable information about enthalpy changes during fracture phases.The task requires a good understanding of the text and the ability to identify the table.However, the complexity of the task is relatively low, as it only involves extracting a table, and the context is not particularly nuanced or ambiguous.The task is useful for advancing knowledge in material science, and the table could be adapted to different contexts or use cases.==================== "Clarity": 5, "Complexity": 2, "Correctness": 5, "Usefulness": 4, "Adaptability": 4, "Total": 20E BENCHMARK DETAILSTo the best of our knowledge, there are two commonly-adopted datasets for scientific literature understanding:• SciRIFF(Wadden et al., 2024)evaluates essential scientific literature understanding capabilities, including information extraction, summarization, question answering, claim verification, and classification.Data points in SciRIFF are notable for their long input contexts and complicated structured outputs.The Qasper and SciFact tasks have two different evaluation methods and thus two results.For MuP and one evaluation method of Qasper, we use GPT-4o as language model evaluator, while the original paper uses GPT-3.5.We note that SciRIFF contains a separate training set used in the SFT stage in our study.• SciAssess(Cai et al., 2024)features an end-to-end benchmark of understanding PDF content.It includes 28 tasks from four scientific domains: biology, chemistry, materials and medicine.SciAssess is used exclusively for testing in our evaluation.Overall, SciRIFF provides basic benchmarks for comprehending short scientific segments and various instructions, while SciAssess presents more challenging tasks involving longer contexts from raw PDFs.F DETAILED PERFORMANCE ON SCIASSESSThe detailed results on each task in SciAssess are shown in Table8.Domain TaskSciTulu-7B Mistral-7B Llama3.1-8BQwen2.5-7BSciLitLLM-7B Qwen2.5-14BLlama3-70B Qwen2.5-72BSciLitLLM-14B GPT3.5 GPT4oG CONTAMINATION STUDYTo investigate data contamination in our datasets and its potential impact on downstream performance, we conducted a contamination analysis in this section.Prior studies consistently indicate that data contamination in pre-training datasets has negligible effects on evaluation performance:• The GPT-4 technical report states: Contamination overall has very little effect on the reported results (OpenAI, 2023).For instance, tasks such as AP US History (73% contamination rate), AP World History (47% contamination rate), and LSAT (39% contamination rate) show negligible differences in performance between the full evaluation set and a clean evaluation set with all contaminated data points removed.• PaLM analyzed datasets with contamination rates ranging from 20% to 75% and concluded that data contamination does not cause meaningful inflation of our reported results(Chowdhery et al., 2023).For our contamination analysis, we adopt the contamination detection method described in the GPT-4 technical report: For each evaluation instance, we randomly extract three substrings of 50 characters each.A match is determined if any of the three sampled substrings appears as a part of the processed training example.The contamination rates for our textbook and journal datasets are summarized in Table9.Our analysis reveals that contamination rates are very low for SciRIFF but higher for SciAssess, particularly in the textbook dataset.To evaluate whether contamination impacts model performance, we assessed SciLitLLM on SciAssess using both the full evaluation set and a clean evaluation set (with all contaminated examples removed).As shown in Table10, the differences in performance between the full and clean evaluation sets are minimal, consistent with prior findings.These results reinforce the conclusion that data contamination does not significantly affect the performance of SciLitLLM.H HUMAN EVALUATION OF DATASET QUALITYTo further validate the effectiveness of the proposed framework for creating the CPT and SFT datasets, we conducted a human evaluation study.This study involved sampling random subsets of unfiltered CPT and SFT data and comparing human annotations to the scores generated by our quality filters.For the CPT dataset, we sampled 50 entries from the textbook and journal datasets.For the SFT dataset, we sampled 50 entries from SciLitIns.Each sampled entry was independently evaluated by four annotators using the same prompts provided to the LLMs.To assess the alignment between human annotations and the quality filter, we computed two measures:
. Sam Ade Marah I Abdin, Ammar Jacobs, Jyoti Ahmad Awan, Ahmed Aneja, Hany Awadallah, Nguyen Awadalla, Amit Bach, Arash Bahree, Harkirat S Bakhtiari, Alon Behl, Misha Benhaim, Johan Bilenko, Sébastien Bjorck, Martin Bubeck, Caio Cai, Teodoro César, Weizhu Mendes, Vishrav Chen, Parul Chaudhary, Allie Chopra, Gustavo Del Giorno, Matthew De Rosa, Ronen Dixon, Dan Eldan, Amit Iter, Abhishek Garg, Suriya Goswami, Emman Gunasekar, Junheng Haider, Russell J Hao, Jamie Hewett, Mojan Huynh, Xin Javaheripi, Piero Jin, Nikos Kauffmann, Dongwoo Karampatziakis, Mahoud Kim, Lev Khademi, James R Kurilenko, Yin Lee, Yuanzhi Tat Lee, Chen Li, Weishung Liang, Eric Liu, Zeqi Lin, Piyush Lin, Arindam Madan, Hardik Mitra, Anh Modi, Brandon Nguyen, Barun Norick, Daniel Patra, Thomas Perez-Becker, Reid Portet, Heyang Pryzant, Marko Qin, Corby Radmilac, Sambudha Rosset, Olatunji Roy, Olli Ruwase, Amin Saarikivi, Adil Saied, Michael Salim, Shital Santacroce, Ning Shah, Hiteshi Shang, Xia Sharma, Masahiro Song, Xin Tanaka, Rachel Wang, Guanhua Ward, Philipp Wang, Michael Witte, Can Wyatt, Jiahang Xu, Sonali Xu, Fan Yadav, Ziyi Yang, Donghan Yang, Chengruidong Yu, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhang, Zhou, 2024Phi-3 technical report: A highly capable language model locally on your phone. CoRR, abs/2404.14219</p>
<p>. Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan M Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick Legresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues De Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro ; Simhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, Chen Zhu, 10.48550/arXiv.2406.11704Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Nar-Nemotron-4 340b technical report. CoRR, abs/2406.11704, 2024</p>
<p>Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using GPT-4. CoRR, abs/2311.073612023</p>
<p>Llama 3 model card. A I , Meta , 2024</p>
<p>. Lozhkov Anton, Ben Allal Loubna, Von Werra Leandro, Wolf Thomas, May 2024Fineweb-edu</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, EMNLP/IJCNLP (1). Association for Computational Linguistics2019</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020NeurIPS</p>
<p>Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Yuqi Yin, Yaqi Li, 10.48550/arXiv.2403.01976Linfeng Zhang, and Guolin Ke. Sciassess: Benchmarking LLM proficiency in scientific literature analysis. CoRR. 2024</p>
<p>Adapting large language models via reading comprehension. Daixuan Cheng, Shaohan Huang, Furu Wei, CoRR, abs/2309.095302023</p>
<p>Instruction pre-training: Language models are supervised multitask learners. Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, Furu Wei, arXiv:2406.144912024arXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. David Cheng, -Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Anna Rogers, Jordan L Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 9-14, 2023. 20231Long Papers), ACL 2023</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Dohan, 10.18653/v1/2023.acl-long.870Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel2023JMLR</p>
<p>The future landscape of large language models in medicine. Jan Clusmann, Fiona R Kolbinger, Sophie Hannah, Muti, Jan-Niklas Zunamys I Carrero, Narmin Eckardt, Chiara Ghaffari Laleh, Lavinia Maria, Sophie-Caroline Löffler, Michaela Schwarzkopf, Gregory P Unger, Veldhuizen, Communications medicine. 311412023</p>
<p>Redpajama: an open dataset for training large language models. 2023Together Computer</p>
<p>Chatlaw: A multi-agent collaborative legal assistant with knowledge graph enhanced mixture-of-experts large language model. Jiaxi Cui, Munan Ning, Zongjian Li, Bohua Chen, Yang Yan, Hao Li, Bin Ling, Yonghong Tian, Li Yuan, 2024</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT (1). Association for Computational Linguistics2019</p>
<p>Tinystories: How small can language models be and still speak coherent english?. Ronen Eldan, Yuanzhi Li, 10.48550/arXiv.2305.07759CoRR, abs/2305.077592023</p>
<p>Practical, efficient, and customizable active learning for named entity recognition in the digital humanities. Alexander Erdmann, David Joseph Wrisley, Benjamin Allen, Christopher Brown, Sophie Cohen-Bodénès, Micha Elsner, Yukun Feng, Brian Joseph, Béatrice Joyeux-Prunel, Marie-Catherine De Marneffe, NAACL-HLT (1). Association for Computational Linguistics2019</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen, 10.48550/arXiv.2306.08018CoRR, abs/2306.080182023</p>
<p>Sciknoweval: Evaluating multi-level scientific knowledge of large language models. Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, Huajun Chen, 10.48550/arXiv.2406.090982024CoRR</p>
<p>Textbooks are all you need. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César, Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo De Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, CoRR, abs/2306.11644Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li2023</p>
<p>Continual pre-training of large language models: How to (re)warm your model?. Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, L Mats, Quentin Richter, Eugene Anthony, Irina Belilovsky, Timothée Rish, Lesort, CoRR, abs/2308.040142023</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Simonyan, CoRR, abs/2203.15556Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre2022</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Florian Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 10.48550/arXiv.2310.06825Mistral 7b. CoRR, abs/2310.06825. 2023</p>
<p>Lifelong pretraining: Continually adapting language models to emerging corpora. Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen, Xiaokai Li, Andrew O Wei, Xiang Arnold, Ren, NAACL-HLT. Association for Computational Linguistics2022</p>
<p>Continual pretraining of language models. Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, Bing Liu, ICLR. OpenReview.net. 2023</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, SOSP. ACM2023</p>
<p>Towards 3d molecule-text interpretation in language models. Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, Qi Tian, In ICLR. Open-Review. 2024net</p>
<p>Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need II: phi-1.5 technical report. Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, CoRR, abs/2309.054632023</p>
<p>Llara: Large language-recommendation assistant. Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang, Xiangnan He, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Patchrec: Multi-grained patching for efficient llm-based sequential recommendation. Jiayi Liao, Ruobing Xie, Sihang Li, Xiang Wang, Xingwu Sun, Zhanhui Kang, Xiangnan He, arXiv:2501.150872025arXiv preprint</p>
<p>Beyond one-model-fits-all: A survey of domain specialization for large language models. Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris White, Quanquan Gu, Carl Yang, Liang Zhao, 10.48550/arXiv.2305.187032023</p>
<p>Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeDecember 6-10, 2023. 2023</p>
<p>Prott3: Protein-to-text generation for text-based protein understanding. Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, Tat-Seng Chua, ACL (1). Association for Computational Linguistics2024</p>
<p>NEXT-MOL: 3d diffusion meets 1d language modeling for 3d molecule generation. Zhiyuan Liu, Yanchen Luo, Han Huang, Enzhi Zhang, Sihang Li, Junfeng Fang, Yaorui Shi, Xiang Wang, Kenji Kawaguchi, Tat-Seng Chua, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>S2ORC: the semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel S Weld, 10.18653/v1/2020.acl-main.447ACL. 2020</p>
<p>Text-guided small molecule generation via diffusion model. iScience. Yanchen Luo, Junfeng Fang, Sihang Li, Zhiyuan Liu, Jiancan Wu, An Zhang, Wenjie Du, Xiang Wang, 272024</p>
<p>An empirical investigation of the role of pre-training in lifelong learning. Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, Emma Strubell, 10.48550/arXiv.2303.08774CoRR, abs/2303.08774OpenAI. GPT-4 technical report. 2023. 202324</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, NeurIPS. 2022</p>
<p>MCAL: minimum cost human-machine active labeling. Hang Qiu, Krishna Chintalapudi, Ramesh Govindan, ICLR. OpenReview.net. 2023</p>
<p>D-cpt law: Domain-specific continual pre-training scaling law for large language models. Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, arXiv:2406.013752024arXiv preprint</p>
<p>5: A party of foundation models. Team Qwen, Qwen2, September 2024</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 21672020</p>
<p>Yaorui Shi, Sihang Li, Taiyan Zhang, Xi Fang, Jiankun Wang, Zhiyuan Liu, Guojiang Zhao, Zhengdan Zhu, Zhifeng Gao, arXiv:2412.07819Renxin Zhong, et al. Intelligent system for automated molecular patent infringement assessment. 2024arXiv preprint</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, 10.18653/v1/2023.emnlp-main.338EMNLP. 2023</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Schärli, Aakanksha Chowdhery, ; R Webster, Gregory S Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, 10.48550/arXiv.2212.13138CoRR, abs/2212.13138Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan2022Blaise Agüera y Arcas, DalePhilip Andrew Mansfield</p>
<p>Dolma: an open corpus of three trillion tokens for language model pretraining research. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Raghavi Khyathi, Jennifer Chandu, Yanai Dumas, Valentin Elazar, Ananya Hofmann, Sachin Harsh Jha, Li Kumar, Xinxi Lucy, Nathan Lyu, Ian Lambert, Jacob Magnusson, Niklas Morrison, Aakanksha Muennighoff, Crystal Naik, Matthew E Nam, Abhilasha Peters, Kyle Ravichander, Zejiang Richardson, Emma Shen, Nishant Strubell, Oyvind Subramani, Evan Pete Tafjord, Luke Walsh, Noah A Zettlemoyer, Hannaneh Smith, Iz Hajishirzi, Dirk Beltagy, Jesse Groeneveld, Kyle Dodge, Lo, 10.18653/v1/2024.acl-long.840ACL. 2024</p>
<p>ERNIE 2.0: A continual pre-training framework for language understanding. Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hua Hao Tian, Haifeng Wu, Wang, AAAI. AAAI Press2020</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, 10.48550/arXiv.2211.09085CoRR, abs/2211.09085Galactica: A large language model for science. 2022</p>
<p>Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, CoRR, abs/2302.139712023</p>
<p>Sciriff: A resource to enhance language model instruction-following over scientific literature. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, Arman Cohan, 10.48550/arXiv.2406.07835CoRR, abs/2406.078352024</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen, 2024</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, NeurIPS2022</p>
<p>Bloomberggpt: A large language model for finance. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David S Rosenberg, Gideon Mann, 10.48550/arXiv.2303.175642023</p>
<p>Pretrained language model in continual learning: A comparative study. Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li, Guilin Qi, Gholamreza Haffari, ICLR. OpenReview.net. 2022</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. 2024</p>
<p>Fingpt: Open-source financial large language models. Hongyang Yang, Xiao-Yang Liu, Christina Dan Wang, 10.48550/arXiv.2306.06031CoRR, abs/2306.060312023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, NeurIPS2023</p>
<p>Mohd Zaki, Krishnan, arXiv:2308.09115A question answering dataset for investigating materials science knowledge of large language models. 2023arXiv preprint</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Zheni Zeng, Yuan Yao, Zhiyuan Liu, Maosong Sun, Nature communications. 138622022</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, 10.48550/arXiv.2401.07950CoRR, abs/2401.079502024a</p>
<p>Chemllm: A chemical large language model. Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, Shufei Zhang, Mao Su, Hansen Zhong, Yuqiang Li, Wanli Ouyang, 10.48550/arXiv.2402.06852CoRR, abs/2402.068522024b</p>
<p>MASSW: A new dataset and benchmark tasks for ai-assisted scientific workflows. Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, Qiaozhu Mei, 10.48550/arXiv.2406.06357CoRR, abs/2406.063572024c</p>
<p>Large language models for scientific synthesis, inference and explanation. Yizhen Zheng, Yee Huan, Jiaxin Koh, Anh T N Ju, Lauren T Nguyen, Geoffrey I May, Shirui Webb, Pan, CoRR, abs/2310.079842023</p>
<p>Table 8: Detailed model performance on SciAssess tasks. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3736 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3736</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3736</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-370cea8b4220917f45a69358c0303df71f5063c7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/370cea8b4220917f45a69358c0303df71f5063c7" target="_blank">ThinkSum: Probabilistic reasoning over sets using large language models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, thinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think – retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum – probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3736.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3736.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sports understanding (plausibility)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sports Understanding — plausibility/posterior estimation of player-action compatibility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses LLM-generated example sets and normalized LLM likelihoods to compute posterior probabilities that a given professional athlete plausibly performed a sport-specific action; low posterior mass on the queried player indicates the sentence is implausible.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci) and InstructGPT (text-davinci-002); also GPT-2 variants for some experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 (davinci) — autoregressive transformer (largest 175B variant) used via OpenAI API; InstructGPT (text-davinci-002) — instruction-tuned GPT-3 variant; GPT-2 family (Small/Medium/Large/XL) used locally for ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate posterior probability that a given sentence 'Player X performed action A' is plausible (i.e., whether player X's sport matches action A) by comparing LLM likelihoods across a generated set of players who perform A plus the queried player.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Think stage: Example generation prompt to produce a set S of players known to perform action A. Sum stage: compute posterior p(y|a) = p_LLM("y a") / sum_{y' in S ∪ {x}} p_LLM("y' a") and threshold the posterior on x to decide plausibility; generation temperature and max_tokens set as described (temperature=0.5 for this task).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BIG-bench — SPORTS UNDERSTANDING task (questions that pair professional sports players with sport-specific actions and ask 'plausible/implausible').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Classification accuracy (plausible vs implausible). Reported numbers (Table 1 / Appendix A.1): ThinkSum using GPT-3 davinci: ~0.71; GPT-3 zero-shot baseline: ~0.50; InstructGPT (text-davinci-002) reported ~0.74; GPT-2 XL ~0.54. Threshold used: 0.01 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>ThinkSum posterior computation substantially outperforms zero-shot direct prompting (davinci) and improves plausibility detection versus naive scoring; InstructGPT sometimes slightly outperforms or matches ThinkSum depending on model variant (text-davinci-002 ≈ 0.74).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on quality/diversity of generated example set S; requires prompt engineering to get generations in a predictable format; computational cost grows with number of generated examples; threshold choice affects decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Normalizing LLM likelihoods over a generated set (posterior computation) yields calibrated-looking posteriors useful for implausibility detection (hallucination/semantic mismatch), and ThinkSum's approach is effective even with smaller model variants when structured as Think+Sum.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ThinkSum: Probabilistic reasoning over sets using large language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3736.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3736.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Known Unknowns (answer-known detection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Known Unknowns — posterior-based detection of whether a question's precise answer is knowable</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses LLM list-extension to propose alternative candidate answers and then forms a posterior over the original candidate and generated alternatives to decide whether the given answer is plausibly correct (known) or should be labeled 'Unknown'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci) and InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 (davinci) large autoregressive model (175B) and its instruction-tuned variant text-davinci-002; generation settings: max_tokens=100, temperature varied (0.5 for list extension).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Classify whether the precise answer to a factual question is known (select the precise answer) or 'Unknown' by comparing normalized LLM likelihoods over the provided candidate and generated alternative answers.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Think: List-extension prompt generates set S of plausible answers. Sum: compute posterior p(y|q) = p_LLM("q ? y") / sum_{y' in S ∪ {a}} p_LLM("q ? y'") and threshold the posterior on the original answer a to decide known vs unknown. Thresholding rule in experiments: margin 1/N_e (N_e number of examples).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BIG-bench — KNOWN UNKNOWNS task (questions where correct option may be a precise fact or 'Unknown').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy (choosing correct precise answer vs 'Unknown'). Reported values: Table 1 and Table 4 show mixed results: with GPT-3 (davinci) direct/coT/ThinkSum variants — ThinkSum (davinci) ≈ 0.54 (lower than some CoT baselines), InstructGPT (davinci-002) ThinkSum ≈ 0.76, CoT ≈ 0.74; average-human baseline listed as ~0.80 in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>ThinkSum sometimes underperforms chain-of-thought or instruction-tuned models on this task for particular model variants (davinci), but with instruction-tuned davinci-002 ThinkSum matches or exceeds CoT; demonstrates sensitivity to model variant and tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance depends strongly on generation quality of alternative answers and on model variant; ThinkSum with vanilla davinci underperformed CoT in some settings; requires careful choice of N_e and thresholding; computational cost of generating and scoring alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Posterior normalization over generated alternatives provides a principled decision rule for 'known vs unknown' and can be effective when combined with instruction-tuned models, showing ThinkSum can be used to quantify uncertainty about factual answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ThinkSum: Probabilistic reasoning over sets using large language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3736.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3736.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uncertainty / hallucination detection (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uncertainty and Hallucination Detection via LLM likelihoods and ThinkSum aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors apply ThinkSum-style posterior computations and likelihood-ratio/thresholding methods to detect implausible or hallucinated model outputs and to assess model uncertainty about facts by comparing conditional and unconditional likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci), InstructGPT (text-davinci-002), GPT-2 family</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive LLMs (GPT-3 davinci 175B), instruction-tuned variants (text-davinci-002), and smaller GPT-2 models for ablations; generation parameters vary (max_tokens up to 1000 for fact generation; temperature 0 or 0.5 depending on task).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Detect hallucinations or high uncertainty in generated statements by computing normalized likelihoods/posteriors over sets of candidate statements or by using ratios of conditional and unconditional likelihoods to correct for surface-form biases.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Think operations include Example/Fact generation and List-of-words likelihood queries; Sum operations include mixture averaging, product aggregation, and likelihood ratio normalization (e.g., divide conditional likelihood by unconditional likelihood) to form calibrated scores or posteriors; thresholding then used to flag hallucinations or unknowns.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>BIG-bench tasks related to hallucination/uncertainty: Known Unknowns, Sports Understanding, and related evaluation slices in Appendix A.1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics are accuracy for detection tasks; paper reports substantial improvements in several hallucination/uncertainty tasks when using ThinkSum-style posterior normalization compared to direct prompting. Specific example numbers: EMOJI MOVIE ThinkSum ≈ 0.80 vs direct davinci ≈ 0.12 (Table 1); other tasks show similar large gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>ThinkSum generally outperforms direct zero-shot/few-shot prompting and auxiliary knowledge appending in uncertainty/hallucination related tasks, and often matches or exceeds CoT when combined with instruction-tuned models; auxiliary knowledge approaches can degrade with long appended prompts while ThinkSum's external aggregation is more robust.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires design of Think prompts and post-processing to ensure generated facts are in analyzable format; post-processing heuristics (filtering short sentences, ensuring single-object mentions) are task-specific; scaling costs increase with number of generated facts and candidate substitutions.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Aggregating probabilities externally (Sum) over LLM-generated hypotheses reduces sensitivity to prompt order/format and can detect implausible or hallucinated statements more reliably than single-pass LLM scoring; smoothing/averaging over many hypotheses helps mitigate spurious high-probability surface patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ThinkSum: Probabilistic reasoning over sets using large language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3736.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3736.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ThinkSum probabilistic aggregation paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ThinkSum — two-stage LLM-based probabilistic inference (Think + Sum)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general two-stage paradigm where LLMs are used in a 'Think' stage to produce sets of candidate strings, facts, translations or likelihood matrices, and a separate 'Sum' stage performs explicit probabilistic inference (mixture/product/ratio/EM) over those outputs to produce final probabilities or decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (GPT-2 family, GPT-3 variants ada->davinci, InstructGPT text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ThinkSum is model-agnostic; experiments used GPT-2 (Small/Medium/Large/XL), GPT-3 (ada,babbage,curie,davinci) and InstructGPT (text-davinci-002) with appropriate decoding settings; GPT-3 davinci corresponds to the 175B model used via OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>General estimation of probabilities/likelihoods of candidate completions, statements, translations, or labels across multiple BIG-bench tasks (semantic relatedness, logical deduction, invented-words, hallucination/uncertainty detection), by explicitly aggregating LLM likelihoods over generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Think stage: parallel LM calls to generate example lists, facts, translations, or to score pairwise likelihoods (e.g., List-of-words prompt). Sum stage: external probabilistic aggregation using mixture averaging (mean of likelihoods across substitutions), product aggregation (multiply likelihoods when multiple constraints must hold), ratio-of-likelihoods normalization (conditional/unconditional), majority/minority voting, and latent-variable inference via EM to cluster items/facts and compute posterior scores.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated across multiple BIG-bench tasks (e.g., INVENTED WORDS, ODD ONE OUT, LOGICAL DEDUCTION, PHRASE RELATEDNESS, KNOWN UNKNOWNS, EMOJI MOVIE, NOVEL CONCEPTS, CODE LINE DESCRIPTION, LANGUAGE IDENTIFICATION).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Task-dependent: accuracy for classification tasks, BLEU for CODENAMES; reported ThinkSum results often substantially exceed direct prompting/GPT-3 few-shot baselines — examples: ODD ONE OUT ThinkSum (text-davinci-002) ≈ 0.84 vs davinci direct ≈ 0.27; LOGICAL DEDUCTION ThinkSum (davinci-002) ≈ 0.77 vs direct davinci ≈ 0.32; INVENTED WORDS ThinkSum (davinci) ≈ 0.64 vs direct davinci ≈ 0.29 (Table 1 & 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>ThinkSum outperforms direct prompting, chain-of-thought (CoT) prompting, and auxiliary-knowledge appending on many evaluated tasks; in some tasks performance depends on model variant and instruction tuning (InstructGPT often improves results). The authors report ThinkSum is less sensitive to prompt format than CoT and auxiliary knowledge approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Computational cost increases with number of parallel LM calls and combinatorial number of substitutions; requires human-guided prompt engineering to select Think and Sum operations per task; some Think outputs can be inconsistent (e.g., inequality direction reversals) and Sum needs to model/correct such failures (e.g., by summing out latent reversal variable); primarily evaluated on BIG-bench tasks, not on forecasting real-world future scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Explicit probabilistic aggregation external to LLM calls enables robust, interpretable likelihood estimates over hypotheses, improving performance on multi-object/multi-fact reasoning tasks; ThinkSum permits combination with latent-variable inference (EM) to produce structured posterior interpretations; approach generalizes across tasks and often enables smaller models to match or exceed larger-model direct-prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ThinkSum: Probabilistic reasoning over sets using large language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An explanation of in-context learning as implicit bayesian inference <em>(Rating: 2)</em></li>
                <li>Language model cascades <em>(Rating: 2)</em></li>
                <li>Generated knowledge prompting for commonsense reasoning <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3736",
    "paper_id": "paper-370cea8b4220917f45a69358c0303df71f5063c7",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "Sports understanding (plausibility)",
            "name_full": "Sports Understanding — plausibility/posterior estimation of player-action compatibility",
            "brief_description": "The paper uses LLM-generated example sets and normalized LLM likelihoods to compute posterior probabilities that a given professional athlete plausibly performed a sport-specific action; low posterior mass on the queried player indicates the sentence is implausible.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci) and InstructGPT (text-davinci-002); also GPT-2 variants for some experiments",
            "model_description": "GPT-3 (davinci) — autoregressive transformer (largest 175B variant) used via OpenAI API; InstructGPT (text-davinci-002) — instruction-tuned GPT-3 variant; GPT-2 family (Small/Medium/Large/XL) used locally for ablations.",
            "prediction_task": "Estimate posterior probability that a given sentence 'Player X performed action A' is plausible (i.e., whether player X's sport matches action A) by comparing LLM likelihoods across a generated set of players who perform A plus the queried player.",
            "method_of_probability_estimation": "Think stage: Example generation prompt to produce a set S of players known to perform action A. Sum stage: compute posterior p(y|a) = p_LLM(\"y a\") / sum_{y' in S ∪ {x}} p_LLM(\"y' a\") and threshold the posterior on x to decide plausibility; generation temperature and max_tokens set as described (temperature=0.5 for this task).",
            "dataset_or_benchmark": "BIG-bench — SPORTS UNDERSTANDING task (questions that pair professional sports players with sport-specific actions and ask 'plausible/implausible').",
            "performance_metrics": "Classification accuracy (plausible vs implausible). Reported numbers (Table 1 / Appendix A.1): ThinkSum using GPT-3 davinci: ~0.71; GPT-3 zero-shot baseline: ~0.50; InstructGPT (text-davinci-002) reported ~0.74; GPT-2 XL ~0.54. Threshold used: 0.01 in experiments.",
            "comparison_to_baselines": "ThinkSum posterior computation substantially outperforms zero-shot direct prompting (davinci) and improves plausibility detection versus naive scoring; InstructGPT sometimes slightly outperforms or matches ThinkSum depending on model variant (text-davinci-002 ≈ 0.74).",
            "limitations_or_challenges": "Relies on quality/diversity of generated example set S; requires prompt engineering to get generations in a predictable format; computational cost grows with number of generated examples; threshold choice affects decisions.",
            "notable_findings": "Normalizing LLM likelihoods over a generated set (posterior computation) yields calibrated-looking posteriors useful for implausibility detection (hallucination/semantic mismatch), and ThinkSum's approach is effective even with smaller model variants when structured as Think+Sum.",
            "uuid": "e3736.0",
            "source_info": {
                "paper_title": "ThinkSum: Probabilistic reasoning over sets using large language models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Known Unknowns (answer-known detection)",
            "name_full": "Known Unknowns — posterior-based detection of whether a question's precise answer is knowable",
            "brief_description": "The paper uses LLM list-extension to propose alternative candidate answers and then forms a posterior over the original candidate and generated alternatives to decide whether the given answer is plausibly correct (known) or should be labeled 'Unknown'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci) and InstructGPT (text-davinci-002)",
            "model_description": "GPT-3 (davinci) large autoregressive model (175B) and its instruction-tuned variant text-davinci-002; generation settings: max_tokens=100, temperature varied (0.5 for list extension).",
            "prediction_task": "Classify whether the precise answer to a factual question is known (select the precise answer) or 'Unknown' by comparing normalized LLM likelihoods over the provided candidate and generated alternative answers.",
            "method_of_probability_estimation": "Think: List-extension prompt generates set S of plausible answers. Sum: compute posterior p(y|q) = p_LLM(\"q ? y\") / sum_{y' in S ∪ {a}} p_LLM(\"q ? y'\") and threshold the posterior on the original answer a to decide known vs unknown. Thresholding rule in experiments: margin 1/N_e (N_e number of examples).",
            "dataset_or_benchmark": "BIG-bench — KNOWN UNKNOWNS task (questions where correct option may be a precise fact or 'Unknown').",
            "performance_metrics": "Accuracy (choosing correct precise answer vs 'Unknown'). Reported values: Table 1 and Table 4 show mixed results: with GPT-3 (davinci) direct/coT/ThinkSum variants — ThinkSum (davinci) ≈ 0.54 (lower than some CoT baselines), InstructGPT (davinci-002) ThinkSum ≈ 0.76, CoT ≈ 0.74; average-human baseline listed as ~0.80 in Table 1.",
            "comparison_to_baselines": "ThinkSum sometimes underperforms chain-of-thought or instruction-tuned models on this task for particular model variants (davinci), but with instruction-tuned davinci-002 ThinkSum matches or exceeds CoT; demonstrates sensitivity to model variant and tuning.",
            "limitations_or_challenges": "Performance depends strongly on generation quality of alternative answers and on model variant; ThinkSum with vanilla davinci underperformed CoT in some settings; requires careful choice of N_e and thresholding; computational cost of generating and scoring alternatives.",
            "notable_findings": "Posterior normalization over generated alternatives provides a principled decision rule for 'known vs unknown' and can be effective when combined with instruction-tuned models, showing ThinkSum can be used to quantify uncertainty about factual answers.",
            "uuid": "e3736.1",
            "source_info": {
                "paper_title": "ThinkSum: Probabilistic reasoning over sets using large language models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Uncertainty / hallucination detection (general)",
            "name_full": "Uncertainty and Hallucination Detection via LLM likelihoods and ThinkSum aggregation",
            "brief_description": "The authors apply ThinkSum-style posterior computations and likelihood-ratio/thresholding methods to detect implausible or hallucinated model outputs and to assess model uncertainty about facts by comparing conditional and unconditional likelihoods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci), InstructGPT (text-davinci-002), GPT-2 family",
            "model_description": "Large autoregressive LLMs (GPT-3 davinci 175B), instruction-tuned variants (text-davinci-002), and smaller GPT-2 models for ablations; generation parameters vary (max_tokens up to 1000 for fact generation; temperature 0 or 0.5 depending on task).",
            "prediction_task": "Detect hallucinations or high uncertainty in generated statements by computing normalized likelihoods/posteriors over sets of candidate statements or by using ratios of conditional and unconditional likelihoods to correct for surface-form biases.",
            "method_of_probability_estimation": "Think operations include Example/Fact generation and List-of-words likelihood queries; Sum operations include mixture averaging, product aggregation, and likelihood ratio normalization (e.g., divide conditional likelihood by unconditional likelihood) to form calibrated scores or posteriors; thresholding then used to flag hallucinations or unknowns.",
            "dataset_or_benchmark": "BIG-bench tasks related to hallucination/uncertainty: Known Unknowns, Sports Understanding, and related evaluation slices in Appendix A.1.",
            "performance_metrics": "Reported metrics are accuracy for detection tasks; paper reports substantial improvements in several hallucination/uncertainty tasks when using ThinkSum-style posterior normalization compared to direct prompting. Specific example numbers: EMOJI MOVIE ThinkSum ≈ 0.80 vs direct davinci ≈ 0.12 (Table 1); other tasks show similar large gains.",
            "comparison_to_baselines": "ThinkSum generally outperforms direct zero-shot/few-shot prompting and auxiliary knowledge appending in uncertainty/hallucination related tasks, and often matches or exceeds CoT when combined with instruction-tuned models; auxiliary knowledge approaches can degrade with long appended prompts while ThinkSum's external aggregation is more robust.",
            "limitations_or_challenges": "Requires design of Think prompts and post-processing to ensure generated facts are in analyzable format; post-processing heuristics (filtering short sentences, ensuring single-object mentions) are task-specific; scaling costs increase with number of generated facts and candidate substitutions.",
            "notable_findings": "Aggregating probabilities externally (Sum) over LLM-generated hypotheses reduces sensitivity to prompt order/format and can detect implausible or hallucinated statements more reliably than single-pass LLM scoring; smoothing/averaging over many hypotheses helps mitigate spurious high-probability surface patterns.",
            "uuid": "e3736.2",
            "source_info": {
                "paper_title": "ThinkSum: Probabilistic reasoning over sets using large language models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "ThinkSum probabilistic aggregation paradigm",
            "name_full": "ThinkSum — two-stage LLM-based probabilistic inference (Think + Sum)",
            "brief_description": "A general two-stage paradigm where LLMs are used in a 'Think' stage to produce sets of candidate strings, facts, translations or likelihood matrices, and a separate 'Sum' stage performs explicit probabilistic inference (mixture/product/ratio/EM) over those outputs to produce final probabilities or decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs (GPT-2 family, GPT-3 variants ada-&gt;davinci, InstructGPT text-davinci-002)",
            "model_description": "ThinkSum is model-agnostic; experiments used GPT-2 (Small/Medium/Large/XL), GPT-3 (ada,babbage,curie,davinci) and InstructGPT (text-davinci-002) with appropriate decoding settings; GPT-3 davinci corresponds to the 175B model used via OpenAI API.",
            "prediction_task": "General estimation of probabilities/likelihoods of candidate completions, statements, translations, or labels across multiple BIG-bench tasks (semantic relatedness, logical deduction, invented-words, hallucination/uncertainty detection), by explicitly aggregating LLM likelihoods over generated hypotheses.",
            "method_of_probability_estimation": "Think stage: parallel LM calls to generate example lists, facts, translations, or to score pairwise likelihoods (e.g., List-of-words prompt). Sum stage: external probabilistic aggregation using mixture averaging (mean of likelihoods across substitutions), product aggregation (multiply likelihoods when multiple constraints must hold), ratio-of-likelihoods normalization (conditional/unconditional), majority/minority voting, and latent-variable inference via EM to cluster items/facts and compute posterior scores.",
            "dataset_or_benchmark": "Evaluated across multiple BIG-bench tasks (e.g., INVENTED WORDS, ODD ONE OUT, LOGICAL DEDUCTION, PHRASE RELATEDNESS, KNOWN UNKNOWNS, EMOJI MOVIE, NOVEL CONCEPTS, CODE LINE DESCRIPTION, LANGUAGE IDENTIFICATION).",
            "performance_metrics": "Task-dependent: accuracy for classification tasks, BLEU for CODENAMES; reported ThinkSum results often substantially exceed direct prompting/GPT-3 few-shot baselines — examples: ODD ONE OUT ThinkSum (text-davinci-002) ≈ 0.84 vs davinci direct ≈ 0.27; LOGICAL DEDUCTION ThinkSum (davinci-002) ≈ 0.77 vs direct davinci ≈ 0.32; INVENTED WORDS ThinkSum (davinci) ≈ 0.64 vs direct davinci ≈ 0.29 (Table 1 & 4).",
            "comparison_to_baselines": "ThinkSum outperforms direct prompting, chain-of-thought (CoT) prompting, and auxiliary-knowledge appending on many evaluated tasks; in some tasks performance depends on model variant and instruction tuning (InstructGPT often improves results). The authors report ThinkSum is less sensitive to prompt format than CoT and auxiliary knowledge approaches.",
            "limitations_or_challenges": "Computational cost increases with number of parallel LM calls and combinatorial number of substitutions; requires human-guided prompt engineering to select Think and Sum operations per task; some Think outputs can be inconsistent (e.g., inequality direction reversals) and Sum needs to model/correct such failures (e.g., by summing out latent reversal variable); primarily evaluated on BIG-bench tasks, not on forecasting real-world future scientific discoveries.",
            "notable_findings": "Explicit probabilistic aggregation external to LLM calls enables robust, interpretable likelihood estimates over hypotheses, improving performance on multi-object/multi-fact reasoning tasks; ThinkSum permits combination with latent-variable inference (EM) to produce structured posterior interpretations; approach generalizes across tasks and often enables smaller models to match or exceed larger-model direct-prompting baselines.",
            "uuid": "e3736.3",
            "source_info": {
                "paper_title": "ThinkSum: Probabilistic reasoning over sets using large language models",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An explanation of in-context learning as implicit bayesian inference",
            "rating": 2
        },
        {
            "paper_title": "Language model cascades",
            "rating": 2
        },
        {
            "paper_title": "Generated knowledge prompting for commonsense reasoning",
            "rating": 1
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        }
    ],
    "cost": 0.015967,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ThinkSum: Probabilistic reasoning over sets using large language models</h1>
<p>Batu Ozturkler<br>Stanford University<br>Stanford, California, USA<br>ozt@stanford.edu<br>Zhen Wang<br>Ohio State University<br>Columbus, Ohio, USA<br>wang.9215@osu.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think - retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum - probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs; Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022) can recall a broad range of basic facts, recognize and mimic</p>
<p>Nikolay Malkin<br>Mila, Université de Montréal<br>Montréal, Québec, Canada<br>nikolay.malkin@mila.quebec<br>Nebojsa Jojic<br>Microsoft Research<br>Redmond, Washington, USA<br>jojic@microsoft.com</p>
<p>various forms in language, and efficiently extrapolate analogies in structure and meaning. These abilities allow LLMs to excel in zero-shot and few-shot tasks formulated as the generation or selection of a likely completion to a prompt. This formulation requires LLMs to perform fast associative thinking, in which each token of text in the sequence making up the answer is generated or scored in one pass through the model and, other than that, no intermediate information is created or retained. This fast thinking is made possible by the compression of information that is repeated in a variety of ways in large training datasets, within the LLM's weights.</p>
<p>However, it is increasingly evident that when reasoning, or slow thinking, is required, failure modes of LLMs are revealed. In our usage, reasoning refers to the sequential manipulation of concepts that can be expressed in language. Tasks that require iterative retrieval of rarely stated knowledge, uncertainties over multiple objects or facts, or multiple steps of deduction are difficult even for the most advanced LLMs (Suzgun et al., 2022). In a recently designed suite of evaluations, BIG-bench (Srivastava et al., 2022), some of the tasks where the gap between machine and human performance is large involve inference sequences with nested counterfactuals (LOGICAL DEDUCTION), concepts introduced through definitions (CONCEPTUAL COMBINATIONS), etc. (see Fig. B.1). These are tasks where a human solver's intuitive feeling of '(in)coherence' is insufficient to produce the right answer, and a sequence of thoughts, along with the use of intermediate results, may be necessary to arrive at the solution, particularly when working memory is insufficient.</p>
<p>We show several tasks in BIG-bench that can be addressed by a two-component mechanism, which we name ThinkSum ${ }^{1}$ :</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Direct Prompting</h1>
<p>A binne is any furry four-legged creature, and a bam is a simple dwelling.
A binne bam is a place for people (55\%) animals (44\%) birds ( $0.87 \%$ ) researchers ( $0.022 \%$ )</p>
<h2>Chain of Thought / Auxiliary Knowledge</h2>
<p>A binne is any furry four-legged creature, and a bam is a simple dwelling.
Examples of binnes: cat, mink, ferret, guinea pig, rabbit.
Examples of bams: hut, cabin, cottage, shelter, shack.
A binne bam is a place for people (51\%) animals (48\%) birds ( $0.76 \%$ ) researchers ( $0.011 \%$ )</p>
<h2>THINKSUM</h2>
<p>A binne is any furry four-legged creature, and a bam is a simple dwelling. $\left.\begin{array}{l}\text { binne }={ \text { cat, mink, ferret, guinea pig, rabbit }} \ \text { bam }={ \text { hut, cabin, cottage, shelter, shack }}\end{array} \right\rvert\, \begin{aligned} &amp; \text { THINK (auxiliary LM calls to define sets) } \ &amp; \text { Sum (aggregate LM likelihoods) }\end{aligned}$</p>
<p>A binne bam is a place for animals (65\%) people (34\%) birds ( $1.5 \%$ ) researchers ( $0.056 \%$ )</p>
<p>Figure 1: An example adapted from the CONCEPTUAL COMBINATIONS (INVENTED WORDS) task, in which models must select the most likely completion of a phrase that includes nonce words whose definitions are given. Top: Direct prompting evaluates completion likelihoods normalized over the four answer choices ('people', 'animals', 'birds', 'researchers'). Middle: Chain-of-thought-like or auxiliary knowledge approaches would query a LLM or knowledge base for additional context. This example shows the brittleness entrusting all 'reasoning' to self-attention in linear text, especially in smaller models, which have stronger recency bias (Malkin et al., 2022): if we simply list generated examples as the additional context in the prompt, the recency bias causes the LLM to still give a higher probability to 'people' than to 'animals', simply because 'bam' (simple dwelling) examples are given after the 'binne' examples. Bottom: Our ThinkSum approach to this task queries a LLM (GPT-2 XL) to produce sets of examples defining the nonce words, then marginalizes over substitutions of these examples into the target phrase.</p>
<ul>
<li>Think (fast thinking / association / knowledge retrieval step): creating an association of text spans with sets of strings. This process may involve generation from a language model, as is the case in Fig. 1, where the novel word 'binne' is associated with the set of strings {'cat', 'mink', . . . } by prompting GPT-3 with the definition and asking for examples. Alternatively, it may consist solely of a scoring mechanism, resulting in the formation of a matrix of probabilities on which probabilistic inference is performed.</li>
<li>Sum (slow thinking / Summarization / reasoning step): probabilistic inference that aggregates generated strings or probabilities to produce the final answer. Summarization typically involves, and often entirely consists of, summing of probabilities of strings (computed in the Think step), as in Fig. 1, where the final word is assumed to be sampled from a mixture of possible substitutions of 'binne' and 'bam' words into the input.
We discuss different ways to Think and to Sum in section §2, but we start with one example, illus-</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>trated in Fig. 1 (bottom), motivated by the CONCEPTUAL COMBINATIONS (INVENTED WORDS) task in BIG-bench. In this task, the LLM is provided with the definitions of two invented words and asked to infer the most plausible sentence that uses a combination of the invented words. As the words are not common or consistently used in the training set, the LLM needs to understand and combine the definitions of the invented words to reason about the meaning of the combination. The LLM is queried to produce example instances of the invented words with the help of the definitions. These example instances can be substituted into the query in place of the invented words. By mapping individual spans of the text of interest to sets, we arrive at a mixture model (in this example, a mixture with 25 components for 5 possible replacements of each word), which can be used in the same manner the original LLM is used, either to score text or to generate it token by token. When we score all candidate completions using this mixture model and normalize over the four choices, the correct answer - that 'binne bams' are for animals and not people becomes the most likely.</p>
<p>An important difference between our ThinkSum and existing chain-of-thought-like prompt engineering methods (Wei et al., 2022; Kojima et al., 2022), is that our reasoning step is not reduced to a generation problem for the LLM, but is performed as a probabilistic inference external to the LLM. This reduces vulnerability to features of the prompt, such as accidental distraction of the LLM by spurious patterns (see Fig. 1, middle). Instead, we engineer the slow thinking process to make parallel calls to the LLM to query for intermediate information, then possibly perform programmatic recombination of strings (Think). The final reasoning step - in which likelihoods obtained from the LLM for the recombinations derived from earlier steps of the reasoning process are combined to make the final prediction - is left to classical probabilistic reasoning (Sum). In a sense, Sum replaces the self-attention mechanism over linear text, which is used as the sole 'reasoning' mechanism in chain-of-thought-like approaches that expect the intermediate 'thoughts' to take the form of generated tokens intervening between the input and output.</p>
<p>Imposing an alternative reasoning system over an associative "knee-jerk reaction" system has an analogy with models of human cognitive processes (Tversky and Kahneman, 1974; Kahneman, 2011) that separate System 1 (fast thinking) and System 2 (slow thinking). System 2 acts as a 'controller' that can prime System 1 to appropriately bias its fast thinking. In the context of reasoning with deep learning models, System 2 has been interpreted as operating with sparse concepts that can be described in language (Bengio, 2017; Goyal and Bengio, 2020). Through repeated usage, the functions of System 2 become compressed into System 1 intuitions, in the same manner that iterative 'reasoning' functions of which smaller LLMs are not capable become zero-shot generation capacities for large LLMs. As is the case with humans, there is always the next frontier of problems where a trained model with remarkable 'intuition' needs to be slowed down. The main claim of this paper is that more is possible with LLMs of existing scale when they are used in concert with a wise controller that allows for probabilistic inference.</p>
<h2>2 ThinkSum</h2>
<h3>2.1 How to Think</h3>
<p>Here we list examples of the "fast thinking" that precedes the summarization stage.</p>
<p>Elementary string manipulations. Standard ways to turn a question into a prompt that can be given to a LLM for generation or scoring involve choices (e.g., of the prompt format) that can be seen as being made by a controlling agent. The default approach to multiple-choice questions is to write them as Cloze tasks. However, there are nontrivial operations used in inference procedures that sometimes work better, such as:</p>
<ul>
<li>Order inversion: Exchanging the order of the question and answers, as in Min et al. (2022).</li>
<li>Premise erasure: Deleting a part of the question. Removing a premise with which the answer is expected to have high mutual information is a step in inference procedures that aim to correct for bias towards answers with high unconditional likelihood (Zhao et al., 2021; Holtzman et al., 2021; Malkin et al., 2022).</li>
</ul>
<p>Substitution and normalization. An example is shown in Fig. 1. Elements from a set may be substituted in place of 'slot' words in a prompt, such as 'cat' substituted for 'binne' in the prompt "A binne bam is a place for". This operation can be combined with syntax-normalization steps that are reliably achieved by standard NLP tools, such as ensuring subject-verb agreement.</p>
<p>Example and list generation. A LLM can be prompted to generate or score lists of words or phrases. We suggest and experiment with three instances of this:</p>
<ul>
<li>Example generation: In Fig. 1, the LLM is prompted to turn a definition or characterizing property, such as 'simple dwelling', into a list of examples. This can be achieved with a prompt such as "A bam is a simple dwelling. Examples: 1.". The generated completion can be parsed into a set to be used later in the inference procedure.</li>
<li>List extension: A similar approach can also be used to hallucinate additional possible answers to questions, as we will show in some of the experiments.</li>
<li>List of words: Similar prompts provide an even simpler Think method that we use for scoring but not generation - in several tasks. Just prompting a LLM with "List of words: $A, B$ ", where $A$ and $B$ are words or phrases, and computing the likelihood of $B$ conditioned on "List of words: $A$," is a good measure of semantic relatedness of $A$ and $B$.</li>
</ul>
<p>Fact generation. This way of Thinking associates an input word with a set of phrases in a similar manner to generating examples from a definition. It can be achieved with prompts such as "List facts about cats. 1." The generated facts are good targets for substitutions of other concepts ('dogs', 'galaxies') in place of the concept ('cats') about which facts are generated. A variation on this asks the LLM to generate differences between two concepts, as shown in Fig. 2 (right).
Translation. The LLM can be prompted to convert between different forms of representing the same concept as a sequence of tokens. We use two basic examples of this in experiments:</p>
<ul>
<li>Translation between languages by prompting the LLM in formats such as "French: J'adore les chats noirs. English:". A very similar approach can be used to convert non-alphabetic symbols, such as emoji, into words with similar meanings.</li>
<li>Converting text to formal (symbolic) structures, like turning a word problem into a collection of mathematical equations.</li>
</ul>
<h3>2.2 How to Sum</h3>
<p>Elementary inference. As above, we begin by listing existing standard ways of turning LLM outputs into answers, which we see as trivial cases of aggregation (Sum).</p>
<ul>
<li>Majority/minority vote (argmin/argmax): a component of most answer selection procedures.</li>
<li>Ratio of likelihoods: Likelihoods from different variants of the same prompt can be combined by considering their ratio or more general loglinear or other mixture. For example, this can be done to correct the likelihood of an answer conditioned on a question by its unconditional likelihood, in combination with the Premise erasure operation described above.
Mixture (average) aggregation. A collection of prompts can be treated as the components of a mixture model over completions. An example is shown in Fig. 1, where substitutions of a set of words yield 25 different prompts. Likelihoods of the completion over these 25 prompts are averaged.
Product aggregation. We use products of likelihoods in two different ways:</li>
<li>In a similar way as mixtures, but when the more natural probabilistic model has all elements of a set (of prompts) generating the answer, such as when a description or definition must be satisfied
by all concepts in a set.</li>
<li>In a task where we are to determine whether a statement $S$ or its negation $S^{\prime}$ is true, we can compute the likelihood of both $S$ and $S^{\prime}$ being true (as posterior over the tokens 'True' and 'False' in an appropriate prompt), then compare $p($ True $\mid S) p($ False $\mid S^{\prime}$ ) ( $S$ is true and $S^{\prime}$ is false) with $p($ False $|S\rangle p($ True $\mid S^{\prime}$ ) ( $S$ is false and $S^{\prime}$ is true).</li>
</ul>
<h2>3 Experiments</h2>
<p>In this section, we perform case studies on three tasks from the BIG-bench suite to demonstrate the possibilities of the inference approaches discussed in $\S 2$. We also experiment with ten other tasks from BIG-bench; the best results are summarized in Table 1 and the methods, grouped by the style of Thinking and Summing, are described in Appendix (§A).</p>
<p>All details of the tasks can be found in the Appendix (§C). Comparisons to direct prompting and algorithms that append retrieved or generated tokens to the prompt are given in §3.4.</p>
<h3>3.1 Conceptual combinations: Invented words</h3>
<p>In InVEnted WORDS, two nonce words $x_{1}, x_{2}$ are defined and the correct statement must be chosen out of a set of statements $S=\left{s_{j}\right}$ that begin with (possibly inflected forms of) " $x_{1} x_{2}$ " (Fig. 1).</p>
<p>We use an Example generation prompt to obtain a set of example words fitting the definitions of $x_{1}$ and $x_{2}$. We thus obtain sets $S_{1}$ and $S_{2}$ of words that can be substituted for $x_{1}$ and $x_{2}$, respectively.</p>
<p>We treat each statement $s_{j}$ as a template into which words $w_{1} \in S_{1}$ and $w_{2} \in S_{2}$ can be substituted by replacing $x_{i}$ with $w_{i}$ and normalizing the syntax to ensure subject-verb agreement. Denoting by $s_{j}\left\langle w_{1}, w_{2}\right\rangle$ such a substitution, we form a vector of probabilities $p_{j}$ by scoring the Substitution of each possible pair of words into each statement and performing Mixture aggregation and considering the Ratio of likelihoods with the template without substitution:</p>
<p>$$
p_{j}=\frac{\frac{1}{\left|S_{1}\right|\left|S_{2}\right|} \sum_{w_{1} \in S_{1}, w_{2} \in S_{2}} p_{\mathrm{LLM}}\left(s_{j}\left\langle w_{1}, w_{2}\right\rangle\right)}{p_{\mathrm{LLM}}\left(s_{j}\right)}
$$</p>
<p>The statement $s_{j}$ with highest likelihood under this normalized mixture, $\arg \max <em j="j">{j} p</em>$, is selected.</p>
<h3>3.2 Odd one out</h3>
<p>We examine possible Think and Sum approaches in depth on the ODD ONE OUT task, in which the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">GPT-3 (davinci) $n$-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ThinkSum</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg. H</td>
<td style="text-align: center;">$n=0$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">InstructGPT</td>
<td style="text-align: center;">GPT-2 XL</td>
</tr>
<tr>
<td style="text-align: center;">InVENTED WORDS (\$3.1)</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.29</td>
</tr>
<tr>
<td style="text-align: center;">ODD ONE OUT (\$3.2)</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: center;">FIVE OBJECTS (\$3.3)</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">SPORTS UNDERSTANDING (\$A.1)</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;">KNOWN UNKNOWNS (\$A.1)</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">MISCONCEPTIONS RUSSIAN (\$A.2)</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">EMOJI MOVIE (\$A.2)</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">PARSINLU READING COMPREHENSION (\$A.2)</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">PHRASE RELATEDNESS (\$A.3)</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: center;">CODENAMES (\$A.3)</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;">NOVEL CONCEPTS (\$A.4)</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;">CODE LINE DESCRIPTION (\$A.4)</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: center;">LANGUAGE IDENTIFICATION (\$A.5)</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0.30</td>
</tr>
</tbody>
</table>
<p>Table 1: Standard metric (BLEU for CODENAMES, accuracy for other tasks) for GPT-3 175B (davinci) and ThinkSum with 175B (davinci), InstructGPT and GPT-2 XL on BIG-bench tasks. A ' - ' indicates that the model and task combination was not evaluated because the model does not reliably execute the appropriate Think prompt. We did not evaluate InstructGPT on LANGUAGE IDENTIFICATION due to the large dataset size and API quota.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: ODD ONE OUT. Left: Performance of GPT-3 ( $n$-shot, $n=0,1,2,3$ ), auxiliary knowledge, and ThinkSum with various model sizes. Middle: Auxiliary knowledge vs. ThinkSum with varying number of differences. Right: Prompt used to generate knowledge statements.
word in a set $W=\left{w_{i}\right}$ that is least semantically related to the others must be chosen (e.g., Pick the odd word out: glass, head, arm, leg, hand, foot).
List of words. We form a semantic relatedness matrix $P_{i j}$ by querying the LLM with a List of words Think prompt for each pair of indices $i, j$ :</p>
<p>$$
P_{i j}=p_{\mathrm{LLM}}\left(w_{j} \mid \text { "List of words: } w_{i}, \text { ") }\right.
$$</p>
<p>This matrix is aggregated by averaging over $j$ (in log domain) and selecting the $i$ with lowest average, i.e., least likelihood of being generated by a product mixture of all words in the set: $i=\arg \min <em j="j">{i} \prod</em>$. This is a case of Product aggregation.} P_{i j</p>
<p>Because this approach is the most successful with all model sizes we experimented with, its performance is reported in Table 1. Remarkably, near-average-human accuracy is maintained for all
model sizes from GPT-2 Small to the largest GPT-3 model (Fig. 2 (left)).</p>
<p>Fact generation. As an alternative approach, we use a Fact generation prompt. An effective way to mine facts for semantic relatedness tasks is to consider two items in the same context in order to get relevant facts regarding how items are related to each other (prompt in Fig. 2 (right)). The demonstration used in the prompt ensures that the LLM generates statements in an expected format, which can be parsed and used for probability computation later. Using this prompt, we obtain a collection of statements $S=\left{s_{i}\right}$ about items $w_{j}$. We treat each generated $s_{i}$ as a template into which different words $w$ can be substituted and denote by $s_{i}\langle w\rangle$ the Substitution of word $w$ into template $s_{i}$. We then form a $|S| \times|W|$ matrix $P_{i j}$, defined</p>
<p>by $P_{i j}=p_{\mathrm{LLM}}\left(s_{i}\left\langle w_{j}\right\rangle\right)$. Then, we can perform Minority voting: we take argmin over $j$ and pick as the answer the most frequently occurring value, i.e., the item that is most often the least likely to fit a generated statement.
Comparison with auxiliary knowledge approaches. We compare our method with a knowledge-based prompting method, herein referred to as auxiliary knowledge. In auxiliary knowledge, we prepend generated facts in the prompt before the question. Details of the prompt for auxiliary knowledge are provided in §D.3. In Figure 2 (middle), we show that the accuracy of Fact generation-based ThinkSum rises as the number of generated facts is increased, while the auxiliary knowledge technique peaks and then degrades as the prompt lengthens.</p>
<p>Fig. 2 (left) shows how performance varies with the size of the LLM used for GPT-3, auxiliary knowledge and ThinkSum on ODD ONE OUT. Even with GPT-2 Small, ThinkSum dramatically improves over much larger largest zero- or few-shot models with or without auxiliary knowledge. A finetuned iteration of the largest GPT-3 model, text-davinci-002, is the only model variant that, with the help of auxiliary knowledge, achieves competitive performance with ThinkSum. This result provides experimental evidence for our claim that while new models may create qualitative jumps, ThinkSum can push the performance limits of smaller models.
Latent variable models. As we have shown, the detection of the odd item can be performed with simple inference operations on items, facts, and their joint likelihoods. However, it is also possible to assume a latent structure in the items and facts, consisting of two or more clusters such that the facts and items belonging to a cluster can be freely interchanged. We describe a problem-specific latent variable model that enables selecting the facts that characterize the majority class, thus explaining why the minority item is ruled as the odd one out and helping interpret the decisions of the system.</p>
<p>We model items $i \in I$ and facts $f \in F$ as being generated from a latent class $c \in{0,1}$. The distribution is modeled as:</p>
<p>$$
P(i, f)=\sum_{c} P(c) P(i \mid c) P(f \mid c)
$$</p>
<p>where $P(i, f)$ is a matrix of likelihoods from the LLM and the semantic components, groupings $P(i \mid c)$ and $P(f \mid c)$, are derived from the matrix using a standard iterative expectation-maximization</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">LoW</th>
<th style="text-align: left;">LVM</th>
<th style="text-align: left;">MV</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">text-davinci-002</td>
<td style="text-align: left;">0.84</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: left;">0.74</td>
<td style="text-align: left;">0.77</td>
<td style="text-align: left;">0.70</td>
</tr>
</tbody>
</table>
<p>Table 2: Different alternatives of probabilistic reasoning with ThinkSum for solving ODD ONE OUT: list of words, latent variable model, minority voting.
(EM; Dempster et al., 1977) inference procedure (see §E). Then, the score for an item $i$ belonging to a cluster and all other items $m \in S,{m \neq i}$ belonging to another cluster can be found as $S_{i}=$ $\sum_{c, c^{\prime} \neq c} P(i \mid c) P(c) \prod_{m \neq i} P\left(m \mid c^{\prime}\right) P\left(c^{\prime}\right)$.</p>
<p>We show the effectiveness of the latent variable models in Table 2, where we analyze different methods for solving ODD ONE OUT using the InstructGPT variants text-davinci-001 and text-davinci-002. For the 'latent variable model' and 'minority voting' methods, we use number of differences $N_{d}=5$. The latent variable model is trained for 200 EM iterations. All probabilistic reasoning methods perform well, outperforming previous baselines reported in Table 1. Inference using EM, as well as the other approaches, can be seen as a Sum (inference) operation and can be applicable in other tasks of similar structure.</p>
<h3>3.3 Logical deduction</h3>
<p>In the LOGICAL DEDUCTION task, different types of items and clues regarding their order are provided (Fig. 3(a)). The goal is to select the correct statement from a set of statements about their placements. The ordering problems involve different types of objects (cars, birds, etc.) and orderings (by size, price, contest ranking, etc.). The task creators emphasize that this task requires parsing information about multiple objects and their relationships, understanding rules regarding ordered objects in various scenarios, and iteratively applying these rules. The LLM calls in the Think stage of ThinkSum can perform mappings required to parse information and understand rules, and the Sum stage can integrate mappings of objects to the placements under these rules. Here, we use a Translation prompt to map the given problem into a set of mathematical (in)equalities (Fig. 3(c)).</p>
<p>The Translation prompt in Fig. 3(b), containing generic ordering statements and object names that are not used in the task as an in-context demonstration, is sufficient to perform the translation from natural language to equations. By prepending this</p>
<p>(a) Question:
"On a shelf, there are five books: a yellow book, a brown book, a gray book, a black book, and a purple book. The yellow book is to the right of the purple book. The black book is to the left of the purple book. The black book is the third from the left. The brown book is the leftmost."
PROBLEM:
There are some number of objects, object1, object2, object3... Some statements about their ordering are given.
We think that object1 is smaller than object2.
We have reason to believe that object3 is the smallest.
First list all objects in the OBJECTS list.
OBJECTS: $\backslash$ n object1 $\backslash$ n object2 $\backslash$ n object3
Next say how many there are.
NUMBER:
3
Then turn each statement into an equality or inequality, e.g.
STATEMENT:
We think that object1 is smaller than object2.
TRANSLATION:
object1&lt;object2
STATEMENT:
We have reason to believe that object3 is the smallest.
TRANSLATION:
object3=1
PROBLEM:
(b) Think Prompt</p>
<p>OBJECTS:
yellow book $\backslash \mathrm{n}$ brown book $\backslash \mathrm{n}$ gray book $\backslash \mathrm{n}$ black book $\backslash \mathrm{n}$ purple book NUMBER: 5
STATEMENT:
The yellow book is to the right of the purple book.
TRANSLATION:
yellow book&gt;purple book
STATEMENT:
The black book is to the left of the purple book.
TRANSLATION:
black book&lt;purple book
STATEMENT:
The black book is the third from the left.
TRANSLATION:
black book $=3$
STATEMENT:
The brown book is the leftmost.
TRANSLATION:
brown book $=1$
(c) LLM Output</p>
<p>Figure 3: Details for LOGICAL DEDUCTION. (a) Example question from the task, (b) demonstration for the Think prompt, (c) example LLM output. The demonstration induces the LLM to generalize from generic objects ordered by size to books ordered by position.
demonstration prompt to a problem statement, we induce the LLM to map the objects in the problem to the set of strings corresponding to numbers from 1 to $N$, where $N$ is the number of objects, and to produce a set of inequalities (Fig. 3(c)).</p>
<p>Once a translation of the problem into a set of inequalities is obtained, the Sum stage considers all possible mappings of items to indices to determine the mapping compatible with the discovered set of (in)equalities. This can be done by an external algorithm or by the LLM itself, as an LLM may be capable of understanding that, for example, " $2&gt;3$ " is a less likely string than " $2&gt;1$ " (see §D.2).</p>
<p>Finally, the probability of each of the candidate statements, like "yellow book=2", can thus be obtained by:</p>
<p>$$
\begin{aligned}
&amp; p\left(\left(\text { yellow book }=2^{\prime \prime} \mid T\right)\right. \
&amp; \propto \sum_{\mathbf{b} \in{1, \ldots, N}^{N}} p_{\mathrm{LLM}}\left(\left{T_{t}\langle\mathbf{b}\rangle: T_{t} \in T\right}\right. \
&amp; \left.\cup\left{\text { "yellow book }=2^{\prime \prime}\langle\mathbf{b}\rangle\right}\right)
\end{aligned}
$$</p>
<p>where $\mathbf{b}$ denotes the vector of positions for the $N$ items (e.g., $(5,2,3,4,1)), T=\left{T_{t}\right}_{t=1}^{N}$ is the set of inequalities obtained from the Translation prompt as a set of strings (e.g., "black book&lt;purple book"), and $s\langle\mathbf{b}\rangle$ denotes the substitution of the corresponding entry in $\mathbf{b}$ in place of the object name in the string $s$ (e.g., " $4&lt;5$ "). The term inside the sum is a case of Product aggregation: the LLM likelihoods of all strings in the set are multiplied.</p>
<p>In summary, our solution to this task involves
composition of two Think operations - a Translation into a set of equations and then Substitution of numbers in place of item names - and two Sum operations - a Product aggregation followed by a Mixture aggregation. (Other options are discussed below.)
Results and discussion. For the 500 LOGICAL DEDUCTION problems with $N=5$ objects, ThinkSum yields an accuracy of $77 \%$ (see Table 1), besting the average human performance. When the necessary summations become large, it becomes very unlikely that pure prompt engineering can be competitive, as even humans need paper and pencil to create and attend to many alternative solutions, and would likely translate the premises into a simpler notation using a single letter (representing a variable to which a numeric value can be assigned) to represent each object, rather than directly attending to the words in the problem statement.</p>
<p>We also test an auxiliary knowledge method akin to chain-of-thought reasoning, where the information obtained with the prompt in Fig. 3 is appended to the LLM input. In particular, the problem, together with its translation into inequalities, is used as a prompt to each of the answer options, and then the option with the highest likelihood is chosen for the answer. This approach does improve over straightforward zero-shot GPT-3 scoring, but only raises the accuracy to $50 \%$ (see $\S 3.4$ and Table 3).
Optimizations, failure modes, and extensions. We have seen that InstructGPT is able both to translate logical deduction problems into (in)equalities</p>
<p>(Fig. 3) and to evaluate each of them after replacement of items with position numbers (§D.2). We conclude that the Sum stage is there simply to search over all possible mappings, the way a human might. But, just as a human might use shortcuts in the search, the Sum stage of ThinkSum could be implemented in more or less efficient ways. For example, instead of summing over all possible assignments of the five items, we can avoid the ones that are not permutations of ${1,2,3,4,5}$. Furthermore, instead of using $p_{\mathrm{LLM}}$ from Fig. D. 1 in (1), we can simply evaluate each inequality externally, giving a high constant probability for each inequality $T_{t}\langle\mathbf{b}\rangle$ that is true and a low probability when it is false, or the summing can be aborted whenever an incorrect statement is detected in a particular assignment $\mathbf{b}$ of positions to items.</p>
<p>The prompt in Fig. 3(b) instructs the LLM to assign positive integers depending on the language used (e.g., the smallest object gets 1), but a common behaviour of the LLM is to generalize to assigning negative numbers, such as using -2 to represent 'second from the end' (or second-largest, etc.). To remain robust to such a behavior of the Think stage, we can convert negative position numbers $r$ into $N+r+1$ before evaluating statements. However, a persistent failure mode of this kind of ThinkSum is that the LLM may translate inequality statements inconsistently with equality statements (e.g., by coding the leftmost item as 1 and being consistent with this choice for other equality constraints, but translating inequality constraints consistently with the reverse order, with 'left of' meaning $&gt;$ ). Such failures can be addressed by careful engineering in the Sum stage, such as by summing out a binary latent variable indicating whether inequalities should be reversed. This increases the number of model evaluations, but also allows for robust auto-correction by the Sum stage of inconsistencies in the Think stage.</p>
<h3>3.4 Comparisons with chain-of-thought and auxiliary knowledge approaches</h3>
<p>ThinkSum vs. auxiliary knowledge. Table 3 shows the comparison of ThinkSum with algorithms that append auxiliary knowledge as an oracle 'reasoning chain'. For PHRASE RELATEDNESS, auxiliary knowledge was generated using the "list differences" prompt shown in Fig. 2 (right). For both auxiliary knowledge and ThinkSum, 6 generated differences were used, as that was the</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Odd one out</th>
<th style="text-align: center;">Phrase relatedness</th>
<th style="text-align: center;">Logical deduction $(N=5)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ThinkSum</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: left;">Ant. knowledge</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.50</td>
</tr>
</tbody>
</table>
<p>Table 3: ThinkSum vs. auxiliary knowledge with text-davinci-002.
best for auxiliary knowledge (see Fig. 2 (middle)). ThinkSum Odd one out and Phrase RELAtEDNESS are solved with the "list of words" prompt. For Logical Deduction, the Think prompt shown in Fig. 3 was included before the question in the prompt. In all cases, ThinkSum outperforms auxiliary knowledge.</p>
<p>ThinkSum vs. chain of thought. Following Wei et al. (2022), we use "chain-of-thought (CoT) methods" to mean LLM scoring approaches that use insertion of generated tokens between the prompt and the target answer. The model is taught, using fewshot demonstrations, how to generate these intermediate tokens. Above we have compared ThinkSum with approaches that add extracted (from an auxiliary LM call), not generated (within the LM's linear workspace) token sequences after the prompt, for the Odd one out, Phrase relatedness, and Logical Deduction tasks (see Table 3).</p>
<p>With suitable examples, it may be possible for a CoT approach to replace the Think phase, by learning from demonstrations to generate the appropriate knowledge, and parts of the Sum phase, although inference over parallel evaluations of the LLM is no longer possible. Our auxiliary knowledge baselines make precisely that generous assumption and focus the comparisons on the need for parallel calls and reasoning over possibilities using probabilistic inference (instead of leaving it to the LLM to make the right conclusions from the list of extracted alternatives).</p>
<p>Although we expect that appending facts in a standard format to the prompt would help the model more than teaching the model to generate these facts, we experimented with CoT approaches on several tasks. Table A. 1 shows example demonstrations and prompt formats used for each task, and Table 4 shows the results using two variants of the largest GPT-3 model.</p>
<p>As expected, ThinkSum outperforms CoT prompting on all tasks with all variants except KNOWN UNKNOWNs with the davinci variant, where direct prompting already performs well. (We did not evaluate ThinkSum with davinci on LOGICAL DEDUCTION because prompts like the one</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">GPT-3 (davinci)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-3 (davinci-002)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">ThinkSum</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">ThinkSum</td>
</tr>
<tr>
<td style="text-align: center;">Odd one out</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;">PHRASE RELATEDNESS</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;">LOGICAL DEDUCTION</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: center;">KNOWN UNKNOWNS</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;">INVENTED WORDS</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.71</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of ThinkSum with chain-ofthought prompting approaches.
in Figure 3 did not reliably produce outputs in the correct format; notice that CoT is barely better than random guessing ( $20 \%$ ).)</p>
<p>When interpreting these results, it is important to note that only one prompt format was evaluated for both CoT and ThinkSum, and the format of prompts and demonstrations can have a strong and often unpredictable effect on the LLM. We observed that CoT approaches are highly sensitive to minor changes in the prompt format or the construction of in-context examples, consistent with the known biases of in-context learning (Lu et al., 2022; Zhao et al., 2021). On the other hand, using structured, shorter components is more reliable, as demonstrated by the efficacy of the Think prompts used in ThinkSum.</p>
<h2>4 Related work</h2>
<p>Improvements to LLM inference. After the discovery of the in-context learning abilities of LLMs, there has been an explosion of interest in improving inference with LLMs in the zero-shot and few-shot setting (Brown et al., 2020; Chowdhery et al., 2022; Rae et al., 2021). One approach to improving the reasoning abilities of LLMs involves appending, or learning to generate, auxiliary knowledge within the prompt (Shwartz et al., 2020; Zelikman et al., 2022; Nye et al., 2021a). Recently, more general auxiliary knowledge or chain-of-thought prompting methods have been proposed (Wei et al., 2022; Wang et al., 2022b; Zhou et al., 2022a; Creswell et al., 2022; Wang et al., 2022a; Liu et al., 2022b), including those that allow a control flow external to the main LLM (Khot et al., 2022). Later, Kojima et al. (2022) showed zero-shot chain-of-thought prompting can improve performance on a variety of reasoning tasks. This method does not require any hand-crafted few-shot examples, which is a shared property with ThinkSum. (Nye et al., 2021b) observed that a dual-system approach where an associative "System 1" and a logical "System 2" can increase coherence of LLMs in tasks such as robust
story generation and grounded instruction following. The two-step paradigm in ThinkSum is similar, where "System 1" is the (querying of the LLM for) fast thinking, and "System 2" is the probabilistic inference step.
Brittleness of chain-of-thought prompting. Despite the recent success of chain-of-thought approaches, recent studies have raised concerns regarding the limitations of chain-of-thought approaches. Webson and Pavlick (2022) observed that instructive prompts perform similarly with misleading or intentionally irrelevant prompts. Additionally, Ye and Durrett (2022) showed improvements due to few-shot chain-of-thought are not observed in question answering, or natural language inference. More critically, few-shot prompts are highly sensitive to the order in which the samples are provided, the prompt format, and the selection of in-context examples, (Lu et al., 2022; Zhao et al., 2021). Thus, it is crucial to design techniques that are robust to such changes in the prompt.
Inference as reasoning. Iterative inference over LLM outputs has been proposed for tackling true/false question answering and commonsense question answering (Jung et al., 2022; Liu et al., 2022a). Xie et al. (2021) presents a Bayesian inference perspective on in-context learning, and Dohan et al. (2022) formalizes and unifies existing prompting techniques in a probabilistic framework. Our work generalizes such approaches to perform arbitrary probabilistic inference outside of the LLM.</p>
<h2>5 Conclusion</h2>
<p>In this paper we presented ThinkSum, a two-step probabilistic inference paradigm that reasons over sets in a structured manner. The fast thinking stage of ThinkSum allows elementary string manipulations as well as natural language prompting, which may enable numerous approaches to solve a natural language task. Even with far smaller model variants, ThinkSum achieves state-of-the-art results on ten difficult tasks in BIG-bench using GPT-family models. The two-step paradigm allows operating over sets instead of manipulating the prompt itself, preventing sensitivity to prompt format during the probabilistic inference in ThinkSum, which is performed outside of calls to the LLM. As a result, ThinkSum is more robust to prompt design, yields more interpretable predictions, and can be combined with many probabilistic inference approaches to tackle a diverse set of tasks.</p>
<h2>Acknowledgments</h2>
<p>The authors thank Alexandros Graikos, Sudha Rao, and Alessandro Sordoni for valuable discussions.</p>
<h2>Limitations</h2>
<p>Our proposed ThinkSum has demonstrated strong performance on thirteen challenging BIG-bench tasks. However, it is important to acknowledge certain limitations of the system.</p>
<p>Firstly, as the number of objects or facts that are reasoned over increases, the computation cost will also rise. However, increasing the number of objects will also make the task harder, and direct prompting may cease to work at all (as we indeed observe in BIG-bench results, such as LOGICAL DEDUCTION with more than five objects), while ThinkSum offers a generalizable methodology, as the atomic Think operations do not increase in complexity as the number of objects grows.</p>
<p>Secondly, when solving a new task, it is necessary to expend human effort to select specific operations in each step, as outlined in $\S 2$. This limitation is shared with prompt engineering of all kinds, including direct or chain-of-thought prompting: finding a prompt for a new task requires an often-cumbersome prompt engineering procedure. We have described ThinkSum as a general twostage paradigm, with an external inference step. This generality aims to facilitate the adaptation of ThinkSum to new tasks, with minimal modifications to the Think and Sum steps. Work on automating the prompt engineering procedure (Zhou et al., 2022b) is a promising path towards overcoming this limitation. An alternative to prompt engineering that does not require such human effort is tuning (i.e., differentiable end-to-end learning) of prompts or model parameters; however, this remains impractical for GPT-3-scale models, and attempts to tune models directly on symbolic reasoning chains have met with limited success (Kassner et al., 2020).</p>
<p>Last but not least, ThinkSum has mainly been evaluated with GPT-3 (davinci) and InstructGPT (text-davinci-002) models. To further improve performance, it may be beneficial to apply ThinkSum to more recent instruction-tuned models such as Flan-PaLM (Chowdhery et al., 2022; Chung et al., 2022), text-davinci-003, ChatGPT, and GPT-4, which seem more capable of robustly performing Think steps.</p>
<h2>Ethics and impact statement</h2>
<p>We foresee no direct or immediate societal impacts arising from this work. However, we would like to emphasize that relying solely on LLMs' associative reactions to prompts can lead to undesired bias in the behaviour of systems. Control of LLMs' reasoning in the way we have proposed can potentially mitigate such bias, due both to the decomposition of the argumentation process into interpretable fact-retrieval steps and to the averaging effect of smoothing out spurious triggers when aggregating many hypotheses and reasoning chains.</p>
<h2>References</h2>
<p>Yoshua Bengio. 2017. The consciousness prior. arXiv preprint arXiv:1709.08568.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Neural Information Processing Systems (NeurIPS).</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39(1):1-38.</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. 2022. Language model cascades. arXiv preprint arXiv:2207.10342.</p>
<p>Nouha Dziri, Andrea Madotto, Osmar Zaïane, and Avishek Joey Bose. 2021. Neural path hunter: Reducing hallucination in dialogue systems via path grounding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2197-2214, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Anirudh Goyal and Yoshua Bengio. 2020. Inductive biases for deep learning of human cognition. arXiv preprint arXiv:2011.15091.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7038-7051, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint arXiv:2205.11822.</p>
<p>Daniel Kahneman. 2011. Thinking, fast and slow. Macmillan.</p>
<p>Nora Kassner, Benno Krojer, and Hinrich Schütze. 2020. Are pretrained language models symbolic reasoners over knowledge? In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 552-564, Online. Association for Computational Linguistics.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. 2022a. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3154-3169, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and Bill Dolan. 2021. A token-level reference-free hallucination detection benchmark for free-form text generation. arXiv preprint arXiv:2104.08704.</p>
<p>Zihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai Prabhumoye, Wei Ping, Mohammad Shoeybi, and Bryan Catanzaro. 2022b. Multi-stage prompting for knowledgeable dialogue generation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1317-1337, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086-8098, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Nikolay Malkin, Zhen Wang, and Nebojsa Jojic. 2022. Coherence boosting: When your pretrained language model is not paying enough attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8214-8236, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316-5330, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021a. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>Maxwell Nye, Michael Tessler, Josh Tenenbaum, and Brenden M Lake. 2021b. Improving coherence and consistency in neural sequence models with dualsystem, neuro-symbolic reasoning. Neural Information Processing Systems (NeurIPS).</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Neural Information Processing Systems (NeurIPS).</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training Gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784-3803, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4615-4629, Online. Association for Computational Linguistics.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.</p>
<p>Amos Tversky and Daniel Kahneman. 1974. Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. Science, 185(4157):1124-1131.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Rationaleaugmented ensembles in language models. arXiv preprint arXiv:2207.00747.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Albert Webson and Ellie Pavlick. 2022. Do promptbased models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300-2344, Seattle, United States. Association for Computational Linguistics.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080.</p>
<p>Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot in-context learning. arXiv preprint arXiv:2205.03401.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D Goodman. 2022. STaR: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. International Conference on Machine Learning (ICML).</p>
<p>Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvininejad. 2021. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393-1404, Online. Association for Computational Linguistics.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022a. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022b. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910.</p>
<h1>A Additional tasks</h1>
<p>Descriptions of all the tasks studied here can be found in §C.</p>
<h2>A. 1 Uncertainty and hallucination detection</h2>
<p>LLMs are prone to generating hallucinations that contain incorrect statements. The likelihoods of these statements are often dominated by short plausible patterns, which also makes it difficult for LLMs to evaluate their own uncertainty about a fact. Thus, detection (Liu et al., 2021; Zhou et al., 2021) and reduction of such hallucinations is crucial for widespread use of LLMs in real applications (Dziri et al., 2021; Shuster et al., 2021).</p>
<h2>A.1.1 Sports understanding</h2>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure A.1: Example posterior probabilities generated from text-davinci-002 for SPORTS UNDERSTANDING with the description "threw a touchdown". The basketball player given in the question Draymond Green has a much lower posterior probability than the generated football players, from which we conclude the sentence "Draymond Green threw a touchdown." is implausible.</p>
<p>Questions in Sports understanding ask to determine whether it is 'plausible' or 'implausible' that a professional sports player $x$ (e.g., 'Draymond Green', a basketball player) performed an action $a$ associated with a sport (e.g., 'threw a touchdown', an action in American football). It is implied that the combination of $x$ and $a$ is plausible if the sport with which player $x$ is associated coincides with the sport in which action $a$ is performed. We consider an approach that does not rely on identifying the latent variable (sport) as an intermediate step and is thus more generalizable to other domains.</p>
<p>We use an Example generation Think prompt to produce a set $S$ of players who perform action $a$, then do Posterior computation by normalizing the likelihood assigned by the LLM to each player in $S$, as well as $x$, performing action $a$ :</p>
<p>$$
\forall y \in S \cup{x} \quad p(y \mid a)=\frac{p_{\mathrm{LLM}}\left(" y a^{\prime \prime}\right)}{\sum_{y^{\prime} \in S \cup{x}} p_{\mathrm{LLM}}\left(" y^{\prime} a^{\prime \prime}\right)}
$$</p>
<p>The statement is considered to be implausible if the posterior on $x$ is sufficiently low (Thresholding) see Fig. A.1.</p>
<h2>A.1.2 Known unknowns</h2>
<p>Questions in the Known UNKNOWNS task ask to determine whether the answer to a question is a certain precise concept or 'unknown'.</p>
<p>Given a question $q$ (e.g., "What was the temperature in Cuzco on the day of the Emperor Vespasian's birth") and the candidate precise answer $a$ (e.g., $25^{\circ} \mathrm{C}$ ), we use a List extension prompt to generate a set $S$ of other possible answers to $q$. We then do a Posterior computation over $S$ and the original answer $a$, similar to that used for Sports understanding:</p>
<p>$$
\forall y \in S \cup{a} \quad p(y \mid q)=\frac{p_{\mathrm{LLM}}\left(" q ? y^{\prime \prime}\right)}{\sum_{y^{\prime} \in S \cup{a}} p_{\mathrm{LLM}}\left(" q ? y^{\prime \prime \prime}\right)}
$$</p>
<p>The answer $a$ is chosen if the posterior on $a$ is sufficiently high (Thresholding), and otherwise 'unknown' is chosen.</p>
<h1>A. 2 Translation between languages and writing systems</h1>
<p>This extends the results on LOGICAL DEDUCTION in §3.3.</p>
<h2>A.2.1 Russian misconceptions.</h2>
<p>In the Misconceptions Russian task, the true statement must be chosen out of a pair of Russian sentences: a statement $s$ and its negation $t$.</p>
<p>We first describe an approach that does not use translation and already performs better than random guessing - and better than baseline methods that simply select the more likely of the two statements using the largest GPT-3 model, which has sufficient knowledge of Russian. We compute the posterior over the two hypotheses " $s$ is true, $t$ is false" and " $s$ is false, $t$ is true":</p>
<p>$$
\begin{aligned}
&amp; p_{\mathrm{LLM}}\left(\left\ulcorner^{\mathrm{T}}\right.\right.\left.\left.\mid \text { "T or F? } s . \text { Answer: }\right\rvert\, \text { ) } p_{\mathrm{LLM}}\left(\left\ulcorner^{\mathrm{F}}\right.\right.\right.\right.\left.\left.\mid \text { "T or F? } t . \text { Answer: }\right\rvert\,\right) \
&amp; p_{\mathrm{LLM}}\left(\left\ulcorner^{\mathrm{F}}\right.\right.\right.\right.\left.\mid \text { "T or F? } s . \text { Answer: }\right\rvert\, \text { ) } p_{\mathrm{LLM}}\left(\left\ulcorner^{\mathrm{T}}\right.\right.\right.\right.\right.\right.\right.\left.\mid \text { "T or F? } t . \text { Answer: }\right\rvert\,
\end{aligned}
$$</p>
<p>where T denotes True and F False in the actual prompt. This is a kind of Product aggregation. If the posterior on the first option is higher, $s$ is chosen as the true statement; otherwise, $t$ is chosen.</p>
<p>This approach can be combined with a Translation prompt that produces translations of $s$ and $t$ into English, then uses these translations in place of $s$ and $t$ in the above computations. The approach can be further extended by sampling a set of translations and performing Mixture aggregation over the translations. Our reported result uses 10 generated translation for each statement, but it is only $2 \%$ higher than the result using one generated translation.</p>
<h2>A.2.2 Emoji movie</h2>
<p>The multiple-choice EMOJI MOVIE task requires selecting the name of a movie from a list $\left{m_{i}\right}$ that is best described by a sequence of emoji symbols $s=\left(s_{1} \ldots s_{n}\right)$. An Order inversion prompt performs best on this task using the Davinci variant of GPT-3: choosing the answer</p>
<p>$$
\underset{i}{\arg \max } p_{\mathrm{LLM}}(s \mid \text { "Emoji describing the movie } m_{i} \text { "). }
$$</p>
<p>We also attempt to use a Translation prompt to obtain a single-word English description $w_{j}$ of each emoji $s_{j}$ in $s$, then score using</p>
<p>$$
\underset{i}{\arg \max } p_{\mathrm{LLM}}\left(w_{1} \ldots w_{n} \mid \text { "Words describing the movie } m_{i} \text { "). }\right.
$$</p>
<p>This approach performs slightly better than Order inversion alone using InstructGPT. However, it does not work with the base GPT-3 models, which do not as reliably translate emoji to English.</p>
<h2>A.2.3 Persian QA</h2>
<p>We solve this standard extractive question answering task by simply translating the passage and question from Persian to English using a Translation prompt, generating English text, up to the first period or line break, following the concatenation of the translated prompt and question, and translating the result back to Persian using another Translation prompt.</p>
<p>No few-shot algorithms have above zero accuracy on this task, indicating models' knowledge is sufficient to translate between languages (probably due to the presence of paired data in the training corpus), but insufficient to reason in the source language without passing through an intermediate latent variable, the translation.</p>
<p>Finally, note that the accuracy is evaluated by exact string match, which contributes to the very low scores. We observed that the answers generated by ThinkSum are often paraphrases or terms related to the correct answers, which suggests that the result could be improved by using the knowledge that the target string always appears verbatim as a substring of the prompt.</p>
<h1>A. 3 Semantic relatedness</h1>
<p>This extends the results on ODD ONE OUT in $\S 3.2$.</p>
<h2>A.3.1 Phrase relatedness</h2>
<p>Each question in the multiple-choice PHRASE RELATEDNESS task requires to determine which of a given set of words or phrases $\left{w_{i}\right}$ is related to a query phrase $q$. We query the LLM for the likelihood of $q$ following a List of words prompt to form a vector of likelihoods:</p>
<p>$$
p_{i}=p_{\mathrm{LLM}}\left(q \mid \text { "List of words: } w_{i}, \text { " }\right)
$$</p>
<p>The answer selected is the one with highest likelihood, $\arg \max <em i="i">{i} p</em>$ (a trivial Sum operation). We note that this is also an instance of Order inversion: the query is scored following a prompt in which each of the candidate answers is substituted.</p>
<h2>A.3.2 Codenames</h2>
<p>Each question in CODENAMES requires selecting the $k$ words from a set $\left{w_{i}\right}$ that are most closely related to a query word $q$. We form a vector $p_{i}$ in the same way as for PHRASE RELATEDNESS, then select the top $k$ entries in $p_{i}$ to produce the output. ${ }^{2}$</p>
<h2>A. 4 Substitution and aggregation</h2>
<p>We give two other example of substitution and aggregation operations complementing the experiments on INVENTED WORDS ( $\S 3.1$ ) and ODD ONE OUT (§3.2).</p>
<h2>A.4.1 Novel concepts</h2>
<p>In the multiple-choice NOVEL CONCEPTS task, a set of words or phrases $W=\left{w_{i}\right}$ and a set of statements $S=\left{s_{j}\right}$ with third-person plural pronoun subjects ('They all...') are given, and the statement which is true for all items in $W$ must be determined.</p>
<p>We treat each statement $s_{j}$ as a template, into which words $w$ can be substituted by replacing 'They all' with $w$. Denoting by $s_{j}\langle w\rangle$ the substitution of $w$ into $s_{j}$, we form a $|W| \times|S|$ matrix $P_{i j}$ by scoring the Substitution of each word into each statement and considering the Ratio of likelihoods with the template without substitution: $P_{i j}=\frac{p_{\mathrm{LLM}}\left(s_{j}\left\langle w_{i}\right\rangle\right)}{p_{\mathrm{LLM}}\left(s_{j}\right)}$. We then perform Product aggregation to select the statement which is most likely to be generated by all words in the set. To be precise, the selected statement is $\arg \max <em i="i">{j} \prod</em>$.} P_{i j</p>
<h2>A.4.2 Code line description</h2>
<p>We solve the CODE LINE DESCRIPTION task, in which a correct comment for a code snippet is to be chosen, using Order inversion and Substitution techniques.</p>
<p>The greatest gain - amounting for all but $1 \%$ of the improvement relative to direct prompting - arises from Order inversion. Instead of ranking the candidate comments $c$ by their likelihood following the given code $s$ (i.e., $p(c \mid s)$ ), we score each candidate comment $c$ by the likelihood of the code to follow $c$ formatted as a Python comment ( $p(s \mid$ "# $c$ ")).</p>
<p>We also experimented with Substitution and Product aggregation, which yielded an additional small accuracy gain. The code snippets are written in Python, which requires code to be formatted using an arbitrary but consistent number of spaces for line indentation. Using the knowledge that the correct comment should be most likely to generate the program in any of its equivalent representations, we scored comments in the manner described in the preceding paragraph, but with $s$ reformatted with different number of indentation spaces $n$. The resulting scores were then multiplied over $n=1,2, \ldots, 6$ and the highest-scoring comment selected.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure B.1: Margin between 0 -shot GPT-3 and average human performance for BIG-bench Lite tasks. Using ThinkSum, we address many of the tasks that have greater than $10 \%$ performance margin with average human, and significantly reduce and often overturn the margin.</p>
<h1>A. 5 Other tasks</h1>
<h2>A.5.1 Language identification</h2>
<p>The multiple choice LANGUAGE IDENTIFICATION task is similar in form and solution to CODE LINE DESCRIPTION and we include it for completeness to show the large difference that can be made by Order inversion.</p>
<p>Rather than scoring all candidate language names $\ell$ following the given sentence $s$ (i.e., $p(s \mid \ell)$ ), we instead score each language name $\ell$ by $p(s \mid$ "The following is a sentence in $\ell$ :") and select the highest-scoring $\ell$ as the answer.</p>
<h2>B BIG-bench Lite</h2>
<p>Figure B. 1 shows the performance margin between an average human and zero-shot GPT-3 on tasks in BIG-bench Lite, a select subset of tasks chosen by the authors of the benchmark to showcase the most important aspects of LLMs that need improvement. The vertical black bar separates the dataset into tasks where GPT-3 is already within the margin of just $10 \%$ compared to the average human accuracy, and the harder tasks (on the left). We show in the main text that some of these harder tasks, in particular EMOJI MOVIE, CONCEPTUAL COMBINATIONS, KNOWN UNKNOWNs, NOVEL CONCEPTS, MISCONCEPTIONS RUSSIAN and LOGICAL DEDUCTION, the margins are shrunk considerably, often exceeding average human performance. Other tasks in BIG-bench lite such as LOGIC GRID PUZZLE and SYMBOL INTERPRETATION share a similar structure to the addressed by ThinkSum, and thus could be investigated as part of future work. Another example where ThinkSum can be applied is the CODE LINE DESCRIPTION task, where we observe in our preliminary experiments that a simple order inversion can significantly outperform average human accuracy.</p>
<h2>C Task descriptions</h2>
<h2>C. 1 Hallucination detection</h2>
<p>Known unknowns. In this task, the aim is to measure the ability of LLMs to identify whether the answer to a question is known, or unknown. If the answer to the question cannot be known, an imaginary answer</p>
<p>is generated, and the model is asked to choose between the imaginary answer and the word "Unknown". If the question has a correct answer, the correct answer and "Unknown" are provided as options. An example question from this task is "How often did Abraham Lincoln cut his toenails?", and the options are "Every Saturday night","Unknown". Similar to the sports understanding task, we can use an LLM to generate other plausible answers with the following prompt: How often did Abraham Lincoln cut his toenails? A possible answer is: \n 1. Every Saturday night. List 4 other possible answers in the same format as the first: $\backslash n 2$. Then, the answer given in the question is predicted to be "known" if its posterior is higher by the second most likely option by some margin. In our experiments, we chose this value to be $1 / N_{e}$ where $N_{e}$ is the number of examples, including the original option.
Sports understanding. In this task, artificial sentences are constructed using the name of professional sports players and actions from particular sports. The model is then asked to identify whether the sentence is plausible, where a sentence is considered plausible if the sport of the player matches the sport of the action described in the sentence. An example from the task is "Statement: Draymond Green threw a touchdown. Plausible/implausible?"</p>
<p>For ThinkSum to be able to parse outputs, GPT-3 generations need to be in a pre-determined format. While larger models can obey a format without demonstrations, for smaller models it is helpful to demonstrate the format with an example. Thus, we use the following prompt: "List 4 examples of players who scored a rabona goal.\n 1. Cristiano Ronaldo\n 2. Erik Lamela\n 3. Mario Balotelli\n 4. Angel Di Maria\n List 4 examples of players who threw a touchdown.\n1.".</p>
<h1>C. 2 Translation</h1>
<p>Emoji movie. In this task, the LLM is presented with emojis that describe the plot of a movie, and the model is asked to guess the correct movie from the given options. The task evaluates the LLMs understanding of emojis, and its knowledge of popular movies. An example from the task is "What movie does this emoji describe? {lying-face-emoji} \n, pinocchio \n heat \n the big sleep, \n inception \n lilo \&amp; stitch
Misconceptions Russian. In this task, the LLM is presented with sentences in Russian that contain misconceptions, and their true counterparts. The task evaluates if the LLM believes common misconceptions. One example of a target pair from the task, translated to English is as follows: "Vaccines do not cause autism. \n Vaccines cause autism."
ParsiNLU reading comprehension. This is a standard extractive QA task in which a short passage and question are provided in Persian and the model must generate the answer, also in Persian.</p>
<h2>C. 3 Semantic relatedness</h2>
<p>Phrase relatedness. In this task, an input phrase, and four target phrases are presented to the language model. The language model is asked to identify the most related choice from the listed target options. An example from the task is "For each word or phrase, identify the most related choice from the listed options. \n Input: home town \n Option: town center \n Option: location \n Option: native city \n Option: home run"
Codenames. In this task, the language model is asked to identify words associated with a given word. An example from the task is "Try to identify the 2 words best associated with the word WHITE from the following list: \n book, anchor, rainbow, shoulder, tunnel, sack, drum, pacific, page, mark, gear, glacier. Give your answer in alphabetical order."
Odd one out. This task is aimed at evaluating the capability of LLMs in semantic relatedness. This task presents the model with four to six words, where all words except one word are semantically or grammatically related to each other. The goal for the language model is to identify the odd word. An example question from the task is "Pick the odd word out: glass, head, arm, leg, hand, foot".</p>
<h2>C. 4 Concept understanding</h2>
<p>In the following tasks, the shared goal is to test the ability of LLMs on concepts over entities that have likely not been observed during training.</p>
<p>Conceptual combinations: Invented words. In this task, the LLM is provided with two invented words, and their definitions in the input. The LLM is then asked to infer the most plausible meaning resulting from the combination of the invented words. As the words are invented, they are not present in the training set, and the LLM needs to understand and combine the definitions of the invented words to reason about the meaning of the combination. An example is: 'The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling. Question: Which of the following sentences best characterizes binne bams?". Similar to SPORTS UNDERSTANDING, we can use the following prompt to force the LLM to obey a fixed format: "List synonyms of binne, separate synonyms by comma:"</p>
<p>Novel concepts. In this task, the LLM is presented with two to four disparate entities that typically would not co-occur frequently, but share an underlying conceptual or linguistic concept. The aim is to test the ability of the LLM to reason about entities that are unlikely to have been observed in the same context during training. In a multiple-choice setting, the LLM is given concepts relating to the entities, and is asked to generate the intended concepts against carefully chosen tempting distractors. The choices are not presented in the prompt. An example question from the task is as follows: "What do the following have in common? 1) bumble bees 2) 01010101 3) race cars", and the answer options are They all make noise, "They all are yellow, They all are binary, They all go fast, They all have stripes".</p>
<h1>C. 5 Other tasks</h1>
<p>Two multiple-choice tasks test the LLM's knowledge of specific domains, such as uncommon languages and programs.
Code line description. This task requires the LLM to select the appropriate text description, out of four choices, for a short snippet of Python code, that could act as a comment describing the behaviour of a function.</p>
<h2>C.5.1 Language identification.</h2>
<p>This task requires the LLM to select, out of eleven choices, the language in which a text is written. The languages represent a diversity of language families and writing systems and most are very infrequent in text found on the Internet.</p>
<h2>D Additional experimental details</h2>
<p>Our experiments are performed using four different sizes of GPT-2 (Small, Medium, Large, and XL) (Radford et al., 2019), GPT-3 with four different model sizes (ada,babbage,curie,davinci) (Brown et al., 2020), and InstructGPT (Ouyang et al., 2022). All GPT-3 experiments are run between August 2022 and September 2022 by using the OpenAI API. Our GPT-2 experiments were run in PyTorch (Paszke et al., 2019) and the Hugging Face Transformers library with a Tesla K80 GPU.</p>
<h2>D. 1 Hyperparameters</h2>
<p>Maximum generation length. For tasks that require example and list generation, such as CONCEPtuAl combinations, Known unknowns, and SPORTS UNDERSTANDING, we use max_tokens $=100$. For fact generation in ODD ONE OUT with auxiliary knowledge and ThinkSum, we use max_tokens $=$ 1000 .</p>
<p>Temperature. All GPT-2 experiments used temperature $=0.5$. For SPORTS UNDERSTANDING and translation tasks, we used temperature $=0.5$ to promote diversity of generated plausible options. All other experiments used temperature $=0$ (greedy decoding).
Number of examples $\left(N_{e}\right)$. For Conceptual combinations we used $N_{e}=2$, and for Known UNKNownS and SPORTS UNDERSTANDING we used $N_{e}=4$.</p>
<p>Threshold. A threshold of 0.01 was used for SPORTS UNDERSTANDING.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure D.1: Probabilities of different (in)equalities according to GPT-3 text-davinci-002 (logit).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure D.2: Auxiliary knowledge prompting applied to Odd ONE OUT. Facts are generated using the "list differences" prompt described in Figure 2 (right) and post-processed according to §D.3.</p>
<h1>D. 2 Using an LLM to evaluate inequalities.</h1>
<p>Using GPT-3 or external algorithms to evaluate inequalities. We show how a LLM can be used to find the truth values of inequalities involving small numbers, rather than resorting to calls to an external system that is aware of arithmetic. Fig. D. 1 shows the matrix of posterior probabilities evaluated using InstructGPT (text-davinci-002) for strings of form " $x=y$ ", " $x<y$ ", " $x>y$ " for $x, y \in{1, . ., 9}$. The probabilities are computed using prompts of the form "True or false: $x&lt;y$ ? The answer is:" and normalizing the probability of the first token over the two options "true" and "false". These are the probabilities evaluated in (1).</p>
<h2>D. 3 Knowledge generation details</h2>
<p>Post-processing. In our knowledge generation experiments for both ThinkSum and the auxiliary knowledge approach, we post-process the generated knowledge statements, to ensure formatting does not harm the predictions of each method. We first remove the extra spaces and the numbers and punctuation generated by the LLM before each fact while enumerating the items of the list. Later, we only keep sentences that contain only one of the objects of interest from the task, to make sure each sentence contains a knowledge statement into which any of the objects can be substituted. Finally, sentences with less than 3 words are removed as these are not likely to contain informative statements.
Auxiliary knowledge. For auxiliary knowledge experiments, we prepend the generated and postprocessed knowledge statements before the question in the task. An example is illustrated in Figure D.2.</p>
<h2>D. 4 Inference Cost for ThinkSum</h2>
<p>The inference cost for ThinkSum scales with the number of parallel calls to the LLM, which is determined for each task by the number of Think prompts used and the number of objects for which likelihood computations are required at the Sum stage. For the tasks that we considered, as the number of Think</p>
<p>prompts is not typically high and the prompts are short, the inference cost increase is marginal. In some cases, ThinkSum is faster than chain-of-thought prompting due to its ability to perform parallel calls to the LLM. For instance, ThinkSum is $23 \%$ faster for PHRASE Relatedness compared to chain-of-thought approaches with 5 facts generated using InstructGPT.</p>
<h1>E Expectation Maximization</h1>
<p>We model items $i \in I$ and facts $f \in F$ as being generated from a latent class $c \in{0,1}$. The distribution is modeled as:</p>
<p>$$
P(i, f \mid c)=P(i \mid c) P(f \mid c) \quad P(i, f)=\sum_{c} P(c) P(i, f \mid c)
$$</p>
<p>where $P(i, f)$ is a matrix of likelihoods from the LLM and the semantic components, groupings $P(i \mid c)$ and $P(f \mid c)$. The iterative expectation-maximization (EM; Dempster et al., 1977) algorithm to derive $P(i \mid c)$ and $P(f \mid c)$ has the following updates:</p>
<p>$$
\begin{aligned}
Q(c \mid i, f) &amp; \propto P(i \mid c) P(f \mid c) P(c) \
P(i \mid c) &amp; \propto \sum_{f} P(i, f) Q(c \mid i, f) \
P(f \mid c) &amp; \propto \sum_{i} P(i, f) Q(c \mid i, f) \
P(c) &amp; \propto \sum_{i, f} P(i, f) Q(c \mid i, f)
\end{aligned}
$$</p>
<p>where $Q(c \mid i, f)$ is the posterior distribution over the latent class $c$ that we maintain for each pair $(i, f)$. EM is run for 200 iterations, which is more than sufficient for convergence.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ODD ONE OUT</th>
<th style="text-align: center;">Words: blue, pink, magenta, banana <br> All words are colors except banana. The odd one out is banana.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Words: pencil, eraser, baby, rule, notebook <br> All words are office supplies except baby. The odd one out is baby.</td>
</tr>
<tr>
<td style="text-align: center;">PHRASE RELATEDNESS</td>
<td style="text-align: center;">For each word or phrase, identify the most related choice from the listed options. <br> Input: Ice Cream <br> Option: Antarctica <br> Option: Titanic <br> Option: Dessert <br> Option: Sour Cream <br> Ice cream is a type of dessert. Therefore, ice cream and dessert are the most related. <br> Answer: Dessert</td>
</tr>
<tr>
<td style="text-align: center;">KNOWN UNKNOWNS</td>
<td style="text-align: center;">What was the population of San Francisco in 2018? <br> Option: 879,676 <br> Option: Unknown <br> The question asks the population of San Francisco in 2018, for which data can be collected. <br> Population data for cities on a yearly basis is available, and thus the answer is known, and <br> it is 879,676 . <br> Answer: 879,676 <br> What was the population of San Francisco yesterday? <br> Option: 891,402 <br> Option: Unknown <br> The question asks the population of San Francisco yesterday. As it is not possible to know <br> the exact population of a city on a daily basis, the answer for this question is unknown. <br> Answer: Unknown</td>
</tr>
<tr>
<td style="text-align: center;">LOGICAL DEDUCTION</td>
<td style="text-align: center;">On a table, there are five plates: a black plate, a white plate, a green plate, a blue plate, and a red plate. The white plate is bigger than the green plate. The red plate is the biggest. The black plate is bigger than the blue plate. The black plate is smaller than the green plate. Which plate is the smallest? <br> Option: The red plate is the smallest. <br> Option: The black plate is the smallest. <br> Option: The white plate is the smallest. <br> Option: The green plate is the smallest. <br> Option: The blue plate is the smallest. <br> The black plate is bigger than the blue plate. The black plate is smaller than the green plate, as a result the green plate is bigger than the blue plate as well. The white plate is bigger than the green plate, which is bigger than the blue plate. As a result, the green plate is bigger than the blue plate. The red plate is the biggest, so it is bigger than the blue plate. Since all other plates are bigger than the blue plate, the blue plate is smallest. <br> Answer: The blue plate is the smallest.</td>
</tr>
<tr>
<td style="text-align: center;">INVENTED WORDS</td>
<td style="text-align: center;">The word 'borger' are animals who bite specific things for fun, and the word 'folpt' is a type of a chewy toy. Question: Which of the following sentences best characterizes borger folpts? <br> Option: Borger folpts are leashes for animals. <br> Option: Borger folpts are toys for infants. <br> Option: Borger folpts are hard to swallow. <br> Option: Borger folpts are pet toys. <br> Borgers are animals, and folpts are chewy toys. Therefore, borger folpts are chewy toys that animals, or pets, can play with. Therefore, the answer is borger folpts are pet toys. Answer: Borger folpts are pet toys.</td>
</tr>
</tbody>
</table>
<p>Table A.1: Few-shot demonstrations used for chain of thought (Table 4).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Because the task is evaluated by BLEU score against the reference answers listed in alphabetical order, we perform the additional step of converting the top indices to the answer in the right format. Alphabetization of short lists is trivial in code, but can also very reliably be done by prompting GPT-3.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>